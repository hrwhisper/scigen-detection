we present a non-traditional retrieval problem we call subtopic retrieval. the subtopic retrieval problem is concerned with finding documents that cover many different subtopics of a query topic. in such a problem  the utility of a document in a ranking is dependent on other documents in the ranking  violating the assumption of independent relevance which is assumed in most traditional retrieval methods. subtopic retrieval poses challenges for evaluating performance  as well as for developing effective algorithms. we propose a framework for evaluating subtopic retrieval which generalizes the traditional precision and recall metrics by accounting for intrinsic topic difficulty as well as redundancy in documents. we propose and systematically evaluate several methods for performing subtopic retrieval using statistical language models and a maximal marginal relevance  mmr  ranking strategy. a mixture model combined with query likelihood relevance ranking is shown to modestly outperform a baseline relevance ranking on a data set used in the trec interactive track.
categories and subject descriptors
h.1  information search and retrieval : retrieval models- language models  dependent relevance
general terms
measurement  algorithms
keywords
subtopic retrieval maximal marginal relevance  language models
1. introduction
　the notion of relevance is central to many theoretical and practical information retrieval models. traditional retrieval models assume that the relevance of a document is independent of the relevance of other documents. this makes it possible to formulate the retrieval problem as computing the relevance  or some correlated
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
metric  for each document separately  and then ranking documents by probability of relevance . in reality  however  this independent relevance assumption rarely holds; the utility of retrieving one document  in general  may depend on which documents the user has already seen. as an extreme example  a relevant document may be useless to a user if the user has already seen another document with the same content. another example is when the user's information need is best satisfied with several documents working together; in this case  the value of any single document may depend on what other documents are presented along with it. some of the issues concerning ranking interdependent documents are discussed in  1  1 .
　in this paper  we study the subtopic retrieval problem  which requires modeling dependent relevance. the subtopic retrieval problem has to do with finding documents that cover as many different subtopics of a general topic as possible. for example  a student doing a literature survey on  machine learning  may be most interested in finding documents that cover representative approaches to machine learning  and the relations between these approaches. in general  a topic often has a structure that involves many different subtopics. a user with a high-recall retrieval preference would presumably like to cover all the subtopics  and would thus prefer a ranking of documents such that the top documents cover different subtopics.
　the same problem  called  aspect retrieval   is investigated in the interactive track of trec  where the purpose is to study how an interactive retrieval system can best support a user gather information about the different aspects of a topic  1  1  1 . here we study the task of automatically ranking documents so as to give good subtopic retrieval. in other words  we retain the basic  query in-ranked list out  model used in traditional retrieval  but seek to modify the ranking so as to include documents relevant to many subtopics.
　clearly  methods based on a traditional relevance-based ranking are unlikely to be optimal for such a problem. moreover  traditional evaluation metrics are also inappropriate for this new retrieval task. we present an initial study of this new problem  describing evaluation metrics  possible methods  and experimental results with these methods.
1. data set
　in order to measure how well a ranking covers different subtopics of some high-level topic  we must have judgments that tell us which documents cover which subtopics. fortunately  the trec interactive track has accumulated many such judgments over the three years when the task was evaluated  trec-1  trec-1  and trec1 . we collect all these judgments and use them for our analysis and experiments. the document collection used in the interactive track is the financial times of london 1 collection  part of the trec-1 ad hoc collection . this collection is about 1mb in size and contains 1 documents  with an average document length of roughly 1 words. six to eight new topics were introduced each year  for a total of 1 topics  all of which were used in the work reported here. these interactive track topics were formed by slightly modifying the original ad hoc trec topics  typically by removing the  narrative  section and adding an  instance  section to explain what a subtopic means for the topic. we generate the query for each topic by concatenating the title and the description of the topic.1
　the following is an example query from trec1 interactive track  number 1i .
number: 1i
title:	robotics
description:
what are the applications of robotics in the world today 
instances:
in the time alloted  please find as many different applications of the sort described above as you can. please save at least one document for each such different application. if one document discusses several such applications  then you need not save other documents that repeat those  since your goal is to identify as many different applications of the sort described above as possible.
　for each topic  the trec  nist  assessors would read a pool of documents submitted by trec participants  and gradually identify a list of instances  i.e.  subtopics  and record which documents contain or cover which instances. for example  for the sample topic 1i shown above  they identified 1 different subtopics  some of which are shown below:
1 'clean room' applications in healthcare & precision engineering 1 spot-welding robotics 1 controlling inventory - storage devices ... ...
　for this topic  the judgment for each document can be represented as a bit vector with 1 bits  each indicating whether the document covers the corresponding subtopic. in our data set  the number of subtopics  i.e.  the range of vector lengths  ranges from 1 to 1  with an average of 1. the number of judged relevant documents available also differs for different topics  with a range of 1 to 1 and an average of about 1 documents per topic. there are also some judgments of non-relevant documents. we did not use these judgments; instead  we assume that any unjudged document is non-relevant  and therefore covers no relevant subtopic. this is a strong assumption  but our hope is that this biased evaluation will still be useful for comparing different rankings. more details about these data and the interactive track can be found in  1  1  1 .
　note that the granularity of subtopics and the criteria to judge whether a document covers a subtopic are inevitably vague and subjective. a binary judgment also means that a document is assumed to either cover or not cover a subtopic  while in reality  the coverage may be somewhere in between.
1. evaluation metrics
　we wish to explore methods for producing a ranked list which performs well on the subtopic retrieval task. it is not immediately

1
 while we have not explored it  a structured query  cf.   can potentially be formulated to include keywords for different subtopics.

recall r
figure 1: typical curves for the functions minrank s r  and minrank sopt r   defined as the minimal rank k at which subtopic recall of r is reached for system s and an optimal system sopt. subtopic precision is defined as the ratio of minrank sopt r  and minrank s r .
obvious how one should evaluate such a ranking. intuitively  it is desirable to include documents from many different subtopics early in the ranking  and undesirable to include many documents that redundantly cover the same subtopics.
　one natural way to quantify success according to the first goal- of covering many different subtopics quickly-is to measure the number of different subtopics covered as a function of rank. more precisely  consider a topic t with na subtopics a1 ... ana  and a ranking d1 ... dm of m documents. let subtopics di  be the set of subtopics to which di is relevant. we define the subtopic recall  s-recall  at rank k as the percentage of subtopics covered by one of the first k documents  i.e. 
s-recall at
na
1 accounting for intrinsic difficulty
　clearly it is desirable for subtopic recall to grow quickly as k increases. however  it is not at all clear what constitutes a  good  level of recall for a particular topic t. for example  consider two topics t1 and t1. for topic t1  there are m/1 relevant documents and m/1 subtopics  and every document di covers exactly one distinct subtopic ai. for topic t1  there are m/1 relevant documents but m subtopics  and every document di covers subtopics ai am/1 ... am. for both t1 and t1  the ranking d1 d1 ... dm/1 is clearly the best possible: however  subtopic recall for small rankings is much better for t1 than for t1. similarly  for any natural measure for redundancy  the degree to which documents in a ranking repeat the same subtopics  the ranking for t1 would appear much worse than the ranking for t1.
　this example suggests that for a measure to be meaningful across different topics  it must account for the  intrinsic difficulty  of ranking documents in a topic. we propose the following evaluation measure. if s is some ir system that produces rankings and r is a recall level  1 ＋ r ＋ 1  we define minrank s r  as the minimal rank k at which the ranking produced by s has s-recall r. we define the subtopic precision  s-precision  at recall r as
s-precision at r 《 minrank ssopt r  minrank   r 
where sopt is a system that produces the optimal ranking that obtains recall r-i.e.  minrank sopt r  is the smallest k such that some ranking of size k has subtopic recall of r.
　the idea of comparing performance to a theoretical optimal is not new ; however  this formulation of the comparison has some nice properties. specifically  we claim that subtopic recall and precision  as defined above  are natural generalizations of ordinary recall and precision  in the following sense: if minrank s r  were defined in terms of ordinary recall rather than subtopic recall  then ordinary precision could be defined as the ratio of minrank sopt r  to minrank s r .
　to see this  consider the hypothetical curves for minrank s r  and minrank sopt r  shown in figure 1. suppose that s and sopt are ordinary retrieval systems  and minrank is defined in terms of ordinary recall. since sopt orders all the relevant documents first  minrank sopt r  = r ， nr  where nr is the number of relevant documents for the topic . now consider a non-optimal system s that has precision p and recall r in the first kr documents. since recall is r  s retrieves rnr relevant documents in the first kr  and its precision is p = rnr/kr = minrank sopt r /minrank s r . the hypothetical curves in figure 1 are consistent with the performance of ordinary ranked retrieval systems: minrank sopt r  grows linearly  and minrank s r  becomes more gradually distant from the line for the optimal system  reflecting the fact that precision decreases as recall increases. since the shape of minrank sopt r  is predictable for ordinary retrieval  it is not necessary to explicitly account for it in measuring performance. for subtopic retrieval  however  minrank sopt r  may have a more complex shape.
　as concrete examples  the left-hand graphs in figures 1 and 1 show subtopic recall and subtopic precision for various ranking schemes  interpolated over 1 points in the usual way  and averaged over all 1 topics in our test suite.
　the s-precision and s-recall metrics are broadly similar to the cumulated gain  cg  measure proposed by jarvelin and kekalianen . however  the cg measure assumes the gain of each document to be independent of other documents  and thus is insufficient for our purposes; in contrast  the  gain  of each document in the sprecision metric depends on other documents.
1 penalizing redundancy
　intuitively  it is undesirable to include many documents that redundantly cover the same subtopics; however  this intuition is not accounted for in the measures of subtopic recall and precision.
　one way to penalize redundancy is to include an explicit measure of the cost of a ranking. we let the cost of a ranking be defined as
　　　　　　　　　　　　　　　k cost d1 ... dk 	《	 a|subtopics di | + b 
i=1
k
	=	a	|subtopics di | + kb
i=1
here b is the cost of presenting a document di to a user  and a is the incremental cost to the user of processing a single subtopic in di.
　proceeding by analogy to the measure introduced above  we define mincost s r  to be the minimal cost c at which the ranking produced by s has s-recall r. we then define the weighted subtopic precision  ws-precision  at recall level r to be
ws-precision at
where again sopt produces the optimal  lowest-cost  ranking that obtains recall r. note that s-precision is a special case of wsgreedy ranking algorithm
inputs: set of unranked documents u; ranking size k for i = 1 ... k do di = argmaxd（u value d;d1 ... di 1 
   u = u   {di} endfor return the ranking
figure 1: a generic greedy ranking algorithm
precision where b = 1 and a = 1. in this paper we will use costs of a = b = 1 for ws-precision.
　again  as concrete examples  the right-hand graphs in figures 1 and 1 show subtopic recall and weighted subtopic precision for various ranking schemes.
1 on computing the metrics
　computing s-precision and ws-precision require computing the optimal values minrank sopt r  or mincost sopt r . unfortunately  this is non-trivial  even given relevance judgments. indeed  it can be reduced to a minimum set-covering problem  which is np-hard. fortunately  the benchmark problems are of moderate size and complexity  and the minimum set cover can often be computed quite quickly using simple pruning heuristics. furthermore  a simple greedy approximation seems to obtain results nearly indistinguishable from exact optimization  except at the highest recall values for mincost.1 in the evaluations of this paper  we used exact values of minrank for all queries. we used exact values of mincost for all queries but one  query 1i   and used a greedy approximation to mincost for query 1i.
1. subtopic retrieval methods
　since it is computationally complex to find an optimal ranking for the subtopic retrieval problem  even when the subtopics are known  some kind of approximation is necessary in practice. a natural approximation is a greedy algorithm  which ranks documents by placing at each rank i the document di that is  best  for that rank relative to the documents before it in the ranking. a generic version of this greedy algorithm is shown in figure 1.
　the key here is to appropriately define the value function-i.e.  to quantify the notion of a  best  document di for rank i. intuitively  di should cover many subtopics not covered by the previous documents d1 ... di 1  and few of the subtopics covered by the previous documents. of course  one cannot compute such a metric explicitly in a value function  since the subtopics are not known to the retrieval system-only the initial query topic. such an evaluation metric must therefore be based on a subtopic model.
　an alternative to explicitly modeling subtopics is to use a similarity function that only implicitly accounts for subtopic redundancy. one such similarity-based approach is the maximal marginal relevance  mmr  ranking strategy . mmr instantiates

1 it is known that set cover is hard to approximate up to a logarithmic factor  and that the greedy algorithm achieves this factor . for the 1 topics considered here  however  the greedy algorithm's performance actually is much better: for the 1 queries for which mincost could be computed exactly  the ws-precision of the greedy approximation is more than 1% for all recall values up to 1  and for recall 1  the ws-precision of the greedy approximation is 1%. code implementing the exact and approximate greedy set covering algorithms is available on request from the authors.
the greedy algorithm of figure 1 using the value function valuemmr d;d1 ... di 1  = αsim1 d q     1   α maxsim1 d dj 
j i
where q is the original query  α is a parameter controlling the relative importance of relevance and novelty  sim1 is a typical retrieval similarity function  and sim1 is a document similarity function that is intended to capture redundancy  or equivalently novelty .
　here we will study both novelty and relevancy in the language modeling framework. first  we will present two ways to measure the novelty of a document  one based on the kl-divergence measure  and another based on a simple mixture model. we will then discuss how to combine novelty and relevance in a cost function.
1 novelty and redundancy measures
　let {θ1 ... θi 1} be the unigram language models for i   1 previously selected documents  which we refer to as reference language models. consider a candidate document di and the corresponding language model θi. our goal is to define a novelty score valuen for which valuen θi;θ1 ... θi 1  will indicate how much novel information document di contains.
1.1 single reference topic model
　let us first consider the simplest case  where we have a single reference model θo  where the o subscript indicates  old  . suppose θn is the new document model. how do we define valuen θn;θo  
　notice that novelty is an asymmetric measure: we are interested in measuring the information in θn which is new relative to θo  not the other way around. for unigram language models  a natural asymmetric distance measure is the kl-divergence d θn||θo   which can be interpreted as the inefficiency  e.g.  in compression  due to approximating the true distribution θn with θo. this leads to a value function of valuekl θn;θo  = d θn||θo .
　another plausible novelty measure is based on a simple mixture model. assume a two-component generative mixture model for the new document  in which one component is the old reference topic model and the other is a background language model  e.g.  a general english model . given the observed new document  we estimate the mixing weight for the background model  or the reference topic model   which can then serve as a measure of novelty or redundancy. the estimated weight can be interpreted as the extent to which the new document can be explained by the background model as opposed to the reference topic model. a similar idea  but with three-component mixture models  has been explored recently to measure redundancy in information filtering .
　more formally  let θb be a background language model with a mixing weight of λ. the log-likelihood of a new document d = w1...wn is
n
	l λ|d θo 	=	log  1   λ p wi |θo  + λp wi |θb  
　　　　　　　　　　　　i=1 and the estimated novelty score is given by
valuemix 
the em algorithm can be used to find the unique λ  that maximizes this score.
1.1 multiple reference topic models
　when there is more than one reference topic model  an appropriate account of the previous models must be made to compute a summarized novelty value for a document. one possibility is to compute a mixture  average  of the reference topic models  so that the problem is reduced to the single reference model case. another possibility is to compute a novelty score for di using each previous dj as a reference topic model θo  and to then combine these scores. the first method is straightforward. for the second  three obvious possibilities for combining the individual novelty scores are taking the minimum  maximum  and average. however  using the maximum distance is unreasonable  since a document would be judged as novel if it is different from a single old document dj  even the case where it is identical to another.
　with two novelty measures for a single reference model and two reasonable ways of computing a combined novelty score over multiple reference models  we have six different novelty measures  as shown in table 1.
basic measureaggregationdi vs dj scores combinedminaverageklklavgminklavgklmixturemixavgminmixavgmixtable 1: novelty measures based on language models.
1.1 comparison of novelty measures
　we compared all six novelty measures on the subtopic retrieval task. in order to focus on the effectiveness of novelty detection alone  we considered the special task of re-ranking relevant documents  using the greedy algorithm of figure 1 and value functions which are appropriate aggregations of the functions valuekl and valuemix. since none of the novelty measures can be used to select the very first document  we used the query-likelihood relevance value function with dirichlet prior smoothing; essentially all different rankings start with the same  presumably most likely relevant  document. the same query-likelihood relevance value function is also used to produce a ranking of all the relevant documents  which we use as our baseline.
　we evaluated the ranking using both the s-precision and wsprecision measures. the results are shown in figure 1. we make the following observations.
　overall  mixavg is the best performing novelty-based ranking  followed by minmix. particularly at high recall levels  mixavg is noticeably better than any of the other measures.
　for both measures  the relevance baseline ranking is relatively good at low levels of subtopic recall  and relatively poor at higher levels of subtopic recall. this is intuitive  since subtopics are more likely to be duplicated later in a ranking when we will have accumulated more subtopic instances. the novelty-based ranking schemes outperform the relevance measure most consistently on the ws-precision measure. this is to be expected since the wsmeasure more heavily penalizes redundancy.
　the kl-based ranking schemes are generally inferior to the mixture-based ranking schemes  by both measures. they are also  perhaps surprisingly  generally inferior to the baseline relevance ranking  especially at high subtopic recall levels. the minmix measure performs slightly better than the avgmix measure  and similarly  the minkl measure performs slightly better the avgkl measure. we note that minmix is most similar to the original mmr measure .

figure 1: comparison of the curves of s-precision  left  and ws-precision  right  versus s-recall for the six novelty measures and thebaseline relevance ranking.
1 combining relevance and novelty
　we now consider how to combine novelty and relevance in a retrieval model. based on other relevance-based retrieval experiments  1  1   we use kl-divergence as a measure for relevance  valuer  and mixavg as a measure of novelty  valuen . unfortunately  a direct interpolation of these measures would not make much sense  since they are not on the same scale. we note that the mixavg estimate of valuen can be loosely interpreted as the expected percentage of novel information in the document  or the probability that a randomly chosen word from the document represents new information. thus  we may consider two probabilities associated with a document d. one is the probability of relevance p rel|d   the other is the probability that any word in the document carries some new information p new|d . this leads to the following general form of the scoring function s di;d1 ... di 1  = c1p rel|di p new|di 

	+	c1p rel|di p new|di 

	+	c1p rel|di p new|di 
	+	c1p rel|di p new|di 
where c1  c1  c1  and c1 are cost constants.
　since whether a non-relevant document carries any new information is not interesting to the user  we assume that c1 = c1. furthermore  we assume that there is no cost if the document is both relevant and  1%  new  i.e.  that c1 = 1.
　intuitively  c1 is the cost of user seeing a relevant  but redundant document  whereas c1 the cost of seeing a non-relevant document. we will finally assume that c1   1  i.e.  that the user cares about redundancy   which allows us to re-write the scoring function in the equivalent form s di;d1 ... di 1 
c1
=	c1 + c1p rel|d 	1      p new|d  c1
	rank	c1
	=	p rel|d 	1      p new |d 
c1
where rank= indicates that the two scores differ by a constant  and therefore give identical rankings. note that a higher p new|d  makes the cost score better  i.e.  lower . further  when   a higher p rel|d  also makes the score lower  but the amount of reduction is affected by the cost ratio cc1 . this ratio indicates the relative cost of seeing a non-relevant document compared with seeing a relevant but redundant document. when the ratio is large  i.e. 
  the influence of p new|d  could be negligible. this means that when the user has low tolerance for non-relevant document  the optimal ranking would essentially be relevance-based  and not affected by the novelty of documents. when c1 = c1  we would score documents based on p rel|d p new|d   which is essentially the scoring formula for generating temporal summaries proposed in   where p rel|d  is referred as p useful|d . in general  there will be a trade-off between retrieving documents with new content and avoiding retrieval of non-relevant documents.
　one technical problem remains  since we do not usually have p rel|d  available when we score documents with the kldivergence function. one possible solution is to consider ranking documents based on the query likelihood  i.e.  p q |d   which
is equivalent to ranking based on the kl-divergence . since valuer = p q |d   we may further assume that p rel|d  is proportional to p q |d . under this assumption  the scoring function can be rewritten as s di;d1 ... di 1  rank=
valuer θi;θq  1   ρ   valuen θi;θ1 ... θi 1  
where  valuer θi;θq  = p q |di  is the query likelihood  and valuen θi;θ1 ... θi 1  is the estimated novelty coefficient using the mixture model method. we refer to this scoring function as a cost-based combination of relevance and novelty.
1. experiments
　in order to evaluate the effectiveness of the proposed method for combining novelty and relevance  we compared it with a welltuned relevance-based ranking baseline. the baseline is the best
relevance-based ranking  in terms of the subtopic coverage measure  using the original  short  queries. this baseline ranking is achieved using the dirichlet prior ranking method  with smoothing parameter set to μ = 1. we explored two tasks: re-ranking relevant documents  the same task used above to evaluate novelty methods   and ranking a mixture of relevant and non-relevant documents. the latter task is the  real  problem of subtopic retrieval. for the sake of efficiency  the results for reranking a mixture of relevant and non-relevant documents are based on using a cost-based ranking scheme to re-rank the 1 top-ranked documents returned by the baseline ranking.
　as a further comparison point  we also tried using pseudofeedback on top of our simple baseline. intuitively  since pseudofeedback adds new terms to a query  it might be expected to increase the diversity  and hence decrease redundancy  of the documents returned as relevant. the feedback approach that we use constructs an expanded query model based on an interpolation of the original maximum-likelihood query model and a pseudo-feedback model with a weight ofon each. the feedback model is estimated based on the top 1 documents  from the simple baseline results  using a mixture model approach to feedback   with the background noise parameter set to 1.  the dirichlet prior smoothing parameter is set to μ = 1  which is approximately optimal for scoring the expanded query.
　we varied the cost parameter ρ between 1 and 1. note that it is unreasonable to set ρ to any value below 1  as it would mean that a larger relevance value corresponds to greater cost. as ρ becomes large  the combination relies more on relevance; with ρ = 1  the formula is almost completely dominated by relevance. notice that subtopic performance can be improved by either improving relevance ranking and keeping redundancy fixed  by improving redundancy and keeping relevance fixed  or by improving both relevance and redundancy.
1 re-ranking relevant documents
　figure 1 presents the results on the simpler task of re-ranking relevant documents. we show results for the cost-based method with ρ = 1 and ρ = 1. combining relevance and novelty with either weighting scheme gives a consistent improvement over both baselines  across all but the lowest recall levels  and for both measures. this is in contrast to using novelty scores alone  which improved over the baseline only for higher subtopic recall levels. this is desirable behavior for a method that combines relevance  which does well at low subtopic recall levels  with novelty  which does well at high recall levels . feedback barely improves upon the baseline retrieval method.
1 ranking mixed documents
　results are presented in table 1 for the more difficult task of ranking a mixed pool of documents  along with an  upper bound  of performance which will be discussed in section 1. we see that the cost-based combination method still improves over the baseline on both measures  but only slightly  and only for larger values of ρ. interestingly  the pseudo-feedback approach also improves slightly over the baseline method for both s-precision and ws-precision. in fact  for s-precision the improvement obtained by the feedback method is somewhat larger than the improvement obtained by the cost-based combination of novelty and relevance.1
1 analysis and discussion
　it is likely that with the addition of non-relevant documents  performance gains due to improving the novelty of documents in a ranking are largely offset by corresponding performance losses due to imperfect relevance ranking. since a relevant document is much more likely to overlap with another relevant document than is a non-relevant document  emphasizing novelty may well tend to move non-relevant documents up in the ranking. it is possible that

1
graphs are not shown for these results  but the curves for all the
methods track each other quite closely.
ranking methodavg s-precisionavg ws-precisionbaseline1-1-cost  ρ = 11-1%1-1%cost  ρ = 1.1+1%1+1%baseline+fb1+1%1+1% upper bound 1+1%1+1%table 1: comparison of s-precision and ws-precision  averaged across 1 s-recall levels  for the task of re-ranking a mixture of relevant and non-relevant documents  using the costbased combination of mixavg novelty and a kl-divergence based relevance ranking.
the gains obtained by increasing the rank of novel relevant documents are largely offset by the cost of also pulling up non-relevant documents in the ranking.
　this hypothesis is supported by the performance of the costbased method on the task of re-ranking relevant documents. to further test this possibility  we conducted another test. recall that the definitions of  weighted  s-precision and s-recall are based on comparing a ranking system s with an optimal system sopt. one can use the same methodology to compare any two ranking systems. to simplify the discussion  let us call the system playing the part of s in a test of this sort the benchmarked system and the system playing the part of sopt the target system. define the wsprecision  at r  of a benchmarked system s1 relative to a target system s1 as
ws-precision at
relative ws-precision is a measure of the difference in performance between s1 and s1-the lower the ws-precision  the larger the performance difference.
　we took the rankings produced by the baseline retrieval system  henceforth sbase  and removed all non-relevant documents  to produce rankings from a hypothetical system srelonlybase . we then performed the same transformation on the cost-based ranking for ρ = 1  henceforth scost  to produce rankings for the hypothetical system.
　our conjecture is that the cost-based method ranks relevant documents better than the baseline system  but also ranks non-relevant documents higher. stated in terms of these hypothetical ranking systems  the conjecture is that  a  ws-precision for sbase relative to srelonlybase will be higher  i.e.  indicate a smaller difference in performance  than the ws-precision for scost relative to srelonlycost and
 b  ws-precision for srelonlybase relative to sopt will be lower  i.e.  indicate a larger performance difference  than the ws-precision for cost
srelonly relative to sopt.
　this conjecture is confirmed by experiments; the results are shown in figure 1. for clarity  we show ws-precision at intermediate levels of s-recall  where the differences between the systems are greatest.

figure 1: comparison of the curves of s-precision  left  and ws-precision  right  versus s-recall for the task of re-ranking relevant documents  using a cost-based combination of mixavg for novelty  and a kl-divergence measure for relevance.

figure 1: comparison of relative ws-precision versus s-recall for the task of re-ranking a mixed pool of documents. on the left  ws-precision of a  real  ranking system relative to a hypothetical ranker that rejects all non-relevant documents  but otherwise does re-order documents. on the right  ws-precision of the hypothetical relevant-document-only ranking relative to the optimal ranking.　a final set of experiments on ranking a mixed pool of documents was based on the observation that none of the methods considered more than modestly improves performance over the original relevance baseline. for each query  we created a subtopic query  or  subquery   for each subtopic  by concatenating the original query q with the description of the subtopic. for instance  for the sample query 1i  we created 1 subqueries  the first of which was  what are the applications of robotics in the world today  'clean room' applications in healthcare & precision engineering.  we then retrieved the top 1 documents for each subquery  using the baseline method with pseudo-feedback  and placed all of the documents returned by any subquery for q into a single pool for q. finally  we ran a noise-tolerant version of a greedy set-covering algorithm. this algorithm uses as a value function the expected number of new subtopics covered by a document  using subquery relevance scores to estimate the relevance of a document to a subtopic.
　unlike the mmr-style algorithms considered above  this algorithm uses an explicit model of the subtopics  which is acquired from the subtopic descriptions using pseudo-feedback. it is quite unreasonable to assume that this much information is available in practice  especially given that the user is unlikely to know all the subtopics in advance. however  it may be useful to consider the performance of this system as an informal upper bound on the performance of retrieval systems that must operate without any explicit model of subtopics.
　the performance of this method is shown in table 1 under the title  upper bound.  average s-precision and averaged ws-precision are improved  but by surprisingly little: s-precision is improved by about 1% over the best realistic method  the baseline with feedback   and ws-precision is improved by about 1% over the best realistic method  cost-based retrieval with ρ = 1 .
1. concluding remarks
　in this paper  we studied a non-traditional subtopic retrieval problem where document ranking is based on dependent relevance  instead of independent relevance  as has been assumed in most traditional retrieval methods. the subtopic retrieval problem has to do with finding documents that cover as many different subtopics as possible  which is often desirable  e.g.  when the user is performing a survey on some topic . traditional retrieval methods and evaluation metrics are insufficient for subtopic retrieval since the task requires the modeling of dependent relevance.
　we proposed a new evaluation framework for subtopic retrieval  based on the metrics of s-recall  subtopic recall  and s-precision  subtopic precision . these measures generalize the traditional relevance-based recall and precision metrics  and account for the intrinsic difficulty of individual topics-a feature necessary for subtopic retrieval evaluation. we also introduced ws-precision  weighted subtopic precision   a further generation of s-precision that incorporates a cost of redundancy.
　we proposed several methods for performing subtopic retrieval based on statistical language models  taking motivation from the maximal marginal relevance technique. we evaluated six novelty measures  and found that a simple mixture model is most effective. we then proposed a cost-based combination of this mixture model novelty measure with the query likelihood relevance ranking. this method was shown to slightly outperform a well-tuned relevance ranking baseline. however  the improvement is most clearly seen for ranking only relevant documents; when working on a mixed set of relevant and non-relevant documents  the improvement is quite small  slightly worse than a tuned pseudo-feedback relevance ranking of the same documents. this indicates that while both relevance and novelty/redundancy play a role in subtopic retrieval  relevance is a dominating factor in our data set.
　in future work  we need to further study the interaction of relevance and redundancy  perhaps by using synthetic data to control factors such as the level of redundancy and the number of subtopics. a major deficiency in all of the mmr style approaches considered here is the independent treatment of relevance and novelty. as a result  there is no direct measure of relevance of the new information contained in a new document. thus  a document formed by concatenating a seen  thus redundant  relevant document with a lot of new  but non-relevant information may be ranked high  even though it is useless to the user. it will be interesting to study how to identify and measure the relevance of the novel part of a document  which is related to the trec novelty track .
acknowledgments
we thank james allan  jamie callan  jaime carbonell  rong jin  and the anonymous reviewers for helpful comments on this work. this research was sponsored in part by the advanced research and development activity in information technology  arda  under its statistical language modeling for information retrieval research program  contract mda1-c-1.
