we present a probabilistic model for a document corpus that combines many of the desirable features of previous models. the model is called  gap  for gamma-poisson  the distributions of the first and last random variable. gap is a factor model  that is it gives an approximate factorization of the document-term matrix into a product of matrices 붦 and x. these factors have strictly non-negative terms. gap is a generative probabilistic model that assigns finite probabilities to documents in a corpus. it can be computed with an efficient and simple em recurrence. for a suitable choice of parameters  the gap factorization maximizes independence between the factors. so it can be used as an independent-component algorithm adapted to document data. the form of the gap model is empirically as well as analytically motivated. it gives very accurate results as a probabilistic model  measured via perplexity  and as a retrieval model. the gap model projects documents and terms into a low-dimensional space of  themes   and models texts as  passages  of terms on the same theme.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-retrieval models  clustering
general terms
algorithms  experimentation  measurement
keywords
probabilistic models  latent semantic analysis  em algorithm
1. introduction
모probabilistic models for document corpora are a central concern for ir researchers. among the applications for a probabilistic model are  i  accurate search and retrieval from
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.

figure 1: the gap model. x and f have gamma and poisson distributions  respectively  and y is a linear multiple of x by the matrix 붦.
the corpus  ii  detection of themes in the corpus and  iii  clustering of documents according to those themes. intuitively  the model should also fit the data well without overfitting. while these desiderata are often considered separately  we present a model which addresses them simultaneously. the model is called  gap  for gamma-poisson  the distributions of the first and last random variable. these distributions were chosen because of good empirical fit for text data  but also because they give a very clean and simple algorithm which is highly efficient. informally  gap models texts as contiguous  passages  of variable length  where each passage contains a fixed probability distribution of words.
모the probabilistic model is shown in schematic form above. x is a matrix whose jth column is a low-dimensional vector representing the contribution of  themes  in document j. each xij is a non-negative random variable representing the contribution of theme i to document j measured as the total length of all passages on that theme. y is a matrix whose jth column is a vector  dimension equals number of terms  representing the expected number of occurences of term i in document j. finally f is matrix where fij is the actual number of occurences of term i in document j. so fij is a discrete  non-negative random variable.
모for empirical reasons  the xij are given a gamma distribution  more on this later . the relation between x and y is a linear matrix
y = 붦x
this follows because the text is modeled as a union of passages of length xij on theme i  and the total term frequencies are a sum over all such passages. finally  if we assume independence of terms in specific locations in the document given the theme  then the fij will have poisson distributions. the matrix 붦 includes the theme information for the corpus. each row of 붦 encodes the term probabilities for a specific theme  which are non-negative . in the process of its execution  the gap algorithm also computes the expected theme weights  matrix x  such that xij represents the estimated total passage length on theme i in document j. the matrix product 붦x is then an approximate factorization of the document-term matrix f.
모gap is a generative model which provides very good modeling of a text corpus  measured using perplexity. it also improves on some of the best current models for text retrieval  we compare it with a smoothed  kl-divergence algorithm . we give experimental results for both problems later in the paper. we believe gap will give excellent results on clustering and other applications of factor models like lsa  but these await further studies. gap factorization is computed using an em-algorithm  derived in section 1. the algorithm has been designed to be as efficient as possible  and has complexity o ndi  where n is the total number of terms in the corpus  d is the model dimension  or number of themes  and i is the number of iterations run  typically 1 . gigabyte datasets have been factored on a 1ghz desktop computer.
모in the next section we discuss the relationship of other work to gap. then in section 1  we derive an em-algorithm for computing a gap factorization. in section 1 we discuss the implementation of the algorithm. this section includes an a explanation  section 1  of how the algorithm maximizes the statistical independence between factors.
1. related work
모because it computes an approximate factorization 붦x of the document-term matrix  gap is related to latent semantic analysis  1  1  1 . it can be used for document retrieval  in either the low-dimensional theme space  or as a smoothing algorithm in term space . gap can also be used for clustering by identifying each theme with a cluster and assigning documents to the cluster of highest weight in the vector x for that document. however  gap factorization differs from lsa in several ways. lsa is based on singular value decomposition which is equivalent to assuming a uniform distribution on x and a spherical gaussian distribution on f. neither match the empirical distributions on real texts. both the gamma prior and poisson distributions in gap are a good match for empirical document data. lsa determines factors by orthogonality  which forces many components of the factorization to be negative. in gap  both matrices 붦 and x have non-negative entries and there is no orthogonality constraint. in fact we show that gap can be used to maximize the independence of components  intuitively a more useful condition than orthogonality.
모the factorization that gap computes is a nmf  nonnegative matrix factorization   of the document corpus. nmf was shown in  to have extremely good performance in trec clustering benchmarks. note though  that the factorization derived from gap will be different from   which is based on least-squares  implicit spherical gaussian distribution on f .
모because gap is based on discrete output variables  it is a generative probabilistic model. that is  it assigns a finite probability to any specific document  and it can generate  random  documents from the corpus. generative probabilistic models have been applied in ir before  1  1   and gap has particular similarities to lda  latent dirichlet allocation   1  1 . we will say more about their relationship during the derivation of the gap model in the next section.
모gap is also related to ica  independent component analysis  algorithms  which also attempt to maximize the statistical independence of factors in a mixture distribution. there are two differences with ica and gap. first  ica normally treats  pure  linear mixtures of factors. since texts have discrete term frequencies  ica is not directly applicable. gap deals with this by interposing a poisson distribution after the factor mixture. secondly  gap uses known priors  gamma distributions  for the factors  based on empirical data. in ica  the prior must be estimated which will generally require more data for a given degree of accuracy. gap can be viewed as a customized ica algorithm for document data  with the appropriate distributions wrapped around the linear factor model.
1. gap model derivation
모first we recall the form of the gamma distribution. a variable x has the gamma distribution with parameters a and b when its pdf is

the parameter b is called the scale parameter  and changing it is equivalent to scaling the variable x by b. the parameter a is called the shape parameter. when a = 1 the distribution reduces to an exponential distribution. when a is an integer  the gamma distribution is the distribution of a sum of a exponential variables. this summation property holds more generally for gamma variables with the same scale parameter. if x1 and x1 are gamma random variables with parameters  a1 b  and  a1 b  respectively  then their sum x1 + x1 is a gamma random variable with parameters  a1+a1 b . as a increases  by the central limit theorem  the gamma looks more and more like a normal distribution. the mean of a gamma random variable with parameters  a b  is ab and its variance is ab1.
모to construct a language model  we let x =  x1 ... xk  be a vector of latent random variables representing the thematic content of a document. each xi encodes the total length of passages about topic i in the document. then xi is assumed to be an independent random variable with a gamma distribution with shape parameter ai and scale parameter bi. independence is a natural assumption  in fact it by assuming it we are defining the kind of factorization we want - one where the themes in the document occur independently. note that there is some necessary dependence between the xi because of document length. namely the expected value of the sum of the xi will be the length of the document. however  when the theme space dimension is not too small  say 1 or greater   the effect of this constraint is minor. since the mean value of xi is ci = aibi and ci is easily observable  we use it to substitute for bi in the parametrization. the prior pdf of xi is then
		for xi   1
and from the properties of the gamma distribution the mean e xi  = ci and variance var xi  = c1i/ai.
from the random vector x we derive a vector y via:
y = 붦x
the vector y gives the expected frequencies of words in the document we are generating. each term frequency in a document is modeled as a poisson variable with expected value yj. the column 붦i of 붦 is a vector of expected frequencies of terms that comprise theme i when the theme has unit weight.

figure 1: the probability distributions of some xi's for the cran corpus  computed using the em recurrence with uniform prior
1 empirical justification
모the model can be interpreted this way: each document contains passages of various lengths  length = number of terms in the passage  on specific themes. each xi encodes a theme within the document such that the magnitude of xi represents the total length of all passages on topic i in the document. each theme i defines a probability distribution over the terms in the corpus  which is encoded as the ith column of 붦. the probability of a term in a particular position in the document will then be a binomial distribution depending on the theme at that position. the ordering of the passages does not affect the distribution of the frequency of a term in the entire document  which will then be a poisson variable yj. the expected value of yj will be the sum over the various themes of the theme length xi times the corresponding 붦 probability for that term. hence the linear form of the model  and the trailing poisson distribution.
모the model so far is quite similar to latent dirichlet allocation  lda  developed by jordan et al.  1  1 . however  there are significant differences in the way the two models handle document length  and these lead to accuracy and performance differences. the differences arise due to the choice of prior distribution on xi. whereas the  intuitive  assumption of multiple themes dictates the gap model so far  as well as lda   it leaves completely open what the choice of distribution on xi should be. it would be tempting at this point to assume a gamma distribution on xi for analytic convenience as we will see. but a better way to proceed is to derive the mode from actual corpus data.
모to do this  we replace the gamma prior with a uniform distribution on the xi  i.e. no prior at all   and run an em algorithm to derive the most likely 붦 and x matrices. figure 1 shows some of the xij pdf's for 1 trec  tipster 1  documents with a  no-prior gap  factorization into 1 themes. the y axis shows the  log scale  probability density for each xi versus the value of that xi. almost all the curves show a good fit to the gamma signature: a long straight section over most of the range  with a distinct  spike  as xi becomes very small. note that the spikes are very similar  while the slopes vary. these two attributes correspond to the shape factor and scale factor respectively of the gamma distribution. the curves suggest that the shape factor is highly preditable while the scale factor varies significantly. in fact when we do a maximum-likelihood estimation of the gamma parameters this data  we find that all best-fit distributions have a shape factor of close to 1 with a standard deviation of 1. the scale factors however vary over a range of about 1 : 1.
모the gamma distribution has another very nice property when used with this model: namely that it has a scale factor  and a scaled gamma distribution is still a gamma distribution. this supports an equivalent model to the version of gap just described: instead of the length of passages on topic i  we can scale xi by one over the document length. the resulting xi represents instead the fraction of the document on topic i. we confirmed this with a similar experiment. namely  we computed a no-prior gap model but where the xi's were scaled by the reciprocal of each document's length. the plots were virtually identical to figure 1. in fact  we prefer to use this  normalized  version of the gap model because it does not require the document length to be generated or explained by the model. the normalized version appears to perform slightly better in some situations  although it is often impossible to measure a difference.
모to contrast further with lda  in lda the themes for each word in the document are chosen independently of themes for other words  according to a dirichlet distribution. this is a more restrictive assumption than the assumption of passages  it is equivalent to assuming all passages are length 1 . our formulation could support many distributions on the xi  length of passages . we tried several others empirically  including lognormal and zipf  that cannot be modelled as indepedendent per-word processes. in fact  a more general model was not needed since the gamma does so well. another way to view this is that gap assumes that documents comprise long passages and can  scale . that is  the fractional mix of topics in a document has a similar distribution independent of document length  it depends on number of passages on the same topic in the document  not their length . lda on the other hand  treats long documents as unions of many one-word documents with the same mixture parameters. the number of passages on the same theme per document is assumed to scale linearly with document length. the result is that the distribution of the fractional mix of topics changes with dirichlet as the document length grows.
모note that while the fixed shape-factor gamma prior is strongly evident in trec data  it may not be true of all document corpora. intuitively  a fixed shape-factor gap model should model well any documents where there are one or a few passages on the same theme in all the documents  there can of course be several passages on different themes in each document  gap models those with the distinct xi . in fact it strongly suggests that for trec data there is usually only one passage per theme per document. for longer documents where there are several passages on the same theme  the distribution should be a sum of gamma-distributed variables. if the scale factors of these match  intuitively they should   then the result would be another gamma variable with a shape factor which is larger. thus gap models should also work well for documents containing multiple passages on the same theme. however  for such documents the shape factor for each theme should be learned for the corpus whereas for corpora such as trec this is not needed. this suggests an interesting as-yet unverified application of gap models: the shape factor of the gamma for each theme provides information about the typical number of passages on that theme in each document the corpus. the larger the shape factor for a theme  the more passages on this same theme occur on average in a document.
1 an em algorithm
모let fj be the observed frequency of word j in a document. the probability distribution for the word count fj is a poisson variable:
k
pr for k 뫟 1
notice that this same expression defines a normalized gamma pdf on the variable yj with scale parameter 1 and shape parameter k + 1  because for integer k  k! = 붞 k + 1  .
the likelihood for a document generated this way is

and the log likelihood is
		 1 
1 the e-step
모ideally  we would first compute the expected value of x. but a simpler approximation is the value of x which maximizes the likelihood. this is reasonable because whereas the gamma prior on xi is highly asymmetric  the posterior for xi conditioned on the observed y is much narrower and closer to a gaussian. its peak and expected value are therefore very close. to find the maximum  we take the derivative wrt xi giving:

모as it turns out  there is a particularly efficient and simple iterative method to solve for the maximum likelihood x. now the goal in em is to compute the expected  not the maximum likelihood value. in many cases however  they will be very close. figure 1 shows typical distributions of x variables given prior and posterior probabilities from gap models of some trec documents. while the gamma priors are highly asymmetric  the summation of likelihood contributions from all the yj's that depend on a particular xi lead to a narrow  gaussian-like distribution whose mean and maximum value are extremely close. the one exception to this rule is the distributions that include zero  since these are truncated at zero. it is clear that in those cases  the expected value of xi should be  pushed away  from zero  as can be seen in the figure.
모before we describe that method  we proceed to derive the m-step. it will turn out that both e- and m-step can be computed using the same basic recurrence.
1 the m-step
모now that we know something about the distribution of x  we can work on the expected log likelihood. first we write

figure 1: sample computed pdfs for xi given the document's observed term frequencies f. vertical lines show mean and maximum likelihood values for
xi
붦j as the jth row of 붦  so that yj = 붦jx. next we expand each yj about its expected value:
yj = yj 1 +  j 
so that  j is a random variable with mean zero. this will help us expand the log term in the expected log likelihood.
모the next step is maximizing the expected log likelihood over 붦. rewriting the log likelihood  1  with the above expansion  and ignoring the terms which are independent of 붦 gives:
	  	 1 
and taking expected values  using e  j  = 1 and discarding terms of order 1 gives
	 	 1 
now since yj = 붦jx  substituting for yj into  1  gives:
	 	 1 
which is now expressed in terms of the sufficient statistics x.

we can now differentiate the expected log likelihood l with respect to 붦ji to find the maximum wrt 붦.
		 1 
and note that these equations are independent for distinct j. if we take the system of equations for all i  we obtain a self-contained system that defines the derivate wrt 붦j:
		 1 
1 recurrence formulae
모at this point  we observe that the equation for the loglikelihood  1  is similar to an entropy or kl-divergence measure. in   lee and seung considered a similar measure called  divergence  for non-negative matrix factorization. the measure they actually considered was
		 1 
while this measure is different from ours  if we differentiate it wrt 붦j  we get
		 1 
which is in fact the negative of the gradient of the log likelihood from equation  1 . since the derivatives agree up to sign  any algorithm which maximizes the log-likelihood of our model over lambda is also minimizing the divergence between y and f. this is intuitively appealing. the frequencies f and the expected values y can be thought of as generalized probability distributions  except their sum is the document length rather than 1. the divergence above is minimized for the maximum likelihood model  and it naturally generalizes the kl-divergence for f and y.
모lee and seung gave a simple  multiplicative recurrence for a matrix factorization f = 붦x which minimizes the divergence. it can be applied directly to our m-step to compute lambda. the recurrence for the m-step is
		 1 
where fik and yik are the actual and expected frequency of

term term i in document k  and xjk is the expected value of xj for document k 뫞 n total documents. lee and seung showed that this recurrence is globally convergent  and is guaranteed to find a unique minimum for 붦.
모it turns out that our e-step can also be expressed as a minimization of divergence. recall that the e-step should minimize:
		 1 
which is equation  1  with substitutions for yj and constant terms removed. and minimizing this expression over x is easily shown to be equivalent to minimizing the divergence in the following matrix factorization wrt x:
		 1 
where
	da = diag a1 ... ad 	dc = diag c1 ... cd 	 1 
모for a matrix factorization f = 붦x  lee and seung's formula for updates to x is:
		 1 
and when applied to the extended factorization  1   this becomes the e-step

where xik is the magnitude of the ith theme in document k  m is the number of terms  and other symbols are per their previous definitions. this recurrence is guaranteed to converge to a unique maximum for x. however  when the recurrences for e and m are combined  the iterations will converge but a global maximum is not guaranteed.
모notice that the numerator of  1  depends only on information about the current document  and that the number of terms in its sum is the number of actual terms in the document  not the total number of terms . the denominator on the other hand has a sum of 붦ji over all terms  but the denominator is the same for all documents  so it only need be computed once. it follows that the total complexity of the e-step is o nd  where n is the sum of the sizes of all documents  total corpus size  and d is the dimension  or number of themes.
모the numerator of the m-step  1  also requires o nd  steps to compute  as does its denominator. it would be tempting to alternate 1 e-step and 1 m-step  but there is more than arithmetic complexity at play. because updating the x value for one document in the e-step requires only data that is local to one document  it has a very small memory footprint and easily fits in a cpu cache  usually in the l1  fastest  cache. on the other hand  updating 붦 requires a pass over the entire document set and is in practice much more expensive. we have found that it is economical to perform 1 e-steps  equation  1   for each m-step  equation  1  . finally  we note that the terms in the m-step formula are sums over n  the number of documents. both numerator and denominator of the m-step can be computed incrementally during the e-steps for each document. thus a combination of 1 e-steps and 1 m-step requires a single pass over the dataset.
모if the corpus is small  it is possible to exploit a similar locality in the m-step recurrence  namely that an update to the row of 붦 corresponding to a particular term depends only on the previous value of this row. however  this requires the matrix x of all x-values to be kept in memory. for large datasets  붦 is often smaller than x and if it isnt  the term set can be reduced by ignoring less-frequent terms in the model. thus in our implementation  붦 resides in memory  and the x-values are saved on disk and read-written as a single pass.
1 component independence
모so far we have described an em algorithm for maximum likelihood estimation of 붦 and x. we also argued that the gap algorithm maximizes the independence between the factor components xi. but why does this follow  the key is the choice of a well-fitting  highly-skewed prior on the xi  the gamma distribution . we observed earlier that the shape parameter a 뫟 1 for a gamma distribution is a measure of its skewed-ness. when a = 1  the distribution is an exponential distribution. as a increases  the distribution approaches a gaussian. from experiment  we have found that typical values for a are in the range 1-1.
모suppose that we explicitly knew identities of two independent components x1 and x1  meaning we knew the value of the corresponding columns of 붦 from which we could derive the values x1 and x1 for all the documents. by our assumption  based on empirical data   x1 and x1 are well-fit by gamma distributions with a close to 1  and the same scale factor. now consider any linear combination xc of x1 and x1. it will be less skewed than either x1 or x1. for example  if it is a sum of x1 and x1  which have the same scale factor   then xc will have a scale factor greater than 1 which is much less skewed than x1 or x1. for a general combination  xc can be decomposed into a gamma distribution with shape factor a  plus a residual distribution. when we compute the likelihood for xc using equation  1  with its gamma prior  it will be strictly lower than the likelihoods for either x1 or x1  whose distributions perfectly match the prior.
모in general  any mixture of actual independent components with the specified gamma distributions will be less likely than the pure distributions. so the likelihood maximization will favor the most independent components. this is very similar to the popular technique of component analysis by kurtosis minimization from speech analysis   which has also been applied to ica. kurtosis is a general measure of  skewedness  of a distribution  and increases when signals are mixed. minimizing it is a way of favoring unmixed components.
모in practice  we do not know the actual shape factors a  nor are the priors on xi's perfectly gamma-shaped. we could estimate the a's during the em algorithm  but this is not a good idea. suppose the algorithm estimates the a's at higher values  less skewed  than the independent components. then it will be easy for the optimization to find a 붦 which mixes several components in order to produce components that approximate the less-skewed distributions. in other words  the algorithm can make a bad guess but find a model that supports this guess.
모a better approach is to introduce an independence  bias  by setting the shape parameters a to be deliberately smaller than their actual values. for instance  if actual shape parameters are in the range 1-1  model shape parameters are set at 1  this has worked well in experiments . the more skewed the actual xi distributions  the better they will fit the a = 1 priors. and the most skewed distributions will also be the most independent.
모the negative effects of this deception are small. the components xi during the algorithm will always be mixtures of actual independent components. there is no degree of freedom in the shape of those distributions per se  that the algorithm needs to find. rather  it only needs to find the best  most independent  linear combinations. the deception causes a slight decrease in the log likelihood. but assuming average document length is much longer than the theme space dimension  then the output  poisson  term in the likelihood will dominate anyway.
1. implementation
모the gap algorithm has been coded in both matlab and native c++ code. its performance is dominated by matrix operations  specifically matrix inversion  multiply and back-substitution. for efficiency  these operations should be handled by cache-aware  optimized lapack routines. in matlab  these operations are built-in. the matlab gap code has been heavily vectorized so its performance is dominated by the lapack matrix primitives. the c++ version of gap uses the intel mkl  math kernel library  implementation of lapack. these libraries have very similar performance  and the result is that both matlab and c++ versions of the gap code have running times within a few percent of each other on most examples.
모the main difference between c++ and matlab versions is their use of memory. the matlab code requires all matrices to be memory-resident. the c++ code allows the document-term and x matrices to be disk files  and makes multiple passes over them. the c++ code can handle a much larger number of documents. it also allows the set of terms processed to be restricted to terms with specified minimum frequency  which also reduces memory demand and allows the c++ version to handle arbitrarily large datasets. both matlab and intel mkl are proprietary software. if an open-source matrix library is desired  it is possible to use the lapack routines from nist  gams.nist.gov  which are available in both fortran and c. however  most of these are not cache-aware and on modern computer hardware  one should expect an order of magnitude  factor of 1  or more loss in performance. there are some open-source cache-aware routines available. we experimented with atlas  available from nist  which did much better than naive lapack routines  but was still much slower than mkl or matlab. however  we did not tune the atlas routines for the cache hierarchy of the pcs we were using which might have made up this difference. another automatic-tuning library is phipac  portable high performance ansi c   which is at http://www.icsi.berkeley.edu/~bilmes/phipac/.
1. experiments
모we studied the performance of gap in two dimensions: first as a language model  by measuring its fit  via perplexity  to a test corpus. a good point of comparison was to lda  latent dirichlet allocation   which used the cranfield dataset  and which included numerical perplexity values for various model dimensions. perplexity is a standard measure of fit and is defined as
perplexity = exp
where d is a document from the corpus and nd is its number of terms.
모we tested two datasets: cran and the trec queries 1 on the 1 document tipster dataset. cran  the cranfield dataset has 1 documents and 1 distinct terms. in   the corpus was divided randomly into training and test sets. we followed this approach  and our training set comprised 1 documents while the test set comprised the remaining 1. a gap model was created from the training set  and the model was used to evaluate document probabilities and hence perplexities for the remaining test documents. both models were evaluated at various dimensions  as shown in figure 1. gap exhibits lower perplexity  better fit  across the dimension scale  especially at low dimensions.
모in a second experiment  we compared retrieval performance of gap with two standard retrieval methods  tfidf and kl-divergence with dirichlet smoothing on a unigram model . the implementation of both methods came from the
lemur toolkit from cmu http://www.cs.cmu.edu/~lemur. the test dataset was again the cranfield corpus
모to apply gap as a retrieval model  kl-divergence was used to measure distance from a query to each document. gap can be used as a smoothing algorithm because it assigns non-zero probabilities to many terms not appearing in a document. in our experiments  we found that smoothing with gap was sufficient by itself for large corpora  such as trec data . either jelinek-mercer or dirichlet smoothing  can be used. we tested jelinek-mercer in our experiments.

figure 1: perplexity results for cranfield corpus

figure 1: retrieval results for trec topics 1
the best results with trec data were for a combination:

where p1 is the observed probability of the term in the document  i.e. the term's relative frequency   and p1 is the gap probability.
모figure 1 shows results for trec topics 1 on the 1-document tipster1 dataset and a 1-dimensional gap model. the baseline curve uses jelinek-mercer smoothing with the same blend factor. that is  it uses the corpus probability for p1 instead of the gap probability. the overall improvement using gap is about 1%. the improvement was higher at 1 dimensions than at lower dimensions. we did not try models with more than 1 dimensions to see if this improvement continued. the running time to generate this model was about 1 hours on a 1 ghz pentium.
모for small corpora such as cran  it was necessary to use a secondary smoothing term from the probability computed from the gap model  and relative frequency of the term in the corpus. for cran  a good choice of probabilities


figure 1: retrieval results for cranfield corpus
where q is a term appearing in the query  nq is the number of occurences in the query  p1 is the term's empirical probability  relative frequency  in the document  p1 is the term's probability as computed by gap  and p1 q  is the probability of the term in the corpus  relative frequency in the corpus .
모the results are shown in figure 1. the upper two plots are the results with gap models of dimension 1 and 1 respectively. the lower two plots are kl-divergence with dirichlet smoothing and tfidf. gap gives consistently better results with either choice of dimension. dimension 1 gives best results  and its average precision  1  is about 1% higher than kl-divergence or tfidf. higher dimensions did not improve retrieval  as would be expected given the short document lengths in the cranfield corpus.
모the running time to compute a 1-dimensional gap factorization for the cranfield corpus was about 1 seconds on a 1 ghz pentium. as we explained in the sections on e- and m-steps  the complexity is the product of corpus size  model dimension  and number of iterations. that is if the total corpus size is n  sum of number of terms in all documents   the model dimension is d  and the number of iterations is i  then the complexity is o ndi .
1. future work
모gap shows promise for a variety of ir tasks. we believe it has applications outside of ir as well. in fact  gap's raison d'etre is for activity analysis from logs of many users' actions when online. specifically  email  im  document and web access are all actions that users perform as part of collaborative knowledge work activities that extend across time and space. the goal of activity analysis is to recognize these longitudinal threads of activity  to cluster them and make predictions from them. gap was developed to model such online activities  and ultimately real-world activities discovered from sensor data.
모it is no accident that it performs well as a document analysis tool. the relationship between texts and  activity records  is a deep one which has been recognized by many influential linguists and literary theorists. various versions of this connection appear in the works of vygotsky  leontev  bakhtin and burke. burke's title  language as symbolic action  captures the idea well. a text is canonically a description of sequences of actions by various actors  in various times and places. the  activities  we seek to discover in activity logs are more than analogous to the  themes  we seek to discover in texts.
모the great potential for gap is where activity analysis and document analysis meet: in context-aware retrieval. gap can be used simultaneously to mine discrete user actions involving texts  sending mail or reading documents   and the content of those texts themselves. the resulting termbased decomposition can be used to disambiguate search terms or even for pro-active  system initiated  retrieval.
1 clustering
모in the short term  we plan to test gap as a clustering algorithm. other researchers  have reported extremely good results with non-negative matrix factorization  nmf  of document corpora for clustering. gap is such a model. it may have advantages over earlier nmf algorithms for reasons we have discussed earlier.
acknowledgements
this work was supported in part by the national science foundation under nsf grant iis-1.
