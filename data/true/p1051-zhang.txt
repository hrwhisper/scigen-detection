contextual advertising on web pages has become very popular recently and it poses its own set of unique text mining challenges. often advertisers wish to either target  or avoid  some specific content on web pages which may appear only in a small part of the page. learning for these targeting tasks is difficult since most training pages are multi-topic and need expensive human labeling at the sub-document level for accurate training. in this paper we investigate ways to learn for sub-document classification when only page level labels are available - these labels only indicate if the relevant content exists in the given page or not. we propose the application of multiple-instance learning to this task to improve the effectiveness of traditional methods. we apply sub-document classification to two different problems in contextual advertising. one is  sensitive content detection  where the advertiser wants to avoid content relating to war  violence  pornography  etc. even if they occur only in a small part of a page. the second problem involves opinion mining from review sites - the advertiser wants to detect and avoid negative opinion about their product when positive  negative and neutral sentiments co-exist on a page. in both these scenarios we present experimental results to show that our proposed system is able to get good block level labeling for free and improve the performance of traditional learning methods.
categories and subject descriptors
h.1  information systems : information systems appli-
cations
general terms
algorithms
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  las vegas  nevada  usa.
copyright 1 acm 1-1-1/1 ...$1.
keywords
sub-document classification  contextual advertising  sensitive content detection  opinion mining
1. introduction
　contextual advertisement is a popular advertising paradigm where web page owners allow ad platforms  like google  yahoo  microsoft  etc.  to place ads on their pages that match the content of their sites. although this is an effective mechanism for advertisers to reach a large audience  it has its problems. many of these problems arise due to the huge variety of content that can appear on a single web page  e.g. news sites  blogs  etc . advertisers are very careful about their image and branding  hence they do not want to show their ads on pages with content like violence  pornography etc.  we call these  sensitive content   . for example  general motors may not want to show chevy ads on any page with violence or accidents. more specifically  they want to avoid web pages that specifically refer to an accident involving a  chevy  suv. often such content may occur only in a part of the page. for example news pages contain information on a range of topics; in them accidents or war may only be covered in a few lines. advertisers are also careful about advertising on product review pages. people's opinion on a product may often be mixed - they may like some features of a product but dislike others. on blog review sites and discussion forums where many people express their opinions  again both positive and negative opinions are very common. advertisers may not wish to advertise on pages which contain negative opinion about their products  or they may wish to specifically target pages which express only positive opinions . thus it is important to detect and separate different types of opinions appearing on such mixed-content pages. clearly in the above two applications there is a significant business need for sub-document methods in addition to document level methods i.e. we not only want to tell if a document has some targeted content in it  but we also want to label the parts of the document where the content is present.
　learning for sub-document classification is a unique practical challenge. the scenario is unlike standard text classification where each of the train and test documents is assumed to be about a single topic. the straighforward way to build such a sub-document classifier is to train on entire pages using page-level labels and test on individual blocks. this method may work well if the target content dominates the positive training pages. but in real web application there are many problems. first  pages can contain unwanted parts like navigation panes  text advertisements  etc. second  they may contain information on multiple topics. methods that clean noisy pages may remove the unwanted parts  but may not be able to separate the individual topics in multi-topic pages. often concepts that advertisers want to target can be broad e.g. the term  sensitive  can apply to many disparate concepts like war  pornography  natural disasters  accidents  etc.  and collecting large amounts of broad coverage singletopic training data is difficult. in practice  to build accurate classifiers  we have to collect large amounts of training pages  pre-clean and hand-label the blocks . this data labeling process is expensive and unscalable to many real-world concepts.
1 our contribution
　in this paper we investigate different methods to train subdocument classifiers when only page level labels are available. first we study the effectiveness of traditional methods for both document- and sub-document level classification to detect the presence of a desired content even when it appears only in a part of a page. further we investigate whether the performance of traditional methods can be improved using multiple-instance learning  mil  techniques. specifically  we consider a particular example of multiple-instance learning called milboost. we show how the problems of sensitive content detection and opinion/sentiment classification for advertising can be considered as 1-class and multi-class versions of milboost  and how this approach can improve the performance of traditional classifiers. these are new applications of the mil framework. in sentiment detection  we show that a naive-bayes based milboost detector performs as well as the best block detector trained with block-level labels. in short  with only page-level labels  milboost can produce a better page-level classifier than traditional methods and at the same time produce a competitive block-level detector  without block-level labeling in the training data.
　the rest of the paper is organized as follows. in section 1 we study the task of sensitive content detection. we show how this task easily fits in to the multiple-instance learning framework. in section 1 we study the problem of opinion mining for advertising. we show that we can solve this problem by extending milboost to a multi-class scenario. section 1 discusses some related work. in section 1 we present computational experiments on these two tasks. we study the performance of the state of the art algorithms  and show how milboost can improve the performance of the traditional base classifiers. we discuss the results and present some screen-shots that demonstrate these algorithms in action.
1. sensitive content detection
　in online advertising  advertisers often have a content blacklist and they do not want their ads to be shown on web pages that contain those sensitive contents. usually  sensitive content categories include crime  war  disasters  terrorism  pornography  etc. to satisfy the advertisers' needs  it is important for an advertising platform to have tools that are capable of detecting those sensitive contents on a web page. as long as a web page contains such content blocks  it will be marked as sensitive and the ad display will be turned off. note that in this paper  we do not differentiate between various sensitive categories  although we could  using the multi-class system we will derive later  but group them as one class labeled as  sensitive . often  the available training web pages are labeled at the page-level  i.e. the labels only tell whether a page contains sensitive content somewhere in it or not.

figure 1: illustration of a positive web page and its content blocks  one positive block suffices to label the whole page as positive
　figure 1 illustrates the above problem - a web page is divided into a number of content blocks based on its html structure. the page in the figure can be thought of as  sensitive   i.e. labeled  positive   since it contains at least one block with senstive content; otherwise it would be labeled as  negative . learning in this type of scenario is different from traditional cases where each page is devoted to one topic. if we use the entire page  we run the risk of learning everything on the page as  sensitive . to avoid this problem we need a classifier that can accurately identify the parts of the page that contain the targeted content  and only learn from those. better still is a classifier that can integrate the two tasks of locating and learning - this type of learning is covered under the multiple-instance learning framework.
1 multiple instance learning
　multiple instance learning  mil   1  1  refers to a variation of supervised learning where labels of training data are incomplete. unlike traditional methods where the label of each individual training instance is known  in mil the labels are known only for groups of instances  also called  bags  . in our above sensitive content detection example  a web page can be considered as a  bag  while each block of text can be thought of as an instance inside this bag. in a 1-class scenario  a bag is labeled positive if at least one instance in that bag is positive  and a bag is labeled negative if all the instances in it are negative. there are no labels on the individual instances. the goal of our mil algorithm is to produce a content detector at the sub-document  block  level without having the block labels in the training data. this can save significant amount of money and effort by avoiding labeling work at the sub-document level.
　why milboost: there are quite a few mil learning methods available. among them we choose milboost  which combines mil approaches with ensemble approaches . the reasons are two-fold: first  the state of the art traditional algorithms use boosting  as we will see later  and we needed a framework to accurately measure the added effectiveness of the mil framework. comparing mil alone against a boosted system will not accurately reveal this difference. but a system using ideas from mil and boosting when compared to the baseline and boosted systems will tease out the effect of boosting and mil separately. secondly  milboost has been successfully applied to a similar problem - the problem of training a face detector to detect multiple faces in pictures  when only picture level labels are available.
1. sentiment classification and detection & opinion mining
　in the previous scenario  we only had one target topic of interest   sensitive  content . there are many applications where a user may be interested in a group of topics  and a page may contain one or more of these topics. the occurance of one of the topics may not preclude the occurance of others. sentiment/opinion mining from review pages or blogs is one such application. it is common to label reviews as  positive  or  negative . however reviews are often not as polar or one sided as the label indicates. an overall negative review may sometimes contain some positive elements and vice versa. blog review sites or discussion forums usually feature many people expressing varied opinions about the same product. these  mixed  opinions may act as noise during the training of traditional classification methods . clearly  this calls for a more granular  paragraph- or sentence-level  study of reviews. once we have a system that can provide labels at a granular level  we can easily find out what aspects of a product that the reviewer likes or dislikes. advertisers like such analysis since it allows them to stay away from pages that express negative reviews of their products. alternatively  they may wish to target pages that only express positive reviews about their products. we will show that it is possible to address this problem using multiple instance learning.
1 multi-target milboost algorithm
　traditional milboost has been applied to only a 1-class case  . to apply milboost to the multi-topic detection task  it needs to be extended to a multiclass  or  multitarget   scenario. in this section we show how to derive the multi-target milboost algorithm. for example  in the sentiment detection task  the  positive  and  negative  opinions can be treated as the target classes and the  neutral  class as the null class in the mil setup.
　in a multi-target scenario  a bag is labeled as belonging to class k if it contains at least one instance of class k. as a result  a bag can be multi-labeled since it may contain instances from more than two different target classes. figure 1 shows such an example for sentiment detection. there are both positive and negative statements on this review web page and according to the definition  this page shall be labeled both positive and negative. the way we deal with multi-labels is by creating duplicates of a bag with multiple labels and assigning a different label to each duplicate. within each duplicate bag  mil will eventually find the instances that match the label of the bag.
　before we derive the algorithm in detail  we would like to discuss the conceptual flow of the training algorithm  see

figure 1: an example of multi-labeled review web pages

figure 1:	conceptual flow of multiple instance learning
figure 1 . first we break the training pages into blocks and guess the block/instance level labels. then we combine the instance labels to derive bag/page labels. we check if the imputed bag labels are consitent with the given training labels; if not  we adaptively adjust the probability of membership of the training instances until the imputed bag labels become consistent with the given labels. in milboost the weight of each instance changes in each iteration according the prediction made by an evolving boosting ensemble.
　initially  all instances get the same label as the bag label for training the first classifier. subsequent classifiers are trained on reweighted instances based on the output of the existing weak classifiers. a detailed description of a 1-class milboost algorithm can be found in . here we derive the multi-class milboost algorithm.
　suppose we have 1...k target classes and class 1 is the null class. for each instance xij of bag bi  the probability that xij belongs to class k  k （ {1 ... k}  is given by a softmax function 

where
yijk = xλtyijkt
t
is the weighted sum of the output of each classifier in the ensemble with t steps. {yijkt } is the output score for class k from instance xij generated by the tth classifier of the ensemble.
　referring to figure 1  a bag is labeled as belonging to class k if it contains at least one instance of class k. if it contains no blocks with labels 1...k  then it is labeled as neutral or the null class. under this definition  the probability that a page has label k is the probability that at least one of its content block has label k. given that the probability of each instance belonging to target class k  and assuming that the blocks are independent of each other  the probability that a bag belongs to any target class k  k   1  is
pik = 1   y 1   pijk .
j（i
this is the  noisy or  model.
　the probability that a page is neutral  or belongs to the null class 1  is the same as the probability that all the blocks in the page are neutral pi1 = qj（i pij1.
the log likelihood of all the training data can be given as k
	loglh = x x logpik + x logpi1	 1 
	k=1{i|li=k}	{i|li=1}
where li is the label of bag bi.
　according to the anyboost framework   the weight on each instance for next round of training is given as the derivative of the log likelihood function with respect to a change in the score of the instance. therefore for the target
classes 
		 1 
for the null class 
		 1 
　understanding the evolution of instance weights: to understand how the re-weighting works  let us look at the weight for a 1-class case :
		 1 
　the weight on each instance evolves in the following way to keep the learner focusing on the concepts that are not absorbed so far by the ensemble. first of all  observe that the overall weight is composed of two parts: bag weight and instance weight pij. for an instance in a negative bag  the bag weight is always  1  while the instance weight determines the magnitude of the weight. generally  negative instances with a high pij will get a high weight  in magnitude  for next round of training  since they are more likely to cause misclassification at the bag level. for a positive bag  if it is correctly classified  pi is high   the weight of all the instances in the bag will be reduced. otherwise  instances with higher pij within the bag  which are potentially good candidates for  real positive  instances  will standout and get more attention in the next round of training.
　note that in multi-target milboost  the weight of each instance is always positive unlike the single-target  1-class  case. it no longer carries the class information by the sign of the weight  as in the single-target case. similar to the singletarget milboost  the weights on instances of a target class bag reduce as the ensemble prediction of the bag approaches the bag label. otherwise  instances with high probability of being the target class will be singled out for next round of training. the weights on the negative instances are also as intuitive as the single-target case. in fact  it can be easily shown that the multi-target milboost scheme is consistent with the single-target case.
1.1 combining weak classifiers
　once the  t + 1 th classifier is trained  the weight on the classifier λt+1 can be obtained by a line search to maximize the log likelihood function.
1.1 choice of classifier ct
　just like in any ensemble learning scenario  we can choose a wide variety of base classifiers ct. in our experiments we show results using naive bayes and decision trees. more details are given later in the paper.
a pseudo-code for 1-class milboost is shown in figure
1.
input: training set t of n bags  each bag bi with ni instances xij bag label li （ {1}  base learner `  integer m number of training rounds 
#initialize weights
for i = 1 : n
for j = 1 : ni
let wij1 = 1    li   1 ;
endfor
endfor
for t = 1 : m
#train base  weak  classifier with weighted instances
ct = ` t wt 1 ;
#combining weak classifiers - line search for λt λt = argmaxλloglh; #refer to  1  #update instance weights using anyboost
for i = 1 : n
for j = 1 : ni
#compute instance probability
let	; let	;
endfor
#compute bag probability let pi = 1   qj（bi 1   pij ;
#update instance weights for j = 1 : ni
	let	;
endfor
endfor
endfor
output: ensemble classifier {c1 c1 ，，，  cm} 
classifier weights:{λ1 λ1 ，，，  λm}figure 1: pseudo code for milboost
1.1 testing
　to test the milboost model on a new page  the page is divided into blocks and the block level probabilities are computed using the classifier. the page level probabilities are obtained by combining the block level probabilities using noisy-or. the block and page level labels are calcualted using thresholds on the probabilities.
1. related work
　multiple instance learning has been applied to a wide range of problems such as drug activity prediction   image object detection  1  1   text categorization   etc. andrew et. al. have applied multiple instance learning combined with maximum-margin learning for text categorization . their work is focused on document-level classification instead of block detection. there are quite a few algorithms developed for multiple instance learning. besides multiple instance boosting which is applied in this paper  other popular mil algorithm include diverse density  dd    emdd   citation knn   etc. all of the above algorithms are designed to solve the traditional two-class problem.
　xin et. al. studied the sensitive content detection problem . the classifiers they built are at the page-level and are trained largely with single-topic web pages. there has not been much work in detecting and locating desired content in blocks. pang et. al. proposed a minimum cut method to identify objective statements in movie reviews . oliver et. al. described an email tagging system that is able to identify certain actionable items inside an email . however  both works require training data with block-level labels.
　others have studied methods to eliminate noisy parts of web pages like banners  advertisements etc. using style based pre-processing  or through summarization . if the main content itself is multi-topic  these two approaches will not be useful. in contrast  our approach elegantly integrates both multi-topic disambiguation and noise removal with the learning step.
1. experiments
1 sensitive content detection
　in our first of experiments we show how traditional algorithms and their milboost-ed versions perform in sensitive content detection. the data set contains two thousand web pages  which are labeled at the page level by human annotators. the  sensitive  pages approximately cover war  crime  disaster and terrorism. the label for each web page is binary  either sensitive or nonsensitive. simple html-tag based heuristics were used to split a web pages into content blocks  see figure 1 . unfortunately  there is no labeling done at the text block level. therefore  the evaluation has to be done at the web page level. for this specific task  it still makes sense since we only need to know whether a page contains sensitive content and we do not care much about whether all sensitive content blocks from a page are caught. the performance of our approach at the block level will be demonstrated in the next section on sentiment detection experiments.
　two popular base classifiers were used to build the milboost ensemble  decision trees  and naive bayes . area under the roc curve  auc  was used to evaluate the effectiveness of the various detectors because it reveals the full-spectrum performance when no specific triggering threshold is specified  an roc curve plots true positive rate versus false positive rate with different classification thresholds  see figure 1. the area under the roc curve is equivalent to the probability that a positive data point will be ranked above a negative point  1  1  .
table 1: comparison between milboost based detectors and their corresponding boosted classifiers for sensitive content detection by auc. decision tree and naive bayes  nb  are the two base classifiers. all classifiers were trained with page-level labels only. bolded numbers indicate that they are statistically significant than the number in the row above them based on paired t-test at 1% significance level  using 1-fold cross validation.
algorithmdecision treenbbase11base+boosting11base+milboost11　we evaluate the performance of various classifiers on the sensitive content task. we start with two different base classifiers - decision trees and naive bayes  nb . then we build boosted versions of these classifiers  and also milboosted versions of them. all the classifiers are trained with pagelevel labels. the task is to classify pages into two classes - sensitive and nonsensitive. both the milboost and the non-milboost versions were run through 1 boosting iterations which end up with an ensemble of 1 classifiers. the depth of a decision tree is fixed at 1. empirical evidence shows that those parameters appear to give good complexity-performance trade-off i.e. increasing the number of boosting iterations beyond 1 gave minimal improvement. the occurance frequency of word unigrams was used as features.

figure 1: roc curve of boosted naive bayes compared to it milboost version for sensitive content detection. the corresponding curves using decision trees shows similar performance
　table 1 shows the performance of the detectors trained with different algorithms. the numbers in the table are the average over five 1-fold cross validations. they clearly indicates the advantage of milboost detectors over both the base classifiers and their traditional boosted versions. with a non-linear base classifier such as decision trees the auc is 1; boosting lifts this performance to 1  a 1% improvement in performance . milboost further improves this performance by another 1%  auc of 1 vs 1 . next  we show results using a robust base classifier like naive bayes - the boosted nb system gave a 1% improvement over the base classifier while the milboost version achieved almost the same performance as the boosted page-classifier  auc of 1 vs 1 . figure 1 shows the roc curves comparing the boosted- and the milboosted naive bayes systems. althoug the auc is about the same  the milboosted system is almost consistently better than the boosted page-classifier at the early part  where usually the operation point exists. this  early lift  brings practical advantage to the milboosted system. overall  milboost framework substantially improves the counterpart page-classifier.
1.1 naive bayes vs decision trees
　it is interesting to observe that naive bayes performed much better than decision trees in this task. we investigated this issue and found that the reason lies in the number of features the two algorithms use. the decision tree ensemble uses only about 1 keywords while nb theoretically uses the whole vocabulary  which is about 1. the bigger feature set enables nb to generalize better at the testing stage. if the feature set used by nb were limited to what decision tree is effectively using  its auc would drop to 1.
a sensitive content detection demo: the milboost detector was used to build a system that is able to highlight sensitive content blocks on a web pages. figure 1 is a screenshot of the demo. the shaded blocks are identified by the content detector as containing sensitive contents.
1 sentiment detection
　for the previous task we did not have block-level labels  hence we were unable to evaluate it at the block level. to demonstrate block-level performance  we now show singletarget milboost performance on a 1-class sentiment detection task.
1.1 sentence level sentiment detection
　for this task we used the subjectivity dataset from the cornell movie review data repository . in this data set  1  objective  and  subjective  sentences are labeled. these sentences were extracted from 1 reviews  which are labeled at the review-level as well. here a review is a  page  and a sentence is a  block . the milboost detector is trained with the review data only using page-level labels  and then evaluated at the sentence-level with sentence level labels. again  decision trees and naive bayes are used as base classifiers. traditional page-level classifiers using boosted nb and decision trees are also built as benchmark algorithms for comparison. in addition  a pagelevel classifier using support vector machines  svm   1  1  is also trained. svm is reported to have the best performance in sentiment detection .  we did not build a milboost system with svm as a base classifier becaues it is not straightforward to incorporate the instance weights into an svm solver .
　in addition  since we have sentence-level labels we also built classifiers that are trained with sentence level labels. this is the best case scenario  but an expensive proposition since labeling is an expensive task. we want to show that for this task  we do almost as well as the classifiers that have sentence level labels.
　the results are shown in table 1. the aucs of the five algorithms on sentence-level sentiment detection are shown. the numbers are averaged over five 1-fold cross validations.
table 1: comparison between milboost detectors and their corresponding boosted base classifiers  nb and d.tree   all trained using page-level labels only. for comparison  performance of the same base classifiers when trained with sentence-level labels are shown in the last row  labeled sent . svm results are given for comparison. the task is sentiment detection at the sentence-level. numbers are auc. bolded number indicates statistically significant difference compared with the above row at 1% significance level
boostedboostedalgorithmnbd.treesvmbaseline111milboost11n/asent111　the first two lines in table 1 compare algorithms using only page-level labels. the milboost system using nb base classifier achieves the highest auc  1 . this performance is comparable with the best sentence detector trained with sentence-level labels  line 1 .
　it turns out that decision tree is not a good classifier for sentiment detection at the sentence-level. nonetheless  milboost improves the performance by about 1%  from 1 to 1  over boosted decision trees. the svm did not do as well as the nb classifiers for sentence classification trained either with the page- or with the sentence-level label. the differences are statistically significant at 1% confidence level. at the page-level  all of them performed very well with auc above 1. the numbers are omitted because of insignificance of the differences.
　we can conclude from these results that traditional classifiers trained to work well on pages are not optimized for sentence level detection and milboost helps improve their performance.
1.1 multi-class sentiment detection
　the sentiment detection problem provides a good testbed for multi-target milboost. regular sentiment detection is naturally a three-class problem with  positive    negative  and  neutral  as class labels. as mentioned before  the  positive  and  negative  classes are the target classes and the  neutral  class is the null class in the milboost setup.

figure 1: screen shot of the output of our milboost based sensitive content detection system  with the detected sensitive content  war  terror  highlighted in yellow.　again  we used the cornell movie review data for the experiment.  positive  and  negative  reviews are from polarity dataset v1 and  neutral  reviews are from subjectivity dataset v1  source reviews . since it is clear that nb performs better than decision trees in these tasks  see the previous results  we only built a multi-class mil system based on naives bayes. the performance of milboost naive bayes  boosted naive bayes and svm for multi-class sentiment detection are compared in table 1. since this is a multi-class problem  we report classification accuracy instead of auc. we can see that our milboost based system improves upon the boosted naive bayes classifier. the performance using svm is comparable to the milboost system.
　unfortunately  we do not have sentence-level labels therefore the evaluation can only be done at the page-level.
table 1: boosted and milboosted naive bayes  and svm in multi-class sentiment detection. bolded number indicates statistically significance compared with the above row.
algorithmaccuracy %boosted nb1milboost1svm1　we build a prototype that is able to run the sentiment detection on real movie review sites on the web to identify positive  negative and neutral statements in a movie review web page. figure 1 shows a screenshot of the demo. the negative blocks are highlighted by dark shades  the negative blocks with light shades while neutral blocks are not highlighted.
1.1 when does mil improve on traditional methods  - an analysis experiment
　we hypothesized before that multiple-instance learning should improve learning of traditional techniques when the amount of mixed content is high. we designed an experiment to verify this. our experiments were run on a car review dataset which contained 1 user reviews from msn autos. each review page is made of the rating score and some review texts. the objective of the learning task to identify negative opinions in review texts. we want to show that as the amount of mixed content increases  mil based approach can help traditional techniques improve.
　unlike our earlier experiments where the pages only had 1 labels  this data set had an overall review rating score from 1. we assume that if the rating score is 1 or below  there will be some negative opinions in the review text. this was also verified by reading a sample of the pages. we further split the negative reviews into two subsets  one with rating scores from 1 to 1  later refered to as  data 1  and the other with ratings from 1 to 1  later refered to as  data 1  . presumably  the percentage of negative sentences in  data 1  will be much higher than that in  data 1 . so if our hypothesis hold right  mil based techniques should give a bigger boost in the latter data set.

figure 1: screen shot of the milboost based multi-class sentiment detection system  with negative opinions highlighted in green  dark shade   and positive opinions highlighted in yellow  light shades . neutral blocks　we trained classifiers on each of the datasets  data 1  and  data 1 . the positive training set for each of these are not highlighted.
experiments remained the same  reviews with rating of 1 or higher . we compare the performance of boosted and milboosted naive bayes on the two training sets. as we don't have sentence-level labels for this dataset  the evaluation is done at the page  review  level. the results summarized in table 1 are averages over 1-fold cross validation.
table 1: experiment to show that milboost helps traditional classifiers in data sets when the ratio of mixed content is higher. results are given on car review datasets with nb base classifier. the task is to identify negative opinions in reviews. numbers are auc. bolded number indicates statistically significant improvement compared with the number in the same row.
training setmilboost nbboosted nbdata 111data 111　from the results in table 1  we observe that for  data 1  with strongly negative reviews  the milboost based system did not improve much over the regular boosted system. for  data 1  however  which has a larger percentage of mixed content  the milboost system gave statistically significant improvement over traditional classifiers of the same complexity. we can conclude that with good quality training data  milboost does not give much advantage over traditional methods. however  if the training data has a high ratio of mixed content  then milboost does provide significant advantages.  readers will notice the the results on  data 1  are poorer than that on  data 1 . this is because there are three times as many pages in  data 1  as in  data 1  and the entire class distribution is highly biased towards positive with positive to negative ratio of 1. if we combine the two data sets we perform even better with auc of 1 for milboosted nb and 1 for boosted nb .
1. conclusion and future work
　in this paper we explored sub-document classification for contextual advertisement applications where the desired content appears only in a small part of a multi-topic web document. specifically we addressed the problem of training such sub-document classifiers when only page level labels are available. we explored various traditional text mining techniques. we also explored the novel application of milboost to this problem. we showed that the milboost system is able to improve on the performance of the traditional classifiers in such tasks  especially when the percentage of mixed content is high. these systems provide good quality block level labels for free  leading to significant savings in time and cost on human labeling at the block level.
　in this paper we did not use the spatial structure of the web pages in our systems. for example  the labels of adjacent content blocks may be correlated  in which case other combination schemes can be used in lieu of the noisy-or  1  1 . these can be used in conjuction with the hierarchical page structure    to improve the performance of the mil framework. these are all potential directions for future work.
