we describe recent extensions to the ephyra question answering  qa  system and their evaluation in the trec 1 qa track. existing syntactic answer extraction approaches for factoid and list questions have been complemented with a high-accuracy semantic approach that generates a semantic representation of the question and extracts answer candidates from similar semantic structures in the corpus. candidates found by different answer extractors are combined and ranked by a statistical framework that integrates a variety of answer validation techniques and similarity measures to estimate a
probability for each candidate. a novel answer type classifier combines a statistical model and hand-coded rules to predict the answer type based on syntactic and semantic features of the question. our approach for the 'other' questions uses wikipedia and google to judge the relevance of answer candidates found in the corpora.
1. introduction
in this paper we describe the ephyra question answering system that has been evaluated in the trec 1 qa main track. the system extends the approach we reported on at last year's trec evaluation  and the overall architecture and design goals have been described in past papers  1  1 . here we focus on recent improvements and new techniques that proved effective in this year's evaluation.
   we augmented our answer extraction approaches for factoid and list questions with a high-accuracy semantic answer extractor that is based on semantic role labeling. the question is transformed into a semantic representation and answer candidates are extracted from phrases which match this representation. different query generation techniques are used to retrieve relevant text passages  ranging from simple keyword queries to specific query strings such as reformulations of the question into answer patterns. wordnet is used to expand query terms with semantically related concepts. as full semantic role labeling of all retrieved sentences is a prohibitively time-intensive task  we carefully select candidates
sautter ira.uka.de
that are further analyzed and transformed into semantic representations. a fuzzy similarity metric is then used to compare these representations to the question representation to identify semantic structures that potentially contain an answer. the similarity measure was designed to be flexible and robust in order to maximize the recall of the answer extractor.
   a statistical framework for answer selection combines the answer candidates produced by this semantic approach and previously developed answer type based and pattern based extractors . the framework estimates the probability of an answer candidate based on a set of answer validation and similarity features. validation features use external semantic resources to verify an answer  while similarity features measure the syntactic and semantic similarity to other candidates.
   one of the crucial steps in answering factoid and list questions is the classification of the questions with respected to the expected answer type. for this purpose  we have developed a classifier that uses both manually encoded rules and a statistical model to predict the answer type given a set of syntactic and semantic features of the question. this hybrid approach outperforms our previous pattern-based classifier.
   to answer the 'other' questions  we use wikipedia and google searches to identify keywords that frequently occur in the proximity of the target. assuming that these keywords provide relevant information on the target  we favor those answer candidates that contain the frequent terms. additional filtering techniques are used to drop redundant and non-informative answers to improve the precision of our responses.
   in this year's trec evaluation  answers were extracted from both the aquaint1 newswire corpus and the blog1 corpus  a crawl of a large volume of web logs. we submitted runs using only the newswire corpus and a combination of both corpora  which gave us an insight into the benefits and also the challenges arising from the use of a large corpus containing a significant proportion of noisy text as an additional source.
   the remainder of this paper is organized as follows. section 1 gives an overview of related approaches. section 1 discusses recent improvements and extensions of our pipeline for factoid and list questions  while section 1 deals with the 'other' questions. section 1 describes our trec runs and summarizes the evaluation results; section 1 outlines open issues for future work.
1. related work
moldovan and novischi  use relations in wordnet  to derive topically related terms for query expansion. terms are considered semantically similar if they are linked through a lexical chain  a sequence of related wordnet synsets. for each of the wordnet relations  a weight has been determined empirically that reflects the degree of similarity between related synsets. we adopt the idea of lexical chains and reuse the proposed weights to calculate confidence scores for semantically related concepts.
   nyberg et al.  describe how the javelin qa system has been extended with domain semantics to answer questions in a restricted domain. a manually created ontology covers frequent concepts in this domain and english expressions with domain-specific meanings.  introduces a lightweight knowledge-based reasoning framework for javelin. questions and text passages are transformed into uniform semantic representations and a flexible unification framework matches questions with relevant passages  using weighted semantic relations between terms.
   the qa system from the national university of singapore  used the semantic role labeling system assert  to answer factoid and list questions in past trec evaluations. predicate-argument structures are extracted from the question and answer sentences. the predicates are then compared using a similarity metric composed of the similarity of the predicate verbs and the similarity of the arguments. we refined the similarity measure for predicates to make it more flexible and robust to parsing errors in order to improve the recall of this approach. the terms in the arguments are expanded with wordnet  and the semantic similarity of the arguments is measured in addition to their syntactic similarity.
   to select the most probable answer s  from an answer candidate list  qa systems have applied several different answer selection approaches. one of the most common approaches relies on external resources such as wordnet  cyc and gazetteers for answer validation  1  1  1 . the web has also been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords . collecting evidence from similar answer candidates to boost the rank of redundant answers is also important for answer selection. one popular approach is to cluster identical or complementary answers  1  1 . our recent work  proposed a unified probabilistic answer ranking framework which combines different techniques to validate answers and exploit answer redundancy for the answer selection task. the results in this year's trec evaluation demonstrate the effectiveness of this framework.
   the evolution of answer type classification approaches mirrors that of natural language understanding algorithms in general: initial approaches consisting of handcoded rules  or patterns  were followed by a variety of data-driven approaches based on simple  or complex  features  with the occasional emergence of a hybrid algorithm . while a machine learning technique based on syntactic and semantic features achieves one of the highest reported accuracies  1%  for classifying english questions with fine granularity  1 types  using a standard data set   it is also interesting to note that day et al.  report that combining a rule-based classifier with a data-driven classifier yields higher accuracy on chinese questions than either approach used in isolation. notably absent is an empirical study comparing different global strategies for combining manually-encoded and automatically-acquired linguistic knowledge for answer type classification.
   a simple but very effective approach for answering the 'other' questions has been introduced by kaisser et al. . google is used to search for text snippets that contain the target of the question and the frequencies of the terms in the snippets are counted. assuming that frequent terms provide important information on the target  answer candidates containing these terms are assigned higher confidence scores. we adopt a similar approach but use wikipedia in conjunction with google to determine the term frequencies.
1. factoid and list questions
our approach for factoid and list questions is based on a pipeline architecture consisting of components for question analysis  query generation  search  and answer extraction and selection. while the pipeline layout has been described in some detail in previous papers  1  1   this section focuses on recent extensions that were deployed for this year's trec evaluation  including an improved answer type classifier  section 1   a high-precision semantic answer extraction approach that is based on semantic role labeling  sections 1 - 1   and a probabilistic framework for the combination and selection of answer candidates  section 1 .
1. answer type classification
classifying questions in terms of their expected answer type is a crucial first step in automatic question answering. some qa systems rely solely on handcoded rules or heuristics to determine the expected type of the answer  while other systems rely solely on automatically-learned patterns or probabilistic models based on features to classify questions. for this year's trec competition  we adopted a hybrid approach that combines manually-encoded rules with a learned model. the result of answer type classification is used downstream in the ephyra system to aid answer extraction and selection.
answer typesacronymlanguagerateanglelegalsentencerelationbirthstonelocationreligionbodypartmaterialscorecauseofdeathmedicaltreatmentsizecolormoneysocialtitlecreaturemusicalinstrumentsportcrimemusictypestyledatenationalitytemperaturediseasenumbertimedramatypepathogentimezonedrugpercentageunitdurationprofessionurlfoodpropernamezodiacsignfrequencyrangetable 1. top-level answer types used in ephyra.
1.1. answer type hierarchy
a crucial but often variable aspect of answer type classification is the answer  or question  typology used. early qa systems used a small set of coarse-grained types  but modern systems  including ephyra  usually use a larger set of finegrained types. because of the underspecified nature of many questions  e.g. where is the conference  as opposed to what city is the conference being held in    the typology used is usually a hierarchy  allowing for varying levels of granularity in the classification. ephyra uses a set of 1 answer types arranged in a hierarchy with 1 top-level categories  which are shown in table 1. the hierarchy was designed to cover answer types frequently found in past trec questions. named entity recognizers have been devised to extract candidate answers of all of these types except for a few high-level categories such as food or event. we use the stanford ne recognizer  to extract answers of the types person  organization and location and rule- and list-based taggers we built for the remaining types.
1.1. features
the features we use for answer type classification fall into three categories: lexical  syntactic  and semantic. they are described in detail below. all of our features actually appear as binary features when input to the learning algorithm.
lexical features
  unigram : individual tokens present in the question.
  bigram : pairs of adjacent tokens in the question.
syntactic features
  focus adj : the focus adjective or adverb of the question; only applicable for how questions that ask about the degree of some property  such as how fast can whales travel 
  main verb : the main verb of the question  determined from the syntactic parse of the question using collins-style head rules.
  wh word : the question word.
  wh determiner : indicates whether the question word serves as a determiner to the focus word.
semantic features
  focus type : the semantic type of the question focus word  e.g. city in which city hosted the 1 winter olympics  ; only applicable for questions with a whword of what or which. the focus word is identified using a manually-compiled set of syntactic patterns which are matched against a syntactic parse of the question. the semantic type of the focus word is determined by
- traversing the wordnet hypernym tree for each sense of the focus word 
- looking up each hypernym synset in a manuallyconstructed  many-to-one mapping1 from wordnet synsets to a set of general answer types 
- adding any successful mappings to a set of candidate types  and
- selecting the candidate type which corresponds to the hypernym synset with the shortest hypernym tree traversal.
consequently  an incorrect disambiguation of the focus word sense is possible  although not common. if the focus word's semantic type cannot be determined using this approach  the focus word itself is used as the value of this feature.
1.1. classification approach
initially  the above features were meant to be used for training a model-based classifier. however  given information as predictive as the focus word type  focus adjective  and the question word  the answer type classification task becomes fairly straightforward. this motivated us to construct a simple rule-based classifier that tests the same features  including bigram and unigram . we also built a hybrid classifier that combines the outputs of the model-based and rule-based classifiers using their associated confidence scores.
approachacc.  % model-based1rule-based1hybrid1table 1. performance of different answer type classification strategies.
   to compare these three approaches  we measured their classification accuracy on trec 1 questions  with questions from trec 1  1 and 1 serving as training data for the model-based classifier and as development data for the rulebased classifier. the results are shown in table 1. because the hybrid classifier outperformed both the model-based classifier and the rule-based classifier  this is the approach we adopted. in the final system used for this year's evaluation  the modelbased component of our approach was trained on questions from trec 1.
1. semantic parsing of questions
we augmented our answer type based and pattern based answer extraction techniques  1  1  with a semantic parsing approach that generates a semantic representation of the question and extracts answer candidates from phrases in the corpus that match this representation.
   the semantic role labeling  srl  system assert was used to label semantic structures in questions and the corpus  using the propbank inventory of semantic roles . srl is a form of shallow semantic parsing that labels predicateargument structures consisting of a target verb  usually describing an event  and a set of arguments along with their semantic roles in the event. for instance  the sentence the cmu campus at the west coast was founded in 1. contains the predicate-argument structure
  target: founded
  arg1: the cmu campus
  argm loc: at the west coast
  argm tmp: in 1
where arg1 refers to the patient or theme  argm loc to the location and argm tmp to the time.
   since srl systems often fail to correctly label the semantic roles in questions  we first transform the question string into a statement  using a number of simple syntactic transformation rules. an appropriate rule is selected based on the interrogative pronoun of the question  adjacent prepositions and noun phrases  and the expected answer type. examples of transformation rules are given in table 1. for instance  applying the second rule  the question in what year was the cmu campus at the west coast established  can be transformed into the statement the cmu campus at the west coast was
  target: established
term: established  ne types: -  aliases: founded  weight 1  
launched  weight 1 
  arg1: the cmu campus
	term:	cmu  ne types: organization 
aliases: carnegie mellon
 weight 1 
	term:	campus  ne types: - 
aliases: -
  argm loc: at the west coast
	term:	west coast  ne types: location 
aliases: -
  argm tmp: missingfig. 1. semantic representation of the sample question.
established now. the phrase now is a placeholder for an argument that specifies the time and represents the answer to the question.
   assert is used to extract predicate-argument structures from the resulting statement. the placeholder argument is dropped and the corresponding semantic role is marked as missing  indicating that this is the information the question is seeking. the arguments are further split into terms  which are units of meaning consisting of one or more tokens  e.g.  carnegie mellon    west coast  . terms are used for query generation and expansion  section 1  and to measure the similarity between predicates in questions and corpus sentences  section 1 .
   we use our ne recognizers and wordnet lookups to extract compound terms from the predicate arguments. wordnet is also used to expand terms with semantically similar concepts  following an approach similar to . each term is mapped to a synset in wordnet and a breadth-first search along wordnet relations identifies related synsets. we make use of relations such as synonym  hypernym  hyponym  holonym and meronym and restrict the search depth to a maximum of two relations. the related terms are assigned confidence values based on the relations on the path from the original term  adopting the weights suggested in .
   figure 1 shows the semantic representation that is generated for the sample question in what year was the cmu campus at the west coast established 
1. query generation and expansion
we generate various types of queries  ranging from recalloriented queries such as bags of keywords to specific query strings which retrieve text passages that closely match the structure of the question:
interrogative pronoun
+ adjacent phrasesanswer typetransformation rulewhereanydrop where  move auxiliary verb to main verb  append placeholder argument here pp  + what + npdate  timedrop  pp  + what + np  move auxiliary verb to main verb or subtypeappend placeholder argument nowtable 1. question transformation rules.  pp  refers to an optional preposition  np to a noun phrase.  keyword queries are duplicate-free sets of the content words in the question.
example: cmu campus west coast established
  term queries consist of the question terms  single tokens or compound expressions   expanded with semantically related concepts.
example:  cmu or  carnegie mellon   campus
 west coast   established or founded or launched 
  predicate queries are formed from the predicate verb and arguments.
example:  the cmu campus   at the west coast  established
  reformulation queries are obtained by rephrasing the question into an answer pattern. example:  the cmu campus at the west coast was established in 
   the above queries are used to retrieve text passages from both the web and the corpora used in this year's trec evaluation  aquaint1 and blog1   and answer candidates are extracted from both sources. the web answers are matched with the answers found in the corpora to identify supporting documents  following the answer extraction approach described in .
   we use google to search the web and the indri search engine  which is part of the lemur toolkit   to retrieve passages from the trec corpora. for the top 1 google snippets  we fetch the entire web documents and convert them to plain text. both the web documents and the passages from the corpora are segmented into sentences.
1. extraction of candidate answers
semantic parsing is a time-intensive task and not all of the retrieved sentences are equally likely to contain an answer. thus we first narrow down the number of candidate sentences before we parse them with assert. a sentence is considered relevant if it meets the following constraints:
  the number of tokens in the sentence falls between upper and lower thresholds that are typically satisfied by a natural language sentence.
  the sentence contains the predicate verb from the question or a semantically related verb.
  if the answer type of the question is known  the sentence must contain an entity of that type.
  the sentence contains at least one additional term that is semantically similar to a term in the question.
   sentences that pass the above tests are parsed and transformed into a semantic representation similar to the one given for the question in figure 1. the semantic structure of each sentence is compared to the question and a similarity score is calculated as described in the following.
   the term similarity of two terms t1 and t1 is defined as the jaccard coefficient of the sets of content words w1 and w1 in the terms:

   the expanded term similarity between an answer term ta and a question term tq takes into account that each question term t has related concepts r = {r1 ... rn} with weights w r1  ... w rn :

	where	w tq  := 1
   the verb similarity of an answer predicate pa and a question predicate pq is the expanded term similarity of the predicate verbs va and vq:
sv  pa pq  := set va vq 
   the argument similarity of an answer predicate pa and a question predicate pq is determined by comparing the sets of terms within the arguments of the predicates  denoted ta and tq. we have extended the concept of the jaccard coefficient to take the semantic similarity of terms into account  rather than just distinguishing between common terms and terms that appear exclusively in one set:
each term in ta is compared to all terms in tq and the maximum of the similarity scores is computed. if the maximum is larger than 1  then the term is assumed to be covered by both predicates and the numerator of the coefficient is incremented by this score  else the denominator is incremented by 1.
   finally  the predicate similarity of an answer predicate pa and a question predicate pq is the product of their verb and argument similarity scores:
sp pa pq  = sv  pa pq  ¡Á sa pa pq 
   this scoring mechanism has been designed to be flexible and robust to parsing errors in order to maximize the recall of the answer extraction. the idea of using a jaccard coefficient to measure the similarity of all arguments as a whole was introduced in . it takes into account that srl systems often fail to assign the correct semantic roles to the arguments  which makes a per-argument comparison infeasible. we extended this idea to perform a fuzzy matching not only for arguments but also at the level of terms.
   if the confidence score of a predicate is larger than 1  it is considered semantically similar to the question and one of the following strategies is used to extract factoid answers:
  if the answer type of the question is known  entities of the expected type are extracted from all arguments of the answer predicate. this takes into account that srl systems often mislabel the arguments.
  if the answer type is unknown and the answer predicate has an argument with the role that is missing in the question  this argument is extracted as an answer.
the confidence score of an answer is the sum of the confidence scores of all the predicates it was extracted from.
1. score normalization and combination
the answer candidates retrieved with this semantic approach are combined with candidates from our answer type based and pattern based extractors  1  1 . since these extraction techniques use different underlying scoring mechanisms that produce incomparable scores  it is necessary to normalize the confidence scores before merging the answers. we trained an adaboost classifier  that uses a decision tree as the underlying weak learner to classify answer candidates into correct and incorrect ones. the classifier is applied to unseen answer candidates and the probability of the positive class is used as a normalized confidence score. the following features were used to train the classifier:
  score assigned to the candidate by the extractor.
  answer extractor that found the candidate.
  predicted answer type s  of the question.
  number of candidates from the same extractor.
  minimum and maximum score over all candidates.
   for one of our runs  we used a simple score combination scheme  known as combmnz   to merge the normalized scores of a candidate found by multiple extractors: the combined score is the sum of the  normalized  scores from all extractors  multiplied by the number of extractors that found the answer. however  this technique was outperformed by the more general answer selection approach described in the following section  which was used for the remaining runs.
1. answer selection
the answer generator  ag  is responsible for selecting the correct answers from the candidates produced by our answer extractors. a statistical framework estimates the probability of an individual answer candidate given a set of validation features that predict its relevance according to external resources  and a number of similarity features that exploit redundancy among the answer candidates. the ag has been described in detail in our previous work .
   to estimate the relevance of an answer candidate  we use four external resources. gazetteers and wordnet are used in a knowledge-based approach  e.g. to check whether a candidate satisfies the relationship described in the question such as isa shanghai  city  or is-in shanghai  china  . the web and wikipedia are used in a data-driven approach. for instance  if there is a wikipedia document whose title matches the answer candidate  the document is analyzed to obtain a tf-idf score  which is used as a relevance feature. web snippets are used to calculate a word distance between an answer candidate and question keywords.
   to identify similar answer candidates found by different extractors  we combine various syntactic and semantic similarity features. string distance metrics such as the levenshtein distance and the cosine similarity are used to measure the syntactic similarity of answer candidates. a database of synonyms was generated from wordnet  the cia world factbook and wikipedia.
   as we use three different approaches to extract answer candidates  and each extractor sometimes produces more than 1 candidates  the ag only estimates probabilities for the top 1 answer candidates from each extractor. the answer candidates are then reranked according to these probabilities. for factoid questions  the highest ranked candidate is chosen as the final answer.
   for list questions  we return all candidates with estimated probabilities of at least 1% of the probability of the top answer. in our experiments on previous trec questions  we first attempted to maximize the f1 score by returning all candidates with probabilities of at least 1. however  the probabilities assigned by the ag turned out to be rather unreliable estimates of the correctness of an answer candidate because

fig. 1. pipeline layout for the 'other' questions.
of the high variance of the original scores from our answer extractors. for instance  the score of the top answer was sometimes very low  which resulted in estimated probabilities below the threshold of 1.
1. 'other' questions
our approach for the 'other' questions uses answer projection from the web onto the corpora  picking up the idea of kaisser et al. . however  we use wikipedia in addition to google to retrieve terms that are important in the context of the target. a google query can be ambiguous and the results may be unrelated to the target  while we can retrieve information from wikipedia unambiguously by searching for an article on the target. we furthermore assume that in the online encyclopedia  users make sure the most relevant information on the subject  target  is given in a concise and concentrated fashion with little noise. for targets not found in wikipedia  we extract terms from google snippets as a fallback solution.
   for selecting the relevant snippets from the corpus  we further deploy several of the filtering techniques introduced last year  to eliminate nonsensical  redundant or irrelevant information. the modular architecture of ephyra enabled us to evaluate various combinations of filters  parameterizations and orders. in the following we describe the key components of our pipeline  illustrated in figure 1.
1. snippet retrieval
we first retrieve whole paragraphs from the corpora  using the indri ir engine from the lemur toolkit . as the initial set of passages can be quite large  we run them through a number of filtering mechanisms before selecting answer candidates:
   reduction of snippet size. in order to allow more finegrained filtering operations and to increase the precision of our answers  we reduce the size of the passages as much as possible. previously reported results  1  1   and an analysis of the snippets that were judged vital or ok in past trec evaluations  led us to the conclusion that the most effective answers are sentences or sentence fragments. thus we split paragraphs into sentences  and we further segment long sentences into their atomic clauses  figure 1  filter 1 . this helps us to deal with additional material outside these clauses  such as explanatory prefixes  e.g.  paris  france  afp ...   and indirect speech  e.g.  secretary gates said that ...  . by retaining only clauses that contribute relevant information we further increase the precision  figure 1  filters 1 & 1 .
   elimination of redundancy. in order to minimize the time required for the web-based scoring process  duplicate snippets are eliminated. semantic duplicates  i.e. snippets providing the same information with slightly different wording  are detected with a bag-of-words comparison mechanism  which stems content words and ignores stop words in the similarity calculation  figure 1  filter 1 .
   elimination of useless snippets. the snippets from the aquaint1 corpus turned out to include two special types of likely useless snippets  which mainly appear if the target is a person's name. in particular  these include long lists of proper names without any further information  and snippets that consist of a person's statement in direct speech. we filter out lists of proper names based on the observation that most of their tokens are either stop words  or parts of a proper name and thus capitalized  figure 1  filter 1 . our experiments on previous trec questions proved that the risk of useful snippets being lost to this filter is very low. direct speech formulations citing a person's statement sometimes contain useful information  but in general they deliver opinions rather than facts  so we decided to also filter them out  figure 1  filter 1 .
1. snippet selection
to select the actual answer snippets from the ones returned by the retrieval and filtering step  we adopted the web-based approach proposed in  with wikipedia as an additional information source. we assume retrieving an article from wikipedia is considerably less ambiguous than retrieving text snippets from a web search engine using the target as a keyword query. only if we do not find a wikipedia article  we use google as a fallback. experimental results on past trec data encouraged us to use this combined approach. the complete scoring process consists of several steps:
1.1. query generation
in order to become less vulnerable to different possible ways of naming the target  we generate several different queries from the target string  as proposed in . in particular  the generation process comprises two steps: first  we identify the type of the target  using the stanford ne recognizer . second  we vary the target string using several modification rules that depend on the target type:
  if the target contains a part enclosed in brackets  we use the part inside and the part s  outside as individual queries.
  if the target contains a proper name  we use it as an individual query.
  if the target is an organization  we produce variations with and without determiner  and we produce an acronym query from the organization's name.
  for the google fallback  we also generate quoted versions of all queries that consist of a proper name.
1.1. web term retrieval
since wikipedia is our primary source of web terms  we first try to retrieve a wikipedia article on the target. if that succeeds  we do not retrieve any further web documents. otherwise  we try to retrieve articles on the parts we extracted from the target during the query generation. if this fails as well  we obtain the top 1 snippets from google for each of our generated queries. once we have obtained all necessary web documents according to the policy above  we extract all the terms from them and count their frequencies.
1.1. snippet scoring
our scoring mechanism for the corpus snippets uses the general method of score computation reported by kaisser et al.   figure 1  filter 1   with the following refinements:  a  a count decrease parameter indicates how the counter of a term is decreased after it has contributed to the score of a snippet that is actually selected.  b  we do not simply score the snippets using the terms they contain  because this would favor longer snippets over shorter ones. instead we normalize the score of a snippet using the logarithm of the number of terms it contains.  c  we also found that using the plain count of web terms for computing the scores of the corpus snippets over-weights common terms  while under-weighting more specific terms. to compensate for this effect  we normalize the term counters by the logarithm of the term's global frequency  which we obtain from a dictionary.
1.1. answer length
to make sure our answer snippets do not exceed the maximum of 1 characters  we finally apply a filter that drops all snippets after the top ones have a combined length of some threshold ¡Ü 1  figure 1  filter 1 . the relatively low impact of precision might encourage to return the maximum allowed number of characters  but our experiments with previous trec targets revealed that with almost every parameter combination for the scoring  the optimal total length of the answer was 1 characters. consequently  we used 1 as the cutoff length.
1. evaluation results
we submitted 1 runs  differing in the answer selection approach and the corpora used for answer projection  factoid and list questions  and to extract information nuggets  'other' questions . ephyra1 exclusively used the aquaint1 corpus  while ephyra1 combined the aquaint1 and blog1 corpora and treated them as a single knowledge source. ephyra1 deployed both corpora to find supporting documents for factoid and list questions  but restricted the nugget extraction for the 'other' questions to the aquaint1 corpus. in the runs ephyra1 and ephyra1 the answer generator  ag  cf. section 1  was used to select and combine candidate answers. ephyra1 applied a simple score combination approach to merge candidates from different extractors  cf. section 1 . figure 1 shows our evaluation results and compares them to the median over all 1 runs.
   the best setup for factoid and list questions  ephyra1  projected the candidate answers found in the web onto both the aquaint1 and the blog1 corpus and deployed the ag to generate the final list of ranked answers. a comparison to the runs ephyra1 and ephyra1 shows to what extend the
blog1 corpus and the ag improved the overall performance.
   for the 'other' questions  it proved most effective to extract information nuggets from the aquaint1 corpus only. ephyra1 and ephyra1 were identical runs that restricted the search to the newswire text. the nuggets extracted from the blog1 corpus in the run ephyra1 rarely contained relevant information on the target  but often they were meaningless sentence fragments or not even natural language phrases.
1. future work
a major bottleneck of the previously described semantic approach for question analysis and answer extraction is the coverage and reliability of the semantic parser. it has been shown that by integrating multiple semantic role labeling  srl  systems  the robustness can be improved significantly . a combination of srl systems is particularly beneficial if the systems use different syntactic parsers . yet there remains a significant portion of questions and answer sentences with semantic structures that do not fit into the schema of predicate verbs and arguments. it would therefore be desirable to cover a wider range of semantic structures such as the semantic frames used in framenet .
ephyra1ephyra1ephyra1median  1 runs corporaaquaint1aquaint1aquaint1 -blog1  factoid  list blog1answer generatoryesnoyes-unsupported  u 11-inexact  x 11-locally correct  l 11-factoid accuracy1111list f1.1.1.1.1other f1  pyramid score 1111average per-series score1111table 1. trec 1 evaluation results.   currently we do not pre-annotate the trec corpora but we build a full-text index and use simple boolean queries for both the web search and to retrieve text passages from the corpora. by annotating the corpora with semantic information and integrating these annotations in the index  we could  a  improve the runtime performance and  b  formulate structured queries that combine evidence from different documents. for instance  one could search for all organizations x in the corpora that satisfy the constraints imposed by the predicate-argument structures based arg1: x  argm-loc: japan  and manufacture arg1: x  arg1: suv  to obtain a list of japanese car makers that offer suvs.
   the answer generator can be further improved by incorporating additional features to validate individual answer candidates and to identify semantically similar answers among the candidates. we will also need to conduct additional experiments to determine a more flexible cutoff strategy for list questions. furthermore  we consider extending the ag to perform answer selection not only for factoid answers but also for more complex answers such as the definitional phrases retrieved for the 'other' questions.
   extensions to our answer type classifier would include analyzing precisely how the knowledge encoded by the rules augments the knowledge contained in the training data  in the hope of discovering whether it helps to resolve noise  cover gaps  or do both. it also seems important to address questions such as: how can we most effectively cover gaps in the data with hand-crafted rules  how can we most effectively cover gaps in the hand-crafted rules by learning from data sets that target specific  variable aspects of language  which of these strategies yields the best performance  are there general linguistic principles that can guide the decision of whether to treat a particular phenomenon using rules or data 
   experiments on past trec data revealed that the effectiveness of our answers to an 'other' question highly depends on the choice of  a  the source of web terms   b  the parameter values for the scoring process  and  c  the cutoff length. therefore we need to thoroughly investigate characteristics of the individual targets and find categories of targets for which an individual combination of term source and parameter values yields the optimal result. we then need to find criteria for assigning each target to a category  so that we can choose a setup for our scoring mechanism accordingly. furthermore  additional filtering techniques and more robust parsing and sentence segmentation approaches will be required to extract useful information nuggets from resources with a high content of noise  such as the blog1 corpus.
acknowledgements
we would like to thank the interact exchange program for making this collaboration between carnegie mellon university and universitat karlsruhe possible.¡§
this work was supported in part by the aquaint program under grant nbchc1.
