approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios  including query optimization and approximate query answering. existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. unfortunately  both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. in this paper  we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. abstractly  our key idea is to break the synopsis into  1  a statistical interaction model that accurately captures significant correlation and independence patterns in data  and  1  a collection of histograms on low-dimensional marginals that  based on the model  can provide accurate approximations of the overall joint data distribution. extensive experimental results with several real-life data sets verify the effectiveness of our approach. an important aspect of our general  model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning  e.g.  wavelets  by providing an effective tool to deal with the  dimensionality curse .
1.	introduction
　capturing the joint data distribution of multi-dimensional data sets through compact and accurate synopses is a fundamental problem arising in a variety of practical scenarios  including query optimization  query profiling  and approximate query answering. costbased query optimizers employ such synopses to obtain accurate estimates of intermediate result sizes that are  in turn  needed to evaluate the quality of different execution plans  1  1 . similarly  query profilers and approximate query processors require compact data synopses in order to provide users with fast  useful feedback on their original query  1  1 . such query feedback  typically  in the form of an approximate answer  allows olap and datamining users to identify the truly interesting regions of a data set and  thus  focus their explorations quickly and effectively  with-

work done while visiting bell laboratories.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
acm sigmod 1 may 1  santa barbara  california  usa copyright 1 acm 1-1/1 ...$1.
out consuming inordinate amounts of valuable system resources. further  users can determine how well-posed/semantically-correct their query is  allowing them to make an informed decision on whether they would like to invest more time and resources to execute it to completion.
　histograms  1  1  constitute a very general class of synopsis structures that offer several advantages  including  1  they are typically built off-line and stored in the dbms catalog  thus incurring almost no run-time overhead   1  they are non-parametric  i.e.  they do not require the data to fit any particular probability distribution  and  1  for most real-world data distributions  there exist compact histograms that produce low-error approximations. as a consequence  histogram-based techniques for approximating onedimensional data distributions have been extensively studied in the research literature  1  1 . further  one-dimensional histograms have now been adopted by several commercial database systems  e.g.  db1  informix  oracle  microsoft  and sybase  replacing the naive and rarely valid uniformity assumption .
　accurate approximationsfor multi-dimensionaldata distributions  pose a problem that is much harder than its well-understood onedimensional counterpart. a simplistic approach  typically employed in the bulk of today's commercialsystems  is to useone-dimensional histograms on the marginal distributions in conjunction with a fullindependence assumption  which basically states that all data attributes are mutually uncorrelated and  thus  joint attribute distributions can be obtained as products of one-dimensional marginals. unfortunately  experience with real-life data proves that the fullindependence model is almost always invalid and can lead to gross approximation errors in practice  1  1 . as a consequence  more recent work has proposedalgorithms for building multi-dimensional histogram synopses that try to directly approximate the joint data distribution of a multi-attribute data set  1  1 . these earlier results have demonstrated that reasonably simple construction procedures can give compact multi-dimensional histograms that outperform full-independence approximations by several orders of magnitude for low to medium data dimensionalities  e.g.  1 to 1 dimensions 1.
　it is a well-known fact  however  that histogram-basedapproaches suffer from the infamous  curse of dimensionality ; that is  histograms become problematic when trying to approximate the joint distributions of the high-dimensional data sets that are typical of modern decision-support applications. the reason is that  as the dimensionality of the data increases  both the storage overhead  i.e.  number of buckets  and the construction cost of histograms that can achieve reasonable error rates increase in an explosive manner . consequently  as the dimensionality of the joint data distribution

 due to space constraints  we omit a detailed discussion of related work; it can be found in the full version of this paper .
reaches or exceeds 1 dimensions the histogram approximation errors start becoming intolerably high .
our contributions. existingwork on multi-dimensionalhistogram synopses has essentially considered only the two extremes of a rich spectrum of possibilities  both of which are unrealistic and doomed to fail for the typically high-dimensional data sets of modern decision-supportapplications. the fact that the full-independence assumption is almost never satisfied in relational database practice and can lead to extremely poor approximations has been well documented  see  for example   1  1  . on the other hand  building multi-dimensional synopses on the full-dimensional space of a multi-attribute table implicitly assumes a  fully-correlated model  of the attribute space and is guaranteed to fail in high-dimensional spaces. the work in this paper is motivated by the observation that  in mostpractical applicationscenarios  such a  fully-correlatedmodel  is also an unrealistic assumption. typically  real-life data tables are characterized by complex correlation patterns  where a certain subset of correlated attributes can be  unconditionally  independent of another attribute subset  partial independence  or  alternatively  can be  conditionally  independent of given a third subset of attributes  conditional independence . as a simple example  consider the attributes of an employee relation. even though it is natural for the salary attribute to be  strongly  correlated with the age attribute  with higher/lower salaries typically going to older/younger employees   there is no reason to believe that either of these attributes would be correlated with an employee's height or weight attributes  which are themselves correlated . also  note that  although the salary and age attributes are pairwise correlated  the two may become largely independent given a third attribute like  for example  an employee's ype  i.e.   years of practical experience  . similar complex correlation patterns abound in real-life data.
motivated by the above observations  in this paper  we propose
dependency-based db histograms  a novel approachto building histogram synopses for high-dimensional data that uses the solid foundation of statistical interaction models  1  1  1  to explore the spectrum of possibilities between the existing  fully-independent  and  fully-correlated  approaches. abstractly  our key technical idea is to break the synopsis of a high-dimensional data set into two basic components:  1  a statistical interaction model that explicitly identifies the  possibly  complex correlation and independence patterns in the data; and   1  a collection of lowerdimensional histograms  built based on the model  that can be used to accurately approximate the overall joint data distribution. to the best of our knowledge  our work is the first to demonstrate the effectiveness and feasibility of statistical interaction models as a tool for dealing with the  curse of of dimensionality  in the construction of histogram synopses for high-dimensional data. the salient technical contributions of our work are summarized as follows.
　db-histogram construction. constructing accurate db-histogram synopses raises the issue of  1  inferring a concise interaction model from the underlying data distribution  and  1  building an effective collection of histograms on the relevant marginals  dictated by the model . we propose using the broad class of decomposable interaction models  for db histograms  since it offers several important advantages  including interpretability and closedform model estimates. we discuss effective forward-selection procedures for building a concise decomposable model for the input data distribution. our most efficient model-selection algorithm is  in fact  a novel contribution of our work that is likely to be of interest to the statistical community . the construction of the marginal histograms dictated by the model discovered raises the important issue of intelligently allocating the storage budget available for the synopsis among the various marginals. we propose an optimal dynamic-programming algorithm for this problem as well as a cheaper greedy heuristic based on the concept of marginal gains that is  in fact  optimal when the histogram error functions follow a diminishing-returns law .
　db-histogram usage. efficiently estimating the selectivity of a range query predicate over  a subset of  the data attributes using db histograms requires new techniques that effectively utilize the model structure in conjunction with the marginal histograms. we propose novel usage algorithms for our db-histogram synopses that exploit the junction tree representation of the model to effectively minimize the number of histogram operations involved. an interesting side-effect of our work lies in the introduction of a new hierarchical representation  termed split trees  for the well-known class of mhist histograms  that is considerably more space efficient than the one originally proposed by poosala and ioannidis. we present novel algorithms for performing the basic mhist operations necessary for our selectivity-estimation procedure that work solely on our space-efficient  split-tree representation for both the input s  and output of the operation.
　experimental validation. we have conducted an extensive experimental study with several real-life data sets to determine the effectiveness our methodology compared to earlier techniques  including sampling and full-dimensional mhist histograms. our results demonstrate that db histograms  a  yield approximate answers of superior quality compared to existing approaches  and  b  have the ability to provide fairly accurate  concise synopses for real-life data with as many as 1 dimensions.
　an important aspect of our general  dependency-based approach is that it can be extended to all data-reduction techniques that are based on data-space partitioning  such as wavelets . typically  all such techniques suffer from the dimensionality curse  which renders them ineffective in medium to high data dimensionalities. building and maintaining a statistical interaction model can help identify significant attribute correlation patterns and  consequently  the interesting lower-dimensional subspaces that should be approximated independently in a dependency-based synopsis. we believe that our interaction model-based methodology provides a viable and effective approach for dealing with data-dimensionality issues that opens interesting new avenues for innovative research in data reduction  e.g.  incremental maintenance or approximate querying of dependency-based synopses .
1.	problem formulation
1	multi-dimensional histogram synopses
joint data distributions: definitions and notation. consider a relational table comprising real- or integer-valued attributes
　　　　　.  the definitions and methodology can be extended to non-numerical attributes by first mapping their domain values into floating-point numbers.  the information in can be accurately captured as an -dimensional array  tensor   whose dimension is indexed by the values of attribute     and whose cells contain the count  or  frequency  of tuples in having the corresponding combination of attribute values.
　more formally  let denote the value domains of attributes   respectively. without loss of generality  we assume that each domain is indexed by the set of integers
　　　　　　　  where denotes the number of distinct elements in . given a combination of attribute values
 	   the joint frequency	of the combination is exactly the number of tuples in the relation that contain the value in attribute   for all . the -dimensional  array with entries represents the joint data  or  frequency  distribution of .
　often  we are interested in the joint distribution of only a subset of the attributes ; this is the case  for example  when a query optimizer needs to estimate the selectivity of a range query with range selections specified only on attributes belonging to a subset of . in probability terms  such scenarios require the marginal data distribution on attributes . any such marginal can be obtained by projecting the joint data distribution array onto the relevant subset of attributes. assuming  without loss of generality  that
                      the marginal data distribution of is defined as	for all value combinations ; that is  we compute the marginal frequencies over  denoted by   by aggregating the joint frequency counts over the domains of all  projected-away  attributes. we also use to denote shanon's entropy measure  for the joint frequency distribution over . letting denote the number of data tuples  the entropy can be expressed as:
	prob	prob

histogram synopses. building compact synopses structures that approximate a joint frequency distribution with reasonable accuracy in limited space is critical for numerous applications  including query optimization and profiling . histogram-based synopses for approximating one-dimensional data distributions have been extensively studied in the research literature  1  1   and have been adopted by several commercial database systems. briefly  a histogram on an attribute is typically constructed by partitioning the frequency distribution of into buckets of consecutive attribute values and employing a uniformity assumption to approximate the frequencies and values present in each bucket. onedimensional histograms can also be used to approximate  multidimensional  joint data distributions through the full-independence assumption for the attributes of interest. given a collection of attributes   full independence essentially requires that all attributes have mutually independent frequency distributions.  mathematical conditions for mutual independence can be found in any standard statistics textbook; see  for example .  mutual independence  in turn  implies that any joint attribute distribution can be obtained as a product of the one-dimensional marginal distributions of the individual attributes; thus  for any  
. unfortunately  experience with
real-life data sets offers overwhelming evidence that the full-independence assumption is almost always invalid and can lead to gross approximation errors in practice .
　ratherthan relying on heuristic independenceassumptions  multidimensional histograms  try to directly approximate the joint distribution of by strategically partitioning the data space into -dimensional buckets in a way that captures the variation in data frequencies and values. similar to the one-dimensional case  uniformity assumptions are made to approximate the distribution of frequencies and values within each bucket . finding optimal histogram bucketizations is a hard optimization problem that is typically -complete even for two dimensions . various greedy heuristics for multi-dimensional histogram construction have been proposed  1  1  and shown to perform reasonably well for low to medium data dimensionalities  e.g.  =1 . like most techniques that rely on space partitioning  however  multidimensional histograms also fall victim to the  curse of dimensionality   which renders them ineffective above 1 dimensions .
1	statistical interaction models: a primer
　for several decades  statisticians have looked into the problem of constructing accurate interaction models for multi-variate contingency tables  i.e.  tables of counts   where the elements of an underlying population are classified according to a number of different categories  or  dimensions   1  1 . traditionally  the goal of such statistical multi-variate analysis is to discover and understand  strong  association patterns in the data and find a structural model of variable interactions that accurately and concisely describes these data patterns.
log-linear models. log-linear models comprise a broad class of statistical interaction models for contingency tables that has been extensively studied in the statistical literature. the foundations for the theory and methods of log-linear models were laid back in the early 1s  culminating in a series of seminal books and monographs on the subject . consider an -dimensional contingency table on categories with entries .  the direct correspondence between contingency tables and joint data distributions should be obvious; both just store the counts/frequencies of distinct value combinations for the data dimensions.  briefly  the most gen-
eral  or  saturated  log-linear model for the table expresses the logarithm of the -dimensional cell counts as a summation of effects whose dimensionality ranges from up to . more formally  the saturated log-linear model for is expressed as
 1 
where the summands capture the interactions between the variables denoted by their subscripts with the terms in the parentheses indexing the specific position of that interaction  in the same order of variables . thus  denotes the interaction effect between values and of variables/dimensions and respectively. abstractly  an -dimensional interaction effect captures the deviations of the means of log-frequencies at that level of aggregation from the means at  coarser  aggregation levels. for example  the one-dimensional effects  also known as main effects  represent the deviations of the means of log-frequencies along dimension from the overall mean . note that linearity in the logarithms of the counts corresponds to multiplicative relationships between the actual frequencies  which means that log-linear models often have very nice interpretations in terms of the independence properties in the underlying set of variables.
　the saturated log-linear model depicted in equation  1  is the most general model for dimensions  in the sense that it specifies enough interaction parameters to accurately capture any variate contingency table.  in that sense  the saturated model corresponds to a model of  fully-correlated  data dimensions.  simpler log-linear models reduce the number of parameters by specifying certain interaction effects to be zero  which can typically be interpreted as a form of  independence  between the corresponding dimensions. as an example  consider the log-linear model for
a	-dimensional table over	in which
                                       for all ; that is 	. it is fairly simple to verify that  for data conforming to this model  the underlying frequencies can be derived based on the one-dimensional marginals as	for all
	  where	is the sum of all frequencies
 i.e.  the number of all data points  in the table.  throughout the paper  we use to denote frequency estimates based on model
　  often omitting the subscript when the model used is obvious.  thus  our example log-linear model is exactly equivalent to the fullindependence model for dimensions. intuitively  this is exactly what we would expect  given that this model sets all interaction effects between any two or three dimensions to zero.
　in general  a log-linear model involves specifying certain interaction effects to vanish and letting the remaining interactions be arbitrary and unknown. the practice and theory of log-linear models typically revolves around a smaller class of models  termed hierarchical models  that are characterized by the following key property: for any   if is specified to vanish then  for any   is also specified to vanish. a simple notation used for describing hierarchical log-linear models is to list their maximal variable interaction components  also known as the model generators   in square brackets. for instance  the saturated -dimensional log-linear model can be denoted as and the full-independence model is simply . more interesting correlation/independence patterns can also be captured with hierarchical models.
partial independence. that is  two disjoint subsets of the data dimensions are  unconditionally  independent of each other. for example  the model specifies that dimension is independent of the dimension pair .
frequency estimates for this model are derived directly from the one- and two-dimensional marginals for variables and respectively:	  for all  again  denotes the frequency total .
conditional independence. that is  two disjoint subsets of the data dimensions are independent given the value s  for a third subset. for example  the model states that dimensions and are independent given a value for dimension . frequency estimates are again directly derived from the relevant two-dimensionalmarginalsas:
	for all	.
we should stress  however  that not all hierarchical log-linear models admit interpretations in terms of correlation/independence patterns amongdimensions. a simple example is the model which  in fact  is the smallest non-interpretable hierarchical model. besides not admitting a  statistical interpretation   an important practical problem with such models is that they also do not admit closed-form estimates based on marginal data distributions. instead  frequency estimates must be obtained through an iterative numerical procedure known as iterative proportional fitting  ipf  that  briefly  tries to discover a  maximum entropy  collection of frequencies that also satisfies the constraints on the marginals imposed by the specific model  1  1 .
decomposablemodels: chordal graphs and junction trees. as a consequence of these problems with general log-linear models  statisticians often focus on restricted subclasses of models that are easier to interpret and/or use. an important such subclass is the class of decomposable models. abstractly  the class of decomposable models comprises the subset of log-linear models that satisfy the following two important properties.
1. interpretability: decomposable models can be described in terms of correlation/independence relationships between the relevant variables. furthermore  decomposable models can always be represented graphically as a markov network  from which the underlyingcorrelation/independencerelationships can be directly inferred.
1. direct frequency estimates: the frequency estimates for a decomposable model can always be obtained directly from the marginals corresponding to the model generators as a closed   product-form  expression. that is  decomposable models obviate the need for iterative numerical techniques like ipf.
　our example full independence      partial independence      and conditional independence     models are decomposable  with the closed-form frequency estimates as defined above for each individual case. figure 1 a  gives the markov networks corresponding to these three models  where the nodes correspond to the model variables and edges represent the existence of an interaction between variables. note that the generators of the model correspond directly to the cliques  i.e.  maximal complete subgraphs  of its markov network representation. the key probabilistic property of a markov network  known as the global markov property   is that if two node sets and are separated by a third node set   then and are conditionally independent given .  for a more thorough introduction to the theory and practice of markov networks  the interested reader is referred to .  it is easy to see that the number and complexity of possible decomposable models grows explosively with the number of variables  1  1  1 . figure 1 b  shows a somewhat more complex example of a -dimensional decomposable model   namely
                 . from the markov graph  it is easy to read  for example  that variables are conditionally independent given variables and  similarly  that variable is independent of given variable . the frequency estimates for
can be simply derived as
. our
work relies heavily on the graph-theoretic properties of the markov network representationof a model ; therefore  we use to refer to both the interaction model and its markov graph in the remainder of this paper.

figure 1: markov networks for  a  three simple -dimensional decomposable models  and  b  a more complex -dimensional decomposable model.  c  a junction tree for the -dimensional example model.

　an interesting property of decomposable interaction models is that they correspond exactly to the class of chordal  or  triangulated  markov network graphs. a chordal graph has the property that any cycle of length four or more has a shortcut  chord  between any two non-consecutive nodes. thus  for example  the model is not decomposable  since it corresponds to a non-chordal markov graph  a -cycle .
　a compactand particularly useful representationof chordalgraphs is provided by junction trees  also known as clique trees   1  1 . briefly  given a chordal graph   a junction tree is a tree structure defined over the cliques  i.e.  generators  of characterized by the following clique-intersection property:  for each pair and of cliques in   the set is contained in every clique on the path connecting and in .  given
a connected graph  a junction tree exists if and only if the graph is chordal ; thus  a junction tree offers a complete characterization of a chordal graph and the corresponding decomposable model. figure 1 c  depicts a junction tree for the model
           . an important property of the junction-tree representation for a decomposable model that the frequency estimates for are always equal to the ratio of the product of the marginal frequencies on all the model cliques divided by the product of the marginal frequencies on the intersections of cliques that are immediately adjacent in the junction tree . as an example  it is simple to verify that the junction tree shown in figure 1 c  directly gives the formula mentioned earlier in this section for .
1	problem statement
　given the aforementioned problems with the full-independence assumption and conventional multi-dimensionalhistograms  we propose a novel approach that employs the solid foundation of interaction models to explicitly identify and exploit statistical patterns in the data during the construction and usage of histogram synopses. our key technical idea lies in breaking the synopsis into  1  a statistical interaction model that accurately captures the independence/correlation patterns in the data  and  1  a collection of  lowdimensional  histograms on marginals that  based on the model  can approximate the overall joint data distribution. more formally  in this paper  we define the notion of a dependency-based histogram synopsis as follows.
　definition 1. a dependency-based db  histogram for a collection of attributes is defined as a pair
　　　　　  where:  1  is an accurate decomposable model for the joint data distribution of ; and   1  is a collection of  low-dimensional  histograms that approximate the marginal distributions for each generator  i.e.  clique  of .  we refer to these histograms as the clique histograms of .  
decomposable vs. general interaction models. definition 1 restricts the class of statistical interaction models for dependencybased histograms to that of decomposable models. there are several reasons for this. first  even though the space of decomposable models is smaller that that of general hierarchical log-linear models  it is still large enough to include a multitude of models that capture complex  interpretable  interaction patterns among the data dimensions. for example  in less than or equal to dimensions  there are hierarchical and decomposable models .  also note that  for any model  there always exists a more general model that is decomposable  since the saturated model is trivially decomposable.  second  as already mentioned above  decomposable models have the benefits of interpretability and closed-form frequency estimates. models that are interpretable in terms of partial and conditional independence can provide useful insights into the intrinsic properties and correlations in the data  even for purposes other than synopsis construction  e.g.  data mining . closedform model estimates are obviously desirable  since they avoid expensive iterative procedures like ipf. ipf-based estimation is especially bad when estimates on certain marginals are required  since the only way to estimate the marginals is to go through the full joint distribution  e.g.  given the model   computing the marginal on requires ipf to rebuild the full
 -dimensional frequency distribution . this is not the case for decomposable models where  as we will see  such marginal computations can be carefully optimized based on the model structure  section 1 . finally  besides the obvious computational benefits in their usage  due to the closed-form estimates   decomposable models have properties that can also substantially reduce the computational effort involved in their construction from the joint distribution data. these properties are thoroughly exploited in our model-selection algorithms  section 1 .
quantifying model accuracy:  goodness-of-fit  testing. our goal is to discover interaction models that accurately capture the correlation patterns in the joint data distribution. thus  we need a measure that quantifies the  distance  of the set frequency estimates based on a given interaction model to the true set of frequencies in the joint distribution. following standard statistical practice  1  1  1   we employ the usual distance measure for probability distributions  namely the kullback-leibler information divergence  also known as  discrimination information  or  crossentropy    defined as

the information divergence can essentially be interpreted as the difference of the information contained in and that contained in about . the quantity is always nonnegative and vanishes if and only if coincides with ; that is  if and only if the model exactly captures the joint data distribution. furthermore  information divergence is equivalent to the likelihood ratio test statistic for the null hypothesis :  model generated     and is directly correlated to the chi-square
 	  distance of the two distributions; in fact  it is well known that
  so that minimizing divergence is equiv-
alent to minimizing	distance .
problem statement: dependency-basedhistogram construction and usage. motivated from the inadequacies of earlier solutions and the success of statistical interaction models in the analysis of contingency tables  this paper proposes a novel approach to building multi-dimensional data synopses grounded on the concept of db histograms. more formally  the problem we attack in this paper can be stated as follows.
 given: a relational table on attributes and a space budget for constructing a synopsis on .
 need: efficient algorithms for:  1  building an accurate and concise db histogram on that uses at most units of space; and   1  effectively utilizing for providing quick approximate answers to selectivity-estimation queries over the joint data distribution and any one of its marginals.
　the accuracy of a db histogram obviously depends on the accuracy of each of its components. our algorithms quantify the accuracy of the interaction model using the standard kullback-leibler information divergence measure  as described above. the accuracy of the clique histograms in can be quantified using any of the standard quality measures for histogram synopses  e.g.  total variance   1  1 .
　the concisenessof the interaction model is another important practical requirement for our db histogram synopsis. the observation here is that a more complex model will always fit the given data better; for example  the saturated model can always provide an exact fit but is not always the correct interaction model for our purposes. essentially  we are interested in the most parsimonious model that captures all the significant correlations and independencies in the data  so that these patterns are explicitly accounted for during the construction of the synopsis. our db-histogram construction algorithms use two distinct strategies to control the conciseness of the induced model .
enforce dimensionality bound on marginals. the idea here is to

impose an upper bound	on the dimensionality of each generator in . this bound limits the complexity of the model by explicitly restricting the dimensionality of each marginal distribution that we need to approximate through a clique histogram in . such a bound can be set heuristically  based on pragmatic constraints on the effectiveness of histogramapproximations; for example  sinceit is well-known that the accuracy of histogram-based synopses drops rapidly above 1 dimensions   a value of may be a reasonable choice for restricting model complexity while not compromising the accuracy of the db histogram.  a more principled approach would be to set the value of based on intrinsic characteristics of the data distribution  such as its fractal dimension  that  in a sense  measures its  distance  from full independence; we intend to explore such methods in future work. 
enforce statistical significance thresholds. this method is based

on the use of significance levels to enforce a lower bound on the statistical significance at which additional complexity is introduced into the model. abstractly  what this means is that when deciding between a  simple  model and a more complex model   is preferred only if the difference in the quality of the estimates  i.e.  the  goodness-of-fit   is sufficiently high to be statistically significant. the goal  of course  is to ensure that the selected model captures only the truly significant patterns in the data while ignoring details that can be attributed to statistical noise  i.e.  avoiding  overfitting  the data .
1. dependency-based histograms 1 model selection
　apart from certain special cases  the general problem of inferring an  optimal  statistical model  decomposable or otherwise  for a given data set is a very hard search problem that can be solved exactly only by exhaustive search . thus  statisticians typically resort to computationally-efficient heuristic search strategies that  although suboptimal  often perform well in practice. the statistical literature offers two broad greedy-search paradigms that form the basis of these heuristics  namely forward selection and backward elimination . briefly  forward selection starts out with a very simple model  e.g.  full independence  and incrementally adds more complexity by including in the model  at each step  the variable interaction s  that result in the highest increase in model accuracy. backward elimination  on the other hand  begins with the most complex  saturated interaction model  corresponding to a complete markov graph  and iteratively reduces its complexity by removing  at each step  the variable interaction s  resulting in the smallest decrease in model accuracy.
　when restricting the search space of the model-selection process to decomposable models  as we do in this work   special care needs to be taken to ensure that only such models are considered by the search strategy. backward-elimination strategies for decomposable models are well established in the statistical literature. unfortunately  such backward-elimination schemes have the disadvantage that they typically require evaluating a large number of intermediate models  especially when the number of attributes  i.e.  the dimensionality of the data  involved is high . backwardelimination strategies are  in fact  particularly bad for building db histograms since  as already mentioned above  histogram synopses are only effective for relatively low dimensionalities  implying that most of the interaction edges in the complete model graph may need to be checked and removed.
　based on the above discussion  we have chosen to employ a forward-selection strategy for our db-histogram construction algorithms. unfortunately  efficient forward selection schemes for decomposable models are not as well understood or researched in the statistical literature  1  1 . in our work  we propose two distinct forward-selection algorithms for building decomposable models for db histograms. our first algorithm is based on a  naive  strategy that operates by introducing arbitrary edges  based on their benefit  and running repeated chordality tests to ensure the decomposability of the model. our second  more efficient algorithm is based on a novel edge-selection strategy that guarantees that only edges that retain the decomposability property are introduced to the current model in  essentially  constant time-per-edge. note that this forward-selection scheme is a novel contribution of our work that is very likely to be of interest to the statistical-modeling community . due to space constraints and to keep the discussion more focused on the histograming issues  the detailed development of our model-selection algorithms for db histograms can be found in the full version of this paper . in the full paper  we also discuss certain important properties of decomposable interaction models and describe how our algorithms exploit them to optimize the computational effort  i.e.  number of entropy calculations  involved in the forward-selection process .
1	building the clique-histogram collection
　the model-selection process provides us with a concise decomposable interaction model that accurately captures the correlations in the data. the next step in building the db-histogram synopsis is the construction of the clique-histogram collection . as their name reveals  our clique histograms are built on the marginal frequency distributions defined by the cliques/generators of   which  by the model-selection process  are guaranteed to capture the most significant correlation patterns in the data. histogram synopses for the clique marginals can then be used  in conjunction with the decomposable model  to effectively approximate the overall joint data distribution as well as any other marginal  by taking advantage of the closed-form model estimates and the junction tree representation of the model  section 1 .
　two key issues arise in the construction of the clique histograms. first  we need to decide on the type of  multidimensional  histograms that will be used to capture the clique marginals. second  given that our end goal is to maximize some overall accuracy measure in the db-histogram approximation within a specified space budget of   we have to devise methods for intelligently allocating the storage units among the different clique histograms being built. we elaborate further on these two issues in what follows.
clique-histogramtypes. the main goal of our work is to demonstrate the importance and viability of statistical interaction modeling techniques for building high-dimensionaldata synopses and not to introduce a new family of histogram structures and construction algorithms. thus  our db-histogram construction algorithms use variants of known histograming techniques to capture the clique marginal distributions for a given decomposable model .
 mhisthistograms are a straightforward adaptation of the stateof-the-art mhist-1 technique proposed by poosala and ioannidis . abstractly  the key idea behind mhist-1 is to produce a hierarchical partitioning of the data space by recursively partitioning the bucket that is in most need of partitioning until the space budget  i.e.  number of available buckets  is exhausted.
　grid histograms are based on a simple generalization of rectangular array partitionings  e.g.    to higher dimensionalities. we use a simple greedy algorithm for building grid histograms that  at each step  partitions the entire data distribution along the dimension that is in most need of partitioning while making sure that the allotted number of buckets is not exceeded.
for both histogram types  the  need of partitioning  is determined based on the histogrampartitioningconstraint  e.g.  v-optimal or maxdiff . as an example  a maxdiff constraint means that the construction algorithm always splits  the bucket or the data distribution  along the dimension with the largest difference in frequency between two adjacent values by placing a bucket boundary between these values .
space allocation. given the limited storage budget available for our db-histogram synopsis  it is crucial to allocate space to the distinct clique histograms in in an intelligent manner. typically  the end goal is to minimize an overall error of the histogram approximation  such as the normalized mean square error or the total variance over all histogram buckets . the problem is somewhat complicated by the fact that the model cliques can be of different arities and  as a consequence  the space required for storing a bucket can vary among the different histograms. more formally  let denote the different cliques in and let  
denote the number of buckets allotted to the histogram for clique and the space requirements of each such bucket  respectively. also  given an algorithm for building the clique histogram  let err denote the overall error of that histogram in approximating the marginal distribution over when exactly buckets are used. our space-allocation problem for the clique-histogram collection can now be stated as follows:
	minimize	err	 	subject to
this is essentially a discrete resource allocation problem with a separable objective function   which can be solved optimally in pseudo-polynomial time using dynamic programming. more specifically  let denote the minimum achievable total error for the first clique histograms when at most units of storage are used; that is 
err
then  obviously  gives the optimal objective value of our clique-histogram space-allocation problem. the computation of the optimalspace allotmentis done based on the following dynamicprogramming formulation:
	err	
with the boundary conditions	err		for all
　　　　. the running-timecomplexity of our dynamic-programming solution is   assuming the error-function values err for individual histograms have been precomputed. note that for grid histograms each split introduced by the construction algorithm can result in a multitude of new buckets; thus  the err functions for grid will have a  piecewise constant  form with the error dropping only when the extra buckets are sufficient for a new split. mhist histograms  on the other hand  have smoother error curves  since each split introduces only one new bucket. we defer presentation of the full details of our dynamic-programming algorithm for space allocation to the full paper.
　a simpler and more efficient heuristic algorithm for our spaceallocation problem  termed incrementalgains  is depicted in figure 1. the key idea in incrementalgains is to incrementally distribute the space budget among the clique histograms based on marginal gains . abstractly  the incrementalgains algorithm works in parallel with the histogram construction. at each step  the next split for each clique histogram  as dictated by the construction algorithm  is evaluated in terms of  a  the improvement that the extra split brings in terms of the overall error in the histogram approximation  and  b  the increase in the amount of space required for storing the new bucket s  introduced by the split. among all candidate splits  incrementalgains selects the split that maximizes the decrease in error per unit of required bucket space and  of course  does not violate our storage-space constraint. the running-time complexity of the incrementalgains algorithm is only
　　　　. furthermore  it is a well-known result  e.g.    that incremental allocation based on marginal gains is  in fact  optimal if the components of the separable objective function  i.e.  the err 's  are convex; that is  when the histogram error functions follow a law of diminishing returns with respect to allotted space. such an assumption may often be satisfied during histogram construction and  therefore  we expect that incrementalgains will typically perform well in practice.

procedure incrementalgains 	  err	 	   	 
input: attribute sets	corresponding to model cliques; total storage space budget	; error measure err	and per bucket storage requirement	for the	clique histogram.
output: feasible bucket allocation that minimizes the overall histogram approximation error.
begin
1. /* all histograms start as one bucket containing the average frequency */
1.   used :=
1. foundone := true
1. while  foundone = true  do
1. foundone := false
1. let be the extra buckets required for adding a new split  as dictated by the construction algorithm  to the histogram for clique  
1. let	err	err	err	 
1. sort all candidate splits in the order	such that
           err	err for each : 
1.
1. while  used	  do
1. if  	  then
1. add a new split to the histogram for clique
1.   used := used -
1. foundone := true
1. end
1. end end
figure 1: the incrementalgains space-allocation algorithm.

1	using dependency-based histograms
　efficiently estimating the selectivity of a range-query predicate over  a subset of  the data attributes using db histograms requires novel usage techniques that effectively utilize the model structure in conjunction with the clique histograms. in this section  we propose algorithms for estimating query-predicate selectivities using db histograms. let denote a db histogram on attributes and let be a range-selection predicate defined  without loss of generality  over the attributes
	  where	; that is 	specifies no ranges over
             . also  let	denotethe cliques/generators in the constructed decomposable model	and let   where	is the histogram built for clique
 	 .
　estimating the selectivity of is accomplished by using to compute the  approximate  marginal frequency distribution over the specified attributes . we demonstrate how this computation can be carried out efficiently by exploiting the junction tree representationof our decomposablemodel	 section1 .
given the  product form  of the selectivity estimates dictated by
　  we also need effective algorithms for computing the product and projection  i.e.   marginalization   of our histogram-based approximations to the model's clique marginals. we propose algorithms for these histogram operations in the second part of this section.  our db-histogram implementation also incorporates several practical optimizations; a discussion can be found in the full paper.  1.1 computing arbitrary marginal distributions
　as we already pointed out in section 1  one of the main advantages of using a decomposable interaction model in is that it allows for direct  closed-form estimates for the joint data distribution that we want to approximate. these estimates have a product form that can be directly read off the junction tree representation of
    1  1   denoted by . recall from section 1 that is a tree over the cliques of that satisfies the clique-intersection property; that is  for each pair and of cliques  the set is contained in every clique on the path connecting and in
     . another important property of the junction tree structure is the separation property:  for each edge	in	the set separates	and	in	.  based on this property and the interpretation of separation in terms of conditional independence in decomposable models  given the edge
　　　　in we can estimate the joint frequency distribution of using the formula . similarly  the form of the estimates for the overall joint frequency distribution can be read off based on the following formula :
		 1 
　the above result directly provides us with a naive method for computing the marginal frequency distribution over and  therefore  estimating the selectivity of : simply build the junction tree   then use equation  1  to reconstruct the full joint frequency distribution based on the clique histograms and  finally  project onto the attributes of interest .  note that  given a chordal graph   a junction tree representation of can be computed very quickly in time . 
however  computing the full frequency distribution is obviously an overkill since  in most practical scenarios  the range predicate specifies selections over only a small subset of attributes  i.e. 
        . we now propose a more efficient algorithm that makes more effective use of the junction tree representation to minimize the computation required for obtaining arbitrary marginals.
　our algorithm views the junction tree as a rooted tree with an arbitrarily-chosen root node  clique . also  with each node in we associate the union of the cliques corresponding to all the descendents of the node in   including itself. this union  denoted by cover     can obviously be computed in a single bottom-up traversal of the tree. the complete outline of our recursive marginal-computation algorithm  termed computemarginal  is shown in figure 1. the input arguments of computemarginal are  1  a node of the  rooted  junction tree   and  1  a set of attributes for which a histogram of their joint frequency distribution is required from the  sub tree rooted at . our description uses the functions project and product; briefly  project      returns the projection of histogram over onto the subset of attributes   whereas product 
　　　  returns the histogram over that results from multiplying and using the separation formula for obtaining frequencies  i.e.  .  the

 

relevant algorithms are described in section 1.1.  briefly  computemarginal first checks whether the clique is a superset of the specified attributes since  in that case  we can get the histogram for by simply projecting out the unnecessary attributes from  step 1 . if cannot compute the required frequency distribution by itself then we keep the portion of whose distribution can be determined from     and recursively use the children of to compute the relevant marginals for the remaining attributes    . in the simpler case  steps 1   there exists a single child of that covers all remaining attributes. then   we can then simply pass all remaining attributes to that child but  of course  we also have to ensure that we augment this set with all attributes in the separator   so that the resulting frequency distributions can be properly multiplied out based on the tree's separation property  steps 1 . in the more complex case  steps 1   none of the children can compute the distribution of by itself  so we must break up into multiple parts such that each part can be handled by a single child node. the resulting marginals are again multiplied out based on the separation property to obtain the overall frequency estimates for  steps 1 . the initial invocation of algorithm
computemarginal sets	equal to the root node of	and
.

procedure computemarginal 	 	 
input: node/clique in the  rooted  junction tree ; collection of attributes whose  approximate  joint frequency distribution is required from the  sub tree rooted at node . output: histogram giving the approximate frequency distribution of .
begin
1. if	then return project 	 	 
1. else
1. let int :=	and diff :=
1. if   diff cover 	  for some child	of	  then
1. if   int	  then return computemarginal 	 	 
1. else
1. let	:=
1. := computemarginal 	  diff	 
1. return project  product 	   	 
1. end
1. else
1. let
1. for each child	of	such that	diff	do
1. let	:=
1. := computemarginal 	 	diff 	 
1. := product      1. end
1. return project 	 	 
1. end
1. end end
figure 1: computingmarginal histogramsusing junction trees.

　algorithm computemarginal is much more efficient than the naive technique described above and  in fact  it can be shown that computemarginal is optimal in terms of the total number of histogram multiplications and projections required. on the other hand  computemarginal does not address the issue of finding the optimal order for multiplying the relevant histograms for obtaining a given marginal. this problem is similar in spirit to the well-known matrix-chain multiplication problem  and  in fact  reduces to that problem in the case of a simple path model graph and a twovariable range predicate . optimizing the multiplication operations for general model graphs and range predicates requires extending these earlier results to much more general tensor products  which is  to the best of our knowledge  an open problem. we intend

to address this issue as part of our future work on db histograms.
1.1	multiplying and projecting clique histograms
　we now discuss the implementationof the basicclique-histogram operations  i.e.  project   and product    that are used in our selectivity-estimation procedure. we focus on projection and multiplication algorithms for mhist histograms; the algorithms for grid histograms are rather straightforward  this was actually the main reason we included them in our study  and they can be found in the full paper.
　we propose a more space-efficient representation for multi-dimensional mhist histograms than the one described in the original paper of poosala and ioannidis . for an -dimensional mhist bucket  their representation requires storing a frequency and the high and low value boundaries in each of the dimensions  resulting in a total of numeric values per bucket. our key observation here is that an mhist histogram is basically a hierarchical binary partitioning of the data space. thus  instead of storing each mhist bucket explicitly  we propose storing the histogram as a tree structure  termed a split tree  that captures the splits performed by the mhist-construction algorithm. each internal node of the split tree just needs to store the split value and the dimension along which the split was performed  whereas each leaf node just needs to store the frequency for the corresponding bucket. given a -bucket -dimensional mhist histogram  it is easy to verify that our split-tree representation requires storing only numbers  which is clearly a significant improvement over the numbers required by the naive representation.
　we propose multiplication and projection algorithms for mhist histograms that work solely on our space-efficient  split-tree representation for both the input and output histograms of the operator. both algorithms make use of a simple procedure termed restrictnode      that takes as input a node in a split tree for a collection of attributes and a restriction on the ranges of  a subset of  these attributes. the result of restrictnode is a split tree derived from the subtree rooted at that only contains the split and leaf nodes that pertain to the input range restriction . the restrictnode operator requires  at most  one traversal of the subtree rooted at . the pseudo-code for projecting an mhist histogram onto a subset of its attributes is shown in figure 1. most of the work for project    is done in a recursive subroutine  termed gensplits  that essentially generates the split-tree structure for the projected histogram. the key requirement for gensplits is to ensure that  in the end  all splits along dimensions in for any of the hierarchically-generated buckets in are reflected in the split tree for the projected histogram. for example  if one bucket of is split on and another on both and need to appear as split points when the histogram is projected onto . this is handled by using the restrictnode procedure to  transfer  all the relevant splits into the currently explored subtree  steps 1 .  note that and are interchangeable in steps 1.  finally  project computes the frequencies for the leaf nodes  i.e.  buckets  in the split tree for the projected histogram by summing the frequencies over all the contributing leaves of ; of course  the summed frequencies have to be appropriately scaled by the relative volume of the bucket along  intra-bucket uniformity assumption   steps 1 .
　the pseudo-code of our algorithm for multiplying two mhist clique histograms and to obtain an mhist histogram on the joint distribution of is depicted in figure 1. the key intuition underlying our algorithm is as follows. let and consider two buckets and
	. if	and	overlap along any of the dimensions in
procedure project 	 	 
input: mhist histogram  split tree 	on	; attributes	on which we want to project	. output:	mhist histogram on	.
begin
1.	:= gensplits root 	   	 	/* split-tree for the projection */
1.	for every leaf	of	do1.	frequency    :=	frequency  where the summation is	over all leaves	of	that containalong the dimensions in	and	volume  along	 /volume 
1.	end
end subroutine gensplits 	 	 along	 input: split-tree node	; subset	of the split-tree attributes on which we want to  project  the subtree rooted at	.
output:	root for a new split-tree structure resulting from the projection of the	-rooted subtree on	.
begin
1. let	and	be the splitting attribute and split value at node
1. let	and	denote the left and right child of
1. := gensplits 	 	 
1. := gensplits 	 	 
1. if	then
1. create new internal node	with children	and	  and split
1. else
1. set	:=
1. for every leaf	of	do
1. let	denote the attribute ranges occupied by
1. replace	with restrictnode 	 	 
1. end
1. end end
figure 1: mhist projection algorithm.

     thenthey result in a new bucket in the product histogramwhose boundaries along the dimensions     are exactly those of  resp.     whereas its boundaries along dimensions in are defined by the intersection of and in those dimensions. our product algorithm employs this observation to generate the split-tree structure for the product histogramby first initializing that structure with one of the input split trees  say  step 1   and then  for each leaf node of   using the restrictnodeprocedure on the other input tree to generate the tree structure for the product buckets  steps 1 . finally  the frequencies for the leaf nodes in the product split tree are computed using the separation formula on the frequencies of the  enclosing  buckets of  
　　　  and project       which  of course  have to be properly scaled by the relative volume of the bucket intersection along  intra-bucket uniformity assumption   steps 1- 1 .
1.	experimental study
　in this section  we present the results of an extensive empirical study in which we compare the quality of approximate answers to selectivity estimation queries obtained using db histograms with various prevalent selectivity estimation algorithms. the major findings of our study can be summarized as follows.
　decomposable models are effective. for all data sets  decomposable models with small complexity  that is  few edges  yield good approximations to the original data set. thus  decomposable models provide us with an effective mechanism for accurately capturing the data distribution of multi-dimensional data sets.
　better approximate answer quality. in general  the quality of the approximate answers returned by db histograms is superior
procedure product 	 	 
input: mhist histograms  split trees 	and	on attribute sets and	  respectively.
output: mhist histogram	on	.
begin
1. set	:=	/* initialize with split tree for an input */
1. for every leaf	of	do
1. let	be the ranges of attributes in	occupied by the bucket at
1. replace	with restrictnode 	 	 
1. end
1. := project 	 	   where
1. for every leaf	of	do
1. let     be the leaves of     and  respectively  that contain along the respective dimensions
1. let	:= volume  along	 /volume 	along	   with and	defined similarly frequency	frequency
1. frequency    := 
frequency
1. end end
figure 1: mhist multiplication algorithm.

to that of competing histograming methods. further  for a number of selectivity estimation queries  the approximation error with db histograms is as much as 1 times smaller than the error for the best competing algorithm.
　db histograms are storage efficient. db histograms provide fairly accurate  less than 1% error  answers to range queries on most real-life data sets  one of which has a dimensionality as high as 1   while requiring less than 1% of the storage space consumed by the original data set.
　thus  our experimental results validate the thesis of this paper that db histogram synopses provide a viable and effective means for approximating the joint distributions of high-dimensional data sets. due to space constraints  we do not present the results of our experiments on construction times or query answering times for db histograms; they can be found in the full paper . note  however  that histogram construction can be speeded up by using random samples of the data to build the interaction models  while the histogram usage techniques described in section 1 can be used to process queries efficiently.
1	experimental testbed and methodology
selectivity estimation techniques. we consider three different selectivity estimation techniques in our study.
　　　　　. we build a multi-dimensional histogram on all the attributes in the base relation using the mhist-1 technique proposed by poosala and ioannidis . using our space-efficient tree representation for these histograms  the storage space required is approximately bytes  where is the number of histogram buckets. here  bytes are required to store the counts for the leaves of the tree  and storing the split dimension and value for the internal nodes requires and bytes  respectively1.
        independence assumption . in this approach  a separate one-dimensional histogram for each attribute of the base table is built. for the purpose of answering queries  it is assumed that all data attributes are mutually independent  and the joint distribution can be obtained as the product of the individual one-dimensional marginals. the storage space required for buckets of histograms is bytes   bytes to specify the separator for each bucket and bytes for the bucket frequency . the buckets for are

 here we assume that  for an internal node in the split tree  storing the split dimension requires 1 byte  regardless of the dimensionality of the data set.
constructed using a procedure similar to incrementalgains  see figure 1  using the set of all attributes as the set of cliques input to the procedure. we used the total variance across all the histogram buckets as the error function.
　db histograms. we construct db histograms using the algorithms described in sections 1 and 1. we consider the following two heuristics for selecting the next best interaction edge to add to model in the forward-selection process.
　　: the edge with the highest statistical significance for the improvement in divergence is chosen.
　　: the edge for whom the ratio of the improvement in divergence and the increase in the total model state space  that is  the sum of the product of the domain sizes for all the attributes in each clique  is maximum  is selected.  the justification for the heuristic can be found in . 
we use histograms for approximating the frequency distributions of the cliques in the decomposable model of the db histogram. thus  the storage space required for buckets is bytes  similar to . further  procedure incrementalgains is used to allocate space among the various clique histograms  with the total variance across all buckets as the error function . there is an additional overhead of storing the junction tree associated with the model  but that is negligible compared to the size of the histogram synopsis.
　unless stated otherwise  when inferring the model for a data set  we fix   the maximum clique size  to be 1  since we found that including 1-dimensional clique histograms decreases the accuracy of db histograms considerably. as a consequence  for a relation containing attributes  is a tree and the db histogram for the table contains clique histograms. we also set the threshold for the statistical significance level    to 1%. however  we found that due to the small value for   the statistical significance rarely eliminated edges during model construction.
　one important selectivity estimation technique that we do not include in our empirical study is random sampling. this technique involves keeping a random sample of the data in memory  against which the queries are run and the answer is appropriately scaled to estimate the result of executing the query on the complete database. in our experiments  we observed that  because of the small storage space allocated to the synopses  hardly any tuples in the sample satisfied the query  and the returned answer was almost always 1. therefore  we do not consider this technique any further.
real-life data sets. in our experiments  we used real-life data sets obtained from the us census bureau  www.census.gov/  as well as a data set on california housing from a 1 survey  lib.stat.cmu.edu/ . due to space constraints  our results with the california housing data set can be found in .
　census data set. we use the current population survey  cps  data source and within it  the person data files of the march questionnaire supplement. we use two different attribute subsets.
1. data set 1. this data set consists of the following attributes  the size of each attribute domain is included in parentheses : race  1   native country of the sample person  1   native country of mother  1   native country of father  1   citizenship  1  and age  1 . we expect the first five attributes to be highly correlated  while the last attribute is relatively independent of the rest of the attributes. this data set contains a total of 1 tuples with 1 distinct tuples  and has a total size of approximately 1kb.
1. data set 1. to demonstrate that our techniques can handle data sets with high dimensionality well  we use a 1 attribute projection of the census data set. in addition to the attributes described above  in this data set  we include the following attributes: industry code 1   no. of hours usually worked at the main job  1   educational attainment  1   census state code  1   county code  1 . data set 1 contains a total of 1 tuples with 1 distinct tuples  and has a total size of approximately 1mb.
query workload. we compare the selectivity estimation techniques on a randomly generated range-selectivity query workload. each query on a single relation specifies ranges for a subset of attributes in the relation and leaves the ranges for the remainder of the attributes unspecified. we refer to a query with specified attribute ranges as a -d query. given a   the -d query workload consists of 1 randomly generated -d queries; for each query  the attributes and the range extents for those attributes are randomly chosen. in our final reported results  we do not consider queries that cover less than 1 tuples in the base relation.
answer-quality metrics. the answers obtained using the three histograming techniques described above are compared with the correct answer computed using the original data set. we use one of two metrics  described below  to gauge the quality of an approximate answer to a query. in the following  the correct answer to a query is and the approximate answer computed using a synopsis is	.
1. absolute relative error  defined as	.
1. multiplicative error  defined as	.
while the relative error is fairly standard  the main reason to use the less common multiplative error metric is that  for higher dimensionality queries  tends to give very small answers for most queries. but even for an answer equal to 1  the absolute relative error is still at most 1. as we will see later  because of this  appears to perform better for high dimensionality queries. the multiplicative error metric corrects this shortcoming of the relative error metric by penalizing very small answers as well.
　in the graphs depicting the results of our experiments  the final reported error for each query workload is obtained by taking the average of the errors for the 1 random queries in the workload.
1	experimental results
1.1	census data set 1
how good are decomposablemodels  our first experimentdemonstrates the effectiveness of decomposable models at approximating the original data set with respect to our range-query workloads. in this experiment  edges are added to the model in decreasing order of the statistical significance of the improvement in approximation due to the edge  disregarding the parameters and  . further  for each clique in the model  we store the projection of the entire data set on clique attributes  and use these projections to answer queries. each projection  in effect  corresponds to a clique histogram with an unlimited number of buckets  and thus captures the data distribution on clique attributes completely and accurately. consequently  errors in the final query answer are solely due to the model and approximation errors due to clique histograms are factored out from the final result.
　figure 1 depicts the performance of the models selected by and for query workloads with different dimensionalities. as we can see  the average errors for both these edge selection heuristics drop rapidly as the complexity of the model increases. for instance  for the decomposable model containing only 1 edges and selected using   the error is less 1%. thus  it follows that decomposable models are good at accurately capturing correlations in the underlying data. further  observe that the error rates for the model chosen using drop much more rapidly compared to
　　. this is not entirely surprising since selects edges for the model assuming perfect clique histograms  which is true in this experiment. however  as we will see later  is perhaps more suitable in practice  since for a majority of realistic environments  due to storage space limitations  each clique histogram can be expected to only coarsely approximate the original distribution.

	 i  db1	 ii  db1
figure 1: effectiveness of decomposable models.

db histogram accuracy. in figure 1  we plot the relative and
multiplicative errors for the various selectivity estimationtechniques on -d query workloads    . the storage space for histograms allocated to each method was fixed at 1kb. from the graphs  it follows that outperforms the other techniques for all -d query workloads  except  ; further  for certain query workloads      the error for is half the error for competing methods. in general  both the db-histograming techniques and perform quite well since decomposable models are good at separating unrelated attributes  and traditional histogram methods like approximatelow-dimensional data sets quite well. further  unlike   since also takes into account the space requirements of clique histograms when selecting edges for the model and the amount of storage is limited  it results in the smallest values for error.
　note that the poor performance of can be attributed to the well-known fact that traditional histograming approaches suffer from the  curse of dimensionality  when trying to approximate distributions for high-dimensional data sets. also  since builds only one-dimensional histograms  it should come as no surprise that it results in the lowest errors for 1-d queries. however  its performance with respect to relative error is somewhat misleading since typically returns small answers for most queries. the multiplicative error is more indicative of 's overall performance  which suffers because of the invalid assumption made by it that attributes are mutually independent.
effect of storage space on histogram accuracy. figure 1 depicts errors for a 1-d query workload as storage space allocated to the selectivity estimation techniques is increased. the behavior for other query workloads show a similar trend.
　from figure 1  we can see that errors for and decrease as the amount of storage space is increased. this is because the increased storage helps each low-dimensional clique histogram to approximate the data more accurately  and thus improves the overall accuracy of the db histogram. however  the extra space has little effect on and due to inherent problems related to approximating high-dimensional data sets and the attribute independence assumption.
tion model that accurately captures the significant attribute correlations and independencies in the data  and  1  a collection of histograms on low-dimensional marginals that  based on the model  can be used to derive accurate approximations of the overall joint data distribution. our experimentation with different real-life data sets has validated our approach  demonstrating that db histograms provide significantly better approximations than conventional histograming techniques. an important feature of our general methodology is that it can be used to enhance the performance of several other data-reduction techniques in medium- to high-dimensionality spaces: statistical interaction models can help identify the signifi-	1-d	1-d	1-d	1-d	1-d	1-d	1-d	1-d
 i  average relative error  ii  average multiplicative error figure 1: results for the 1-d census data set.

figure 1: effect of storage space.

1.1	census data set 1
　we present results from our experiments on the census data set 1 in figure 1. the total storage space allocated to the synopses for this data set was 1kb  approximately 1% of the original data set . the results are similar to those for the census data set 1  with doing much better than the other two techniques on the combination of our two error metrics.
figure 1: results for the 1-d census data set.

1.	conclusions
　in this paper  we have proposed dependency-based  db  histograms  a novel approach to histogram-based synopses that effectively overcomes the  curse of dimensionality  by employing the solid foundation of statistical interaction models to explicitly identify and exploit the dependence patterns in the data. the basic idea is to break the synopsis into  1  a decomposable interaccant attribute correlation patterns in the data and  therefore  the interesting lower-dimensionalsubspaces that should be approximated independently in a synopsis.
acknowledgments: we would like to thank jose＞ pinheiro and vishy poosala for several helpful discussions related to this work.
