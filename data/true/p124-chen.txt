the class imbalance problem is encountered in a large number of practical applications of machine learning and data mining  for example  information retrieval and filtering  and the detection of credit card fraud. it has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier's suboptimal performance. this is even more true when the imbalanced data are also high dimensional. in such cases  feature selection methods are critical to achieve optimal performance. in this paper  we propose a new feature selection method  feature assessment by sliding thresholds  fast   which is based on the area under a roc curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. fast is compared to two commonly-used feature selection methods  correlation coefficient and relevance in estimating features  relief   for imbalanced data classification. the experimental results obtained on text mining  mass spectrometry  and microarray data sets showed that the proposed method outperformed both relief and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred  the classification performance of the proposed method was significantly improved compared to correlation and relief-based methods.     
categories and subject descriptors 
i.1  pattern recognition : design methodology - feature evaluation and selection.  
general terms 
algorithms. 
keywords 
feature selection  imbalanced data classification  roc. 
1. introduction 
one of the greatest challenges in machine learning and data mining research is the class imbalance problem presented in realworld applications. the class imbalance problem refers to the issues that occur when a dataset is dominated by a class or classes that have significantly more samples that the other classes of the dataset. imbalanced classes are seen in a variety of domains and many have major economic  commercial  and environmental concerns. some examples include text classification  risk management  web categorization  medical diagnosis/monitoring  biological data analysis  credit card fraud detection  oil spill identification from satellite images. 
while the majority of learning methods are designed for wellbalanced training data  data imbalance presents a unique challenging problem to classifier design when the 
misclassification costs for the two classes are different  i.e.  costsensitive classification  and accordingly  the overall classification rate is not appropriate to evaluate the performance. the class imbalance problem could hinder the performance of standard machine learning methods. for example  it is highly possible to achieve the high classification accuracy by simply classifying all samples as the class with majority samples. the practical applications of cost-sensitive classification arise frequently  for example  in medical diagnosis   in agricultural product inspection   in industrial production processes   and in automatic target detection . analyzing the imbalanced data thus requires new methods than those used in the past. 
the majority of current research in the class-imbalance problem can be grouped into two categories: sampling techniques and algorithmic methods  as discussed in two workshops at the aaai conference  and the icml conference   and later in the sixth issue of sigkdd exploration  see  for example  a review by weiss  . the sampling methods involve leveling the class samples so that they are no longer imbalanced. typically  this is 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd'1  august 1  1  las vegas  nevada  usa. 
copyright 1 acm 1-1-1/1...$1. 
 
done by under-sampling the larger class  1  or by over-sampling the smaller one  1  or by combination of these techniques . algorithmic methods include adjusting the costs associated with misclassification so as to improve performance  1   shifting the bias of a classifier to favor the rare class  1   creating adaboost-like boosting schemes  1   and learning from one class . 
the class imbalance problem is even more severe when the dimensionality is high. for example  in microarray-based cancer classification  the number of features is typically tens of thousands ; in text classification  the number of features in a bag of words is often more than an order of magnitude compared to the number of training documents . both sampling techniques and algorithmic methods may not work well for high dimensional class imbalance problems. indeed  van der putten and van someren analyzed the coil challenge 1 datasets and concluded that to overcome overfitting problems  feature selection is even more important than classification algorithms . a similar observation was made by forman in highly imbalanced data classification problems . as pointed out by forman   no degree of clever induction can make up for a lack of predictive signal in the input space  . this holds even for the svm which is engineered to work with hyper-dimensional datasets.  forman  found that the performance of the svm could be improved by the judicious use of feature selection metrics. it is thus critical to develop effective feature selection methods for imbalanced data classification  especially if the data are also high dimensional.   
while feature selection has been extensively studied  1   its importance to class imbalance problems in particular was recently realized and attracted increasing attention from machine learning and data mining community. mladenic and grobelnik examined the performance of different feature selection metrics in classifying text mining data from the yahoo hierarchy . after applying one of nine different filters  they tested the classification power of the selected features using na ve bayes classifiers. their results showed that the best metrics choose common features and consider the domain and learning machine's inherent characteristics. forman found improved results with the use of multiple different metrics  but the best performing results were those selected by metrics that focused primarily on the results of the minority class . zheng  wu  and srihari empirically tested different ratios of features indicating membership in a class versus features indicating lack of membership in a class . this approach resulted in better accuracy compared to using one-sided metrics that solely score features indicating membership in a class and two-sided metrics that simultaneously score features indicating membership and lack of membership.   
one common problem with standard evaluation statistics used in previous studies  like information gain and odds ratios  is that they are dependent on the choice of the true positive  tp   false positive  fp   false negative  fn   and true negative  tn . these parameters are set based on a preset threshold. consider imbalanced data classification with two different feature sets. the first feature set may yield higher tp  but lower tn  than the second feature set. by varying the decision threshold  the second feature set may produce higher tp and lower tn than the first feature set. thus  one single threshold cannot tell us which feature set is better. this is an artifact of using a parametric statistic to evaluate a classifier's predictive power . if we vary the classifier's decision threshold  we can find these statistics for each threshold and see how they vary based on where the threshold is placed.  a receiver operating characteristic  or roc curve  is one such non-parametric measure of a classifier's power that compares the true positive rate with the false positive rate. while the roc curve has been extensively used for evaluating classification performance in class imbalance problems  it has not been directly applied for feature selection. in this paper  we construct a new feature selection metric based on an roc curve generated on optimal simple linear discriminants and select those features with the highest area under the curve as the most relevant. unlike other feature selection metrics which depend on one particular decision boundary  our metric evaluates features in terms of their performance on multiple decision hyperplanes and is more appropriate to class imbalance problems.   
the rest of our paper is organized as follows.  section 1 provides a brief discussion about two commonly-used filter methods: correlation coefficient  cc   and relevance in estimating features  relief . in section 1  we follow with a description of the proposed new method  feature assessment by sliding thresholds  fast . in section 1  we present the results comparing the performance of the linear support vector machines  svm  and 1-nearest neighbor  1-nn  classifiers using features selected by each metric. these results are measured on two microarray  two mass spectrometry  and one text mining datasets.  finally  we give our concluding remarks in section 1. 
1. feature seelction methods 
in this section  we briefly review two commonly-used feature selection methods  cc and relief.  
1 correlation coefficient 
the correlation coefficient is a statistical test that measures the strength and quality of the relationship between two variables.  correlation coefficients can range from -1 to 1. the absolute value of the coefficient gives the strength of the relationship; absolute values closer to 1 indicate a stronger relationship. the sign of the coefficient gives the direction of the relationship: a positive sign indicates then the two variables increase or decrease with each other and a negative sign shows that one variable increases as the other decreases. 
in machine learning problems  the correlation coefficient is used to evaluate how accurately a feature predicts the target independent of the context of other features.  the features are then ranked based on the correlation score . for problems where the covariance cov  xi   y  between a feature   xi   and the target  y  and the variances of the feature  var  xi    and target  var y   are known  the correlation can be directly calculated: 
cov xi y 
                                   r i  =                          1  var xi  var y 
equation 1 can only be used when the true values for the covariance and variances are known. when these values are unknown  an estimate of the correlation can be made using pearson's product-moment correlation coefficient over a sample of the population  xi  y .  this formula only requires finding the mean of each feature and the target to calculate: 
m
                      r i  = 뫉k=1	k i	i	k	            1   x	  x   y   y 
   m 1 m 1 뫉k=1 k i i 뫉k=1 k i
	 x	  x    	 y   y  
where m is the number of data points.  
correlation coefficients can be used for both regressors and classifiers.  when the machine is a regressor  the range of values of the target may be any ratio scale.  when the learning machine is a classifier  we restrict the range of values for the target to 1 .  
we then use the coefficient of determination  or r i 1   to enforce a ranking of the features according to the goodness of linear fit between individual features and the target . 
when using the correlation coefficient as a feature selection metric  we must remember that the correlation only finds linear relationships between a feature and the target.  thus  a feature and the target may be perfectly related in a non-linear manner  but the correlation could be equal to 1.  we may lift this restriction by using simple non-linear preprocessing techniques on the feature before calculating the correlation coefficients to establish a goodness of non-linear relationship fit between a feature and the target . 
another issue with using correlation coefficients comes from how we rank features.  if features are solely ranked on their value  with features having a positive score getting picked first or vice versa  then we risk not choosing the features that have the strongest relationship with the target.  conversely  if features are chosen based on their absolute value  zheng  wu  and srihari argue that we may not select a ratio of positive to negative features that gives the best results based on the imbalance in the data .  finding this optimal ratio takes empirical testing  but it can result in extremely strong results. 
1 relief 
relief is a feature selection metric based on the nearest neighbor rule designed by kira and rendell . it evaluates a feature based on how well its values differentiate themselves from nearby points.  when relief selects any specific instance  it searches for two nearest neighbors: one from the same class  the nearest hit   and one from the other class  the nearest miss .  we then calculate the relevance of each attribute a by the rule: 
w a  = p different value of a | nearest miss   
             - p different value of a | nearest hit                                1  
this is justified by the thinking that instances of different classes should have vastly different values  while instances of the same class should have very similar values. because the true probabilities cannot be calculated  we must estimate the difference in equation 1. this is done by calculating the distance between random instances and their nearest hits and misses. for discrete variables  the distance is 1 if the same and 1 if different; for continuous variables  we use the standard euclidean distance.  we may select any number of instances up to the number in the set  and more selections indicate a better approximation .  algorithm 1 details the pseudo-code for implementing relief. 
algorithm 1  relief : 
set all w a  = 1 
for i =1 to m 
select instance r randomly 
find nearest hit h and nearest miss m 
for a=1 to number of features 
w a  = w a  - dist a  r  h /m 
모모모모w a  = w a  + dist a  r  m /m  
the original version of relief suffered from several problems.  
first  this method searches only for one nearest hit and one nearest miss. noisy data could make this approximation inaccurate. second  if there are instances which have missing values for features  the algorithm will crash because it cannot calculate the distance between those instances. kononenko created multiple extensions of relief to address these issues .  relief-a allowed the algorithm to check multiple nearest hits and misses.  relief-b  c  and d gave the method different ways to address missing values. finally  relief-e and f found a nearest miss from each different class instead of just one and used this to better estimate the separability of an instance from all other classes. these extensions added to relief's adaptability to different types of problems. 
1. method description: fast 
in this section  we propose to assess features based on the area under a roc curve  which is determined by training a simple linear classifier on each feature and sliding the decision boundary for optimal classification. the new metric is called fast  feature assessment by sliding thresholds .  
most single feature classifiers set the decision boundary at the mid-point between the mean of the two classes . this may not be the best choice for the decision boundary. by sliding the decision boundary  we can increase the number of true positives we find at the expense of classifying more false positives.  alternately  we could slide the threshold to decrease the number of true positives found in order to avoid misclassifying negatives.  thus  no single choice for the decision boundary may be ideal for quantifying the separation between two classes. 
we can avoid this problem by classifying the samples on multiple thresholds and gathering statistics about the performance at each boundary.  if we calculate the true positive rate and false positive rate at each threshold  we can build an roc curve and calculate the area under the curve.  because the area under the roc curve is a strong predictor of performance  especially for imbalanced data classification problems  we can use this score as our feature ranking: we choose those features with the highest areas under the curve because they have the best predictive power for the dataset. 
by using a roc curve as the means to rank features  we have introduced another problem: deciding where to place the thresholds. if there are a large number of samples clustered together in one region  we would like to place more thresholds between these points to find how separated the two classes are in this cluster. likewise  if there is a region where samples are sparse and spread out  we want to avoid placing multiple thresholds between these points so as to avoid placing redundant thresholds between two points. one possible solution is to use a histogram to determine where to place the thresholds. a histogram fixes the bin width and varies the number of points in each bin. this method does not accomplish the goals detailed above.  it may be the case that a particular histogram has multiple neighboring bins that have very few points.  we would prefer that these bins be joined together so that the points would be placed into the same bin.  likewise  a histogram may also have a bin that has a significant proportion of the points.  we would rather have this bin be split into multiple different bins so that we could better differentiate inside this cluster of points. 
we use a modified histogram  or an even-bin distribution  to correct both of these problems.  instead of fixing the bin width and varying the number of points in each bin  we fix the number of points to fall in each bin and vary the bin width.  this even-bin distribution accomplishes both of the above goals: areas in the feature space that have fewer samples will be covered by wider bins  and areas that have many samples will be covered by narrower bins.  we then take the mean of each sample in each bin as our threshold and classify each sample according to this threshold. algorithm 1 details the pseudo-codes for implementing fast. 
algorithm 1  fast : 
k: number of bins 
n: number of samples in dataset 
m: number of features in dataset 
split = 1 to n with a step size n/k 
for i = 1 to m 
 	x is a vector of samples' values for feature i 
 	sort x 
for j = 1 to k 
bottom = round split j  +1 top = round split j+1   mu = mean x bottom to top   classify x using mu as threshold tpr i  j  = tp/# positive fpr i  j  = fp/# negative 
calculate area under roc by tpr  fpr 
one potential issue with this implementation is how it compares to the standard roc algorithm of using each possible threshold as the standard is simpler but requires more computations.  we conducted a pilot study using the cns dataset to measure the difference between the fast algorithm and this standard.  our findings showed that with a parameter of k=1  1% of the fast scores were within plus-minus 1 of the exact auc score  and 1% were within plus-minus 1.  additionally  the fast algorithm was nearly ten times as fast.  thus  we concluded that the approximation scores were sufficient. 
note that the fast method is a two-sided metric.  the scores generated by the fast method may range between 1 and 1.  if a feature is irrelevant to classification  its score will be close to .1.  if a feature is highly indicative of membership in the positive or negative class or both  it will have a score closer to 1.  thus  this method has the potential to select both positive and negative features for use in classification. 
1. experimental results 
1 data sets 
we tested the effectiveness of correlation coefficient  relief  and fast features on five different data sets.  two of the data sets are microarray sets  two are mass spectrometry sets  and one is a bag-of-words set. each of the microarray and mass spectrometry data sets has a small number of samples  a large number of features  and a significant imbalance between the two classes. the bag-of-words data set also has a small number of samples with a large number of features  but we artificially controlled the class skew to show differences in performance on highly imbalanced classes versus balanced classes. the microarray sets were not preprocessed. the mass spectrometry sets were minimally preprocessed by subtracting the baseline  reducing the amount of noise  trimming the range of inspected mass/charge ratios  and normalizing. the bag-of-words set was constructed using rainbow  to extract the word counts from text documents.  these data sets are summarized in table 1. 
because the largest data set has 1 samples  we used 1-fold cross-validation to evaluate the trained models.  each fold had a class ratio equal to the ratio of the full set.  the results for each fold are combined with each other to obtain test results for the entire data set.  to stabilize the results  we repeated the crossvalidation 1 times and averaged over each trial. 
table 1. data set descriptions  
cns central nervous system embryonal tumor data . this data set contains 1 samples: 1 have medulloblastomas and 1 have other types of tumors or no cancer.  there are 1 genes in this data set. lymph lymphoma data . this data set contains 1 samples: 1 are diffuse large b-cell lymphomas  and 1 are folicular lymphomas. there are 1 genes in this data set. ovary ovarian cancer data .  this data set contains 1 samples: 1 are benign tumors  and 1 are malignant tumors.  there are 1 mass/charge ratios in this data set. prost prostate cancer data . this data set contains 1 samples: 1 have no evidence of cancer  and 1 have prostate cancer. there are 1 mass/charge ratios in this data set. nips nips bag-of-words data . this data set contains 1 documents: 1 cover neurobiology topics  and 1 cover various applications topics.  there are 1 words in this data set.  the set was rebalanced for five separate class ratios: 1  1  1  1  and 1.  the neurobiology class was the class shrunk to account for these imbalances.  
1 evaluation statistics 
the standard accuracy and error statistics quantify the strength of a classifier over the overall data set.  however  these statistics do not take into account the class distribution. forman argued that this is because a trivial majority classifier can give good results on a very imbalanced distribution . it is more important to classify samples in the minority class at the potential expense of misclassifying majority samples. however  the converse is true as well: a trivial minority classifier will give great results for the minority class  but such a classifier would have too many false alarms to be usable.  an ideal classifier would perform well on both the minority and the majority class. 
the balanced error rate  ber  statistic looks at the performance of a classifier on both classes. it is defined as the average of the error rates of two classes as shown in equation 1. if the classes are balanced  the ber is equal to the global error rate. it is commonly used for evaluating imbalanced data classification . we used this statistic to evaluate trained classifiers on test data. 
1  	fp	fn	  1   fp +tp	fn +tn  
                         ber =  	+	                        1  
1 results 
we evaluated the performance of fast-selected features by comparing them with features chosen by correlation coefficients and relief.  many researchers have used standard learning algorithms that maximize accuracy to evaluate imbalanced datasets.  zheng  used the naive bayes classifier and logistic regression methods  and forman  used the linear svm and noted its superiority over decision trees  naive bayes  and logistic regression.  the object of study in these papers  and in our research  was the performance of the feature selection metrics and not the induction algorithms.  thus  we chose to evaluate the metrics using the performance of the linear svm and 1-nn classifiers.  these classifiers were chosen based on their differing classification philosophies. the 1-nn method is a lazy algorithm that defers computation until classification. in contrast  the svm computes a maximum separating hyperplane before classification. 
the classification results are summarized in figs. 1  where dashed lines with square markers indicate classifiers using relief-selected features  with one nearest hit and miss   dashed lines with star markers indicate classifiers using correlationselected features  and dashed lines with diamond markers indicate classifiers using fast-selected features  with 1 bins . the solid black line indicates the baseline performance where all the features are used for classification.  
figures 1 and 1 show the ber versus the number of features selected using an 1-nn classifier and a linear svm for cns data  respectively. fast features significantly outperformed relief and correlation features when using the 1-nn classifier. when using the svm classifier  fast features performed the best for less than 1 features; and for more than 1 features  there was little difference between feature sets. for all the cases  using a small set of features outperforms the baseline with all the original features. similar results can be obtained for other datasets. for example  figures 1 and 1 show the results for lymph data with an 1-nn and a linear svm  respectively. due to page limits  we are not able to show the results for all the four datasets. instead  we include the average results here. figures 1 and 1 show the ber scores averaged over the four datasets with an 1-nn classifier and a svm  respectively. for comparison  the baseline performance of the classifier using all features is also included.   
another evaluation statistic commonly used on imbalanced datasets is the area under the roc  auc .  this statistic is similar in nature to the ber in that it weights errors differently on the two classes. in this study  it lines up well with the design philosophy of fast.  fast selects features that maximize the auc  so it is reasonable to believe that a learning method using fast-selected features would also maximize the auc.  we also used this statistic to evaluate trained classifiers on test data. figures 1 and 1 show the auc scores averaged over the four datasets with an 1-nn classifier and a svm  respectively. not surprisingly  fast outperforms cc and relief. 
 
 
figure 1. ber for cns using an 1-nn classifier 
 
figure 1. ber for cns using a svm classifier 
 
 figure 1. ber for lymph using 1-nn classifiers figure 1. ber for lymph using a svm classifier 
 
 
figure 1. ber averaged over cns  lymph  ovary  and 
prost using an 1-nn classifier 
 
figure 1. ber averaged over cns  lymph  ovary  and prost using a svm classifier 
 
figure 1. auc averaged over cns  lymph  ovary  and prost using an 1-nn classifier 
 
 
figure 1. auc averaged over cns  lymph  ovary  and 
prost using a svm classifier 
 
figure 1. auc for cns using a svm classifier 
 
figure 1. auc for prost using a svm classifier 
 
figure 1. training data distribution of cns with the two best relief-selected features 
 
the average results in figures 1 and 1 agree with the belief that svm's are robust for high-dimensional data. up to 1 reliefselected features did not improve the ber or the auc of the svm.  additionally  up to 1 correlation-selected features did not improve the ber. on the other hand  the svm using more than 1 fast-selected features did see a significant improvement on both ber and auc.  thus  our results agree with the general finding that svm's are resistant to feature selection  but also agree with the findings presented by forman  that svm's can benefit from prudent feature selection. specific examples of this improvement in our datasets can be seen in figures 1 and 1 using fast on the ber scores for the cns and lymph datasets  respectively  and in figures 1 and 1 using fast on the auc scores for the cns and prost datasets  respectively.  
the results for the 1-nn classifiers  seen in figures 1 and 1  are even more striking.  both relief and correlation-selected features improved on the baseline performance of the classifier significantly for a minimum of 1 features selected.  fastselected features saw a significant jump in performance over that seen using relief and correlation-selected features; the 1-nn classifiers using only 1 fast-selected features beat the baseline. 
 
 
figure 1. training data distribution of cns with the two best correlation-selected features 
 
figure 1. training data distribution of cns with the two best fast-selected features 
why would fast features outperform correlation and relief features by such a significant margin for both 1-nn and svm classifiers  we visualized the features selected by the correlation  relief  and fast methods to answer this question.  we show the training data of the cns dataset with the two best features.  figures 1 show the data using the best two relief features  the best two correlation features  and the best two fast features respectively. fast features appear to separate the two classes and group them into smaller clusters better than correlation and relief features.  this may explain why fast features perform better using both the svm and 1-nn classifiers; svm's try to maximize the distance between two classes  and 1nn classifiers give the best results when similar samples are clustered close together. 
finally  we show the effects of different class ratios on the performance of each feature selection metric.  figures 1 and 1 show the ber versus class ratios for the nips dataset with the svm and 1-nn classifiers  respectively.  not surprisingly  as the class ratio increases  the ber tends to increase accordingly.  for both the 1-nn and svm classifiers  correlation and fast features performed comparably well for datasets up to a 1 class ratio.  for the 1 ratios  fast features performed significantly better than correlation features.  relief features did not perform well on this dataset for any of the class ratios. 
we conclude that fast features perform better than relief and correlation features; this boost in performance is especially large when the selected feature set is small and when the classes are extremely imbalanced. because using less features helps classifiers avoid overfitting the data when the sample space is small  we believe that the fast metric is of interest for use in learning patterns of real world datasets  especially those that have imbalanced classes and high dimensionality. 
 
figure 1. ber for nips using svm classifiers 
 
figure 1. ber for nips using 1-nn classifiers 
1. conclusion 
classification problems involving a small sample space and large feature space are especially prone to overfitting.  feature selection methods are often used to increase the generalization potential of a classifier.  however  when the dataset to be learned is imbalanced  the most-used metrics tend to select less relevant features.  in this paper  we proposed and tested a feature selection metric  fast  that evaluates the relevance of features using the area under the roc curve by sliding decision line in onedimensional feature space. we compared the fast metric with commonly-used relief and correlation coefficient scores on two mass spectrometry and two microarray datasets that have small sample sizes and imbalanced distributions.  fast features performed considerably better than relief and correlation features; the increase in performance was magnified for smaller feature counts  and this makes fast a practical candidate for feature selection. 
one interesting finding from this research was that correlation features tended to outperform relief features for class imbalance and small sample problems  especially when the svm classifier was used. this may have occurred because the correlation coefficient takes a global view as to whether a feature accurately predicts the target; in contrast  relief  especially when the number of nearest hits and misses selected is small  has a local view of a feature's relevancy to predicting the target.  if there are small clusters of points that are near each other but far away from the main cluster of points  these points can act as each others' nearest hits while being a great distance from the nearest misses.  thus  features that have this quality could be scored rather high when they are  in fact  highly irrelevant to classification.  there is strong evidence for this claim in fig. 1. there are multiple small clusters of points  some from the majority class and some from the minority class  that are close to each other but a significant distance away from the nearest miss.  this would greatly affect the score of these two features and make them appear more relevant. figures 1 clearly point to this deficiency as the performance of both svm's and 1-nn classifiers using relief features is only marginally better  or worse  than chance and significantly behind classifiers using correlation or fast features.  
our future work will investigate the use of other metrics for feature evaluation. for example  researchers have recently argued that precision-recall curves are preferable when dealing with highly skewed datasets . whether or not the precision-recall curves are also appropriate to small sample and imbalanced data problems remains to be examined.  
1. acknowledgments 
this work is supported by the us national science foundation award iis-1. we would also like to the reviewers for their valuable comments. 
