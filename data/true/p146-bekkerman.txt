we describe a text categorization approach that is based on a combination of feature distributional clusters with a support vector machine  svm  classifier. our feature selection approach employs distributional clustering of words via the recently introduced information bottleneck method  which generates a more efficient word-cluster representation of documents. combined with the classification power of an svm  this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency. comparing the accuracy of our method with other techniques  we observe significant dependency of the results on the data set. we discuss the potential reasons for this dependency.
1. introduction
모text categorization is a fundamental task in information retrieval with rich body of knowledge that has been accumulated in the past 1 years . the  standard  approach to text categorization has so far been using a document representation in a word-based 'input space'  i.e. as a vector in some high  or trimmed  dimensional euclidean space where each dimension corresponds to a word  and then has been relying on some classification algorithm  trained in a supervised learning manner. since the early days of text categorization  the theory and practice of classifier design has significantly advanced and several strong learning algorithms have emerged  see e.g.  1  1  . in contrast  despite numerous attempts to introduce more sophisticated document representation techniques  e.g. based on higher order word statistics  1  1  1  or nlp  1  1   the simple minded independent word-based representation  known as bag-of-words  bow   remained very popular. indeed  to-date the best multi-class  multi-labeled categorization results for the well-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  september 1  1  new orleans  louisiana  usa.
copyright 1 acm 1-1/1 ...$1.
known reuters-1 data set  are based on the bow representations  1  1 .
모in this paper we give further evidence to the usefulness of a more sophisticated text representation method  which is based on applications of the recently introduced information bottleneck  ib  clustering framework  1  1  1  1 . specifically  in this approach  ib clustering is used for representing a document in a feature cluster space  instead of feature space   where each cluster is a distribution over document classes. as we show  this relatively new distributional representation  that was first explored in this context by  and then by  1  1   combined with a support vector machine  svm  classifier  1  1   allows for high performance categorization of the well known 1 newsgroups  1ng  data set . in contrast  we show that the categorization of 1ng using the strong algorithmic word-based setup of dumais et al.   which achieved the best reported categorization results for the reuters data set  is inferior.
모at the outset  these findings are perhaps not surprising since the use of distributional word clusters  instead of words  for representing documents  has several advantages. first  the word clustering performs a sophisticated dimensionality reduction  which implicitly considers correlations between the various features  terms or words . in contrast  popular greedy approaches for feature selection such as mutual information  information gain and tfidf  see   only consider each feature individually. second  the clustering achieved by the ib method provides a good solution to the statistical sparseness problem that is prominent in the straightforward word-based document representation. finally  the clustering of words allows for extremely compact representations  with minor information compromises  that allow the use of strong but computationally intensive classifiers.
1모nevertheless  when we tested our categorization setup  with word cluster representation  on the reuters data set  modapte split  we could not obtain any accuracy improvement over the best known categorization results of dumais et al.  wordbased representation . we hypothesize that this difference occurs because the articles in the reuters data set were categorized on the basis of only a few keywords. if this hypothesis is correct it might mean that with respect to this data set  no significant improvement can be achieved by representations that are more sophisticated than bag-of-words.
in section 1 we present our study of this question and our attempts to characterize the differences between the 1ng and the reuters data sets.
모the rest of this paper is organized as follows. in section 1 we discuss known categorization results for the two data sets we consider  1ng and reuters  and previous work that investigated the use of word cluster representation for text. in section 1 we briefly present the algorithmic components we use starting from mutual information for feature selection  the information bottleneck method and distributional clustering  the deterministic annealing clustering algorithm and support vector machines. although each of these components has been known and used  we believe this is the first attempt to apply all these components together. in section 1 we present our experimental setup and in section 1 we give a detailed description of our results. finally  in section 1 we summarize our conclusions.
1. related results
모dumais et al.  reported on the best-known multi-label categorization of the reuters data set  modapte split . dumais et al.'s method applies the support vector machines  svm  learning scheme over a reduced bow representation  where the feature reduction is based on a greedy word mutual information to the class labels. this method leads to a break-even result of 1% on the 1 largest categories of reuters. joachims  uses an svm for a multi-labeled categorization of the reuters data set as well  but without feature reduction  and achieves break-even of 1%. in  joachims also investigate uni-labeled categorization of the 1ng data set and using the rocchio algorithm of  applied on a mutual information reduced bow representation  he obtains 1% accuracy.
모using the distributional clustering scheme of pereira et al.   baker and mccallum  apply a distributional clustering of words  represented as distributions over their classes  that is  classes of documents in which they appear   to generate a more sophisticated text representation via word clusters. in   this representation is applied to the 1ng data set  using a naive bayes classifier over the word clusters. the result is 1% accuracy  using a uni-labeled categorization. baker and mccallum also compared their methods to other feature reduction techniques such as clustering words with latent semantic indexing  see e.g.    mutual information  and markov  blankets  feature selection   the classifier was naive bayes in all cases . their conclusion is that categorization based on word-cluster representation is slightly inferior in accuracy to categorization based on bow but the word-cluster representation is significantly more efficient.
모in this paper we investigate the strength of the word clustering approach for document representation. this type of distributional clustering is essentially a supervised application of the information bottleneck  ib  method of tishby et al. . in   slonim and tishby explore the properties of this word cluster representation and motivate it within the more general ib method. finally  in   the same authors show that categorization with representation based on ib-clustering of words can actually improve the categorization accuracy compared to bow representation whenever the training set is small. these results are obtained using a naive bayes classifier and the data sets are a number of class subsets of the 1ng.
1. methods and algorithms
1 feature selection via mutual information
모feature selection  or feature reduction  is a general term for techniques for dimensionality reduction. considering a  high dimensional  vectorial representation of the data  these techniques attempt to select an optimal subset of vector components onto which data points will be projected. the incentives are to improve classification quality  via noise reduction  and to reduce computational complexity. the selection of an optimal feature subset is a hard problem that suffers from a combinatorial explosion. therefore  despite the existence of some sophisticated methods  see e.g.   many authors consider simple and greedy approaches . dumais et al.  used the following method  based on mutual information  mi . let c and w be binary random variables indicating whether or not the category c and the word w occurred. the mutual information between c and w is defined as follows:

where ew and ec are boolean indicator random variables of the word w and the category c  respectively. in one of the experimental setups described below we use this mutual information technique for feature selection.
1 information bottleneck and distributional clustering
모distributional clustering using mutual information optimization was introduced by pereira  tishby  and lee  for distributions of verb-object pairs. the original algorithm aimed at minimizing the average distributional similarity  in terms of the kullback-leibler divergence   between the conditional p verb|noun  and the noun centroids distributions. this algorithm turned out to be a special case of a more general principle  termed the information bottleneck  ib  method by tishby  pereira  and bialek . here the question of relevant encoding of one variable with respect to another variable was posed and formulated  and a general converging algorithm introduced.
모relevant encoding of the random variable x relies on  soft  partitioning of x into domains that preserve the mutual information between x and another given variable  y . the resulting partition  or clusters of x  constitute an approximate sufficient partition that enables the construction of an optimal code  e.g. binary tree  over x  that provides all the information that x has on y . denoting the induced partition  or set of clusters  by x   the problem has a simple variational formulation: maximize the mutual information i x y    with respect to the partition p x |x   under a constraint on i x x   . namely  find the optimal tradeoff between the minimal partition of x and the maximum preserved information on y .
모the resulting self consistent equations essentially coincides with the original distributional clustering algorithm and can be written as
  
1where z 붹 x  is a normalization factor  and p y |x   in the exponential is defined implicitly  through bayes' rule  in terms of the partition  assignment  rules p x |x   p y |x   =
 . the parameter 붹 is a la-
grange multiplier introduced for the constrained information. viewed as an inverse temperature it can be used as an annealing parameter to choose a desired resolution.
1 distributional clustering via deterministic annealing
모the ib self-consistent equations can be iterated and are guaranteed to converge for every value of 붹. this is in fact analogous to the blahut-arimoto algorithm in information theory . the value of 붹 can be modified  from very low  high  temperature   which corresponds to very poor distributional resolution  to very high  low  temperature   which corresponds to higher resolution  i.e. more clusters . this procedure  known as deterministic annealing  was introduced in the context of clustering by rose et. al. . we employed this top-down hierarchical clustering procedure in this work. when applying this algorithm one has to use an appropriate annealing rate in order to identify  phase transitions  which correspond to cluster splits. note that for small data sets an alternative agglomerative algorithm that avoids this problem has been developed in   see also the approximate faster agglomerative procedure in
 .
1 support vector machines  svms 
모the support vector machine  svm   1  1  is an inductive learning scheme that has recently proved to be successful along various application domains. in particular  some evidences indicate that svm is also a good choice for text categorization  1  1 . whenever the data is linearly separable  linear svm computes the maximum margin linear classifier. for the non-linearly separable case there is an extension  that allows for cost dependent training errors. several authors advocated the choice of linear svm  as opposed to kernel-based svm  due to their speed in both training and classification time and their generalization abilities with respect to textual domains. in all our experiments we used a linear svm. the implementation we used was the svmlight package of joachims . in the multi-labeled setting  see below  we applied standard binary  threshold  svms and in the uni-labeled setting we applied confidence-rated  binary  svms that generate instance distances to decision boundaries.
1 putting it all together
모a straightforward approach to dealing with multi-class  multi-labeled categorization with m classes is to decompose the problem into m binary problems. there exist recent decomposition methods that seem to be more powerful  see e.g.  . nevertheless  for simplicity and for comparison with related results we chose this straightforward decomposition.
모in the case of uni-labled  multi-class  categorization we again used the above decomposition into m binary problems and employed a standard  max-win  approach whereby a document is categorized into a class whose classifier has the maximum confidence rate among all the classifiers.
모we present and experiment with two algorithmic schemes. the first one is based on bow using the greedy mutual information feature selection technique  see equation  1   whereby the k most discriminating words  words  are selected  the articles are projected on them and then the svm classifier is trained on the projected articles  for details see algorithm 1 . the second scheme is based on ib distributional clustering whereby the words of the training set documents are clustered into k clusters   pseudo-words   using the deterministic annealing implementation of the information bottleneck method  see sections 1 and 1  respectively   and the rest of the procedure is similar to the first setup except that articles are now projected onto pseudowords and not on words  for details see algorithm 1 . both the algorithmic schemes described are for the multi-labeled setup. the uni-labeled variants can be obtained straightforwardly.
bow-based training
input: c =  c1 ... cm  - set of categories
dtrain =  d1 ... dn  - document training set  is a bow representation of di and
ci is the set of categories di belongs to
k - feature set size
output: h =  h1 ... hm  - set of binary classifiers
 w1 ... wm  - set of selected features for each category
let wtrain be the set of words in dtrain for each category ci 뫍 c do
initiate ti+ 뫹   set of positive examples initiate ti  뫹   set of negative examples for each word w 뫍 wtrain compute i w ci  according to eq  1 
sort words in wtrain according to i w ci  extract k top words wi 뫹  w1 ... wk  for each article project if ci 뫍 cj then
모add projection d  to ti+ else
모add projection d  to ti  end if
end for
train an svm over ti+ and ti  to construct a binary classifier hi
end for
bow- based classification
input:  - a test article
h =  h1 ... hm  - set of binary classifiers
 w1 ... wm  - set of selected features for each category
output: l =  l1 ... lm  - set of boolean labels  where li 뫍
{1}  1 means that d belongs to ci 
for each classifier hi 뫍 h do
project
모run hi on projection d  to obtain li end foralgorithm 1: multi-labeled mi feature selection + svm
1. experimental setup
1 the data sets
1모the reuters-1 corpus contains 1 articles taken from the reuters newswire . each article is typically designated to one or more semantic categories such as  earn  
ib-based training
input: c =  c1 ... cm  - set of categories
dtrain =  d1 ... dn  - document training set  is a bow representation of di and
ci is the set of categories di belongs to
k - feature set size
output: h =  h1 ... hm  - set of binary classifiers f - the mapping function of words to pseudo-words
let wtrain be the set of words in dtrain
for each word w in wtrain do
build a vector
  
where nw ci  is number of w-occurrences in ci
end for
cluster the set of vectors vw onto k clusters pw =
 pw1 pw1 ... pwk  using the ib method for each word w in wtrain do
map the w to its closest pseudo-word pwi: set f w  = pwi
end for
for each article
모project end for for each category ci in c do initiatialize ti+ 뫹  ; ti  뫹   for each article d 뫍 dtrain do if ci 뫍 cj then
모add projection d  to ti+ else
모add projection d  to ti  end if
end for
train an svm over ti+ and ti  to construct a binary classifier hi
end for
ib-based classification
input:- a test article
h =  h1 ... hm  - set of binary classifiers f - mapping of words into word-clusters
output: l =  l1 ... lm  - set of boolean labels  where li 뫍
{1}  1 means that d belongs to ci 
for each classifier hi 뫍 h do
project
모run hi on projection d  to obtain li end foralgorithm 1: multi-labeled ib word clustering + svm
 trade    corn  etc. and the total number of categories is 1. we used the modapte split  which consists of a training set containing 1 articles and a test set containing 1 articles.1 in both the training and test sets we preprocessed each article so that any additional information except for the title and the body was removed.
모the 1 newsgroups  1ng  corpus contains 1 articles taken from the usenet newsgroups collection . each article is designated to one or more semantic categories and the total number of categories is 1  all of them are of about the same size. most of the articles have only one semantic tag and about 1% of the articles have two or more labels. following  we used the  xrefs  field of the article headers to detect multi-labeled documents and to remove duplications. we preprocessed each article so that any additional information except for the subject and the body was removed.1in addition  we filtered out lines which seemed to be a part of binary files sent as attachments or text delimiters. a line is considered to be a  binary   or a delimiter  if it is longer than 1 symbols and contains no blanks. overall we removed 1 such lines  where most of these occurrences appeared in a small number of articles . in both data sets we lowered the case of all letters and following  in the reuters set we generated distinct features for all the words appearing article titles.
1 cross-validated training and parameter setting
모since the standard split of reuters is fixed  cross-validation is not applicable. in our experiments with the 1ng data set we used 1-fold cross-validation. that is  we split it randomly and uniformly into 1 parts  roughly 1 articles in each part  1 articles in each category . in each random partition we used 1 of the articles for training and the remaining 1 for testing. note that this split to 1 and 1 is proportional to the training to test set size ratios in the modapte split of reuters.
모in order to improve the results we tuned the svm algorithm parameters. we used linear svmlight and tuned the parameters c  which controls training error costs  and j  cost-factor for negative and positive examples . for both parameters we fixed a set of reasonable values and then tested the svm classifier using all possible combinations with respect to a validation subset  which following   was selected to be 1 random subset of the training set.
1모parameter setting is different for multi-labeled and unilabeled categorization. for the multi-labelled setting we set the parameters of each individual  binary  classifier independent of the other classifiers. the processes of parameter tuning in our uni-labeled setting is more complex. since we use the max-win approach  the categorization of a document is dependent on all the binary classifiers involved. for instance  if all the classifiers except for one are perfect  this last bad classifier can generate maximum confidence rates for all the documents  which results in extremely poor performance. therefore  a global tuning of all the binary classifiers is necessary. however  in the case of the 1ng  where we have 1 binary classifiers  a global exhaustive search is too time consuming and a clever search in this high dimensional parameter space can be considered. instead we simply utilized the information we have on the 1ng to reduce the size of the parameter space. specifically  among the 1 categories of 1ng there are some highly correlated ones and we split the list of the categories into 1 groups as in table 1. for each group the parameters were tuned together and independently from other groups.
talk.religion.misc soc.religion.christian alt.atheismsci.medsci.electronics comp.sys.mac.hardware comp.sys.ibm.pc.hardwaresci.crypttalk.politics.guns talk.politics.mideast talk.politics.misccomp.os.ms-windows.misc comp.graphics comp.windows.xrec.autos rec.motorcyclesrec.sport.hockey rec.sport.baseballsci.spacetable 1: a split of 1ng's categories into thematic groups.
1 performance measures
모when measuring the performance of a multi-class multilabeled categorization it is meaningless to use the standard accuracy measure. it has been customary to use instead either a break-even point  bep   which  for a binary categorization problem  is the arithmetic average of precision and recall  or the f-measure which is the harmonic average of them.1 when considering a categorization into m classes c1 ... cm  we use a binary decomposition to m classifiers h1 ... hm  where the i-th classifier is responsible for discriminating between ci and the rest of the classes. for each classifier hi we compute a confusion matrix of four entries 붸i 붹i 붺i and 붻i where 붸i counts the number of samples that were classified by hi into category ci whose true label sets include ci; 붹i counts the number of samples that were classified by hi into ci but their label sets do not include ci; similarly  붺i  and 붻i  respectively  count the number of samples that were classified  ci by hi and their true label sets do  respectively  do not  contain ci. thus  the precision of hi equals  and its recall is . the total
 'micro-averaged'  precision p and recall r are given by:
		.
finally  the total micro-averaged bep is given by and the total micro-averaged f-measure is . note that the micro-averaged precision and recall are simply weighted averages  weighted by class sizes  of the precisions and recalls of the individual classifiers. following dumais et al.  and for comparison with this work  we report the total micro-averaged bep in all the multi-labeled categorization experiments below.
모in the uni-labeled experiments we used the traditional accuracy measure. since we used the 1ng  multi-labeled  set also for the uni-labeled experiment  we followed joachims  and considered a  uni-labeled  categorization of a document to be correct if the labeled assigned by the  max-win  classifier was among the set of this document's possible labels  note that the data set we used is multi-labeled .
1. results and discussion
1 multi-labeled categorization
모table 1 summarizes the multi-labeled categorization results obtained by the above two methods over the reuters  1 largest categories  and the 1ng data sets. note that the 1% result for the reuters data set was established by dumais et al. in .
모our results show an interesting difference in the quality of the two methods described above  when applied to the reuters and 1ng data sets. first  as far as we know  the 1% bep is the first reported result for a multi-labeled categorization of the 1ng data set. previous attempts for multi-labeled categorization of this set were performed by   without overall reported performance . when we computed the  micro-averaged  bep corresponding to the bow representation  following the setting described in   we could not obtain results better than 1% even when we  unfairly  allowed the algorithm to tune its parameters over the respective test sets  for each of the folds . this result  which is obtained of course under unrealistic conditions   can indicate an upper bound on the accuracy that can be achieved using this algorithmic setup. however  when we increased the number of features  selected using mi  up to 1  the results improved  using the  unfair  parameter tuning  to 1%. thus  we respect to 1ng  the algorithmic setup based on distributional word-clusters outperforms the bow setup with respect to both categorization accuracy  bep  and representation efficiency.
모we repeated the same experiment over the reuters data set but here we obtained opposite results. now the ib wordcluster representation lost its advantage and achieved a bep of 1%  note that under the  unfair  parameter tuning the ib-based algorithm achieved a bep of 1%   which is slightly smaller than the results obtained using the bow representation as reported by .1
algorithmic setupbep  reuters bep  1ng svm + mi selection  k = 1 1 1  1
 unfair svm + mi selection  k = 1 11  1
 unfair svm + ib clustering  k = 1 11  1table 1: multi-labeled categorization bep results for 1ng and reuters. k is the number of selected words or word-clusters. all 1ng results are averages of 1-fold cross-validation. 'unfair' indicates unfair parameter tuning over the test sets
모what makes the performance of these two representation methods different over these data sets  why does the bow representation outperforms the ib-based representation over reuters 
1모perhaps the key to the answer is related to the process which generated the labeling of these data sets. as noted by lewis  see    the reuters-1 set contains articles that appeared on the reuters newswire in 1 and were assembled and indexed into categories by a few personnel from reuters ltd. presumably  the manual indexing of the reuters articles relied mainly on a restricted set of keywords that the indexers looked for. in contrast  the articles in the 1ng were labeled by their own creators  by choosing and sending the articles to the recipient newsgroups   and their annotation relied on full understanding of the articles and their context.

figure 1: mutual information sorted histograms of best discriminating features for two categories. top: earn of reuters; bottom: rec.sport.hockey of 1ng
모in order to test this hypothesis  for each category in both data sets we computed the mutual information between words appearing in the category and the category. then we sorted these words by decreasing values of their mutual information. for instance  in figure 1 we show two graphs of the mi behavior and it could be seen that the graph of  earn   reuters  goes down much sharper than the one of rec.sport.hockey  1ng   which indicates that only a few words of reuters contribute to the text categorization. as can be seen  the scales of the y-axis of the two graphs differ by one order of magnitude. in order to compare them we plot them in figure 1 on a percentage scale where each mutual information value is linearly transformed so that a value of x in a dynamic range of  a b  is transformed to  x   a / b   a . when we consider the dynamic range of the 1 most informative words in each category we obtain the normalized  and sorted  histograms of figure 1. when put on the same scale  the graphs indicate that the 1ng categories are characterized by more features than those of
reuters.
모in figure 1 we show two learning curves plotting the obtained bep as a function of the number of words used. in

1
1 1 1 1 features
figure 1: normalized mutual information sorted histograms of both 'earn'  reuters  and
'rec.sport.hockey'  1ng 
the figure we see two curves: one  which describes the learning rate with respect to reuters  and the second with respect to 1ng. as can be seen  the bep of reuters almost approaches its maximum with only 1 words  that were chosen with the greedy  non-optimal mutual information method . this means that other words do not contribute much. however  the graph of 1ng constantly goes up while its slope constantly lowers.
모in addition  we note that with only one word per category the bep result for the entire reuters corpus is 1% while for 1ng it is much lower  1% . finally  in table 1 we list the individual bep result for categorizing the 1 largest categories in reuters based on three words. for instance  based on the words  vs    cts  and  loss  it is possible to achieve a bep categorization of earn which is over 1%. we note that the word  vs  appears in 1% articles of category earn  that is  1 articles among total 1 in this category . this word appears in only 1 non-earn articles in the test set and therefore  vs  can  by itself  categorize earn with very high precision.1 this phenomenon was already noticed by joachims  who showed that a classifier built on only one word   wheat   can lead to extremely high accuracy of distinguishing between the category wheat and the others within a uni-labeled setting.
1 uni-labeled categorization
모we performed similar categorization experiments on 1ng using the uni-labeled setting. the results are shown in table 1. as it is in the multi-labeled setting  the advantage of word-clusters  the ib scheme  over bow is clear when using the same  1  number of features. however  if we increase the number of selected words this gap decreases and with k = 1 words the bow accuracy result is essentially the same  note however that the reported bow results were obtained using unfair parameter tuning . in contrast  when we increase the number of word clusters up to 1 we do not achieve any significant accuracy gain  see figure 1 .
1모thus  in the uni-labeled setting  the accuracy disadvantage of the bow representation can be better traded-off us-

figure 1: learning curves  bep vs. number of words  for the reuters-1 and the 1ng over the mutual information top 1  above  and the top 1  below  words using bow-based representation and svm
ing more words. unlike to the multi-labeled setting where we observed both accuracy and representation efficiency advantage of the word-clusters representation  here we only observe representation efficiency advantage over bow.
1 computational efforts
모we ran our experiments on a pentium iii 1mhz 1g ram pc under windows 1. for the setup of the mi feature selection and svm classification  the computational bottleneck was the svm training  for which a single run could take a few hours  depending on the parameter values. in general  the smaller the parameters c and j are  the quicker the algorithm runs.1
모as for ib-clustering and svm classification  the svmlight runs faster on the input vectors of pseudo-words. however  the clustering itself can take up to one hour on the entire 1ng set  and requires much memory  up to 1g ram for a run . the overall training and test time over the entire 1ng in the multi-labled setting is about 1 hours  1 hours for each of the 1 cross-validation folds .
모the computational bottleneck in the uni-labeled setting on 1ng is the svm parameter tuning. the ib-based experiment runs for about 1 hours  while the word-based experiment runs for a little bit less than 1 hours  1 days .
category1st word1nd word1rd wordbepearnvs+cts+loss+1%acqshares+vs inc+1%money-fxdollar+vs exchange+1%grainwheat+tonnes+grain+1%crudeoil+bpd+opec+1%tradetrade+vs cts 1%interestrates+rate+vs 1%shipships+vs strike+1%wheatwheat+tonnes+wheat+1%corncorn+tonnes+vs1% 
table 1: three best words in terms of mi and their categorization bep rate of the 1 largest categories of reuters. the micro-average over these categories is 1%. '+' means that the word contributes by its appearance  ' ' means that the word contributes by its disappearance
algorithmic setupaccuracy  1ng svm + mi selection  k = 1 1  1  unfair svm + mi selection  k = 1 1  1  unfair svm + ib clustering  k = 1 	1	1
table 1: uni-labeled categorization accuracy of 1ng. k is the number of selected words or wordclusters. all accuracies are averages of 1-fold crossvalidation
1. concluding remarks
모we have shown that a cluster-based representation of texts using the information bottleneck method  combined with a support vector machine classifier  leads to both uni- and multi-labeled categorization of the 1ng data set that is superior to the best known word-based techniques. we believe that these results show the potential advantage of more sophisticated text representations. on the 1ng data set  with respect to multi-labeled categorization  we obtain both accuracy  bep  and representation efficiency advantages. with respect to uni-labeled categorization we gain no accuracy advantage but still maintain representation efficiency.
모nevertheless  we found no advantage to our technique in the  multi-labeled  categorization of the reuters set and we hypothesize that this is due to some inherent differences in the ways the two data sets were generated. clearly  future work in text categorization could benefit from a comparative study with respect to a larger variety of data sets.
1. acknowledgements
1모this research was supported by the israeli ministry of science. we sincerely thank thorsten joachims and rob schapire for their generous help in preparation of this paper. we would also like to thank susan dumais  andrew mccallum  yoram singer  noam slonim and tong zhang for their prompt responses and fruitful discussions. r. elyaniv is a marcella s. gelltman academic lecturer.

figure 1: accuracy vs. number of word-clusters; uni-labeled setting  1ng
