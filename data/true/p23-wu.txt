subsequence similarity matching in time series databases is an important research area for many applications. this paper presents a new approximate approach for automatic online subsequence similarity matching over massive data streams. with a simultaneous online segmentation and pruning algorithm over the incoming stream  the resulting piecewise linear representation of the data stream features high sensitivity and accuracy. the similarity definition is based on a permutation followed by a metric distance function  which provides the similarity search with flexibility  sensitivity and scalability. also  the metric-based indexing methods can be applied for speed-up. to reduce the system burden  the event-driven similarity search is performed only when there is a potential event. the query sequence is the most recent subsequence of piecewise data representation of the incoming stream which is automatically generated by the system. the retrieved results can be analyzed in different ways according to the requirements of specific applications. this paper discusses an application for future data movement prediction based on statistical information. experiments on real stock data are performed. the correctness of trend predictions is used to evaluate the performance of subsequence similarity matching.
1. introduction
　many applications generate data streams and there is an increasing need to maintain statistical information online. stream databases are distinguished from conventional databases in several aspects. raw data is too large to be stored in a traditional database for efficient data management. querying on the stream database is difficult in set-oriented data management systems. because the data is changing constantly  a single-pass search over the stream is mandatory since it is infeasible or impossible to rewind the stream.

 this work is part of censsis  the center for subsurface sensing and imaging systems  under the engineering research centers program of the national science foundation  award # eec1 .
this work is partially supported by nsf grant iis-1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  paris  france.
copyright 1 acm 1-1/1 . . . $1.
the answers of the query usually are approximate and partial answers. examples of stream databases can be found in stock market quotes  sensor data  telecommunication systems  and network management.
　subsequence matching in time series databases tries to find subsequences from the large data sequences in the database that are similar to a given query sequence. it is important in data mining and is used for pattern matching  future movement prediction  new pattern identification  rule discovery and computer aided diagnosis. stream data are naturally ordered in time. some streams are ordered in a fixed time interval and can be treated as stream time series directly. some streams come in irregularly and special procedures are needed in order to apply time series techniques. for example  there are thousands of stock transactions every second  which may be carried out at any time and there are different numbers of transactions at different times.
　existing techniques on time series subsequence matching mainly focus on discovering the similarity between an online querying subsequence and a traditional database. the queried data are static and are accessed using an index. research in time series data streams is in its preliminary stage. only some basic statistical measures such as moving averages and standard derivation have been addressed. there is recent research  1  1  1  1  on similarity matching over data streams. the papers  treat pair-wise correlated statistics in an online fashion  focusing on similarity for whole data streams  not on subsequence similarity. the papers  1  1  treat similaritybased continuous pattern queries with prediction  which can be extended to answer nearest neighbors on a stream time series. last   uses an index structure for k-nn search on data streams. in contrast  we are investigating application-guided subsequence matching over online financial data streams and online query subsequences. our database is a dynamic stream database which stores recent financial data. it will be automatically updated as new stream data comes in. so our database includes the most recent historical data. the query subsequence is automatically generated based on the current state of the data stream. and our new similarity measure satisfies the special requirements of financial data analysis.
　subsequence similarity of financial data streams has its unique properties. first  according to elliott wave theory   the movement of the stock market can be predicted by observing and identifying a repetitive pattern of waves. based on this wave theory  the online piecewise linear representation of the stream data should be in an up-down-up-down repetitive pattern  the zigzag shape . keogh et al.  summarized four well-known algorithms for time series segmentation. none of them has addressed the zigzag requirement. the result of the compression algorithm in  is in shape  but the algorithm does not satisfy the real time re-

figure 1: subsequence similarity with different relative positions:  a  two subsequences differ in the relative positions of the lower points;  b  two subsequences differ in the relative positions of the upper points.

figure 1: subsequence similarity with time scaling and amplitude rescaling.
quirements for online stock data analysis because it has a longer delay to identify an extreme point when the extrema ratio is large. it is necessary to have a new online segmentation algorithm that can quickly and accurately identify potentially important points. second  the relative position of the upper and lower end points plays an important role in subsequence similarity. figure 1 shows two examples of two pairs of subsequences which would be considered similar using existing subsequence similarity measures. but technical analysis of financial data is also concerned with the relative position of the upper end points as well as the relative position of the lower end points. the two pairs of subsequences in figure 1 would not be considered similar by financial data analysts. third  subsequence similarity should be flexible with regard to time shifting and scaling  price shifting and amplitude rescaling   is the value difference of two adjacent end points . financial data technical analysis assumes that the amplitude difference is more important than the time difference. for example  in figure 1  all four subsequences are derived from a sequence with time scaling  or amplitude rescaling  or both. the pairs s and s   s and s have the same amplitude changes  but different time changes  and the pairs s and s   s and s have the same time changes but different amplitude changes. according to financial analysts  s and s   s and s are more similar while s and s   s and s are less similar. a new subsequence similarity definition that allows amplitude rescaling  but with limitations  is required.
　our new online event-driven subsequence similarity matching takes into account and gracefully handles the special properties of financial data analysis. we make the following main contributions:
1. we propose a 1-tier online simultaneous segmentation and pruning algorithm. it takes a raw financial data stream as input and produces a stream of piecewise linear representation end points. the end points are in an upper-lower-upperlower repetitive pattern  the zigzag shape . this tiered segmentation and pruning algorithm provides the piecewise linear representation with high sensitivity and accuracy. the algorithm runs in linear time and with constant memory.
1. we explore an alternative similarity measure for subsequence matching  where a metric distance function is defined based on a permutation of the subsequence. the permutation ensures two subsequences have the same relative positions. the distance function controls the extent of amplitude rescaling. the new definition provides subsequence similarity search with sensitivity  flexibility and scalability. any existing metricbased indexing technology can be employed for search speedup.
1. we perform event-driven subsequence similarity matching over an up-to-date database using the end points of the piecewise linear representation. the query will be carried out only when there is a new end point. the automatically generated query subsequence is the most recent subsequence of the end points  which reflects the most recent information of the raw financial data stream. a new mechanism that can turn on or off the search engine is enabled.
1. we apply a new definition of trend for financial data streams using the results of subsequence similarity search to predict future data movement. our definition of trend does not place any restrictions on the characteristics of the stock streams on which it is applied. the market can be a bull market  a bear market or a no-trend market. our event-driven subsequence similarity search is more accurate in seizing critical points for a trend period than algorithms which search at all time instances. in addition  our approach is 1 times faster than searching at all time instances.
　the rest of the paper is organized as follows. section 1 briefly discusses related work on subsequence matching and data stream processing. section 1 describes our strategy for data processing over incoming streams. the subsequence similarity matching of the resulting piecewise linear representation is explained in detail in section 1. one application of our similarity search for trend prediction is discussed in section 1. section 1 presents our experiment results and section 1 concludes this paper and provides some future research directions.
1. related work
　similarity search in time series is useful for many data mining applications. agrawal et al.  has first introduced whole sequence similarity matching. faloutsos et al.  generalized it to subsequence similarity matching. the basic idea is to transform the sequence into the frequency domain using a discrete fourier transformation  dft . then the first few features are extracted and the euclidean distance is used as the similarity distance function. multidimensional indexing methods such as the r*-tree  can be applied for fast search. in subsequence matching  the r*-tree stores only minimum bounding rectangles  mbr . new research based on subsequence search has grown in several aspects. new methods in constructing mbrs reduce false negatives . keogh et al. proposed piecewise aggregate approximation  paa  to reduce the dimensionality and to support fast sequence matching using r-trees. other feature extraction functions  such as the discrete wavelet transformation  dwt   1  1   adaptive piecewise constant approximation  apac    and single value decomposition  svd   have been proposed to reduce the dimensionality of time series data. new distance functions such as dynamic time warping  1  1  and longest common subsequences  have been explored to overcome the brittleness of the euclidean distance measure or its variations  1  1  1 .
　data streams have attracted more research interest recently  1  1  1  1  1  1  1  1  1  1  1  1 . babu et al.  showed how to define and evaluate continuous queries over data streams. some basic statistics over data streams have been studied. datar et al.  studied single stream statistics using sliding windows. gehrke et al.  studied statistics for correlated aggregates over multiple data streams using histograms. gao et al.  1  1  introduce a new strategy of continuous queries with prediction on a stream time series. liu et al.  treat knn search over data streams using index structures. zhu et al.  proposed a new method for statistics over thousands of data streams. their research focuses on pair-wise correlation using a grid-based data structure. data stream clustering algorithms include stream  1  1   fractal clustering   and clustream . stream aims to provide guaranteed performance of data stream clustering and clustream is developed for clustering large evolving data streams.
　stock data analysis has attracted researchers for years. autoregressive and moving average are long used techniques  for stock market prediction. in the field of data mining  intensive research has been done on the application of neural networks to stock market prediction . stock trends can be also predicted based on the association of trends with news articles . fink and pratt applied subsequence similarity matching in compressed time series by identifying major extrema . the previous work does not concern the real time requirements of online financial data analysis. for instance  the -test based piecewise segmentation in  works on static historical time series in the training phase. the compression algorithm in  runs in an online fashion. but it will take longer delay time to identify the previous extremum  which is not practical for stock trading where early detection of a potential end point is critical.
　our work differs from previous research in several aspects. the problem addressed here is online subsequence search over financial data steams and we have addressed the special requirements of financial data technical analysis. our distance measure for subsequence similarity is a metric distance function based on a permutation. the subsequence matching process is triggered by new online events. our database maintains up-to-date information with newly arrived data  not previously obtained data.
1. online data stream processing
　translating massive data streams into manageable data for the database  which can be queried and indexed upon is an important step for data stream subsequence similarity matching. this section discusses the data preprocessing steps before similarity search which result in piecewise linear representation of incoming streams. the process of data stream aggregation  segmentation and pruning is explained in more detail below.
1 aggregation and smoothing
　piecewise linear representation of the data streams requires the data streams to have one fixed value for each time interval. the incoming data streams may arrive at any time. aggregation over raw data streams is both necessary and important for practical applications. a stream may acquire different aggregate values for different purposes. for example  in stock market analysis  the open  high  max   low  min   close  and volume  sum  values of one quote over a time interval  minutes  hours  days  months or years  are very important information.
　aggregation makes sure there is a unique value for each time instance over a fixed time interval. if we draw the data movement with time  we can see a lot of shorter-time random oscillation over a longer-term trend. we need to filterout the noise before further data processing. we use the standard moving average which is widely used in the financial market  to smooth the data:

where x i  is the value for i = 1  1  ...  n and n is the number of periods. calculates the p-interval moving average time series which assigns equal weight to every point in the averaging interval. by smoothing through the moving average  shorter-term noise will be filtered out while a clean trend signal is generated.
1 piecewise linear representation
　piecewise linear representation uses line segments to approximate a time series  1  1 . our approach is new because we adopt a tiered online segmentation and pruning strategy. we do not segment over the price stream directly  instead we segment over one financial indicator  bollinger band percent  %b    to be the first base input for line segmentation. then we prune over the end points of the %b line segments based on some criteria of %b. the final line segments over the raw data stream are obtained by pruning on the previous line segments with criteria based on the raw price stream. we will explain in detail why we choose %b to do the segmentation and how the tiered structure provides high sensitivity and accuracy in the online segmentation.
1.1 %b indicator
　bollinger bands  are widely used financial indicators which provide relative definitions of high and low values for time series. the bands are curves drawn above and below a moving average by a measure of standard derivation. an example of time series and bollinger bands is shown in figure 1b. the three curves are defined as follows:
middle band = p-period moving average upperband = middle band + 1	p-period standard deviation lowerband = middle band - 1	p-period standard deviation
%b  shown in figure 1c  is another popular indicator derived from bollinger bands. %b tells us the current state within the bands. the formula for %b is the following:


%b is chosen to be the first base for linear segmentation because of the following. first  %b has a smoothed moving trend similar to the price movement. if the price moves in an up trend  %b is also in an up trend. and if the price is in a down trend  %b is also in a down

corresponding stream of %b values;	 d  plr of %b without pruning;	 e  plr of %b with pruning only on %b during segmentation;	 f  plr ofraw stream data with pruning on %b and raw data during segmentation.
trend. the upper and lower end points of %b correspond to the upper and lower end points of the raw price data. second  %b is a normalized value of the real price. most %b values are between -1 and 1 no matter what real price values are. so we can set a uniform segmentation threshold for %b which we could not do over the real price. for example  if the average price of a stock is $1  a change of $1 may be considered as a big movement. but to a stock with an average price about $1  $1 difference can only be considered as noise. third  %b is very sensitive to the price change. it will manifest the price change accurately without any delay. so segmentation over %b is more suitable than segmentation directly over the raw price data stream.
1.1 segmentation
　segmentation is based on the %b values. for each time instance  there is a corresponding %b value. segmentation over %b finds optimized upper and lower end points of the piecewise linear representation for %b. figure 1d shows the segmentation results of figure 1c. our segmentation algorithm is different from others not only because of a different definition of upper and lower end points but also the resulting end points of our segmentation are in the zigzag shape which is not the case in other algorithms.
　our segmentation algorithm uses a sliding window with varying size. the sliding window can only contain at most m points  beginning after the last identified end point and ending right before the current point  as shown in figure 1. if there are more than m points between the last end point and the current point  only the last m points are contained in the sliding window. the segmentation tries to find a possible upper or lower point only in the current sliding window. an upper point is defined as follows  the definition of a lower point is symmetric and thus is omitted here :
	suppose the current point is p	. the upper point	 	 
  is a point in the current sliding window that satisfies:
1. = max  x values of current sliding window  ;
1.  where	is the given error threshold  ;
1. is the last one satisfying the above two conditions.
　figure 1 shows an example of an upper point. here  is the current point. the previous identified end point is   so the

figure 1: a sliding window which finds an upper point. suppose that m = 1  = 1  is the last identified end point  is the current point. the actual sliding window size m is 1. is a new upper point.
sliding window currently contains m = 1 points starting from . both and have the maximum value  but only is found as a upper point because it is the last one with the maximal value in the sliding window. another thing needed to be mentioned here is the delay time  which is the time difference between the actually time of an end point and the time when it is identifies as an end point. although the upper point is at   it is only identified at . the delay time for identifying is . the threshold plays an important role in the delay and the number of line segments. a smaller will reduce the delay time but result in a larger number of short line segments  some of which may still be noise. a larger will decrease the number of line segments but with longer delay. if is too large  some useful information will be filtered out. there is a tradeoff between the delay time and more accurate piecewise linear representation. we propose an optimized algorithm for simultaneous online segmentation and pruning. the new algorithm will reduce the delay time yet will give more accurate piecewise linear representation.
1.1 pruning
　before going into detail for our online segmentation and pruning algorithm  we first introduce the rationale and approach for pruning. to the best of our knowledge  no other published algorithm does pruning. pruning is the process to remove noise-like line segments along with the segmentation process. segmentation tries to find potential end points using a smaller threshold   so new end points can be identified with shorter delay time. pruning is smoothing over recently identified end points. noise introduced by small will be filtered out by the pruning process and more accurate line segments are generated. this segmentation and pruning mechanism helps to quickly identify a new end point yet with accurate piecewise linear representation. the shorter delay time is very important for real time applications such as stock data analysis. the end points are generally critical points for stock transactions. the earlier such points are identified  the better the chances are for profitable stock trading.
　the pruning process itself is a two-step process. first  %b is used in the filter step. but when mapping %b pruned end points onto raw data  the piecewise linear representation on raw data may still have some noise. it is possible that the %b data values change considerably while the raw data values change very little. so we need a refinement step. pruning on the raw data stream not only removes the oscillations of a trend  but also enforces the zigzag shape. under rare conditions  the end points mapped directly from %b end points may not be in the zigzag shape. figure 1e and 1f shows the pruning results on both %b and on the raw stream data. the thick dotted line segments are new line segments generated by the pruning process. the corresponding filled line segments covered by dotted lines are removed.
　the actual technique for pruning is following. if the absolute %b or raw data values of two adjacent end points  called the amplitude  differs by less than a certain value  that line segment should be removed. note there may be different values for pruning on %b from those used in pruning the raw data stream. the tricky part is we must keep the zigzag shape of the end points  so we must remove two adjacent end points at the same time. this creates a problem as shown in figure 1. here  the line segment is under the pruning threshold  so pruning is needed. there are several ways to remove .
　in online segmentation and pruning  at each new end point  we check the previous line segment for pruning. for example  in figure 1  at the time when end point e is identified  line segment is tested for pruning. first we check the need for pruning on %b. if needed  pruning is carried out. then the system waits for next stream data to come in and no pruning on raw data is done. if no pruning on %b is needed  the same line segment is checked for pruning on raw data. so there is at most one pruning at each end point. the pruning algorithm is the same for pruning on both %b and raw data.
　we compare the last end point with the third last end point to see which one gives a better piecewise linear representation. if the two points are upper points  the one with the larger value will be kept. otherwise  if both lower points  the one with the smaller value will be kept. figure 1 gives an example for pruning with the last end point as a lower point. end points e and c are compared. if e has smaller value  end points c and d will be removed from the end points stream  and a new line segment is generated  figure 1b . if c has smaller value  end points d and e will be removed. line segment will remain  figure 1b .
1 online segmentation and pruning
　our online subsequence similarity matching is based on the similarity between two subsequences of end points. a single-pass for online segmentation and pruning is mandatory. to reduce the time delay in identifying end points and improve piecewise linear representation  we use different thresholds for segmentation and prun-

	figure 1: two possible ways for pruning line segment	.
ing: a smaller threshold for segmentation over %b  a larger threshold for pruning over %b  and a separate for pruning over raw stream data. a smaller threshold for segmentation will ensure the sensitivity and reduce delay. a larger pruning threshold will filter out noise. our experiments show that   are suitable for most stock prices. the value of is flexible and varies according to different users. experiments have shown that1% to 1% of the price change over the trading period has reasonable results. for instance  for intra-day trading  if a quote's average daily price change is $1  between $1 to $1 all can achieve pretty good results.
　the online segmentation and pruning are running simultaneously. whenever an upper/lower point is identified by the segmentation process  the previous line segment is checked for pruning as mentioned in section 1.1. to better explain the online segmentation and pruning algorithm  an animation of the process is illustrated in figure 1. suppose now we are after the time when is identified as an upper point  figure 1a . as time goes on  p    is identified to be a potential lower point  figure 1b . a temporary line segment is generated. the line segment immediately before is checked for pruning. since the amplitude of the line segment on %b is larger than   and that of raw stream is larger than   neither pruning on %b nor on raw stream is needed. similarly  end points p    and p    are identified as potential end points without pruning  figure 1c .
　a pruning is encountered when p    is identified as a potential upper point  figure 1d . the line segment is checked for pruning. since the amplitude of is less than   a pruning process is required. the last end point p    and the third last end point p    are compared for a better piecewise linear representation on %b. since both points are upper points  the one with the larger value will be kept. here  the value at t is larger  end points p    and p    are removed  and line segments on both %b and the raw stream are removed. a new line segment is created.
　continuing the segmentation and pruning process to time   a new potential lower end point is identified without pruning. another pruning process is encountered at time when a new potential upper point is identified  figure 1e . the amplitude for the previous line segment on %b is larger than   so no pruning on %b is required. but the amplitude of is less than threshold   pruning on raw data stream is required. by comparing the raw price values at and   the end point at is kept while end points and are removed.
　as a summary for figure 1  for time to   two end points on the raw data stream are identified  i.e.  the end points at and . all other potential end points are removed by pruning on either %b or the raw stream. the end points of %b are only a temporary tool and will not be kept in the final piecewise linear representation of the raw data stream. also we have the following observations:

figure 1: illustration of the online segmentation and pruning.if an end point has one following line segment whose amplitude is larger than the pruning threshold on both %b and raw data stream  that end point is fixed  i.e.  it can not be removed by further segmentation and pruning process.
after pruning  if an end point has two following line segments  that end point is fixed.
when a new potential end point is identified but no pruning is needed  a new fixed end point will be produced.
the online segmentation and pruning algorithm will only affect the last three end points.
　combining the above observations  it is easy to understand that the online segmentation and pruning can be done with varyinglength sliding windows  starting from the last fixed end point to the current data of the stream. and there are at most three end points that need to be kept for the following segmentation and pruning procedure. all the fixed end points are updated into the database in real time  so the database has up-to-date information.
1 dynamic adjustment
　occasionally a stock quote will have a dramatic change in price caused by a stock split or stock merge. upon stock merge  or split   the current stock price will have a sharp increase  or decrease . a dynamic adjustment is needed to correct the historical data  which is adjusted with the same ratio for the change. in the case of a merge  we only need to increase the historical data values according to the merge ratio. but in the case of stock split  not only we need to decrease the historical data values  but also we need to prune on the historical plr  since some line segments are under the threshold . thus a recursive pruning on the historical data is carried out.
　another optimization is to approximate the stream at different granularities by constructing hierarchical plr end points. the database stores the base end points  which is obtained by plr on the raw streams with base   and . it can be used for similarity matching of plr query subsequence over 1 minute raw data and with the same thresholds. when a query subsequence is with other time granularities  such as 1 minutes  or with different thresholds  larger than the base thresholds   a dynamic process to construct the plr with the same conditions as the query subsequence is performed on the historical base plr end points.
1. subsequence similarity matching
　our online event-driven subsequence similarity matching over data streams is based on the piecewise linear representation of the stream. in this section  we first provide details about our new definition of subsequence similarity. then we introduce event-driven online subsequence similarity matching.
1 subsequence similarity
　the subsequences in our application are subsequences of end points. the subsequence similarity matching in our application finds the subsequences of end points that are similar to the query subsequence. for simplicity  the retrieved subsequence and the query sequence have the same number of end points.
　there have been many research efforts for efficient similarity search based on euclidean distance or its variations  1  1  1  1  1   dtw distance  1  1  1   or lcs distance . however  they do not address the special requirements of financial data anylysis. for example  the distance functions do not concern the relative position of corresponding end points. we hereby propose a new subsequence similarity definition which is more appropriate for financial data analysis.
　our similarity distance function is based on the relative positions  the permutations  of the upper and lower end points in the subsequence. the permutation of a sequence s with n elements is a permutation of 1  1  ...  n. it is calculated through the following steps. consider a stream of end points:
first  we divide the end points into two subsets by putting all the upper points into one subset and all lower end points into another. in each subset  the end points are still in the order of time. without loss of generality  suppose that is an upper point and n is even  we will get a new sequence of the n points as
next we sort the x values of each subset. we will get another sequence
where	 	  is a permutation of 1  1  ...   n-1 and	is a permutation of 1  1  ...   n.
　
is called the permutation of s.
it represents the relative positions of the upper end points and the lower end points. with the permutation of a subsequence  we can define subsequence similarity as following:
definition 1. given two subsequences s and s':
s and s' are similar if they satisfy the following two conditions:
s and s' have the same permutation. where

	and	 	and	1 and are user-defined parameters.
the value is dependent on the raw data pruning threshold   and special applications. our experiments show that the optimal value of is around  when = 1 and = 1.
　in all of our experiments on financial data we use = 1 and = 1. we have made our definition of similarity more general because its metric properties can be proved in the more general case and therefore it may prove useful for non-financial data as well.
　the subsequence similarity definition seems brittle in the special case of figure 1. the values at and only differ a tiny bit in the two sequences  but the permutations of the two sequences are different. they will not be considered similar using our similarity definition. we can still handle the special case by changing the search algorithm still using the similarity definition. in our search algorithm  the permutations of the query subsequence and the retrieved subsequences are compared first  if the same permutation  the distances are calculated. if a query subsequence has any pairs of upper points  or lower points  with distance under a certain predefined threshold  we consider the query subsequence to have two permutations. subsequences of the two possible permutations are both searched. in the worst case  the distances between the query subsequence and all possible subsequences will be computed. since after piecewise linear representation to reduce the dimensions  the subsequence lengths of the plr end points are below 1  the possible permutations are limited and the special cases are uncommon  so the query performance is still reasonable.
　the two parts of the similarity definition are both necessary and complementary which can be illustrated in figure 1. the permutation is concerned only with relative positions of the end points and not with the differences of actual prices. the permutation alone provides our similarity search with the flexibility of time scaling and amplitude rescaling. the amplitude distance function is more sensitive to the change of amplitudes. the two parts together give our similarity search with flexibility  sensitivity and scalability.
　next we want to show that our distance function is a metric function and we can use metric distance indexing methods for faster search. first we introduce the following lemma.
lemma 1. if a  b  c1 .lemma 1. if a  b  c   1 and  then.
figure 1: special cases that are brittle under our similarity definition yet can be handled using our query engine.
　lemma 1 can be proved easily by listing all the possible combinations of a  b and c. lemma 1 can be proved by the properties of inequality.
　theorem 1. for sequences s  s'  with the same length   the distance d s  s'  is metric.
　proof. to prove is metric  we need to prove it is symmetric and reflexive  and it satisfies the triangle inequality. obviously and   so is symmetric and reflexive. next  we need to prove that satisfies the triangle inequality  i.e. 	.
　using the definition of the distance function as a sum of an amplitude component and a time component  if we prove the triangle equality for both components  it will certainly be true for by lemma 1. we thus show:
and
where
　　    and are positive since they are absolute values.     and are positive since according the properties of time series data. and we know that and are non-negative. the proof completes due to lemma 1. 
1 event-driven subsequence match
　stream data comes in continuously. performing similarity search upon all incoming data is not efficient for massive stream data management  especially not for real time applications such as stock market analysis. another possible option is to do similarity search after a fixed time period  for example every 1 minutes . this will reduce the computation burden but it is insensitive to the changes between two query times and may lose some potentially important information. the event-driven similarity search proposed here will reduce the huge computation burden over the system as well as maintain sensitivity to changes.
　an event means a new potential end point is being identified and no pruning is need. for example  in figure 1  when    
       are identified as potential end points  they are called events; while no event occurs when are identified.
the event-driven subsequence similarity matching performs automatic subsequence similarity search only at the time when there is a new event. the similarity search is totally automatic. the search requests are automatically generated by the online segmentation and pruning algorithm. the automatically generated query subsequence is the most recent n fixed and potential end points  including newly identified potential end point at the event .
　the automatic event-driven subsequence similarity has a trigger  which separates the online similarity search  the query engine  from the online segmentation and pruning process  the data engine . the data engine is for data acquisition and database updating  which runs all the time and processes each incoming stream data. the query engine can be turned on or off without affecting the data engine. this is a very friendly feature for application users. for some time periods  an application user may not want to trade  so the query engine is off while the data acquisition engine is still on. when the user returns to trade  the query engine is turned on with an up-to-date database.
1. trend prediction
　the results of our online event-driven subsequence similarity matching can be analyzed using different analytical or statistical approaches for different applications. practical utilizations of our subsequence similarity matching include trend prediction  new pattern recognition  and dynamic clustering of multiple data streams based on subsequence similarity. as a sample application  trend prediction is discussed in details as follows.
　each historical end point has a trend. a trend of an end point is the tendency of the raw stream after a given number     of end points from the current end point. the trend of one end point may be different for different durations of time. we define the trend of an end point based on the number of end points. trend-k is the overall trend from the current end point to the next end point. for simplicity  we define four trends: up  down  notrend 
undefined. given an end point e and its next	end point
　  the trend of e is defined as follows  where is a user defined parameter :
if	  e.trend = up; if	  e.trend = down;
	if	  e.trend = notrend;
	if	does not exist  e.trend = undefined.
according to the above definition  the most recent k end points have trend of undefined. all other historical end points have fixed trends of up  down or notrend. k is important in determining the trend of an event. figure 1 gives an simple example of how the value of k affects the trend. for example  when k = 1  b has a down trend  the price at is lower than that at by or more ; but when k = 1  b has an up trend  the price at is higher than that at by or more ; and when k = 1  b has an notrend trend  the price between and is less than  . our experiments show that  if we choose the value of to be 1% to 1% of the average price change over a period  it is optimal for short-term trading  such as intra-day trading. long-term trading favors a larger value.
　subsequence similarity search returns a list of end points. each is the last end point of one retrieved subsequence. simple statistical information are carried out on trends of retrieval end points  and the statistical results is used to predict the trend at the query event. our statistical approach is simply to count how many up  down  notrend end points. then we calculate the percentage of each trend d using the following formula:

　if there is a large number of similar subsequences at an event  its trend can be predicted based on f d  value of each trend. we propose the following scheme:
if	  predict notrend;



figure 1: trends of end points.
otherwise 
if	  predict up; else  predict down.
here is a user-defined threshold  e.g. 1%. for instance  if the similarity search retrieved 1 similar subsequences from history  and the statistics show historical trends with 1% up  1% down  1%  the future moving trend of the query event can be predicted as up. this is because f up -f down =1%  which is larger than f notrend +  =1% . as another example  if the historical trends are 1% up and 1% down  even though f up  f down   we really should predict notrend since they are close.
1. performance evaluation
1 experimental setup
　we have evaluated the performance of our online event-driven subsequence similarity search based on the correctness of trend predictions. real stock data are used in our experiments. for each incoming data stream  aggregated values per minute have been accumulated. more than 1 1 data points from 1 different stocks were used in experiments. first  about 1 1 historical data points are used as a test bed to set up all parameters and build the initial database. another 1 new data points are tested for online similarity search followed by trend prediction. for simplicity  for one query  the query is performed on a single stream  which is the same as the query subsequence.
figure 1: correctness of trend predictions.  a  with different similarity measures;  b  by different query mechanisms;  c  average　all our experiments are conducted on a dell optiplex gx 1 with pentium r  1 processor  1ghz cpu  1gb ram. series of experiments have been carried out on how to choose each parameter. for example  there is a series of experiments for p-interval moving average with p = 1  1  1  1  1  1  1  1. another series is for segmentation with = 1  1  1  1  1  1 and = 1  1  1  1. for the experiments discussed below  following parameter setting is constant with moving average delay time to identify an end point.
p = 1  segmentation sliding window size m = 1  segmentation threshold = 1  pruning threshold = 1. other parameters depend on the properties of raw data streams. is about 1% 1% of the average daily price change of a raw data stream. the similarity threshold of our distance function is. trend duration k  trend range are user-specified as is the subsequence length parameter n. typically  is between 1 and 1.
　although we defined similarity using a distance function which includes time as well as amplitude  we do not use time here   is zero and is one . our new similarity measure is thus called perm + amp  denoting that it is based only on the permutation and the amplitude.
　our experiments demonstrated that raw streams can be grossly grouped according to their average price changes over a fixed time period. if the average daily price changes are almost the same  the best parameter setting on one stream is almost the best setting on another stream. but the same setting may have quite different performance for two streams with different average price changes. the following discusses the performance of one group of streams whose average daily price changes are between $1 to $1. other groups have displayed the same pattern with different parameter setting.
1 experiments on similarity definition
　prediction accuracy using our new similarity measures is compared with similarity measures based on euclidean distance. the euclidean distance mentioned here allows subsequence similarity with price shifts and time scaling. each experimental set has more than 1 queries and 1 predictions. the comparison is based on the accuracy in trend prediction. figure 1a displays the percentage of correct predictions in 1 experimental sets. accuracy based on our new similarity measure  perm+amp   which uses an amplitude distance function over permutation  achieved superior results. the correctness of prediction based on the euclidean distance function  perm+euc  is 1% less than that based on our new similarity measure  perm+amp . trend predictions based on permutation only  perm only  and amplitude distance function only  amp only  are also summarized in figure 1a. their performance is much less accurate than the combined one. these results also prove that the two parts of our similarity definition are complementary to each other and both are important. the percentage of correct trend predictions decreases more than 1% if using similarity measure based on permutation only or the distance function only. the same conclusion can also be summarized with euclidean distance by comparing the performance among permutations only  perm only   euclidean distance only  euc only   and the combination of permutations and euclidean distance  perm+euc .
1 experiments on event-driven matching
　our query engine uses an event-driven similarity search mechanism instead of querying for a fixed period. the correctness of trend predictions is summarized in figure 1b. a series of experiments have been performed over different fixed time periods  ranging from 1 minute  ft1  to 1 minutes  ft1 . fti means query every i minutes. it is clearly shown that event driven subsequence similarity search  perm+amp  has outperformed search over any fixed period fti. the different fixed time period searches have almost the same average correctness. this is because we use the most recent end points to search the database and predict the movement for the query time. the query sequence does not concern how far away the query point to the last identified potential end point  the delay time . it is easy to understand that the closer the query point to the last end point  the shorter delay time  and thus the better prediction accuracy. the fixed time period queries have almost the same prediction correctness because the correctness is the average accuracy in prediction.
　figure 1c shows average delay time for different query mechanisms. the delay time for event-driven similarity was 1min  while the delays for all the fixed periods were all around 1min. this delay time explains the lower correctness in trend prediction with fixed time periods. each line segment covers about 1 raw data points. although search over a fixed period could gives better prediction when the query point is close to the last end points  there is more changes the query points with fixed period would be far away from last end point.
　
figure 1: subsequence similarity matching over differenct data streams.  a  correctness of trend predictions;  b the correspond-
　
ing data streams;  c  the unadjusted raw ebay stream.
1 experiments on data streams
　experiments on the correctness of trend prediction over different streams have been performed to test the effects by stream properties. figure 1a compares the correctness of trend predictions using different similarity measures   and
　　  and different search mechanisms over different data streams. and use event-driven similarity matching. is the average correctness of prediction with fixed time interval from 1min to 1min. it can be seen that the eventdriven similarity search using our new similarity measure has better performance in all the streams. figure 1b shows the raw data streams we used in our experiments whose results are in figure 1a. from looking at these raw data streams  we can see that our method works well for a wide variety of data streams. the correctness of trend predictions are more than 1% for all the streams. it works better when the market is in an overall bull/bear market  erts  cof and qlgc   where the trend predictions are more than 1% correct. even when the market is a no-trend market   and
          our prediction scheme still works well  with prediction correctness more than 1%. on the contrary  the correctness of predictions based on euclidean distance or with fixed time interval varies according to the characteristics of different streams  sometimes only achieving totally random predictions  1% correctness .
　the stream of ebay is of great interest because it requires dynamic adjustment as described in section 1. figure 1b shows the adjusted stream of ebay and figure 1c is the raw ebay stream without adjustments. figure 1c starts with a bear market. then it changes to a bull market  followed by a no-trend market  before the dashed line . at the time of the dashed line  the stock has a share split of 1  one share to two shares split  and the price has a sharp drop from $1 to $1. our event-driven subsequence similarity matching dynamically adjusts this special situation gracefully  with 1% correct predictions. the corresponding predictions with euclidean distance are only 1% correct. the correctness of predictions based on fixed time intervals is much worse  almost random - 1% correct .
1 experimentsonquerysubsequencelength and trend duration
　a series of experiments has been performed to test the effect of subsequence lengths and trend durations. for financial data analysis  the lengths of subsequences of the resulting plr range from 1 to 1. our experimental results show that when the average lengths between two adjacent plr end points is between 1 to 1  our method will have better prediction results. when the subsequence has 1 points  there are 1 intervals  line segments . thus the corresponding subsequence of raw streams contains 1 to 1 data points. when the subsequence has 1 points  there are 1 intervals. thus the corresponding subsequence of raw streams contain 1 to 1 data points.
　for each of the experiments shown in figure 1  we use the same streams and query periods using our method. we only vary the query subsequence length. for longer query subsequences  there are more permutations. hence there are fewer similar subsequences available for distance comparison. in spite of this  figure 1a shows that longer query subsequences result in better prediction correctness. figure 1b illustrates the relative number of similar subsequences  with same permutation  based on length. in addition  because there are fewer similar subsequences  the number of times a prediction can be made i.e.   up  or  dowm   but not  notrend   is smaller with longer query subsequences and this is also illustrated in figure 1b.
figure 1: trend predictions with different subsequence lengths and trend durations.  a  correctness of trend predictions with different subsequencelengths;  b  relative prediction accuracy with differentsubsequence lengths;  c  correctness of trend predictions　figure 1c shows the correctness of trend predictions with different trend duration . we can see that the closer the prediction events to the query events  the more accurate the predictions are. with different trend duration	.
1 experiments on cpu cost and query time
　we define the cpu cost as the average computation time for subsequence similarity matching by sequential scan. the relative cpu cost is measured relative to the cpu cost of ft1 one minute time periods . as shown in figure 1  it is easy to understand that the cpu cost decreases as the fixed query time intervals increase. it also shows cpucost of event-driven subsequence similarity matching is about the same as ft1  1-minute time periods . the cpu cost of similarity matching based on permutations and a distance function  amp+perm and euc+perm  is only a little higher based on permutations  perm only . this is because we store the permutations of historical data and thus save run time computations. we only compute the distance between the historical subsequences whose permutations is the same as that of the current query subsequence. it also explains why similarity matching methods based on distance functions only  amp only and euc only  have higher cpu cost. they do not have the pre-computed permutations as a filter  so they need to compute the distance between each historical subsequence and the query subsequence. so our event-driven similarity matching mechanism has greatly reduced the cpu cost.
　our event-driven similarity matching runs in real time. using similarity based on permutations and the amplitude distance function  the average response time for a single query is only 1 milliseconds for subsequences with length 1 and queries on the database with 1 plr end points  which corresponds to 1 1 raw stream data points. the raw financial data streams come in with irregular time intervals  and we aggregated the raw data with a fixed time interval  which is 1 minute in our case. a 1-millisecond responding time is fast enough for real-time predictions.
1. conclusions & future work
　in this paper  we introduced a new approach for event-driven subsequence similarity matching based on a newly defined subsequence similarity measure over financial data streams. upon studying the special requirements and real-time application needs of financial data analysis  we proposed a new simultaneous online segmentation and pruning algorithm for piecewise linear representation of raw financial data streams. the new algorithm used tiered

figure 1: relative cpu cost to evaluate query with different similarity measures and matching mechanisms.
processes for incremental segmentation. it features quick identification of new end points yet maintains accurate segmentation. we also defined a new subsequence similarity measure for subsequence matching. the new similarity measure is composed of two parts  a permutation and a distance function. experimental results showed it has better performance than subsequence similarity measures based on euclidean distance. an event-driven online subsequence similarity search approach is proposed  in which automatic online queries are generated only at a time when a line segment is generated. the new search mechanism had about 1 times less computational burden than the scheme to query at each time instance. performance experiments demonstrated that event-driven search outperformed the searches with any fixed time period. using the similarity search results as a guidance  we have achieved promising trend prediction correctness  average 1% . our approach works well for a wide variety of streams. the query response time is fast for real time applications.
　future research can proceed in several directions. our immediate plans include incorporating indexing in the search algorithm. since we have shown that our distance function is metric  a number of indexes  1  1  1  1  1  1  may be applicable. another possibility is to explore a weighted statistical function to improve trend prediction. the weighted statistics will consider the effects of similarity distance and time difference. still another problem is finding an algorithm for dynamic clustering of multiple streams. last  scheduling and concurrency control of multiple queries over massive data streams in real-time applications is also of great research interest.
