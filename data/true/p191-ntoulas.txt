the web search engines maintain large-scale inverted indexes which are queried thousands of times per second by users eager for information. in order to cope with the vast amounts of query loads  search engines prune their index to keep documents that are likely to be returned as top results  and use this pruned index to compute the first batches of results. while this approach can improve performance by reducing the size of the index  if we compute the top results only from the pruned index we may notice a significant degradation in the result quality: if a document should be in the top results but was not included in the pruned index  it will be placed behind the results computed from the pruned index. given the fierce competition in the online search market  this phenomenon is clearly undesirable.
　in this paper  we study how we can avoid any degradation of result quality due to the pruning-based performance optimization  while still realizing most of its benefit. our contribution is a number of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guarantees that the top-matching pages are always placed at the top search results  even though we are computing the first batch from the pruned index most of the time. we also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of 1 million web pages.
categories and subject descriptors
h.1  information storage and retrieval : content analysis and indexing; h.1  information storage and retrieval : information search and retrieval
general terms
algorithms  measuring  performance  design  experimentation
keywords
inverted index  pruning  correctness guarantee  web search engines
1. introduction
　the amount of information on the web is growing at a prodigious rate . according to a recent study   it is estimated that the

 work done while author was at ucla computer science department.  this work is partially supported by nsf grants  iis-1  iis1  and cns-1. any opinions  findings  and conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect the views of the funding institutions.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
web currently consists of more than 1 billion pages. due to this immense amount of available information  the users are becoming more and more dependent on the web search engines for locating relevant information on the web. typically  the web search engines  similar to other information retrieval applications  utilize a data structure called inverted index. an inverted index provides for the efficient retrieval of the documents  or web pages  that contain a particular keyword.
　in most cases  a query that the user issues may have thousands or even millions of matching documents. in order to avoid overwhelming the users with a huge amount of results  the search engines present the results in batches of 1 to 1 relevant documents. the user then looks through the first batch of results and  if she doesn't find the answer she is looking for  she may potentially request to view the next batch or decide to issue a new query.
　a recent study  indicated that approximately 1% of the users examine at most the first 1 batches of the results. that is  1% of the users typically view at most 1 to 1 results for every query that they issue to a search engine. at the same time  given the size of the web  the inverted index that the search engines maintain can grow very large. since the users are interested in a small number of results  and thus are viewing a small portion of the index for every query that they issue   using an index that is capable of returning all the results for a query may constitute a significant waste in terms of time  storage space and computational resources  which is bound to get worse as the web grows larger over time .
　one natural solution to this problem is to create a small index on a subset of the documents that are likely to be returned as the top results  by using  for example  the pruning techniques in  1  1   and compute the first batch of answers using the pruned index. while this approach has been shown to give significant improvement in performance  it also leads to noticeable degradation in the quality of the search results  because the top answers are computed only from the pruned index  1  1 . that is  even if a page should be placed as the top-matching page according to a search engine's ranking metric  the page may be placed behind the ones contained in the pruned index if the page did not become part of the pruned index for various reasons  1  1 . given the fierce competition among search engines today this degradation is clearly undesirable and needs to be addressed if possible.
　in this paper  we study how we can avoid any degradation of search quality due to the above performance optimization while still realizing most of its benefit. that is  we present a number of simple  yet important  changes in the pruning techniques for creating the pruned index. our main contribution is a new answer computation algorithm that guarantees that the top-matching pages  according to the search-engine's ranking metric  are always placed at the top of search results  even though we are computing the first batch of answers from the pruned index most of the time. these enhanced pruning techniques and answer-computation algorithms are explored in the context of the cluster architecture commonly employed by today's search engines. finally  we study and present how search engines can minimize the operational cost of answering queries while providing high quality search results.
ip
ip
	 a 	 b 
figure 1:  a  search engine replicates its full index if to increase query-answering capacity.  b  in the 1st tier  small pindexes ip handle most of the queries. when ip cannot answer a query  it is redirected to the 1nd tier  where the full index if is used to compute the answer.
1. cluster architecture and cost savings from a pruned index
　typically  a search engine downloads documents from the web and maintains a local inverted index that is used to answer queries quickly.
inverted indexes. assume that we have collected a set of documents d = {d1 ... dm} and that we have extracted all the terms t = {t1 ... tn} from the documents. for every single term ti （ t we maintain a list i ti  of document ids that contain ti. every entry in i ti  is called a posting and can be extended to include additional information  such as how many times ti appears in a document  the positions of ti in the document  whether ti is bold/italic  etc. the set of all the lists i = {i t1  ... i tn } is our inverted index.
1 two-tier index architecture
　search engines are accepting an enormous number of queries every day from eager users searching for relevant information. for example  google is estimated to answer more than 1 million user queries per day. in order to cope with this huge query load  search engines typically replicate their index across a large cluster of machines as the following example illustrates:
example 1 consider a search engine that maintains a cluster of machines as in figure 1 a . the size of its full inverted index if is larger than what can be stored in a single machine  so each copy of if is stored across four different machines. we also suppose that one copy of if can handle the query load of 1 queries/sec. assuming that the search engine gets 1 queries/sec  it needs to replicate if five times to handle the load. overall  the search engine needs to maintain 1 〜 1 = 1 machines in its cluster. 
　while fully replicating the entire index if multiple times is a straightforward way to scale to a large number of queries  typical query loads at search engines exhibit certain localities  allowing for significant reduction in cost by replicating only a small portion of the full index. in principle  this is typically done by pruning a full index if to create a smaller  pruned index  or p-index  ip  which contains a subset of the documents that are likely to be returned as top results.
　given the p-index  search engines operate by employing a twotier index architecture as we show in figure 1 b : all incoming queries are first directed to one of the p-indexes kept in the 1st tier. in the cases where a p-index cannot compute the answer  e.g. was unable to find enough documents to return to the user  the query is answered by redirecting it to the 1nd tier  where we maintain a full index if. the following example illustrates the potential reduction in the query-processing cost by employing this two-tier index architecture.
example 1 assume the same parameter settings as in example 1.
that is  the search engine gets a query load of 1 queries/sec
algorithm 1	computation of answer with correctness guarantee
input q =  {t1 ... tn}  i i + k   where
{t1 ... tn}: keywords in the query
            i i + k : range of the answer to return procedure
 1   a c  = computeanswer q ip 
 1  if  c = 1  then
 1  return a
 1  else
 1  a = computeanswer q if 
 1  return afigure 1: computing the answer under the two-tier architecture with the result correctness guarantee.
and every copy of an index  both the full if and p-index ip  can handle up to 1 queries/sec. also assume that the size of ip is one fourth of if and thus can be stored on a single machine. finally  suppose that the p-indexes can handle 1% of the user queries by themselves and only forward the remaining 1% queries to if. under this setting  since all 1/sec user queries are first directed to a p-index  five copies of ip are needed in the 1st tier. for the 1nd tier  since 1%  or 1 queries/sec  are forwarded  we need to maintain one copy of if to handle the load. overall we need a total of 1 machines  five machines for the five copies of ip and four machines for one copy of if . compared to example 1  this is more than 1% reduction in the number of machines. 
　the above example demonstrates the potential cost saving achieved by using a p-index. however  the two-tier architecture may have a significant drawback in terms of its result quality compared to the full replication of if; given the fact that the p-index contains only a subset of the data of the full index  it is possible that  for some queries  the p-index may not contain the top-ranked document according to the particular ranking criteria used by the search engine and fail to return it as the top page  leading to noticeable quality degradation in search results. given the fierce competition in the online search market  search engine operators desperately try to avoid any reduction in search quality in order to maximize user satisfaction.
1 correctness guarantee under two-tier architecture
　how can we avoid the potential degradation of search quality under the two-tier architecture  our basic idea is straightforward: we use the top-k result from the p-index only if we know for sure that the result is the same as the top-k result from the full index. the algorithm in figure 1 formalizes this idea. in the algorithm  when we compute the result from ip  step 1   we compute not only the top-k result a  but also the correctness indicator function c defined as follows:
definition 1  correctness indicator function  given a query q  the p-index ip returns the answer a together with a correctness indicator function c. c is set to 1 if a is guaranteed to be identical  i.e. same results in the same order  to the result computed from the full index if. if it is possible that a is different  c is set to 1. 
　note that the algorithm returns the result from ip  step 1  only when it is identical to the result from if  condition c = 1 in step 1 . otherwise  the algorithm recomputes and returns the result from the full index if  step 1 . therefore  the algorithm is guaranteed to return the same result as the full replication of if all the time.
　now  the real challenge is to find out  1  how we can compute the correctness indicator function c and  1  how we should prune the index to make sure that the majority of queries are handled by ip alone.
question 1 how can we compute the correctness indicator function c 
　a straightforward way to calculate c is to compute the top-k answer both from ip and if and compare them. this naive solution  however  incurs a cost even higher than the full replication of if because the answers are computed twice: once from ip and once from if. is there any way to compute the correctness indicator function c only from ip without computing the answer from if 
question 1 how should we prune if to ip to realize the maximum cost saving 
　the effectiveness of algorithm 1 critically depends on how often the correctness indicator function c is evaluated to be 1. if c = 1 for all queries  for example  the answers to all queries will be computed twice  once from ip  step 1  and once from if  step 1   so the performance will be worse than the full replication of if. what will be the optimal way to prune if to ip  such that c = 1 for a large fraction of queries  in the next few sections  we try to address these questions.
1. optimal size of the p-index
　intuitively  there exists a clear tradeoff between the size of ip and the fraction of queries that ip can handle: when ip is large and has more information  it will be able to handle more queries  but the cost for maintaining and looking up ip will be higher. when ip is small  on the other hand  the cost for ip will be smaller  but more queries will be forwarded to if  requiring us to maintain more copies of if. given this tradeoff  how should we determine the optimal size of ip in order to maximize the cost saving  to find the answer  we start with a simple example.
example 1 again  consider a scenario similar to example 1  where the query load is 1 queries/sec  each copy of an index can handle 1 queries/sec  and the full index spans across 1 machines. but now  suppose that if we prune if by 1% to ip1  i.e.  the size of ip1 is 1% of if   ip1 can handle 1% of the queries  i.e.  c = 1 for 1% of the queries . also suppose that if if is pruned by 1% to ip1  ip1 can handle 1% of the queries. which one of the ip1  ip1 is preferable for the 1st-tier index 
　to find out the answer  we first compute the number of machines needed when we use ip1 for the 1st tier. at the 1st tier  we need 1 copies of ip1 to handle the query load of 1 queries/sec. since the size of ip1 is 1% of if  that requires 1 machines   one copy of ip1 requires one machine. therefore  the total number of machines required for the 1st tier is 1〜1 = 1  1 copies of ip1 with 1 machine per copy . also  since ip1 can handle 1% of the queries  the 1nd tier has to handle 1 queries/sec  1% of the 1 queries/sec   so we need a total of 1〜1 = 1 machines for the 1nd tier  1 copies of if with 1 machines per copy . overall  when we use ip1 for the 1st tier  we need 1 + 1 = 1 machines to handle the load. we can do similar analysis when we use ip1 and see that a total of 1 machines are needed when ip1 is used. given this result  we can conclude that using ip1 is preferable. 
　the above example shows that the cost of the two-tier architecture depends on two important parameters: the size of the p-index and the fraction of the queries that can be handled by the 1st tier index alone. we use s to denote the size of the p-index relative to if  i.e.  if s = 1  for example  the p-index is 1% of the size of if . we use f s  to denote the fraction of the queries that a p-index of size s can handle  i.e.  if f s  = 1  1% of the queries return the value c = 1 from ip . in general  we can expect that f s  will increase as s gets larger because ip can handle more queries as its size grows. in figure 1  we show an example graph of f s  over s.
　given the notation  we can state the problem of p-index-size optimization as follows. in formulating the problem  we assume that the number of machines required to operate a two-tier architecture

figure 1: example function showing the fraction of guaranteed queries f s  at a given size s of the p-index.
is roughly proportional to the total size of the indexes necessary to handle the query load.
problem 1  optimal index size  given a query load q and the function f s   find the optimal p-index size s that minimizes the total size of the indexes necessary to handle the load q. 
　the following theorem shows how we can determine the optimal index size.
theorem 1 the cost for handling the query load q is minimal when the size of the p-index  s  satisfies. 
proof the proof of this and the following theorems is omitted due to space constraints.
　this theorem shows that the optimal point is when the slope of the f s  curve is 1. for example  in figure 1  the optimal size is when s = 1. note that the exact shape of the f s  graph may vary depending on the query load and the pruning policy. for example  even for the same p-index  if the query load changes significantly  fewer  or more  queries may be handled by the p-index  decreasing  or increasing f s . similarly  if we use an effective pruning policy  more queries will be handled by ip than when we use an ineffective pruning policy  increasing f s . therefore  the function f s  and the optimal-index size may change significantly depending on the query load and the pruning policy. in our later experiments  however  we find that even though the shape of the f s  graph changes noticeably between experiments  the optimal index size consistently lies between 1%-1% in most experiments.
1. pruning policies
　in this section  we show how we should prune the full index if to ip  so that  1  we can compute the correctness indicator function c from ip itself and  1  we can handle a large fraction of queries by ip. in designing the pruning policies  we note the following two localities in the users' search behavior:
1. keyword locality: although there are many different words in the document collection that the search engine indexes  a few popular keywords constitute the majority of the query loads. this keyword locality implies that the search engine will be able to answer a significant fraction of user queries even if it can handle only these few popular keywords.
1. document locality: even if a query has millions of matching documents  users typically look at only the first few results . thus  as long as search engines can compute the first few top-k answers correctly  users often will not notice that the search engine actually has not computed the correct answer for the remaining results  unless the users explicitly request them .
　based on the above two localities  we now investigate two different types of pruning policies:  1  a keyword pruning policy  which takes advantage of the keyword locality by pruning the whole inverted list i ti  for unpopular keywords ti's and  1  a document pruning policy  which takes advantage of the document locality by keeping only a few postings in each list i ti   which are likely to be included in the top-k results.
　as we discussed before  we need to be able to compute the correctness indicator function from the pruned index alone in order to provide the correctness guarantee. since the computation of correctness indicator function may critically depend on the particular ranking function used by a search engine  we first clarify our assumptions on the ranking function.
1 assumptions on ranking function
　consider a query q = {t1 t1 ... tw} that contains a subset of the index terms. the goal of the search engine is to return the documents that are most relevant to query q. this is done in two steps: first we use the inverted index to find all the documents that contain the terms in the query. second  once we have the relevant documents  we calculate the rank  or score  of each one of the documents with respect to the query and we return to the user the documents that rank the highest.
　most of the major search engines today return documents containing all query terms  i.e. they use and-semantics . in order to make our discussions more concise  we will also assume the popular and-semantics while answering a query. it is straightforward to extend our results to or-semantics as well. the exact ranking function that search engines employ is a closely guarded secret. what is known  however  is that the factors in determining the document ranking can be roughly categorized into two classes:
query-dependent relevance. this particular factor of relevance captures how relevant the query is to every document. at a high level  given a document d  for every term ti a search engine assigns a term relevance score tr d ti  to d. given the tr d ti  scores for every ti  then the query-dependent relevance of d to the query  noted as tr d q   can be computed by combining the individual term relevance values. one popular way for calculating the query- dependent relevance is to represent both the document d and the query q using the tf.idf vector space model  and employ a cosine distance metric.
　since the exact form of tr d ti  and tr d q  differs depending on the search engine  we will not restrict to any particular form; instead  in order to make our work applicable in the general case  we will make the generic assumption that the query-dependent relevance is computed as a function of the individual term relevance values in the query:
	tr d q  = ftr tr d t1  ... tr d tw  	 1 
query-independent document quality. this is a factor that measures the overall  quality  of a document d independent of the particular query issued by the user. popular techniques that compute the general quality of a page include pagerank   hits  and the likelihood that the page is a  spam  page  1  1 . here  we will use pr d  to denote this query-independent part of the final ranking function for document d.
　the final ranking score r d q  of a document will depend on both the query-dependent and query-independent parts of the ranking function. the exact combination of these parts may be done in a variety of ways. in general  we can assume that the final ranking score of a document is a function of its query-dependent and query-independent relevance scores. more formally:
	r d q  = fr tr d q  pr d  	 1 
for example  fr tr d q  pr d   may take the form fr tr d q  pr d   = α ， tr d q  +  1   α  ， pr d   thus giving weight α to the query-dependent part and the weight 1   α to the query-independent part.
　in equations 1 and 1 the exact form of fr and ftr can vary depending on the search engine. therefore  to make our discussion applicable independent of the particular ranking function used by search engines  in this paper  we will make only the generic assumption that the ranking function r d q  is monotonic on its parameters tr d t1  ... tr d tw  and pr d .

figure 1: keyword and document pruning.
algorithm 1	computation of c for keyword pruning
procedure
 1  c = 1
 1  foreach ti （ q
 1  if  i ti  （/ ip  then c = 1
 1  return cfigure 1: result guarantee in keyword pruning.
definition 1 a function f α β ... ω  is monotonic if  α1 − α1   β1 − β1  ...  ω1 − ω1 it holds that: f α1 β1 ... ω1  − f α1 β1 ... ω1 .
　roughly  the monotonicity of the ranking function implies that  between two documents d1 and d1  if d1 has higher querydependent relevance than d1 and also a higher query-independent score than d1  then d1 should be ranked higher than d1  which we believe is a reasonable assumption in most practical settings.
1 keyword pruning
　given our assumptions on the ranking function  we now investigate the  keyword pruning  policy  which prunes the inverted index if  horizontally  by removing the whole i ti 's corresponding to the least frequent terms. in figure 1 we show a graphical representation of keyword pruning  where we remove the inverted lists for t1 and t1  assuming that they do not appear often in the query load.
　note that after keyword pruning  if all keywords {t1 ... tn} in the query q appear in ip  the p-index has the same information as if as long as q is concerned. in other words  if all keywords in q appear in ip  the answer computed from ip is guaranteed to be the same as the answer computed from if. figure 1 formalizes this observation and computes the correctness indicator function c for a keyword-pruned index ip. it is straightforward to prove that the answer from ip is identical to that from if if c = 1 in the above algorithm.
　we now consider the issue of optimizing the ip such that it can handle the largest fraction of queries. this problem can be formally stated as follows:
problem 1  optimal keyword pruning  given the query load q and a goal index size s ， |if| for the pruned index  select the inverted lists ip = {i t1  ... i th } such that |ip| ＋ s ， |if| and the fraction of queries that ip can answer  expressed by f s   is maximized.	
unfortunately  the optimal solution to the above problem is intractable as we can show by reducing from knapsack  we omit the complete proof .
theorem 1 the problem of calculating the optimal keyword pruning is np-hard.	
　given the intractability of the optimal solution  we need to resort to an approximate solution. a common approach for similar knapsack problems is to adopt a greedy policy by keeping the items with the maximum benefit per unit cost . in our context  the potential benefit of an inverted list i ti  is the number of queries that can be answered by ip when i ti  is included in ip. we approximate this number by the fraction of queries in the query load q that include the term ti and represent it as p ti . for example  if 1 out of 1 queries contain the term computer 
algorithm 1	greedy keyword pruning hs procedure
 1   ti  calculate	.
 1  include the inverted lists with the highest
hs ti  values such that |ip| ＋ s ， |if|.figure 1: approximation algorithm for the optimal keyword pruning.
algorithm 1	global document pruning v sg
procedure
 1  sort all documents di based on pr di 
 1  find the threshold value τp  such that only s fraction of the documents have pr di    τp
	 1 	keep di in the inverted lists if pr di    τpfigure 1: global document pruning based on pr.
then p computer  = 1. the cost of including i ti  in the pindex is its size |i ti |. thus  in our greedy approach in figure 1  we include i ti 's in the decreasing order of p ti /|i ti | as long as |ip| ＋ s ， |if|. later in our experiment section  we evaluate what fraction of queries can be handled by ip when we employ this greedy keyword-pruning policy.
1 document pruning
　at a high level  document pruning tries to take advantage of the observation that most users are mainly interested in viewing the top few answers to a query. given this  it is unnecessary to keep all postings in an inverted list i ti   because users will not look at most of the documents in the list anyway. we depict the conceptual diagram of the document pruning policy in figure 1. in the figure  we  vertically prune  postings corresponding to d1 d1 and d1 of t1 and d1 of t1  assuming that these documents are unlikely to be part of top-k answers to user queries. again  our goal is to develop a pruning policy such that  1  we can compute the correctness indicator function c from ip alone and  1  we can handle the largest fraction of queries with ip. in the next few sections  we discuss a few alternative approaches for document pruning.
1.1 global pr-based pruning
　we first investigate the pruning policy that is commonly used by existing search engines. the basic idea for this pruning policy is that the query-independent quality score pr d  is a very important factor in computing the final ranking of the document  e.g. pagerank is known to be one of the most important factors determining the overall ranking in the search results   so we build the p-index by keeping only those documents whose pr values are high  i.e.  pr d    τp for a threshold value τp . the hope is that most of the top-ranked results are likely to have high pr d  values  so the answer computed from this p-index is likely to be similar to the answer computed from the full index. figure 1 describes this pruning policy more formally  where we sort all documents di's by their respective pr di  values and keep a di in the p-index when its
algorithm 1	local document pruning v sl
n: maximum size of a single posting list
procedure
 1  foreach i ti  （ if
 1  sort di's in i ti  based on pr di 
 1  if |i ti | ＋ n then keep all di's
 1  else keep the top-n di's with the highest pr di figure 1: local document pruning based on pr.
algorithm 1	extended keyword-specific document pruning
procedure
 1  for each i ti 
 1  keep d （ i ti  if pr d    τpi or tr d ti    τtifigure 1: extended keyword-specific document pruning based on pr and tr.
pr di  value is higher than the global threshold value τp. we refer to this pruning policy as global pr-based pruning  gpr .
　variations of this pruning policy are possible. for example  we may adjust the threshold value τp locally for each inverted list i ti   so that we maintain at least a certain number of postings for each inverted list i ti . this policy is shown in figure 1. we refer to this pruning policy as local pr-based pruning  lpr . unfortunately  the biggest shortcoming of this policy is that we can prove that we cannot compute the correctness function c from ip alone when ip is constructed this way.
theorem 1 no pr-based document pruning can provide the result guarantee.	
proof assume we create ip based on the gpr policy  generalizing the proof to lpr is straightforward  and that every document d with pr d    τp is included in ip. assume that the kth entry in the top-k results  has a ranking score of r dk q  = fr tr dk q  pr dk  . now consider another document dj that was pruned from ip because pr dj    τp. even so  it is still possible that the document's tr dj q  value is very high such that r dj q  = fr tr dj q  pr dj     r dk q . 
therefore  under a pr-based pruning policy  the quality of the answer computed from ip can be significantly worse than that from if and it is not possible to detect this degradation without computing the answer from if. in the next section  we propose simple yet essential changes to this pruning policy that allows us to compute the correctness function c from ip alone.
1.1 extended keyword-specific pruning
　the main problem of global pr-based document pruning policies is that we do not know the term-relevance score tr d ti  of the pruned documents  so a document not in ip may have a higher ranking score than the ones returned from ip because of their high tr scores.
　here  we propose a new pruning policy  called extended keyword-specific document pruning  eks   which avoids this problem by pruning not just based on the query-independent pr d  score but also based on the term-relevance tr d ti  score. that is  for every inverted list i ti   we pick two threshold values  τpi for pr and τti for tr  such that if a document d （ i ti  satisfies pr d    τpi or tr d ti    τti  we include it in i ti  of ip. otherwise  we prune it from ip. figure 1 formally describes this algorithm. the threshold values  τpi and τti  may be selected in a number of different ways. for example  if pr and tr have equal weight in the final ranking and if we want to keep at most n postings in each inverted list i ti   we may want to set the two threshold values equal to τi  τpi = τti = τi  and adjust τi such that n postings remain in i ti .
　this new pruning policy  when combined with a monotonic scoring function  enables us to compute the correctness indicator function c from the pruned index. we use the following example to explain how we may compute c.
example 1 consider the query q = {t1 t1} and a monotonic ranking function  f pr d  tr d t1  tr d t1  . there are three possible scenarios on how a document d appears in the pruned index ip.
1. d appears in both i t1  and i t1  of ip: since complete information of d appears in ip  we can compute the exact
algorithm 1	computing answer from ip
input query q = {t1 ... tw}
output a: top-k result  c: correctness indicator function procedure
 1  for each di （ i t1  “ ，，， “ i tw 
 1  for each tm （ q
 1  if di （ i tm 
 1  tr  di tm  = tr di tm 
 1  else
 1  tr  di tm  = τtm
 1  f di  = f pr di  tr  di t1  ... tr  di tn  
 1  a = top-k di's with highest f di  values
 1  c = j 1 otherwisei （ a appear in all i ti   ti （ q 1 if all dfigure 1: ranking based on thresholds trτ ti  and prτ ti .
score of d based on pr d   tr d t1  and tr d t1  values in ip: f pr d  tr d t1  tr d t1  .
1. d appears only in i t1  but not in i t1 : since d does not appear in i t1   we do not know tr d t1   so we cannot compute its exact ranking score. however  from our pruning criteria  we know that tr d t1  cannot be larger than the threshold value τt1. therefore  from the monotonicity of f  definition 1   we know that the ranking score of d  f pr d  tr d t1  tr d t1    cannot be larger than f pr d  tr d t1  τt1 .
1. d does not appear in any list: since d does not appear at all in ip  we do not know any of the pr d   tr d t1   tr d t1  values. however  from our pruning criteria  we know that pr d  ＋ τp1 and ＋ τp1 and that tr d t1  ＋ τt1 and tr d t1  ＋ τt1. therefore  from the monotonicity of f  we know that the ranking score of d  cannot be larger than f min τp1 τp1  τt1 τt1 .	
　the above example shows that when a document does not appear in one of the inverted lists i ti  with ti （ q  we cannot compute its exact ranking score  but we can still compute its upper bound score by using the threshold value τti for the missing values. this suggests the algorithm in figure 1 that computes the top-k result a from ip together with the correctness indicator function c. in the algorithm  the correctness indicator function c is set to one only if all documents in the top-k result a appear in all inverted lists i ti  with ti （ q  so we know their exact score. in this case  because these documents have scores higher than the upper bound scores of any other documents  we know that no other documents can appear in the top-k. the following theorem formally proves the correctness of the algorithm. in  fagin et al.  provides a similar proof in the context of multimedia middleware.
theorem 1 given an inverted index ip pruned by the algorithm in figure 1  a query q = {t1 ... tw} and a monotonic ranking function  the top-k result from ip computed by algorithm 1 is the same as the top-k result from if if c = 1. 
proof let us assume dk is the kth ranked document computed from ip according to algorithm 1. for every document di （ if that is not in the top-k result from ip  there are two possible scenarios:
　first  di is not in the final answer because it was pruned from all inverted lists i tj  1 ＋ j ＋ w  in ip. in this case  we know that pr di  ＋ min1＋j＋wτpj   pr dk  and that tr di tj  ＋ τtj   tr dk tj  1 ＋ j ＋ w. from the monotonicity assumption  it follows that the ranking score of di is r di    r dk . that is  di's score can never be larger than that of dk.
　second  di is not in the answer because di is pruned from some inverted lists  say  i t1  ... i tm   in ip. let us assume r． di  = f pr di  τt1 ... τtm tr di tm+1  ... tr di tw  . then  from tr di tj  ＋ τtj 1 ＋ j ＋ m  and the monotonicity assumption 

figure 1: fraction of guaranteed queries f s  answered in a keyword-pruned p-index of size s.
we know that r di  ＋ r． di . also  algorithm 1 sets c = 1 only when the top-k documents have scores larger than r． di .
therefore  r di  cannot be larger than r dk .	
1. experimental evaluation
　in order to perform realistic tests for our pruning policies  we implemented a search engine prototype. for the experiments in this paper  our search engine indexed about 1 million pages  crawled from the web during march of 1. the crawl started from the open directory's  homepage and proceeded in a breadth-first manner. overall  the total uncompressed size of our crawled web pages is approximately 1 tb  yielding a full inverted index if of approximately 1 tb.
　for the experiments reported in this section we used a real set of queries issued to looksmart  on a daily basis during april of 1. after keeping only the queries containing keywords that were present in our inverted index  we were left with a set of about 1 million queries. within our query set  the average number of terms per query is 1 and 1% of the queries contain at most 1 terms.
　some experiments require us to use a particular ranking function. for these  we use the ranking function similar to the one used in . more precisely  our ranking function r d q  is
           r d q  = prnorm d + trnorm d q   1  where prnorm d  is the normalized pagerank of d computed from the downloaded pages and trnorm d q  is the normalized tf.idf cosine distance of d to q. this function is clearly simpler than the real functions employed by commercial search engines  but we believe for our evaluation this simple function is adequate  because we are not studying the effectiveness of a ranking function  but the effectiveness of pruning policies.
1 keyword pruning
　in our first experiment we study the performance of the keyword pruning  described in section 1. more specifically  we apply the algorithm hs of figure 1 to our full index if and create a keyword-pruned p-index ip of size s. for the construction of our keyword-pruned p-index we used the query frequencies observed during the first 1 days of our data set. then  using the remaining 1-day query load  we measured f s   the fraction of queries handled by ip. according to the algorithm of figure 1  a query can be handled by ip  i.e.  c = 1  if ip includes the inverted lists for all of the query's keywords.
　we have repeated the experiment for varying values of s  picking the keywords greedily as discussed in section 1.the result is shown in figure 1. the horizontal axis denotes the size s of the p-index as a fraction of the size of if. the vertical axis shows the fraction f s  of the queries that the p-index of size s can answer. the results of figure 1  are very encouraging: we can answer a significant fraction of the queries with a small fraction of the original index. for example  approximately 1% of the queries can be answered using 1% of the original index. also  we find that when we use the keyword pruning policy only  the optimal index size is s = 1.
fraction of queries guaranteed for top-1 per fraction of index

figure 1: fraction of guaranteed queries f s  answered in a document-pruned p-index of size s.
fraction of queries answered for top-1 per fraction of index

figure 1: fraction of queries answered in a document-pruned p-index of size s.
1 document pruning
　we continue our experimental evaluation by studying the performance of the various document pruning policies described in section 1. for the experiments on document pruning reported here we worked with a 1% sample of the whole query set. the reason behind this is merely practical: since we have much less machines compared to a commercial search engine it would take us about a year of computation to process all 1 million queries.
　for our first experiment  we generate a document-pruned p-index of size s by using the extended keyword-specific pruning  eks  in section 1. within the p-index we measure the fraction of queries that can be guaranteed  according to theorem 1  to be correct. we have performed the experiment for varying index sizes s and the result is shown in figure 1. based on this figure  we can see that our document pruning algorithm performs well across the scale of index sizes s: for all index sizes larger than 1%  we can guarantee the correct answer for about 1% of the queries. this implies that our eks algorithm can successfully identify the necessary postings for calculating the top-1 results for 1% of the queries by using at least 1% of the full index size. from the figure  we can see that the optimal index size s = 1 when we use eks as our pruning policy.
　we can compare the two pruning schemes  namely the keyword pruning and eks  by contrasting figures 1 and 1. our observation is that  if we would have to pick one of the two pruning policies  then the two policies seem to be more or less equivalent for the p-index sizes s ＋ 1%. for the p-index sizes s   1%  keyword pruning does a much better job as it provides a higher number of guarantees at any given index size. later in section 1  we discuss the combination of the two policies.
　in our next experiment  we are interested in comparing eks with the pr-based pruning policies described in section 1. to this end  apart from eks  we also generated document-pruned pindexes for the global pr-based pruning  gpr  and the local prbased pruning  lpr  policies. for each of the polices we created document-pruned p-indexes of varying sizes s. since gpr and lpr cannot provide a correctness guarantee  we will compare the fraction of queries from each policy that are identical  i.e. the same results in the same order  to the top-k results calculated from the full index. here  we will report our results for k = 1; the results are similar for other values of k. the results are shown in figure 1.
average fraction of docs in answer for top-1 per fraction of index

figure 1: average fraction of the top-1 results of p-index with size s contained in top-1 results of the full index.
fraction of queries guaranteed for top-1 per fraction of index  using keyword and document

figure 1: combining keyword and document pruning.
the horizontal axis shows the size s of the p-index; the vertical axis shows the fraction f s  of the queries whose top-1 results are identical to the top-1 results of the full index  for a given size s.
　by observing figure 1  we can see that gpr performs the worst of the three policies. on the other hand eks  picks up early  by answering a great fraction of queries  about 1%  correctly with only 1% of the index size. the fraction of queries that lpr can answer remains below that of eks until about s = 1%. for any index size larger than 1%  lpr performs the best.
　in the experiment of figure 1  we applied the strict definition that the results of the p-index have to be in the same order as the ones of the full index. however  in a practical scenario  it may be acceptable to have some of the results out of order. therefore  in our next experiment we will measure the fraction of the results coming from an p-index that are contained within the results of the full index. the result of the experiment is shown on figure 1. the horizontal axis is  again  the size s of the p-index; the vertical axis shows the average fraction of the top-1 results common with the top-1 results from the full index. overall  figure 1 depicts that eks and lpr identify the same high  「 1%  fraction of results on average for any size s − 1%  with gpr not too far behind.
1 combining keyword and document pruning
　in sections 1 and 1 we studied the individual performance of our keyword and document pruning schemes. one interesting question however is how do these policies perform in combination  what fraction of queries can we guarantee if we apply both keyword and document pruning in our full index if 
　to answer this question  we performed the following experiment. we started with the full index if and we applied keyword pruning to create an index iph of size sh ， 1% of if. after that  we further applied document pruning to iph  and created our final pindex ip of size sv，1% of iph. we then calculated the fraction of guaranteed queries in ip. we repeated the experiment for different values of sh and sv. the result is shown on figure 1. the x-axis shows the index size sh after applying keyword pruning; the y-axis shows the index size sv after applying document pruning; the z-axis shows the fraction of guaranteed queries after the two prunings. for example the point  1  1  1  means that if we apply keyword pruning and keep 1% of if  and subsequently on the resulting index we apply document pruning keeping 1%  thus creating a pindex of size 1%，1% = 1% of if  we can guarantee 1% of the queries. by observing figure 1  we can see that for p-index sizes smaller than 1%  our combined pruning does relatively well. for example  by performing 1% keyword and 1% document pruning  which translates to a pruned index with s = 1  we can provide a guarantee for about 1% of the queries. in figure 1  we also observe a  plateau  for sh   1 and sv   1. for this combined pruning policy  the optimal index size is at s = 1  with sh = 1 and sv = 1.
1. related work
　 1  1  provide a good overview of inverted indexing in web search engines and ir systems. experimental studies and analyses of various partitioning schemes for an inverted index are presented in  1  1  1 . the pruning algorithms that we have presented in this paper are independent of the partitioning scheme used.
　the works in  1  1  1  1  1  are the most related to ours  as they describe pruning techniques based on the idea of keeping the postings that contribute the most in the final ranking. however   1  1  1  1  do not consider any query-independent quality  such as pagerank  in the ranking function.  presents a generic framework for computing approximate top-k answers with some probabilistic bounds on the quality of results. our work essentially extends  1  1  1  1  1  1  1  by proposing mechanisms for providing the correctness guarantee to the computed top-k results.
　search engines use various methods of caching as a means of reducing the cost associated with queries  1  1  1  1 . this thread of work is also orthogonal to ours because a caching scheme may operate on top of our p-index in order to minimize the answer computation cost. the exact ranking functions employed by current search engines are closely guarded secrets. in general  however  the rankings are based on query-dependent relevance and queryindependent document  quality.  query-dependent relevance can be calculated in a variety of ways  see  1  1  . similarly  there are a number of works that measure the  quality  of the documents  typically as captured through link-based analysis  1  1  1 . since our work does not assume a particular form of ranking function  it is complementary to this body of work.
　there has been a great body of work on top-k result calculation. the main idea is to either stop the traversal of the inverted lists early  or to shrink the lists by pruning postings from the lists  1  1  1  1 . our proof for the correctness indicator function was primarily inspired by .
1. concluding remarks
　web search engines typically prune their large-scale inverted indexes in order to scale to enormous query loads. while this approach may improve performance  by computing the top results from a pruned index we may notice a significant degradation in the result quality. in this paper  we provided a framework for new pruning techniques and answer computation algorithms that guarantee that the top matching pages are always placed at the top of search results in the correct order. we studied two pruning techniques  namely keyword-based and document-based pruning as well as their combination. our experimental results demonstrated that our algorithms can effectively be used to prune an inverted index without degradation in the quality of results. in particular  a keyword-pruned index can guarantee 1% of the queries with a size of 1% of the full index  while a document-pruned index can guarantee 1% of the queries with the same size. when we combine the two pruning algorithms we can guarantee 1% of the queries with an index size of 1%. it is our hope that our work will help search engines develop better  faster and more efficient indexes and thus provide for a better user search experience on the web.
