in many applications involving continuous data streams  data arrival is bursty and data rate fluctuates over time. systems that seek to give rapid or real-time query responses in such an environment must be prepared to deal gracefully with bursts in data arrival without compromising system performance. we discuss one strategy for processing bursty streams - adaptive  load-aware scheduling of query operators to minimize resource consumption during times of peak load. we show that the choice of an operator scheduling strategy can have significant impact on the run-time system memory usage. we then present chain scheduling  an operator scheduling strategy for data stream systems that is near-optimal in minimizing run-time memory usage for any collection of singlestream queries involving selections  projections  and foreign-key joins with stored relations. chain scheduling also performs well for queries with sliding-window joins over multiple streams  and multiple queries of the above types. a thorough experimental evaluation is provided where we demonstrate the potential benefits of chain scheduling  compare it with competing scheduling strategies  and validate our analytical conclusions.
1. introduction
　in a growing number of information processing applications  data takes the form of continuous data streams rather than traditional stored databases. these applications share two distinguishing characteristics that limit the applicability of standard relational database technology:  1  the volume of data is extremely high  and  1  on the basis of the data  decisions are arrived at and acted upon in close

 supported in part by a rambus corporation stanford graduate fellowship and nsf grant iis-1.
 supported in part by nsf grant iis-1.
 supported in part by siebel scholarship and nsf grant iis1.
′supported in part by nsf grant iis-1  an okawa foundation research grant  an snrc grant  and grants from microsoft and veritas.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
to real time. the combination of these two factors make traditional solutions  where effectively data is loaded into static databases for offline querying  impractical for many applications. motivating applications include networking  traffic engineering  network monitoring  intrusion detection   telecommunications  fraud detection  data mining   financial services  arbitrage  financial monitoring   ecommerce  clickstream analysis  personalization   and sensor networks.
　these applications have spawned a considerable and growing body of research into data stream processing   ranging from algorithms for data streams to full-fledged data stream systems such as aurora   hancock   niagara   stream   tangram  1  1   tapestry   telegraph   tribeca   and others. for the most part  research in data stream systems has hitherto focused on devising novel system architectures  defining query languages  designing space-efficient algorithms  and so on. important components of systems research that have received less attention to date are run-time resource allocation and optimization. in this paper we focus on one aspect of run-time resource allocation  namely operator scheduling.
　there are some features unique to data stream systems  as opposed to traditional relational dbmss  which make the run-time resource allocation problem different and arguably more critical: first  data stream systems are typically characterized by the presence of multiple long-running continuous queries. second  most data streams are quite irregular in their rate of arrival  exhibiting considerable burstiness and variation of data arrival rates over time. this phenomenon has been extensively studied in the networking context  1  1  1 . data network traffic is widely considered to be self-similar and to exhibit long-range dependence. similar findings have been reported for web-page access patterns and e-mail messages . consequently  conditions in which data stream queries are executed are frequently quite different from the conditions for which the query plans were generated. therefore  adaptivity becomes critical to a data stream system as compared to a traditional dbms.
　various approaches to adaptive query processing are possible given that the data may exhibit different types of variability. for example  a system could modify the structure of query plans  or dynamically reallocate memory among query operators in response to changing conditions  as suggested in   or take a holistic approach to adaptivity and do away with fixed query plans altogether  as in the eddies architecture  1  1 . while these approaches focus primarily on adapting to changing characteristics of the data itself  e.g.  changing selectivities   we focus on adaptivity towards changing arrival characteristics of the data  similar to  1  1  1 . as mentioned earlier  most data streams exhibit considerable burstiness and arrival-rate variation. it is crucial for any stream system to adapt gracefully to such variations in data arrival  making sure that we do not run out of critical resources such as main memory during the bursts. the focus of this paper is to design techniques for such adaptivity.
　when processing high-volume  bursty data streams  the natural way to cope with temporary bursts of unusually high rates of data arrival is to buffer the backlog of unprocessed tuples and work through them during periods of light load. however  it is important for the stream system to minimize the memory required for backlog buffering. otherwise  total memory usage can exceed the available physical memory during periods of heavy load  causing the system to page to disk  which often causes sudden and dramatic decrease in overall system throughput. as we will show in section 1  the operator scheduling strategy used by the data stream system has a significant impact on the total amount of memory required for backlog buffering. the question we address in this paper is how to most efficiently schedule the execution of query operators to keep the total memory required for backlog buffering at a minimum  assuming query plans and operator memory allocation are fixed.
1 road map
　the rest of this paper is organized as follows. we begin in section 1 by briefly describing our model for the processing of stream queries and illustrating through an example that the run-time memory requirement can be greatly affected by the operator scheduling strategy  thereby motivating the need for an intelligent scheduling strategy. in section 1 we formalize the problem of operator scheduling that we consider. our analytical results on the performance of proposed scheduling strategies are provided in section 1. in particular  we present chain scheduling  a near-optimal scheduling strategy for the case when all queries involve a single stream  possibly joining with stored relations via foreign-key joins. when joins with stored relations are not foreign-key joins  we present a weaker guarantee. in section 1 we introduce various other scheduling strategies and discuss  qualitatively  how they compare with chain scheduling. in section 1 we extend chain scheduling to queries involving windowed-joins over multiple streams. in section 1 we describe our experimental comparison of the performance of different scheduling strategies. we describe related work in section 1 before ending with our conclusions in section 1.
1. stream query processing: model and assumptions
　data stream systems are characterized by the presence of multiple continuous queries  1  1  1  1 . query execution can be conceptualized as a data flow diagram  as in    which is a directed acyclic graph  dag  of nodes and edges  where the nodes are pipelined operators  aka boxes  that process tuples and edges  aka arrows  represent composition of operators. an edge from node a to node b indicates that the output of node a is an input to node b. the edge  a b  also represents an input queue that buffers the output of operator a before it is input to the operator b. input data streams are represented as  leaf  nodes that have no input edges and query outputs are represented as  root  nodes that have no output edges.
　if a single data stream is input to multiple queries  we assume that multiple copies of the stream are created by the system and fed to each of the queries separately. consequently  we assume that all streams participate in only one query and therefore every incoming tuple is input to only a single query. if instead of making multiple copies  the system chooses to share the input buffer across multiple queries  the optimal strategy may differ; we plan to consider this case in future work. in this paper  we will concentrate on the case of a fixed query plan which does not change over time.
　as mentioned earlier  we assume that all operators execute in a streaming or pipelined manner. operators like select and project naturally fit in this category. a join operator can also be made to work in this manner  using a symmetric hash join implementation . while operators like select and project do not need to maintain any run-time state  a symmetric-hash-join operator needs to maintain state that is proportional to the size of the input seen so far  which is unbounded for streams. however  for applications on streams  a relevant notion of join is that of a sliding-window join  1  1  1  1  1   where a tuple from one stream is joined on arrival with only a bounded window of tuples in the other stream  and vice versa. consequently  the state maintained by a sliding-window join operator is bounded. an example of a sliding-window join is a tuple-based sliding-window join where the window on each stream is specified as a fixed number of tuples  1  1 . clearly  the runtime state stored by a tuple-based sliding-window join operator is of fixed size.
　in summary  the operators that we consider act like filters which operate on a tuple and produce s tuples  where s is the selectivity of the operator. the selectivity s is at most 1 for the select and project operators  but it may be greater than 1 for a join. for the rest of this paper  the reader should keep in mind that when we refer to the selectivity s of an operator  we are referring to the above notion of viewing the operator as a filter that  on an average  produces s tuples on processing 1 tuple. we assume that the run-time state stored by each operator is fixed in size and thus the variable portion of the memory requirement is derived from the sizes of the input queues to operators. the memory for all input queues is obtained from a common system memory pool. the aim of operator scheduling is to minimize the total memory requirement of all queues; in particular  to keep the memory requirement below a certain threshold.
　an important strategy for adapting to higher rates of data arrival is load shedding  i.e.  dropping input tuples based on some criterion such as quality-of-service . load shedding may become necessary when the input rate consistently exceeds the maximum processing rate of the system so that backlogs in the input queues will build up and eventually we will run out of memory. there are various strategies for load shedding  see    but that is not the focus of this paper. these strategies are orthogonal to those that we discuss in this paper and could be used in conjunction. instead  we focus on the following issue: even though the average arrival rate is within computational limits  there may be bursts of high load leading to high memory usage to buffer the backlog of unprocessed tuples. how do we efficiently schedule operators in order to keep the peak memory usage at a minimum  our assumption that the arrival rate is within computational limits implies that it is eventually possible to clear the backlog of unprocessed tuples  when the bursts of high arrival rate have receded.
　every tuple that enters the system must pass through a unique path of operators  referred to as an operator path.  recall that we do not share tuples among query plans.  if the arrival rate of the tuples is uniform  then there is not much to be done. the following simple scheduling strategy will have the minimum memory requirement: whenever a tuple enters the system  schedule it through all the operators in its operator path. if any operator in the path is a join that produces multiple tuples  then schedule each resulting tuple in turn through the remaining portion of the operator path. henceforth  this strategy will be referred to as the fifo  first in  first out  strategy. but note that  as mentioned earlier  such uniformity in arrival is seldom the case and hence we need more sophisticated scheduling strategies guaranteeing that the queue sizes do not exceed the memory threshold. the following example illustrates how a scheduling strategy can fare better than fifo and make the difference between exceeding memory threshold or not.
　example 1. consider a simple operator path that consists of two operators: o1 followed by o1. assume that o1 takes unit time to process a tuple and produces 1 tuples1  i.e.  its selectivity is 1. further  assume that o1 takes unit time to operate on 1 tuples  alternatively  1 time units to operate on 1 tuple  and produces 1 tuples  i.e.  o1 outputs the tuple out of the system and hence has selectivity 1. thus  it takes 1 units of time for any tuple to pass through the operator path.
　we assume that  over time  the average arrival rate of tuples is no more than 1 tuple per 1 units of time. this assumption guarantees that we do not have an unbounded build-up of tuples over time. however  the arrival of tuples could be bursty. consider the following arrival pattern: a tuple arrives at every time instant from t = 1 to t = 1  then no tuples arrive from time t = 1 through t = 1.
consider the following two scheduling strategies:
  fifo scheduling: tuples are processed in the order that they arrive. a tuple is passed through both operators in two consecutive units of time  during which time no other tuple is processed.
  greedy scheduling: at any time instant  if there is a tuple that is buffered before o1  then it is operated on using 1 time unit; otherwise  if tuples are buffered before o1  then 1 tuples are processed using 1 time unit.
the following table shows the total size of input queues for the two strategies:
timegreedy schedulingfifo scheduling11111.1.1111.1.1111.1.1after time t = 1  input queue sizes for both strategies decline until they reach 1 after time t = 1. observe that greedy scheduling has smaller maximum memory requirement than fifo scheduling. in fact  if the memory threshold is set to 1  then fifo scheduling becomes infeasible  while greedy scheduling does not. 1
　this example illustrates the need for an intelligent scheduling strategy in order to execute queries using limited amount of memory. the aim of this paper is to design scheduling strategies that will form the core of a resource manager in a data stream system.
　here are some desirable properties of any such scheduling strategy:
  the strategy should have provable guarantees on its performance in terms of metrics such as resource utilization  response times  and latency.
  because it will be executed every few time steps  the strategy should be efficient to execute.
  the strategy should not be too sensitive to inaccuracies in estimates of parameters such as queue sizes  operator selectivities  and operator execution times.
　the algorithms and heuristics developed in this paper do not achieve all the stated goals  although we feel that they represent an important first step. our algorithms have some shortcomings; for example  they do not incorporate application-level quality-ofservice requirements  so some important tuples that are waiting to be processed may be delayed for an unacceptably long time before being scheduled. our future work will be directed towards removing such shortcomings.
1. operator scheduling and memory requirements
　as mentioned earlier  query execution can be captured by a data flow diagram  where every tuple passes through a unique operator path. thus queries can be represented as rooted trees. every operator is a filter that operates on a tuple and produces s tuples  where s is the operator selectivity. obviously  the selectivity assumption does not hold at the granularity of a single tuple but is merely a convenient abstraction to capture the average behavior of the operator. for example  we assume that a select operator with selectivity 1 will select about 1 tuples of every 1 tuples that it processes. henceforth  a tuple should not be thought of as an individual tuple  but should be viewed as an convenient abstraction of a memory unit  such as a page  that contains multiple tuples. over adequately large memory units  we can assume that if an operator with selectivity s operates on inputs that require one unit of memory  its output will require s units of memory.
　the fifo scheduling strategy  from example 1  maintains the entire backlog of unprocessed tuples at the beginning of each operator path. thus the sizes of intermediate input queues will be small in case of fifo  albeit at the cost of large queues at the beginning of each operator path. in fact  the greedy strategy performed better than fifo in example 1 precisely because it chose to maintain most of its backlog at the input queue between the first operator and the second. since the first operator had a low selectivity  it was beneficial to buffer two fractional tuples  each of size 1  at this intermediate queue  rather than buffer a single tuple of size 1 at the beginning of the operator path. this suggests that it is important to consider the different sizes of a tuple as it progresses through its operator path. we capture this using the notion of a progress chart  illustrated in figure 1.
　the horizontal axis of the progress chart represents time and the vertical axis represents tuple size. the chart contains m + 1 operator points  t1 s1   t1 s1  ...  tm sm   marked by circles  where 1 = t1   t1   t1   ...   tm are positive integers. the points represent an operator path consisting of m operators  where the ith  1 ＋ i ＋ m  operator takes ti   ti 1 units of time to process a tuple of size si 1  at the end of which it produces a tuple of size si. the selectivity of operator i is si/si 1.  we assume that operators can be preempted in the middle of their execution and resumed later; a time unit represents the smallest duration an operator must be continuously run before preemption. 

	time	 t1 s1  =  1 	time
	 a  all operators have selectivity s ＋ 1	 b  one operator has selectivity s   1
figure 1: progress charts　adjacent operator points are connected by a solid line called the progress line representing the progress of a tuple along the operator path. we imagine tuples as moving along this progress line. a tuple τ enters the system with size s1 = 1. after being processed by the ith query operator  the tuple has received ti total processor time and its size has been reduced to si. at this point  we say that the tuple has made progress p τ  = ti. a good way to interpret the progress chart is the following - the ith horizontal segment represents the execution of the ith operator  and the following vertical segment represents the drop in tuple size due to operator i's selectivity. for every operator path  the last operator has selectivity sm = 1. this is because it will eject the tuples it produces out of the system and they no longer need to be buffered. if all operator selectivities are less than or equal to 1  the progress chart is nonincreasing as shown in figure 1 a . however  for query plans that include a join operator with selectivity greater than 1  the progress chart looks like the one shown in figure 1 b .
　we assume that the selectivities and per-tuple processing times are known for each operator. we use these to construct the progress chart as explained above. selectivities and processing times could be learned during query execution by gathering statistics over a period of time. if we expect these values to change over time  we could use the following strategy as in : divide time into fixed windows and collect statistics independently in each window; use the statistics from the ith window to compute the progress chart for the  i + 1 st window.
　consider an operator path with m operators represented by the operator points  t1 s1  ...  tm sm . for any point  t s  along the progress line  where ti 1 ＋ t   ti for some 1 ＋ i ＋ m  the derivative of the point with respect to the jth operator point  tj sj  is given by d t s j  =   tsjj  ts   for m − j − i. the derivative is undefined for j   i. the derivative is nothing but the negative slope of the line connecting the point  t s  to an operator point to its right. the steepest derivative at the point  t s  for ti 1 ＋ t   ti is denoted d t s  = maxm−j−i d t s j   and the operator point for which the maximum is achieved1 is defined as the steepest descent operator point  sdop t s  =  tb sb  where b = min{j|m − j − i and d t s j  = d t s }.
　consider the following subsequence of operator points: start with the point x1 =  t1 s1 . let x1 = sdop x1   let x1 = sdop x1   and so on  until we finally reach the point xk =  tm sm . if we connect this sequence of points x1 x1 ... xk by straight line segments  we obtain the lower envelope for the progress chart. in figures 1 a  and 1 b   the lower envelope is represented by a dashed line. observe that the lower envelope is convex.
　we make the following simple observation regarding the lower envelope.
　proposition 1. let  t1 s1   t1 s1  ...  tk sk  denote the sequence of points on the lower envelope for a progress chart. the magnitude of the slopes for the segments joining  ti 1 si 1  and  ti si   for 1 ＋ i ＋ k  must be non-increasing with i.
　we give only a brief sketch of the proof. suppose there exists an index i such that the magnitude of slope for the segment joining  ti si  to  ti+1 si+1  is strictly greater than that of the segment joining  ti 1 si 1  to  ti si . then  the slope of the segment joining  ti 1 si 1  to  ti+1 si+1  is strictly greater than that for the segment joining  ti 1 si 1  to  ti si . in that case  by definition   ti si  is not the sdop for the point  ti 1 si 1   and hence does not belong to the lower envelope.
1. scheduling strategies and performance bounds
　we are now ready to present our proposed scheduling strategy and derive its performance bounds. we begin by defining a framework for specifying a strategy. ideally  one can view a scheduling strategy as being invoked at the end of every unit of time; the smallest duration for which operators should be run without preemption. on each invocation  a strategy must select an operator from among those with nonempty input queues and schedule it for the next time unit. in reality  we need not invoke the strategy after every time unit. it turns out that in most cases it is only required to do so periodically or when certain events occur  e.g.  an operator that is currently scheduled to run finishes processing all tuples in its input queue  or a new block of tuples arrives on an input stream. all scheduling strategies considered in this paper choose the next operator to schedule based on statically-assigned priorities  i.e.  scheduling and operator execution do not change operator priorities. thus  the scheduling strategy itself causes little overhead since priorities need not be recomputed whenever operators are scheduled or stopped. under this model  we will describe strategies that assign priorities to different operators across all queries and provide guarantees where possible.
　as we will see momentarily  the priority that our scheduling strategy assigns to each operator is completely determined by the progress chart that the operator belongs to. we need to ensure that our estimates for selectivities and per-tuple processing times  based on which the progress charts are computed  are not very outdated. therefore  we periodically recompute these progress charts  based on the statistics that are gathered over the most recent window of time during query execution. the task of recomputing the progress charts from these statistics is straightforward and incurs little overhead.
　the queries that we consider can be categorized into the following two types.
1. single-stream queries: these queries typically involve selections and projections over a single stream  may involve joins with one or more static stored relations  and possibly end with a grouping and aggregation. this is a fairly common class of queries in data stream applications. singlestream queries are discussed in section 1.
1. multi-stream queries: a distinguishing feature of this class of queries is that they involve at least one join between two streams. such queries are typically used to correlate data across two or more streams  e.g.  a query that joins network packet streams from two routers to find packets that passed through both routers . as indicated earlier  we assume that all joins over streams are tuple-based sliding-window joins. multi-stream queries are discussed in section 1.
1 single-stream queries
　we present an operator scheduling algorithm that we call chain scheduling. the name  chain scheduling  comes from the fact that our algorithm groups query operators into operator chains corresponding to segments in the lower envelope of the query progress chart.
　given an operator path and its progress chart p  let p1 denote the lower envelope simulation of p  defined as the progress chart whose progress line consists of the lower envelope of p. consider a tuple τ and a progress line segment li joining  ti si  to  ti+1 si+1  in p1. we say that tuple τ lies on li if ti ＋ p τ    ti+1.  recall from section 1 that p τ  denotes the progress made by τ along the operator path.  moreover  we say that τ is at the beginning of li if p τ  = ti and that it is in the middle of li if ti   p τ    ti+1.
　consider a data stream system with n distinct operator paths represented by the progress charts p = {p1 ... pn} with lower envelope simulations p1 = {p1 ... pn1 }. the chain scheduling strategy  henceforth simply chain  for brevity  for such a system proceeds as follow:
chain: at any time instant  consider all tuples that are currently in the system. of these  schedule for a single time unit the tuple that lies on the segment with the steepest slope in its lower envelope simulation. if there are multiple such tuples  select the tuple which has the earliest arrival time.
　the way we describe our strategy  it may appear that it is  tuplebased   i.e.  we make decisions at the level of each tuple. that is not the case - chain statically assigns priorities to operators  not tuples  equaling the slope of the lower-envelope segment to which the operator belongs. at any time instant  of all the operators with tuples in their input queues  the one with the highest priority is chosen to be executed for the next time unit.
　using the special structure of the lower envelope  we show that chain is an optimal strategy for the collection of progress charts p1. to this end  define a clairvoyant strategy as one which has full knowledge of the progress charts and the tuple arrivals in the future. clearly  no strategy can actually have knowledge of the future  but the notion of clairvoyance provides a useful benchmark against which we can compare the performance of any valid scheduling strategy. we will compare chain to clairvoyant strategies.
　theorem 1. let c denote the chain scheduling strategy and a denote any clairvoyant scheduling strategy. consider two otherwise identical systems  one using c and the other using a  processing identical sets of tuples with identical arrival times over operator paths having progress charts p1. at every moment in time  the system using a will require at least as much memory as the system using c  implying the chain strategy is optimal over any collection of lower envelopes.
　proof sketch: since tuple arrival times are the same for each system  differences in memory requirements at time t will be due to the number of tuples that each system has been able to process by that time. let c t  and a t  denote the number of tuples that have been  consumed  through processing  the total reduction in size over all tuples  by the two strategies at any time instant t. we wish to show that for all t  c t  − a t .
　let d1   d1   ，，，   dl   1 denote the distinct slopes of the segments in the lower envelopes  arranged in a decreasing order. the slope of a segment is the fraction of a tuple consumed  reduction in the size of the tuple  when a tuple moves for a unit time along the segment. at any instant t  let tci  respectively  tai   denote the number of time units moved along all segments with slope di by the strategy c  respectively  strategy a . since strategy c prefers to move along the segment with the steepest slope  and since by proposition 1 the slopes are nonincreasing for any lower envelope  it follows that ttai for 1 ＋ i ＋ l.
　the number of tuples that are consumed by strategy c is given by c t  = p1＋i＋l tci di  while the number consumed by a is given by a t  = p1＋i＋l tai di. since d1   d1   ，，，   dl   1 and t tai for 1 ＋ i ＋ l  it follows that c t  − a t .	
　we now consider the performance of chain on general progress charts  beginning with the following observation:
　proposition 1. for all segments with a particular slope  chain guarantees that there is at most one tuple that lies in the middle of one of these segments. the remaining such tuples must be at the beginning of their respective segments. consequently  this strategy maintains the arrival order of the tuples.
　to see that proposition 1 is true  recall that amongst all the tuples lying on the steepest-descent segment s   chain prefers to keep moving the tuple with the earliest timestamp  so it will keep moving this tuple until it has cleared the segment and moved to the next segment.
　when chain is implemented over a general progress chart p  it  pretends  that tuples move along the lower envelopes  although in reality the tuples move along the actual progress chart p. however  we show that this does not matter too much - the memory requirements of chain on p are not much more than its memory requirements on the lower envelope simulation p1. consider a segment joining  ti si  to  ti+1 si+1  on any of the lower envelopes of p. let δi denote the maximum  over this segment  of the difference between the tuple-size coordinates  vertical axis  of p and its lower envelope p1 for the same value of the time coordinate
 horizontal axis .
　lemma 1. let c t  denote the number of tuples that are consumed by the chain strategy moving along the lower envelopes. let ac t  denote the number of tuples that are actually consumed by chain when tuples move along the actual progress charts. at any time instant t  ac t  − c t  pi δi where the sum pi δi is taken over all segments corresponding to all lower envelopes.
　proof sketch: consider a tuple making the same moves along the time axis for the two functions corresponding to the actual progress chart and the lower envelope. the size of the tuple is same when it is at the beginning of any segment. the size differs only if the tuple is in the middle of any segment. moreover  for any segment i  the maximum difference is equal to δi  as per the definition of δi. proposition 1 guarantees that the chain strategy will have at most one tuple in the middle of any segment. putting all this together  we obtain that ac t  − c t    pi δi. 
　another simple observation that we make about progress charts follows from the fact that the lower envelope always lies beneath the actual progress chart.
　proposition 1. for any progress made by a tuple along the progress chart p  if we use the lower envelope simulation of p to measure the memory requirement instead of the actual progress chart p  then we will always underestimate the memory requirement.
　it follows from proposition 1 that the memory requirement for any strategy on progress chart p will be greater than the memory requirement of chain on the lower envelope simulation of p  since we proved earlier  theorem 1  that the chain strategy is optimal over any collection of lower envelopes.
　we can combine the preceding observations to prove a statement about the near-optimality of chain on a general progress chart p. since the performance of the chain strategy over the lower envelope is a lower bound on the optimum memory usage  even for clairvoyant strategies   we obtain that the chain strategy applied to the actual progress chart is optimal to within an additive factor of pi δi. for the important case where all operator selectivities are at most 1  which corresponds to queries where there are no nonforeign-key joins to stored relations  we can give a tight bound on pi δi to obtain the following result:
　theorem 1. if the selectivities of all operators are at most 1 and the total number of queries is n  then ac t  − c t    n.
　proof sketch: when the selectivities of all operators are at most 1  the progress chart is a non-increasing step function as shown in figure 1 a . then  for a segment joining  ti si  to  ti+1 si+1   it must be the case that δi = si si+1. consequently  the sum of δj over all segments j that belong to the same lower envelope  same query  equals 1. as a result  the sum pi δi over all segments in all lower envelopes is bounded from above by the number of queries n. combining this with lemma 1 implies that ac t  − c t    n.

　thus  when all selectivities are at most 1  chain differs from optimal by at most one unit of memory per operator path. we emphasize that the guarantee is much stronger than merely saying that the maximum  over time  memory usage is not much more than the maximum memory usage of an optimal strategy. in fact  we are guaranteed that the chain strategy will be off by at most one unit of memory  per query  as compared to any clairvoyant strategy  with an unfair knowledge of future tuple arrivals   at all instants of time and not just when we compare the maximum memory usage. this is a fairly strong worst-case bound on the performance of chain. it is quite surprising that  even without the knowledge of future arrivals  chain is able to maintain near-optimality at all time instants.
　our analysis of chain is tight in that there exist cases where it will suffer from being worse than the optimal strategy by an additive factor of n.
1 comparison with other strategies
　before proceeding to the case of queries joining multiple streams  we present other natural scheduling strategies against which we will compare chain scheduling:
1. round-robin: the standard round-robin strategy cycles over the list of active operators and schedules the first operator that is ready to execute. on being scheduled  an operator runs until a fixed time quantum expires or an input queue to the operator becomes empty. in contrast to chain  and other priority-based scheduling strategies  round-robin has the desirable property of avoiding starvation  i.e.  no operator with tuples in its input queue goes unscheduled for an unbounded amount of time . with chain  especially during bursts  ready-to-execute operators in low-priority chains may have to wait for a while before they are scheduled. however  the simplicity and starvation avoidance of round-robin come at the cost of lack of any adaptivity to bursts.
1. fifo: the fifo strategy  example 1  processes input tuples in the order of arrival  with each tuple being processed to completion before the next tuple is considered. in general  fifo is a good strategy to minimize the overall response time of tuples in the query result. like round-robin  fifo ignores selectivity and processing time of operators and shows no adaptivity to bursts.
1. greedy: in the greedy strategy  each operator is treated separately  as opposed to considering chains of operators  and has a static priority  1   s1 /t1  where s1 is the selectivity and t1 is the per-tuple processing time of that operator. this ratio captures the fraction of tuples eliminated by the operator in unit time. the problem with this strategy is that it does not take into account the position of the operator vis-a-vis other operators in the operator path. for instance  suppose a fast  highly selective operator h follows a few less selective operators. although operator h will get high priority  the ones preceding it will not. as a result  at most time instants h will not be ready to be scheduled as its input queues will be empty. this demonstrates the need to prioritize earlier operators in an inductive manner  a notion that is captured by the lower envelope in chain.
we conclude this subsection with some discussion points:
pushing down selections. query optimizers try to order operators so that more selective operators precede those that are less selective  making it likely that query plans have very selective operators early on. it is precisely on this type of query plan that chain performs best compared to strategies such as fifo and roundrobin. the fifo strategy  which does not exploit the low selectivity of operators at the beginning of a query plan  will accumulate a large backlog of unprocessed tuples at the beginning of each operator path during bursty periods  as illustrated in example 1. the round-robin scheduling strategy will have a similar problem since it treats all ready operators equally. interestingly  greedy will mirror chain if the operators in the plan are in decreasing order of priority  when each operator will form a chain on its own. still  noncommutativity of operators will sometimes result in query plans that favor chain over greedy. for example  tuple-based slidingwindow joins like the ones we consider in this paper do not commute with most other operators including selections. pushing a selection down below a tuple-based sliding-window join will change the result of the join by filtering out some tuples before they reach the join  slowing the rate at which tuples expire from the sliding window.
starvation and response times. as mentioned earlier  the
chain strategy may suffer from starvation and poor response times  especially during bursts. as ongoing work  we are considering how to adapt our strategy to take into account additional objectives that we do not currently consider  such as the objectives of minimizing query-result latency and avoiding starvation. we suspect that a randomized strategy combining chain with fifo might do well for a metric that takes into account both memory and tuple latency.
schedulingoverhead. clearly  the scheduling overhead is negligible for simple strategies like round-robin and fifo. in chain  scheduling decisions need to be made only when an operator finishes processing all tuples in its input queue  or when a new block of tuples arrives in an input stream. in either case  the scheduling decision involves picking an operator from the highest-priority chain that contains a ready operator. underlying progress charts and chain priorities need to be recomputed only when operator selectivities or tuple-processing times change; otherwise  the operator chains and their priority order is fixed. recomputing progress charts from statistics and chains  and their priorities  from these progress charts takes very little time. thus  chain also incurs negligible overhead; greedy behaves similarly. context switching overhead. the context switching overhead incurred by a scheduling strategy depends on the underlying query execution architecture. we assume that all operators and the scheduler run in a single thread. to get an operator to process its input queues  the scheduler calls a specific procedure defined for that operator. this query execution model is similar to the framework implemented in some recent data stream projects  1  1 . context switching from one operator to another is equivalent to making a new procedure call  which has low cost in modern processor architectures. context switching costs can become significant if different operators are part of separate threads  as in aurora . even if context switching costs are significant  we do not expect these costs to hamper the effectiveness of chain. compared to other scheduling policies like round-robin and fifo  chain tends to minimize the number of context switches. chain will force a context switch only when an operator finishes processing all tuples in its input queue  or when a new block of tuples arrives in an input stream and unblocks a higher-priority operator than the one currently scheduled.
throughput. all techniques perform the same amount of computation  considering that scheduling costs and context switching costs are negligible. we would expect throughput to be the same over time  provided the main memory threshold is not exceeded.
during bursts  fifo would momentarily produce more result tuples compared to the other strategies.
　the experimental results in section 1 validate the intuitive statements and analytical results presented in this section.
1 multi-stream queries
　we now extend chain scheduling to queries with multiple streams that contain at least one sliding-window join between two streams. for presentation  in this particular section a  tuple  refers to a single stream tuple  as opposed to a larger unit of memory such as a page  as is the case in the rest of the paper.
　recall that we assume tuple-based sliding-window joins. we assume that every tuple has a globally unique timestamp  across all streams   and that tuples within a single stream arrive in increasing order of timestamp. for instance  the stream system might be timestamping tuples when they arrive in the system. a procedural description of the result of a tuple-based sliding-window join is as follows: for a join between streams r and s  when a tuple arrives on stream r  it will be joined with the latest ws tuples in s  where ws is the size of the sliding window on stream s . symmetric processing occurs for tuples arriving on stream s. when tuples with timestamps t1 and t1 are joined  the timestamp of the result tuple is max t1 t1 .
　the synchronization inherent in the above semantics restricts freedom in operator scheduling. to guarantee correctness and ensure that join output is produced in sorted timestamp order  we need to synchronize the two inputs to a join by processing them in strict timestamp order across both input streams  similar to a merge sort on the timestamp attribute . in other words  when joining streams r and s  no tuple from r with timestamp t will be processed by the join operator until it has processed all tuples in s with timestamp less than t. as a result  a sliding-window join operator will block if any one of its input queues is empty  even if tuples are available in the other input queue.
1.1 extending chain scheduling to joins
　in order to extend chain scheduling  we first need to extend the progress chart model to multi-stream queries. a query with multiple streams is a rooted tree with input streams at the leaves of the tree. we break up the tree into parallel operator paths  one for each input stream  that connect individual leaf nodes  representing input streams  to the root of the tree. the operator paths thus obtained can share common segments. each operator path is individually broken up into chains for scheduling purposes. for example  consider a query with a sliding-window join  ./  between two streams r and s  followed by a selection condition  σ  on the output of the join. additionally  there is a project operator  π  on the stream r before it joins s. the decomposition of the tree corresponding to this query gives two operator paths r ★ π ★./★ σ and s ★./★ σ. the ./★ σ segment is shared between the operator paths. note that the join operator is part of both operator paths and will therefore be part of two operator chains when these paths are broken up into chains for scheduling. however  as discussed earlier in this section  the join operator always processes tuples in strict timestamp order across its input queues irrespective of the chain  as part of which  it gets scheduled. furthermore  the sliding-window join operator will block if any one of its input queues is empty.
　the per-tuple processing times  t  and selectivities  s  for all operators other than join are defined in a straightforward manner  similar to the case of single-stream queries. we now specify the quantities  t s  for a join operator between two streams.
a sliding-window join is abstracted using a model similar to
　
the one described in . let the average number of tuples in a stream s per unit time  as per the timestamp on tuples  be λs. like average tuple selectivity  λs is a convenient abstraction to capture the average behavior of sliding-window join operators. consider a sliding-window join operator between streams r and s that processes tuples with timestamps belonging to an interval of size t1. during this run  the join operator will process  on an average  t1 λr + λs  input tuples and produce t1 λrαw s  + λsαw r   result tuples  where αw s  is the  average  selectivity of the semijoin of stream r with the sliding window for s  i.e.  the average number of tuples from the sliding window for s that a tuple from r joins with. αw r  is defined analogously. the system time taken for this run is t1 〜  λr 〜 tr + λs 〜 ts   where tx is the average time taken to process a tuple from stream x. here ts includes the time taken to compare the head tuples in the queues for r and s  the time to probe the sliding window for r and produce result tuples  and the time to update the sliding window for s. given λr λs αw r  αw s  tr ts  for input streams r and s  the selectivity s for the sliding-window join is given by λrαw s +λsαw r  λr+λs and per-tuple processing time t  wall clock
time  is given by λr〜λrtr++λλssts . it is easy to inductively derive λs1 for any stream s1 that is the result of an intermediate operator in the query plan.
　having specified the  t s  values for the windowed join operator between two streams  we build the progress chart for each operator path as described in section 1. our basic chain strategy remains unchanged. the only difference is the following: earlier an operator could be blocked  cannot be scheduled  only when input queue is empty. however  in the case of a join  the left input queue might have tuples while the right input queue could be empty. in such a case  the chain corresponding to the left queue might want to schedule the join operator  but it cannot do so because the operator is blocked for input on the right input queue. this chain will not be considered for scheduling until the join operator becomes unblocked.
　as before  the scheduling strategy is executed whenever an operator finishes processing all tuples in one of its input queues  or when a new block of tuples arrives in some input stream  and the highest-priority ready chain is scheduled as always.
　unlike the single-stream case  we do not have any analytical results for our adaptation of chain to the multiple-stream case. however  experimental results suggest that chain performs extremely well  for both single-stream and multiple-stream queries  compared to the other scheduling strategies that were considered.
1. experiments
　in this section we briefly describe our simulation framework and the results of various experiments that we conducted to compare the performance of different scheduling policies: chain  fifo  greedy  and round-robin.
　we begin with a brief description of our simulation framework. we ignore the cost of context switching and scheduling overhead in our simulations. as discussed in section 1  chain makes fewer context switches as compared to fifo  greedy  or round-robin. therefore  had we included context-switching costs  the relative performance of chain would have been even better than shown here.
　fifo processes each block of input stream tuples to completion before processing the next block of tuples in strict arrival order. round-robin cycles through a list of operators and each ready operator is scheduled for one time quantum. the size of the time quantum does affect the performance of round-robin  but it does

figure 1: queue size vs. time  single stream  two operators  real data set 
not change the nature of the results presented here.
　the notion of progress chart captures the average behavior of the query execution in terms of the sizes of memory units as they make progress through their operator paths. the experiments we describe were designed by choosing a particular progress chart to use for both the real and synthetic data sets and then adjusting selection conditions and join predicates to closely reflect the progress chart. of course  during actual query execution there are short-term deviations from the average behavior captured in the progress chart. in our experiments  we follow the query execution and report the memory usage at various times.
　the experiments described here used static estimates for operator selectivities and processing times  derived from a preliminary pass over the data  to build the progress charts. we also performed experiments that allowed adaptivity by dividing time into windows and using the statistics gathered during the ith time window to build progress charts for the  i+1 st time window. the results were similar to our other experiments  and we do not report them here.
　next we briefly describe the data sets that were used in the various experiments:
1. synthetic data set: the networking community has performed considerable research on how to model bursty traffic to most closely approximate the distributions prevalent in most real data sets. a good reference is the paper by willinger  paxson  et al. . based on their  on/off  model we generate synthetic bursty traffic by flows that begin according to a poisson process  with mean inter arrival time equal to 1 time unit  and then send packets continuously for some duration chosen from a heavy-tailed distribution. we used the pareto distribution for packet durations  which has a probability mass function given by p x  = αkαx α 1  for α k   1 x − k. we used k = 1 and α = 1 in our experiments. while the arrival times are generated as above  the attribute values are generated uniformly from a numeric domain; this allows us to choose predicates with desired selectivities.
1. real data set: the internet traffic archive  is a good source of real-world stream data sets. one of their traces  named  dec-pkt   contains one hour's worth of all widefigure 1: queue size vs. time  single stream  two operators  synthetic data set 
area traffic between digital equipment corporation and the rest of the world. we use this trace as the real-world data set for our experiments. attributes such as ip addresses and packet sizes were used in selection and join predicates. the exact predicates were chosen to give the desired selectivities for each experiment.
1 single-stream queries without joins
　our first experimental results compare the performance of different scheduling strategies for single-stream queries without joins. we first consider a simple query with two operators. its progress chart in terms of coordinates  ti si  of operator points is:  1    1.1   and  1   where times are in microseconds. in terms of the terminology in section 1  a tuple in the progress chart contains 1 individual tuples of size 1 bytes each. this query consists of a fast and highly selective operator followed by a slow operator that  consumes  much fewer tuples per unit time. this is similar to example 1.
　figures 1 and 1 show the variation in total queue size over time for the real and synthetic data set  respectively. we observe that chain and greedy have almost identical performance for this simple query plan. this is explained by the fact that each operator forms a chain of its own and hence they are expected to behave identically. round-robin performs almost as well as chain on both data sets. in later experiments we will see how as the number of operators increase  the performance of round-robin degrades. fifo performs badly with respect to chain since it under-utilizes the fast and highly selective first operator during any burst.
　the second query that we consider has four operators with selectivities less than 1. its progress chart in terms of coordinates  ti si  of operator points is:  1    1.1    1.1    1.1   and  1 . the third operator is fast and highly selective and it comes between two operators that have much lower tuple consumption per unit time. this is a typical scenario where greedy is expected to perform badly as compared to chain. indeed  we observe this in figures 1 and 1  which show the variation in total queue size over time for this query on the real and synthetic data sets  respectively.  for legibility  we have not shown the performance of fifo and round-robin in figures 1 and 1 respectively  both of which perform nearly as bad as greedy in either case.  befigure 1: queue size vs. time  single stream  four operators  real data set 
cause greedy does not schedule the less selective second operator during bursts  the fast and selective third operator remains underutilized  explaining greedy's bad performance. because it uses the lower envelope to determine priorities  chain is able to recognize the usefulness of the third operator although it is sandwiched between two  unproductive  operators. notice that unlike the previous case round-robin does badly compared to chain in figure 1.
1 queries having joins with stored relations
　recall from section 1 that a join with a relation could result in an operator with selectivity strictly greater than one. the real data set that we worked with did not include stored relations  so we report experimental results over synthetic data only. the progress chart used here in terms of coordinates  ti si  of operator points is:  1    1.1    1.1    1.1   and  1   where the second operator is a join with a stored relation. figure 1 shows the performance of different scheduling strategies for the bursty synthetic data.  for legibility  we have not shown the performance of round-robin in figure 1  which performs as badly as greedy. also  we have not connected the points corresponding to fifo by line segments.  since fifo and round-robin do not take operator selectivities into account  their performance remains more or less similar to what we observed in the previous experiments  section 1 . because of the low priority of the join operator  during bursts greedy does not utilize the fast and selective operator which follows the join. on the other hand  the first three operators comprise a single chain in chain  so the fast and selective third operator does get used during bursts  leading to vastly improved performance over greedy  figure 1 .
1 queries with sliding-window joins between streams
　we study the performance of the different strategies for a query over two streams r and s that are joined by a sliding-window join. both semijoins in the sliding-window join have an average selectivity of 1. the output of the windowed join passes through two selection conditions σ1 and σ1. furthermore  before joining with s  stream r passes through a selection condition σ1. the selection conditions σ1 and σ1 are not very selective  while σ1  the selection figure 1: queue size vs. time  single stream  four operators  synthetic data set 
following the join  is very selective. the performance graphs for this query over the real and synthetic data sets are shown in figures 1 and 1 respectively. we observe that greedy and fifo perform much worse than chain. round-robin compares well with chain for the real data set but does really badly on the synthetic data set. as in the experiment described in section 1  greedy does badly because of the sliding-window join preceding the highly selective operator. the low priority of the join discourages greedy from scheduling it so greedy under-utilizes the highly selective operator following it. fifo performs badly for the reasons mentioned earlier  namely the presence of a less selective operator  σ1  with relatively high tuple processing time.
1 multiple queries
　finally  we compared the performance of different strategies over a collection of 1 queries: a sliding-window join query similar to the one presented in the last experiment  section 1  and two singlestream queries with selectivities less than 1 similar to those presented in section 1. the performance graphs for this query workload over real and synthetic data sets are shown in figures 1 and 1 respectively. for improved legibility of figure 1  we have not connected the points corresponding to chain and fifo by line segments. the graphs clearly show that chain beats all others by a huge margin. the reason for the impressive performance of chain is the increase in complexity and size of the underlying problem. this is because chain is able to pick out the particular chain of operators that is most effective at reducing memory usage and will schedule it repeatedly during a burst of input arrival. on the other hand  since there are a larger number of operators in this multi-query experiment compared to the earlier single-query experiments  round-robin ends up executing the best operator much less frequently than if there were a lesser number of operators. in other words  as the number of operators increases  the fraction of time round-robin does the  right thing  decreases. the same statement holds for fifo as well. greedy performs badly for reasons mentioned earlier: the queries consist of highly selective operators sandwiched between not so selective ones  which chain recognizes  but greedy fails to recognize. this experiment suggests that as we increase the number of queries the benefits of an intelligent strategy like chain become more pronounced. indeed  the results in figure 1: queue size vs. time  single stream  s   1  synthetic data 
figures 1 and 1 were obtained by going from a single query to a collection of merely three queries. in a real system with many more queries  the benefits will undoubtedly be even greater.
1. related work
　recently  there has been considerable research activity pertaining to stream systems and data stream algorithms. an overview is provided in the recent survey paper by babcock et al. . most closely related to our paper is the suite of research in adaptive query processing.  see the ieee data engineering bulletin special issue on adaptive query processing  . some of the recent papers from the telegraph project  1  1  1  1  pertain to novel architectures and strategies for adaptive query processing over streams. however  while their research focuses primarily on adapting to changing data characteristics  we focus on adapting to changing arrival characteristics of data  in particular the bursty nature of data as documented in the networking community  1  1  1  1 . earlier work on adaptive query processing includes the query scrambling work by urhan et al.   the adaptive query execution system tukwila  for data integration  and mid-query re-optimization techniques developed by kabra and dewitt . more closely related to ours is the work on dynamic query operator scheduling by amsaleg et al.  aimed at improving response times in the face of unpredictable and bursty data arrival rates  the xjoin operator of urhan and franklin  which is optimized to reduce initial and intermittent delay  and the work on dynamic pipeline scheduling for improving interactive performance of online queries . however  in all these cases  the focus is on improving response times  and run-time memory minimization is not the consideration.
　various operator scheduling strategies have been suggested for stream systems  ranging from simple ones like round-robin scheduling  to more complex ones that aim at leveraging intra- and inter-operator nonlinearities in processing . however  to the best of our knowledge  none have considered the problem of scheduling with the aim of minimizing memory usage. also  somewhat related to our work is the set of papers  1  1  1  1  championing the use of sliding-window joins for stream systems. the rate-based optimization framework of viglas and naughton  considers the problem of static query optimization with the modified aim of maximizing the throughput of queries for stream systems; however  it
　
figure 1: queue size vs. time  sliding-window join and 1 selections  real data set 
does not address run-time scheduling.
　the problem of allocating main memory among concurrent operators in a traditional dbms in order to speed up query execution has been considered in  1  1  1 . these techniques do not extend directly to data stream systems.
1. conclusion and open problems
　we studied the problem of operator scheduling in data stream systems  with the goal of minimizing memory requirements for buffering tuples. we proposed the chain scheduling strategy and proved its optimality for the case of single-stream queries with selections  projections  and foreign key joins with static stored relations. furthermore  we showed that chain scheduling performs well for other types of queries  including queries with sliding-window joins. as ongoing work  we are considering an adaptation of our strategy to take into account additional objectives that we do not currently consider  such as minimizing latency and avoiding starvation. we suspect that a randomized strategy combining chain with fifo might do well for a metric that takes into consideration both memory requirements and tuple latency. other open problems include designing scheduling strategies for adaptive query plans  where the structure of the query plan itself is allowed to change over time  and considering sharing of computation and memory in query plans.
