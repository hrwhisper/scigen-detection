discriminative sequential learning models like conditional random fields  crfs  have achieved significant success in several areas such as natural language processing or information extraction. their key advantage is the ability to capture various non-independent and overlapping features of inputs. however  several unexpected pitfalls have a negative influence on the model's performance; these mainly come from an imbalance among classes/labels  irregular phenomena  and potential ambiguity in the training data. this paper presents a data-driven approach that can deal with such hard-to-predict data instances by discovering and emphasizing rare-but-important associations of statistics hidden in the training data. mined associations are then incorporated into these models to deal with difficult examples. experimental results of english phrase chunking and named entity recognition using crfs show a significant improvement in accuracy. in addition to the technical perspective  our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset.
categories and subject descriptors
i.1  artificial intelligence : learning; h.1  information systems applications : miscellaneous
general terms
algorithms  experimentation
keywords
discriminative sequential learning  feature selection  information extraction  text segmentation  association rule
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  chicago  illinois  usa.
copyright 1 acm 1-1-x/1 ...$1.
1. introduction
　conditionally-trained or discriminative models like maximum entropy  maxent    discriminative hmms   maximum entropy markov models  memms    and crfs  have achieved significant success in many  sequential  labeling and segmenting tasks  such as part-of-speech  pos  tagging   text segmentation or shallow parsing  1  1   information extraction  1  1   object detection in computer vision   image analysis and labeling  1  1   and biological sequence modeling . the noticeable advantage of these models is their flexibility to integrate a variety of arbitrary  overlapping  and non-independent features at different levels of granularity from the observed data.
　however  applications employing these models with fixed and hand-built feature templates usually generate a huge number of features  up to millions  e.g.  in . this is because one usually utilizes complex templates including conjunctions of atomic context predicates  e.g.  n-gram of words or pos tags  to cover as many combinations of statistics as possible without eliminating irrelevant ones. as a result  models using long and fixed conjunction templates are heavily overfitting and time-consuming to train because they contains many teacher-specific and redundant features. to reduce these drawbacks  mccallum  proposed a likelihood- driven feature induction for crfs that is based on a famous feature inducing strategy for exponential models . this method iteratively adds the conjunctions of atomic observational tests that most increase conditional log-likelihood into the model until some stopping criteria are reached. in spite of attaining a trade-off between the number of used features and model accuracy  this strategy may ignore rare but sensitive conjunctions with smaller likelihood gains that are still critical to model performance. also  when the number of atomic context predicates is large  the number of conjunctions becomes explosive; and thus ranking all conjunctions by likelihood gain is very expensive.
　in this paper  we propose a data-driven approach that can identify and emphasize rare-but-important associations or co-occurrences of statistics1 hidden the training data to

1
 in this paper  terms like   atomic  context predicates     singleton  statistics   or   atomic  observational tests  are used interchangeably to refer to particular kinds of contextual information observed from the training data
improve prediction accuracy for hard-to-classify instances. the main motivation and the underlying idea of this approach are based on the fact that  sequential  data  such as natural language or biological information  potentially contain the following phenomena that should be the major sources of prediction errors:
  ambiguous data instances usually contain unclear contextual clues that may result in misleading predictions. for instance  it is quite difficult for a phrase chunker to determine whether the word plans in the text the trip plans for japan is a singular verb or a plural noun.
  irregular instances are recognized as exceptions to the common statistics or decisions. for example  a pos tagger may mark walk as a noun in the sentence the disabled walk very slowly because of a regular sequential dependency that a noun should go after an adjective. however  the correct interpretation is that the disabled  i.e.  the disabled people  is the subject and walk is a plural verb rather than a noun.
  unbalanced data occurs when the distribution of classes in the training data is unbalanced. for example  the number of english noun phrases  np  is much larger than that of other phrase types  e.g.  adjective phrases  adjp . this may lead to low prediction accuracy for minor classes due to the dominance of major ones.
  frequently-observed vs. less-observed data instances: for instance  a named entity recognizer may identify new york university as a location while it is in fact an organization. this is because new york is observed more frequently than new york university.
  long dependencies in sequence data: several kinds of sequential training data contain long dependencies among observations at different positions in a sequence. the problem is that one can not always use a sliding window large enough to capture such useful clues because it would generate too many irrelevant features.
　data instances falling into the above situations should be hard examples. thus  the prediction of their labels does not usually obey the frequently observed statistics. in other words  the simple aggregation of singleton context predicates may lead to misleading predictions because the common statistics always overwhelm uncommon ones. to overcome this pitfall  a model should rely on rare-but-important associations or conjunctions of singleton context predicates to win the dominance of common decisions. in the first example  most contextual supports surrounding plans  e.g.  trip is a singular noun  plans ends with s  tend to say that plans is a singular verb rather than a part of a noun phrase. it is  however  quite easy for the model to recognize plans as a plural noun if relying on an important association like  if the word after plans for is capitalized  then plans should be a plural noun . this association rule emphasizes a rare but important co-occurrence of three factors: plans  for  and the next word is initially capitalized  such as a location like a city or a country . although such kind of associations may only occur several times in a whole dataset  their appearance is an important source of evidence to deal with difficult instances.
　in spite of their benefit  mining all rare-but-important associations of singleton statistics in big datasets is challenging because the number of candidates is prohibitively large. fortunately  we find that association rule mining techniques  such as fp-growth   are very useful for discovering such patterns. in our method  the set of rare-but-important associations is a special subset of rare but highly confident association rules discovered in the training data. selected associations are then integrated into the learning process according to one of three ways to improve the prediction accuracy for hard instances:  a  associations as normal features   b  associations as normal features with weighted feature values  and  c  associations as constraints for the inference process.
　derived from a reasonable assumption about rare-but- important associations and the robustness of association rule mining techniques  our approach offers the following distinctive characteristics:  1  rare-but-important associations are globally discovered from a huge number of candidates with any length and any combination of singleton statistics;  1  models with those associations can deal with difficult instances while preventing overfitting by avoiding long and fixed conjunction templates;  1  users can choose a suitable way to incorporate selected associations into their models. particularly  1%-confidence associations can be integrated into the model in terms of constraints for inference; and  1  our method can be used to improve any discriminative sequential learning application  especially for highly ambiguous and imbalanced data;  1  finally  our work also highlights a potential connection between pattern mining and statistical learning from large datasets.
　the rest of this paper is organized as follows. section 1 briefly introduces linear-chain crfs  a typical sequential learning model. section 1 presents the proposed approach. section 1 describes and discusses the experimental results. section 1 reviews related work. finally  conclusions are given in section 1.
1. discriminative sequential learning
　the goal of labeling/tagging for sequential data is to learn to map observation sequences to their corresponding label sequences  e.g.  the sequence of pos tags for words in a sentence. discriminative hmms   memms   and crfs  were intentionally designed for such sequential learning applications. in contrast to generative models like hmms   these models are discriminative  i.e.  trained to predict the most likely label sequence given the observation sequence. in this paper  crfs are referred to as the undirected linear-chain of model states  i.e.  conditionally-trained finite state machines  fsms  that obey the first-order markov independence assumption. the strength of crfs is that they can combine both the sequential property of hmms and the philosophy of maxent as well as global normalization that can avoid the label-bias problem . in our work  crfs were used to conduct all experiments.
1 conditional random fields
　let o =  o1 o1 ... ot  be some observed data sequence. let s be a set of fsm states  each of which is associated with a label  l （ l. let s =  s1 s1 ... st  be some state sequence  crfs  define the conditional probability of a state sequence given an observation sequence as
             t 1
pθ s o  = z o exp λkfk st 1 st o t     1  t=1 k
where z o  = s'  is a normalization summing over all label sequences. fk denotes a feature function in the language of maximum entropy modeling and λk is a learned weight associated with feature fk. each fk is either a per-state or a transition feature:
 1 
	 	 1 
where δ denotes the kronecker-δ. a per-state feature  1  combines the label l of current state st and a context predicate  i.e.  the binary function xk o t  that captures a particular property of the observation sequence o at time position t. for example  the current label is jj  adjective  and the current word is  sequential . a transition feature  1  represents sequential dependencies by combining the label l of the previous state st 1 and the label l of the current state st  such as the previous label = jj and the current label l = nn  noun .
1 inference in crfs
　inference in crfs is to find the most likely state sequence s  given the observation sequence o  s  = argmaxs pθ s|o 
t
=	argmaxs	exp	λkfk st 1 st o t 	 1  t=1 k
　in order to find s   one can apply a dynamic programming technique with a slightly modified version of the original viterbi algorithm for hmms . to avoid an exponential- time search over all possible settings of s  viterbi stores the probability of the most likely path up to time t which accounts for the first t observations and ends in state si. we denote this probability to be  t si   1 ＋ t ＋ t  1  and  1 si  to be the probability of starting in each state si. the recursion is given by:
 t+1 si  = maxsj	 t sj exp	λkfk sj si o t 	 1 
k
　the recursion terminates when t = t   1 and the biggest unnormalized probability is p  = argmaxi  t si  . at this time  we can backtrack through the stored information to find the most likely sequence s .
1 training crfs
　crfs are trained by setting the set of weights θ = {λ1 ...} to maximize the log-likelihood  l  of a given training data set d = { o k  l k  }nk=1:
n	1  j   j 	λk
	l =	log pθ l	|o	   	1σ1 	 1 
	j=1	k
where the second sum is a gaussian prior over parameters with variance σ1  which provides smoothing to deal with sparsity in the training data .
　when the labels make the state sequence unambiguous  the likelihood function in exponential models such as crfs is convex  thus searching the global optimum is guaranteed . however  the optimum can not be found analytically. parameter estimation for crfs requires an iterative procedure. it has been shown that quasi-newton methods  such as l-bfgs   are most efficient  1  1 . this method can avoid the explicit estimation of the hessian matrix of the log-likelihood by building up an approximation of it using successive evaluations of the gradient.
　l-bfgs is a limited-memory quasi-newton procedure for unconstrained optimization that requires the value and gradient vector of the function to be optimized. let sj denote the state path of training instance j in training set d  then the log-likelihood gradient component of λk is
n
	δl	 j 	 j 
	=	ck s	 o	    δλk j=1
n
       pθ s|o j  ck s o j     λσk1   1  j=1 s
where ck s o  is the count of feature fk given s and o  equal to    i.e. the sum of fk st 1 st o t  values for all positions  t  in the training sequence. the first two terms correspond to the difference between the empirical and the model expected values of feature fk. the last term is the first-derivative of the gaussian prior.
1. improving discriminative sequential learning
　this section presents the proposed framework in detail:  1  how to discover rare-but-important associations from the training data and  1  how to integrate those associations into discriminative sequential learning models  e.g. crfs.
1 mining rare-but-important associations
　this section first presents the concept rare-but-important associations in discriminative sequential learning based on the traditional association rules   and then describes a method to discover such patterns from the training data.
1.1 associations in sequential training data
table 1: transactional database of pos tagging data
sequential training data d o k 1  l k 1  : ...
 o k  l k  : ... highly rb ambiguous jj data nns ...
 o k+1  l k+1  : ...c.p. templates	a = {wt  1 wt wt+1 suf1 wt  1 }
transactional database td...
rb  ... wt:highly  wt+1:ambiguous  ...
jj  wt 1:highly  wt:ambiguous  wt+1:data  suf1 wt 1 :ly
nns  wt 1:ambiguous  wt:data  ...  suf1 wt 1 :us
...　recall that the training dataset for sequential learning is denoted as  where o k  and l k  are the kth data observation and label sequences  respectively. let a = {a1 a1  ... am} be the set of context predicate templates in which each template ai captures a particular type of contextual information about data observations. in a sense  a is similar to the set of attributes in a relational table. applying all templates in a to each position in every training sequence  o  in the training data d  we obtain a transactional database td in which each transaction consists of a label and a list of active context predicates.
　for example  the first part of table 1 shows the training data d for pos tagging in which each training sequence  o l  is an english sentence together with pos tags of words. the second part is a set of 1 context predicate templates: the identities of the previous word  wt 1   the current word  wt   the next word  wt+1   and the 1-character suffix of the previous word  suf1 wi 1  . the third part is the transactional database td after applying templates in a for d.
　let i = {x1 x1 ... xn} be the set of all possible context predicates in the transactional database td  let l be the set of all labels  and t = {t1 t1 ... tm} be the set of all transactions in td. our target is to examines every predictive association rule r having the form below 
	x   l	 1 
where the left hand side  lhs  of
  i  is a conjunction of p context predicates in i  and the right hand side  rhs  of rule r  i.e.  l （ l  is a particular label. the support of the rule r  denoted as sup r   is the number of transactions in t containing {l} “ x  and the confidence of r  denoted as conf r   is the conditional probability that a transaction in t has the label l given that it contains x  i.e.  conf r  = sup x “ {l} /sup x . in a sense  this kind of rule is similar to the associative classification rules in  1  1  except that our work mainly focuses on rare-but-important associations as discussed in the next section.
1.1 rare-but-important associations
　derived from the predictive association rules defined in  1  and the concepts of support and confidence factors  we present a descriptive definition of rare-but-confident associations below.
definition 1. let lsup and usup be two integers that are
much smaller than the total number of transactions in t  i.e.  lsup    and let lconf be a real number that satisfies the condition 1 ＋ lconf ＋ 1 and lconf a predictive association rule r in  1  is called a rare-but- confident if:
lsup ＋ sup r  ＋ usup and conf r  − lconf
　all predictive association rules satisfying definition  1  are rare-but-confident. however  not all of them are important. this is based on the important observation that:  if most context predicates in the lhs of a rare-but-confident rule r strongly support the label l  then the rule r is trivial . in other words  if most context predicates in the lhs of r largely support label l in a separated manner  there is no need to examine the co-occurrence of all items in the lhs  and the model can still work properly without this rule. for example  in named entity recognition  the rule :new … wt:york … wt+1:weather   labelt=location is not important because both  wt 1:new  and  wt:york  strongly support the label  location   and thus their conjunction should be unnecessary. in other words  the named entity recognizer can predict the label  location  for the word  york  without the above rule because both  new  and  york  are frequently observed in the training data as a location name  i.e.  new york . based on this observation  we define the concept of  rare-but-important  associations as follows 
　definition 1. a rare-but-confident rule r: x   l is considered to be rare-but-important if there exists at least another label such that the sum of support counts for the label l from the context predicates in the lhs of r is larger than that for the label l  i.e. 

　why are predictive association rules satisfying definition  1  important  intuitively  if such a rule  r  exists in the training data but is not being discovered and emphasized  the model may predict the label l for any data transaction holding all context predicates in the lhs of r when the correct label is l. this is because most singleton context predicates in lhs of r tend to support the label l rather than l. this is why the appearance of predictive association rules satisfying definition  1  is important. there should be more sophisticated definitions and conditions of rare-but- important predictive association rules. however  we choose the above definition because of the trade-off between the rigorousness and the simplicity of calculation.
　for instance  the predictive association rule :new … wt:york … wt+1:university   labelt=organization is important for recognizing the named entity type of the current word   york   since there is another label   location   that should satisfy the condition addressed in defin-
ition  1   i.e.   organization . this is because bothx（x sup x   location   newxand（x sup yorkx
strongly support the label  location  rather than  organization . thus  the appearance of the above rule can help the model to recognize  new york university  as an organization rather than a location.
1.1 discovering rare-but-important associations
　mining rare-but-important associations from the transactional database td encounters the following problems:  1  the number of data items  i.e.  the number of atomic context predicates and labels |i “l|  is relatively large; and  1  the support thresholds  i.e.  lsup and usup  are very small compared to the number of transactions |t |. this means that there are a huge number of combinations of items that must be examined during the mining process.
　fortunately  fp-growth   a frequent pattern mining algorithm without candidate generation  can discover such associations in an acceptable computational time. this is because fp-growth employs a fp-tree  an extended prefix tree structure  to store crucial  quantitative information about frequent patterns in such a way that more frequently occurring items will have better chances of sharing nodes than less frequently occurring ones. all mining operations are then performed on the fp-tree in a partitioning  recursive fashion without candidate generation. see  for a complete description of this algorithm.
taking the sequential training data 
the set of context predicate templates a = {a1 a1  ... am}  the lower and upper support thresholds lsup  usup  lsup ＋
   and the lower confidence threshold lconf  1 ＋ lconf ＋ 1 and lconf   1  as inputs  rare-but-important association mining includes the following steps:
1. transforming the sequential training data d to a transactional database td by applying all predicate templates in a. td includes the set of items i “ l  all possible generated context predicates and all labels   the set of all transactions t .
1. mining all itemsets with supports larger or equal tolsup using the fp-growth algorithm.
1. generating all rare-but-confident association rules inthe form of x   l  1  with supports belonging to  lsup  usup   and the minimum confidence threshold lconf.
1. selecting all possible rare-but-important associationrules from rare-but-confident ones by applying the condition stated in definition  1 .
　in the fourth step  to determine whether or not a rare- but-confident rule  r: x   l  is rare-but-important  we have to scan the database to compute the sums of supports of context predicates in the lhs of r for all other labels. this is an expensive operation. fortunately  we can perform this on the fp-tree by traversing the node-links of each label  starting at the header table  and looking upward and downward to count the supports from context predicates appearing in the lhs of r. see  1  1  for the detailed description of fp-tree.
1 incorporating rare-but-important associations into conditional random fields
　this section presents three ways to incorporate the rare- but-important associations discovered from the training data into crfs:  1  associations as normal features   1  associations as features with emphasized feature functions  and  1  associations as constraints for the inference process.
1.1 rare-but-important associations as normal features of conditional random fields
　all rare-but-important associations are in the form x   l  in which  is a conjunction of p context predicates and l （ l is a particular label. these associations can be integrated into crfs in terms of normal per-state features as follows.
fk per state  st o t  = δ st l {xi1 o t  … ... … xip o t }
　these per-state features are similar to those in  1  except that they capture a co-occurrence of p atomic context predicates rather than a single one. the features are treated as normal features and are trained together.
1.1 rare-but-important associations as normal features with weighted feature values
　it is noticeable that rare-but-important features are infrequently observed in the training data  and thus their learned weights should be small. this means that their contributions  in several cases  may not be sufficient to win the dominance of common statistics  i.e.  frequently observed singleton features. to overcome this drawback  we emphasize rare-but-important features by assigning larger feature function values compared to normal features.
ip
	1	otherwise
where δ st l  and {xi1 o t …...…xip o t } are considered as logic expressions  and v is larger than 1  the feature value of normal features . v should be large if the occurrence frequency of the feature  also the support of the rare-but- important association  is small. thus  for each feature generated from a rare-but-important association r  v is equal to  usup sup r +1 . this ensures that v is always bigger than 1 and inversely proportional to the support of r  i.e.  the occurrence frequency of the feature.
1.1 rare-but-important associations as constraints for the inference process
　constrained crfs are extensions of crfs in which useful constraints are incorporated into the inference process  i.e.  the viterbi algorithm  to correct potential errors existing in the most likely output state sequence for each input observation sequence. kristjansson et al.  proposed this extension with the application to interactive form filling  in which users can examine the filling process and make necessary corrections in terms of their own constraints. a re-correction applied at a particular position will propagate though the viterbi sequence to make automatic updates for labels at other positions  i.e.  the correction propagation capability.
　this section presents the integration of rare-but-important associations with 1%-confidence into the viterbi algorithm in terms of data-driven constraints to make corrections directly to the inference process of crfs. unlike those used in   our constraints are 1%-confidence associations and are automatically discovered from the training data.
　normally  crfs use a variant of the traditional viterbi algorithm to find the most likely state sequence given an input observation sequence. to avoid an exponential-time search over all possible settings of state sequence  this algorithm employs a dynamic programming technique with a forward variable  t+1 si  in definition  1 .
　let r = {r1 r1 ... rq} be a set of q rare-but-important associations with 1%-confidence  and each ru  1 ＋ u ＋ q  has the form . each ru （ r is considered to be a constraint for the inference process. at each time position in the testing data sequence  we check whether or not the set of active context predicates at the current position holds the lhs of any rule ru （ r. if yes  the most likely state path must go though the current state with the label lu  i.e.  the rhs or rule ru   and the possibility of passing though other labels equals to zero. the constrained forward variable is re-defined as follows.
	maxsj  t sj exp	k λkfk sj si o t 
 t+1 si  =	if δ si lu  and {xu1 o t  … ... … xup o t }
1 otherwise
                                                    1  the constraint applied at the time position t will propagate though the whole sequence and make some re-corrections for labels at other positions  mostly around the position t . one problem is that when the number of constraints  i.e.  the number of 1%-confidence rare-but-important predictive association rules  is large  the time for examining the lhs of every rule at each position in the testing sequence also becomes large. to overcome this obstacle  we propose the following algorithm for a fast checking for constraints at a particular time position t in the testing sequence.
　let r = {r1 r1 ... rq} be the set of 1%-confidence rules  also known as constraints  and let x = {x1 x1 ... xm} be the set of m active context predicates observed at the current position t. the target of the following algorithm is to check whether or not x holds the lhs of any constraint ru （ r. if yes  choose the constraint with the longest lhs.
1. for each xi （ x  lookup the set of constraints ri   r in which the lhs of every constraint in ri contains xi. denote .
1. for each constraint be the sum of occurrence frequency of rj in r1  r1  ...  rm.
1. find the pair  such that cj is the largest number satisfying the condition: cj equals to the number of all context predicates in the lhs of rj.
　if this algorithm find a constraint rj  then apply this constraint to the current position t with formula  1   otherwise  apply the normal viterbi recursion as formula  1 .
1. experimental results
1 experimental settings
　all the experiments were performed with our c/c++ implementation of crfs - flexcrfs1 - on 1ghz  1gb ram  pentium iv processor with redhat linux. all crf models were trained using the limited-memory quasi-newton method for unconstrained optimization  l-bfgs . unlike those used in   our crf models are simpler and easier to implement by obeying the first-order markov property  i.e.  the label of the current state depends only on the label of the previous state.
　training and testing data for english phrase chunking and named entity recognition can be found at the shared tasks of conll1 and conll1  respectively.
1 phrase segmentation
　phrase chunking  an intermediate step toward full parsing of natural language  identifies phrase types  e.g.  noun phrase - np  verb phrase - vp  pp - prepositional phrase  etc.  in text sentences. here is an example of a sentence with phrase marking:   np he   vp reckons   np the current account deficit   vp will narrow   pp to   np only #
1 billion   pp in   np september . 
1.1 training and testing data
　the training and testing data for this task is available at the shared task for conll-1. the data consist of the same partitions of the wall street journal corpus  wsj : sections 1 as training data  1 sentences  1 tokens  and section 1 as testing data  1 sentences  1 tokens . each line in the annotated data is for a token and consists of three columns: the token  a word or a punctuation mark   the part-of-speech tag of the token  and the phrase type label  label for short  of the token. the label of each token indicates whether the token is outside a phrase  o   starts a phrase  b-phrasetype   or continues

1
 the documents and source code of flexcrfs are available at http://www.jaist.ac.jp/゛hieuxuan/flexcrfs/flexcrfs.html
1 http://cnts.uia.ac.be/conll1/chunking/
1 http://cnts.uia.ac.be/conll1/ner/ a phrase  i-phrasetype . for example  the label sequence of the above sentence is  b-np b-vp b-np i-np i-np i- np b-vp i-vp b-pp b-np i-np i-np i-np b-pp b-np o . this dataset contains 1 phrase types as shown in the first column of table 1. two consecutive data sequences  sentences  are separated by a blank line.
1.1 feature selection
　on the phrase chunking dataset  we use feature templates as shown in table 1. all transition features obey the first- order markov dependency that the label  l  of the current state depends on the label  l  of the previous state  e.g.   l = i-np  and   = b-np  . each per-state feature expresses how much influence a context predicate  x o t   observed surrounding the current position t has on the label  l  of the current state. a context predicate captures a particular property of the observation sequence. for instance  the per-state feature  l = i-np  and  wordt 1 is the  indicates that the label of the current state should be i-np  i.e.  continue a noun phrase  if the previous word is the.
　table 1 describes both transition and per-state feature templates. context predicates for per-state features are identities of words  pos tags of words surrounding the current position t  such as words and pos tags at positions t   1  t   1  t  t + 1  t + 1  i.e.  window size is 1 .
table 1: feature templates for phrase chunking
transition feature templatescurrent state: stprevious state: st 1llper-state feature templatescurrent state: stcontext predicate: x o t lwt 1; wt 1; wt; wt+1; wt+1 wt 1 … wt; wt … wt+1
pt 1; pt 1; pt; pt+1; pt+1 pt 1 … pt 1; pt 1 … pt pt … pt+1; pt+1 … pt+1 pt 1 … pt 1 … pt pt 1 … pt … pt+1 pt pt+1 pt+1	…	…
　we also employ 1-order conjunctions of the current word with the previous  wt 1 …wt  or the next word  wt …wt+1   and 1-order and 1-order conjunctions of two or three consecutive pos tags within the current window to make use of the mutual dependencies among singleton properties.
　with the feature templates shown in table 1 and the feature rare threshold of 1  i.e.  only features with occurrence frequency larger than 1 are included into the crf model   1 context predicates and 1 crf features were generated from 1 training data sequences.
1.1 mining rare-but-important associations
　let i be the itemset of 1 data items  i.e.  the union set of 1 context predicates and 1 phrase labels; t be the set of 1 data transactions corresponding to 1 tokens of the training data  the maximum transaction length is 1  i.e.  1 context predicate templates plus the label . let the lower support  lsup  and upper support  usup  thresholds be 1 and 1  respectively; the lower confidence  lconf  threshold be 1 or 1%. in fact  all output rules have the confidence of 1% because lconf = 1 and therefore larger than all other confidence levels. we also confine the length of the lhs of all rare-but-important associations between 1 and 1. there are several reasons why we confine the lhs length between 1 and 1. first  although simple rules  i.e.  with shorter lhs length  are usually useful for generalization  we only examine rules complex enough  with lhs length − 1  because our main target is to deal with hard data instances which are not frequently observed in the training data. we also observed that the number of rare-but-important with lhs length smaller than 1 was small. second  rules with lhs length larger than 1 are usually too specific and most of them are covered by rules with lhs length of 1  1 or 1. also  mining and generating all long rules is time-consuming. for these reasons  we only considered rules with lhs length between 1 and 1.
　the mining process for rare-but-important associations took 1 hours using fp-growth algorithm and the filter criteria presented in definitions  1  and  1 . the output was a set of 1 rare-but-important associations with an lhs length between 1 and 1  support between 1 and 1  and confidence of 1%. this set of associations were integrated into the crf model in terms of normal features  normal features with weighted feature values  and constraints for the inference process.
1.1 results
table 1: the performance of english phrase chunking without rare-but-important associations
phrase#hm.#ml.#mt.pr.rc.f1.np11.1.1.1pp11.1.1.1vp11.1.1.1sbar11.1.1.1adjp11.1.1.1advp11.1.1.1prt11.1.1.1lst11.1.1.1intj11.1.1.1conjp11.1.1.1ucp11.1.1.1avg1.111avg1.11.1.1.1table 1: the performance of english phrase chunking with rare-but-important associations as normal features of crfs
phrase#hm.#ml.#mt.pr.rc.f1.np11.1.1.1pp11.1.1.1vp11.1.1.1sbar11.1.1.1adjp11.1.1.1advp11.1.1.1prt11.1.1.1lst11.1.1.1intj11.1.1.1conjp11.1.1.1ucp11.1.1.1avg1.111agv1.11.1.1.1table 1: the performance of english phrase chunking with rare-but-important associations as normal features with weighted feature values
phrase#hm.#ml.#mt.pr.rc.f1.np11.1.1.1pp11.1.1.1vp11.1.1.1sbar11.1.1.1adjp11.1.1.1advp11.1.1.1prt11.1.1.1lst11.1.1.1intj11.1.1.1conjp11.1.1.1ucp11.1.1.1avg1.111agv1.11.1.1.1table 1: the performance of english phrase chunking with rare-but-important associations as constraints for inference
phrase#hm.#ml.#mt.pr.rc.f1.np11.1.1.1pp11.1.1.1vp11.1.1.1sbar11.1.1.1adjp11.1.1.1advp11.1.1.1prt11.1.1.1lst11.1.1.1intj11.1.1.1conjp11.1.1.1ucp11.1.1.1avg1.111agv1.11.1.1.1　table 1 shows the highest performance  achieved at the 1th l-bfgs iteration  of the phrase chunking task trained on the original set of 1 crf features without rare- but-important associations. in each line  the first column is the phrase type; the second  #hm.  is the number of human annotated phrases; the third  #ml.  is the number of phrases automatically marked by the crf model; the fourth  #mt.  is the number of correct phrases marked by the model; the last three columns are precision  pr.   recall  rc.   and f1-measure  f1   respectively. the last two lines are the average performance calculated in two ways: precision-recall based and phrase based; the first is based on the precision and recall values of separated phrase types and the second is based on the average numbers of human- annotated  model  and correct phrases. the first average f1  1%  reflects the balance and the trade-off among per- label performances while the second average f1  1%  reflects the total performance.
　table 1  which has the same format as table 1  describes the performance of phrase segmentation when discovered rare-but-important associations were integrated into the crf model as normal features. the highest average f1- measure achieved at the 1th l-bfgs iteration is 1%  i.e.  1% higher than the original performance.
　table 1 shows the performance when all rare-but-important associations were incorporated into the crf model in the form of features with weighted feature values. the highest average f1-measure  at the 1th iteration  is 1%  i.e.  1% higher than the original performance.
　table 1 describes the performance when all 1%-confidence rare-but-important associations were used as constraints for the inference process. the highest average f1-measure is 1%  i.e.  1% higher than the original performance.
1 named entity recognition
　named entity recognition  ner   a subtask of information extraction  identifies names of persons  per   organizations  org   locations  loc   times  date/time   and quantities  number  currency  percentage  in natural language. here is an example of an english sentence with named entities marked:   loc germany  's representative to the  org european union  's veterinary committee  per werner zwingmann  said on wednesday ... 
1.1 training and testing data
　the training and testing data for english named entity recognition are provided at the shared task for conll-1. the dataset is a collection of news wire articles from the reuters corpus. the training set consists of 1 sentences  1 tokens   and the testing data contains two parts: the development test set  testa: 1 sentences  1 tokens  and the final test set  testb: 1 sentences  1 tokens . the data files contain four columns separated by a blank space. each token  a word or a punctuation mark  has been put on a separate line and there is an empty line after each sentence  sequence . the first item on each line is a token  the second is the part-of-speech tag of the token  the third is a phrase type tag  like the label in phrase chunking  of the token  and the fourth is the named entity label  label for short . the label of each token indicates whether the token is outside a named entity  o   or inside a named entity
namedentitytype . if two named entities of the same type immediately follow each other  the first token of the second named entity will have tag b-namedentitytype. for example  the named entity label sequence of the above sentence is  i-loc o o o o i-org i-org o o o i-per i-per o o o ... .
1.1 feature selection
　on the named entity recognition dataset  we used the feature templates shown in table 1. all transition features also conform to the first markovian property. each context predicate for a per-state feature is one of the following types:  1  the identities of words  wt 1  wt 1  wt  wt+1  wt+1    1  the pos tags of words  pt 1  pt 1  pt  pt+1  pt+1    1  the phrase tags of words  ct 1  ct 1  ct  ct+1  ct+1   and  1  several simple regular expressions or formats of words such as  the first character of a word is capitalized   isinitialcapitalized    all chars of a word are capitalized   isallcapitalized   etc. like the phrase chunking task  all context predicates are captured within a window with size of 1. our feature templates are simpler than those used in the previous work presented at the conll1 shared task and in  in two ways: only five simple format properties were captured  compared to 1 regular expressions in    and no external dictionaries were used such as the lists of people names  organization names  countries  cities  etc.
　with the feature templates described in table 1 and the feature rare threshold of 1  1 context predicates and 1 features were generated from 1 training sequences.
table 1: feature templates for ner
transition feature templatescurrent state: stprevious state: st 1llper-state feature templatescurrent state: stcontext predicate: x o t lwt 1; wt 1; wt; wt+1; wt+1 pt 1; pt 1; pt; pt+1; pt+1 ct 1; ct 1; ct; ct+1; ct+1
isinitialcapitalized wk 
isallcapitalized wk  isnumber wk 
isalphanumber wk 
isfirstword wk 
where k	t	1 t	1 t t + 1 t + 1	（ {  	 	}
1.1 mining rare-but-important associations
　let i be the itemset of 1 data items  i.e.  the union set of 1 context predicates and 1 named entity labels; t be the set of 1 data transactions corresponding to 1 tokens of the training data  the maximum transaction length is 1  i.e.  1 context predicate templates plus the label . let the lower support  lsup  and upper support  usup  thresholds be 1 and 1  respectively; the lower confidence  lconf  threshold be 1 or 1%. we also examine rules with the lhs length between 1 and 1.
　the mining process for rare-but-important associations took about 1 hours using fp-growth algorithm and the filter criteria described in definitions  1  and  1 . the output was a set of 1 rare-but-important associations with an lhs length between 1 and 1  support between 1 and 1  and confidence of 1%. this set of associations was integrated into the crf model in terms of normal features  normal features with weighted feature values  and constraints for the inference process.
1.1 results
table 1: the performance of english named entity recognition without rare-but-important associations
netype#hm.#ml.#mt.pr.rc.f1.org11.1.1.1per11.1.1.1loc11.1.1.1misc11.1.1.1avg1.111avg1.11.1.1.1　table 1 shows the highest performance  f1 of 1%  achieved at the 1th l-bfgs iteration  of the ner task trained on the original set of 1 crf features. this table has the same format as table 1 except that the first column of each line is the named entity type.
　table 1  which has the same format as table 1  displays the experimental results of ner when all rare-but-important table 1: the performance of english named entity recognition with rare-but-important associations as normal features of crfs
netype#hm.#ml.#mt.pr.rc.f1.org11.1.1.1per11.1.1.1loc11.1.1.1misc11.1.1.1avg1.111agv1.11.1.1.1table 1: the performance of english named entity recognition with rare-but-important associations as normal features with weighted feature values
netype#hm.#ml.#mt.pr.rc.f1.org11.1.1.1per11.1.1.1loc11.1.1.1misc11.1.1.1avg1.111agv1.11.1.1.1associations were integrated into crf model in terms of normal features. the highest f1-measure is 1%  i.e.  1% higher than the original performance. table 1 shows the results of ner in the case rare-but-important associations were encoded into the model in the form of normal features with weighted values. the highest average f1 is 1%. table 1 demonstrates the performance when all 1%-confidence rare-but-important associations were integrated into the inference process in terms of viterbi constraints. the highest f1 obtained in this case is 1%.
1 discussion
　we can see that the integration of rare-but-important associations into crf models can improve the performance of both the phrase chunking and named entity recognition tasks. the f1-measure  avg1.  of phrase chunking increases from 1% to 1%  1%  and 1% corresponding to three methods of encoding rare-but-important associations. similarly  the f1-measure of ner increases from 1% to 1%  1%  and 1%. the precision- recall based f1-measure  avg1.  also increases from 1% to 1%  1%  and 1% for phrase chunking and from 1% to 1%  1%  and 1% for named entity recognition. this demonstrates that our approach can improve not only total performance but also the balance among classes/labels.
netype#hm.#ml.#mt.pr.rc.f1.org11.1.1.1per11.1.1.1loc11.1.1.1misc11.1.1.1avg1.111agv1.11.1.1.1　we can also draw some conclusions from the experimental results:  1  rare-but-important associations as normal crf features  the first method  can significantly enhance the total performance; however  treating rare-but-important associations as normal features can not fully utilizes their advantages;  1  rare-but-important associations as constraints for inference  the third method  are sometimes too aggressive because they are globally true on one training dataset but may not be true on another; and  1  treating rare- but-important associations as normal features with emphasized values should be the favorable choice because they are neither too loosely nor too tightly integrated with the table 1: the performance of english named entity recognition with rare-but-important associations as constraints for inference models. the experimental results show that this method achieves both high total performance and a balance among classes/labels.
　we also did the experiments with rare-but-confident rules. we observed two important points that  1  the numbers of rare-but-important rules  both noun phrase chunking and named entity recogniztion  were much larger than those of rare-but-important ones; and  1  the experimental results were sometimes worse because of the overfitting problem. this means that there is a large proportion of rare-but- important rules that are unnecessary for capturing difficult data instances.
　the experimental results reported in this paper do not represent the best possible performances on phrase chunking and named entity recognition because:  1  our feature templates are relatively simple to keep the set of features compact; this is convenient for mining associations  training again and again during conducting the experiments;  1  unlike the crf model in   all our crf models obey the first-order markov property to reduce the number of features and the training time.
1. related work
　discriminative  sequential  learning models have been applied successfully in different natural language processing and information extraction tasks  such as pos tagging   text chunking  1  1   information extraction  1  1   computer vision and image analysis  1  1  1   and biological modeling . normally  one can extract features from sequential data within a relatively large window size  i.e.  the history size of contextual information  and make high-order combinations of atomic observational tests  e.g.  the conjunctions of two or three consecutive words in a sentence  in the hope that they will capture as many useful predictive clues as possible. unfortunately  such useful conjunctions are sparsely distributed in the feature space  and thus one unintentionally includes a large number of redundant conjunctions into the model. inspired by this obstacle  our work aims at picking up useful conjunctions from a large array of conjunction candidates while keeping the set of features simple. the data-driven search with respect to support and confidence factors based on association rule mining techniques can discover desired conjunctions with an acceptable computational time.
　mccallum  proposed an automated feature induction for crfs that can dramatically reduce the number of used features. this likelihood-driven approach repeatedly adds features with high likelihood gains into the model. the set of induced features contains both atomic observational tests and conjunctions of them. the main difference between this work and ours is that mccallum focuses on features with high likelihood-gains in order to reduce the number of used features as much as possible  while the main target of our method is to discover rare-but-important associations or co-occurrences of weak statistics from the training data to highlight difficult examples. further  our method can examine any combination or conjunction of context predicates because of the exhaustive working method of association rule mining techniques.
　an error-driven method that combines boosting technique into the training process of crfs  to minimize an upper bound on the ranking loss that was adapted to label sequences. this method also focuses on hard observation sequences  but without integrating new useful conjunctions of basic features. another boosting-like training for crfs is based on the use of  gradient tree   to learn many conjunctions of features. one problem is that this method requires adding many trees for the training process.
1. conclusions and future work
　in this paper  we proposed a data-driven approach that can discover and highlight rare-but-important associations or co-occurrences of singleton context predicates from the sequential training data to deal with hard examples. discovered associations are integrated into the exponentiallytrained sequential learning models as normal features  features with weighted values  and constraints for the inference process. the experimental results show that rare-but- important associations can improve the model performance by fighting against the dominance of singleton but common statistics in the training data.
　though rare-but-important associations can enhance the prediction accuracy for hard examples  our approach is currently based on the occurrence frequency of statistics and the existence of rare-but-important associations in the training data. we believe that there is an indirect theoretical relation between the occurrence frequencies of statistics and the learned weights of the model's features. our future work will focus on this potential relation to estimate the extent to which useful patterns  e.g.  rare-but-important associations  discovered from the training data can improve the performance of discriminative  sequential  learning models.
1. acknowledgments
　we would like to thank dr. bart goethals  department of math and computer science  antwerpen university  for sharing his lightweight and efficient implementation of the fp-growth algorithm. we would like to say thank you to prof. jorge nocedal  department of electrical and computer engineering  school of engineering and applied science  northwestern university  the author of fortran implementation of the l-bfgs optimization procedure. we also would like to thank prof. sunita sarawagi  kr school of information technology  iit bombay  the author of the java crfs package  which is the precursor of our c/c++ crfs toolkit.
