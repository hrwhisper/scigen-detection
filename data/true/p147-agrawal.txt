pattern matching over event streams is increasingly being employed in many areas including financial services  rfidbased inventory management  click stream analysis  and electronic health systems. while regular expression matching is well studied  pattern matching over streams presents two new challenges: languages for pattern matching over streams are significantly richer than languages for regular expression matching. furthermore  efficient evaluation of these pattern queries over streams requires new algorithms and optimizations: the conventional wisdom for stream query processing  i.e.  using selection-join-aggregation  is inadequate.
　in this paper  we present a formal evaluation model that offers precise semantics for this new class of queries and a query evaluation framework permitting optimizations in a principled way. we further analyze the runtime complexity of query evaluation using this model and develop a suite of techniques that improve runtime efficiency by exploiting sharing in storage and processing. our experimental results provide insights into the various factors on runtime performance and demonstrate the significant performance gains of our sharing techniques.
categories and subject descriptors
h.1  database management : systems general terms
algorithms  design  performance  theory
keywords
event streams  pattern matching  query optimization
1.	introduction
　pattern matching over event streams is a new processing paradigm where continuously arriving events are matched

 
 this work has been supported in part by nsf grants ccf 1 and ccf 1 and a gift from cisco. *authors of this paper are listed alphabetically.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
against complex patterns and the events used to match each pattern are transformed into new events for output. recently  such pattern matching over streams has aroused significant interest in industry  1  1  1  1  due to its wide applicability in areas such as financial services   rfidbased inventory management   click stream analysis   and electronic health systems . in financial services  for instance  a brokerage customer may be interested in a sequence of stock trading events that represent a new market trend. in rfid-based tracking and monitoring  applications may want to track valid paths of shipments and detect anomalies such as food contamination in supply chains.
　while regular expression matching is a well studied computer science problem   pattern matching over streams presents two new challenges:
　richer languages. languages for pattern matching over event streams  1  1  are significantly richer than languages for regular expression matching. these event pattern languages contain constructs for expressing sequencing  kleene closure  negation  and complex predicates  as well as strategies for selecting relevant events from an input stream mixing relevant and irrelevant events. of particular importance is kleene closure that can be used to extract from the input stream a finite yet unbounded number of events with a particular property. as shown in   the interaction of kleene closure and different strategies to select events from the input stream can result in queries significantly more complex than regular expressions.
　efficiency over streams. efficient evaluation of such pattern queries over event streams requires new algorithms and optimizations. the conventional wisdom for stream query processing has been to use selection-join-aggregation queries  1  1  1  1 . while such queries can specify simple patterns  they are inherently unable to express kleene closure because the number of inputs that may be involved is a priori unknown  which we shall prove formally in this paper . recent studies  1  1  1  have started to address efficient evaluation of pattern queries over streams. the proposed techniques  however  are tailored to various restricted sets of pattern queries and pattern matching results  such as patterns without kleene closure   patterns only on contiguous events   and pattern matching without output of complete matches .
　the goal of this work is to provide a fundamental evaluation and optimization framework for the new class of pattern queries over event streams. our query evaluation framework departs from well-studied relational stream processing due to its inherent limitation as noted above. more specifically 
 a  query 1:
pattern seq shelf a  ゛ register b   exit c 
where    skip till next match a  b  c  {                   a.tag id = b.tag id        and     a.tag id = c.tag id 
                  /* equivalently   tag id  */     } 
within    1 hours b  query 1:
pattern seq alert a  shipment+ b     where    skip till any match a  b     {                     a.type = 'contaminated'          and       b.from = a.site        and       b i .from = b i-1 .to }
within    1 hours c  query 1:
pattern seq stock+ a     stock b 
where    skip till next match a     b  {
                     symbol          and       a.volume   1
       and       a i .price   avg a ..i-1 .price         and       b.volume   1%*a a.len .volume } within    1 hourfigure 1: examples of event pattern queries.the design of our query evaluation framework is based on three principles: first  the evaluation framework should be sufficient for the full set of pattern queries. second  given such full support  it should be computationally efficient. third  it should allow optimization in a principled way. following these principles  we develop a data stream system for pattern query evaluation. our contributions include:
  formal evaluation model. we propose a formal query evaluation model  nfab  that combines a finite automaton with a match buffer. this model offers precise semantics for the complete set of event pattern queries  permits principled optimizations  and produces query evaluation plans that can be executed over event streams. the nfab model also allows us to analyze its expressibility in relation to relational stream processing  yielding formal results on both sufficiency and efficiency for pattern evaluation.
  runtime complexity analysis. given the new abstraction that nfab-based query plans present  we identify the key issues in runtime evaluation  in particular  the different types of non-determinism in automaton execution. we further analyze worst-case complexity of such query evaluation  resulting in important intuitions for runtime optimization.
  runtime algorithms and optimizations. we develop new data structures and algorithms to evaluate nfab-based query plans over streams. to improve efficiency  our optimizations exploit aggressive sharing in storage of all possible pattern matches as well as in automaton execution to produce these matches.
　we have implemented all of the above techniques in a java-based prototype system and evaluated nfab based query plans using a range of query workloads. results of our performance evaluation offer insights into the various factors on runtime performance and demonstrate significant performance gains of our sharing techniques.
　the remainder of the paper is organized as follows. we provide background on event pattern languages in section 1. we describe the three technical contributions mentioned above in section 1  section 1  and section 1  respectively. results of a detailed performance analysis are presented in section 1. we cover related work in section 1 and conclude the paper with remarks on future work in section 1.
1.	background
　in this section  we provide background on event pattern languages  which offers a technical context for the discussion in the subsequent sections.
　recently there have been a number of pattern language proposals including sql-ts   cayuga  1  1   sase+  1  1   and cedr .1 despite their syntactic variations  these languages share many features for pattern matching over event streams. below we survey the key features of pattern matching using the sase+ language since it is shown to be richer than most other languages . this language uses a simple event model: an event stream is an infinite sequence of events  and each event represents an occurrence of interest at a point in time. an event contains the name of its event type  defined in a schema  and a set of attribute values. each event also has a special attribute capturing its occurrence time. events are assumed to arrive in order of
the occurrence time.1
　a pattern query addresses a sequence of events that occur in order  not necessarily in contiguous positions  in the input stream and are correlated based on the values of their attributes. figure 1 shows three such queries.
　query 1 detects shoplifting activity in rfid-based retail management : it reports items that were picked at a shelf and then taken out of the store without being checked out. the pattern clause specifies a sequence pattern with three components: the occurrence of a shelf reading  followed by the non-occurrence of a register reading  followed by the occurrence of an exit reading. non-occurrence of an event  denoted by '゛'  is also referred to as negation.
　each component declares a variable to refer to the corresponding event. the where clause uses these variables to specify predicates on individual events as well as across multiple events  enclosed in the '{' '}' pair . the predicates in query 1 require all events to refer to the same tag id. such equality comparison across all events is referred to as an equivalence test  a shorthand for which is ' tag id ' . finally  the query uses a within clause to specify a 1-hour time window over the entire pattern.
　query 1 detects contamination in a food supply chain: it captures an alert for a contaminated site and reports a unique series of infected shipments in each pattern match. here the sequence pattern uses a kleene plus operator to compute each series of shipments  where '+' means one or more . an array variable b    is declared for the kleene plus component  with b referring to the shipment from the origin of contamination  and b i  referring to each subsequent shipment infected via collocation with the previous one. the predicates in where clearly specify these constraints on the shipments; in particular  the predicate that compares b i  with b i   1   i   1  specifies the collocation condition between each shipment and its preceding one.
　query 1 captures a complex stock market trend: in the past hour  the volume of a stock started high  but after a period when the price increased or remained relatively stable  the volume plummeted. this pattern has two components  a kleene plus on stock events  whose results are in a     and a separate single stock event  stored in b. the predicate on a addresses the initial volume. the predicate on a i   i   1  requires the price of the current event to exceed the average of the previously selected events  those previously selected events are denoted by a ..i   1  . this way  the predicate captures a trend of gradual  not necessarily monotonic  price increase. the last predicate compares b to a a.len   where a.len refers to the last selected event in a     to capture the final drop in volume.
　besides the structure and predicates  pattern queries are further defined using the event selection strategy that addresses how to select the relevant events from an input stream mixing relevant and irrelevant events. the strategy used in a query is declared as a function in the where clause which encloses all the predicates in its body  as shown in figure 1. the diverse needs of stream applications require different strategies to be used:
　strict contiguity. in the most stringent event selection strategy  two selected events must be contiguous in the input stream. this requirement is typical in regular expression matching against strings  dna sequences  etc.
　partition contiguity. a relaxation of the above is that two selected events do not need to be contiguous; however  if the events are conceptually partitioned based on a condition  the next relevant event must be contiguous to the previous one in the same partition. the equivalence tests  e.g.   symbol  in query 1  are commonly used to form partitions. partition contiguity  however  may not be flexible enough to support query 1 if it aims to detect the general trend of price increase despite some local fluctuating values.
　skip till next match. a further relaxation is to completely remove the contiguity requirements: all irrelevant events will be skipped until the next relevant event is read. using this strategy  query 1 can conveniently ignore all the readings of an item that arise between the first shelf reading and an exit or register reading. similarly  query 1 can skip values that do not satisfy the defined trend. this strategy is important in many real-world scenarios where some events in the input are  semantic noise  to a particular pattern and should be ignored to enable the pattern matching to continue.
　skip till any match. finally  skip till any match relaxes the previous one by further allowing non-deterministic actions on relevant events. query 1 illustrates this use. suppose that the last shipment selected by the kleene plus reaches the location x. when a relevant shipment  e.g.  from x to y  is read from the input stream  skip till any match has two actions:  1  it selects the event in one instance of execution to extend the current series  and  1  it ignores the event in another instance to preserve the current state of kleene closure  i.e. location x  so that a later shipment  e.g.  from x to z  can be recognized as a relevant event and enable a different series to be instantiated. this strategy essentially computes transitive closure over relevant events  e.g.  all infected shipments in three hours  as they arrive.
　finally  each match of a pattern query  e.g.  the content of a    and b variables for query 1  is output as a composite event containing all the events in the match. two output formats are available  1  1 : the default format returns all matches of a pattern. in contrast  the non-overlap format outputs only one match among those that belong to the same partition  for strict contiguity  treat the input stream as a single partition  and overlap in time; that is  one match in a partition is output only if it starts after the previous match completes. language support is also available to compute summaries for composite events and compose queries by feeding events output from one query as input to another  1  1 . these additional features are not a focus of this paper and can be readily plugged in the query evaluation framework proposed below.
1.	formal semantic model
　after describing event pattern queries  we study their evaluation and optimization in the rest of the paper. in this section  we present a formal evaluation model that offers precise semantics for this new class of pattern queries  ′1 . we also offer compilation algorithms that translate pattern queries to representations in this model  thereby producing query evaluation plans for runtime use  ′1 . this model further allows us to analyze its expressibility in relation to relational stream processing  yielding formal results on both sufficiency and efficiency for pattern evaluation  ′1  .
1	an evaluation model: nfab automaton
　our query evaluation model employs a new type of automaton that comprises a nondeterministic finite automaton  nfa  and a match buffer  thus called nfab  to represent each pattern query. formally  an nfab automaton  a =  q e θ q1 f   consists of a set of states  q  a set of directed edges  e  a set of formulas  θ  labelling those edges  a start state  q1  and a final state  f. the nfab for query 1 is illustrated in figure 1
　states. in figure 1 a   the start state  a  is where the matching process begins. it awaits input to start the kleene plus and to select an event into the a unit of the match buffer. at the next state a i   it attempts to select another event into the a i   i   1  unit of the buffer. the subsequent state b denotes that the matching process has fulfilled the kleene plus  for a particular match  and is ready to process the next pattern component. the final state  f  represents the completion of the process  resulting in the creation of a pattern match.
　in summary  the set of states q is arranged as a linear sequence consisting of any number of occurrences of singleton states  s  for non-kleene plus components  or pairs of states  p p i   for kleene plus components  plus a rightmost final state  f. a singleton state is similar to a p state but without a subsequent p i  state.
　edges. each state is associated with a number of edges  representing the actions that can be taken at the state. as figure 1 a  shows  each state that is a singleton state or the first state  p  of a pair has a forward begin edge. each second state  p i   of a pair has a forward proceed edge  and a looping take edge. every state  except the start and final states  has a looping ignore edge. the start state has no edges to it as we are only interested in matches that start with selected events.

θa i  ignore = 
  a i .symbol=a.symbol …    a i .price avg a ..i-1 .price θb ignore = 
  b.symbol=a.symbol …
   b.volume 1%*a a.len .volume  c  example formulas after optimization
θ*a i  take = θa i  take … a i .time a.time+1 hourθ*a i  ignore = θa i  ignore … a i .time a.time+1 hourθ*a i  proceed = θb begin ‥   θ*a i  take …  θ*a i  ignore figure 1: the nfab automaton for query 1.
　each edge at a state  q  is precisely described by a triplet:  1  a formula that specifies the condition on taking it  denoted by θq edge   1  an operation on the input stream  i.e.  consume an event or not   and  1  an operation on the match buffer  i.e.  write to the buffer or not . formulas of edges are compiled from pattern queries  which we explain in detail shortly. as shown in figure 1 a   we use solid lines to denote begin and take edges that consume an event from the input and write it to the buffer  and dashed lines for ignore edges that consume an event but do not write it to the buffer. the proceed edge is a special -edge: it does not consume any input event but only evaluates its formula and tries proceeding. we distinguish the proceed edge from ignore edges in the style of arrow  denoting its  behavior.
　non-determinism. nfab automata may exhibit nondeterminism when at some state the formulas of two edges are not mutually exclusive. for example  if θp i  take and θp i  ignore are not mutually exclusive  then we are in a nondeterministic skip-till-any-match situation. it is important to note that such non-determinism stems from the query; the nfab model is merely a truthful translation of it.
　nfab runs. a run of an nfab automaton is uniquely defined by  1  the sequence of events that it has selected into the match buffer  e.g.  e1  e1 and e1   1  the naming of the corresponding units in the buffer  e.g.  a  a  and b for query 1  and  1  the current nfab state. we can inductively define a run based on each begin  take  ignore  or proceed move that it takes. moreover  an accepting run is a run that has reached the final state. the semantics of a pattern query is precisely defined from all its accepting runs. these concepts are quite intuitive and the details are omitted in the interest of space.
　pattern queries with negation and query composition are modeled by first creating nfab automata for subqueries without them and then composing these automata. in particular  the semantics of negation is that of a nested query  as proposed in . for instance  query 1 from figure 1 first recognizes a shelf reading and an exit reading that refer to the same tag; then for each pair of such readings it ensures that there does not exist a register reading of the same tag in between. to support negation using nfab  we first compute matches of the nfab automaton that includes only the positive pattern components  then search for matches of the nfab automaton for each negative component. any match of the latter eliminates the former from the answer set.
1	query compilation using nfab
　we next present the compilation rules for automatically translating simple pattern queries  without negation or composition  into the nfab model. composite automata for negation or composed queries can be constructed afterwards by strictly following their semantics. the resulting representations will be used as query plans for runtime evaluation over event streams.
　basic algorithm. we first develop a basic compilation algorithm that given a simple pattern query  constructs an nfab automaton that is faithful to the original query. in the following  we explain the algorithm using query 1 as a running example.
　step 1. nfab structure: as shown in figure 1  the pattern clause of a query uniquely determines the structure of its nfab automaton  including all the states and the edges of each state.
　the algorithm then translates the where and within clauses of a query into the formulas on the nfab edges.
　step 1. predicates: the algorithm starts with the where clause and uses the predicates to set formulas of begin  take  and proceed edges  as shown in figure 1 b .1 it first rewrites all the predicates into conjunctive normal form  cnf   including expanding the equivalence test  symbol  to a canonical form  e.g.  a i .symbol = a.symbol. it then sorts the conjuncts based on the notion of their last identifiers. in this work  we call each occurrence of a variable in the where clause an identifier  e.g.  a  a i   a a.len   and b for query 1. the last identifier of a conjunct is the one that is instantiated the latest in the nfab automaton. consider the conjunct  b.volume   1% * a a.len .volume . between the identifiers b and a a.len   b is instantiated at a later state.
　after sorting  the algorithm places each conjunct on an edge of its last identifier's instantiation state. at the state a i  where both take and proceed edges exist  the conjunct is placed on the take edge if the last identifier is a i   and on the proceed edge otherwise  e.g.  the identifier is a a.len  . for query 1  the proceed edge is set to true due to the lack of a predicate whose last identifier is a a.len .
　step 1. event selection strategy: the formulas on the ignore edges depend on the event selection strategy in use. despite a spectrum of strategies that pattern queries may use  our algorithm determines the formula of an ignore edge at a state q  θq ignore  in a simple  systematic way:
	strict contiguity:	false
	partition contiguity:	   partition condition 
skip till next match:	   take or begin condition  skip till any match:	true
　as shown above  when strict contiguity is applied  θq ignore is set to false  disallowing any event to be ignored. if partition contiguity is used  θq ignore is set to the negation of the partition definition  thus allowing the events irrelevant to a partition to be ignored. for skip till next match  θq ignore is set to the negation of the take or begin condition depending on the state. revisit query 1. as shown in figure 1 b   θa i  ignore is set to  θa i  take at the state a i   causing all events that do not satisfy the take condition to be ignored. finally  for skip till any match  θq ignore is simply set to true  allowing any  including relevant  event to be ignored.
　step 1. time window: finally  on the begin or proceed edge to the final state  the algorithm conjoins the within condition for the entire pattern. this condition is simply a predicate that compares the time difference between the first and last selected events against the specified time window.
　optimizations. in our system  the principle for compiletime optimization is to push stopping and filtering conditions as early as possible so that time and space are not wasted on non-viable automaton runs. we highlight several optimizations below:
　step 1. pushing the time window early: the within condition  currently placed on the final edge to f  can be copied onto all take  ignore  and begin edges at earlier states. this allows old runs to be pruned as soon as they fail to satisfy the window constraint. despite the increased number of predicates in all edge formulas  the benefit of pruning nonviable runs early outweighs the slight overhead of predicate evaluation. figure 1 c  shows θa i  take and θa i  ignore after this optimization for query 1.
　step 1. constraining proceed edges: we next optimize a proceed edge if its current condition is true and the subsequent state is not the final state  which is the case with query 1. at the state a i   this proceed edge causes nondeterminism with the take  or ignore  edge  resulting in a new run created for every event. to avoid non-viable runs  we restrict the proceed move by  peeking  at the current event and deciding if it can satisfy the begin condition of the next state b. we disallow a proceed move in the negative case. an exception is that when the take and ignore edges at a i  both evaluate to false  we would allow an opportunistic move to the state b and let it decide what can be done next. the resulting θa i  proceed is also shown in figure 1 c .
　it is important to note that while our compilation techniques are explained above using pattern queries written in the sase+ language   all the basic steps  steps 1  and optimizations  steps 1  are equally applicable to other pattern languages  1  1  1 .
1	expressibility of nfab
　in this section  we provide an intuitive description of the expressibility of the nfab model  while omitting the formal proofs in the interest of space  detailed proofs are available in  . we briefly describe the set  d nfab   that consists of the stream decision problems recognizable by nfab automata.
　proposition 1. d nfab  includes problems that are complete for nondeterministic space logn  nspace logn   and is contained in the set of problems recognizable by readonce-left-to-right nspace logn  machines .
　the idea behind the proof of the first part of proposition 1 is that a single kleene plus in a skip-till-any-match query suffices to express directed graph reachability which is complete for nspace logn . query 1 is an example of this. conversely  an nfab reads its stream once from left to right  recording a bounded number of fields  including aggregates  each of which requires o logn  bits.
	θignore	θignore
  a θbegin	a i  θproceed	b θbegin	f
	θtake	results a   	b
events
r1  e1 e1 e1 e1 e1             e1
 e1 e1                            e1 e1 e1 e1 e1 e1 e1 e1   e1	e1    e1     e1     e1     e1      e1     e1   e1 ...	r1
price	1    1     1     1     1      1     1    1
r1
volume 1   1     1   1     1      1     1    1	  ...                                           ...       
figure 1: example pattern matches for query 1.
　we can also prove that any boolean selection-join-aggregation query  a subset of sql that relational stream systems mostly focus on  is in d nfab . furthermore as is well known  no first-order query even with aggregation can express graph reachability . thus  query 1 is not expressible using just selection-join-aggregation. formally  we have
　proposition 1. the set of boolean selection-join-aggregation queries as well as the set of queries in regular languages are strictly contained in d nfab .
　finally  full sql with recursion  expresses all polynomialtime computable queries over streams   so this is a strict superset of d nfab . however  this language includes many prohibitively expensive queries that are absolutely unnecessary for pattern matching over event streams.
1.	runtime complexity
　having presented the query evaluation model and compilation techniques  we next turn to the design of a runtime engine that executes nfab-based query plans over event streams. the new abstraction that these query plans present and the inherent complexity of their evaluation raise significant runtime challenges. in this section  we describe these challenges in ′1 and present analytical results of the runtime complexity in ′1. our runtime techniques for efficient query evaluation are presented in the next section.
1	key issues in runtime evaluation
　the runtime complexity of evaluating pattern queries is reflected by a potentially large number of simultaneous runs  some of which may be of long duration.
　simultaneous runs. for a concrete example  consider query 1 from figure 1  and its execution over an event stream for a particular stock  shown in figure 1. two patterns matches r1 and r1 are produced after e1 arrives  and several more including r1 are created after e1. these three matches  r1  r1  and r1  overlap in the contained events  which result from three simultaneous runs over the same sequence of events.
　there are two sources of simultaneous runs. one is that an event sequence initiates multiple runs from the start state and a newer run can start before an older run completes.
for example  e1 and e1 in figure 1 both satisfy θa begin and thus initiate two overlapping runs corresponding to r1 and r1. a more significant source is the inherent nondeterminism in nfab  which arises when the formulas of two edges from the same state are not mutually exclusive  as described in ′1. there are four types of nondeterminism in the nfab model:
　take-proceed. consider the run initiated by e1 in figure 1. when e1 is read at the state a i   this event satisfies both θa i  take and θa i  proceed  causing the run to split by taking two different moves and later create two distinct yet overlapping matches r1 and r1. such take-proceed nondeterminism inherently results from the query predicates; it can occur even if strict or partition contiguity is used.
　ignore-proceed. when the event selection strategy is relaxed to skip till next match  the ignore condition θa i  ignore is also relaxed  as described in ′1. in this scenario  the ignore-proceed nondeterminism can appear if θa i  ignore and θa i  proceed are not exclusive  as in the case of query 1.
　take-ignore. when skip till any match is used  θa i  ignore is set to true. then the take-ignore nondeterminism can arise at the a i  state.
　begin-ignore. similarly  when skip till any match is used  the begin-ignore nondeterminism can occur at any singleton state or the first state of a pair for the kleenu plus.
　duration of a run. the duration of a run is largely determined by the event selection strategy in use. when contiguity requirements are used  the average duration of runs is shorter since a run fails immediately when it reads the first event that violates the contiguity requirements. in the absence of contiguity requirements  however  a run can stay longer at each state by ignoring irrelevant events while waiting for the next relevant event. in particular  for those runs that do not produce matches  they can keep looping at a state by ignoring incoming events until the time window specified in the query expires.
1	complexity analysis
　for a formal analysis of the runtime complexity  we introduce the notion of partition window that contains all the events in a particular partition that a run needs to consider. let t be the time window specified in the query and c be the maximum number of events that can have the same timestamp. also assume that the fraction of events that belong to a particular partition is p  as a special case  strict contiguity treats the input stream as a single partition  so p = 1% . then the size of the partition window  w  can be estimated using tcp.
　the following two propositions calculate a priori worstcase upper bounds on the number of runs that a pattern query can have. the proofs are omitted in this paper. the interested reader is referred to  for details of the proofs.
　proposition 1. given a run ρ that arrives at the state p i  of a pair in an nfab automaton  let rp i  w  be the number of runs that can branch from ρ at the state p i  while reading w events. the upper bound of rp i  w  depends on the type s  of nondeterminism present:
 i  take-proceed nondeterminism  which can occur with anyevent selection strategy  allows a run to branch in a number of ways that is at most linear in w.
 ii  ignore-proceed nondeterminism  which is allowed by skiptill-next-match or skip-till-any-match  also allows a run to branch in a number of ways that is at most linear in w.  iii  take-ignore nondeterminism  allowed by skip-till-anymatch  allows a run to branch in a number of ways that is exponential in w.
　proposition 1. given a run ρ that arrives at a singleton state  s  or the first state of a pair  p  in an nfab automaton  the number of ways that it can branch while reading w events  rs/p w   is at most linear in w when skip-tillany-match is used  otherwise it is one.
　given an nfab automaton with states q1  q1  ...  qm = f  the number of runs that can start from a given event e  r e  grows with the number of the runs that can branch at each automaton state except the final state. that is  r e = rq1 w1  rq1 w1 ... rqm 1 wm 1   where w1  w1  ...  wm 1 are the numbers of events read at the states q1  q1  ...  qm 1 respectively  and . obviously    . then all the runs that can start from a sequence of events e1  ...  ew is at most
. following propositions 1 and 1  we have the following upper bounds on the total number of runs for a query:
　corollary 1. in the absence of skip till any match  the number of runs that a query can have is at most polynomial in the partition window w  where the exponent is bounded by the number of states in the automaton. in the presence of skip till any match  the number of runs can be at most exponential in w.
　these worst case bounds indicate that a naive approach that implements runs separately may not be feasible. in particular  each run incurs a memory cost for storing a partial or complete match in the buffer. its processing cost consists of evaluating formulas and making transitions for each input event. it is evident that when the number of runs is large  the naive approach that handles runs separately will incur excessively high overhead in both storage and processing.
　importance of sharing. the key to efficient processing is to exploit sharing in both storage and processing across multiple  long-standing runs. our data structures and algorithms that support sharing  including a shared match buffer for all runs and merging runs in processing  are described in detail in the next section. in the following  we note two important benefits of such sharing across runs.
　sharing between viable and non-viable runs. viable runs reach the final state and produce matches  whereas nonviable runs proceed for some time but eventually fail. effective sharing between viable runs and non-viable runs allow storage and processing costs to be reduced from the total number of runs to the number of actual matches for a query. when most runs of a query are non-viable  the benefit of such sharing can be tremendous.
　sharing among viable runs. sharing can further occur between runs that produce matches. if these runs process and store the same events  sharing can be applied in certain scenarios to reduce storage and processing costs to even less than what the viable runs require collectively. this is especially important when most runs are viable  rendering the number of matches close to the total number of runs.
　coping with output cost. the cost to output query matches is linear in the number of matches. if a query produces a large number of matches  the output cost is high even if we can detect these matches more efficiently using sharing. to cope with this issue  we support two output modes for applications to choose based on their uses of the matches and requirements of runtime efficiency. the verbose mode enumerates all matches and returns them separately. hence  applications have to pay for the inherent cost of doing so. the compressed mode returns a set of matches  e.g.  those ending with the same event  in a compact data structure  in particular  the data structure that we use to implement a shared match buffer for all runs. once provided with a decompression algorithm  i.e.  an algorithm to

figure 1: creating a shared versioned buffer for q1.
retrieve matches from the compact data structure  applications such as a visualization tool have the flexibility to decide which matches to retrieve and when to retrieve them.
1.	runtime techniques
　based on the insights gained from the previous analysis  we design runtime techniques that are suited to the new abstraction of nfab-based query plans. in particular  the principle that we apply to runtime optimization is to share both storage and processing across multiple runs in the nfab-based query evaluation.
1	a shared versioned match buffer
　the first technique constructs a buffer with compact encoding of partial and complete matches for all runs. we first describe a buffer implementation for an individual run  and then present a technique to merge such buffers into a shared one for all the runs.
　the individual buffers are depicted in figure 1 a - c  for the three matches from figure 1. each buffer contains a series of stacks  one for each state except the final state. each stack contains pointers to events  or events for brevity  that triggered begin or take moves from this state and thus were selected into the buffer. further  each event has a predecessor pointer to the previously selected event in either the same stack or the previous stack. when an event is added to the buffer  its pointer is set. for any event that triggers a transition to the final state  a traversal in the buffer from that event along the predecessor pointers retrieves the complete match.
　we next combine individual buffers into a single shared one to avoid the overhead of numerous stacks and replicated events in them. this process is based on merging the corresponding stacks of individual buffers  in particular  merging the same events in those stacks while preserving their predecessor pointers. care should be taken in this process  however. if we blindly merge the events  a traversal in the shared buffer along all existing pointers can produce erroneous results. suppose that we combine the buffers for r1 and r1 by merging e1 in the a i  stack and e1 in the b stack. a traversal from e1 can produce a match consisting of e1  e1  e1  e1  and e1  which is a wrong result. this issue arises when the merging process fails to distinguish pointers from different buffers.
　to solve the problem  we devise a technique that creates a shared versioned buffer. it assigns a version number to each run and uses it to label all pointers created in this run. an issue is that runs do not have pre-assigned version
  a  structure of computation state for query 1
 1  version number v; 1  current state q; 1  pointer to recent event in buffer pe;
1  start time t; 
identifier  
aa i a i a i  symbolprice*volume1  value vector v = attribute    
	operation   set  	sum  	count  	set  
	1	1
 b  run ρr1: after e1
1 v=1; 1 q=a i ; 1 pe=e1 in stack a i ;
1 t=e1.time; 
	goog 1	1
1 v =	1	1
 c  run ρr1: after e1
1 v=1; 1 q=a i ; 1 pe=e1 in stack a i ;
1 t=e1.time; 
	goog 1	1
1 v =	1	1figure 1: computation state of runs for q1.
numbers  as the non-determinism at any state can spawn new runs. in this technique  the version number is encoded as a dewey number that dynamically grows in the form of id1 .idj    1 ＋ j ＋ t   where t refers to the current state qt. intuitively  it means that this run comes from the initiation from the start state  and the idthj instance of splitting at the state qj from the run that arrived at the state  which we call an ancestor run. this technique also guarantees that the version number v of a run is compatible with v1 of its ancestor run  in one of the forms:  i  v contains v1 as a prefix  or  ii  v and v1 only differ in the last digit idt and idt of v is greater than that of v1.
　a shared versioned buffer that combines the three matches is shown in figure 1 d . all pointers from an individual buffer now are labeled with compatible version numbers. the erroneous result mentioned above no longer occurs  because the pointer from e1 to e1 with the version number 1.1 is not compatible with the pointer from e1 to e1  in the a i  stack  with the version number 1.
　as can be seen  the versioned buffer offers compact encoding of all matches. in particular  the events and the pointers with compatible version numbers constitute a versioned view that corresponds exactly to one match. to return a match produced by a run  the retrieval algorithm takes the dewey number of the run and performs a traversal from the most recent event in the last stack along the compatible pointers. this process is as efficient as the retrieval of a match from an individual buffer.
1	nfab execution with multiple runs
　each run of nfab proceeds in two phases. in the pattern matching phase  it makes transitions towards the final state and extends the buffer as events are selected. in the match construction phase  it retrieves a match produced by this run from the buffer  as described in the previous section. our discussion in this section focuses on algorithms for efficient pattern matching.
1.1	basic algorithm
　we first seek a solution to evaluate individual runs as efficiently as possible. our solution is built on the notion of computation state of a run  which includes a minimum set of values necessary for future evaluation of edge formulas. take query 1. at the state a i   the evaluation of the take edge requires the value avg a ..i   1 .price . the buffer can be used to compute such values from the contained events  but it may not always be efficient. we trade off a little space for performance by creating a small data structure to maintain the computation state separately from the buffer.
figure 1 a  shows the structure of the computation state for query 1. it has five fields: 1  the version number of a run  1  the current automaton state that the run is in  1  a pointer to the most recent event selected into the buffer in this run  1  the start time of the run  and 1  a vector v containing the values necessary for future edge evaluation. in particular  the vector v is defined by a set of columns  each capturing a value to be used as an instantiated variable in some formula evaluation.
　revisit the formulas in figure 1. we extract the variables to be instantiated from the right operands of all formulas  and arrange them in v by the instantiation state  then the attribute  and finally the operation. for example  the 1st column in the v vector in figure 1 a  means that when we select an event for a  store its symbol for later evaluation of the equivalence test. the 1nd and 1rd columns jointly compute the running aggregate avg a ..i 1 .price : for each event selected for a i   the 1nd column retrieves its price and updates the running sum  while the 1rd column maintains the running count. the 1th column stores the volume of the last selected a i  to evaluate the formula involving b.
　for each run  a dynamic data structure is used to capture its current computation state. figure 1 b  and 1 c  depict the computation state of two runs ρr1 and ρr1 of the nfab for query 1. their states shown correspond to r1 and r1 after reading the event e1 in figure 1.
　when a new event arrives  each run performs a number of tasks. it first examines the edges from the current state by evaluating their formulas using the v vector and the start time of the run. the state can have multiple edges  e.g.  take  ignore  and proceed edges at the state a i    and any subset of them can be evaluated to true. if none of the edge formulas is satisfied  the run fails and terminates right away; common cases of such termination are failures to meet the query-specified time window or contiguity requirements. if more than one edge formula is satisfied  the run splits by cloning one or two child runs. then each resulting run  either the old run or a newly cloned run  takes its corresponding move  selects the current event into the buffer if it took a take or begin move  and updates its computation state accordingly.
　finally  we improve the basic algorithm when the nonoverlap output format described in ′1 is used. recall that this format outputs only one match among those that belong to the same partition and overlap in time. since we do not know a priori which run among the active ones for a particular partition will produce a match first  we evaluate all the runs in parallel as before. when a match is actually produced for a partition  we simply prune all other runs for the same partition from the system.
1.1	merging equivalent runs
　to improve the basic algorithm that evaluates runs separately  we propose to identify runs that overlap in processing and merge them to avoid repeated work. the idea again stems from an observation of the computation state. if two runs  despite their distinct history  have the same computation state at present  they will select the same set of events until completion. in this case  we consider these two runs equivalent. figure 1 shows an example  where query 1 is modified by replacing the running aggregate avg   with max  . the structure of its computation state is modified accordingly as shown in part  b . the column in bold is the new column for the running aggregate max   on a i . parts
 a  query 1: change the aggregate in query 1 to  a i .price − max a ..i-1 .price  

 b  structure of computation state
1  v; 1  q; 1  pe; 1  t; 1  values v
1  state mer
    ma i :
    mb:aa i a i symbolpricevolumeset  	max  	set  
	1	1ging masks m	1	1	1	1
 c  run ρi: after e1
1 v=1; 1 q=a i ; 1 pe=e1 in stack a i ;
1  t=e1.time; 1  v = goog	1
	1	1
 d  run ρj: after e1
1 v=1; 1 q=a i ; 1 pe=e1 in stack a i ;
1 t=e1.time; 1  v =	goog 1
	1	1figure 1: an example for merging runs.
 c  and  d  show two runs after reading the event e1 from the stream in figure 1: they are both at the state a i  and have identical values in v . their processing of all future events will be the same and thus can be merged.
　the merging algorithm is sketched as follows. the first task is to detect when two runs become equivalent  which can occur at any state qt after the start state. the requirement of identical v vectors is too stringent  since some values in v were used at the previous states and are no longer needed. in other words  only the values for the evaluation at qt and its subsequent states need to be the same. to do so  we introduce an extra static field m  shown in figure 1 b   that contains a set of bit masks over v . there is one mask for each state qt  and the mask has the bit on for each value in v that is relevant to the evaluation at this state. at runtime  at the state qt we can obtain all values relevant to future evaluation  denoted by v t...   by applying the mask  mqt ‥ mqt+1 ‥ ...  to v . two runs can be merged at qt if their v t...  vectors are identical.
　another task is the creation of a combined run  whose computation state will be extended with all the version numbers and start times of the merged runs. the version numbers of the merged runs are cached so that later in the match construction phase  we can identify the compatible predecessor pointers for these runs in the shared buffer and retrieve their matches correctly. we also need to keep the start times of the merged runs to deal with expiration of runs. recall that a run expires when it fails to meet the query-specified time window. since the merged runs may have different start times  they can expire at different times in execution. to allow the combined run to proceed as far as possible  we set the start time of the combined run as that of the youngest merged one  i.e.  the one with the highest start time. this ensures that when the combined run expires  all its contained runs expire as well. finally  when the combine run reaches the final state  match construction is invoked only for the contained runs that have not expired.
1.1	backtrack algorithm
　for purposes of comparison  we developed a third algorithm called the backtrack algorithm for evaluating pattern queries. this algorithm was inspired by a standard implementation for pattern matching over strings and its adaptation in  as a basic execution model for event pattern matching. the basic idea is that we process a single run per partition at a time  which we call the singleton run for the partition. the singleton run continues until either it produces a match or fails  while the evaluation of any runs created during its processing  e.g.  as a result of nondeterminism  is postponed. if the singleton run fails  then we backtrack and process another run whose evaluation was

figure 1: an example for the backtrack algorithm.
previously postponed for the partition. if the singleton run produces a match  we may backtrack depending on the output format: we backtrack if all results are required; we do not if only non-overlapping results are needed.1
　we adapted the implementation of our basic algorithm described in ′1.1 to implement the backtrack algorithm. we highlight the changes through the example given in figure 1. in this example  ρi represents run i  qj state j  and ek an event that occurs at time k. we describe how the backtrack algorithm evaluates the event stream e1 e1 e1 e1 e1 e1 for a generic query with a single kleene plus component:
  e1 creates a new run  ρ1  at the start state  q1. ρ1 becomes the singleton run.
  e1 results in a nondeterministic move at q1. we create run ρ1 and add it together with the id of its current state  q1  and the id of the current event  e1  to a stack holding all postponed runs. ρ1 remains as the singleton run because it is proceeding to the next nfab state.
  process ρ1 until it fails with event e1 at state q1.
  backtrack by popping the most recently created run  ρ1 in this example  from the stack. resume processing ρ1  the new singleton run  at state id q1 by reading events in the buffer starting from e1.
  ρ1 produces a match with e1.
　if we view the creation of runs as a tree that expands during event processing  the backtrack algorithm processes runs in a depth first search manner. that is  we process the singleton run until it either fails or produces a result and then we backtrack to the most recent run that was created during the processing of the singleton run. our basic algorithm  on the other hand  expands the  run tree  in a breadth first search manner; it creates and evaluates all runs at once.
1	memory management
　there are a number of data structures that grow in proportion to the size of the input event stream. since the input event stream is infinite  consistent performance over time can only be achieved by actively maintaining these data structures. to this end  we prune data structures incrementally and reuse expired data structures whenever possible.
　there are two key data structures that we actively prune using the time window during runtime. one is the shared match buffer. after each event is processed  we use the timestamp of this event and the time window to determine the largest timestamp that falls outside the window  called the pruning timestamp. we use the pruning timestamp as a key to perform a binary search in each stack of the match buffer. the binary search determines the position of the most recent event that falls outside the window. we prune the events  more precisely  container objects for those events  at and before this position from the stack. similarly  we prune events from a global event queue in the system using the pruning timestamp.
　to further optimize memory usage  we reuse frequently instantiated data structures. as objects are purged from the match buffer  we add them to a pool. when a new stack object is requested  we first try to use any available objects in the pool and only create a new object instance when the pool is empty. recycling stack objects as such limits the number of object instantiations and quiesces garbage collection activity. similarly  we maintain a pool for nfab run objects  i.e.  the dynamic data structures that maintain the computation state of runs. whenever an nfab run completes or fails  we add it to a pool to facilitate reuse.
1.	performance evaluation
　we have implemented all the query evaluation techniques described in the previous sections in a java-based prototype system containing about 1 lines of source code. in this section  we present results of a detailed performance study using our prototype system. these results offer insights into the effects of various factors on performance and demonstrate the significant benefits of sharing.
　to test our system  we implemented an event generator that dynamically creates time series data. we simulated stock ticker streams in the following experiments. in each stream  all events have the same type  stock  that contains three attributes  symbol  price  volume  with respective value ranges  1    1    1 . the price of those events has the probability p for increasing   for decreasing  and  for staying the same. the values of p used in our experiments are shown in table 1. the symbol and volume follow the uniform distribution.1 we only considered two symbols; adding more symbols does not change the cost of processing each event  on which our measure was based  because an event can belong to only one symbol.
table 1: workload parameters
parameter	values used
probprice increase1  1es  event s1 partition-contiguityselection strategy s1 skip-till-next-matchpa i   iterator predicate p1 true;used in kleene closure p1 a i .price   a i   1 .price; p1 a i .price   aggr a ..i-1 .price  aggr = max | min | avgw  partition window size1 eventsresult output formatall results  default   non-overlapping results　queries were generated from a template   pattern stock+ a     stock b  where es { symbol  … a.price %1==1 … pa i  … b.volume  1} within w   whose parameters are explained in table 1. for event selection strategy  we considered partition contiguity  s1  and skip till next match
 s1  because they are natural choices for the domain of stock tickers. the iterator predicate used in kleene closure  pa i   was varied among three forms as listed in table 1. note that take-proceed non-determinism naturally exists in all queries: for some event e  we can both take it at the state a i  based on the predicate on price  and proceed to the next state based on the predicate on volume. the partition window size w  defined in ′1  was used to bound the number of events in each partition that are needed in query processing. the performance metric is throughput  i.e.  the number of events processed per second. in all experiments  throughput was computed using a long stream that for each symbol  contains events of size 1 times the partition window size w. all measurements were obtained on a workstation with a pentium 1.1 ghz cpu and 1 gb memory running java hotspot vm 1 on linux 1.1. the jvm allocation pool was set to 1mb.
1	effects of various factors
　to understand various factors on performance  we first ran experiments using the shared match buffer  ′1  and the basic algorithm that handles runs separately  ′1.1 . in these experiments  the probability of price increase in the stock event stream is 1.
　expt 1: varying iterator predicate and event selection strategy  es （ s1  s1   pa i  （  p1  p1  p1   w=1 . in this experiment  we study the behavior of kleene closure given a particular combination of the iterator predicate  p1  p1  or p1  and event selection strategy  s1 or s1 . for stock tickers with an overall trend of price increase  p1 using the aggregate function max performs similarly to p1  and p1 using avg is similar to p1 using min. hence  the discussion of p1 below focuses on its use of min.
　figure 1 a  shows the throughput measurements. the x-axis shows the query types sorted first by the type of predicate and then by the event selection strategy. the yaxis is on a logarithmic scale. these queries exhibit different behaviors  which we explain using the profiling results shown in the first two rows of table 1.
table 1: profiling results for expt1
p1p1p1p1p1p1match length1111num. runs/time step1111matching cost % 111construction cost % 111　for the predicate p1 which is set to true  s1 and s1 perform the same because they both select every event in a partition  producing matches of average length 1. simultaneous runs exist due to multiple instances of initiation from the start state and take-proceed non-determinism  yielding an average of 1 runs per time step  we call the cycle of processing each event a time step .
　for p1 that requires the price to strictly increase  s1 and s1 differ by an order of magnitude in throughput. since p1 is selective  s1 tends to produce very short matches  e.g.  of average length 1  and a small number of runs  e.g.  1 run per time step. in contrast  the ability to skip irrelevant events makes s1 produce longer matches  e.g.  of average length 1. furthermore  s1 still produces 1 runs per time step: due to the ignore-proceed nondeterminism that s1 allows  but s1 does not   a more selective predicate only changes some runs from the case of take-proceed nondeterminism to the case of ignore-proceed.
　finally  p1 requires the price of the next event to be greater than the minimum of the previously selected events. this predicate has poor selectivity and leads to many long matches as p1. as a result  the throughput was close to that of p1 and the difference between s1 and s1 is very small.
　in summary  selectivity of iterator predicates has a great effect on the number of active runs and length of query matches  hence the overall throughput. when predicates are selective  relaxing s1 to s1 can incur a significant additional processing cost.
　we also obtained a cost breakdown of each query into the pattern matching and pattern construction components  as shown in the last two rows of table 1. as can be seen  pattern matching is the dominant cost in these workloads  covering 1% to 1% of the total cost. reducing the matching cost is our goal of further optimization.
　expt 1: varying partition window size  es （ s1  s1   pa i  （  p1  p1  p1  . the previous discussion was based on a fixed partition window size w. we next study the effect of w by varying it from 1 to 1. the results are shown in figure 1 b . we omitted the result for p1 in the rest of the experiments as it is the same as p1.
　the effect of w is small when a selective predicate is used and the event selection strategy is s1  e.g.  p1. however  the effect of w is tremendous if the predicates are not selective  e.g.  p1 and p1  and the event selection strategy is relaxed to s1. in particular  the throughput of p1 and p1 decreases quadratically. our profiling results confirm that in these cases  both the number of runs and the length of each match increase linearly  yielding the quadratic effect.
　we further explore the efficiency of our algorithm by taking into account the effect of w on the query output complexity  defined as peach match length of the match . it serves as an indicator of the amount of computation needed for a query. any efficient algorithm should have a cost linear in it. figure 1 c  plots the processing cost against the output complexity for each query  computed as w was varied. it shows that our algorithm indeed scales linearly. the constants of different curves vary naturally with queries. the effect of further optimization will be to reduce the constants.
1	basic versus backtrack algorithms
　recall from ′1 that our basic algorithm evaluates all runs simultaneously when receiving each event. in contrast  the backtrack algorithm  popular in pattern matching over strings and adapted in  for event pattern matching  evaluates one run at a time and backtracks to evaluate other runs when necessary. we next compare these two algorithms.
　expt1: all results. in this experiment we compare the two algorithms using the previous queries and report the results in figure 1 d . these results show that the throughput of our basic algorithm is 1 to 1 times higher than the backtrack algorithm across all queries except for p1  where the basic algorithm achieves a factor of 1 over the backtrack algorithm.
　the performance of the backtrack algorithm is largely attributed to repeated backtracking to execute all the runs and produce all the results. the throughput results can be explained using the average number of times that an event is reprocessed. the backtrack algorithm reprocesses many events  e.g.  an average 1 time for each event for queries using s1  resulting in their poor performance. in contrast  our basic algorithm never reprocesses any event. the only case of backtrack where this number is low is p1 with short duration of runs  yielding comparable performance. as can



	 f  benefit of merging  expt1 	 g  benefit of merging  expt1 
figure 1: experimental results.be seen  if all results are to be generated  our basic algorithm is a much better choice.
　expt1: non-overlapping results. we next compare these two algorithms when only non-overlapping results are required. the difference from the case of all results is that we may not need to examine all simultaneous runs to produce such results. however  it is unknown a prior which run among the active ones will produce a result first. in this experiment  we instrumented both algorithms to return shortest non-overlapping results.
　we first reran all previous queries with non-overlap. these queries exhibit similar trends for the two algorithms  as illustrated using p1 and p1 in the 1st and 1nd groups of bars in figure 1 e . we observed that this set of queries are ideal for the backtrack algorithm since little backtracking is needed. this is because the last predicate b.volume  1 is not selective given uniform distribution of volume in events. hence  once a run starts  it is likely to produce a match during a short period of time  eliminating the need to backtrack. our basic algorithm runs fast for a similar reason: even though it executes multiple runs at the same time  the quick generation of a result allows it to prune other runs  hence reducing its overhead of executing multiple runs.
　we next modified the query workload so that a random run is less likely to produce a result-requiring each algorithm to search through runs to generate the result. to do so  we extended the two queries using predicate p1 with another pattern component: pattern stock+ a     stock b  stock c   where c.price   a.price…c.price   a a.len .price. we denote the modified queries using and show its results using the 1rd and 1th groups of bars in figure 1 e . in particular  the basic algorithm yields an order of magnitude higher throughput than the backtrack algorithm for. in this workload  c's predicates are selective so most runs fail at this state  causing the backtrack algorithm to repeatedly invoke backtracking and try other runs.
1	nfab with merging of runs
　we showed in the above experiments that our basic algorithm provides comparable or better performance than the backtrack algorithm in most workloads tested. in the following experiments  we omit the backtrack algorithm and extend the basic algorithm with merging of equivalent runs. expt 1: merging runs of expt 1. in this experiment  we applied the merging algorithm to the queries in expt 1.
figure 1 f  shows the results; it also includes the numbers of the basic algorithm from figure 1 a  for comparison. as can be seen  the merging algorithm yields significant performance gains over the basic algorithm for all the queries tested  e.g.  achieving a factor of 1 for p1  1 for p1  and 1 for p1. it ran all of the expensive queries  which either use less selective predicates such as p1 and p1 or the event selection strategy s1  at over 1 events per second. expt 1: merging runs of expt 1. we further applied merging to all the queries in expt 1 when the partition window size w was varied. we focused on the three queries using s1  namely  p1  p1  and p1  because they are more expensive than  if not the same as  their counterparts using s1. recall that the partition window size affects query output complexity; a larger size leads to more query matches and longer matches  hence a higher sequence construction cost. since sequence construction is common to both the basic and merging algorithms  to better understand their differences we turned off sequence construction in this experiment and measured only the cost of sequence matching. figure 1 g  shows the benefits of merging in throughput gain  defined as new throughput/old throughput. as can be seen  merging offers remarkable overall throughput gains  ranging from a factor of 1 to 1. results of the three queries can be further explained using two factors shown in table 1: the sharing opportunity  captured by the percentage of runs that were successfully merged in columns 1 to 1 of the table  and the overhead of maintaining the match buffer  captured by the total cost of buffer updates in columns 1 to 1.
　these three queries represent interesting combinations of these two factors. the predicate p1  = true  allows the most sharing opportunity: all of the overlapping runs for the same partition can be merged. however  p1 leads to long query matches  hence a high overhead of maintaining the match buffer as events are selected. the predicate for price strictly increasing  p1  still allows significant sharing: as soon as two runs have the same price from the last selected events  they can be merged. in addition  p1 is selective so the buffer update cost is relatively low. as a result  p1 achieves higher throughput gains for large values of w. finally  the predicate p1 requires two runs to agree on the minimum price in order to be merged  offering a somewhat less sharing opportunity. since it is not selective  it also has a high buffer update cost as p1. combining both factors  p1 achieves only limited throughput gains for large values of w.
table 1: profiling results for expt1
wruns merged  % buffer update cost  sec p1p1p1p1p1p11.1.1.1.1.1.11111111.1.1.1.1.1.1111111　other experiments. to explore the effects of different data characteristics  we also varied the probability p for stock price increase in the event stream. we briefly summarize the results below. when a smaller value of p is used  e.g.  p = 1  the queries using predicate p1 have the same performance because they simply select every event in each partition. in comparison  the queries using predicates p1 and p1 produce fewer matches and hence all achieve higher throughput numbers. the advantage of our basic algorithm over backtrack still holds  but with a smaller margin. on the other hand  the difference in throughput between the merging and basic algorithms is even higher. this is because fewer matches mean a smaller cost of sequence construction  and the benefit of merging is magnified in the presence of a low sequence construction cost common to both algorithms.
1.	related work
　much related work has been covered in previous sections. we discuss broader areas of related work below.
　event languages for active databases  1  1  1  1  1  1  offer temporal operators including sequencing and kleene closure  but do not support complex predicates to compare events. as we showed in this paper  such predicates are crucial in pattern definition. those languages also lack efficient implementation over high-volume streams.
　traditional pub/sub systems  1  1  offer predicate-based filtering of individual events. our system significantly extends them with the ability to match complex patterns across multiple events. cayuga  1  1  supports patterns with kleene closure and event selection strategies including partition contiguity and skip till next match  but does not allow output of complete matches. in comparison  our system supports more event selection strategies and output of complete matches both in the evaluation model and in runtime optimization. the cayuga implementation focuses on multi-query optimization  which is directly applicable when our system is extended to handle multiple queries.
　sequence databases  1  1  offer sql extensions for sequence data processing. sequin  uses joins to specify sequence operations and thus cannot express kleene closure. sql-ts  adds new constructs sql to handle kleene closure  but restricts pattern matching to only contiguous tuples in each relevant partition. its optimization based on predicate containment can be integrated into our system for workloads that exhibit such containment relationships.
　many recent event systems  1  1  1  1  offer relatively simple event languages and stream-based processing. these systems lack important constructs for pattern matching such as kleene closure and choices of event selection strategies. in particular  our work significantly extends prior work  with kleene closure and event selection strategies  two features that fundamentally complicate event pattern matching  a formal rich model nfab for evaluation and theoretical analysis  and a suite of sharing techniques. the evaluation framework of  is further extended to handle out-of-order events . sase+  provides a rich language for pattern matching but lacks implementation details.
　there have also been theoretical studies on the underlying model of complex event processing. cedr  proposes a new temporal model that captures the duration of events and analyzes the consistency of event processing for outof-order streams. a related study  designs a temporal model for such events that has a bounded representation of timestamps and offers associativity of the sequencing operator. these results can be applied to guide the extension of our system to handle events with duration.
1.	conclusions
　in this paper  we studied the evaluation and optimization of pattern queries over event streams. we rigorously defined a query evaluation model  the nfab automata  analyzed its expressibility  and provided compilation techniques for building query plans based on this model. we also analyzed the runtime complexity and developed sharing techniques for efficient runtime evaluation. our system could process tens of thousands of events per second for fairly expensive queries and offers much higher throughput for cheaper queries. our sharing techniques also produced remarkable performance benefits  ranging from 1% to 1%  over a spectrum of query workloads.
　we plan to continue our research in a few directions. we will extend our system to handle out-of-order events by augmenting the nfab model with techniques including invalidation and re-computation: the nfab proceeds as before and  when receiving an out-of-order event  invalidates some of the existing runs affected by the event and recomputes from valid intermediate steps. in addition  our current implementation of queries with negation and composition is strictly based on the query semantics. we will explore new opportunities for optimization that are particularly suitable for these queries. finally  we will study robust pattern matching over uncertain events that are produced from a variety of sensor networks.
1.	repeatabilityassessmentresult
　figures 1 a   1 b   1 d   1 e  have been verified by the sigmod repeatability committee.
