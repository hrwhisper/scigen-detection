	marko grobelnik 	natasa milic-frayling 
jo ef stefan institute 	microsoft research ltd ljubljana  slovenia 	cambridge  united kingdom 
	tel.: +1 1 	tel.: +1 1 
marko.grobelnik ijs.si   natasamf microsoft.com 
 	   
 	 
consumption  and processing time. in addition  features are often 

this paper explores feature scoring and selection based on weights from linear classification models. it investigates how these methods combine with various learning models. our comparative analysis includes three learning algorithms: na ve bayes  perceptron  and support vector machines  svm  in combination with three feature weighting methods: odds ratio  information gain  and weights from linear models  the linear svm and perceptron. experiments show that feature selection using weights from linear svms yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms. the results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.  
categories and subject descriptors 
h.1  information storage and retrieval : indexing.  
general terms 
algorithms  performance  experimentation. 
keywords 
text classification  information retrieval  vector representation  feature selection  feature scoring  linear svm  svm normal.  
1. introduction 
in text classification  feature selection is typically used to achieve two objectives: reduce the size of the feature set in the data representation in order to optimize the use of computing resources and to remove noise from the data in order to optimize the classification performance.  
a number of standard methods  such as stop word removal or linguistic normalization using stemming  are routinely applied and contribute to the reduction of the feature space  memory scored and ranked using some feature weighting scheme that reflects the importance of the feature for a given task. only a selected subset of top scoring features is used for further processing.  
a fundamental question that arises in this practice is the relationship between the scoring method for feature selection and the classification model that further uses these features. are there compatibility criteria that the two should satisfy in order to yield optimal classification performance  
in a study of na ve bayes text classifier   feature selection based on odds ratio scores consistently lead to statistically significant improvements in classification performance in comparison to the full feature set. this was attributed to the fact that the feature selection method was 'compatible' with the used classification algorithm. indeed  based on statistics used in na ve bayes and odds ratio  one could characterize them as compatible in the sense that the features with higher odds ratio weights are expected to be more influential within the na ve bayes classifier. thus  feature selection based on odds ratio weights is expected to be effective in tuning the performance of the na ve bayes. 
in order to gain more insight into this question  we use a novel approach of scoring and selecting features based on weights from linear classifiers. more precisely  we consider an iterative procedure that typically involves a subset of training data to train linear support vector machines  svm  or perceptron. the weights of the generated model  i.e.  the normal to the hyperplane that separates the classes  are applied to rank the features and select a smaller subset. in the second iteration the full set of data is used within the reduced feature space to train the classifiers and then perform classification of new data. we observe how this type of feature selection method combines with its 'own' classification model and with others.    
the second practical issue is how to select features to take into account available computing resources. is there an effective way to perform feature selection so to optimize performance for the given computing resources  this can  in fact be achieved by 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  july 1  1  sheffield  south yorkshire  uk. 
copyright 1 acm 1-1/1...$1. 
 
using vector sparsity  or density  as the feature selection criteria as suggested in . we explore the performance optimization for several classifiers in conjunction with different feature weighting algorithms. 
the main contributions of our work include:  1  widening the perspective on the feature selection task by showing how linear classifiers themselves can be used to address that issue   1  demonstrating how sparsity based feature selection can guide optimization of computer resource use  and  1  providing empirical data about the behavior of various combinations of feature selection and learning methods which enable us to make an informed conjecture on the compatibility issue for feature selection methods and the learning algorithm. our experiment results suggest that it is the sophistication of the feature selection method rather than similarity with the underlying learning algorithm  i.e.  apparent compatibility with the classification model  that plays the crucial role in optimizing classification performance. in particular  methods such as feature selection using weights of a linear svm model  which take into account statistical properties of all the features and documents simultaneously  generally tend to perform better than other explored methods across all text learning algorithms examined. 
the paper is structured in the following way. we first give a  brief description of feature selection and text classification methods used in the experiments. then we describe the experimental set-up and the sparsity based feature selection. we show how sparsity is used to optimize the use of computing resources. we conclude with a summary of observations and an outline of future work. 
1. learning algorithms and feature selection methods 
in this study we investigate three learning algorithms  na ve bayes  perceptron  and support vector machines  svm   and three feature weighting methods  odds ratio  information gain  and weights from linear classification models. we expected that combinations of these methods would provide valuable insight in the nature of feature selection methods and their compatibility with classification models. 
1 learning algorithms  
na ve bayes. we use the multinomial model as described in . the predicted class for document d is the one that maximizes the posterior probability p c|d  ≦ p c  旭t p t|c  tf t d   where p c  is the prior probability that a document belongs to class c  p t|c  is the probability that a word w  chosen randomly in a document from class c  equals t  and tf t d  is the  term frequency   or the number of occurrences of word t in a document d. training simply consists of estimating the probabilities p t|c  and p c  from the training documents.  
perceptron . this algorithm trains a linear classifier in an incremental way as a neural unit using an additive update rule. class prediction for a document represented by the vector x is sgn wtx  where w is a vector of weights obtained during training. computation starts with w = 1 and considers each training example xi in turn. if the present w classifies xi correctly it is left unchanged  otherwise it is updated according to the additive rule: w ↘ w + yi xi where yi is the correct class label of the document xi  yi = +1 for a positive document  yi = -1 for a negative one .  
support vector machine  svm  . this algorithm trains a linear classifier of the form sgn wtx + b . learning is posed as an optimization problem with the goal of maximizing the margin  i.e.  the distance between the separating hyperplane wtx + b = 1 and the nearest training vectors. an extension of this formulation  known as the soft margin  also allows for a wider margin at the cost of misclassifying some of the training examples. the dual form of this optimization task is a quadratic programming problem and can be solved numerically. we used the svmlight v.1 program  to train the svm models. this program is particularly suitable for our purposes because of its optimizations for working with linear kernels. 
we work with linear svm and perceptron models because the existing literature on text categorization indicates that  for this problem  the nonlinear versions of these algorithms gain very little in terms of performance. 
1 feature selection methods 
all feature selection methods considered here involve assigning a weight to each feature  ranking the features based on the weights  and retaining only a specified number of features. odds ratio. let p t|c  be the probability of a randomly chosen word being t  given that the document it was chosen from belongs to a class c. then odds t|c  is defined as p t|c / 1-p t|c   and the odds ratio equals to 
or t  = ln odds t|c+ /odds t|c-  . 
obviously  this scoring measure favors features that are representative of positive examples. as a result a feature that occurs very few times in positive documents but never in negative documents will get a relatively high score. thus  many features that are rare among the positive documents will be ranked at the top of the feature list. odds ratio is known to work well with the na ve bayes learning algorithm . 
information gain. here both class membership and the presence/absence of a particular term are seen as random variables  and one computes how much information about the class membership is gained by knowing the presence/absence statistics  as is used in decision tree induction  e.g.  in  . indeed  if the class membership is interpreted as a random variable c with two values  positive and negative  and a word is likewise seen as a random variable t with two values  present and absent  then using the information-theoretic definition of mutual information we may define information gain as: 
ig t  = h c  - h c|t  
         = 曳而 c p c=c t=而  ln p c=c t=而 /p c=c p t=而  . 
here  而 ranges over {present  absent} and c ranges over {c+  c-}. as pointed out above  this is the amount of information about c  the class label  gained by knowing t  the presence or absence of a given word . 
feature selection based on linear classifiers. using weights from the svm classification model for feature selection was suggested in  and studied in detail for linear svm in . here we extend this idea to general linear classifiers and discuss the procedure by referring to linear svm and perceptron.  
both svm and perceptron  as linear classifiers  output predictions of the form: prediction x  = sgn wtx + b  = sgn 曳j wjxj + b . thus  a feature j with the weight wj close to 1 has a smaller effect on the prediction than features with large absolute values of wj. the weight vector w can also be seen as the normal to the hyperplane determined by the classifier to separate positive from negative instances. thus we often refer to the procedure as normal based feature selection. one speculates that since features with small |wj| are not important for categorization they may also not be important for learning and  therefore  are good candidates for removal.  
a theoretical justification for retaining the highest weighted features in the normal has been independently derived in a somewhat different context in . the idea is to consider the feature important if it significantly influences the width of the margin of the resulting hyper-plane; this margin is inversely proportional to ||w||  the length of w. since w = ﹉i 汐i xi for a linear svm model  one can regard ||w||1 as a function of the training vectors x1  ...  xl  where xi =  xi1  ...  xid   and thus evaluate the influence of feature j on ||w||1 by looking at absolute values of partial derivatives of ||w||1 with respect to xij.  of course this disregards the fact that if the training vectors change  the values of the multipliers 汐i would also change. nevertheless  the approach seems appealing.  for the linear kernel  it turns out that 
﹛﹛﹛﹛﹛﹛﹛﹛﹉i | ||w||1/ xij| = k |wj| where the sum is over support vectors and k is a constant independent of j. thus the features with higher |wj| are more influential in determining the width of the margin. the same reasoning applies when a non-linear kernel is used because ||w||1 can still be expressed using only the training vectors xi and the kernel function. 
note that the normal-based approach to feature weighting and selection involves an important issue: the selection of a set of instances over which one trains the normal w in order to arrive at the feature weights. since training an svm model requires a considerable amount of cpu time  and practically requires all the training vectors to be present in main memory all the time  it is desirable to use a subset of the training data in order to facilitate features selection and then retrain the classifier1 over the full data set but using the reduced feature space. furthermore  it is useful to explore how the selection of the training set affects feature selection and the performance of the final classifier. we illustrate these issues by applying normal based feature selection to different samples of the training data: the full set  one quarter and one sixteenth of the full data size. 
1. experiment design 
1 data 
experiments conducted in this study use data from the reuters corpus  http://about.reuters.com/researchandstandards/corpus/  volume 1 and a sample of 1 categories from the set of 1 categories that comprise the reuters classification scheme. the experimentation setup is similar to the study presented in .  the full reuters corpus of 1 news articles  dated from 1 august 1 through 1 august 1  is divided into a  training period  that includes 1 articles and a  test period  with the remaining 1 documents. for training we used a sample of 1 documents from the training period and tested the classifiers on the complete document set from the test period. as described in   the selected subset of training data closely matches the distribution of categories across documents in the full reuters collection.  
we also use the same 1 categories that were selected in  based on a preliminary document classification experiment that involved training the svm over a smaller training set for the complete set of reuters categories and classifying a test set of 1 documents.  the selection criterion was based on two characteristics of a category: the distribution of positive examples in the whole corpus and the precision recall break-even point for the category achieved in the preliminary experiment. these statistics for the selected subset of 1 categories approximately follows the distribution for all 1 categories. the selected set of categories includes: godd  c1  gpol  ghea  c1  e1  gobit  m1  m1  gspo  e1  e1  c1  e1  e1  and c1.  
a document may belong to one or more categories; therefore  we regard each category as a standalone binary classification problem where the task is to predict if a document belongs to this category or not. 
our document representation is based on the bag-of-words model. we convert text of each document into lowercase  remove stopwords  using a standard set of 1 stop-words   and eliminate words occurring in less than 1 training documents. we represent each document as a tf-idf vector normalized to the unit length  except for na ve bayes when we simply use tf term weighting. 
1 sparsity of data representation 
in order to compare classification performance of several feature selection methods and address the issue of computing resource consumption  we observe the density or  more commonly used term  the sparsity of vectors associated with individual documents. to be precise  we use the term  sparsity  to refer to the average number of terms per document or  more technically  the average number of nonzero components in the vectors by which documents are represented.  
it has been shown in  that the vector sparsity varies significantly when the same number of features is retained for different feature weighting methods. similarly  a fixed sparsity level yields features sets of very different sizes. this is a rather important observation since it is expected that the performance of the data processing algorithm will depend on the representation of individual documents. as an illustration  figure 1 shows how specifying the sparsity levels for odds ratio  information gain  and svm normal based feature selection affects the performance of the svm classifier. each sparsity level is  in turn  achieved by retaining a number of highly scored features. we show the same performance data with respect to the number of features retained for each of the feature selection methods.    
sparsity of document vectors  on the other hand  directly affects consumption of computing resources  both the memory for storing the sparse vectors and the time needed to perform calculations. while reducing the total number of features is expected to increase sparsity  the rate at which this is achieved depends on the term weighting scheme  or ultimately the corpus properties such as term distribution  i.e.  number of documents in the corpus  or labeled by a particular class  that contain the term . for examples  if the number of features to be retained is fixed  weighting schemes that lead to retaining rare features generally incur a low cost to data storage and calculations. the opposite is true for weighting schemes that favor features common across documents. 

figure 1. macroaverages of f1 measure for the svm classifier for various feature set sizes and different feature weighting methods. the chart on the left shows the performance against the number of features while the one on the right shows performance at various sparsity levels. the sparsity-based charts allow for a more useful comparison of feature selection methods; e.g.  odds ratio is quite successful at very sparse representations  even though it uses a large vocabulary. thus  just specifying a fixed number of features does not allow for a reliable control over resource consumption. for that reason  we propose to work directly with the requirement on the sparsity level as the cut-off criterion for feature selection. 
1 evaluation measures 
in our analyses we use standard evaluation measures: precision p and recall r of the classifier  the combined measure f1 defined as f1 = 1pr/ p+r   and precision-recall break-even  bep  point. because of the space limitation we report only statistics for the f1 measure  in particular the macroaverages of f1  calculated for each experiment as the average f1 measure over all categories . analyses based on microaveraged statistics provide qualitatively similar results. 
1. results 
1 effects of feature selection on classification performance 
in addition to two standard feature weighting methods  odds ratio and information gain  we report classification results for data representations obtained by normal based feature selection using linear svm and perceptron.  
for both svm and perceptron linear classifier we use three sets of training data to obtain feature sets and corresponding weights: the full training set and randomly chosen subsets of 1 and 1 of the documents. training the classifiers on these sets results in three normal vectors for each method: svm-1  svm-1  and svm-1 for the linear svm and perceptron-1  perceptron-1  perceptron-1 for the perceptron. these 1 feature rankings are considered  together with the odds ratio and information gain  for feature selection and training of the classifiers.   
for each rank in the list of features we can calculate the average sparsity of vectors achieved if only features from that rank and above were retained. similarly  given a sparsity requirement we can find the cut off rank below which the features are discarded in order to achieve the specified vector sparsity.  
it is interesting to note that specifying sparsity leads to nonuniform number of features across categories  depending on the distribution characteristics of features typical for the category. this is in contrast with the common practice of specifying the fixed number or percentage of top ranking features to be used for all categories. 
for each of the nine sets of features we train the learning algorithms: na ve bayes  perceptron  and linear svm  over the training data represented by the reduced set of features. we specify the reduction level in terms of the desired sparsity of vectors. in particular  both perceptron and svms  are in this phase retrained over the full training set  represented by the reduced set of features. the resulting classifiers are then applied to the test data.  
the charts in figure 1 show the macroaveraged f1 performance statistics for specific levels of vector sparsity on the horizontal axes: 1  1  1  1  1  1  and 1 terms per document  as well as for the sparsity of the full document vectors  about 1 terms per document on average. 
na ve bayes 
our experiments with na ve bayes confirm the known fact that na ve bayes benefits from appropriate feature selection . detailed analysis shows that this is particularly true for categories with smaller number of training data  while for the largest few categories there is little or no improvement from feature selection.  
odds ratio seems to work well with na ve bayes when documents are made moderately sparse  1 to 1 terms per document  which  on average  requires 1 to 1 of the best features to be retained  but not extremely sparse. on the other hand  earlier research found that odds ratio works well even if 
 

figure 1. macroaveraged f1 performance of different feature selection methods in combination with different machine learning algorithms. note different scales for f1 values on the vertical axes. the horizontal axes refer to sparsity  the average number of nonzero components per training vector. in this case  this is the average number of terms per training document after terms rejected by feature selection have been removed . 
 
 only a few hundred features were kept . thus  it would be worth exploring whether sparsity is a more reliable parameter in predicting classifier performance than the number of features retained.  
information gain  on the other hand  works well when documents retain about 1 terms on average per document  which corresponds to keeping around 1 best features on average  the actual number varies considerably from one category to another  with e1 needing as few as 1 and ghea as many as 1 .  
still higher performance with na ve bayes can be achieved with feature selection based on perceptron and linear svm weights. for svm  this is true even if much smaller data sets are used in the feature selection phase. table 1 provides detailed performance results. however  as it can be seen from figure 1  even with the best of the tried feature selection methods  na ve bayes cannot match the performance of the perceptron and the linear svm classifiers. 
      table 1. this table shows  for each feature selection method  the best na ve bayes classification performance and the sparsity at which it was achieved. 
﹛﹛feature selection/ranking method sparsity which yielded best performance macroaveraged f1 at that sparsity odds ratio 1 1 information gain 1 1 svm-1 1 1 svm-1 1 1 svm-1 1 1 perceptron-1 1 1 perceptron-1 1 1 perceptron-1 1 1 perceptron 
our experiments show that the perceptron learning algorithm does not combine well with the feature selection methods considered here. the only case in which performance is actually improved is for the sparsity level of 1 terms per document with perceptron-1  thus a slight reduction in sparsity from 1 when all features are used. it is worth noting that the reduction of 1 features on average per document corresponds to retaining around 1 out of 1 original features. the associated increase in f1  although statistically significant  is marginal  1 % . otherwise  performance drops quickly and considerably as more and more features are discarded. 
 when using svm-based rankings  the performance is lower although the differences with respect to the perceptron weighting are small. they are greatest when both feature selectors are allowed to inspect the full document set: perceptron-1 may achieve f1 values up to 1 % above those achieved by svm-1 at the same sparsity. the differences between perceptron-1 and svm-1 and between perceptron-1 and svm-1 are smaller and often insignificant. thus  in scenarios when it is desirable to reduce the feature set by first training linear models over smaller training data sets the two feature scoring methods combine equally well with the perceptron as the classifier. furthermore  while the svm-based feature rankings achieve lower precision and lower f1 values than the perceptron ones  they tend to have higher recall and break-even point for the same subset of training documents. 
in combination with the perceptron classifier  feature ranking by the information gain criterion usually performs worse than the rankings of linear classifiers. the difference is slight but statistically significant. on the other hand  the feature ranking by odds ratio performs much worse  interestingly  it combines well with the svm; see the next section . we speculate that the perceptron type training  which considers individual documents sequentially  is negatively affected by the tendency of odds ratio to favor features characteristic of positive documents  in particular those that are rare and absent from negative documents. it would be interesting to see whether perceptron models with margins  are more robust in that respect. 
linear svm 
similarly to perceptron  the linear svm does not benefit from feature selection but is much less sensitive to the reduction of the feature space. here one can reduce the feature set to 1 terms per document on average  while still losing only a few percent in terms of the f1 performance measure. as in the case of perceptron  statistically significant  though rather small  improvements to the f1 performance in comparison to the full feature set are achieved only when using svm-1 or perceptron-1 feature sets  thus increasing the sparsity from 1 to 1 terms per document on average. 
odds ratio works well in combination with the linear svm classifier  particularly when very sparse documents representations are required. indeed  at sparsity ≒ 1 it is significantly better than information gain. there it also outperforms svm-1. at more extreme levels of sparsity it performs better than svm-1 and even svm-1  for sparsity ≒ 1 .  
an interesting observation is that the perceptron-based feature weightings perform much worse with the svm classifier than the svm-based weightings. both feature weighting methods performed quite similarly in combination with the perceptron as the classifier  see previous section . in addition  perceptron-1 is generally not significantly better than perceptron-1  and for extremely sparse documents it is  in fact  significantly worse. 
looking at precision and recall separately shows that percep- tron-based feature rankings have particularly poor recall. from sparsity curves that show how the density of document vectors increases with the number of features retained we conclude that the perceptron-based feature weighting has greater preference for rare features than svm-based  although not nearly so great as odds ratio   especially for features typical of positive documents. we speculate that this may be the cause of poor recall  consequently  poor f1 . interestingly  information gain tends to produce high precision but relatively low recall  whereas the opposite holds for odds ratio. the svm-based rankings  on the other hand  are good at both precision and recall  particularly the latter.  
1 feature selection in the memory constraint scenario 
when text classification is performed with limited memory resources we have a choice of limiting the number of documents used for training or limiting the feature set used to represent the documents or both. the question is how this trade-off affects the performance of the system.  for some training algorithms  such as na ve bayes or perceptron  this is not really a problem  because only  one or more  sequential passes through the training data are needed for training. svm  on the other hand  requires more or less random access to training documents  which must therefore be kept in main memory during training .  
suppose that we have n training documents  and their vectors have s nonzero components on average; thus  m = n s units of memory are required to store the training set. if only m/1 units of memory are available  one could use full vectors of just n/1 training documents; or use feature selection to reduce vectors to sparsity s/1 and work with n/1 training documents  etc. in general  if one needs to reduce memory consumption to m/a  one could keep n/a training documents and use sparsity of s/ a/a  terms per document where a ﹋  1  a . thus for a given a we are seeking a for which classification performance is the best.  
figure 1 shows the performance of different learning algorithms for a = 1  1  1  1  1 and a = 1  1  1  ...  a with svm weights used in feature selection. each line corresponds to a fixed memory level requirement: m  m/1  m/1  m/1  and m/1 and the plotted points to f1 measures corresponding trade-off values  e.g.  for m/1 these values are:  n/1  s    n/1  s/1    n  s/1  .  
for the na ve bayes learner  we observe the expected result: feature selection improves performance and  therefore  for all values of memory reduction a  maximally reducing the feature set to sparsity s/a is best. of course  the performance increases with the number of training examples used. however  most of the improvement comes from feature selection rather than the increase of the training set. 
perceptron  on the other hand  suffers from feature selection so much that keeping fewer documents and using the complete feature set works best  i.e.  a = a is the best choice; even doubling the number of documents cannot offset the loss of performance caused by the removal of features. 
the most interesting behavior is observed with the svm classifier. here  non-trivial trade-offs provide best results. for example  for the memory constraint m/1  the trade-off point  n/1  s  yields f1 = 1 while  n  s/1  results in an increase to f1 = 1. similarly  for the memory constraint of m/1 we have the following trade-offs:  n/1  s    f1 = 1   n/1  s/1    f1 = 1 and  n  s/1    f1 = 1. thus  for the svm classifier it is generally better to increase the sparsity in order to include a larger number of training examples. 
1. conclusions 
our experiments show that feature scoring and selection based on the normal obtained from the linear svm combines very well with all the classifiers considered in the study. in particular  in combination with the na ve bayes classifier it seems to be far more effective than odds ratio for all levels of sparsity below 1 features per vector. even the perceptron produces much better feature rankings for na ve bayes than odds ratio. on the basis of 

figure 1. comparison of various combinations of feature selection and training set reduction. each line connects combinations that have equal memory requirements expressed in terms of the memory m required to represent the entire training  without feature selection applied. the performance varies depending on how the training algorithm reacts to the tradeoff between feature selection and sampling of the document set. if we follow a line for a particular memory requirement from left to right  the representation of documents is more and more complete  i.e.  fewer features were discarded  and fewer training documents are used to train the classifier after feature selection. feature selection is here based on the feature set ranked by normals built from the largest training set that fits the memory requirements  e.g.  if the memory limit is m/1  then 1 of all training documents is used to learn the model and apply feature selection . 
 
this observation our conjecture is that  on our dataset  the complexity and sophistication of the feature scoring algorithm plays a larger role in the success of the feature selection method than its compatibility in design with the classifier. indeed  at the first glance it seems that the odds ratio  which closely follows the feature scoring used within na ve bayes  should provide the best selection for that classifier. however  a method like svm that tunes the normal by taking into account all the data points simultaneously seems to be most successful in scoring features.  
svm feature ranking combines relatively well with the perceptron classifier considering the rather dramatic negative effect of feature selection on the perceptron. one could argue that  because the perceptron-1 is the best performing feature ranking with the perceptron classifier  the conjecture we proposed in section 1 is weakened. however  when we consider smaller subsets of the training data  e.g.  n/1  and look for higher performance levels  f1   1  of the perceptron classifier we see that svm-based and perceptron-based feature selection have almost identical effect. this makes us believe that our conjecture is in the right direction.  
in the experiments with the memory constraint  we have seen that the perceptron classifier does not allow for a trade-off since it is adversely affected by feature selection while na ve bayes always benefits from feature selection. svm classifier  on the other hand  can benefit from combining feature selection and adjustment of the training set to fit the memory constraint. namely  although svm classifier does not improve with feature selection itself  section 1  subsec. linear svm   it yields better performance if feature selection allows a larger training set to be used to train the final model after feature selection. 
sparsity of vectors representing the data has been found to be useful for comparing different feature selection methods  particularly because the learning algorithms are more sensitive to sparsity rather than to the number of features as such. finally  as has been found by other researchers in the field of text categorization  our experiments confirm that svm outperforms perceptron and naive bayes as a learning algorithm for text categorization. our further work will expand this study to additional classifiers and data sets  including scenarios that do not necessary involve text data. 
