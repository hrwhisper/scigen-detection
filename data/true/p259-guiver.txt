in this paper we address the issue of learning to rank for document retrieval using thurstonian models based on sparse gaussian processes. thurstonian models represent each document for a given query as a probability distribution in a score space; these distributions over scores naturally give rise to distributions over document rankings. however  in general we do not have observed rankings with which to train the model; instead  each document in the training set is judged to have a particular relevance level: for example  bad    fair    good   or  excellent . the performance of the model is then evaluated using information retrieval  ir  metrics such as normalised discounted cumulative gain  ndcg . recently taylor et al. presented a method called softrank which allows the direct gradient optimisation of a smoothed version of ndcg using a thurstonian model. in this approach  document scores are represented by the outputs of a neural network  and score distributions are created artificially by adding random noise to the scores. the softrank mechanism is a general one; it can be applied to different ir metrics  and make use of different underlying models. in this paper we extend the softrank framework to make use of the score uncertainties which are naturally provided by a gaussian process  gp   which is a probabilistic non-linear regression model. we further develop the model by using sparse gaussian process techniques  which give improved performance and efficiency  and show competitive results against baseline methods when tested on the publicly available letor ohsumed data set. we also explore how the available uncertainty information can be used in prediction and how it affects model performance.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-retrieval models  search process
general terms
algorithms  experimentation
keywords
information retrieval  ranking  gaussian process  learning
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  singapore.
copyright 1 acm 1-1-1/1 ...$1.
1. ir metrics
¡¡our task in information retrieval is to choose and present  possibly in an ordered list  a set of documents relevant to the query entered by a user. in order to design and improve retrieval systems we need to evaluate the quality of the retrieved results. in practice this is usually done using a set of judged documents for multiple queries. the documents are judged for relevance to the query either using binary labels or a graded scale. an ir metric or utility is a function of the judged labels for a returned set of documents. many such metrics have been developed  see e.g.   to try to capture various aspects of user preference for the retrieved set. for example  in a ranked list a user pays most attention to the head of the list  and we therefore want to make sure we get the very relevant documents right at the top.
1 ndcg
¡¡normalized discounted cumulative gain  ndcg   is an ir metric that is a function of graded relevance labels  for example 1   bad   to 1   excellent    which focuses on the top of a ranking using a discount function. it is defined  assuming a top rank of 1  as:
r 1
	gr = g r 1max x g r d r  .	 1 
r=1
the gain g r  of the document at rank r may for example be linear in the rank g r  = l r   or an exponential function g r  = 1l r   where l r  denotes the label  or rating  of the document at rank r. r is the truncation rank. where no subscript is defined  it should be assumed that r = n  the number of documents associated with the query under consideration. a popular choice for the rank discount is
 d r  = 1/log 1 + r  and gr max is the maximum value of   obtained when the documents are optimally ordered by decreasing label value. the intuition behind the ndcg metric is as follows. the discount function ensures we focus on getting documents in the right order at the top of the ranking. the gain function ensures we place more emphasis on the highly relevant documents. the normalization allows us to meaningfully average over many different queries  for which there may be some queries with many more relevant documents than others.
1. learning to rank
¡¡to produce a ranked list of documents for a given query  many methods use a score function to map from document  and query  features x to a real-valued score s. for a document indexed by j:
	sj = f xj w   	 1 
where w is a vector of parameters defining the score function. to produce a ranking the documents are ordered according to score. an ir metric such as ndcg is then used to evaluate the ranked list.
