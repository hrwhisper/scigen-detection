in the selective dissemination of information  or publish/ subscribe  paradigm  clients subscribe to a server with continuous queries  or profiles  that express their information needs. clients can also publish documents to servers. whenever a document is published  the continuous queries satisfying this document are found and notifications are sent to appropriate clients. this paper deals with the filtering problem that needs to be solved efficiently by each server: given a database of continuous queries db and a document d  find all queries q ﹋ db that match d. we present data structures and indexing algorithms that enable us to solve the filtering problem efficiently for large databases of queries expressed in the model awp which is based on named attributes with values of type text  and word proximity operators.
categories and subject descriptors
h.1  information storage and retrieval : content analysis and indexing-indexing methods; h.1  information storage and retrieval : information search and retrieval- information filtering
general terms
algorithms
keywords
selective dissemination of information  filtering algorithms  performance evaluation
 this work was supported in part by the european commission projects diet  1th framework programme ist/fet  and evergrow  1th framework programme ist/fet . christos tryfonopoulos is partially supported by a ph.d. fellowship from the program heraclitus of the greek ministry of education.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
1.	introduction
﹛in the selective dissemination of information  sdi or publish/subscribe  paradigm  clients subscribe to a server with continuous queries or profiles that are expressed in some well-defined language and capture their information needs. clients can also publish documents to servers. when a document is published  the continuous queries satisfying the document are found and notifications are sent to appropriate clients.
﹛this paper deals with the filtering problem that needs to be solved efficiently by each server: given a database of continuous queries db and a document d  find all queries q ﹋ db that match d. this functionality is crucial for a server because we expect deployed sdi systems to handle millions of client queries. we concentrate on selective dissemination of textual information using the data model awp originally presented in  1  1 . data model awp is based on named attributes with values of type text  and its query language includes attributes  comparison operators  equals  and  contains  and word proximity operators from the boolean model of information retrieval  ir  . by using linguistically motivated concepts such as word instead of arbitrary strings  awp strives to be useful to certain applications e.g.  alert systems for digital libraries or other commercial systems where similar models are supported already for retrieval. in work presented in  and   we discuss the distributed alert system dias and its ancestor  the peer-to-peer system p1p-diet1  that uses awp as its meta-data model for describing and querying digital resources and a new filtering algorithm named bestfittrie for matching incoming documents against stored continuous queries.
﹛in this paper we develop and evaluate efficient main-memory algorithms that are able to filter millions of continuous awp queries in just a few hundred milliseconds. the filtering algorithms we present are the first in the literature that deal with ir-based models like awp. the algorithms closest to ours are the ones employed in the boolean version of sift  where documents are free text and queries are conjunctions of keywords. sift has been the inspiration for this work and the results presented in sections 1 and 1 extend and improve the results of . in particular  we evaluate experimentally algorithms bf  swin and prefixtrie that are extensions of the algorithms bf  key and tree of  for the model awp. we also discuss in detail the new algorithms bestfittrie and lcwtrie as alternatives to prefixtrie  and compare them under various experimental settings.
﹛work on recent filtering algorithms for xml-based query languages  1  1  1  is complementary to ours. the query languages of  1  1  1  cannot express word proximity as in awp and the same is true for other w1c xml query languages. the recent w1c working draft  and papers like  1  1  1  are expected to pave the way for the introduction of ir concepts in xquery/xpath. our work on awp serves a similar goal but we have chosen to work with the simple concept of a flat document interpreted under the boolean model of ir  instead of xml and the vector space model. the main ideas of this paper can be transferred to languages such as  1  1  1  and this is where we concentrate our current efforts.
﹛the rest of the paper is organized as follows. section 1 presents the model awp and section 1 presents the filtering algorithms we developed. section 1 gives a flavour of our query creation methodology and evaluates the algorithms experimentally under various different parameters. finally  section 1 hints at our current work.
1.	the data model awp
﹛in  we present the data model awp for specifying queries and textual resource meta-data in sdi systems. awp is based on the concept of named attributes with values of type text. the query language of awp offers boolean and proximity operators on attribute values as in the work of  which is based on the boolean model of information retrieval  ir .
syntax. let 曳 be a finite alphabet. a word is a finite non-empty sequence of letters from 曳. let v be a  finite or infinite  set of words called the vocabulary. a text value s of length n over vocabulary v is a total function s : {1 ... n} ↙ v.
﹛let i be a set of  distance  intervals i = { l u  : l u ﹋ n l ≡ 1 and l ≒ u}﹍{ l ﹢  : l ﹋ n and l ≡ 1}. a proximity formula is an expression of the form w1  i1 ﹞﹞﹞  in 1 wn where w1 ... wn are words of v and i1 ... in are intervals of i. operators  i are called proximity operators and are generalizations of the traditional ir operators kw and kn . proximity operators are used to capture the concepts of order and distance between words in a text document. the proximity word pattern w1   l u  w1 stands for  word w1 is before w1 and is separated by w1 by at least l and at most u words . the interpretation of proximity word patterns with more than one operator  i is similar. a word pattern over vocabulary v is a conjunction of words and proximity formulas. an example of a word pattern is applications ＿ selective   1  dissemination   1  information.
﹛let a be a countably infinite set of attributes called the attribute universe. in practice attributes will come from namespaces appropriate for the application at hand e.g.  from the set of dublin core metadata elements1.
﹛a document d is a set of attribute-value pairs  a s  where a ﹋ a  s is a text value over v  and all attributes are distinct. the following set of pairs is a document:
{  author  john smith   
 title  selective dissemination of information in p1p ...   
 abstract  in this paper we show that ...   }
a query is a conjunction of the form
a1 = s1 ＿ ... ＿ an = sn ＿ b1 w wp1 ＿ ... ＿ bm w wpm where each ai bi ﹋ a  each si is a text value and each wpi is a word pattern. the following formula is a query:
author =  john smith  ＿
title w  selective   1  dissemination   1  information  ＿ p1p
semantics. the semantics of awp have been defined in  and will not be presented here in detail. it is straightforward to define when a document d satisfies an atomic formula of the form a = s or b w wp  and then use this notion to define when d satisfies a query . the example document given above satisfies the example query.
1.	filtering algorithms
﹛in this section we present and evaluate four main memory algorithms that solve the filtering problem for conjunctive queries in awp. because our work extends and improves previous algorithms for sift   we adopt terminology from sift in many cases.
1	the algorithm bestfittrie
﹛bestfittrie uses two data structures to represent each published document d: the occurrence table ot d  and the distinct attribute list dal d . ot d  is a hash table that uses words as keys  and is used for storing all the attributes of the document in which a specific word appears  along with the positions that each word occupies in the attribute text. dal d  is a linked list with one element for each distinct attribute of d. the element of dal d  for attribute a points to another linked list  the distinct word list for a  denoted by dwl a   which contains all the distinct words that appear in a d .
﹛to index queries bestfittrie utilises an array  called the attribute directory  ad   that stores pointers to word directories. ad has one element for each distinct attribute in the query database. a word directory wd bi  is a hash table that provides fast access to roots of tries in a forest that is used to organize sets of words - the set of words in wpi  denoted by words wpi   for each atomic formula bi w wpi in a query. the proximity formulas contained in each wpi are stored in an array called the proximity array  pa . pa stores pointers to trie nodes  words  that are operands in proximity formulas along with the respective proximity intervals for each formula. there is also a hash table  called equality table  et   that indexes all text values si that appear in atomic formulas of the form ai = si.
﹛when a new query q of the form given above arrives  the index structures are populated as follows. for each attribute ai 1 ≒ i ≒ n  we hash text value si to obtain a slot in et where we store the value ai. for each attribute bj 1 ≒ j ≒ m  we compute words wpj  and insert them in one of the tries with roots indexed by wd bj . finally  we visit pa and store pointers to trie nodes and proximity intervals for the proximity formulas contained in wpj.
﹛let us now explain how each word directory wd bj  and its forest of tries are organised. the main idea behind this data structure is to store sets of words compactly by exploiting their common elements. in this way  memory space is preserved and filtering becomes more efficient as we will see below.
﹛definition 1. let s be a set of sets of words and s1 s1 ﹋ s with s1   s1. we say that s1 is an identifying subset of s1 with respect to s iff s1 = s1 or   r ﹋ s such that s1   r.
the sets of identifying subsets of two sets of words s1 and s1 with respect to a set s is the same if and only if s1 is identical to s1. table 1 shows some examples that clarify these concepts.
﹛the sets of words words wpi  are organised in the word directory wd bi  as follows. let s be the set of sets of words currently in wd bi . when a new set of words s arrives  bestfittrie selects the best trie in the forest of tries of wd bi   and the best location that trie to insert s. the algorithm for choosing t depends on the current organization of the word directory and will be given below.
﹛throughout its existence  each trie t of wd bi  has the following properties. the nodes of t store sets of words and other data items related to these sets. let sets-of-words t  denote the set of all sets of words stored by the nodes of t. a node of t stores more than one set of words if and only if these sets are identical. the root of t  at depth 1  stores sets of words with an identifying subset of cardinality one. in general  a node n of t at depth i stores sets of words with an identifying subset of cardinality i + 1. a node n of t at depth i storing sets of words equal to s is implemented as a structure consisting of the following fields:
  word n : the  i + 1 -th word wi of identifying subset {w1 ... wi 1 wi} of s where w1 ... wi 1 are the words of nodes appearing earlier on the path from the root to node n.
  query n : a linked list containing the identifier of query q that contained word pattern wp for which {w1 ... wi} is the identifying subset of sets-of-words t .
  remainder n : if node n is a leaf  this field is a linked list containing the words of s that are not included in {w1 ... wi}. if n is not a leaf  this field is empty.
  children n : a linked list of pairs  wi+1 ptr   where wi+1 is a word such that {w1 ... wi wi+1} is an identifying subset for the sets of words stored at a child of wi and ptr is a pointer to the node containing the word wi+1.
the sets of words stored at node n of t are equal to
{w1 ... wn}﹍remainder n   where w1 ... wn are the words on the path from the root of t to n. an identifying subset of these sets of words is {w1 ... wn}. figure 1 shows the general form of our index structure  we have omitted et and pa . the part of wd bi  corresponding to the queries of table 1 is shown in full including lists query n  and remainder n . the purpose of remainder n  is to allow for the delayed creation of nodes in trie. this delayed creation lets us choose which word from remainder n  will become the child of current node n depending on the sets of words that will arrive later on.
﹛the algorithm for inserting a new set of words s in a word directory is as follows. the first set of words to arrive will create a trie with a randomly chosen word as the root and the rest stored as the remainder. the second set of words will consider being stored at the existing trie or create a trie of its own. in general  to insert a new set of words s  bestfittrie iterates through the words in s and utilises the hash table implementation of the word directory to find all candidate tries for storing s: the tries with root a word of s. to store sets as compactly as possible  bestfittrie then looks for a trie node n such that the set of words  {w1 ... wn} ﹍ remainder n   ﹎ s  where {w1 ... wn} is the set of words on the path from the root to n  has maximum cardinality. there may be more than one node that satisfies this requirements and such nodes might belong to different tries. thus bestfittrie performs a depth-first search down to depth |s|   1 in all candidate tries in order to decide the optimal node n. the path from the root to n is then extended with new nodes containing the words in 而 =  s   {w1 ... wn}  ﹎ remainder n . if s   {w1 ... wn} ﹍ remainder n   then the last of these nodes l becomes a new leaf in the trie with query l  = query n  ﹍ {q}  q is the new query from which s was extracted  and remainder l  = remainder n  而. otherwise  the last of these nodes l points to two child nodes l1 and l1. node l1 will have word l1  = u  where u ﹋ remainder n   而  query l1  = query n  and remainder l1  = remainder n    而 ﹍ {u} . similarly node l1 will have word l1  = v  where v ﹋ s  {w1 ... wn}﹍而   query l1  = q and remainder l1  = s    {w1 ... wn} ﹍ 而 ﹍ {u} . the complexity of inserting a set of words in a word directory is linear in the size of the word directory.
﹛the filtering procedure utilises two arrays named total and count. total has one element for each query in the database and stores the number of atomic formulas contained in that query. array count is used for counting how many of the atomic formulas of a query match the corresponding attributes of a document. each element of array count is set to zero at the beginning of the filtering algorithm. if at algorithm termination  a query's entry in array total equals its entry in count  then the query matches the published document  since all of its atomic formulas match the corresponding document attributes.
﹛when a document d is published at the server  filtering proceeds as follows. bestfittrie hashes the text value c d  contained in each document attribute c and probes the et to find matching atomic formulas with equality. then for each attribute c in dal d  and for each word w in dwl c   the trie of wd c  with root w is traversed in a breadth-first manner. only subtrees having as root a word contained in c d  are examined  and hash table ot d  is used to identify them quickly. at each node n of the trie  the list query n  gives implicitly all atomic formulas c w wpi that can potentially match c d  if the proximity formulas in wpi are also satisfied. this is repeated for all the words in dwl c   to identify all the qualifying atomic formulas for attribute c. then the proximity formulas for each qualifying query are examined using the polynomial time algorithm prox from . for each atomic formula satisfied by c d   the corresponding query element in array count is increased by one. at the end of the filtering algorithm arrays total and count are traversed and the values stored for each query are compared. the equal entries in the two arrays give us the queries satisfied by d.
1	other filtering algorithms
﹛to evaluate the performance of bestfittrie we have also implemented algorithms bf  swin and prefixtrie. bf  brute force  has no indexing strategy and scans the query database sequentially to determine matching queries. swin  single

idquery bi w wpiidentifying subsets1bi w databases{databases}1bi w relational   1  databases{databases  relational}1bi w databases ＿ relational{databases  relational}1bi w  software   1  neural   1  networks  ＿  software   1  relational   1  databases {databases  relational  neural}  ...1bi w optimal ＿  artificial   1  intelligence  ＿ relational ＿ databases{databases  relational  artificial  intelligence  optimal}  ...1bi w artificial ＿ relational ＿ intelligence ＿ databases ＿ knowledge{databases  relational  artificial  intelligence  knowledge }  ...table 1: identifying subsets of words wpi  with respect to s = {words wpi   i = 1 ... 1}.figure 1: bestfittrie organisation of the atomic queries of table 1

figure 1: prefixtrie organisation of the atomic queries of table 1
word index  utilises a two-level index for accessing queries in an efficient way. a query of the form presented at the beginning of this section is indexed by swin under all its attributes a1 ... an b1 ... bm and also under n text values s1 ... sn and m words selected randomly from wp1 ... wpm. more specifically swin utilises an et to index equalities and an ad pointing to several wds to index the atomic containment queries. atomic queries within a wd slot are stored in a list. prefixtrie is an extension of the algorithm tree of  appropriately modified to cope with attributes and proximity information. tree was originally proposed for storing conjunctions of keywords in secondary storage in the context of the sdi system sift. following tree  prefixtrie uses sequences of words sorted in lexicographic order for capturing the words appearing in the word patterns of atomic formulas  instead of sets used by bestfittrie . a trie is then used to store sequences compactly by exploiting common prefixes .
﹛algorithm bestfittrie constitutes an improvement over prefixtrie. because prefixtrie examines only the prefixes of sequences of words in lexicographic order to identify common parts  it misses many opportunities for clustering  see figure 1 . bestfittrie keeps the main idea behind prefixtrie but  a  handles the words contained in a query as a set rather than as a sorted sequence and  b  searches exhaustively the forest of trie to discover the best place to introduce a new set of words. this allows bestfittrie to achieve better clustering as shown in figures 1 and 1  where we can see that it needs only one trie to store the set of words for the formulas of table 1  whereas prefixtrie introduces redundant nodes that are the result of using a lexicographic order to identify common parts. this node redundancy can be the cause of deceleration of the filtering process as we will show in the next section. to improve beyond bestfittrie it would be interesting to consider re-organizing the word directory every time a new set of words arrives  or periodically  but this might turn out to be prohibitively expensive. in this work we have not explored this approach in any depth.
1.	experimental evaluation
﹛to carry out the experimental evaluation of the algorithms described in the previous section  we needed data to be used as incoming documents  as well as user queries. it may not be difficult to collect data to use in the evaluation of filtering algorithms for sdi scenarios. for the model awp considered in this paper there are various document sources that one could consider: meta-data for papers on various publisher web sites  e.g.  acm or ieee   electronic newspaper articles  articles from news alerts on the web  e.g.  http://www.cnn.com/email  etc. however  it is rather difficult to find user queries except by obtaining proprietary data  e.g.  from cnn's news alert system .
﹛for our experiments we chose to use a set of documents downloaded from researchindex1 and originally compiled in . the documents are research papers in the area of neural networks and we will refer to them as the nn corpus1. because no database of queries was available to us  we developed a methodology for creating user queries using words and technical terms extracted automatically from the research index documents using the c-value/nc-value approach of . the extracted multi-word technical terms are used to create proximity formulas and also as conjunctions of keywords in user queries. for the formulation of user queries author names and words extracted from paper abstracts are also used. the attribute universe for the experiments presented in this section consists of paper title  authors  abstract and body.
﹛more specifically the basic concept for the query creation in our methodology is that of a unit. atomic queries are created as conjunctions of units selected uniformly from unit sets  whereas queries are created as conjunctions of atomic queries selected from the attribute universe with a probability pci. in our scenario four different types of units sets exist:
  author unit set. this set contains the last names of authors appearing in the full citation graph of researchindex. each author appears in the author unit set as many times as the in-degree of the papers he has published. thus the probabilitypp 汐  of author 汐 to appear in a query is p 汐  = n汐/ k﹋v汐 nk  where n汐 is the number of papers in the citation graph that cite the work of author 汐  and v汐 is the author vocabulary obtained also by the full citation graph.
  proximity formulas unit set. this set contains proximity formulas created using the extracted multi word terms. the technical terms with more than five words were excluded since they were noise  and the set was produced after applying upper and lower nc-value cut-off thresholds for the remaining terms. the proximity operators in this set contain distances according to the number of words contained in each multi-word term.
  keywords from technical terms. this unit set contains keywords extracted from technical terms. these keywords are used as conjuncts in the creation of atomic queries.
  nouns from abstracts. this set contains the nouns used in the corpus abstracts after the cut-off of the most and least frequent words. the rationale behind this is that abstracts are intended to be a comprehensive summary of the publication content  thus nouns from abstracts are appropriate candidates for use in queries.
﹛an example of a user query created artificially from the methodology briefly sketched above is:
	 author w riedel 	＿
 title w implementation ＿  rbf   1  networks  
for space reasons the full description of the methodology is omitted but the interested reader can refer to .
﹛all the algorithms were implemented in c/c++  and the experiments were run on a pc  with a pentium iii 1ghz processor  with 1gb ram  running linux. the results of each experiment are averaged over 1 runs to eliminate any fluctuations in the time measurements.
1	varying the database size
﹛the first experiment that we conducted to evaluate our algorithms targeted the performance of the four algorithms under different sizes of the query database. in this experiment we randomly selected one hundred documents from the nn corpus and used them as incoming documents in the

figure 1: effect of the query database size in filtering time

figure 1: performance in terms of throughput for the algorithms of section 1
query databases of different sizes. the size and the matching percentage for each document used was different but the average document size was 1 words  whereas on average 1% of the queries stored matched the incoming documents. as we can see in figure 1  the time taken by each algorithm grows linearly with the size of the query database. however swin  prefixtrie and bestfittrie are less sensitive than brute force to changes in the query database size. the trie-based algorithms outperform swin mainly due to the clustering technique that allows the exclusion of more non-matching atomic queries filtering. we can also observe that the better exploitation of the commonalities between queries improves the performance of bestfittrie over prefixtrie  resulting in a significant speedup in filtering time for large query databases. additionally  figure 1 contrasts the algorithms in terms of throughput were we can see that bestfittrie gives the best filtering performance managing to process a load of about 1kb per second for a query database of 1 million queries.
﹛in terms of space requirements bf needs about 1% less space than the trie-based algorithms  due to the simple data structure that poses small space requirements. additionally the rate of increase for the two trie-based algorithms is similar to that of bf  requiring a fixed amount of extra space each time. from the experiments above it is clear that bestfittrie speeds up the filtering process with a small extra storage cost  and proves faster than the rest of the algorithms  managing to filter as much as 1 million queries in less the 1 milliseconds  which is about 1% times faster than the sequential scan method and 1% faster than
prefixtrie.
1	varying the document size / matching
percentage
﹛in this set of experiments we wanted to observe the behaviour of the four algorithms in two different aspects. initially we varied the matching percentage of the queries for a specific document to observe the matching time variations  and secondly we varied the length of the document to observe the sensitivity of each of the algorithms to such a change.
﹛for the first experiment we used two documents a and b that contained the same number of distinct words and the same attributes  but number of queries that matched each document was different. notice that the way the algorithms are designed the important parameter of a document is the number of distinct words contained  rather than its size. this happens because the probing of the query index uses the distinct words contained in the attribute text. practically the increase in the number of distinct words  increases the probability of a specific word contained in a query  to be also contained in the incoming document. this in turn increases the number of queries with proximity formulas that need to be evaluated1  which is a time consuming process. the size of the document is of smaller importance  since it only increases the number of positions of a specific word in the document  and thus the number of checks at proximity evaluation time. however due to the algorithm presented in  the majority of the positions of a specific word in a document can be excluded from the proximity evaluation.
﹛figure 1 shows the % increase in matching time for two documents a and b with the same number of distinct words  but different number of queries matching them. document b contained 1 words  and matched 1% more queries than document a  which contained 1 words. both documents contained four  attribute value  pairs  and the query database contained 1 million queries. apart from bf which showed a 1% increase in the matching time  bestfittrie appears to be the most sensitive to the increase in the matching percentage  showing a 1% increase in filtering time   in contrast to prefixtrie and swin  which appear to be less affected  with 1% and 1% increase respectively . this can be explained as follows. the trie structure of prefixtrie and bestfittrie forces them to explore a big number of child nodes when a word node appears in a document  in contrast to swin that searches in either case all the nodes that are hashed under a specific word. this means that in higher matching percentages  the trie-based algorithms loose some of the advantages offered by their sophisticated data structures and show greater sensitivity to the matching degree. however the trie-based algorithms are still signifi-

figure 1: % increase in filtering time for a 1% increase in the number of matching queries
cantly faster  with bestfittrie being the faster algorithm of all four despite the high increase.
﹛in the second experiment we wanted to observe the behaviour of the four algorithms when the size of the incoming document increases. this time two documents with about the same number of queries matching them were chosen  and the variations in the performance of the four algorithms were examined. document a was 1 words long and contained 1 distinct words  whereas document b was 1 words long and contained 1 distinct words  that is about 1% more words than document a. the differences in the performance were below 1% in matching time for swin  prefixtrie and bestfittrie  whereas bf showed an increase of about 1%. the insensitivity of swin  prefixtrie and bestfittrie in the document size is mainly due to the hash representation of the document and the way the matching process is carried out. during the matching process we actually consider only the distinct words of the document  that are obviously significantly less than the document itself for large documents   and check the existence of a word in the document using a hash function  which provides fast answer times. moreover the proximity evaluation is not greatly affected from the large number of word positions inside a document due to the well-designed proximity evaluation algorithm of  that allows the omission of a large number of word positions in a document.
﹛since in an sdi scenario one may not always have to deal with large documents  for example  if awp is used for describing metadata about research papers  we carried out experiments with documents with smaller size. more specifically experiments with documents of mean size of 1 words  show that bestfittrie performs even better in terms of filtering time  being 1 times faster that prefixtrie and about 1 times faster that bf  as opposed to 1 and about 1 times faster respectively for documents of mean size 1 words .
1	updating the query database
﹛in this experiment we investigated the update time for the four different algorithms. to measure the average time needed to insert a single query in the database we worked as follows. starting with the empty database  we measured the total time needed to populate it with 1k queries  and proceeded in a similar way by adding  bulks  of 1k queries

figure 1: query insertion time for different query database sizes
in our database and measuring the total insertion time per bulk. subsequently the average insertion time per query for a given bulk of queries can be found simply by dividing the total time measured with the population of the bulk to produce a single point in the graph of figure 1.
﹛it should be clear that for bf and swin the query insertion time will be constant on average  since bf does a simple insertion at the end of a list  while swin utilises a hash table and inserts each atomic query in the beginning of the list pointed to by the hash table slot. for the trie-based algorithms the query insertion is a more complex process that involves the examination of lists at every level of the trie. while prefixtrie examines only a single path in a single trie of the forest  bestfittrie needs to examine several paths in the trie and also several tries  in the usual case as many tries as the number of words in a profile . our is remarks are verified by the graph in figure 1 that shows the average insertion time in milliseconds for a query q for a given database size. in this figure we can see that bestfittrie needs about 1% more time than prefixtrie to insert a query in a database with 1 million profiles. this is a standard tradeoff where the algorithm spends some extra time at indexing to save it at query execution.
1	incorporating ranking information
﹛to examine the performance of the two trie-based algorithms  namely prefixtrie and bestfittrie   we modified them in order to take into account information about the frequency of occurrence of words in documents. more specifically we use the document frequency of a word wi  denoted by dfi   which represents the number of documents in a collection that contain wi  to identify the frequent and infrequent words among the documents. in an sdi scenario where no document collection is available  we can compute dfi on the collection of recently processed documents   say k most recent documents arrived  where k is large enough . using these information we created variations of the trie-based algorithms that use different heuristics for storing user queries in tries.
﹛the rank heuristic stores the most frequent words among the documents  that is the words with the highest df  near the roots of the tries  while the less frequent words  that is the words with the lowest df  are pushed deeper in them

figure 1: incorporating word frequency information into the trie-based algorithms  and its effect in filtering time
resulting in relatively few big and  wide  tries  since their roots will exist in more queries . the algorithms using the rank heuristic are prefixtrie-rank and bestfittrie-rank.
﹛contrary to rank  the inverse rank heuristic  irank   stores the least frequent words of the queries near the roots of the tries  while the frequent ones are pushed deeper in the tries  resulting in many narrow tries. thus more queries are put in subtrees of words occurring less frequently  resulting in less lookups during filtering time. the algorithms using the irank heuristic are prefixtrie-irank and bestfittrieirank.
﹛the probability that any word wi appears in an incoming document d is defined to follow probability distribution d wi   where 1 ≒ d wi  ≒ 1. the number of nodes n that will be examined within each trie depends on the clustering heuristic and is equal to
     n = d w1 n1 + d w1 n1 + ... + d w|v| n|v|  1  where ni is the number of nodes in the trie that have word wi as root node  and |v | is the size of our vocabulary. the sum
                 n1 + n1 + ... + n|v|  1  is positive and it is always less than or equal to the number of words in the query database.
﹛from equations 1 and 1 we can see that the number n of nodes examined is minimised if we assign more words to wd slots pointing to words  trie roots  with smaller probability to appear in a document. based on the above observation we created a modification of bestfittrie  called lcwtrie  least common word  by limiting bestfittrie to consider only one candidate trie during insertion: the one that has the least frequent word of the atomic query as root. this way  the atomic query can only be inserted in that trie  or that trie will be created if it does not exist   while the remainder of the words of the atomic query will be organised following the insertion algorithm of bestfittrie  this will give us the best organisation considering only this trie instead of the whole forest .
﹛in figure 1 we present the performance of prefixtrie and bestfittrie and their ranking variations. we can see that

figure 1: performance of lcwtrie in comparison to the two faster filtering algorithms
using the rank heuristic the performance of both algorithms deteriorates  due to the creation of large tries that need bigger exploration time. we can also observe that the irank heuristic improves the performance of both trie-based algorithms  with the greater effect shown on prefixtrie that becomes faster than bestfittrie-irank. this improvement in performance for both algorithms was expected as shown earlier in this section.
﹛figure 1 presents the performance of the three faster algorithms  namely prefixtrie-irank  bestfittrie-irank and lcwtrie. bestfittrie-irank prioritises clustering over frequency information by examining all candidate tries and choosing the one that has the most common words. word frequency information plays a secondary role  allowing the algorithm to choose between tries with the same common words the trie that has the highest ranked word as a root. on the other hand  prefixtrie-irank and lcwtrie are designed to show a preference in frequency information against clustering. more specifically both algorithms examine exactly one candidate trie  that with the least frequent word as root. additionally  lcwtrie organises the query within that trie in the best possible way  taking into account common words between the queries already stored. in contrast  prefixtrie-irank does not care about clustering and stores the query according to frequency information only  that is the word with the lowest rank goes deeper in the trie.
﹛our observations about the significance of frequency information presented in the beginning of the section are verified. from the experiments of figure 1 we see that lcwtrie performs similarly with prefixtrie-irank  although it presents a slight advantage for large query databases  due to the clustering within the trie. additionally both algorithms outperform bestfittrie that owes its performance mainly to clustering  giving little consideration to frequency information.
1.	outlook
﹛we are currently working on sdi with data models based on xml and queries based on xquery/xpath with ir features  phrases  word proximity  similarity etc. .
