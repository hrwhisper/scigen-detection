data fusion is the combination of the results of independent searches on a document collection into one single output result set. it has been shown in the past that this can greatly improve retrieval effectiveness over that of the individual results.
　this paper presents probfuse  a probabilistic approach to data fusion. probfuse assumes that the performance of the individual input systems on a number of training queries is indicative of their future performance. the fused result set is based on probabilities of relevance calculated during this training process. retrieval experiments using data from the trec ad hoc collection demonstrate that probfuse achieves results superior to that of the popular combmnz fusion algorithm.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval
general terms
algorithms  experimentation
keywords
information retrieval  data fusion  probfuse
1. introduction
　in the past  many algorithms have been developed to address the information retrieval  ir  task of identifying which documents in a database are most relevant to a given topic or query. more recently  researchers have focussed on
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
attempting to improve upon the performance of individual ir models by combining the outputs of a number of such models into a single result set   .
　the task of fusing result sets produced by using a number of ir models to query the same document collection has become known as data fusion . this is different to collection fusion   which involves the fusion of result sets that have been produced from querying distinct document collections that have little or no overlap.
　this paper is organised as follows: in section 1 we describe the problem that fusion is intended to solve. section 1 outlines previous work that has been undertaken by other researchers in this field. in section 1 we describe probfuse  a novel probabilistic algorithm for data fusion. section 1 describes our experiments to evaluate the performance of the probfuse algorithm on inputs taken from two text retrieval conferences  trec . we also compare this with the performance of the popular combmnz approach . finally  our conclusions and future work are outlined in section 1.
1. problem description
　of the numerous approaches to ir that have been proposed  none has been shown to achieve superior performance to all others in all situations. this may be as a result of difference in policies regarding query or document preprocessing  the algorithms used and representations of documents and queries. individual ir systems have been shown to retrieve different documents in response to the same queries when operating on the same document collection . this has been observed even where the overall retrieval effectiveness of these different systems has been similar .
　retrieval performance has been shown to be improved by fusing the result sets produced by a number of different ir systems into a single result set. a number of different approaches to data fusion are outlined in section 1.
　vogt and cottrell  identify three  effects   any of which can be leveraged by a fusion technique. in some cases  a number of input result sets agree on the relevance of particular documents. fusion techniques that take this agreement into account when compiling the fused result set will perform well in such circumstances. this is described as the  chorus effect . experiments carried out by lee  have shown that this is a very significant effect for data fusion tasks.
　the exploitation of the chorus effect is the principal difference between the data fusion and collection fusion tasks. for data fusion  each individual ir technique is searching an identical document collection. this means that whenever a document is contained in multiple result sets  this can be presumed to infer relevance. however  when the document collections being searched are disjoint  collection fusion   this situation clearly cannot arise. in situations where the collections are partially overlapping  the presence of a document in multiple result sets cannot be used as an indication of greater relevance than a document that only appears in one. a document may only appear in a single result set because either the other ir models did not consider it to be relevant to the given query or it was not contained in the other document collections. thus  in order for a fusion technique to make use of the chorus effect  it must be known that the document collections that are being queried by the different inputs have a very high degree of overlap.
　they also describe the  skimming effect . multiple result sets are more likely to result in higher recall  i.e. the fraction of relevant documents that have been retrieved  than a single one. a fusion technique can take advantage of this by  skimming  the top documents from each result set  as that is where the relevant documents are most likely to occur.
　the  dark horse effect  is an apparent contradiction to the chorus effect. here  fusion effectiveness can be improved by identifying one input result set whose quality is of a substantially different level to the others. this may be due to either unusually high or unusually low numbers of relevant documents being returned. the apparent contradiction arises because while the chorus effect argues that fusion techniques should take as many of the input result sets as possible into account  the dark horse effect argues in favour of identifying a single input result set
1. background research
　there are two principal categories of fusion techniques. some algorithms make use of the score assigned to each document in each input result set to calculate a final score for that document in the fused result set. because these raw scores are not always directly comparable  e.g. one input result set might assign scores in a range of 1 while another uses 1   score-based techniques frequently make use of a score normalisation phase before fusion takes place. this typically involves the mapping of all scores to a common range. others make use of the rank each document occupies in each of the inputs  as the scores are not always available.
　a linear combination model has been used in a number of studies    . under this model  a weight is calculated for each input model. in order to calculate a ranking score for a document  the score it is assigned by each input model is multiplied by that model's weight. these are then summed to get the final ranking score for the fused result set. a variation on the linear combination model using normalised scores was used in  and .
　a number of fusion techniques based on normalised scores were proposed by fox and shaw . of these  combsum and combmnz were shown to achieve the best performance and have been used in subsequent research. under combsum  a document's score is calculated by adding the normalised scores returned by the individual input models. its combmnz score is found by multiplying the combsum score by the number of non-zero relevance scores that it was assigned. in particular  lee  achieved positive results using combmnz on the trec-1 data set. these techniques have also been used in real-world systems: the metacrawler  and savvysearch  meta search engines both use combsum to fuse results.
　research by manmatha et al.  demonstrated that the scores given to documents by an ir system can be modelled using a normal distribution for relevant documents and an exponential distribution for nonrelevant documents. this was possible even when relevance judgments were not available for the queries in question. using bayes' rule  it was then possible to calculate the probability of relevance  given the score. when performing fusion  these probabilities were averaged  producing performance approaching that of combmnz.
　perhaps the simplest rank-based fusion technique is interleaving . under this system  the fused result set is constructed by firstly taking the top-ranked document from each input result set  followed by the second-ranked documents and so on. this approach operates on the assumption that each of the inputs are of similar effectiveness  and has been shown empirically to fall short of its goal of outperforming its inputs . voorhees et al.  proposed two variations on simple interleaving  in which training data was used to weight the input models according to past performance. at the interleaving stage  different quantities of documents were taken from each result set  depending on these weights  rather than taking equal amounts from each.
　lee  proposed a rank-based variation on combmnz  in which a function of each document's rank in each input result set was used as an alternative for normalised scores.
　aslam and montague compared the fusion process to a democratic election in which there are few voters  the input models  and many candidates. they achieved positive results by implementing adapted implementations of two algorithms designed for that situation. borda-fuse  awards a score to each document depending on its position in each input result set  with its final score being the sum of these. condorcet-fuse  ranks documents based on a pairwise comparison of each. a document is ranked above another if it appears above it in more input result sets.
　other techniques have been proposed that make use of the actual textual content of the documents returned  . others rely on the individual input models providing metadata about the returned documents  other than simply a ranked list with relevance scores .
1. probabilistic fusion
　in this section  we describe probfuse  a probabilistic approach to data fusion. probfuse ranks documents based on their probability of relevance to the given query. this probability is calculated during a training phase  and depends on which input system returned the document amongst its results and the position in the result set in which the document was returned.
　the inputs to the fusion process are a number of collections of result sets that are produced by different ir models running the same queries on the same document collection. in order to run probfuse  we first build a set of probabilities for each input set. these probabilities are calculated by analysing the performance of each individual model on a number of training queries.
　rather than using the exact position a document occupies in each result set  the input result sets are divided into x segments. for each segment  the probability that a document being returned in this segment is relevant to a given query is calculated. this probability is averaged over t% of the total queries that are available.
　in a training set of q queries  p dk|m   the probability that a document d returned in segment k is relevant  given that it has been returned by retrieval model m  is given by:
		 1 
　where |rk q| is the number of documents in segment k that are judged to be relevant to query q  and |k| is the total number of documents in segment k.
　in the past  it has been demonstrated that probfuse achieves significantly better results than combmnz when applied to small document collections . for these collections  full relevance judgments are available  so the relevance of every document is known during the training phase. for larger collections  however  this is not the case  as the relevance judgments are incomplete  i.e. for some documents  it is unknown whether they are relevant or nonrelevant to the given queries . for this reason  we also use a slight variation of the probability calculation. this allows us to observe the effects  if any  of different methods of dealing with unjudged documents.
　equation 1 takes all the documents in a segment into account  assuming unjudged documents to be nonrelevant. our modified probability calculation ignores unjudged documents and thus only takes into account documents that have been judged to be either relevant or nonrelevant. in this case  the probability p dk|m  is given by
		 1 
　where |rk q| is the number of documents in segment k that are judged to be relevant to query q  and |nk q| is the number of documents in segment k that are judged to be nonrelevant to query q.
　we refer to probfuse runs using the probability calculation in equation 1 as probfuseall and those using equation 1 as probfusejudged. from these equations  it can be seen that for document collections with complete relevance judgments  the probabilities calculated by probfuseall and probfusejudged will be identical.
　after the training phase is complete and a set of probabilities for each input model has been built  we can then use this to construct a fused result set for subsequent queries. for these  the ranking score sd for each document d is given by
m
	sd = x p dkk|m 	 1 
                                   m=1 where m is the number of retrieval models being used  p dk|m  is the probability of relevance for a document dk that has been returned in segment k in retrieval model m  and k is the segment that d appears in  1 for the first segment  1 for the second  etc. . for any input model that does not return document d in its result set at all  p dk|m  is considered to be zero  in order to ensure that documents do not receive any boost to their ranking scores from models that do not return them as being judged relevant.
　this approach to data fusion attempts to make use of the three effects described in section 1 above. by using the sum of the probabilities  we attach more significance to documents that have been returned by multiple input models  thus exploiting the chorus effect. the division by the segment number k gives a greater weight to documents that appear early in each of the individual result sets  making use of the skimming effect. finally  because the probabilities are calculated based on the actual past performance of each input model  we attach greater importance to input models that are more likely to return relevant documents in particular segments  dark horse effect .
1. experiments and evaluation
　in this section  we describe the experiments that were performed to evaluate the effectiveness of probfuse. the probfuse algorithm was applied to a number of different combinations of input result sets and the resulting fused result was compared to that of the popular combmnz algorithm. combmnz is easily implemented and has been shown to perform well on data fusion tasks . this has made it an attractive choice when choosing a baseline technique to compare with. as such  it has become the standard algorithm to use  .
　in order to run combmnz  two steps must be performed. firstly  the scores attributed to each document by each input must be normalised  so that they lie in a common range. a number of different normalisation strategies have been proposed. we have chosen the one used by lee   as it is the one most commonly used for comparison and has been described as  standard normalisation  . lee's implementation of combmnz calculates normalised scores using
 
　where max sim and min sim are the maximum and minimum scores that are actually seen in the input result set. once the scores have been normalised  combmnzd  the combmnz ranking score for any document d is given by
s
	combmnzd = xns d 〜 |nd   1|	 1 
s=1
　where s is the number of result sets to be fused  ns d is the normalised score of document d in result set s and |nd   1| is the number of non-zero normalised scores given to d by any result set.
1 experimental setup
　as our inputs  we used data from the ad hoc retrieval track of the trec-1 and trec-1 conferences. this data consists of the topfile  a collection of result sets  produced by each of the groups that participated in those conferences. each topfile contains result sets for 1 topics  queries : trec-1 uses trec topics 1 and trec-1 uses topics 1. 1 topfiles are available for trec-1  while 1 are available table 1: inputs to trec-1 experimental runs first second third fourth fifth

acqnt1clartmbrkly1assctv1assctv1citri1crnllaclartaerima1nyuir1crnleadortd1dortd1lsia1mfrutfua1padre1eth1eth1lsia1mw1rutfua1xerox1nyuir1inq1virtu1siems1xerox1padre1pircs1vtc1westp1table 1: inputs to trec-1 experimental runs
first	second	third	fourth	fifth

brkly1anu1man1anu1aut1dcu1anu1aut1dcu1clcluscity1genrl1colm1ethal1erlia1clthesibmge1cor1crkusg1genrl1ethas1ibms1alnmfull1vtwna1ibms1bgenrl1kusg1lnmfull1vtwnb1uwgcx1ibmgd1mds1pircsaalfor trec-1. only the topfiles in category a were considered for trec-1  as category b participants operated on only a subset of the data.
　for each of these two data sets  we performed 1 experimental runs. each experimental run firstly involved choosing six random topfiles to use as inputs. in order to eliminate the results being skewed by the ordering of the topics  we produced five random orderings for the topics and performed data fusion using both probfuse and combmnz over each of these. the performance evaluation values for each run are the average for each of those five random orderings. no input topfile was used in multiple runs. the inputs used for each of the five runs for trec-1 and trec-1 are shown in table 1 and table 1 respectively. the inclusion of this list of inputs is intended to aid the reproduction of our experiments.
　each run was performed for a variety of training set sizes defined as a percentage of the number of available queries. in the case of the trec-1 and trec-1 data  the number of available queries is always 1. the number of segments into which each result set was divided was also varied. we used training set sizes  t such that t （{1  1  1  1  1} and numbers of segments  x such that x （{1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1}.
　in order to ensure comparability  combmnz was only applied to those queries used in the fusion phase of probfuse  with the training queries being ignored. this will cause the evaluation results for combmnz to vary as the training set size changes  since the number of remaining queries on which fusion is performed also changes.
　the goal of these experiments is to empirically determine the combination of training set size and number of segments  denoted by x  that achieves the greatest retrieval performance  both in terms of the evaluation scores it receives and its performance in relation to combmnz. we approach this by firstly identifying a training set size that results in high performance for both the trec-1 and trec-1 input sets. this is discussed in section 1. once this is done  we examine the performance of both variations of probfuse for different values of x to find an x-value that performs well on both input sets when averaged over the five runs. this is done in section 1.
　the evaluation of the fused output result sets was performed by treceval  which is the evaluation software used for the trec conferences . we use two evaluation measures in our experiments. firstly  we use mean average precision  map  to find our training set size and x-value. map is the mean of the precision scores obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . documents for which a relevance judgment is not available are considered to be nonrelevant.
　after identifying this training set size and x-value  we then examine each of the five experimental runs to make a comparison with the combmnz algorithm in a more detailed manner. at this stage  in addition to map we also make use of the bpref evaluation measure. bpref only takes judged documents into account and is inversely related to the fraction of judged nonrelevant documents that are retrieved before relevant documents . the analysis of these evaluation results is contained in section 1.
1 training set size
　initially  the map measure was used to identify which training set sizes resulted in the best performance. table 1 and table 1 show the average map achieved when using each training set size  along with its improvement over the corresponding figure for combmnz. the map score included in those figures is the average map score for all values of x  the number of segments  at that training set size over all five runs. the average map scores for combmnz vary with training set size  despite combmnz not making use of any training phase. in each of those tables  the highest map score and the greatest improvement over combmnz for each of the probfuse variants are marked in bold type.

figure 1: trec-1 map scores for t = 1%  average over 1 runs 
　from table 1  we can see that the highest average map score by both probfuse variations on the trec-1 inputs was achieved using a training set size of 1%. the biggest per-

table 1: trec-1 average map scores for various training set sizes
training t%combmnzprobfusedifferenceprobfusedifferenceallv. mnzjudgedv. mnz1%11+1%1+1%1%11+1%1+1%1%11+1%1+1%1%11+1%1+1%1%11+1%1+1%table 1: trec-1 average map scores for various training set sizes
training t%combmnzprobfusedifferenceprobfusedifferenceallv. mnzjudgedv. mnz1%11+1%1+1%1%11+1%1+1%1%11+1%1+1%1%11+1%1+1%1%11+1%1+1%figure 1: trec-1 map scores for t = 1%  average over 1 runs 
centage increase over combmnz was when using a training set size of 1%. for the trec-1 inputs  seen in table 1   we note that the highest average map score was when using a training set size of 1%. the greatest percentage improvement over combmnz was achieved for training set sizes of 1% and 1% for probfuseall and probfusejudged respectively.
　both variations of probfuse achieve higher average map scores than combmnz for every training set size. this is the case for both the trec-1 and trec-1 input sets.
　for the purposes of section 1  we have chosen to use a training set size of 1%. at that level  probfuseall and probfusejudged both achieve either their highest map score or their highest increase over combmnz for both sets of inputs. this would not be the case had we chosen a training set size of 1%  as probfuseall achieves its biggest improvement over combmnz at a training set size of 1%. at 1%  both probfuse variations achieve their highest average map scores on the trec-1 inputs. for the trec-1 inputs  both variations achieve their highest improvement over combmnz  and their average map scores are within 1% of the highest they achieve at any level.
1 number of segments
　figure 1 and figure 1 show the map scores for each number of segments for a training set size of 1% for the trec-1 and trec-1 inputs respectively. each of these map scores is the average of the map scores achieved in each of the five runs.
　it is interesting to note that probfusejudged and probfuseall both show near-identical results for both input sets. in each graph  performance is at its worst for a value of x = 1. this is to be expected  as in that situation  each result set is being divided into only two segments  so the probability of relevance being assigned to each document is based on whether it appears in the first or second half of the result set  which is too coarse a measure. initially  as x increases  the average map score improves. a gradual decline is then seen for higher values of x.
　the principal difference in the trend in the two graphs is in the point at which the average map score reaches its peak. this peak is reached for a much lower x-value on the trec-1 inputs  where x = 1. for trec-1  the map score continues improving gradually until the point where x = 1. thereafter  both show a downward trend as x increases.
　an x-value of 1 yields the best map score  on average  over both input sets. for that reason  this is the x-value we have chosen to use when analysing the individual runs in section 1.
1 analysis of individual runs
　having identified a high-performing training set size  1%  and number of segments to divide each result set into  1   we can examine the five individual experimental runs in more detail.
table 1: trec-1 performance of five individual runs for t = 1% and x = 1
combmnzprobfuseallprobfusejudgedmapbprefmapbprefmapbpreffirst111  +1% 1  +1% 1  +1% 1  +1% second111  +1% 1  -1% 1  +1% 1  -1% third111  -1% 1  -1% 1  -1% 1  -1% fourth111  +1% 1  -1% 1  +1% 1  -1% fifth111  +1% 1  -1% 1  +1% 1  -1% average111  +1% 1  +1% 1  +1% 1  +1% 　table 1 shows the results of the five individual runs on the trec-1 input set. in that table  figures in parentheses represent the percentage difference to the corresponding score for combmnz. figure 1 shows the map scores for combmnz  probfuseall and probfusejudged for each run on the trec-1 data  and figure 1 shows the bpref scores for trec-1.
　both variants of probfuse achieved a higher map score than combmnz for all runs except for  third . on that run  combmnz scored slightly higher. it is important to highlight that both probfuse variations actually achieve their highest map scores for that run. the map score for combmnz is also the highest it achieves for any run. the lower map score achieved by probfuse on the  third  run can therefore be attributed to an unusually high map score being achieved by combmnz on that run  rather than probfuse underperforming.
　using the bpref measure  the performance of probfuse is slightly below that of combmnz for four of the five runs. for this reason  it cannot be said that probfuse outperforms combmnz on the trec-1 inputs under bpref. however  neither probfuse variant drops below 1% of combmnz's bpref score on any run  and for the  first  run  probfuse achieves a vastly superior result  leading to a better average performance.
　under both the map and bpref measures  probfusejudged achieves higher performance than probfuseall on each of the five experimental runs. however  this increase only exceeds 1% for the map score on the  fourth  run  and never exceeds 1%.
　table 1 shows a similar table detailing the five individual experimental runs on the trec-1 input set. figure 1 shows the map scores for combmnz  probfuseall and probfusejudged for each run on the trec-1 data  and figure 1 shows the bpref scores for each fusion technique.
　here  probfuse outperforms combmnz on each of the runs using both evaluation measures. in particular  runs  second    third  and  fifth  show large performance gains for both probfuse variations over combmnz for both the map and bpref measures.
　as with trec-1  probfusejudged performs better than probfuseall  although once again the degree of improvement is less than 1% in almost all cases. the exception to this is the map score for the  first  run  which is the only case where probfuseall performs better than probfusejudged. this is particularly interesting in the case of the bpref scores  as bpref only takes judged documents into account  ignoring documents for which relevance judgments are not available. for this level of incompleteness  the performance of probfuseall is similar to that of probfusejudged. it is left to future work to determine if this remains the case as the available relevance judgments become more incomplete.

figure 1: trec-1 map scores for t = 1% and x =
1

figure 1: trec-1 bpref scores for t = 1% and x = 1
　the performance on the trec-1 inputs is superior to combmnz when evaluated using the map measure  with the bpref scores falling only slightly below those of combmnz in some cases. for the trec-1 inputs  probfuse has shown significant improvement over combmnz when evaluated by both map and bpref.
1. conclusions and future work
　in this paper  we have described probfuse  a data fusion algorithm that relies on the probability of relevance to cal-

table 1: trec-1 performance of five individual runs for t = 1% and x = 1
combmnzprobfuseallprobfusejudgedmapbprefmapbprefmapbpreffirst111  +1% 1  +1% 1  +1% 1  +1% second111  +1% 1  +1% 1  +1% 1  +1% third111  +1% 1  +1% 1  +1% 1  +1% fourth111  +1% 1  +1% 1  +1% 1  +1% fifth111  +1% 1  +1% 1  +1% 1  +1% average111  +1% 1  +1% 1  +1% 1  +1% figure 1: trec-1 map scores for t = 1% and x =
1

figure 1: trec-1 bpref scores for t = 1% and x = 1
culate a ranking score for documents in a fused result set. these probabilities are calculated based on the position of relevant documents in result sets returned in response to a number of training queries.
　in experiments using data from the ad hoc track of the trec-1 and trec-1 conferences  two variations of probfuse were shown to significantly outperform the popular combmnz algorithm over a number of different combinations of inputs. probfuseall achieved a map score that was  on average  1% higher than combmnz on the trec-1 inputs and 1% higher on trec-1. similarly  the average bpref score was 1% higher than combmnz on trec-1 inputs and 1% higher on trec-1.
　these results follow on from experiments on small document collections for which complete relevance judgments are available . due to the incomplete nature of the relevance judgments for trec-1 and trec-1  we also tested a variation of probfuse  called probfusejudged  that only takes judged documents into account when calculating its probabilities. probfusejudged achieved an increase of 1% over combmnz on the trec-1 inputs and an increase of 1% on trec-1. the map scores it achieved were 1% higher than combmnz on trec-1 and 1% higher on the trec-1 data.
　it is interesting to note that probfusejudged only achieved marginal performance gains over probfuseall  even using the bpref evaluation measure  which only takes judged documents into account.
　our future work will apply probfuse to larger collections with greater levels of incompleteness in their available relevance judgments  e.g. the web track collections of later trec conferences . we intend to investigate whether the performance of probfusejudged and probfuseall diverges as the level of incompleteness increases. in addition  the relevance judgments for some collections differentiate between different degrees of relevance  e.g. for the wt1g collection  documents can be judged nonrelevant  relevant and highly relevant . we also intend to investigate whether adjustments to our probability calculations that take this information into account will be beneficial.
　another potential research direction is to investigate the possibilities of applying probfuse to a document collection without the necessity of using training data from that collection. this could potentially involve the use of the techniques outlined by manmatha et al.  to estimate the probability of relevance  or alternatively training probfuse on one document collection in order to apply it to another.
