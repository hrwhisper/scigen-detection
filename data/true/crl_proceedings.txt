this paper describes the systems used by crl in the cross-lingual ir and q&a tracks.
the cross-language experiment was unique in that it was run interactively with a mono-lingual user simulating how a true cross-language system might be used. the methods used in the q&a system are based on language processing technology developed at crl for machine translation and information extraction.
cross-lingual ir
can monolingual users create good multilingual queries 
our interest in interactive and cross language text retrieval has led to the design of a unique user interface for the cross language task.  while many automatic techniques for query term translation and disambiguation have been proposed and tested  little work has involved the evaluation of a cross language system in combination with its user. we and others have proposed designing an interface that allows the user to help disambiguate terms provided by a system by providing  backtranslations  of the system selected terms from which a monolingual user can select the appropriate meanings. the mulinex system  http://mulinex.dfki.de  provides a query assistant feature with just such an interface.  for our cross-language track experiment we wanted to see if these types of interfaces would help a monolingual user create good multilingual queries.
in our experiment  a single english speaker  who had little or no experience with german  french  or italian  generated queries in each of these languages for the cross-language track run.  for each topic  the user would read the english title  description  and narrative  and select the english terms from these sections judged to be the best query terms. they were only allowed to select terms that were contained in the original english topic. then for each of the other target languages  the system showed extended english definitions of potentially relevant cross language query terms and phrases alongside their translations  see figure 1 . only those terms that actually occur in the target data were presented to reduce the number of alternative terms. the user then selected the english definition that most accurately reflects the intention in the original query. the query terms selected for each language were used to retrieve and rank documents for that language and the results for all languages were merged into the final ranked list.
retrieval
we conducted our cross-language runs using the unicode retrieval system architecture  ursa  a multilingual retrieval engine that indexes and retrieves text using a common encoding scheme for all languages. therefore  encoding for all texts were first converted to unicode. ursa indexing of

figure 1.  user-assisted cross-language query translation
the text used only simple stemming procedures specific for english  french  italian and german. no other language specific compound word  phase indexing or other types of language processing was attempted.  consequently  one could expect an overall improvement in the performance of the base lined system  given the right effort.  in this experiment we were only concerned with comparing the performance of the system when the cross-language queries were generated by the system with help from a monolingual english-only speaker and the queries that were hand built by native language speakers and provided by nist.
this was a preliminary experiment designed to test the feasibility of our approach. as usual the quality of the bilingual dictionaries will have a strong effect on the outcome.  some good query terms just were not present in the bilingual dictionaries used. in addition  our retrieval and ranking software could be better tuned to take advantage of the forms of the dictionary entries and phrases.
merging
merging the trec multi-lingual queries is a constant issue for the cross-language studies. our query system produces an ordered list  by score  of document id's and the score for each language. the scores for each language are not comparable therefore the query results can not be merged using the score directly. our technique was similar to that reported by the ibm group at trec-1  and involved obtaining a probability estimate that a returned document is relevant which and comparing these estimates between language retrieval systems. we used trec-1 topics and results with our query system to obtain the performance for each language in terms of a sequence of relevance probabilities based on a precision score ordered by rank. to obtain the relevance probability we compared the results of the query system to the nist supplied relevance tables  qrels  that specifies whether a document is relevant to a particular query.  for each language  we generated a table mapping a rank index  from one to one thousand  to a precision score at that rank. for example  this tells us that a document at rank index 1 has a precision of 1  whereas a document at rank index 1 may have a precision of only 1. these rank-precision tables are roughly linear when plotted on a graph of precision vs. log  rank   so using linear regression an estimate of the relevance probability can be obtained for a given rank. these probability values are directly comparable between the different query systems  so the results for each query system can

figure 1.  precision -recall performance of the cross-language retrieval system comparing the trec supplied topics to those generated by the monolingual english user with help from the system
be merged using the probability value as a sort key.
analysis
the primary analysis compares the results obtained by the monolingual user to the results obtained with the hand-translated queries provided by nist for the cross language topics. as can be seen in figure 1  the overall precision/recall curves for the two conditions are quite similar indicating that the user who knows no italian  french  or german can use the system to generate queries that are as good as ones generated from the human translated trec topics for these languages.  the combined results shown in figure 1 contain a large portion of english documents as well as the languages the user does not know.  so  a more informative look at the data is shown in table 1.  here the data show that indeed the english user is not doing as well as the baseline provided by the human translated trec topics.  for italian and french  the user is doing about 1% of the baseline performance and for german it is worse  1 - 1 percent of the baseline .
aveprecall at p .1 italian - english user.1.1italian - baseline trec.1.1% baseline1%1%french - english user.1.1french - baseline trec.1.1% baseline1%1%german - english user.1.1german - baseline trec.1.1% baseline1%1%table 1.  retrieval performance for individual languages comparing english user with trec monolingual queries.
the fall off in performance retrieval can have a number of reasons. for example the dictionaries that were used could have been lacking significant query terms or phrases.  indeed  if a query term could not be translated  the present system would not provide any alternative.  therefore  a number of simple improvements can be made to make the system better.  with the more sophisticated tuning of the system  it can be expected that monolingual users will indeed be able to query in languages they cannot understand.
question answering
extraction based method
crl's approach to the q&a problem is based on the mikrokosmos ontology . the ontology is intended to allow the representation of  complex meanings. it consists of around 1 concepts linked using  1 relationship types. each concept is linked to other concepts through up to 1 different relationships. the ontology is being used principally to support machine translation  but recently we have been investigating its use as a control architecture for information extraction  1 . in this application a static template is defined by naming slots and defining potential slot fillers using the names of concepts from the ontology.  for example:
election
      { elect    elect }
      { person-elected    human}
      { place    place }
      { date    time }
      { position-electedto   social-role }
defines an election  template. the first element being the slot label  and the second the appropriate concept that  must  be attached to an element that would fill this slot.  our idea for question answering was to use the question to dynamically define such a template  partially filled with strings from the question   use a boolean retrieval system to retrieve documents in which the key phrases  or equivalents occur  and extract the missing information -- the answer by carrying out the extraction process.
the amount of effort involved in this task was a total of six man weeks. wherever possible off-theshelf components were used. the boolean retrieval was not completed in time for the evaluation  and the top five documents supplied by the at&t retrieval engine were used. this had an impact on performance  as our whole method  at present  is dependent on information being localized in a single sentence in the document  which is not guaranteed with a general purpose ad-hoc retrieval.
methodology
our complete system consists of three main phases:
  question analysis - recognize question structure and type
  retrieval - query building and document structuring
  answer generation - sentence selection and answer selection.
each of these is described briefly in the sections below.
question analysis
the basic processing undergone by the question and by sentences in the retrieved documents is the same. first the document is processed by a part of speech tagger  this marks each word in the sentence with one part of speech. in our current system we use a statistical tagger from mitre. the text is run independently through the crl diderot name recognition system . this recognizes names of organizations  places  people  and a variety of other units of interest  dates  money percentages etc.  the current complete list is shown in the table below. the labels are names of concepts from the mikrokosmos ontology.
table of elements recognized for the q&a task
linear-sizeelectricitypopulationdensitynationalityareaenergytemporalobjectinhabitantvolumevelocitytime-objectmaterialliquid-volumeaccelerationageevent-namemasstemperaturename-humanproduct-typeratecomputermemoryorganizationnumeric-typepressureplacedatethe results of part of speech tagging and name and concept recognition are  merged and the words are grouped into phrases   preference being given to the text units discovered by concept recognition. verb and noun phrases and prepositional phrases are identified. a simple lexicon based stemming algorithm is then applied to the heads of all phrases and provides the citation forms needed to support lookup in the english to ontology lexicon.
patterns are then applied to recognize noun phrase and verb phrase; phrases recognized by the name and measure recognition phase are not merged into noun phrases. in every case a head noun is identified. the head noun or verb is looked up in an english to ontology lexicon. at this point we are ready to match the question against a set of skeletal question structures held in a  question lexicon . this allows the many ways that a question can be specified to all be mapped to a request for the same answer. each entry consists of three parts:
 type of answer needed      additions to retrieval query       question pattern 
where:
 type of answer needed  specifies the ontological type of the answer needed
　　　 additions to retrieval query  specifies ontological concepts that should be mapped to lexical items to be used in the query process
　　　 question pattern  is a pattern containing strings  which should be in the question  ontological types   and kleene stars  which allow matching any unit of question text. there is an implied  *  at the end of every question pattern. currently there are some 1 question patterns in the system. below we show the patterns used to handle questions on temperature.
temperature question patterns
temperature     thermometric-unit       * what * temperature
temperature     thermometric-unit       * how hot
temperature     thermometric-unit       * how cold
temperature     thermometric-unit       * how many degrees
temperature     thermometric-unit       * how high * temperature
temperature     thermometric-unit       * how low * temperature
temperature     thermometric-unit       * what * melting point temperature     thermometric-unit       * what * boiling point
temperature     thermometric-unit       * what * freezing point
temperature     thermometric-unit       * how many * thermometric-unit
temperature is a concept which is an object consisting of a numeric-unit and a thermometric unit. the second element specifies that lexical entries attached to the concept thermometric-unit should be included in the queries generated by the retrieval component of the system.  the first pattern would recognize  at what temperature does tin melt '. the last pattern contains a concept in addition to strings  in lower case. this would match questions such as  how many degrees centigrade is the melting point of tin  .
the question recognition system uses dynamic programming to select the closest matching question pattern. strings are matched with strings in the question  and concepts are matched with the head concepts found for each phrase. if a direct match is not found the concept's parent in the  is-a  hierarchy will also be tried. this information is then passed both to the retrieval system query builder and to the answer extraction system.
retrieval
the query building component of the system was not integrated in time for use in this evaluation . instead the top 1 documents returned by the at&t system  which were provided for the evaluation  were used. a brief description of the eventual operation of the query builder is given here.
our goal is to find a text with a single sentence which specifies the answer in the context of all the constraints of the question. however  the constraints may need to be relaxed  and synonyms generated to allow a matching sentence to be found. the query system also expands the answer indicator concepts using the ontological lexicon. the thermometric-unit will become  centigrade or fahrenheit or kelvin or c or f or k . a boolean retrieval system is used and the initial query attempts to find all the phrases in a single sentence. if this fails then a second retrieval is attempted using head words. a third retrieval is attempted where head words are substituted by their synonms. if all the above fail then the synonym query is retried with the constraint that all the terms are in a paragraph.
the benefits of giving all the terms in a question equal weighting  and of only performing stemming and term expansion in response to the initial query failure  are that texts are obtained where all the information specified is found in a close  context.
document structuring
the retrieved documents undergo the same language processing steps as was carried out on the query. each sentence is part of speech tagged. name recognition is run on  whole documents  which allows much more accurate performance than processing single sentences. phrases are recognized  and heads of phrases are looked up in the english to ontology lexicon.  the resulting structure  for each document  is then passed to the question answering phase.
answer generation
the structured question is used as a template and matched against each sentence in the document. each sentence receives a score for each string and each concept in the question which matches a text unit in the sentence. if no text unit matches the concept required then the sentence is rejected  otherwise the answer string is produced accompanied by a score for the number of question slots filled in producing this answer. a high number of slots gives a high score. once all the documents have been processed all the answers are sorted by score and the top five picked.  in this preliminary system the answer selection process only requires the answer concept and does not specifically check that the expected answer object is present. thus tall would be an acceptable answer for a linear size question. for the trec tasks the answers were expanded on either side up to the maximum allowable number of bytes containing whole words. the initial answers produced by
sample question and answer
the following shows a question  the resulting structure  and the set of answers obtained  all from the same document from the la times.
% how tall is the eiffel tower 
answer-indicator linear-unit np  the eiffel tower   tower 
la1             1 crl  1-foot
la1             1 crl  short
la1             1 crl  1-foot
la1             1 crl  1-foot
     la1             1 crl  too tall performance results were submitted for the 1 byte and the 1 byte tasks.
1 byte responses
1 - no answer
1 - no correct answer
1 - correct answer in 1 responses
mean rank - 1
1 byte responses
1 - no answer
1 - no correct answer
1 - correct answer in 1 responses mean rank - 1
future work
our top priority at the moment is to get retrieval integrated into the system. more  sophisticated algorithms for answer selection are also required. for example not producing as an answer something which was specified in the question; some modicum of syntax will also help in matching sentences to the question template. the question lexicon needs to be expanded to cover more question types.
a web search version will be built to allow the demonstration of the process on non-trec data. other  knowledge sources will be incorporated to handle answers that are unlikely to be explicitly specified in documents  what is the capital of france  . the method is not language independent  but the components used part of speech tagging  phrase recognition  name recognition and an ontological lexicon are already available for spanish and chinese  so the development of  question answering systems for these languages should be possible in a relatively short period of time  1 .
