peer-to-peer systems have emerged as a robust  scalable and decentralized way to share and publish data. in this paper  we propose p-ring  a new p1p index structure that supports both equality and range queries. p-ring is fault-tolerant  provides logarithmic search performance even for highly skewed data distributions and efficiently supports large sets of data items per peer. we experimentally evaluate p-ring using both simulations and a real distributed deployment on planetlab  and we compare its performance with skip graphs  online balancing and chord.
categories and subject descriptors: h.1 physical design: access methods  h.1 systems - distributed databases
general terms: algorithms  management  performance.
keywords: peer-to-peer systems  range queries  load balancing.
1.	introduction
　peer-to-peer  p1p  systems have emerged as a new paradigm for structuring large-scale distributed systems. their key advantages are their scalability  due to resource-sharing among cooperating peers  their fault-tolerance  due to the symmetrical nature of peers  and their robustness  due to self-reorganization after failures. due to the above advantages  p1p systems have made inroads for content distribution and service discovery applications  1  1  1  1 . one of the requirements of such systems is to support range queries. for example  in a large computing grid  where each node advertises its resources  one might need to find all the nodes in the grid with enough main memory for a memory intensive application:  select * from allnodes m where m.memory   1gb .

 research done while at cornell university.
 this material is based upon work supported by the national science foundation under grant 1  by the air force under grant afosr f1-1  and by naval academy under a narc grant. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
copyright 1 association for computing machinery. acm acknowledges that this contribution was authored or co-authored by an employee  contractor or affiliate of the u.s. government. as such  the government retains a nonexclusive  royalty-free right to publish or reproduce this article  or to allow others to do so  for government purposes only.
sigmod'1  june 1  1  beijing  china.
copyright 1 acm 1-1-1/1 ...$1.
　while there have been several promising p1p range index structures that have been proposed in the literature  they have certain limitations. skip graphs  and p-trees  can only handle a single data item per peer  and hence  are not well-suited for large data sets. the index proposed by gupta et al.  only provides approximate answers to range queries  and can miss results. mercury  and p-grid  1  1  provide probabilistic  as opposed to absolute  guarantees on search and load-balancing  even when the p1p system is fully consistent. baton  only provides search performance proportional to log1p  where p is the number of peers; when p is large  the small base of the logarithm can lead to excessive search cost. baton*  provides search performance proportional to logdp  but it does not prove any guarantees on load balancing.
　we propose p-ring  a new p1p range index. p-ring provides exact answers to range queries on arbitrary ordered domains  and scales to a large number of peers and data items. p-ring provides provable guarantees on load-balancing  with load imbalance factor of at most 1  for any given 1   1. p-ring provides search performance of o logdp   where p is the number of peers in the system  and d is a tunable parameter. we are not aware of any other p1p index that provides the same functionality and performance.
　when designing p-ring we were faced with two challenges. first  the data items have to be distributed among peers such that range queries can be answered efficiently  while still ensuring that all peers have roughly the same number of data items  for load balance . techniques developed for equality queries are not applicable as they distribute data items based on their hash value; since hashing destroys the order of the items  range queries cannot be answered efficiently. we need to devise a scheme that clusters data items by their data value  and balances the number of items per peer  even in the presence of highly skewed insertions and dele-
tions. our first contribution is a scheme that provably maintains a load imbalance of at most 1  for any given 1  1  between any two peers in the system  while achieving amortized constant cost per insertion and deletion. this achieves a better load balance factor when compared to that of 1 proposed by ganesan et al.   while keeping the amortized insert/delete cost constant.
　our second challenge was to devise a query router that is robust to peer failures and provides logarithmic search performance even in the presence of skewed data distributions. our p-ring router  called hierarchical ring  hr   is highly fault-tolerant  and a router of order d provides guaranteed o logdp + m  range search performance in a stable system with p peers  where m is the number of peers with data items in the query range. even in the presence of highly skewed insertions  we can guarantee a worst-case search
p1p index	finditems predicate 
insertitem item  deleteitem item 

content router sendreceive msg  predicate 
replication managerdata storeinsertitems itemslist  deleteitems itemslist fault tolerant ringgetsuccessor   joinring knownpeer 
leavering  figure 1: p1p framework
cost of o r ， d ， logdp + m   where r is the number of peer insertions per stabilization unit of the router  we will formally define all terms later .
　in a simulation study  we compare the performance of p-ring to an extension of skip graphs  and to chord. our performance results indicate that p-ring outperforms the above extension of skip graphs in terms of both query and update cost. p-ring offers the same  if order d of hr is 1  or better  if d   1  search performance than chord  but at a higher cost  due to the support of additional functionality  range queries as opposed to only equality queries . we also present preliminary experimental results from a real distributed implementation of p-ring  chord and online balancing  deployed on planetlab   a network of computers distributed around the world.
1.	model and architecture
system model. we assume that each peer in the system can be identified by an address  ip address and port number   and peers can communicate through messages. a peer can join a p1p system by contacting some peer that is already part of the system. a peer can leave the system at any time without contacting any other peer. we assume that while in the system  the peers follow the distributed indexing protocol. this assumption is consistent with other papers in the literature  1  1  1 . in this paper we use p to denote the number of peers in the system.
　we assume that each data item  or short  item  stored in a peer exposes a search key value from a totally ordered domain that is indexed by the system. without loss of generality  we assume that search key values are unique. duplicate values can transparently be made unique by appending the address of the peer where the value originates and a version number. we use n to denote the number of items in the system.
　for simplicity  we assume that the query distribution is uniform  so the load of a peer is determined by the number of data items stored at the peer. the algorithms introduced in this paper work with any definition of the load.
　we define the load imbalance in a system to be the ratio between the most loaded and the least loaded peer in the system. system architecture. we have implemented p-ring in an architecture similar to the modular framework of . we now overview the relevant components of the framework  figure 1 .
fault tolerant ring: the fault tolerant ring connects the peers in the system along a ring  and provides reliable connectivity among these peers even in the face of peer failures. for a peer p  we can define the succ p   respectively  pred p   to be the peer
p1

figure 1: fault tolerant ring
adjacent to p in a clockwise  resp.  counter-clockwise  traversal of the ring. figure 1 shows an example of a fault tolerant ring. if peer p1 fails  the ring will reorganize such that succ p1  = p1  so the peers remain connected. figure 1 shows the ring api. the ring provides methods to get the address of the successor  join the ring or gracefully leave the ring  of course  a peer can leave the ring without calling leavering due to a failure . in our implementation of p-ring  we use chord's fault tolerant ring .
data store: the data store is responsible for distributing the items to peers. ideally  the distribution should be uniform so that each peer stores about the same number of items  achieving storage balance. one of the main contributions of this paper is a new data store for p-ring  which can effectively distribute items even under skewed insertions and deletions  see section 1 .
content router: the content router is responsible for efficiently routing messages to peers that have items satisfying a given predicate. the second major contribution of this paper is a new content router that can route range queries efficiently  see section 1 . replication manager: the replication manager ensures that items assigned to a peer are not lost if that peer fails. we use the replication manager proposed in cfs .
p1p index: the p1p index is the index exposed to the end user. it supports search functionality by using the functionality of the content router  and supports item insertion and deletion by using the functionality of the data store.
1.	p-ring data store
　the main challenge in devising a data store for p1p range indices is handling data skew. we would like the items to be uniformly distributed among the peers so that the load is nearly evenly distributed among the peers. most existing p1p indices achieve this goal by hashing. items are assigned to peers based on the hash value of their search key. such an assignment has been shown to be very close to a uniform distribution with high probability . however  hashing destroys the value ordering among the search key values  and thus cannot be used to process range queries efficiently  for the same reason that hash indices cannot be used to handle range queries efficiently .
　for p-ring to support range queries  we assign items to peers directly based on their search key value. in this case  the ring ordering is the same as the search key value ordering  wrapped around the highest value. the problem is that now  even in a stable p1p system with no peers joining or leaving  some peers might become overloaded due to skewed data insertions and/or deletions. we need a way to dynamically reassign and maintain the ranges associated to the peers. this section presents our algorithms for handling data skew. all our algorithms guarantee correctness in face of concurrent operations  as we can apply the techniques introduced by linga et al. .
1	handling data skew
　the search key space is ordered on a ring  wrapping around the highest value. the data store partitions this ring space into ranges and assigns each of these ranges to a different peer. let p.range =  p.lb p.ub  denote the range assigned to p. all items in the system with search key in p.range are said to be owned by p. let p.own denote the list of all these items. let |p.own| denote the number of items in p.own and hence in p.range. the number of ranges is less than the total number of peers in the system and hence there are some peers which are not assigned any range. such peers are called helper peers. the others are called owner peers. let p denote the set of all peers  and let o be the subset of owner peers in p. using these notations  the load imbalance is defined as. in this section  we present algorithms to maintain the load imbalance at not more than two.
　analogous to b+-tree leaf page maintenance  the number of items in every range is maintained between bounds ` = sf and u = 1 ， sf  where sf is the  storage factor   a parameter we will talk more about in section 1. whenever the number of items in p's data store becomes larger than u  due to many insertions into p.range   we say that an overflow occurred. in this case  p tries to split its assigned range  and implicitly its items  with a helper peer. whenever the number of items in p's data store becomes smaller than ` = sf  due to deletions from p.range   we say that an underflow occurred. peer p tries to acquire a larger range and more items from its successor in the ring. in this case  the successor either redistributes its items with p  or gives up its entire range to p and becomes a helper peer.
　example consider the data store in figure 1 which shows the helper peers p1 and p1  and the ranges and search key values of items assigned to the other peers in the system  range  1  with items with search keys 1 and 1 are assigned to peer p1 etc. . assume that sf is 1  so each peer in the ring can have 1 or 1 items. when an item with search key 1 is inserted into the system  it will be stored at p1  leading to an overflow. as shown in figure 1  the range  1  is split between p1 and the helper peer p1. p1 becomes the successor of p1 on the ring and p1 is assigned the range  1  with item with search key 1.
　split algorithm 1 shows the pseudo-code of the split algorithm executed by a peer p that overflows. we use the notation p::fn   when function fn   is invoked at peer p  and p.ringnode refers to the fault tolerant ring component of the p-ring at peer p. during a split  peer p tries to find a helper peer p1  see section 1  and transfer half of its items  and the corresponding range  to p1. after p1 is found  line 1   half of the items are removed from p.own and p.range is split accordingly. peer p then invites peer p1 to join the ring as its successor and maintain p1.range. the main steps of the algorithm executed by the helper peer p1 are shown in algorithm 1. using the information received from p  p1 initializes its data store component  the ring component and the other index components above the data store.
　merge and redistribution if there is an underflow at peer p  p executes the merge algorithm given in algorithm 1. peer p invokes the initiatemergemsghandler function on its successor on the ring. the successor sends back the action decided  merge or redistribute  a new range newrange and the list of items newitemslist that are to be re-assigned to p  line 1 . p appends newrange to p.range and newitemslist to p.own.
　the outline of the initiatemergemsghandler function is given in algorithm 1. the invoked peer  p1 = succ p   checks whether a redistribution of items is possible between the two  siblings   line 1 . if yes  it sends some of its items and the correspondalgorithm 1 : p.split  
1: p1 = gethelperpeer  ;
1: if p1 == null then
1:	return;
1: end if
1: //execute the split
1: splititems = p.own.splitsecondhalf  ;
1: splitv alue = p.own.lastvalue  ;
1: splitrange = p.range.splitlast splitv alue ;
1: p1::joinringmsghandler p splititems splitrange ;

algorithm 1 :	p1.joinringmsghandler p  splititems  splitrange 

1: p1.range = splitrange;
1: p1.own = splititems;
1: p1.ringnode.joinring p ;
algorithm 1 : p.merge  
1: //send message to successor and wait for result
1:  action newrange newitemslist 	=
p.ringnode.getsuccessor  ::
initiatemergemsghandler p  |p.own| ;
1: p.own.add newitemslist ;
1: p.range.add newrange ;
algorithm	1	:	 action newrange newitemslist  p1.initiatemergemsghandler p numitems 
1: if numitems + |p1.own|   1 ， sf then
1:	//redistribute
1:	compute nbitemstogive;
1:	splititems = p1.own.splitfirst nbitemstogive ;
1:	splitv alue = splititems.lastvalue  ;
1:	splitrange = p1.range.splitfirst splitv alue ;
1:	return  redistribute splitrange splititems ;
1: else
1:	//merge and leave the ring
1:	splititems = p1.own;
1:	splitrange = p1.range;
1:	p1.ringnode.leavering  ;
1:	return  merge  splitrange  splititems ;
1: end if

ing range to p. if a redistribution is not possible  p1 gives up all its items and its range to p  and becomes a helper peer.
　example. consider again figure 1 and assume that item with search key value 1 is deleted. now there is an underflow at peer p1 and peer p1 calls initiatemergemsghandler in p1. since p1 has only one item  redistribution is not possible. peer p1 sends its item to p1 and becomes a helper peer  with no range to own. as shown in figure 1  peer p1 now owns the whole range  1 .
1	managing helper peers
we first discuss the pros and cons of using helper peers.
　the main advantage of using helper peers is the decrease in cost of re-balancing operations. ganesan et al. showed in  that any efficient load-balancing algorithm that guarantees a constant imbalance ratio  as our algorithm does  needs to use re-order operations. a highly loaded peer finds a lightly loaded peer that can give its load to some neighbor peer  and take over some of the load of the
	p1	p1	p1	p1	1	p1figure 1. data store  ds figure 1. ds after splitfigure 1. ds after merge	p1	p1	p	p1	p1
p1
	p1
1
p1
p1
p1p1
p1
1highly loaded peer. in all of the previous approaches to load balancing that we are aware of  1  1  1   the lightly loaded peer is already part of the ring  so it needs to leave the ring before joining it in a new place. leaving the ring is an expensive operation: new neighbors are established in the ring  the items of the peer are sent to the neighbor s   more replicas are created to compensate for the loss of the replicas stored at the leaving peer  and finally  the routing structure adjusts for the change. by using helper peers that are not part of the ring  all these costs are eliminated  leading to more efficient and faster load balancing.
　using helper peers might seem to contradict the symmetry of the p1p systems. however  the number of helper peers is usually small  and they change over time  due to re-balancing. moreover  section 1 introduces a scheme that uses the helper peers for load balancing among all peers.
　now  let us see how we manage the helper peers. recall that helper peers are used during split and are generated during merge. there are three important issues to be addressed. first  we need a reliable way of  storing  and finding helper peers. second  we need to ensure that a helper peer exists when it is needed during split. finally  even though helper peers do not have a position on the ring  they should be able to query the data in the system.
　to solve the first issue  we create an artificial item  〕 p1.address  for every helper peer p1  where 〕 is the smallest possible search key value. this item is inserted into the system like any regular item. when a helper peer is needed  an equality search for 〕 is issued. the search is processed as a regular query and the result is returned to the requester. as there are much fewer helper peers than regular items  managing helper peers does not significantly increase the load on the peers storing them. using the hr router from section 1  the cost of inserting  deleting  or finding a helper peer is o logdp .
to ensure that a helper peer exists when an overflow occurs  we
. the number of items n and the number of peers p can be dynamically estimated by each peer at no additional message cost  see section 1 .
　finally  each helper peer maintains a list of owner peers to which it can forward any query to be processed.
1	load balancing using helper peers
　the load-balancing scheme proposed maintains the number of items stored by each owner peer within strict bounds. however  the helper peers do not store any items  so there is no  true  load balance among peers. we propose now an extension to the basic scheme  which uses the helper peers to truly  balance the load. the extended scheme is provably efficient  i.e.  every insert and delete of an item has an amortized constant cost. also the load imbalance is bounded by a small constant.
observe that if we somehow assign items to helper peers too while maintaining the bounds ` and u on the number of items assigned  we are able to bound the load imbalance by u` . we therefore extend the functionality of helper peers. every helper peer is obliged to  help  an owner peer already on the ring. a helper peer helps an owner peer by managing some part of the owner peers range and hence some of its load. if owner peer p has k helpers q1 q1 ... qk  p.range =  lb ub  is divided into  lb = b1 b1   b1 b1  ...  bk ub  such that each sub-range has equal number of items. peer p is now responsible for  bk ub . each of p's helpers  qj  becomes responsible for one of the other ranges  say  bj 1 bj . let q.resp be the list of items peer q is responsible for and q.rangeresp be the corresponding range. q participates in routing and all queries dealing with q.rangeresp will reach q. however  p still owns all the items in  lb ub  and is responsible for initiating the load balance operations. also  any insert or delete that reaches a helper peer is forwarded to the owner peer  who will ensure that the items own are evenly divided among itself and the helpers. in this context  the definition of the load imbalance becomes . in this section  we provide algorithms to maintain the load imbalance at not more than 1 + 1  for any 1   1. the extended scheme  extloadbalance  is very similar to the basic scheme. in the split algorithm algorithm 1  only step 1 changes  and a new step is added. if p already has helper peers  then p uses one of the helpers q to perform the split. the helper peer q will become an owner peer. else  p issues a search for a helper peer. during the split  not only the range and items of p are split  but also the helper peers of p. at the end of the split  p redistributes the new p.own with its new reduced set of helpers. in the merge algorithm  algorithm 1  p contacts the first successor on the ring who is an owner peer  p1. if the decision is to merge  all the data items are moved from p to p1 and p and all the associated helpers leave the ring and start helping some other randomly chosen peers. if the decision is to redistribute  the items are moved from p1.own to p.own. the items in p1.own and respectively p.own are re-distributed among the helper peers of p1 and respectively p.
　to bound the load imbalance close to   extloadbalance has an additional load balancing operation called usurp. algorithm 1 shows the pseudo-code of the usurp algorithm executed by an owner peer p. during usurp  an owner peer p can usurp  or take ove〔r  a helper peer q of another owner peer p1  if |p.resp| − 1 + δ|q.resp|  for a given constant δ   1. the helper peer q starts helping p  so |p.resp| is reduced. the getleastloadedhelperpeer   function can be implemented by using the hr  section 1  to maintain information about the least loaded peer.
　our algorithms will bound the load imbalance in the system by a small constant  1   at an amortized constant cost for item insertions and deletions. we can prove the following:
algorithm 1 : p.usurp  
1: //find least loaded helper peer and its  master 
1:  q p1  = getlea〔stloadedhelperpeer  ;
1: if |p.resp| − 1 + δ|q.resp| then
1:	p.sethelperpeer q ;
1:	redistribute p.own among new set of helpers;
1:	redistribute p1.own among new set of helpers;
1: end if

　theorem 1. consider an initial load balanced system. for every sequence σ of item inserts and deletes  and constants ` u 1 such that u` − 1 and 1   1 
  load imbalance: the sequence of split  merge and usurp operations performed by algorithm extloadbalance is such that after the completion of any prefix of σ  the current partition of ranges  the assignment of ranges to owner peers and assignment of helper peers to owner peers satisfy
1. ` ＋ |p.own| ＋ u for all owner peers p （ o;
1. load imbalance = .
  efficiency: if u` − 1 + 1  the above sequence of split  merge and usurp operations is such that the amortized cost of an insert or a delete operation is a constant.
proof. full proofs for all theorems are given in .
　we sketch now the proof for bound on the load imbalance. since u − 1`  it is easy to see that during the course of the algorithm extloadbalance  the split and the merge operations bound the size of p.own within ` and u. unlike p.own  we can bound ` ＋ |p.resp| ＋ u  only for all p （ o with no helper peers. an owner peer p with |p.own| = ` could have helpers making |p.resp|   `. however  thanks to usurp〔 operations  there cannot exists a q （ p such that |q.resp|   1  1 + δ  ， |p.resp|. by setting δ =  1 + 1 1   1  we get the required bound. 
　we now sketch the proof for the efficiency result. our cost model is very similar to the one used in ganesan et al. . the only cost we consider is the cost of moving items due to the load balancing operations. there are three major components to the cost: a  data movement: we model this cost as being linear in the number of items moved from one peer to the other. b  distributing items amongst helper peers  whenever the set of items owned by a peer p or the set of helpers h p  changes: we use |p.own| as a very conservative estimate of the number of items moved in this case.
c  load information: our algorithm requires non-local information about the least loaded helper peer. we assume that this cost can be included in the data movement cost.
　let ` = sf and u =  1 + 1 sf for some 1   1. recall that sf = dn/pe. to prove the amortized constant cost of insert and delete we use the potential function Φ = Φo +Φr  where Φo is the ownership potential and Φr is the responsibility potential defined
as follows:p
	Φo =	p（p φo p   where
 helper peer 
sf
sf sf p
　Φr = q（p φr q   where  and constants co and cr will be defined later.
　we show that the increase in the potential Φ due to an insert or delete is bounded by a constant  and the  maximum  cost of a rebalancing operation is smaller than the  minimum  decrease in the potential Φ due to re-balancing. these facts prove that the amortized cost of an insert or delete operation is constant.
insert: during insert operation  an item is inserted into p.own for some p and inserted into q.resp for some q （ h p  “ {p}. φr q  increases  while φo p  increases if u1 ＋ |p.own| ＋  1 + 1 sf  and decreases if sf ＋ |p.own| ＋ l1. the maximum increase in Φ occurs when both φr q  and φo p  increase and this increase is

	sf	1
＋ co  1sf c 1 co sf 1
sf
		 1 
delete: during delete operation  an item is deleted from p.own for some peer p and deleted from q.resp for some q （ h p  “ {p}. analogous to the insert case  we can show that the maximum increase in potential to be
		 1 
　we showed in 1 and 1 that the increase in the potential Φ due to an insert or delete is bounded by a constant. we show now that the maximum cost of a re-balancing operation  split  merge or redistribute  and usurp  is smaller than the minimum decrease in the potential Φ due to that re-balancing operation. split: first let us look at the decrease in the ownership potential
  splitΦo. during a split  a peer p owning |p.own| =  1 + 1 sf items  gives half of its items to a helper peer q. after the split  both sf items. hence  the final ownership potentials of p and q are 1. also  the initial ownership potential of q is 1 since before the split q was not an owner peer.
sf
　next  consider the change in the responsibility potential. when h p  1=    q is chosen from h p  and the helper peers are distributed amongst p and q evenly. in this case  the responsibilities change only when the number of helpers apart from q  i.e.  |h p    {q}|  is odd  say 1h + 1. this is because the sf items in p and q are distributed amongst h + 1 and h + 1 peers respectively. in this case the decrease in Φr would be

sf
sf
when h p  =    p splits its items with a peer q  where q （
h p1  for some p1. let h1 = |h p1 |  l1 = |p1.own|. we have 
!
!

cr
	−	
sf
	−	1
hence the minimum decrease in the potential due to a split is
.
　now  we show that the cost of a split operation is at most  1 + sf. in the case when p  the splitting peer  has a helper q  the cost is contributed by the transfer of sf items and the redistribution of items amongst p's and q's helpers. when p does not have any helpers  p splits with the helper q of some other owner peer p1. here the cost involves transfer of items from p to q and the re-distribution of items amongst p1's remaining helpers.
　in order to have the cost of split lower than the decrease in the potential due to split  we need co and cr such that
	sf  sf	 1 
redistribute: analogous to the change in potential due to split  we can prove that the minimum decrease in potential due to redistribute is	crsf.
　the cost of the redistribute operation  is at most sf. the cost involves transfer of sf items and the redistribution of the final set of items owned by the two peers involved in the transfer amongst their helpers.
the cost-potential equation for a redistribute operation becomes:
	crsfsf	 1 
 merge: again analogous to the split case  we can prove that the minimum decrease in potential due to merge is  mergeΦ − sf
　the cost of a merge is at most sf  since the cost only involves transfer of sf items to the more loaded peer and redistribution of at most sf items amongst the new set of helper peers.
the cost-potential equation for a merge operation becomes:
	sf	 1 
usurp: we can prove that the minimum decrease in potential due to an usurp operation issf  where κh =  1 + 〔
1  1 + 1  1 + δ   1   κh is the maximum number of helper peers assigned to an owner peer .
　the usurp operation costs `1 + `1 ＋ 1 + 1 sf  where the two non free peers involved own `1 and `1 items respectively. the cost arises due to the redistribution amongst the new set of helpers. the cost-potential equation for an usurp operation becomes:
	sf	 1 
solving equations 1  1  1  1  we get

by setting the constants cr and co to values as shown above  we can prove that the amortized cost of inserts and deletes is a constant when sf does not change. the proof for the amortized constant cost for insert/delete in the case where sf does change due to the change in the number of items in the system  is omitted here due to space constraints. 
1.	p-ring content router
　the goal of our content router is to efficiently route messages to peers in a given range. the main challenge is to handle skewed distributions. since the search keys can be skewed  the peer ranges may not be of equal length.
　we devise a new content router called hierarchical ring  or short  hr  that can handle highly skewed distributions. in this section we describe the content router  the routing algorithm and the maintenance algorithms. we then give analytical bounds for the search performance in a stable system and under heavily skewed insertion patterns.
1	hierarchical ring
　the hr is based on the simple idea of constructing a hierarchy of rings. let d be an integer   1  called the order of hr. at the lowest level  level 1  each peer p maintains a list of the first d successors on the ring. using the successors  a message could always be forwarded to the last successor in the list that does not overshoot the target   skipping  up to d-1 peers at a time. for instance  figure 1 shows a hierarchy of rings with order  d  1. as shown  peer p1 is responsible for the range  1   p1 is responsible for  1  and so on. each peer knows its successor on the ring: succ p1  = p1  succ p1  = p1  and so on. at level 1 in the
1
	figure 1. hr level 1	figure 1. hr levels 1  1  and 1
hr  each peer maintains a list of 1 successors  as shown. suppose p1 needs to route a message to a peer with value 1. p1 will route the message to p1 and p1 will forward the message to p1  the final destination.
　at level 1  we again maintain a list of d successors. however  a successor at level 1 corresponds to the dth successor at level 1. using these successors  a message can always be routed to the last successor in the list that does not overshoot the target   skipping  up to d1   1 peers at a time. figure 1 shows the content of level 1 nodes at each peer in the ring. if p1 needs to route a message to a peer with value 1  p1 will route the message directly to p1  the final destination   using the list at level 1. the procedure of defining the successor at level l+1 and creating a list of level l+1 successors is iterated until no more levels can be created. in figure 1  for peer p1  succ1 p1  = p1  which overshoots p1  so no more levels can be constructed for p1.
　note that we are conceptually indexing positions in the ring  i.e. at level l  a peer p has pointers to peers that are dl peers away  instead of values  which allows hr to perform well  regardless of the data distribution.
　formally  the data structure for a hr of order d is a doubly indexed array node level  position   where 1 ＋ level ＋ numlevels and 1 ＋ position ＋ d. the hr is defined to be consistent if and only if at each peer p:
  p.node = succ p 
  p.node j + 1  = succ p.node j    1 ＋ j   d
  p.node l + 1  = p.node l  d  
  p.node l + 1  j + 1  = p.node l + 1  j .node l + 1   1 ＋ l   numlevels  1 ＋ j   d
  the successor at numlevels of the last peer in the list at numlevels level  wraps  around  so all the peers are indeed indexed:
p.node numlevels .lastpeer.node numlevels 	（  p p.node numlevels .lastpeer 
from this definition  it is easy to see that a consistent hr of order d  has at most dlogd pe levels  and the space requirement for the hr at each peer is o d ， logd p .
1	maintenance
　peer failures and insertions  as well as splits and merges at the data store level  perceived as peer insertions  and respectively departures  at the content router level   disrupt the consistency of the hr. we have a remarkably simple stabilization process that runs periodically at each peer and repairs the inconsistencies in the hr. the algorithm guarantees that the hr structure eventually becomes fully consistent after any pattern of concurrent insertions and deletions  as long as the peers remain connected at the ring level.

1: i = 1;
1: repeat
1:	root=p.stabilizelevel i ;
1:	i + +;
1: until  root 
1: p.numlevels = i   1;
algorithm 1 : p.stabilizelevel int i 
1: succentry = p.node i ;
1: p.node i  = succentry.node i ;
1: insert i succentry ;
1: if p.node i .lastpeer.node i  （
 p p.node i .lastpeer  then
1:	return true
1: else
1:	p.node i + 1  = p.node i  d ;
1:	return false;
1: end if

　the stabilization process is important for the performance of the queries  but not for their correctness. as long as the peers are connected at the ring level  queries can be processed by forwarding them along the successor pointers. the stabilization process fixes the inconsistencies in the hr in order to provide logarithmic search performance for queries. we chose to have a periodic stabilization process that repairs the inconsistencies in the hr over performing reactive repairs  as the latter can lead to high maintenance costs in case of high churn . using a periodic stabilization mechanism is similar to most other p1p index structures  1  1  1  1 .
　the algorithm executed periodically by the stabilization process is shown in algorithm 1. the algorithm loops from the lowest level to the top-most level of the hr until the highest  root  level is reached  as indicated by the boolean variable root . since the height of the hr data structure could actually change  we update the height  p.numlevels  at the end of the function.
　algorithm 1 describes the stabilization process within each level of the hr structure at a peer. the key observation is that each peer needs only local information to compute its own successor at each level. thus  each peer relies on other peers to repair their own successor at each level. when a peer p stabilizes a level  it contacts its successor at that level and asks for its entries at the corresponding level. peer p replaces its own entries with the received entries and inserts its successor as the first entry in the index node  lines 1 and 1 . the insert procedure inserts the specified entry at the beginning of the list at given level  and it ensures that no more than d entries are in the list and none of the entries in the list overshoots p  if the list does wraps around  this should be the last level . line 1 checks whether this level should be the last level in the hr. this is the case if all the peers in the system are already covered. if this level is not the root level  the stabilization procedure computes the successor at the higher level  line 1  and returns.
　the periodic stabilization process runs independently at each peer  without the need for synchronization. regardless of the order in which the stabilization process is run at different peers  stabilization of some level i in the hr structure at some peers might occur before the stabilization at level i 1 at some other peers   the stabilization process will move the hr structure towards a more consistent state. eventually  the entire hr becomes consistent  as shown in theorem 1.
definition we define a stabilization unit  su  to be the time needed to run the stabilizelevel procedure at some level in all peers.
　theorem 1  stabilization time . given that at time t there are p peers in the system  the fault tolerant ring is connected and the successor pointers are correct  and the stabilization procedure starts running periodically at each peer  at time t +  d   1 dlogd pesu the hr is consistent with respect to the p peers  if no peers fail.
　due to space constraints  we omit the proof here; full proofs for all theorems are given in the technical report .
1	storage factor estimation
　the algorithms in section 1 have one parameter: the storage factor sf or `  the required minimum number of items stored by a peer. sf depends on  number of items and p = number of peers . each peer estimates n and p as follows. each entry p1 in the hr at a peer p stores two additional counters to estimate the number of peers and the number of items in the range  p p1 . these counters are aggregated bottom-up and the highest-level values are used as estimates for n and p. maintaining the counters does not increase the number of messages in the system  as we piggyback the numbers on the hr stabilization messages. our experiments show that p-ring achieves a load imbalance of approximately two  even in a dynamic system  which proves that the estimated sf is accurate.
1	routing
　the content router component supports the sendreceive msg range  primitive. we assume that each routing request originates at some peer p in the p1p system. for simplicity of presentation  we assume that the range has the form  lb ub .
　the routing procedure shown in algorithm 1 takes as input the lower-bound  lb  and the upper-bound  ub  of the requested range  the message to be routed  and the address of the peer where the request originated. rangemin p  denotes the low end value of p.range  and p.node i  j .iv alue and p.node i  j .peer denote the value  and respectively the address of the peer stored in the hr entry p.node i  j   we used p.node i  j .iv alue = rangemin p.node i  j .peer  . each peer selects the farthest away pointer that does not overshoot lb and forwards the request to that peer. once the algorithm reaches the lowest level of the hr  it traverses the successor list until the value of a peer exceeds ub  lines 1 . note that every peer which is responsible for a part of  lb ub  is visited during the traversal along the ring. at the end of the range scan  a routingdonemessage is sent to the originator  line 1 .
　example: consider a routing request for the range  1  that is issued at peer p1 in figure 1. the routing algorithm first determines the highest hr level in p1 that contains an entry whose value is between 1  value of p1  and 1  the lower bound of the range query . in the current example  this corresponds to the first entry at the second level of p1's hr nodes  which points to peer p1 with value 1. the routing request is hence forwarded to p1. p1 follows a similar protocol  and forwards the request to p1  which appears as the first entry in the first level in p1's hr nodes . since p1 is responsible for items that fall within the required range  p1 processes the routed message and returns the results to the originator p1  line 1 . since the successor of p1  p1  might store items in the  1  range  the request is also forwarded to p1. p1 processes the request and sends the results to p1. the search terminates at p1 as the value of its successor  1  does not fall within the query range.
algorithm 1 : p.routehandler lb  up  msg  originator 
1: // find maximum level that contains an 1: // entry that does not overshoot lb.
1: find the maximum level l such that   j   1 such that p.node l  j .iv alue （  rangemin p  lb .
1: if no such level exists then
1:	//handle the message and send the reply
1:	send p.handlemessage msg   originator ;
1:	if rangemin succ p   （  rangemin p  ub  then
1:	// if successor satisfies search criterion
1:	send route lb ub msg originator requesttype   succ p  ;
1:	else
1:	send routingdonemessage originator ;
1:	end if
1: else
1:	find maximum k such that
p.node l  k .iv alue （  rangemin p  lb ;
1:	send route  lb ub msg originator  
p.node l  k .peer  ;
1: end if

　in a consistent state  the routing will go down one level in the hr every time a routing message is forwarded in line 1. this guarantees that we need at most dlogd pe steps to find lb  if the hr is consistent. if the hr is inconsistent  the routing cost may be more than dlogd pe. even if the hr is inconsistent  it can still route requests by using the entries to the maximum extent possible  and then sequentially scanning along the ring. in section 1  we show that the search performance of hr does not degrade much even when the index is temporarily inconsistent.
　it is important to note that in a p1p system we cannot guarantee that every route request terminates. for example  a peer p could crash in the middle of processing a request  in which case the originator of the request would have to time out and try the routing request again. this model is similar to that used in most other p1p systems.  1  1  1 .
　we can formally prove the following properties of routing in hierarchical ring.
　theorem 1  search performance in stable state . in a stable system of p peers with a consistent hr structure of order d  range queries take at most dlogd pe + m hops  where m is the number of peers in the requested range.
　proof. from the definition of hr data structure  a consistent hr of order d for p peers has dlogd pe levels. in a consistent hr  the routing procedure goes down one level every time a routing message is forwarded to another peer. the peer with the lowest value in the requested range is found once the lowest level is reached. after the first answer is found  all the other answers are found by following the successor links. this ensures that the maximum number of hops needed to answer a range query in a stable hr is dlogd pe + m  where m is the number of peers in the requested range. 
　theorem 1  search performance during insertions . if we have a stable system with a consistent hr of order d and we start inserting peers at the rate r peers/stabilization unit  range queries take at most dlogd pe + 1r d   1 dlogd pe + m hops  where p is the current number of peers in the system  and m is the number of peers in the requested range.
　proof. let t1 be the initial time and p1 be the number of peers in the system at time t1. for every i   1 we define ti to be ti 1 +  d   1 dlogd pi 1 e ， su and pi to be the number of peers in the system at time ti. we call an old peer to be a peer that can be reached in at most dlogd pe hops using the hr. if a peer is not old  we call it new. at any time point  the worst case search cost for equality queries is dlogd pe + x  where dlogd pe is the maximum number of hops using the hr to find an old peer and x is the number of new peers. x is also the maximum number of hops to be executed using the successor pointers to find any one of the new x peers  the worst case is when all new peers are successors in the ring .
　we show by induction on time that the number of new peers in the system at any time is at most 1r d 1 dlogd pe  which proves the theorem.
　as the base induction step we prove that at any time point in the interval  t1 t1  there are no more than 1r d 1 dlogd pe new peers and at time t1 there are no more than rddlogd pe new peers. from hypothesis  at t1 the hr is consistent  so there are no new peers. at the insertion rate of r peers/su  at any time point in  t1 t1   the maximum number of peers inserted is r d   1 dlogd p1 e  which is smaller than r d   1 dlogd pe. this proves both statements of the base induction step.
　we prove now that if the maximum number of new peers at time ti is rddlogd pe  than  at any time point in  ti ti+1  the maximum number of new peers is 1r d   1 dlogd pe and the maximum number of new peers at time ti+1 is r d 1 dlogd pe  where i − 1. the maximum number of peers inserted between ti and ti+1 is r d   1 dlogd pi e which is smaller than r d   1 dlogd pe. from the induction hypothesis  at time ti there were at most r d   1 dlogd pe new peers. between ti and ti+1  some old peers can become new and new peers can become old  due to changes in the hr structure. however  the total number of entries in the hr structure does not decrease  so the number of old peers becoming new cannot be higher than the number of new peers becoming old. out of the peers in the system at time ti  at most r d 1 dlogd pe of them are new at any time between ti and ti+1. adding the peers inserted since ti we get that at any time point in  ti ti+1  the maximum number of new peers is 1r d 1 dlogd pe. from theorem 1  at time ti+1  all the peers existing in the system at time ti are integrated into the hr structure. this means that all peers existing at time ti are/became old peers at time ti+1  which leaves the maximum number of new peers at time ti+1 to be at most r d   1 dlogd pe  the peers inserted between ti and ti+1 .
　from induction it follows that at any time  the maximum number of new peers is no more than 1r d   1 dlogd pe  which means that equality queries take at most dlogd pe + 1r d   1 dlogd pe hops. 
1.	related work
　most of the indexing techniques developed for distributed databases  e.g.   1  1  1   are not designed for highly dynamic peers and therefore are not appropriate for a p1p environment.
　can   chord   pastry  and tapestry  implement distributed hash tables to provide efficient lookup of a given key value. since a hash function destroys the ordering in the key value space  these structures cannot process range queries efficiently.
　gupta et al.  present a technique for computing range queries using order-preserving hash functions. this system provides approximate answers to range queries  as opposed to the exact answers provided by p-ring. the performance of the system proposed by daskos et al.  depends on certain heuristics for insertion  and does not offer any performance guarantees. sahin et al.  propose a caching scheme for queries  but no performance guarantees are provided for range queries which were not previously asked.
　skip graphs  are a randomized structure based on skip lists. ptree  is a p1p index structure based on the b+ trees. skip graphs and p-tree support routing of range queries  but  as opposed to p-ring  they do not support multiple items per peer. online balancing  is a load balancing scheme for distributing items to peers with a provable bound of 1 for load imbalance with constant amortized insertion and deletion cost. the p-ring data store achieves a better load balance with a factor of 1 + 1  while keeping the amortized insert/delete cost constant. additionally  we also propose a new content router  the hierarchical ring. mercury  is a randomized index structure determined by a sampling mechanism. p-grid  1  1  is a randomized trie-based index. unlike p-ring  mercury and p-grid provide only probabilistic guarantees even when the index is fully consistent. baton  is a binary balanced tree with nodes distributed to peers in a p1p network. the p-ring content router is more flexible  by allowing the application to choose higher values for d  the order of the hr  and thus to decrease the search cost  and the p-ring data store provides provable guarantees on the load balance. baton*  is extension of baton  that provides search performance proportional to logdp  but does not prove any guarantees on load balancing.
1.	experimental evaluation
　we evaluate our system both using a simulation and a real implementation running on planetlab. we focus on two main aspects. first  we evaluate the performance of the p-ring data store. as a baseline  we compare it with the hash-based chord data store. in planetlab  we also compare it with online balancing . second  we evaluate the performance of the p-ring content router  and compare it with skip graphs and chord. we also consider the interaction between the two components in the presence of peer insertions and deletions  system  churn  . in all experiments  all the components of the index  fault-tolerant ring  data store  replication  content router  are implemented and working  but we are only measuring the metrics of interest for the particular experiment.
1	simulation setup
　we developed a simulator in c++ to evaluate the index structures. we implemented the p-ring data store  section 1   hierarchical ring  section 1   skip graphs   and chord . since skip graphs was originally designed for only a single item per peer  we extended it to use the p-ring data store so that it could scale to multiple items per peer. for all the approaches  we implemented the same fault tolerant ring  and replication manager .
　we use three performance metrics: 1. index message cost - the average number of messages per minute  1 simulator time units  required for maintaining the index; 1. index bandwidth cost - the average number of bytes per minute required for maintaining the index; 1. search cost - the number of messages required to evaluate a range query  averaged over 1 random searches. since the main variable component in the cost of range queries is finding the item with the smallest qualifying value  retrieving the other values has a fixed cost of traversing the relevant successor peers   we only report that cost. this also enables us to compare against chord.
　we varied the following parameters: insertionrate  similarly  deletionrate  is the rate of item insertions  deletions  into the system  default is 1 operations per second . iteminsertionpattern  similarly  itemdeletionpattern   specifies the skew in the values inserted  deleted  into the system. a value of ip for this parameter means that all insertions are localized within a fraction ip of the search key space  default is 1 . numpeers is the number of peers

in the system  default is 1 . for each experiment we vary one parameter and use the default values for the rest. we first evaluate the data store and content router components in a stable system configuration  without peer joins/failures ; we then investigate the effect of peer churn.
1	experimental results: data store
　the performance of the data store partially depends on the performance of the content router  when inserting/deleting items or searching for helper peers . to isolate these effects  we fix the pring content router to have orders 1 and 1 for this set of experiments and investigate different orders in next section. as a baseline for comparison  we use chord  which is efficient due to hashing  but does not support range queries.
　varying item insertion rate figure 1 shows the index message cost as a result of varying insertionrate. the message cost increases linearly with insertionrate because each item insertion requires a search message to locate the peer that should store the item. the message cost increases faster for the p-ring data store than for chord because the p-ring additionally needs to split and merge due to item skew  while chord simply hashes the items. this difference quantifies the overhead of supporting range queries  using the p-ring data store  as opposed to simple equality queries  using the chord data store . finally  the message cost for the pring data store decreases as we use a content router of higher order because the search becomes more efficient with higher order content routers. the graph showing the index bandwidth cost is similar and is not shown. we also obtained similar results by varying itemdeletionpattern.
　varying item insertion pattern figure 1 shows the index message cost as a result of varying iteminsertionpattern from 1 - highly skewed distribution  to 1 - uniform distribution. for the chord data store  as expected  we do not observe any significant variation in message cost. the message cost also remains relatively stable for p-ring data store. this suggests that the p-ring data store effectively manages item skew by splitting and merging as required. the surprising fact is that for p-ring  the cost for uniform distribution is higher than for highly skewed distributions. the reason is that the cost of finding helper peers for split is included in the index cost. in skewed cases  most inserts happen close to 1  so most splits happen at peers close to 1. since the helper peers are stored as items with search key value 〕  see section 1   they are also stored close to 1  so the search cost for finding a helper peer is very low  compared with the uniformly random case. the graph showing the index bandwidth cost is similar  and we also obtained similar results by varying itemdeletionpattern.
1	experimental results: content router
　we now investigate the performance of the p-ring content router  and compare it with skip graphs and chord.
　varying number of peers figure 1 shows the search cost when varying the number of peers. as expected  the search cost

figure 1. number of peers	figure 1. performance
increases logarithmically with the number of peers  note the logarithmic scale on the x-axis  for all the content routers. however  the search cost for the different content routers varies significantly. in particular  skip graphs has significantly worse search cost because the index structure of order d has search performance o d 〜 logdp   where p is the number of peers in the system . in contrast  chord has search cost o log1p  and a p-ring of order d has search cost o logdp . due to the large base of the logarithm  the p-ring of order 1 significantly outperforms the other index structures.
　varying order figure 1  figure 1  and figure 1 summarize the results of varying the order of the hierarchical ring. as expected  the search cost is o logdp . the index message cost decreases with order because there are fewer levels in the hr that need to be stabilized  recall that the number of levels in a hr of order d is logdp . however  the index bandwidth cost decreases slightly and then increases because  at higher orders  more information has to be transferred during index stabilization. specifically  each stabilization message in a hr of order d has to transfer o d  information  the entries at one level . hence  the total bandwidth requirement is o d ， logdp   which is consistent with the experimental results. this shows the tradeoff between maintenance cost and search cost - a higher value of d improves search but increases bandwidth requirements.
1	experimental results: system churn
　figure 1 shows the effect of peer insertions and failures on index performance  for 1 insertions/failures per second  the results with other rates is similar   starting with a system of 1 peers. the basic tradeoff is between search cost and index bandwidth cost. when the content router is stabilized at a high rate  bandwidth cost is high due to many stabilization messages  but the search cost is low since the content router is more consistent. on the other hand  when the content router is stabilized very slowly  the bandwidth cost decreases but the search cost increases. for p-ring and chord  the increase in search cost is small  even if the content router is temporarily inconsistent.
　as shown in figure 1  the p-ring content router always dominates skip graphs due to its superior search performance. chord outperforms p-ring of order 1 because chord does not have the overhead of dealing with splits and merges. however  p-ring of order 1 offers a better search cost  albeit at a higher bandwidth cost  while also supporting range queries. we obtained similar results for search cost vs. index message cost.
1	results from planetlab
　we present preliminary results from our planetlab deployment. we implemented p-ring  online balancing fibbing algorithm   and chord. the code base has more than 1 lines of c++ code and uses tcp/ip as communication protocol. we deployed our system on 1 random machines in planetlab   a network of computers distributed around the world.

	figure 1. search cost vs. hr order	figure 1. message cost	figure 1. bandwidth costitemchurn in a first set of experiments  we study the performance of the system as items are inserted and deleted from the system  item churn . to see the effect of items insertions and deletions on the load balance  we start the system by inserting 1 peers and no data items. then  we randomly insert/delete items in three phases: insert only  insert and delete  and delete only. in each phase we execute 1 operations  at a rate of 1 operation/second. the items are inserted according to a zipfian distribution with domain  1  1  and a skew parameter of 1. the items to be deleted are chosen uniformly at random from the existing items.
　figure 1 shows the load imbalance for p-ring and online balancing during the course of the experiment. the load imbalance is measured each minute the load imbalance for p-ring is almost always close to 1  while for online balancing  the load imbalance is above 1  but below 1 for the most part. the load imbalance is temporarily higher at the beginning of the insert phase  and at the end of the delete phase. there are two reasons for this. first  since we start and end with no data items  the average number of items is very low at the beginning and at the end. so inserting or deleting a few items can make a big difference in the load imbalance. second  the ranges assigned to peers need to adapt to the change in data distribution. we see that the ranges adapt quickly  so after only a few minutes  the load imbalance is below the theoretical bound. this figure also shows that our method of estimating the storage factor for p-ring gives correct results  as the load imbalance is indeed close to the theoretical one.
　figure 1 shows the average message cost for the maintenance of the data store component for chord  p-ring and online balancing. similar trends were obtained for the bandwidth cost. we expected the maintenance cost for p-ring and online balancing to be clearly higher than for chord data store  due to the load-balancing operations. however  the differences in message costs are not big  especially during the insert/delete phase since there are very few re-balancing operations and the ranges have already adapted to the data distribution. moreover  the item insert and delete message cost is similar for all structures  we used hr of order 1 for p-ring and online balancing   and this cost is the major component of the maintenance cost. note that for online balancing  the cost of maintaining an additional index on the load of the peers was not taken into consideration.
　figure 1 shows the evolution of load imbalance for p-ring for different skew parameters. we see that regardless of how skewed the distribution is  the ranges adapt to the distribution using the re-balancing operations. a similar graph was obtained for online balancing.
　peer churn in a second set of experiments  we study the effects of peer insertions and failures on load balancing. for these experiments we start the system by inserting 1 peer and 1 data items with indexing attribute values following a zipfian distribution with domain  1  1  and skew parameter 1. then  peers randomly join/leave the system  in three phases: join only  join and leave  and leave only. in each phase we execute 1 operations  at the 1 operations/second rate.
　figure 1 shows the evolution of load imbalance for p-ring and online balancing  as peers join and leave the system. both algorithms adapt to the changes in the system  however the load imbalance is more variable than in the item churn case  see figure 1 . this is due to the fact that changes in the set of peers in the system  where each peer stores many data items  have a bigger impact on the number of items temporarily stored at each peer  and therefore on the load imbalance  when a peer fails/leaves the system  all the items previously stored by the failed peer will be recovered by its successor  since items are replicated  and that peer will temporarily be very overloaded; similarly  peer insertions could lead to underflow . as expected  the load imbalance is lower for p-ring  than for online balancing.
　figure 1 shows the average message cost  results for bandwidth cost are similar  for maintaining the data store component for pring  online balancing and chord. the average number of messages is higher at the beginning as the 1 items are inserted into the system  and there are only a few peers into the system. after the items are inserted  the average number of messages decreases. figure 1 shows the details of the average message cost  with the highest values eliminated. once all the items were inserted  the data store message cost for chord is close to zero. this is because the chord data store does not try to re-balance the ranges associated to the peers even during churn. the difference in cost between chord and p-ring and online balancing comes from the load balancing operations effectuated by p-ring and online balancing  and represents the cost associated with providing extra functionality: explicit load balance  as opposed to the implicit load balance provided by hashing.
1.	conclusions
　we have introduced p-ring  a novel fault-tolerant p1p index structure that efficiently supports both equality and range queries in a dynamic p1p environment. p-ring effectively balances items among peers even in the presence of skewed data insertions and deletions and provides provable guarantees on search performance. our experimental evaluation shows that p-ring outperforms existing index structures  sometimes even for equality queries  and that it maintains its excellent search performance with low maintenance cost in a dynamic p1p system.
1
	1	1
simulation time  minutes 1	1	1	1
simulation time  minutes 1	1	1	1	1
simulation time  minutes figure 1. load imbalance - churnfigure 1. ds cost - churnfigure 1. ds cost details - churn1	1	1	1	1	1	
