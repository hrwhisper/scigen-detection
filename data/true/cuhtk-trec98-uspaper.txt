this paper presents work done at cambridge university  on the trec1 spoken document retrieval  sdr  track. the broadcast news audio was transcribed using a 1-pass gender-dependent htk speech recogniser which ran at 1 times real time and gave an overall word error rate of 1%  the lowest in the track. the okapi-based retrieval engine used in trec-1 by the city/cambridge university collaboration was supplemented by improving the stop-list  adding a bad-spelling mapper and stemmer exceptions list  adding word-pair information  integrating part-of-speech weighting on query terms and including some pre-search statistical expansion. the final system gave an average precision of 1 on the reference and 1 on the automatic transcription  with the r-precision being 1 and 1 respectively.
the paper also presents results on a new set of 1 queries with assessments for the trec-1 test document data used for development purposes  and analyses the relationship between recognition accuracy  as defined by a pre-processed term error rate  and retrieval performance for both sets of data.
1. introduction
spoken document retrieval  sdr  combines state of the art technology from the fields of speech recognition and information retrieval. we combine the high performance htk speech recogniser with the tried and tested okapi-based retrieval engine to produce a good sdr system  then develop some extensions to improve the system further. we evaluated performance during development on the trec-1 sdr document data using a set of 1 queries developed in-house  cu1   and applied our final system in the trec-1 sdr track.
this paper firstly describes the trec sdr task and the data used in both development and evaluation of our sdr system. the speech recogniser is described in detail in section 1  where the performance of all the sites participating in the cross-recogniser runs is given. the retrieval engine is then described in section 1 emphasising the innovations introduced for the trec-1 evaluation and giving results based on both the cu1 development set and the trec-1 evaluation set. a summary of these results is presented in section 1. the relationship between the output of the speech recogniser and the input of the retriever is discussed in section 1  leading to the introduction of a processed term error rate  ter  to represent the recognition accuracy for sdr systems. section 1 presents the relationship between this ter and retrieval performance for different speech recognisers and shows the degradation of retrieval performance with increased ter. finally  conclusions are offered in section 1.
1. description of trec sdr task
for the trec-1 sdr track  audio from american broadcast radio and tv news programs is presented along with a list of manuallygenerated document-boundaries. natural language text queries  such as  have their been any volcanic eruptions in montserrat recently   are then provided. the participating sites must generate a transcription of the audio automatically and run an ir engine on this transcription to provide a ranked list of potentially relevant documents.
real relevance assessments generated by humans are then used to evaluate the ranked list in terms of the standard ir measures of precision and recall. sites may also run their retrieval system on a manuallygenerated reference transcription  baseline transcription s  provided by nist and cross-recogniser transcriptions generated by other participating sites.
1. description of data
there are two main considerations when describing the data for sdr. firstly the audio data used for transcription  and secondly the query/ relevance set used during retrieval. table 1 describes the main properties of the former  whilst table 1 describes the latter  for the development and evaluation data sets.
developmentevaluationname of datatrec-1 testtrec-1 testnominal length of audio1 hours1 hoursnumber of documents1number of different shows1approx. number of words11average doc length1 words1 wordstable 1: description of data used
developmentevaluationname of query setcu1trec-1 testnumber of queries1average length of query1 words1 wordsnumber of relevant docs1mean # rel docs per query1 docs1 docstable 1: description of query and relevance sets used
1. the htk broadcast news transcription system
the input data is presented to our htk transcription system as complete episodes of broadcast news shows and these are first converted to a set of segments for further processing. the segmentation uses gaussian mixture models to divide the audio into narrow and wideband audio and also to discard parts of the audio stream that contains no speech  typically pure music . the output of a phone recogniser is used to determine the final segments which are intended to be acoustically homogeneous. further details of the segmenter are given in .
each frame of input speech to be transcribed is represented by a 1 dimensional feature vector that consists of 1  including   cepstral parameters and their first and second differentials. cepstral mean normalisation  cmn  is applied over a segment.
our system uses the limsi 1 wsj pronunciation dictionary augmented by pronunciations from a tts system and hand generated corrections. cross-word context dependent decision tree state clustered mixture gaussian hmms are used with a 1k word vocabulary. the full htk system  operates in multiple passes and incorporates unsupervised maximum likelihood linear regression  mllr  based adaptation and uses complex language models via lattice rescoring and quinphone hmms. this system gave a word error rate of 1% in the 1 darpa hub1 broadcast news evaluation.
the trec-1 htk sdr system uses the first two passes of the 1 htk broadcast news system  in a modified form for reduced computational requirement. the first pass uses gender independent  bandwidth dependent cross-word triphone models with a trigram language model to produce an initial transcription. the output of the first pass is used along with a top-down covariance-based segment clustering algorithm  to group segments within each show to perform unsupervised test-set adaptation using maximum likelihood linear regression based model adaptation  1  1 . a second recognition pass through the data is then performed using a bigram language model to generate word lattices using adapted gender and bandwidth specific hmms. these bigram lattices were expanded using a 1-gram language model and the best path through these lattices gives the final output. this system runs in about 1 times real-time on a sun ultra1 and achieves an error rate of 1% on the 1 hub1 evaluation data. it should be noted that the error rates on hub1 data and trec data are not strictly comparable in part due to the differences in quality of the reference transcriptions.
the hmms for trec-1 used hmms trained on 1 hours of acoustic data and the language model was trained on broadcast news transcriptions ranging in date from 1 to may 1 supplied by the ldc and primary source media  about 1 million words in total . the language model training texts also included the acoustic training data  about 1k words . these data were supplemented by 1 million words of texts from the los angeles times and washington post covering the span of the evaluation period  june 1 to april 1 inclusive . using all these sources a 1k wordlist was chosen from the combined word frequency list whilst ensuring that the number of new pronunciations which had to be created was manageable. the final wordlist had an oov rate of 1% on the trec-1 data.
development work on the trec-1 test corpus was done using two htk based systems. the two pass system  htk-1  was similar in design to the final trec-1 system but used hmms trained on only the allowable 1 hours of acoustic training data and used a reduced set of
texts for language model training data and only data that was allowable for the trec-1 tests. this system gave a word error rate of 1% on the trec-1 test data. the other system used in trec-1 development was a single pass system  htk-1  and ran in about 1 times real time. this was similar to the first pass of the two pass system but used more pruning and gave a word error rate 1% on the trec-1 sdr test data.

figure 1: processing for sdr speech recognition
1. wer results from cross recogniser runs
we have also used alternative automatic transcriptions to assess the effect of error rate on retrieval performance  namely  for trec-1 the baseline supplied by nist  computed by ibm  and the transcription obtained by sheffield university . for trec-1 there are the 1 nist-supplied baselines generated from the cmu recogniser  and crossrecogniser runs from dragon  att  sheffield and dera. the full set of comparisons with other sdr sites is given in table 1.
trec-1 test datacorr.subdelinserrnist/ibm baseline11111sheffield11111htk-1.1.1.1.1.1htk-1.1.1.1.1.1trec-1 test datacorr.subdelinserrcuhtk11111dragon11111att11111nist/cmu base1.1.1.1.1.1sheffield11111nist/cmu base1.1.1.1.1.1dera run 1.1.1.1.1.1dera run 1.1.1.1.1.1table 1: wer results for development and evaluation

   nb: development wers were found on a document  story  basis  but evaluation wers were on an episode basis.
1. ir system development
1. benchmark system
our benchmark retriever was the okapi-based system used by the city/ cambridge university collaboration for quasi-spoken document retrieval in the trec-1 evaluation . the overall sdr system architecture is illustrated in figure 1.

figure 1: the overall sdr system architecture
the ir system is split into two stages. firstly a preprocessor stops and stems the words in all the documents using a porter stemmer  and an inverted index file is generated which contains the number of documents in the collection   the length of each document  dl   the number of documents containing each query term    and the number of times the term occurs in the given document  tf .
following  and  the main retrieval engine generates a score for each document for each query by summing the combined weights  cw for each query term produced from the formula:
tf
	cw	
	ndl	tf
where ndl is the length of document normalised by the average dl and and are tuning constants. the final ranked list of documents is thus produced for each query by sorting the returned weights in descending order.
we used our two data sets to explore various refinements to this benchmark system  for the moment disregarding whether they can be respectably motivated within the probabilistic model. thus for example  as there is some demonstrated retrieval value in simple phrases  we experimented with this. though it is impossible with such a small data set to assess how useful various retrieval devices are as means of offsetting speech recognition errors  the fact that with larger collections error compensation may be more important suggested that it was worth undertaking some initial work on devices that not only seem to have some general utility  as shown in past trecs   but also may have some particular value in the spoken document context.
the next sections report our comparative experiments using the cu1 queries/assessments for development  and the trec-1 data for evaluation  with results presented using a subset of the full trec measures.
1. improvements   - stopping
the first preprocessing stage is to remove stop words  so ir performance may thus be affected by which words are defined as stop words.
work was done to stabilise our existing standard stoplist. initially extra query-specific words  such as find and documents  were added to the stoplist for queries only. this meant two stop lists were used  one for the documents and one for the queries. this was useful  but we went further and developed a new pair of stoplists specifically for the broadcast news data. thus 'words' occurring in broadcast news which represent hesitations in speech  such as uh-huh or hmmm were defined as stop words and finally some common function words which appeared to have been overlooked such as am were also added.
the improvement in performance on the development data is given in table 1. the gain in average precision is 1% on the reference and 1% on the automatic transcriptions. the greater improvement on the latter is due to the introduction of recogniser-specific hesitations into the stopword list.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkt1 stop111111new stop111111table 1: effect of using new stop lists for the cu1 data set
the corresponding performance from introducing these new stoplists on the trec-1evaluation is given in table 1. the average precision on the reference increases by 1% whilst for the automatic transcriptions  the increase is 1% these results confirm the benefit of using the new stopword lists.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkt1 stop111111new stop111111table 1: effect of using new stop lists for the trec-1 data set
1. improvements   - mapping
we tried adding a mapping list for word variants. some of these mappings only affect the reference transcriptions  but were included to allow proper reference/speech comparison. others might be important for the automatically transcribed spoken documents.
reference transcriptions from previous broadcast news evaluations were used to generate a list of commonly misspelt words. these are mostly names  such as chechnia/chechnya or zuganauf/zuganov/ zyuganov  but the list also include some words or phrases in common usage which are often misspelt  such as all right/alright and baby sit/baby-sit/babysit. this list was then used to correct the misspelt words by mapping the transcriptions accordingly.
a few synonyms were also added to the mappings to allow words like united states/u.s. to be made equivalent. a stemming exceptions list was also made to compensate for known problems with the porter stemmer  such as equating news and new  but not government and governmental. for ease of implementation this was also included in the mapping step.
the effect of adding this mapping stage to the preprocessing when using the new stoplists is given in table 1 for the development data and table 1 on the evaluation data.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtknew stop111111+ mapping111111table 1: effect of adding mapping for the cu1 data set
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtknew stop111111+ mapping111111table 1: effect of adding mapping for the trec-1 data set
the mapping increased average precision by 1% on the reference and 1% on the automatic transcriptions for the development data. the corresponding effect on the trec-1 evaluation data was disappointing  with an increase of 1% on the reference  but a decrease of 1% on the automatic transcriptions. when the systems were analysed in more detail  it was found that the average precision on the htk transcriptions remained unaltered for 1 queries  increased for 1 queries  and decreased for 1 queries. the largest decrease occurred on query 1:
what are the latest developments in gun control in the u.s.  in particular  what measures are being taken to protect children from guns 
after stopping this became: latest developments gun control us particular measures protect children guns
since the training transcriptions did not always consistently write the word gunshot as either one or two words  the mapping file contained the map  gun+shot gunshot . unfortunately  although this would have helped had the query contained the word gunshot  it actually degraded performance in this case  as an instance of the word gun had effectively disappeared from two relevant documents.
difficulties with words like this exist whether or not mapping is carried out. for example  suppose the word gunshot had been in the query and no mapping had been implemented: our system would not have found stories transcribed as gun shot. therefore  whether a given mapping is beneficial or not  may depend on exactly what the query terms are. this is an inherent difficulty with this type of system and requires either some expansion  or a way of allowing words such as gunshot to match both gunshot and gun + shot during the scoring to solve it.
a similar problem arises with query 1: find reports of fatal air crashes.
if the mapping  air+force airforce is implemented  then the score for the relevant document ee1 which contains two instances of the word-pair  air+force  decreases from rank 1 to rank 1 due to the two occurrences of the word air  which can no longer be found.
the conclusion is therefore that mapping in order to correct bad spellings and allow exceptions to the stemming algorithm is a good thing and will improve retrieval performance. however  mappings which convert two words into one when it is not always clear whether the word should exist as one word  a hyphenated word  or two separate words  should be used sparingly  at the system's peril.
1. improvements   - word-pair modelling
past trec tests have shown that there is some value in the use of phrasal terms  though these need not be linguistically  as opposed to statistically  defined. the most common method is to use a file-based phrasal vocabulary. however linguistically-motivated phrasal terms drawn from the request topic have also been used e.g. with inquery  and we decided to try this.
each query was tagged using a brill tagger  and pairs of adjacent words with no interceding punctuation  which followed the sequence n/n or j/n  where is a noun or name and is an adjective  were marked as word-pairs. these word-pairs were then weighted and added to the query terms. the indexing procedure was refined to allow term position indexes  tpis  to be stored in an augmented inverted file. the normal combined weight measure was applied to the wordpairs  after using the tpis to find which documents the word-pairs occurred in.
an example of the word-pairs added in this way is:
cu1 :	how many people have been murdered by the ira in
northern ireland
northireland
trec-1: what are the latest developments in gun control in the u.s.  in particular  what measures are being taken to protect children from guns  guncontrol
the effect of applying this word-pair modelling on the original trec1 system and the system after the new stoplists and mapping had been applied are given in tables 1 and 1 for the cu1 and trec-1 tasks respectively. the lines labelled 'a' lone  show the impact of this device alone  the lines labelled 'c' ombined  show the impact of the wordpair device when added to the previous stopping and mapping.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtka111111a+wp111111c111111c+wp111111table 1: effect of adding word-pair weights for the cu1 data set
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtka111111a+wp111111c111111c+wp111111table 1: effect of adding word-pair weights for the trec-1 data set
the addition of word-pair information in development increased the average precision of the combined system by 1% on the reference and 1% on the automatic transcriptions. the device was therefore included in the evaluation system. unfortunately it had no effect on average precision for the automatically generated trec-1 transcriptions and actually worsened performance on the reference. this may be influenced by the number of  stopped  query terms  1 per query for cu1  1 for trec-1  and the number of word-pairs added  1 per query for cu1  1 for trec-1   or may be a result of the different properties of the document sets.
1. improvements   - part-of-speech weighting
work in the past within the okapi framework has not significantly investigated the use of explicit  as opposed to implicit  linguistic term characterisation within the probabilistic model  though the model does not constrain linguistic criteria for the initial choice of base terms . we nevertheless decided to study the use of linguistic information  admittedly in an fairly ad-hoc way  but with a view to possibly exploring it more rigorously later.
it seems that certain classes of words convey more information than others. for example  proper names are generally more helpful in finding specific information than commonly used verbs. to exploit this fact different weights were given to the query terms depending on their part-of-speech.
the query terms were tagged using the brill tagger and then subsequently divided into one of four groups: proper noun  pn   common noun  cn   adjective or adverb  aa  and the rest  mainly consisting of verbs and hence denoted vb. the weights which gave the greatest increase in average precision on the development data and were therefore incorporated into the retrieval system proved to be:
proper noun  names 1common noun1adjective & adverbs1verbs and the rest1confirming the belief that names generally hold more specific indications than common nouns  which in turn are better then adjectives  adverbs and verbs.
the effect of applying this pos weighting on the original trec-1 system and the system after the new stoplists  mapping and wordpairs had been applied are given in tables 1 and 1 for the cu1 and trec-1 tasks respectively. also included in table 1 are the results for the case of  pn=1 cn=1 aa=1 vb=1  labelled as opt as they provided an optimal set for the improvement in average precision for the htk transcriptions on the trec-1 evaluation data. the results without including word-pairs on the trec-1
data are given as a comparison and labelled as c-wp.
avepr-prec 1docsavepr-prec 1docsrefrefrefhtkhtkhtka111111a+ pos111111c111111c+ pos111111table 1: effect of adding pos weighting for the cu1 data set
implementing the chosen pos weights gave an increase in average precision of 1% on the reference and 1% on the htk transcriptions for the cu1 development set and 1% on the reference and 1% on the htk transcriptions for the trec-1 evaluation set. increasing the relative weights of nouns further for the trec-1 task can be seen to increase average precision on the htk-recogniser run  whilst leaving the reference unaffected. an additional increase can be gained by removing the word-pair device.

   a small bug in the integration of pos weighting and mapping resulted in the gain in the submitted evaluation run being slightly lower than that quoted here.
avepr-prec 1docsavepr-prec 1docsrefrefrefhtkhtkhtka111111a+pos111111a+opt111111c111111c+pos111111c+opt111111c-wp111111c-wp+pos111111c-wp+opt111111table 1: effect of adding pos weighting for the trec-1 data set
care should be taken when increasing the difference between the query pos weights since this naturally interacts with the stemming because the information about part-of-speech in the documents is generally lost during the stemming procedure.
1. improvements   - statistical pre-search expansion
following widespread trec practice we decided to try pre-search expansion using statistical term co-occurrences   1  1  both as a device for including terms related to the query terms and to compensate for inadequate stemming. in principle this should be based on actual text data  for example  by using a parallel text corpus  but we used the transcribed document set. note this is not ideal due to the document set being small and the presence of transcription errors.
our expansion system  e xpansion   adds a new stem based on an original stem when the probability of the original term being present in a document given that the expanded term is present    is greater than a half. this is equivalent to saying the expanded stem is more likely to occur when the original stem is present than when it is absent. only stems which occur in more documents than the original were added and terms which occur in over 1% of the documents were not expanded to reduce over-expansion problems.
an additional development   r oots   was also included to enhance the stemming process. this involved adding stems which have a common root of at least five letters with the original stem.
the basic weight assigned to an expanded term is   namely the probability of the original term occurring given the expanded term. the weights for the original source and expanded terms are both normalised to give a total equal to the original term weight. thus for a term weight  t  the weights for the original and expanded terms are:

the results are given in tables 1 and 1 for the cu1 and trec-1 data respectively.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtka111111a+e111111a+r111111c111111c+e111111c+r111111table 1: effect of adding expansion for the cu1 data set
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtka111111a+e111111a+r111111c111111c+e111111c+r111111c-wp111111c-wp+e111111c-wp+r111111table 1: effect of adding expansion for the trec-1 data set
these results show that the basic expansion  exp1  increases average precision on both the benchmark and combined system for cu1 queries. the addition of roots to the expansion process increases average precision on the benchmark cu1 system  but does not for the combined system. this is probably because the additional terms are added as a consequence of bad performance by the stemmer  for example californian california  teaching teacher. hence  when the expansion was added to the system which already included the stemming exceptions list  the performance no longer increased. the system used in the evaluation  therefore used the basic expansion system  exp1  but not the roots expansion as it was thought the stemmerexceptions list offered better compensation for the problems with stemming.
the expansion device increased average precision on the combined trec-1 system by 1% on the reference and 1% on the automatic transcriptions. the roots expansion decreased average precision on the automatic transcription as predicted  but actually increased average precision on the reference system. this may be partially due to the fact that the stemmer-exceptions list was manually generated from just the trec-1 test data.
it is important to note that only a few words in a few queries have been expanded. the effect of this kind of query expansion could therefore be very different with a larger set of queries and documents. in addition  term co-occurrences may be better estimated using a larger distinct but similar collection of documents  e.g previous broadcast news stories .
1. improvements   - tuning model parameters
finally the model parameters and were tuned to give maximum average precision on the htk transcriptions for the combined system on the cu1 data set. the results for this are given in table 1 for the cu1 data set and 1 for the trec-1 data set.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtka111111a+tune111111c111111c+tune111111table 1: effect of tuning model parameters on the cu1 data set

   these results are better than those submitted due to a bug in the expansion code.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtka111111a+tune111111a+opt111111c111111c+tune111111c+opt111111c-wp111111c-wp+tune111111c-wp+opt111111table 1: effect of tuning model parameters on the trec-1 data set
1. summary of ir results
1. summary of results on cu1 data
to summarise the foregoing results we show two tables 1 and 1  showing respectively the separate contributions of the various devices detailed above  and their combined effects. the overall improvement in average precision on the htk transcriptions from adding all these new features is 1%.
the equivalent results for trec-1 are given in section 1.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkorig111111stop111111 +map111111wp111111pos111111exp111111tune111111table 1: effect of devices applied separately on the cu1 data set
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkorig111111+stop111111+map111111+wp111111+pos111111+exp111111+tune111111table 1: effect of devices applied in combination on the cu1 data
set
1. summary of results on trec-1 data
it can be seen that although it was previously thought that adding the word-pair device decreases performance  in fact the performance is higher if the word-pair information is included. this shows the dangers of making general conclusions from such small increases in pre-
cision on a relatively small data set. it also illustrates the interaction that occurs between the devices.
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkorig111111stop111111 +map111111wp111111pos111111exp111111tune111111table 1: devices applied separately on the trec-1 data set
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkorig111111+stop111111+map111111+wp111111+pos111111+exp111111+tune111111run*111111* the loss in the submitted run was due to bugs in the pos weighting and expansion code
table 1: devices applied in combination on the trec-1 data set
avepr-p 1docsavepr-p 1docsrefrefrefhtkhtkhtkorig111111+stop111111+map111111+pos111111+exp111111+tune111111table 1: cumulative improvements on trec-1 without wp
1. representing recognition accuracy in sdr
1. word and term error rates
speech recognition accuracy is conventionally expressed in terms of word error rate  wer . to calculate this an alignment of the hypothesised and reference transcriptions is made and the number of insertion       deletion     and substitution     errors are found. for words in the reference transcription  the word error is then given by:
	wer	
when the transcriptions are subsequently used for information retrieval  wer does not accurately reflect the input to the retrieval stage  see figure 1 . firstly  stop words are removed  some words are mapped  and the words are stemmed; secondly  the order of the words is not considered in the standard retrieval case  so an alignment is not necessary  and finally a traditional substitution error can be thought of as two errors  as it not only misses a correct word  but also introduces a spurious one. when investigating recognition accuracy for sdr  we therefore use a term error rate
	ter	
where and represent the number of times word occurs in the reference and the transcription . ter therefore models the output of the pre-processor rather than the speech recogniser and is more appropriate when considering subsequent retrieval performance.

figure 1: defining recognition accuracy during processing for sdr
1. stopping  stemming and mapping
the pre-processing stages of stopping  stemming and mapping have a great influence on the property of the data input to the information retriever. for example  the number of words for each stage for the trec-1 and trec-1 test data using our trec-1 preprocessor is given in table 1.
trec-1 data
recogniseroriginal+stop+map+abbrev+stemreference11ibm baseline11sheffield11htk-111htk-111trec-1 data
recogniseroriginal+abbrev+map+stop+stemreference111cuhtk111dragon111att111base111sheff111base111dera111dera111table 1: number of words for trec-1 and trec-1 sdr after various stages of processing
1.1. word error rates
the corresponding wer at each of these processing stages is given in table 1 and the relationship between these wers is shown in figure 1.
the results on the trec-1 data suggest that the wer after stopping and stemming can be predicted reasonably accurately from the original wer. however  this is not as clear from the trec-1 results. the dera1 run is the only one where the wer goes up after stopping  meaning that the stopped words are recognised better on average than the non-stop words. this is not usually the case for asr systems  since the smaller stop-words which carry less information content are generally more confusable than content words.
trec-1 data
recogniseroriginal+stop+map+abbrev+stemibm baseline1111sheffield1111htk-1.1.1.1.1htk-1.1.1.1.1trec-1 data
recogniseroriginal+abbrev+map+stop+stemcuhtk11111dragon11111att11111base1.1.1.1.1.1sheffield11111base1.1.1.1.1.1dera1.1.1.1.1.1dera1.1.1.1.1.1table 1: % word error rate for trec-1 and trec-1 sdr

figure 1: correlation between word error rates whilst preprocessing.
1.1. term error rates
term error rates are more appropriate when considering speech recognition for sdr problems because they model the input to the retriever more accurately. note  not producing any output gives a ter of 1% whereas misrecognising every word as on oov word produces a ter of 1%  due to each substitution error counting as both an insertion and deletion error. misrecognising every word will in practise give a ter of below 1% as the word ordering is unimportant  so some recognition errors will cancel out.
the corresponding ter at each of the pre-processing stages is given in table 1 and the relationship between these ters is shown in figure 1.
it is interesting to note that on both data sets at low ter  complete preprocessing does not seem to affect the ter. as the recognition performance of the system decreases  so the effect on ter of preprocessing increases.
trec-1 data
recogniseroriginal+stop+map+abbrev+stemibm baseline111sheffield111htk-1.1.1.1htk-1.1.1.1trec-1 data
recogniseroriginal+abbrev+stop+stem+mapcuhtk1111dragon1111att1111base1.1.1.1.1sheffield1111base1.1.1.1.1dera1.1.1.1.1dera1.1.1.1.1table 1: % term error rate for trec-1 and trec-1 sdr

figure 1: correlation between term error rates whilst preprocessing.
it is interesting to compare the relationship between our new metric  the stopped/ stemmed/ mapped ter and the standard measure of speech recognition performance  namely the unprocessed word error rate. a graph showing the relationship between these is shown in figure 1.
the difference between unprocessed wer and processed ter increases as wer increases. this implies that in fact the input to the retriever degrades more rapidly than would be predicted from the wer.

figure 1: relationship between unprocessed wer and processed ter
it is also interesting to realise that stopping the documents increases term error rate  although it decreases  aligned  word error rate. this is thought to be because the majority of cancelling errors occur with the shorter  stopped words  so the cancelling effect is reduced by stopping  hence increasing ter. stemming will always reduce both wer and ter.
1. relationship between ter and ir performance
the average precision  r precision and precision at 1  1 and 1 documents recall is given for the different sdr runs in table 1. this relationship between the average precision and the stop/stem/map ter is plotted in figure 1 with r-precision in figure 1. the interpolated recall-precision averages  plotted in figure 1 show the ir performance of the different systems.
these results show that in general average precision decreases with ter. the insertion rate may be an important influence on this as the trec-1 base 1 recogniser  which has an insertion rate of 1% as opposed to the others which have between 1 and 1%  seems to produce worse ir performance than predicted. the relationship between r-precision and ter is not as clear cut and the precision-recall graphs show the degradation of ir performance on trec-1 is not just related to ter.
1. conclusions
it is not possible to draw any strong conclusions from our trec-1 experiments. this is partly because  in contrast to trec as a whole  there is no trend data: trec-1 used the different  known-item retrieval task. but more importantly  the test data is too small for reliable and informative inference. it seems to be the case that the basic okapi-

figure 1: relationship between ter and average precision
trec 1 data

figure 1: relationship between ter and r precision trec-1 data
pteravepr-prec. 1docs 1docsreference11111htk-1.1.1.1.1.1htk-1.1.1.1.1.1sheffield11111ibm base11111trec-1 data
pteravepr-prec. 1docs 1docsreference11111cuhtk11111dragon11111att11111base1.1.1.1.1.1sheff11111base1.1.1.1.1.1dera1.1.1.1.1.1dera1.1.1.1.1.1table 1: effect of ter on ir performance

figure 1: overall ir performance using the different recognisers
style system works satisfactorily on both reference and automaticallytranscribed data  illustrating its power for documents of a rather different discourse type from those used hitherto  and for ones in a different medium. but while retrieval performance using our recogniser transcriptions is near that for the reference data  it is not clear what impact recognition failures would have on retrieval with a much larger data set or very different forms of query. for the same reason  while we can hypothesise that particular retrieval devices may be not just useful in general but particularly appropriate for speech data  we cannot come to any firmly predictive conclusions on their individual or combined value.
we have investigated the effects of changing stop-lists  adding badspelling correctors  a stemming exceptions list and basic synonym mapping  including word-pair information  weighting query terms by their part-of-speech and adding pre-search statistical expansion. whilst all of these have been shown to increase ir performance under certain circumstances  the increases are small. nevertheless the combination of these devices led to an increase in average precision on the trec-1 evaluation data of 1% on the reference and 1% on the automatic transcriptions over last year's system.
acknowledgements
our thanks go to steve renals at sheffield for providing their transcriptions for the trec-1 test data and to cedric auzanne at nist for confirmation of wer results for the trec-1 evaluation. this work is in part funded by an epsrc grant reference gr/l1.
