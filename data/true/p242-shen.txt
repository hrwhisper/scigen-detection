web-page classification is much more difficult than pure-text classification due to a large variety of noisy information embedded in web pages. in this paper  we propose a new webpage classification algorithm based on web summarization for improving the accuracy. we first give empirical evidence that ideal web-page summaries generated by human editors can indeed improve the performance of web-page classification algorithms. we then propose a new web summarization-based classification algorithm and evaluate it along with several other state-of-the-art text summarization algorithms on the looksmart web directory. experimental results show that our proposed summarization-based classification algorithm achieves an approximately 1% improvement as compared to pure-text-based classification algorithm.  we further introduce an ensemble classifier using the improved summarization algorithm and show that it achieves about 1% improvement over pure-text based methods. 
categories and subject descriptors 
h.1.m  information systems application : miscellaneous; i.1  pattern recognition : applications-text processing; 
general terms 
algorithms  experimentation  verification. 
keywords 
web page categorization  web page summarization  content body  
1. introduction 
with the rapid growth of the world wide web  www   there is an increasing need to provide automated assistance to web users for web page classification and categorization.  such an assistance is helpful in organizing the vast amount of information returned by keyword-based search engines  or in constructing catalogues that organize web documents into hierarchical collections; examples of the latter include the yahoo   
 http://www.yahoo.com   directory and the  looksmart  directory  http://search.looksmart.com . there is evidence that categorization is expected to play an important role in future search services.  for example  research conducted by chen and dumais shows that users prefer navigating through catalogues of pre-classified content .   such a strong need  however  is difficult to meet without automated web-page classification techniques due to the labor-intensive nature of human editing.  
on a first glance  web-page classification can borrow directly from the machine learning literature for text classification .  on closer examination  however  the solution is far from being so straightforward.  web pages have their own underlying embedded structure in the html language. they typically contain noisy content such as advertisement banner and navigation bar.  if a pure-text classification method is directly applied to these pages  it will incur much bias for the classification algorithm  making it possible to lose focus on the main topics and important content.  thus  a critical issue is to design an intelligent preprocessing technique to extract the main topic of a web page. 
in this paper  we show that using web-page summarization techniques for preprocessing in web-page classification is a viable and effective technique.  we further show that instead of using an off-the-shelf summarization technique that is designed for pure-text summarization  it is possible to design specialized summarization methods catering to web-page structures.  in order to collect the empirical evidence that summarization techniques can benefit web classification  we first conduct an ideal case experiment  in which each web page is substituted by its summary generated by human editors. compared to using the full-text of the web pages  we gain an impressive 1% improvement in f1 measurement.  in addition  in this paper  we also propose a new automatic web summarization algorithm  which extracts the main topic of a web page by a page-layout analysis to enhance the accuracy of classification. we evaluate the classification performance with this algorithm and compare to some traditional state-of-the-art automatic text summarization algorithms including supervised methods and unsupervised learning methods. experiment results on looksmart web directory show that all summarization methods can improve the micro f1 measure. finally  we show that an ensemble of  
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  july 1  1  sheffield  south yorkshire  uk. summarization methods can achieve about 1% improvement relatively on micro f1 measure  which is very close to the upper bound achieved in our ideal case experiment. 
the rest of the paper is organized as follows. in section 1  we present the related works on web classification and summarization. then we present our proposed unsupervised and supervised summarization algorithms in section 1. in section 1  the experimental results on looksmart web directory are shown copyright 1 acm 1-1/1...$1. 

as well as some discussions. finally  we conclude our work in section 1. 
1. related work 
recently much work has been done on web-page summarization . ocelot  is a system for summarizing web pages using probabilistic models to generate the  gist  of a web page. the models used are automatically obtained from a collection of human-summarized web pages.  in   buyukkokten et al. introduces five methods for summarizing parts of web pages on handheld devices where the core algorithm is to compute the words' importance using tf/idf measures and to select important sentences using luhn's classical method . in   delort exploits the effect of context in web page summarization  which consists of the information extracted from the content of all the documents linking to a page. it is shown that summaries that take into account of the context information are usually more relevant than those made only from the target document.  
some research has been done to enhance categorization by summarization   but these works handle pure text categorization only. in   kolcz et al. uses summarization as a feature selection method and applies a simple extraction-based technique with several heuristic rules.  
our work is related to that for removing noise from a web page. in this aspect  yi et al. propose an algorithm by introducing a tree structure  called style tree  to capture the common presentation styles and the actual contents of the pages in a given web site . however  the style tree is difficult to be built when the number of web sites is large. 
the structure of a web page is influenced by many factors. chen et al. pointed out in  that when authoring a web site  the editors usually first conceive the information structure of the site in their mind.  they then compile their thoughts into cross-linked web pages by html language and finally  some extra information  such as navigation bar  advertisement  and copyright information are inserted to prettify the whole page. since html is a visual representation language  much useful information about the content organization is lost after the authoring step.  in order to find the important structural information again  two methods have been widely used. one is to extract title and meta-data included in html tags to represent the semantic meaning of the web pages. it's usually true that title and meta-data should be good information to be used by their authors to indicate the main content of web pages. however we can not fully rely on them due to the following reasons.  first  title and meta-data may be empty in some web pages. for example  in our dataset  about 1% of the pages are without any meta-data and 1% pages are without a title. second  some of titles and meta-data may be meaningless since website designer may not fill them in and may simply set them by default  with such useless names as  page1    page1 . finally  web site designers may misuse or even give the wrong title or meta-data fields to cheat search engines in order to boost up their ranking.  
therefore  it is critical for us to extract the main topic of a web page by automatically analyzing their context features  such as the anchor text pointing to a web page .  in this direction  glover et al.  provided an analysis of the utility of text in citing documents for classification and proved that anchor text was valuable. nevertheless  this should be done with care; chakrabarti  studied the role of hyperlink in hypertext classification and pointed out that a na ve use of terms in the linked neighborhood of a web page could even degrade the classification performance.  
to summarize  our aim is to apply web-page summarization to web-page classification  rather than using pure-text summarization for the purpose.  we will show that the special nature of web pages have a large impact on the classification performance. 
1. web-page summarization  
in this section  we consider how to analyze the complex implicit structure embedded in web pages  and how to use this information for summarization of web pages. our approach is to extract most relevant contents from the web pages and then pass them on to a standard text classification algorithm. 
in particular  we will consider four different methods for conducting the web page summarization.  the first method corresponds to an adaptation of luhn's summarization technique.  the second method corresponds to using latent semantic analysis on web pages for summarization.  the third method corresponds to finding the important content body as a basic summarization component.  finally  the fourth method looks at summarization as a supervised learning task.  we combine the results of all four summarization methods into an ensemble of summarizers  and use it for web page summarization. 
1 adapted luhn's summarization method 
we adapt luhn's method that is designed text summarization for the purpose of web-page summarization.  luhn's method is a systematic approach to perform summarization which forms the core of the field today . in this extraction-based method  every sentence is assigned with a significance factor  and the sentences with the highest significance factor are selected to form the summary. in order to compute the significance factor of a sentence  we need to build a  significant words pool  which is defined as those words whose frequency is between highfrequency cutoff and low-frequency cutoff that can be tuned to alter the characteristics of the summarization system. after this is done  the significant factor of a sentence can be computed by luhn's method as follows:  1  set a limit l for the distance at which any two significant words could be considered as being significantly related.  1  find out a portion in the sentence that is bracketed by significant words not more than l non-significant words apart.  1  count the number of significant words contained in the portion and divide the square of this number by the total number of words within the portion. the result is the significant factor related to s. 
in order to customize this procedure for web-pages  we make a modification to luhn's algorithm. in our web classification task  the category information of each page is already known in the training data  thus significant-words selection could be processed within each category. in this way  we build significant words pool for each category by selecting the words with high frequency after removing the stop words in that category and then employing luhn's method to compute the significant factor.  
there are two advantages for this modification.  first  the prior knowledge of categories is utilized in summarization. second  some noisy words which may be relatively frequent in an individual page will be removed through the use of statistics over multiple documents. when summarizing the web pages in the training set  the significant score of each sentence is calculated according to the significant-words pool corresponding to its category label. for a testing web page  we do not have the category information. in this case  we will calculate the significant factor for each sentence according to different significant words pools over all categories separately. the significant score for the target sentence will be averaged over all categories and referred to as sluhn . the summary of this page will be formed by the sentences with the highest scores. 
1 latent semantic analysis  lsa  
latent semantics analysis  lsa  has been successfully applied to information retrieval  as well as many other related domains. its power is derived from its ability to represent terms and related concepts as points in a very high dimensional  semantic space  . in the text summarization area  gong  is one of the works that has successfully applied the lsa to pure text.  in this section  we will review how to apply lsa to summarization. 
to begin with  lsa is based on singular value decomposition  svd   a mathematical matrix decomposition technique that is applicable to text corpora experienced by people. given an m*n matrix a =  a1 a1 ...an   with each column vector ai representing the weighted term-frequency vector of sentence i in the document under consideration  the svd is defined as: 
a u1v t 
where u =  uij  is an m*n column-orthonormal matrix whose columns are called left singular vectors;   = diag  1   1 ...  n  is an n*n diagonal matrix whose diagonal elements are non-negative singular values sorted in descending order. v =  vij  is an n¡Án othonormal matrix whose columns are called right singular vectors. 
as noted in   lsa is applicable in summarization because of two reasons.  first  lsa is capable of capturing and modeling interrelationships among terms by semantically clustering terms and sentences.  second  lsa can capture the salient and recurring word combination pattern in a document which describes a certain topic or concept.  in lsa  concepts are represented by one of the singular vectors where the magnitude of the corresponding singular value indicates the importance degree of this pattern within the document. any sentence containing this word combination pattern will be projected along this singular vector. the sentence that best represents this pattern will have the largest index value with this vector. we denote this index value as slsa and select the sentences with the highest slsa to form the summary. the pseudo-code of svd-based summarization method can be found in . 
1 content body identification by page layout analysis 
the structured character of web pages makes web-page summarization different from pure-text summarization. this task is difficult due to a number of  noisy  components on a web page  such as the navigation bar  advertisement  and copyright information. in order to utilize the structure information of web pages  we employ a simplified version of the function-based object model  fom  as described in . 
in a nutshell  fom attempts to understand an authors' intention by identifying the object's function and category. in fom  objects are classified into a basic object  bo   which is the smallest information body that cannot be further divided  or a composite object  co  which is a set of objects  bo or co  that perform some functions together.  an example of a bo is a jpeg file.  in html contents  a bo is a non-breakable element within two tags or an embedded object. there is no other tag inside the content of a bo. according to this criterion  it is easy to find out all the bos inside a web page.  likewise  cos can be detected by a layout analysis of web pages. the basic idea is that objects in the same category generally have consistent visual styles so that they are separated by apparent visual boundaries  such as table boundaries  from the objects in other categories.    
after detecting all the bos and cos in a web page  we could identify the category of each object according to some heuristic rules. detailed examples of these rules are shown in ; here we give an overview only.  first  the categories of objects include: 
1  information object: this object presents content information.  
1  navigation object: this object provides navigation guide.  
1  interaction object: this object provides user side interaction.  
1  decoration object: this object serves for decoration purpose.  
1  special function object: this object performs special functions such as ad  logo  contact  copyright  reference  etc. 
in order to make use of these objects  from the above types of objects  we define the content body  cb  of a web page which consists of the main objects related to the topic of that page; these are the objects that convey important information about the page. the algorithm for detecting cb is as follows: 
1. consider each selected object as a single document and build the tf*idf index for the object. 
1. calculate the similarity between any two objects using cosine similarity computation  and add a link between them if their similarity is greater than a threshold. the threshold is chosen empirically. after processing all pairs of objects  we will obtain a linked graph to connect different objects. 
1. in the graph  a core object is defined as the object having the most edges. 
1. extract the cb as the combination of all objects that have edges linked to the core object. 
finally  we will assign a score scb to each sentence  for which 
scb 1 if the sentence is included in  content body ; otherwise  scb 1 . finally  all sentences with scb equal to 
1 give rise to the summary of the web page in question. 
1 supervised summarization 
besides the unsupervised summarization algorithms described above  some researchers also focus on generating the summary using machine learning approaches . in this paper  we also employ a supervised approach for web summarization  by making full use of the labeled training data. a set of features are first extracted from each of a web page. then  a supervised learning algorithm is applied to train the summarizer to identify whether a sentence should be selected into its summary or not. there are a total of eight features utilized in our algorithm  where five of them are common features for text document and web page and the rest three of them are specific to web page layout. 
some notations are defined as follows:  
pn: the number of paragraphs in a document; 
sn: the number of sentences in a document; 
plk: the number of sentences in a certain paragraph k 
para i : the associated paragraph of sentence i 
tfw: the number of occurrences of word w in a target web page; 
sf w: the number of sentences including the word w in the b page; 
given a set of sentences si  i = 1... sn  in a page  the eight features are defined as follows: 
 1  fi1 measures the position of a sentence si in a certain paragraph.  
 1  fi1 measures the length of a sentence si  which is the number of words in si.  
 1  fi1=  tfw*sf w. this feature takes into account not only the number of word w into consideration  but also its distribution among sentences. we use it to punish the locally frequent words.  
 1  fi1 is the similarity between si and the title. this similarity is calculated as the dot product between the sentence and the title. 
 1  fi1 is the cosine similarity between si and all text in the page. 
 1  fi1 is the cosine similarity between si and meta-data in the page. 
 1  fi1 is the number of occurrences of word from si in special word set. the special word set is built by collecting the words in the web page that are italic or bold or underlined. 
 1  fi1 is the average font size of the words in si. in general  larger font size in a web page is given higher importance. 
 
after extracting these eight features from a web page  we apply the na ve bayesian classifier to train a summarizer  as in . 
 
1
   p  f j | s   s  p s   s  j 1
p s   s | f1  f1... f1   1	 
¡¡p  f j   j 1
where p s s  stands for the compression rate of the summarizer  which can be predefined for different applications  p  fi   is the probability of each feature i  and p  fi | s s   is the conditional probability of each feature i.  the latter two factors can be estimated from the training corpus. each sentence will then be assigned a score by the above equation  which is denoted as ssup . 
1 an ensemble of summarizers 
by combining the four methods presented in the previous sections  we obtain a hybrid web-page. given an incoming web page  we calculate the importance score for each sentence by the four summarization algorithms separately. the final score of a sentence is the sum of the four scores.  
s sluhn slsa scb ssup 
the sentences with the highest s will be chosen into the summary.  
1. experiments 
in order to test the effectiveness of summarization for web classification  several experiments are conducted. firstly  we test the web page classification on the human created summaries in order to find out whether the summarization can help classification of web pages at all. having confirmed this hypothesis  we compare our proposed  content body identification summarizer  with two traditional algorithms: adapted luhn's algorithm and lsa-based methods  as well as the supervised summarizers. finally  our ensemble of summarizers is evaluated. in our experiments  we also study the variation of different parameter settings for composing the best summarizer.  
1 data set 
in our experiments  we use about 1 millions web pages crawled from the looksmart web directory  http://search.looksmart.com . due to the limitation of network bandwidth  we only downloaded about 1 thousand descriptions of web pages that are manually created by human editors. since it is a time-consuming task to run experiments on this large data set  we randomly sampled 1% of the pages with descriptions for our experiment purpose. the extracted subset includes 1 pages  which are distributed among 1 categories  we only consider the top two level categories on looksmart website . the largest category  library society  consists of 1 pages; while the smallest category  people & chat find people  consists of only 1 pages. table 1 and table 1 show the number of pages for the three largest categories and three smallest categories. in order to reduce the uncertainty of data split  a 1-fold cross validation procedure is applied in our experiments.  
table 1. the three largest categories 
category name total train test library society 1 1 1 travel destinations 1 1 1 entertainment celebrities 1 1 1  
table 1. the three smallest categories 
category name total train test sports news & scores 1 1 1 people & chat personals 1 1 1 people & chat find people 1 1 1 1 classifiers 
since the focus of this paper is to test the effectiveness of web summarization for classification  we choose two popular classifiers in our experiments.  one is a na ve bayesian classifier    and another is a support vector machine .  
1.1 na ve bayesian classifier  nb  
the na ve bayesian classifier  nb  is a simple but effective text classification algorithm which has been shown to perform very well in practice  . the basic idea in nb is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. as described in   most researchers employ nb method by applying bayes' rule: 
	p c |t  	n p w | c ;t  n  w  d  
p c | d ;t   	j  k 1	k	j	k	i	  
j	i ||c| p cr	   k 1	n  wk  di   n
	r 1	|t 	p wk | cr ;t  
where p   c j |t    can be calculated by counting the frequency with each category c j occurring in the training data; | c | is the number of categories; p wi | c j   stands for probability that word wi occurs in class cj which maybe small in training data  so the laplace smoothing is chosen to estimate it; n  wk  d i   is the number of occurrences of a word wk in di ; n is the number of words in the training data. 
1.1 support vector machine  svm  
support vector machine  svm  is a powerful learning method recently introduced by v.vapnik et al. . it is well founded in terms of computational learning theory and has been successfully applied to text categorization  .  
svm operates by finding a hyper-surface in the space of possible inputs. the hyper-surface attempts to split the positive examples from the negative examples by maximizing the distance between the nearest of the positive and negative examples to the hypersurface. intuitively  this makes the classification correct for testing data that is near but not identical to the training data. there are various ways to train svms. one particularly simple and fast method is sequential minimal optimization  smo  developed by j. platt which is available on . his sequential minimal optimization algorithm breaks the large quadratic programming  qp  problem down into a series of small qp problems to be solved analytically. thus the smo algorithm is efficiently applicable for large feature and training sets. 
1 evaluation measure 
we employ the standard measures to evaluate the performance of web classification  i.e. precision  recall and f1-measure . precision  p  is the proportion of actual positive class members returned by the system among all predicted positive class members returned by the system. recall  r  is the proportion of predicted positive members among all actual positive class members in the data. f1 is the harmonic average of precision and recall as shown below: 
f1upur/ pr  
to evaluate the average performance across multiple categories  there are two conventional methods: micro-average and macroaverage. micro-average gives equal weight to every document; while macro-average gives equal weight to every category  regardless of its frequency. in our experiments  only microaverage will be used to evaluate the performance of classification.  
1 experimental results and analysis 
1.1 baseline 
a simple way to perform web classification is to treat it as a pure-text document. in our experiment  the state-of-the-art text classification algorithms  nb & svm  are applied to build the baseline system. firstly  web pages are converted to pure text document by removing the html tags. then  each document is tokenized with a stop-word remover and porter stemming . finally  each web page is represented as a bag-of-words  in which the weight of each word is assigned with their term frequency 1. in order to speed-up the classification  a simple feature selection method   document frequency selection  df     is applied in our experiment. in our experiments  the words whose df is lower than six are removed from feature set. finally  we obtain the classification results based on the selected word features  as shown in the  full-text  row of table 1 and table 1. from these two tables  we found that svm achieves 1 in micro-f1  which outperform the nb's result by about 1% relatively.  we also found that the variance of 1-fold cross validation is quite small  about 1%   which indicates that the classification is stable on this dataset. 
table 1. experimental results on nb 
 microp micror micro-f1 full-text 1¡À1 1¡À1 1¡À1 title 1¡À1 1¡À1 1¡À1 meta-data 1¡À1 1¡À1 1¡À1 description 1¡À1 1¡À1 1¡À1 content body 1¡À1 1¡À1 1¡À1 luhn 1¡À1 1¡À1 1¡À1 lsa 1¡À1 1¡À1 1¡À1 supervised 1¡À1 1¡À1 1¡À1 hybrid 1¡À1 1¡À1 1¡À1  
table 1. experimental results on smox 
 microp micror micro-f1 full-text 1¡À1 1¡À1 1¡À1 title 1¡À1 1¡À1 1¡À1 meta-data 1¡À1 1¡À1 1¡À1 description 1¡À1 1¡À1 1¡À1 content body 1¡À1 1¡À1 1¡À1 luhn 1¡À1 1¡À1 1¡À1 lsa 1¡À1 1¡À1 1¡À1 supervised 1¡À1 1¡À1 1¡À1 hybrid 1¡À1 1¡À1 1¡À1 1.1 results on human's summary 
in order to test the effectiveness of summarization techniques for web classification  we conduct a feasibility study in our experiment. we extract the description of each web page from the looksmart website and consider it as the  ideal  summary for the page. since the description is authored by the web directory editors  the quality is considered to be good enough to be the summary for the page. we apply the classifiers directly on these descriptions instead of the full text of the web pages. this experiment can help us understand whether in the best case  summarization can help improve the classification. in addition  the title and meta-data of a web page can also be considered as a kind of summary. an example of the description  title and metadata is shown in figure 1 and the classification results on these  ideal summary  are shown in the related rows of table 1 and table 1. compared to full-text classification  classification on human-authored  description  can significantly improve the f1 measure by more than 1% using either classifier. however  classification on  pure title  or  pure meta-data  achieves worse f1-measure results as compared to the baseline system; this is because these descriptions are usually short and do not contain sufficient information. through analyzing on the special cases  we found that web-page  descriptions  can easily help the enduser to understand the meaning of the web page. although the title can play this role also to some extend  their short lengths is indeed impossible to represent the full meaning of the page. the uneven quality of the meta-data because some of them are the default values  also prevents them from achieving good results.  
 through the  ideal case  experiments  we have found that the  ideal summary  can indeed help improve the web classification performance.  in addition  if the summary is not done properly  then the  bad summary  can hurt the performance. hence  in the rest of the experiments  we hope to achieve a similar  good  summary by our automatic web summarization techniques. 
 
description: aap - do yourself a favor: skip the tan warns about 
the effects of suntans  including wrinkles and skin cancer. from the american academy of pediatrics. 
title:  	aap - do your skin a favor: skip the spring break tan 
meta-data:  null    figure 1. an example of the human-supplied  good summary : the description  title and meta-data of a page. 
1.1 results on unsupervised summarization algorithms  
in this section  we evaluate our proposed web summarization algorithms.  we test and compare the content-body identification by page layout analysis  as well as the other two summarization algorithms including  adapted luhn's algorithm  and  lsa .  
as mentioned in section 1  we set a threshold value to determine whether there is a link between the two objects on a web page. in our experiment  the threshold is set to be 1. through our experiments  we found that most of the unrelated objects in web pages  such as copyright and advertisement banner  can be easily removed by our algorithm. for example  in figure 1  the web page is segmented into four objects by our proposed page layout analysis algorithm. within these objects  only object 1  title  and object 1  main body  are selected as content body; object 1  banner  and object 1  copyright  are removed as noisy data. for luhn's algorithm and lsa algorithm  the compression rate is set as 1% and 1% respectively in our experiments. from table 1 and table 1  we found that these three unsupervised summarization algorithms are comparable on classification experiment. all of them can achieve more than 1% improvement as compared to the baseline system.   
1.1 result on supervised summarization algorithm 
 in this experiment  since the web-page description is authored by web-directory editors instead of extracted from the web pages automatically  we need to tag each sentence as positive or negative example for training the supervised summarizer. in our experiment  we define one sentence as positive if it's similarity with the description is greater than a threshold  1 in this paper   and others as negative. the f1 measure of the supervised method  denoted by supervised  is shown in table 1 and table 1  when compression rate equals to 1% . we found it can achieve about 1% relatively improvement compared to baseline system  which is a little worse than unsupervised algorithms. the reason may be that our training data selection is not precise since we only rely on the similarity to descriptions. 
1.1 result on hybrid summarization algorithm 
through the above experiments  we found that both unsupervised and supervised summarization algorithms can improve the classification accuracy to some extent. but none of them can approach the upper bound of the system set by classification by human edited summary. therefore  in this experiment we are investigating the relative merits of these summarization techniques for classification  and compare with an ensemble of them. from table 1 and table 1  we found that all of the summarization algorithms were complementary. the ensemble of summarization methods can achieve about an impressive 1% improvement as compared to baseline system  which is also very near to the upper bound of the system. in this experiment  we use the same weighting for each summarization algorithm. we will consider the different weighting schema in the later section. 
1.1 performance on different compression rates 
 in order to find the relationship between the performance of classification and the compression rate of summarization  we conducted the experiments and the results are shown in table 1 and table 1. 
table 1. performance of cb with different threshold with nb 
 1 1 1 1 content body 1¡À1 1¡À1 1¡À1 1¡À1  
table 1. performance on different compression rate with nb 
 1% 1% 1% 1% luhn 1¡À11¡À1 1¡À11¡À1 lsa 1¡À11¡À1 1¡À11¡À1 supervised 1¡À11¡À1 1¡À11¡À1 hybrid 1¡À11¡À1 1¡À11¡À1 from table 1 and table 1 we found that all the methods reach their peak performance when the compression rate is 1% or 1%  for cb when the threshold equals to 1 . however  when the compression rate rises to 1%  the performance of some methods such as lsa and supervised summarization become worse than the baseline. this may be ascribed to the inclusion of noises with the raise of the compression rate. 
1.1 effect of different weighting schemata 
in the section  experiments are conducted to test the effect of different weighting schema. we tested five cases denoted by  schemas 1 which assigns different weights to different summarization scores  in addition to the original schema which sets an equal weight for each summarization algorithm. for simplicity  we modified the equation in section 1 as  
s w1sluhn w1slsa w1scb w1ssup 
schema1: we assigned the weight of each summarization method in proportion to the performance of each method  the value of micro-f1 .  
schema1: we increased the value of wi  i=1  1  1  1  to 1 in schema1 respectively and kept others as one. 
from the results shown in table 1  we can conclude that different schemata made no obvious difference.  
table 1. effect of different weighting schema with nb 
 microp micror micro-f1 origin 1¡À1 1¡À1 1¡À1 schema1 1¡À1 1¡À1 1¡À1 schema1 1¡À1 1¡À1 1¡À1 schema1 1¡À1 1¡À1 1¡À1 schema1 1¡À1 1¡À1 1¡À1 schema1 1¡À1 1¡À1 1¡À1 1 case studies 
in the experiments above  we observed that all the summarization methods achieve some clear improvement as compared to the baseline by either nb or svm classifier. in order to find out the underlying reasons why summarization can help the classification  in this section we conduct a further case study experiment. 
we randomly selected 1 web pages that are correctly labeled by all our summarization based approaches but wrongly labeled by the baseline system  denoted as set a  and 1 pages randomly from the testing pages  denoted as set b . we find that the average size of pages in a which is 1k is much larger than that in b which is 1k. the difference shows that the useful information about the content of web pages is more likely to be missing in larger-sized pages. the summarization techniques can help us to extract this useful information from the large pages.   
to illustrate  we show a relatively simple web page  http://www.aap.org/advocacy/releases/safeskin.htm  from a to show how our approaches work. this example page is shown in figure 1.  the summary given by the human editor including the description  title and meta-data is shown in figure 1. as we can see  the description is very clear and indeed captures the web page's main topic without introducing noise. thus the performance based on this summary is always the best one. however the meta-data for this page is empty  which accounts for the poor performance of classification based on  pure meta-data . 
by analyzing the layout of the page  we can separate it into four objects as shown in figure 1. objects 1 and 1 were extracted as content body  which correspond nicely human intuition since objects 1 and 1 were not that related to the topic of the page. 
the summaries created by luhn's method  lsa and supervised method are not shown in this paper due to space limitation.  we found that most of the sentences selected by the above summarization method are correctly included in the summary. though the supervised method itself may introduce some noise  the ensemble-based method can successfully rule out the noise.  

figure 1. an example to illustrate our approaches 
1. conclusions and future work 
in this paper  several web-page summarization algorithms are proposed for extracting the most relevant features from web pages for improving the accuracy of web classification. as illustrated by our ideal-case experiment  the summary created by human editors can achieve more than 1% improvement by the micro-f1 measure as compared to the pure text of the web pages. this observation validates the need to find better web-page summarization methods.  we evaluated web-page categorization on several state-of-the-art automatic document summarization algorithms  as well as an algorithm by utilizing the layout analysis of web pages. experimental results show that automatic summary can achieve a similar improvement  about 1% improvement  as the ideal-case accuracy achieved by using the summary created by human editors. 
in this paper  we only considered a web page as an isolated document. however  more and more research works demonstrate that the hyperlink is one of the important features for web search and analysis. in the future  we will investigate methods for multidocument summarization of the hyperlinked web pages to boost the accuracy of web classification.  
