　　this paper describes work in progress on a computer program that uses syntactic constraints to derive the meanings of verbs from an analysis of simple english example stories. the central idea is an extension of winston's  winston 1  program that learned the structural descriptions of blocks world scenes. in the new research  english verbs take the place of blocks world objects like arch and 
towkr  with frame-based descriptions of causal relationships serving as the structural descriptions. syntactic constraints derived from the parsing of story plots arc used to drive an analogical matching procedure. analogical matching gives a way to compare descriptions of known words to unknown words. the  meaning  of a new verb is learned by matching pan of the causal network description of a story precis containing the unknown word to a set of such descriptions derived from similar stories that contain only known words. the best match forges an assignment between objects and relations such that the unknown veib is matched to a known verb  with the assignment being guided by syntactic constraints. the causal network surrounding the unknown item is then used as a scaffolding to construct a network representing the use of the novel word in a particular context. words  and their associated stories  that are  best matches  are grouped together into a similarity network  according to the match score. 
i word acquisition and definitions 
   this paper describes an analogical matching system that attempts to learn the causal descriptions of new words. the end result is that there arc no  definitions  in the sense of necessary and sufficient conditions determining word meanings; rather  what is output is an interconnected set of descriptions of the actual use of words in context  under a particular theory of  context   namely  causal network structure . the use of analogical matching here should not be viewed as a necessary ingredient of the learning system  but rather one way to represent a program's knowledge about the world. in other words  story plots serve as a proxy for systematic understanding of how the world works  and by matching stories the program can determine that a novel situation will work like an old one. 
　　the word learning program is also embedded into a larger system that can acquire new syntactic rules for knglish  as described in  berwick 1 1 . the word learning component uses the larger system's determination of the syntactic category of a new word and its predicate-argument structure  if a verb . these last 
 this paper reports work done at the artificial intelligence laboratory of the massachusetts institute of technology. support for the laboratory's artificial intelligence research is supported in part by the advanced research projects agency of the department of defense under office of naval reseaarch contract n1-c-o1. 
two abilities arc based on the x-bar theory of  jackcndon'1  and a theory of syntax that assumes a strong principle of lexical transparency  roughly  that the semantic argument structure of a verb appears at all levels of representation . the overall system thus provides a working example of how syntactic and semantic constraints can interact to aid in the learning of language. 
   conceptually  the underlying assumption is that the meaning of a word is determined by the role it plays in a causal network description of an event  and that similar words are those that play similar roles in the description of similar events. to take an example that will be discussed in detail in section 1  consider the following scenario: 
suppose we arc given two versions of the story of 
	macbeth  	one 	reporting 	that 	 macbeth 	murders 
duncan  and the other that  macbeth assassinates 
	duncan . 	further suppose that  murder  but not 
 assassinate  is a known word. we should conclude that  assassinate  is most like  murder   since  comparing stories  it seems to us that  assassinate  plays the same role that  murder  docs in the macbeth plot. we should also conclude that  assassinate  has political overtones  since we note that the macbeth story includes such relations as  macbeth wants to be king  and  macbeth becomes king . probing further  later stories should inform us that the uses of  assassinate  and  murder  are slightly different  since  murder  needn't carry that political connotations that  assassinate  docs. we should also be able to use the story of hamlet to deduce the same kind of relationship between  murder  and  assassinate . 
litis scenario illustrates the kind of learning to be modeled by 
the program. the actual retrieval and matching of candidate stories is considerably more complex than that described in  winston 
1 . two separate levels of filtering  geared to linguistic and  script-like  constraints  are invoked before causal network matching is even attempted  largely on account of the computational cost of the matching- proccdure . 
ii a scenario shows the program in action 
   perhaps the best way to gain some feel for the details of the acquisition procedure is to run through an example. here is the actual input for the macbeth story: 
ma is a story about macbeth lady-macbcth duncan and macduff. macbeth is an evil noble. iady-macbeth is a greedy ambitious woman. duncan is a king. macduff is a noble  lady-macbcth persuades macbeth to want to be king because she is greedy. macbeth is 
1 r. berwick 
evil because l.ad -macbeth is able to influence him and because lady-macbeth is greedy. she is able to influence him because he is married to her and because he is weak. macbeth assassinates duncan with a knife. macbeth assassinates duncan because macbeth wants to be king and because macbeth is evil. i ady-macbeth kills herself. macduff is angry. macduff kills macbeth because macbeth murdered duncan and because 
macduff is loyal to duncan. 
using the techniques described in  winston 1  and  katz and winston 1   the system builds a  causal network description  of the story  as shown below. this network is basically an object-oriented semantic network  with objects  agents  and qualities  and sometimes prepositional attitudes like beliefs or desires  serving as the nodes in the network and verbs serving as the links between nodes. the network is directed. it is dubbed  causal  because a major component is the addition of directed cause links indicating that an object or relation at the link tail causally contributes to the relation or action at its head.  there arc some other special link labels. the   hq  link label denotes a two-place relation between nodes that is to be read as  x has quality y ; ako x = a-kind-of x. some relations are added by one-step inference demons  described below; for example   murder --  adds  hq dead .  i iq evil  as appropriate. for reasons of clarity  some links in the description are omitted in the diagram below. there are also several evident weaknesses of the representation that will not be dealt with here. most notably  aside from the implicit ordering imposed by causal relationships  there is no notion of temporality.  

   now suppose that a number of other stories have been previously analyzed and stored in causal network form  e.g.  macbeth. hamlet  julius caesar  and the taming of the shrew  as discussed in  winston 1   and that none of these previous plot summaries used the word  assassinate.  
　　the first job of the program is to successfully parse the input story containing the unknown word. here  the new story differs in 
just two ways from the old story since the new word is used twice. 
the techniques described in  berwick 1   based on the 
marcus parser  marcus 1  arc used to syntactically analyze the input: 
 1  consider the sentence   macbeth assassinates duncan . since all s's arc assumed to be of the form np-vp  the parser predicts the existence of an np subject. 
 1  with the np disposed of  the parser now predicts that an inflectional element and a verb phrase  vp  will be found. now the constraint  sec  jackendoff 1   that english verb phrases are headed by verbs forces  assassinate  to be a verb. 
 1  finally   duncan  is parsed as an np and noted as an object of the unknown verb. since arguments arc assumed obligatory until proven otherwise by positive examples  see  berwick 1  for a discussion of this learnability constraint   this np is presumed to be a required argument of  assassinate . 
　　with the analysis of the novel sentences in hand  the remainder of the story analysis proceeds as before. a causal network representation can be built that is nearly identical to the network built previously  but with some key differences that flow from me lack of understanding of  assassinate.  most importantly  since there arc no one-step inference demons attached to  assassinate   the uq relations of duncan being dead and macbeth being evil cannot be deduced. still  the proper syntactic analysis permits the entire network to be built save for these two relations  since the connections between objects and actions are actually syntactic. 
　　when the plot-unit portion of the system is complete  then the next step will be to output a a high-level description of the story in the style suggested by the work of  lchncrt 1 . note that the plot summary is intended to be a computational aid to story retrieval  and is not the central thrust of this work. roughly  plot unit theory is grounded on a  chemical  model of representation. it assumes that the causal relationships in a story can be summarized by extracting out the  molecular structure  of the causal network. this is done by imposing a theory of plot molecular structure onto the more basic causal network. while there is no space to give the details of this procedure here  we simply note that given a network description  we can try to form a corresponding description in terms of  atomic  plot abstraction units like  loss  or  gain   an agent experiences something negative or positive   this exercise is far from easy  however  and the rules for carrying out this translation 
are still under development. 
　　since the plot-unit step is still under development  it will not be further described here. instead  what is currently done is to simply retrieve descriptions of a   stories in the shakespeare precis database. in our running example  these are the stories of macbeth  hamlet  julius caesar  and the taming of the shrew. 
   what next  the learning procedure takes the the causal network descriptions of each of the candidate stories and run the analogy matching program  pairing objects and relations in the new macbeth story against objects and relations in each of the candidates until the best match is obtained. as mentioned  we restrict our attention to just that portion of the network that is immediately connected to the causal links emanating from  assassinate . this means that distantly related events and relations are not brought in for matching at all.  this metric of locality could of course be modified  but seems a reasonable first stab at a way to fix local context.  the matching program as designed can be primed to consider only matches that keep intact the objects on either end of the unknown  assassinate  link  this means that the match of  macbeth  in macbeth story #1 against  duncan  in macbeth story #1 is not even attempted  since this would be incompatible with the direction of the  murder  and  assassinate  links . 
   the match scores obtained are as follows. ma-1 is the new macbeth story. 
	ma 	ham 	jul 	shr 
	ma-1 	1 	1 	1 	1 
not surprisingly  ma gets the highest match score  with the algorithm mating  assassinate  in ma-1 to  murder  in ma but the algorithm also pairs  assassinate  in ma-1 with  kill  in hamlet and julius caesar  demonstrating that it would not be necessary to have an absolutely identical story in order to extract a useful word meaning. the algorithm weds  assassinate  to  love  in the laming of the shrew  but this receives a low match score. 
　　next we modify the causal network description associated with  assassinate  according to the best match story. any relations that locally associated with  murder  in the ma-1 network are added to the ma-1 description. in our example  this means that the  duncan hq dead  and  macbeth hq evil  links will be added to the ma-1 network. this modification corresponds to the assumption that since  assassinate  has been deduced as most similar to  murder   the inferences following from  murder  will also be true of  assassinate.   this of course is only approximately so  and later story examples where the target of assassination survives -- if explicitly indicated in the story -- would modify this link.  
　　finally  the  meaning  of the novel verb is stored by associating with the word entry  i  a pointer to its causal network description; and  ii  a list of  nearest neighbor  words  according to the results of the matching algorithm. the similarity metric is normalized by the value obtained from matching an identical story against itself. in the case of multiple word matches  as with  kill   the best match word in both hamlet and julius caesar   the highest scoring value is used. in the example above  the new entry for  assassinate  includes the related words of  murder   score .1 ;  kill   .1   and  love   .1 .  the score for ma is not 1 because the  hq dead  and  iiq evil  relations associated with  murder  arc not present in the initial ma-1 description.  the program also adds difference pointers indicating why the stored causal network description is different from each of the network descriptions associated with these three words. in the case at hand  there are no such differences  but if the best match story was  say  about a general homicide  then the  ako  links would indicate that the characters in the  assassinate  story were all a-kind-of political figures  whereas those in the general homicide story need not be. difference pointers could then be used to encode some of the distinctions between  murder  and  assassinate.  
ill open problems 
   in this short paper  many important topics currently under investigation could not be covered. these include: 
 - better generalization schemes arc required. currently  all details about a related story are kept. the only possible generalization is via the  ako  hierarchy that drives the formation of difference pointers between word networks. this means that the choice of ako vocabulary is crucial  since if the category'  noble  is not known then no generalizations about the category  noble  is possible. a better way of creating generalized categories out of old 
	r. berwick 	1 
ones is needed  perhaps based on the work of  keil 1  on the development 
of semantic categories. 
-- plot unit summarization must be added to reduce the computational burden of matching against all known stories. 
-- the use of causal network descriptions of stories to summarize world knowledge must be carefully examined. 
