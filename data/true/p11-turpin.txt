several recent studies have demonstrated that the type of improvements in information retrieval system effectiveness reported in forums such as sigir and trec do not translate into a benefit for users. two of the studies used an instance recall task  and a third used a question answering task  so perhaps it is unsurprising that the precision based measures of ir system effectiveness on one-shot query evaluation do not correlate with user performance on these tasks. in this study  we evaluate two different information retrieval tasks on trec web-track data: a precision-based user task  measured by the length of time that users need to find a single document that is relevant to a trec topic; and  a simple recall-based task  represented by the total number of relevant documents that users can identify within five minutes. users employ search engines with controlled mean average precision  map  of between 1% and 1%. our results show that there is no significant relationship between system effectiveness measured by map and the precision-based task. a significant  but weak relationship is present for the precision at one document returned metric. a weak relationship is present between map and the simple recall-based task.
categories and subject descriptors
h.1  information storage and retrieval : miscellaneous; d.1  software engineering : metrics-complexity measures  performance measures
general terms
performance  design  experimentation  human factors

 supported in part by a discovery project grant from the australian research council.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
keywords
search engines  information retrieval evaluation  user study
1. introduction
　the field of information retrieval has a well-established tradition of experimental evaluation  dating back to cleverdon's  cranfield  experiments   and continuing through the ongoing series of text retrieval conferences  trec  1. the general approach for evaluating adhoc retrieval  where a static collection is searched for documents that are relevant to previously unknown topic  requires: a collection of documents that is to be searched; a set of queries that represent user information needs and are run against the collection; and a set of relevance judgements that indicate  for each query  which documents satisfy the current information need and which do not. evaluations are typically run as a batch process  where the retrieval system fetches a pre-specified number of answer documents for each query  with no user interaction. performance is quantified using a variety of metrics derived from the number of relevant answers that have been found. commonly reported measures include mean average precision  map   precision at 1 documents retrieved  p 1   and bpref  these metrics are defined in section 1 . indeed  much ir research focuses on demonstrating improvements in these metrics.
　however  recent studies have demonstrated that improvements in these metrics do not translate into a direct benefit for users. a study by hersh et al.  shows that instance recall - where users try to identify different aspects of a question within a limited timeframe - does not improve with small increases in mean average precision of the underlying search system on the scale that is commonly reported in ir results. allan et al.  confirm this result  using bpref   but also show that for larger  specific increases in bpref  users do benefit on the instance recall task. turpin and hersh  demonstrate a lack of improvement when users are engaged in a question answering task for a small number of questions. a possible reason for the lack of correlation between underlying system effectiveness and user performance could be the nature of the search tasks that have been examined. instance recall - as its name implies - is inherently recalloriented  1  1 . however  mean average precision  while including a recall component  evaluates systems predominantly using precision  1  1 . similar observations hold true for metrics such as p 1. a question answering task introduces additional complications into the retrieval process: users not only need to identify relevant documents  but are also required to extract specific factoids to answer detailed questions. this introduces additional cognitive load on the user that may be not be reflected in document level relevance judgements  or by extension map calculations.
　we investigate user performance based on two much simpler tasks. the first is a precision-oriented task  requiring users to find one document that is relevant to the supplied query. such a search task may be expected to more closely reflect the system effectiveness that metrics such as map and bpref are measuring: recent research has demonstrated that users focus on the top ranked answers when looking at a ranked list of search results . since users only need to find one document  the position of relevant documents in the answer list as captured in the map metric would intuitively seem important. the second task that we consider is a simple recall-based task  measured by the number of relevant documents that users can identify in a five minute time period. this is simpler than the previously investigated instance-recall tasks  in that we do not require users to find novel information with each document discovered; different documents  that could repeat previously discovered information  are possible answers. although a lack of correlation between system effectiveness metrics and user performance for more complex search tasks has been demonstrated  we wish to investigate whether a relationship exists between the metrics and simpler search tasks  in particular  tasks that typify the millions of searches conducted on the web each day.
　we have modelled our user interface on the interfaces of popular web search engines such as google  yahoo or msn. by presenting users with a web search engine interface  we hope to examine whether the batch precision measures predict user performance on a simple web search task.
　in our experiments  users were required to find documents that were relevant to a query in a short amount of time. the effectiveness of ranked lists for users was controlled using map  so we could measure user performance as a function of effectiveness. in all of our experiments and analysis  however  we could find no correlation between system performance measured with map and user performance on the precision task  and only a negligible improvement in performance on the recall task when map is increased.
　related work  including previous user-studies and details of ir system effectiveness metrics  is reviewed in section 1. we then provide details of our experimental setup including the search task  collections and topics  in section 1. results are presented in sections 1 and 1. we discuss our results in section 1  and present conclusions in the final section.
1. related work
　information retrieval has a strong history of experimental evaluation. two main methodologies are batch processing evaluation  and user-based evaluation.
batch processing retrieval evaluation
in the adhoc or batch processing paradigm  1  1   a set of queries is run against a static collection of documents. the task of a retrieval system is to identify those documents in the collection that are relevant to the query. for evaluation purposes  relevance judgements are used to determine which documents are correct answers  and which are not. that is  a human manually examines each answer that a retrieval system returns  and decides whether the document is relevant for the query.
　to enable the comparison of different retrieval systems  various system effectiveness metrics have been proposed. most metrics are based on two properties of the answer set: precision  which focuses on how early in the ranking relevant documents are returned  and is defined as the number of relevant and retrieved documents as a proportion of the total number of retrieved documents; and recall  which is concerned with the completeness of the answer set  and is defined as the number of relevant and retrieved documents as a proportion of the total number of relevant documents in the collection.
　mean average precision  map  is one of the most widelyused system metrics  and gives a single numerical figure to represent system effectiveness . average precision for a single query is calculated by taking the mean of the precision scores obtained after each relevant document is retrieved  with relevant documents that are not retrieved receiving a precision score of zero. map is then the mean of average precision scores over a set of queries. map is a popular metric  and has been shown to be stable both across query set size  and variations in relevance judgements .
　map assumes that complete relevance information is available - that is  for each query  every document in the collection is examined and evaluated as being relevant or not relevant. as collection sizes continue to increase  obtaining complete relevance information becomes problematic. trec uses a pooling approach  where only those documents that are returned as possible answers to a query by participating systems are manually evaluated - all other documents in the collection are assumed to be not relevant. while some relevant documents may thus remain unidentified  this approach has been demonstrated to work effectively for the comparison of different retrieval systems . as collection sizes continue to increase  however  the proportion of unjudged documents also increases  introducing a risk that a significant number of relevant documents remains unidentified. to overcome this problem  buckley and voorhees have recently proposed the bpref measure . this measure only uses information from judged documents  and is a function of how frequently relevant documents are retrieved before non relevant documents. for evaluations with complete relevance information  bpref and map are strongly correlated .
　precision can also be calculated at particular cutoff points in the ranked list of answers that is returned by a retrieval system. precision at 1 documents retrieved  p 1  is obtained by calculating the precision of a result set considering only the first 1 items in the ranked list. p 1 is a popular measure because it reflects the default number of answers that are returned on a single result page by popular web search engines. precision at 1 document retrieved  p 1  is calculated based only on the relevance of the first item in the answer list.
user-based retrieval evaluation
the user-based evaluation of retrieval systems is complementary to the batch processing approach; here the focus is generally on the end users of retrieval systems. the evaluation of users as they perform search tasks has been studied as part of the trec conferences  first in a dedicated interactive track  1  1   and later in interactive  sub-tracks  . we focus here on studies that have investigated the relationship between batch processing metrics and usability as demonstrated by users engaged in different search tasks.
　hersh et al. investigated whether batch and user evaluations give the same results for an instance recall task . for this type of search task  users are required to find and mark documents that contain as many different instances about a topic as possible. for example  for a topic  dangerous wildlife in africa   users would need to identify documents that mention as many different types of dangerous african wildlife as they can. users were presented with search results from two systems: a baseline system with a map of 1  and an improved search system with a map of 1. despite the fact that the difference in map between the systems was statistically significant  there was no evidence of a corresponding difference in user performance.
　allan et al.  investigated performance for an instance recall task at the passage level; that is  users were required to identify particular passages in documents that are relevant to a topic. in contrast to the experiments of hersh et al.  answer lists were artificially created at different levels of system quality  as measured by the bpref measure. this enables the comparison of user performance across a large range of underlying system effectiveness levels - users were presented with lists that had a bpref in the range from 1 to 1. allan et al. found that different levels of bpref can have a statistically significant effect on user performance  but only at certain ranges of bpref level. in particular  recall  normalized by the time taken to find the answers  is significant only between system bpref of 1 to 1   hard  topics   and between 1 and 1   easy  topics . for the intermediate ranges  there is no relationship between user performance and bpref.
　the relationship between system effectiveness and user performance for a question answering task was considered by turpin and hersh . in contrast to an instance recall task  here users were required to identify a number of factoid answers to a question  or to choose a correct response from two possible answers . two search systems  with map scores of 1 and 1  respectively  were evaluated; no significant improvement in user performance for the question answering task was observed.
　while the hersh et al. study showed no correlation between user instance recall and system map for low map values  typical of those reported in ir studies   the allan et al. study showed that some correlation was present for higher map values. the turpin and hersh study showed no correlation between user's ability to answer questions with systems of differing map at low values  but it is unknown what happens on a question answering task  or a simple informational web search   when the effectiveness of the retrieval system increases to higher levels of map. we attempt to address this issue in this study.
1. methods
　the lack of evidence for differences in effectiveness at the user level based on difference in system effectiveness as measured by metrics such as map and bpref is of concern  as ultimately it is end users that retrieval systems aim to satisfy. however  previous experiments have focused on user search tasks that may promote aspects of searcher behaviour
 num  number: 1
 title  is the world going to end 1
 desc  description:
identify individuals or groups predicting the end of the world in the year 1.
 narr  narrative:
