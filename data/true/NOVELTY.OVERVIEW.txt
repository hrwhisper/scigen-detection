the novelty track was first introduced in trec 1. given a trec topic and an ordered list of documents  systems must find the relevant and novel sentences that should be returned to the user from this set. this task integrates aspects of passage retrieval and information filtering. this year  rather than using old trec topics and documents  we developed fifty new topics specifically for the novelty track. these topics were of two classes:  events  and  opinions . additionally  the documents were ordered chronologically  rather than according to a retrieval status value. there were four tasks which provided systems with varying amounts of relevance or novelty information as training data. fourteen groups participated in the track this year.
1 introduction
the novelty track was introduced as a new track last year . the basic task is as follows: given a topic and an ordered set of relevant documents segmented into sentences  return sentences that are both relevant to the topic and novel given what has already been seen. this task models an application where a user is skimming a set of documents  and the system highlights new  on-topic information.
　there are two problems that participants must solve in the novelty track. the first is identifying relevant sentences  which is essentially a passage retrieval task. sentence retrieval differs from document retrieval because there is much less text to work with  and identifying a relevant sentence may involve examining the sentence in the context of those surrounding it. we have specified the unit of retrieval as the sentence in order to standardize the task across a variety of passage retrieval approaches  as well as to simplify the evaluation.
　the second problem is that of identifying those relevant sentences that contain new information. the operational definition of  new  is information that has not appeared previously in this topic's set of documents. in other words  we allow the system to assume that the user is most concerned about finding new information in this particular set of documents  and is tolerant of reading information he already knows because of his background knowledge. since each sentence adds to the user's knowledge  and later sentences are to be retrieved only if they contain new information  novelty retrieval resembles a filtering task.
　to allow participants to focus on the filtering and passage retrieval aspects separately  this year the track offered four tasks. the base task was to identify all relevant and novel sentences in the documents. the other tasks provided varying amounts of relevant and novel sentences as training data. some groups which chose to focus on passage retrieval alone did only relevant sentence retrieval in the first task.
1 input data
last year  the track used 1 topics from trecs 1  1  and 1  along with relevant documents in rank order according to a top-performing manual trec run. the assessors' judgments for those topics were remarkable in that almost no sentences were judged to be relevant  despite the documents themselves being relevant. as a consequence  nearly every relevant sentence was novel. this was due in large part to assessor disagreement  the assessors were not the original topic authors  and drift  the document judgments were all made several years ago .
　to both solve the assessor drift problem and to achieve greater redundancy in the test data  this year we constructed fifty new topics on a collection of three contemporaneous newswires. for each topic  the assessor composed the topic  selected 1 relevant documents by searching the collection  and labeled the relevant and novel sentences in the documents.
as an added twist  1 of the topics concerned events such as the bombing at the 1 olympics in atlanta  while the remaining topics focused on opinions about controversial subjects such as cloning  gun control  and same-sex marriages. the topic type was indicated in the topic description by a  toptype tag. the documents for the novelty track were taken from the aquaint collection. this collection is unique in that it contains three news sources from overlapping time periods: new york times news service  jun 1 - sep 1   ap  also jun 1 - sep 1   and xinhua news service  jan 1 - sep 1 . we intended that this collection would exhibit greater redundancy and thus less novel information  increasing the realism of the task. the assessors  in creating their topics  searched the aquaint collection using webprise  nist's ir system  and collected 1 documents which they deemed to be relevant to the topic.
　once selected  the documents were ordered chronologically.  chronological ordering is achieved trivially in the aquaint collection by sorting document ids.  this is a significant change from last year's task  in which they were ordered according to retrieval status value in a particular trec ad hoc run. last year's ordering was motivated by the idea of seeking novel information in a ranked list of documents  whereas this year  the task more closely resembles reading new documents over time. this approach seems to make more sense when working with news articles  since background information tends to occur more completely in earlier articles and is summarized more briefly as time goes on and new information is reported. with relevance ranking  one can identify novel sentences but there is no sense of which document should come first.
　the documents were then split into sentences  each sentence receiving an identifier  and all sentences were concatenated together to produce the document set for a topic.
1 task definition
this year  there were four tasks:
task 1. given the set of 1 relevant documents for the topic  identify all relevant and novel sentences.  this was the same as last year's task. 
task 1. given the relevant sentences in all 1 documents  identify all novel sentences.
task 1. given the relevant and novel sentences in the first 1 documents only  find the relevant and novel sentences in the remaining 1 documents.
task 1. given the relevant sentences from all 1 documents  and the novel sentences from the first 1 documents  find the novel sentences in the last 1 documents.
　these four tasks allowed the participants to test their approaches to novelty detection given different levels of training: none  partial  or complete relevance information  and none or partial novelty information. participants were provided with the topics  the set of sentence-segmented documents  and the chronological order for those documents. for tasks 1  training data in the form of relevant and novel  sentence qrels  were also given. the data were released and results were submitted in stages to limit  leakage  of training data between tasks. depending on the task  the system was to output the identifiers of sentences which the system determined to contain relevant and/or novel relevant information.
1 evaluation
1 creation of truth data
judgments were created by having nist assessors manually perform the task. from the concatenated document set  the assessor selected the relevant sentences  then selected those relevant sentences that were novel. each topic was independently judged by two different assessors  the topic author and a  secondary  assessor  so that the effects of different human opinions could be assessed.
1 analysis of truth data
since the novelty task requires systems to automatically select the same sentences that were selected manually by the assessors  it is important to analyze the characteristics of the manually-created truth data in order to better understand the system results. in particular  there were several concerns raised by the peculiarities of last year's data.
1. what percentage of the sentences were marked relevant  and how does this vary across topics and across assessors 
1. did the quantity of relevant and new information improve from last year  in particular  are more sentences relevant  and are fewer relevant sentences novel 

figure 1: percentage of relevant and novel sentences  both primary and secondary assessors   compared to1  both minimum and maximum assessors .
1. how different are the results of the secondary assessor from the primary assessor who authored the topic and selected the documents 
1. is there any difference between  event topics  and  opinion topics   in terms of amounts of relevant and new information 
　table 1 shows the number of relevant and novel sentences selected for each topic by each of the two assessors who worked on that topic. the column marked  assr-1  precedes the results for the primary assessor  whereas  assr-1  precedes those of the secondary assessor. the column marked  rel  is the number of sentences selected as relevant; the next column   %total   is the percentage of the total set of sentences for that topic that were selected as relevant. the column marked  new  gives the number of sentences selected as novel; the next column   %rel   is the percentage of relevant sentences that were marked novel. the column  sents  gives the total number of sentences for that topic  and  type  indicates whether the topic is about an event  e  or about opinions on a subject  o .
　one of the most striking aspects of table 1 is the difference in relevant and new percentages from last year. the median percentage of relevant sentences is 1%  compared with about 1% last year. for novel sentences  the median is 1%  compared with 1% last year. figure 1 illustrates the range of relevant and novel sentences  and compares it to the 1 data. whereas last year  almost no sentences were selected as relevant  and as a result nearly every relevant sentence was novel  this year the distributions of relevant and novel sentences are much more reasonable.
　the analysis of assessor effects is complicated by the fact that only four of the seven assessors  b  c  d  and e  acted as both primary and secondary assessors. assessor a only judged as a primary assessor  and assessors f and g only judged as secondary assessors  i.e.  they judged other assessors topics  but did not author their own .
　as we might expect  there is a large effect from the assessors. for relevant sentence selection  this effect is more significant than either topic type or judgment round. the four assessors who judged topics in both rounds  b  c  d  and e  were quite different from each other  but judged similarly from the first round to the second. for novel sentences  it's a different story; differences between assessors are more pronounced in the first round  but in the second they are all quite similar to each other. overall  the number of novel sentences selected is more uniform across

table 1: analysis of relevant and novel sentences by topic
topictypesentsassr-1rel%totalnew%relassr-1rel%totalnew%reln1o1a1.11f1.11n1e1d1.11b1.11n1e1c1.11e1.11n1e1b1.11d1.11n1e1b1.11g1.11n1e1b1.11c1.11n1e1e1.11d1.11n1e1d1.11g1.11n1e1b1.11f1.11n1e1c1.11f1.11n1e1c1.11g1.11n1o1c1.11d1.11n1o1a1.11f1.11n1o1d1.11g1.11n1o1c1.11b1.11n1e1e1.11f1.11n1o1b1.11g1.11n1o1b1.11d1.11n1o1d1.11c1.11n1o1d1.11b1.11n1o1e1.11g1.11n1o1d1.11f1.11n1o1e1.11d1.11n1o1e1.11b1.11n1o1d1.11c1.11n1o1c1.11e1.11n1o1c1.11e1.11n1o1b1.11e1.11n1o1d1.11b1.11n1o1c1.11g1.11n1o1c1.11d1.11n1o1b1.11c1.11n1e1c1.11g1.11n1e1e1.11d1.11n1e1e1.11b1.11n1e1c1.11f1.11n1e1d1.11g1.11n1o1d1.11f1.11n1e1b1.11e1.11n1e1b1.11e1.11n1e1c1.11b1.11n1e1c1.11b1.11n1e1e1.11c1.11n1e1c1.11d1.11n1e1e1.11f1.11n1e1e1.11c1.11n1e1c1.11b1.11n1e1d1.11e1.11n1e1d1.11c1.11n1e1e1.11c1.11
assessors than relevant sentences. figure 1 illustrates these differences.
　last year  we found that the assessors tended to pick consecutive groups of sentences as relevant  despite being instructed otherwise. this year  we did not restrict them from selecting consecutive sentences  instead allowing them to select whatever they felt was necessary. as might be expected  this along with the greater amount of relevant sentences chosen resulted in a much higher occurrence of consecutive relevant sentences. on average  1% of relevant sentences were selected immediately adjacent to another relevant sentence. the median length of a string of consecutive relevant sentences was 1; the mean was 1 sentences.
　overall  there was not a large difference between the primary and secondary assessor in terms of the number of relevant and novel sentences selected. figure 1 a  shows that the secondary assessors tended to be a little more restrictive in their judgments  but this difference is not statistically significant. this implies that the marked difference in judgment patterns we see between this year and last is not only due to an assessor effect. having more recent documents and topics  and allowing the assessors to select the relevant documents  probably also played a role.
　there is a larger difference between event and opinion topics. figure 1 b  illustrates this. opinion topics tended to have a lower percentage of relevant and a higher percentage of novel sentences than events. the higher percentage of novel sentences is actually due to the lower percentage of relevant sentences. the difference is statistically significant for relevant sentences  but not for novel ones.
　while it may be the case that having multiple news sources from the same time period increased redundancy over last year's topics  having stories from two or three wires did not make a significant difference in the number of novel sentences. only one topic  1  drew stories from a single news source; all others involved either two or three sources. on average  1% of relevant sentences were novel for topics with two sources  and 1% for those with three. both of these are less than the new percentage for topic 1  1%   but with only one topic we can't make any conclusions.
　to summarize  the topics and judgments are much improved over last year. while there are differences in judging between the two assessment rounds  and between the different topic types  once again differences between assessors are dominant. differences are more marked for relevant sentence selection than for novelty  indicating that there is a real difference between these two tasks.
1 scoring
the sentences selected manually by the nist assessors were considered the truth data. in contrast to last year  where concerns about assessors selecting groups of sentences for context drove the evaluation to use the assessor with the fewest selected relevant sentences  the so-called  minimum assessor    this year the judgments by the topic author were taken as the truth data. the judgments by the secondary assessor were taken as a human baseline performance in the task.
　because relevant and novel sentences are returned as an unranked set in the novelty track  we cannot use traditional measures of ranked retrieval effectiveness such as mean average precision. the track guidelines specified the f measure as the primary evaluation measure for the track. the f measure  from van rijsbergen's e measure  is itself derived from set precision and recall. for the novelty track  the  set  in question is the set of retrieved sentences  rather than documents as in the retrieval case . relevant and novel sentence retrieval are evaluated separately. let m be the number of matched sentences  i.e.  the number of sentences selected by both the assessor and the system  a be the number of sentences selected by the assessor  and s be the number of sentences selected by the system. then sentence set recall is m/a and precision is m/s.
　as previous filtering tracks have demonstrated  setbased recall and precision do not average well  especially when the assessor set sizes vary widely across topics. consider the following example as an illustration of the problems. one topic has hundreds of relevant sentences and the system retrieves 1 relevant sentence. the second topic has 1 relevant sentence and the system retrieves hundreds of sentences. the average for both recall and precision over these two topics is approximately .1  the scores on the first topic are 1 for precision and essentially 1 for recall  and the scores for the second topic are the reverse   even though the system did precisely the wrong thing. while most real submissions won't exhibit this extreme behavior  the fact remains that recall and precision averaged over a set of topics is not a good diagnostic indicator of system performance. there is also the problem of how to define precision when the system returns no sentences  s = 1 . not counting that question in the evaluation for that run means differ-

 a  relevant sentences

 b  new sentences
figure 1: assessor effects.
 a  primary and secondary assessors

 b  event and opinion topics
figure 1: differences between assessment rounds and topic types.

f  beta=1

figure 1: the f measure  plotted according to its precision and recall components. the lines show contours at intervals of 1 points of f.
ent systems are evaluated over different numbers of topics  while defining precision to be either 1 or 1 is extreme.  the average scores given in appendix a defined precision to be 1 when s = 1 since that seems the least evil choice. 
　to avoid these problems  the primary measure for novelty track runs is the f measure. this measure is a function of set recall and precision  together with a parameter β which determines the relative importance of recall and precision. a β value of 1  indicating equal weight  is used in the novelty track. fβ=1 is given as:
1 〜 p 〜 r
f = 
p + r
alternatively  this can be formulated as
1 〜  # relevant sentences retrieved 
f = 
 # retrieved sentences  +  # relevant sentences 
　for any choice of β  f lies in the range  1   and the average of the f measure is meaningful even when the judgment sets sizes vary widely. for example  the f measure in the scenario above is essentially 1  an intuitively appropriate score for such behavior. using the f measure also deals with the problem of what to do when the system returns no sentences since recall is 1 and the f measure is legitimately 1 regardless of what precision is defined to be.
　note  however  that two runs with equal f scores do not indicate equal precision and recall. figure 1 illustrates the shape of the f measure in precisionrecall space. an f score of 1  for example  can reflect a range of precision and recall scores. thus  two runs with equal f scores may be performing quite differently  and a difference in f scores can be due to changes in precision  recall  or both.
1 participants
table 1 lists the 1 groups that participated in the trec 1 novelty track. all but one group attempted the first task  and nearly every group tried every task. the rest of this section contains short summaries submitted by most of the groups about their approaches to the novelty task. for more details  please refer to the group's complete paper in the proceedings.
　in general  most groups took a similar approach to the problem. relevant sentences were selected by measuring similarity to the topic  and novel sentences by dissimilarty to past sentences. as can be seen from the following descriptions  there is a tremendous variation in how  the topic  and  past sentences  are modeled  and in how similarity is computed when sentences are involved. many groups tried variations on term expansion to improve sentence similarity  some with more success than others.
1 ccs/university of maryland 
table 1: organizations participating in the trec 1 novelty track
runs submittedrun prefixtask 1task 1task 1task 1center for computing science / u. marylandccsum11chinese academy of sciences  cas-ict ict11chinese academy of sciences  cas-nlpr nlpr11cl researchclr11indian institute of technology bombayiitb1institut de recherche en informatique de toulouseirit1lexiclone  inc.lexiclone1meiji universitymeiji11national taiwan universityntu11tsinghua universitythu11university of iowauiowa11university of maryland baltimore countyumbc1university of michiganumich11university of southern california-isiisi1for the 1 duc task of forming a summary based on the relevant and novel sentences  we tested a system based on a hidden markov model  hmm . in this work  we use variations of this system on the tasks of the trec novelty track. our information retrieval system couples a query handler  a document clusterer  and a summary generator with a convenient user interface. our summarization system uses an hmm to find relevant sentences in a document. the hmm has two types of states  corresponding to relevant and non-relevant sentences. the observation sequence scored by the hmm is composed of the number of signature terms and topic terms contained in each sentence. a signature term is a term that statistically occurs more frequently in the document set than in the document collection at large  and a subject term is a signature term which also occurs in the headlines or subject lines of a document. the counts of these terms are normalized within a document to have a mean of zero and variance of one. we determine the relevant sentences in a document based on the hmm posterior probability of each sentence being relevant. in particular  we choose the number of sentences to maximize the expected utility  which for trec is simply the f1 score.
　several methods were explored to find a subset of the relevant sentences that has good coverage but low redundancy. in our multi-document summarization system  we used the qr algorithm on term-sentence matrices. for this work  we explored the use of the singular value decomposition as well as two variants of the qr algorithm.
1 chinese	academy	of	sciences  ict  
the novelty track can be treated as a binary classification problem: relevant sentences vs. irrelevant sentences  or new vs. non-new. in this way  we applied variants of techniques that have been employed for text categorization problem. to retrieve the relevant sentences  we compute the similarity between the topic and sentences using vector space model. the features for each topic are obtained by employing χ1 statistic and each feature is also weighted using the χ1 statistic. if the similarity exceeds a certain threshold  the sentence is considered as relevant. in addition  we try several techniques in an attempt to improve the performance. one is that the narrative section in the topic is analyzed to obtain the negative features and negative vector of the topic. we determine the relevance by adding similarity between the negative vector and sentence as a negative factor.
the second  the threshold for different docs in each topic is dynamically adjusted according to the doc density  rather than fixed in the whole period. we have implemented the knn algorithm and winnow algorithm for classifying the sentences into relevant and irrelevant sentences in the novelty task 1. to detect the new sentences from the relevant sentences  we try several methods  such as maximum marginal relevance  mmr  measure  winnow algorithm and word overlapping within sentences. what's more  we attempt to detect novelty by computing semantic distance between sentences using wordnet.
1 chinese	academy	of	sciences  nlpr  
for finding relevant sentences  we use a new statistical model called  term similarity tree  to make the process of query expansion more flexible and controllable. then  relevant feedback is used for additional modification for queries. serveral different methods for similarity computing are developed to improve the performance. they are  simple window    dynamic window    active window . the key notion is that the window-based method can ensure that the closer the query words in sentences  the higher the similarity value. finally  dynamic thresholds are used for different topics  which usually brings 1% increase of averagef measure. for finding new sentences  we define a value called  new information degree   nid  to present whether a sentence includes new information related to the former sentences. if the value of nid is big  this sentence is reserved  or it will be discard. there are two different ways to define nid of the latter sentence related to the former sentence. one is based on idf value of terms and the other is based on bi-gram sequences.
1 cl research 
the cl research system parses and processes text into an xml representation  tagging the text with discourse  noun  verb  and preposition characteristics. the topic characterizations  titles  descriptions  and/or narratives  and the relevant documents provided by nist were processed in this way. componential analysis of the degree to which topic characterizations corresponded to sentences was used as the basis for determining relevance  using various scoring metrics. similar componential analysis was used to compare each relevant sentence with all those that preceded it in order to assess novelty. several variables were used as the basis for different runs under the different tasks  which also provided prior information that could be exploited   providing useful experimental results that will inform selection among alternatives for approaching the novelty task.
1 irit-sig 
in trec 1  irit improved the strategy that was introduced in trec 1. a sentence is considered as relevant if it matches the topic with a certain level of coverage. this coveragedepends on the category of the terms used in the texts. three types of terms were defined for trec 1 highly relevant  lowly relevant and non-relevant  like stop words . in trec 1 we introduced a new class of terms: highly non-relevant terms. terms from this category are extracted from the narrative parts of the queries that describe what will be a non-relevant document. a negative weight can be assigned to these words.
　with regard to the novelty part  a sentence is considered as novel if its similarity with each of the previously processed and selected-as-novel sentences does not exceed a certain threshold. in addition  this sentence should not be too similar to a virtual sentence made of the n best-matching sentences.
1 university of southern californiaisi
to identify opinion sentences  we used unigrams to indicate subjectivity. in addition to three baseline algorithms  we employed two sets of subjectivityindicating words  either positive or negative valence  with appropriate strengths . one set was collected manually and extended with wordnet synonyms. the other was learned automatically from the wall street journal. the words' relative scores and the algorithm's cutoff parameters were determined in a series of experiments. to our surprise the trec results showed that one of our baselines  indicating that every sentence carries an opinion  actually beat the algorithm using the manually collected words. to identify event sentences  we adopted a standard ir procedure  treating each sentence as a separate document. for each event topic  we used all its non-stop words as query to extract event sentences. again  the cutoff parameter was determined by experiment. we were happy to see that this method worked relatively well.
1 lexiclone 
for the sake of convenience we decided that on the word-per-word level  any language is about 1 percent nouns  1 percent verbs and 1 percent adjectives. except for prepositions  conjunctions  interjections  pronouns and other parts of speech that make up the remaining 1 percent  the rest of the language is a combination of these three dominant elements  or can be reduced to them . lexiclone establishes all possible combinations of nouns  verbs and adjectives for each sentence. we call these combinations  triads .  actually  a triad is a smallest possible  key  phrase from a sentence.  after that we find sentences that have triads.
1 meiji university 
for identifying relevant sentences  we employed following information-filtering-based approach. we regarded sentences as very short documents. initial profiles  which are made from topic descriptions  are expanded conceptually. conceptual fuzzy sets  which we proposed previously  are used for conceptual expansion. if the cosine similarity between the expanded profile and a word vector of each sentence exceeds a threshold  the sentence is regarded as relevant. for identifying new sentences  we considered two measures; sentence score and redundancy score. 1  for calculating a sentence score  we used nwindow-idf as a time window. local sentence score is calculable by using document frequency of past n documents. 1  redundancy score is the maximum value of the similarity with the sentence judged to be novel in the past.
1 national taiwan university 
according to the results of trec 1  we realized the major challenge issue of recognizing relevant sentences is a lack of information used in similarity computation among sentences. in trec 1  ntu attempts to find relevant and novel information based on variants of employing information retrieval  ir  system. we call this methodology ir with reference corpus  which can also be considered an information expansion of sentences. a sentence is considered as a query of a reference corpus  and similarity between sentences is measured in terms of the weighting vectors of document lists ranked by ir systems. basically  we looked for relevant sentences by comparing their results on a certain information retrieval system. two sentences are regarded as similar if they are related to the similar document lists returned by ir system. in novelty parts  similar analysis is used to compare each relevant sentence with all those that preceded it to find out novelty. an effectively dynamic threshold setting which is based on what percentage of relevant sentences is within a relevant document is presented.
1 tsinghua university 
research in ir group of tsinghua university on this year's novelty track mainly focused on four aspects:  1  unsupervised relevance judgment  where qe and pseudo relevance feedback has been used.  1  efficient sentence redundancy computing: we used unsymmetrical sentence  overlap  metric  sub-topic redundancy elimination and sentence clustering.  1  supervised sentence classification  where a svm classifier has been used and got encouraging results;  1  supervised redundancy threshold learning. a new ir system named tminer has been built on which all experiments have been performed.
1 university of iowa 
our approach is basically the same as that used last year. we use new named entity and noun phrase triggering  guarded by a dual threshold of sentence similarity and full-document similarity. if the full document is sufficiently similar and the current sentence is sufficiently similar  the number of newly-detected named entities and noun phrases is compared against a minimum threshold and if the minimum is met  the current sentence is declared to be novel. the named entities used include persons  organizations and place names. relevance is simple term similarity.
1 university of maryland baltimore county 
to find the relevant sentences  we used a method comprising of query expansion and sentence clustering. in the query expansion step  we experimented with two methods  one was to determine highly cooccurring terms by means of a svd analysis and  the other was by determining meaningful terms as obtained by a language analysis of the narrative section for each topic. the sentences  per topic  were clustered and the top clusters were selected based on similarity scores of the cluster centroids and the expanded query. all the sentences from the selected clusters are chosen as the relevant sentences.
　to find the novel sentences  we experimented with two methods. one  based on a text summarization method  was clustering relevant sentences and choosing one sentence each from the selected clusters to make up the set of novel sentences. in the second method  using a sentence-sentence similarity matrix  of relevant sentences   the dissimilarity between sentences was used to determine novel sentences.
1 university of michigan 
first we used the mead summarization software to compute scores for each sentence on features such as length  position  word overlap with query  title and description. since we trained maximum entropy classifiers  these scores were then discretized. once the mead features were calculated  discretized and formatted  we used the maxent-1.1 software to train our models for novel and relevant sentences.
　for tasks 1 and 1  once the maxent models had been trained for classifying novel and relevant sentences and were used to produce a ranked list of sentences as to how likely they were to be novel or relevant  we then chose differing percentage cut offs for each run in an attempt to maximize recall and precision on our devtest data set. for tasks 1 and 1  we noted that the f-measure for a baseline algorithm of submitting all relevant sentences as being novel was quite high. therefore  we focused on trying various discretizations of our feature scores in order to improve the classifier's performance on the devtest set
1 results
figures 1  1  1  and 1 show the average f scores in each task. task 1 scores are shown alongside the  scores  of the secondary assessor  who may be considered to have been performing this task. within the margin of error of human disagreement  these lines can be thought of as representing the best possible performance. the best systems are performing at this level. nine runs have novelty f scores of 1 because those runs did not return any novel sentences.
　tasks 1 and 1 show novelty retrieval performance closely tracking relevant retrieval performance. only a few runs near the bottom of the performance range did better at retrieving novel sentences than relevant ones. this seems somewhat surprising  since while the retrieved set of relevant sentences places a bound on recall for the novel set  since only retrieved sentences can be labeled novel   any level of precision is possible  and thus there isn't any reason why fnovel shouldn't exceed frelevant. however  to achieve this most systems would have had to make a very large improvement in precision when retrieving novel sentences.
　as stated previously  sometimes it can be hard to understand what the f score means in terms of the actual behavior of each run. figure 1 shows the f scores for task 1  along with each run's corresponding average recall and precision. note for example the run isiall1  run #1 on the x axis   which retrieved only relevant sentences  and retrieved all of them; for this run  averagerecall was 1 but precision was 1. it is very interesting to note that average recall seems to correlate more closely to the f scores  although f is defined to be a harmonic mean between the two. this may mean that within each run  recall was more consistent across topics than was precision. the scores for tasks 1 show how many of the systems can take advantage of training data  both for relevance and novelty. comparing the graph of tasks 1 and 1  we can see that having more relevance information dramatically improves novelty retrieval effectiveness. moreover  comparing tasks 1 and 1  we can see that having relevant sentences is more valuable than having novel sentences for training  since the top systems do not improve from task 1 to task 1.
　the graphs for tasks 1 and 1 compare the runs against a baseline system which merely returns all the relevant sentences  provided as training data in these tasks  as novel. the best systems are performing above this baseline  indicating that they are being somewhat selective in what they return as novel.
　event topics were easier than opinion topics. figure 1 illustrates this phenomenon in task 1. relevant sentence retrieval scores are on the left  novelty retrieval scores on the right. the graphs show the overall average along with the averages for event and opinion topics for each run. nearly every run did better at events than opinions; the exceptions are umbc and ntu for relevant sentences  and ntu and one irit run for novel sentences.
　as the systems receive more relevant sentences as training data  they improve on opinion topics. in task 1  where systems received some relevant and novel training data   all systems perform as well or better on event topics than on opinions. however  in tasks 1 and 1  where the systems receive complete relevance information  the situation is reversed: all systems do better on opinion topics. clearly  the systems are less able to identify relevant sentences in opinion topics  but if they know which ones are relevant  they do better on opinion topics than on events. having a small amount of relevant sentence training data  as in task 1  is not sufficient to boost a system's overall performance.
