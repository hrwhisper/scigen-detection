we propose an affine extension to adds  aadd  capable of compactly representing context-specific  additive  and multiplicative structure. we show that the aadd has worstcase time and space performance within a multiplicative constant of that of adds  but that it can be linear in the number of variables in cases where adds are exponential in the number of variables. we provide an empirical comparison of tabular  add  and aadd representations used in standard bayes net and mdp inference algorithms and conclude that the aadd performs at least as well as the other two representations  and often yields an exponential performance improvement over both when additive or multiplicative structure can be exploited. these results suggest that the aadd is likely to yield exponential time and space improvements for a variety of probabilistic inference algorithms that currently use tables or adds.
1 introduction
algebraic decision diagrams  adds   provide an efficient means for representing and performing arithmetic operations on functions from a factored boolean domain to a real-valued range  i.e.  bn ↙ r . they rely on two main principles to do this:
1. adds represent a function bn ↙ r as a directed acyclic graph - essentially a decision tree with reconvergent branches and real-valued terminal nodes.
1. adds enforce a strict variable ordering on the decisions from the root to the terminal node  enabling a minimal  canonical diagram to be produced for a given function. thus  two identical functions will always have identical add representations under the same variable ordering.
﹛as shown in figure 1  adds often provide an efficient representation of functions with context-specific independence   such as functions whose structure is conjunctive  1a  or disjunctive  1b  in nature. thus 

figure 1: some example adds showing a  conjunctive structure  f = if a ＿  b and＿ c 1c  1 additive  b  disjunctive f = 1a +structure1b + c 
 f = if a ˍ b ˍ c 1   1a+1b+c and multiplicative  f = 污   structure  top and bottom sets of terminal values  respectively . the high  true  edge is solid  the low  false  edge is dotted.
adds can offer exponential space savings over a fully enumerated tabular representation. however  the compactness of adds does not extend to the case of additive or multiplicative independence  as demonstrated by the exponentially large representations when this structure is present  1c . unfortunately such structure often occurs in probabilistic and decision-theoretic reasoning domains  potentially leading to exponential running times and space requirements for inference in these domains.
1 affine algebraic decision diagrams
to address the limitations of adds  we introduce an affine extension to the add  aadd  that is capable of canonically and compactly representing context-specific  additive  and multiplicative structure in functions from bn ↙ r. we define aadds with the following bnf:
	g	::=	c + bf
	f	::=	1 | if fvar ch + bhfh cl + blfl 
here  ch and cl are real  or floating-point  constants in the closed interval  1   bh and bl are real constants in the half-open interval  1   fvar is a boolean variable associated with f  and fl and fh are non-terminals of type f. we also impose the following constraints:

figure 1: portions of the adds from figure 1c expressed as generalized aadds. the edge weights are given as hc bi.
1. the variable fvar does not appear in fh or fl.
1. min ch cl  = 1
1. max ch + bh cl + bl  = 1
1. if fh = 1 then bh = 1 and ch   1. similarly for fl.
1. in the grammar for g  we require that if f = 1 then b = 1  otherwise b   1.
expressions in the grammar for f will be called normalized aadds and expressions in the grammar for g will be called generalized aadds.1
﹛let v al f 老  be the value of aadd f under variable value assignment 老. this can be defined recursively by the following equation:

lemma 1. for any normalized aadd f we have that v al f 老  is in the interval  1   min老 v al f 老  = 1  and if f =1 then max老 v al f 老  = 1.
﹛we say that f satisfies a given variable ordering if f = 1 or f is of the form if fvar ch + bhfh cl + blfl  where fvar does not occur in fh or fl and fvar is the earliest variable under the given ordering occuring in f. we say that a generalized aadd of form c+bf satisfies the order if f satisfies the order.
lemma 1. fix a variable ordering. for any nonconstant function g mapping bn  ↙ r  there exists a unique generalized aadd g satisfying the given variable ordering such that for all 老 ﹋ bn we have g 老  = v al g 老 .
﹛both lemmas can be proved by straightforward induction on n. the second lemma shows that under a given variable ordering  generalized aadds are canonical  i.e.  two identical functions will always have identical aadd representations.
﹛as an example of two aadds with compact additive and multiplicative structure  figure 1 shows portions of

figure 1: two aadd nodes f1 and f1 and a binary operation op with the corresponding notation used in this paper.
the exponentially sized adds from figure 1c represented by generalized aadds of linear size.
﹛we let op denote a binary operator on aadds with possible operations being addition  substraction  multiplication  division  min  and max denoted respectively as   a      r   min ﹞ ﹞   and max ﹞ ﹞ .
1 related work
there has been much related work in the formal verification literature that has attempted to tackle additive and multiplicative structure in representation of functions from bn ↙ bm. these include *bmds  k*bmds  evbdds  fevbdds  hdds  and *phdds . while space limitations prevent us from discussing all of the differences between aadds and these data structures  we note two major differences: 1  these data structures have different canonical forms relying on gcd factorization and do not satisfy the invariant property of aadds that internal nodes have range  1 . 1  the fact that these data structures have integer terminals  bm  requires a rational or direct floating-point representation of values in r. in our experience  this renders them unusable for probabilistic inference due to considerable numerical precision error and computational difficulties.
1 algorithms
we now define aadd algorithms that are analogs of those given by bryant 1 except that they are extended to propagate the affine transform of the edge weights on recursion and to compute the normalization of the resulting node on return.
﹛all algorithms rely on the helper function getgnode given in algorithm 1 that takes an unnormalized aadd node of the form if v ch+bhfh cl+blfl  and returns the cached  generalized aadd node of the form hcr+brfri.1
1 reduce
the reduce algorithm given in algorithm 1 takes an arbitrary ordered aadd  normalizes and caches the internal nodes  and returns the corresponding generalized aadd. one nice property of the reduce algorithm is that one does not need to prespecify the structure that
algorithm 1: getgnode v hch bh fhi hcl bl fli   ↙ hcr br fri
input	: v hchnode bh fidhifor hclhigh/low bl fli : branchesvar  offset  mult  and
output : hmultiplier cr br fri :andreturncanonicalvaluesnodefor offset id
the aadd should exploit. if the represented function contains context-specific  additive  or multiplicative independence  the reduce algorithm will compactly represent this structure uniquely and automatically w.r.t. the variable ordering as guaranteed by previous lemmas.
1 apply
the apply routine given in algorithm 1 takes two generalized aadd operands and an operation as given in figure 1 and produces the resulting generalized aadd. the control flow of the algorithm is straightforward: we first check whether we can compute the result immediately  otherwise we normalize the operands to a canonical form and check if we can reuse the result of a previously cached computation. if we can do neither of these  we then choose a variable to branch on and recursively call the apply routine for each instantiation of the variable. we cover these steps in-depth in the following sections.
terminal computation
the function computeresult given in the top half of table 1  determines if the result of a computation can be immediately computed without recursion. the first entry in this table is required for proper termination of the algorithm as it computes the result of an operation applied to two terminal 1 nodes. however  the other entries denote a number of pruning optimizations that immediately return a node without recursion. for example  given the operation h1f1ih1f1i  we can immediately return the result h1 + 1i.
recursive computation
if a call to apply is unable to immediately compute a result or reuse a previously cached computation  we must recursively compute the result. for this we have two
algorithm 1: reduce hc b fi   ↙ hcr br fri
input	: hc b fi : offset  multiplier  and node id output : hcr br fri : return values for offset  multiplier  and node id
cases  the third case where both operands are 1 terminal nodes having been taken care of in the previous section :
f1 or f1 is a 1 terminal node  or: we assume the operation is commutative and reorder the operands so that f1 is the 1 node or the operand whose variable comes later in the variable ordering.1 then we propagate the affine transform to each of f1's branches and compute the operation applied separately to f1 and each of f1's high and low branches. we then build an if statement conditional on normalize it to obtain the generalized aadd node hcr br fri for the result:
hch bh fhi = apply hc1 b1 f1i hc1 + b1 h b1 h f1 hi op 
hcl bl fli = apply hc1 b1 f1i hc1 + b1 l b1 l f1 li op 
hcr br fri = getgnode f1var hch bh fhi hcl bl fli 
f1 and f1 are non-terminal nodes and:
since the variables for each operand match  we know the result hcr br fri is simply a generalized if statement branching on  with the true case being the operator applied to the high branches of f1 and f1 and likewise for the false case and the low branches:

canonical caching
if the aadd apply algorithm were to compute and cache the results of applying an operation directly to the operands  the algorithm would provably have the same time complexity as the add apply algorithm. yet  if we were to compute h1f1ih1f1i and cache the result hcr + brfri  we could compute h1 + 1i  h1 + 1i = h 1cr + 1  + 1brfri without recursion.
﹛this suggests a canonical caching scheme that normalizes all cache entries to increase the chance of a cache hit. the actual result can then be easily computed from the cached result by reversing the normalization. this ensures optimal reuse of the apply operations cache and can lead to an exponential reduction in running time over the non-canonical caching version.
﹛we introduce two additional functions to perform this caching: getnormcachekey to compute the canonical cache key  and modifyresult to reverse the normalization in order to compute the actual result. these algorithms are summarized in the bottom half of table 1.
1 other operations
while space limitations prevent us from covering all of the operations that can be performed  efficiently  on aadds  we briefly summarize some of them here: min and max computation: the min and max of a generalized aadd node hc + bfi are respectively c and c + b due to  1  normalization of f.
restriction: the restriction of a variable xi in a function to either true or false  i.e. f|xi=t/f  can be computed by returning the proper branch of nodes containing that test variable during the reduce operation.
sum out/marginalization: a variable xi can be summed  or marginalized  out of a function f simply the sum of the restricted functions  i.e. xi=f .
negation/reciprocation: while it may seem that negation of a generalized aadd node hc + bfi would be as simple as h c +  bfi  we note that this violates our normalization scheme which requires b   1. consequently  negation must be performed explicitly as 1 a hc + bfi. likewise  reciprocation  i.e.   must be performed explicitly as 1  r  hc + bfi.
variable reordering: a generalization of rudell's add variable reordering algorithm  that recomputes edge weights of reordered nodes can be applied to aadds without loss of efficiency.
1 cache implementation
if one were to use a naive cache implementation that relied on exact floating-point values for hashing and equality testing  one would find that many nodes which should be the same under exact computation often turn out to have offsets or multipliers differing by ㊣1e-1 due to numerical precision error. this can result in an exponential explosion of nodes if not controlled. consequently  it is better to use a hashing scheme that considers equality within some range of numerical precision error 1. while
algorithmcr br fri	1:	apply hc1 b1 f1i hc1 b1 f1i op   ↙
h
input	: hc1 b1 f1i hc1 b1 f1i op : nodes and op output : hcr br fri : generalized node to return begin
 ;
it is difficult to guarantee such an exact property for an efficient hashing scheme  we next outline an approximate approach that we have found to work both efficiently and nearly optimally in practice.
﹛the node cache used in getgnode and the operation result cache used in apply both use cache keys containing four floating-point values  i.e.  the offsets and multipliers
computeresult hc1 b1 f1i hc1 b1 f1i op   ↙ hcr br frioperation and conditionsreturn valuehc1 + b1i hopi hc1 + b1i; f1 = f1 = 1h c1 hopi c1  + 1 ﹞ 1imax hc1 + b1i hc1 + b1i ; c1 + b1 ≒ c1hc1 + b1imax hc1 + b1i hc1 + b1i ; c1 + b1 ≒ c1hc1 + b1ihc1 + b1i  hc1 + b1i; f1 = f1h c1 + c1  +  b1 + b1 f1imax hc1 + b1i hc1 + b1i ; f1 = f1   c1 ≡ c1 ＿ b1 ≡ b1  ˍ  c1 ≡ c1 ＿ b1 ≡ b1 c1 ≡ c1 ＿ b1 ≡ b1 : hc1 + b1i c1 ≡ c1 ＿ b1 ≡ b1 : hc1 + b1inote: for all max operations above  return opposite for minhc1 + b1i hopi hc1 + b1i; f1 = 1 op ﹋ { a}h c1 hopi c1  + b1ihc1 + b1i hopi hc1 + b1i; f1 = 1 c1 ≡ 1 op ﹋ {   r }h c1 hopi c1  +  b1 hopi c1 f1inote: above two operations can be modified to handle f1 = 1 when op ﹋ {  }othernulland modifyresult hcr br fri   ↙ hcr1  br1  fr1ioperation and conditionsnormalized cache key and computationresult modificationhc1 + b1i  hc1 + b1i; f1 =1hcr + brfri = h1 + 1i  h1 +  b1/b1 f1ih c1 + c1 + b1cr  + b1brfrihc1 + b1i a hc1 + b1i; f1 =1hcr + brfri = h1 + 1i a h1 +  b1/b1 f1ih c1   c1 + b1cr  + b1brfrihc1 + b1i   hc1 + b1i; f1 =1hcr + brfri = h c1/b1  + f1i   h c1/b1  + f1ihb1cr + b1brfrihc1 + b1i  r  hc1 + b1i; f1 =1hcr + brfri = h c1/b1  + f1i  r  h c1/b1  + f1ih b1/b1 cr +  b1/b1 brfrimax hc1 + b1i hc1 + b1i ; f1 =1  note: same for minhcr + brfri = max h1 + 1i h c1   c1 /b1 +  b1/b1 f1i h c1 + b1cr  + b1brfriany hopi not matching above: c1 + b1 op c1 + b1hcr + brfri = hc1 + b1i hopi hc1 + b1ihcr + brfrih	i h	i h	i
table 1: input and output summaries of the computeresult  getnormcachekey  and modifyresult routines.for two aadd nodes .	if we consider this 1-tuple of floating-point values to be a point in euclidean space  then we can measure the error between two 1-tuples hu1 u1 u1 u1i and hv1 v1 v1 v1i as the l1  euclidean  distance between these points. as a consequence of the  know that
.
this is shown graphically for two dimensions in figure 1.
﹛based on this inequality  we can use the following approximate hashing scheme for hv1 v1 v1 v1i: compute the l1 distance between hv1 v1 v1 v1i and the origin; for hashing  extract only the bits from the floating-point base representing a fractional portion greater than 1 and use this as the integer representation of the hash key  we are effectively discretizing the distances into buckets of width 1 ; for equality  test that the true l1 metric between hu1 u1 u1 u1i and hv1 v1 v1 v1i is less than 1. while this hashing scheme does not guarantee that all 1-tuples having distance from the origin h1 1i within 1 hash to the same bucket  some 1-tuples within 1 could fall over bucket boundaries   we found that with bucket width 1 = 1e-1 and numerical precision error generally less than 1e-1  there was only a small probability of two nodes equal within numerical precision straddling a bucket boundary. for the empirical results described in section 1  this hashing scheme was sufficient to prevent any uncontrollable cases of numerical precision error.

figure 1: a 1d visualization of the hashing scheme we use. all points within 1 of hu1 u1i  the dark circle  lie within the ring having outer and inner radius a hashing scheme which hashes all points within the ring to the same bucket guarantees that all points within 1 of hu1 u1i also hash to the same bucket. we can use squared distances to avoid a ﹟   thus the hash key for hv1 v1i can be efficiently computed as the squared distance from hv1 v1i to the origin.
1 theoretical results
here we present two fundamental results for aadds:
theorem 1. the time and space performance of reduce and apply for aadds is within a multiplicative constant of that of adds in the worst case.
proof sketch. under the same variable ordering  an add is equivalent to a non-canonical aadd with fixed edge weights c = 1 b = 1. thus the add reduce and apply algorithms can be seen as analogs of the aadd algorithms without the additional constant-time

figure 1: comparison of apply operation running time  top  and table entries/nodes  bottom  for tables  adds and aadds.left to right:  pi 1ixi    pi 1ixi    污pi1ixi     污pi1ixi   and constant-space overhead of normalization. since max pi 1ixi pi 1ixi . note the linear time/space for aadds. probability tables  cpts  pj and corresponding opera-

normalization can only increase the number of reduce and apply cache hits and reduce the number of cached nodes  it is clear that an aadd must generate equal or fewer reduce and apply calls and have equal or fewer cached nodes than the corresponding add. this allows us to conclude that in the worst case where the aadd generates as many reduce and apply calls and cache hits as the add  the aadd is still within a multiplicative constant of the time and space required by the add.
theorem 1. there exist functions f1 and f1 and an operator op such that the running time and space performance of apply f1 f1 op  for aadds can be linear in the number of variables when the corresponding add operations are exponential in the number of variables.
proof sketch. two functions and apply operation examples where this holds true are pni=1ixipin=1ixi and
  examples of these operands as adds and aadds were given in figures 1c and 1.  because these computations result in a number of terminal values exponential in n  the add operations must require time and space exponential in n. on the other hand  it is known that the operands can be represented in linear-sized aadds. due to this structure  the apply algorithm will begin by recursing on the high branch of both operands to depth n. then  at each step as it returns and recurses down the low branch  the respective additive and multiplicative differences will be normalized out due to the canonical caching scheme  thus yielding cache hits for all low branches. this results in n cached nodes and 1n apply calls for the aadd operations.
1 empirical results
1 basic operations
figure 1 demonstrates the relative time and space performance of tables  adds  and aadds on a number of basic operations. these verify the exponential to linear space and time reductions proved in theorem 1.
1 bayes nets
for bayes nets  bns   we simply evaluate the variable elimination algorithm   under a fixed  greedy treewidth minimizing variable ordering  with the conditional tions replaced with those for tables  adds  and aadds:

table 1 shows the total number of table entries/nodes required to represent the original network and the total running time of 1 random queries  each consisting of one query variable and one evidence variable  for a number of publicly available bns
 http://www.cs.huji.ac.il/labs/compbio/repository  and two noisy-or and noisy-max cpts with 1 parent nodes. note that the intermediate probability tables were often too large for the tables or adds  but not the aadds  indicating that the aadd was able to exploit additive or multiplicative structure in these cases. also  the aadd yields an exponential to linear reduction on the noisy-or-1 problem and a considerable computational speedup on the noisy-max-1 problem by exploiting the additive and multiplicative structure inherent in these special cpts .
1 markov decision processes
for mdps  we simply evaluate the value iteration algorithm  using a tabular representation and its extension for decision diagrams  to factored mdps from the sysadmin domain .1 here we simply substitute tables  adds  and aadds for the reward function r  value function v   and transition model dynamics pi in the following factored mdp value iteration update:
v t+1 s1 ... sn  = r s1 ... sn  + 污 max 1xn1   1y pi si1|parents si1  a  #v t s1 ... s1n  
a
	s1...s	p ...pn
 figure 1 shows the relative performance of value iteration until convergence within 1 of the optimal value
bayes nettableaddaadd# table entriesrunning time# add nodesrunning time# aadd nodesrunning timealarm11 s1.1 s1.1 sbarley1eml 1eml 11 mcarpo1.11 s1.1 shailfinder1.1 s1.1 s1.1 sinsurance1 s1 s1 snoisy-or-11 s1.1 s1.1 snoisy-max-11 s1.1 s1.1 stable 1: number of table entries/nodes in the original network and variable elimination running times using tabular  add  and aadd representations for inference in various bayes nets.  eml denotes that a query exceeded the 1gb memory limit.
figure 1: mdp value iteration running times  top  and number of entries/nodes  bottom  in the final value function using tabular  add  and aadd representations for various network configurations in the sysadmin problem.for networks in a star  bidirectional  and independent ring configuration. while the reward and transition dynamics in the sysadmin problem have considerable additive structure  we note that the exponential size of the aadd  as for all representations  indicates that little additive structure survives in the exact value function. nonetheless  the aadd-based algorithm still manages to take considerable advantage of the additive structure during computations and thus performs comparably or exponentially better than adds and tables.
1 concluding remarks
we have presented the aadd and have proved that its worst-case time and space performance are within a multiplicative constant of that of adds  but can be linear in the number of variables in cases where adds are exponential in the number of variables. and we have provided an empirical comparison of tabular  add  and aadd representations used in bayes net and mdp inference algorithms  concluding that aadds perform at least as well as the other two representations  and often yield an exponential time and space improvement over both.
﹛in conclusion  we should emphasize that we have not set out to show that variable elimination and value iteration with aadds are the best bayes net and mdp inference algorithms available - many other approaches propose to handle similar structure efficiently via specialpurpose problem-structure or algorithm modifications. rather  we intended to show that transparently substituting aadds in two diverse probabilistic inference algorithms that used tables or adds could yield exponential performance improvements over both by exploiting context-specific  additive  and multiplicative structure in the underlying representation. these results suggest that the aadd is likely to yield exponential time and space improvements for a variety of probabilistic inference algorithms that currently use tables or adds  or at the very least  a gross simplification of these algorithms.
