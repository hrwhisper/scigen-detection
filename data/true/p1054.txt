in this paper  we present a novel association-based method called sat-mod for text classification. sat-mod views a sentence rather than a document as a transaction  and uses a novel heuristic called modfit to select the most significant itemsets for constructing a category classifier. the effectiveness of sat-mod has been demonstrated comparable to well-known alternatives such as linearsvm and much better than current document-level words association based methods on the reuters corpus. 
categories and subject descriptors 
i.1.m 	 computing 	methodologies : 	document 	and 	text 
processing - miscellaneous.  
general terms 
algorithms  performance. 
keywords 
text classification  modfit  moderate itemset fittest  heuristic. 
1. introduction 
text classification  tc  is to realize the task of assigning one or more  multi-labeled  predefined category labels to unlabeled natural language text documents based on their content. tc has become more and more important due to the flourishing of digital documents over the internet and intranets. it has extensive applications in online news classification  email filtering  and the like. many methods have been proposed for tc  including na ve bayes  decision trees  k-nn  linearsvm and association rule based   or simply association based  methods  1 . 
an association rule for tc is indeed similar to an if-then rule manually defined by domain experts in the knowledge engineering method  which is the most popular real-world approach to tc in the 1s. to the best of our knowledge  all the current association based text classification and clustering methods all exploit document-level co-occurring words  itemsets   which are a group of words co-occurring in the same document. two arc algorithms are proposed in   both viewed a document as a single transaction and used the traditional database coverage heuristic for selecting significant itemsets. document-level frequent itemsets are also exploited for text clustering  e.g. the fihc algorithm . the very recently proposed emailsift  also takes each mail instead of a sentence in the email as a transaction.  

however  the basic semantic unit in a document is actually a sentence. words co-occurring in the same sentence are usually associated in one way or the other  and are more meaningful than the same group of words spanning several sentences in a 
document. hence we view a sentence rather than a document as the basic semantic unit and present a novel association-based tc method called sat-mod. 
1. mining co-occurring words 
1 document frequent itemset 
in daily life  usually people are liable to emphasize some core ideas by repeating some representative words in different sentences  thus frequently repeated words tend to represent a facet of the whole  document subject  of a document. those content words are captured by document frequent itemsets  abbr. dfis  in our sat-mod method. a dfi is a group of words cooccurring in a minimum number  document minsup  of sentences in a  supporting  document. the supporting document is said to be covered by the dfi. with each word as an item  and each natural sentence as a transaction  we can use frequent itemsets mining algorithm such as the classical apriori  to mine dfis in a document  and represent each training document as a set of dfis. 
naturally  document minsup should be set to guarantee that a dfi occurs in at least 1 sentences. hence a document minsup of value 1 is called the natural document minsup.  
1 mining contexts 
as argued in   although most content words are much more likely to occur again in a document once they have occurred once  in many cases  the probability of reusing a content word immediately after its first occurrence is lower than general since we are taught to avoid repetitive writing. usually authors may alternately use synonyms to avoid dull repeat. however we believe some content words  especially proper nouns such as our dfi  do not avoid direct repeat. thus a compromise could be made that content words would repeat in near paragraphs  and we can use a sliding window to construct different mining contexts. for simplicity  a mining context is respectively called a unit  a multi  and a full mining context when the sliding window size p is accordingly set to be 1  k  1  k  p   and p  here p is the total number of paragraphs in the document. given a mining context  content words can be captured by itemsets which are document frequent in that context. 
1. sat-mod 

 
copyright is held by the author/owner s . www 1  may 1--1  1  chiba  japan. acm 1-1/1. 
 
1dfis are then used to generate category frequent itemsets  abbr. cfis . a cfi with respect to a pre-defined category is an itemset whose category support  the number of supporting documents in that category  provided that the cfi is a dfi in each supporting document  is no less than a user-specified minimum number  category minsup . all the cfis are collected using a category prefix-tree  and the tree is then pruned by our novel heuristic called modfit  i.e. the moderate itemset fittest  intuition as follows: 1  intuitively an itemset is usually harder to appear than its proper subsets in a sentence and also harder to be document frequent in a document  hence an itemset tends to have more discriminating power than its proper subsets; 1  on the other hand  a too long itemset may cause overfitting and hence lose its discriminating power for unlabeled documents. 
based on modfit  we should seek a definition which can make a moderate itemset have a classification confidence greater than any of its underfitting proper subsets or overfitting proper supersets. 
definition 1  confidence of a cfi w.r.t a category ci   the confidence  denoted as conf i== ci   is defined as the ratio of si to stot  i.e. conf i ==  ci  = si / stot  where si is the category support of i in ci  and stot is the total number of distinct supporting documents covered by i in the whole training set. 
intuitively  the modfit heuristic equals to moderately extending a single word with other words along a natural sentence. using modfit pruning  we will keep all synonymic itemsets that only partly share some items with each other. in addition  we do mot need the very expensive step of removing covered documents in the database coverage heuristic. the pruned tree is finally taken as the category classifier. figure 1 is just an illustration of a category prefix-tree where each node contains three counters: ic  iconf and is. we use ic and iconf to respectively hold category support and confidence  i.e  conf i ==  c   of the itemset i which corresponds to the host node. the counter is is for holding document support of i in an unlabeled document du in subsequent classifying phase. 

figure 1. a category prefix-tree for items 1  1  1  and 1. 
given a set of predefined categories {c1  c1  ...  cm}  and a document du to be classified  we first need to identify all the  common itemsets  shared between each category ci  1 = i  = m  and du  i.e. category intersection ci   du   and count their supports in du. the category similarity is then derived as follows: sim ci ¡û du   =¡Æ iconf ¡Áis  ¡Á i ¡Êci   du   
where  i¡Êci   du  has a value of 1 if i is truly an itemset in the intersection  ci   du  and otherwise 1. we then realize the multilabeled classification task using a classifying strategy as follows: first assign du with the category label of the category having the highest similarity score; second specify a threshold called label minsup in percentage  and if any other category has a similarity score greater than that percentage of the highest score  then du is also assigned the label of that category. 
we studied how crucial parameters would affect the effectiveness of sat-mod  and also made a comparison with other alternatives including sat-foil  using the foil heuristic  i.e. database coverage  instead of modfit  in terms of  classification accuracy. so far  we have only considered exploiting itemsets with a maximum size of 1 which seems already enough.  our experimental study shows that the natural document minsup can be a quite reasonable choice to capture sentential word cooccurrence. the underlying reason is consistent with modfit pruning  because natural document minsup can keep more synonyms. since natural document minsup is actually a bottom bound of document minsup  we can use it as a default and hence logically remove this parameter. the study of mining contexts provides a proof of previous assertions on the clumping of content words: usually content words are more liable to be repeated in different sentences distributed in different paragraphs  but the distribution is not very regular  hence it is better to use the whole document as the mining context. currently we have got very encouraging classification results  referring to table 1  on relatively short documents such as the reuters  the measures of other well-known methods are obtained from  1  1 . 
table 1. bep on 1 largest categories of reuters 
bepsat-mod
labelminsupsat-foil
labelminsuparc-bc ¦Ä=1bayesrocchioc1k-nnlinearsvm1%1%1%1%1%1%acq11111111111corn11111111111crude11111111111earn11111111111grain11111111111interest11111111111money-fx11111111111ship11111111111trade11111111111wheat11111111111micro-avg11111111111macro-avg111111111111. conclutions 
we have proposed the sat-mod exploiting a novel modfit heuristic  which has very promising classification accuracy on relatively short documents such as the reuters. in addition  it has inherent readability and refinability of acquired classification rules. 
1. acknowledgments 
this work was supported by china nsf grant no. 1 and chongqing nsf research grant no. 1. 
