the paper presents a novel approach to unsupervised text summarization. the novelty lies in exploiting the diversity of concepts in text for summarization  which has not received much attention in the summarization literature. a diversity-based approach here is a principled generalization of maximal marginal relevance criterion by carbonell and goldstein .
　we propose  in addition  an information-centric approach to evaluation  where the quality of summaries is judged not in terms of how well they match human-created summaries but in terms of how well they represent their source documents in ir tasks such document retrieval and text categorization.
　to find the effectiveness of our approach under the proposed evaluation scheme  we set out to examine how a system with the diversity functionality performs against one without  using the bmir-j1 corpus  a test data developed by a japanese research consortium. the results demonstrate a clear superiority of a diversity based approach to a nondiversity based approach.
keywords
text summarization  clustering  mdl  bmir-j1
1. introduction
　prior work on automatic text summarization come in two kinds: supervised and unsupervised. supervised approaches  1  1  1  1  typically make use of human-made summaries or extracts to find features or parameters of summarization algorithms  while unsupervised approaches determine relevant parameters without regard to human-made summaries. one of the problems with the former approach has to do with its underlying assumption that human-made summaries are reliable enough to be used as  gold standards  for automatic summarization. recently  research on human summarization has witnessed some conflicting results about the validity of the assumption.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  september 1  1  new orleans  louisiana  usa.
copyright 1 acm 1-1/1 ...$1.
　nomoto and matsumoto   for instance  asked a large group of university students to identify 1% sentences in a text which they believe to be most important  which was drawn from one of various domains in a news paper corpus. they reported a rather modest result of around 1%  kappa  agreement among students on their choices. also salton  reports low inter-subject agreement on paragraph extracts from encyclopedias. on the other hand  there have been some reports to the contrary. marcu  found 1%  percent  agreement among judges on sentence extracts from expository articles; jing et al.  found quite high percent agreement  1%  for extractions from trec articles.
　it is not known at present what factors are involved in influencing the reliability of summarization and therefore we do not know whether there is any principled way of eliciting reliable summaries from humans.
　another problem associated with the approach concerns the portability of a summarization system: deploying the system in a new domain usually requires one to collect a large amount of data  which need to be manually annotated  and then train the system. besides being costly  the annotation work is known to be quite labor-intensive  which prompted  for example  marcu  to address the problem of automating the construction of summarization corpora.
　the present work attempts to overcome the portability problem as well as the reliability problem by taking a novel approach to summarization  which neither learns from nor relies on human-created summaries for evaluation.
1. the approach
　like previous extract-based approaches  1  1  1   we define a summary as a set of sentences extracted verbatim from a text  which cover major substance of that text.
　however  the present approach significantly differs from previous work in taking an information-centric approach to evaluation. we evaluate summaries  not in terms of how well they match human-made extracts  1  1   nor in terms of how much time it takes for humans to make relevance judgments on them   but in terms of how well they represent source documents in usual ir tasks such as document retrieval and text categorization. we are interested in asking whether it is possible to replace documents altogether by corresponding summaries and do as well with them in ir tasks. thus an ideal summary would be one whose rank order in retrieved documents is same as that of its source document  or whose assigned category is same as that of its source document. this notion of summary as a perfect surrogate of its source  while left unexplored in research on summarization  permits a simple and objective characterization of how well summaries represent the original documents.
　there have been arguments against extractive summarization in the computational linguistics literature  because extracts generally lack fluency or cohesion. however  morris  found in a reading comprehension study that humans were able to perform as well reading 1%-1% extracts as the original full texts and expert-created abstracts. humans were able to capture enough of the information from extracts so that they could perform as if they had read their full-length versions.
1. diversity-based summarization
　assuming that the problem of summarization is one of finding a subset of sentences in text which in some way represents its source text  a natural question to ask is  'what are the properties of text that should be represented or retained in a summary ' katz's work  is enlightening in this regard. in his work on language modeling   he made an important observation that the numbers of occurrences of content words in a document do not depend on the document's length  that is  the frequencies per document of individual content words do not grow proportionally with the length of a document. where is the missing mass that accounts for the discrepancy between the document length and frequencies of content words  he resolves the apparent puzzle by showing that it is the number of different content words in text that increases with the document length.
　katz's observation illuminates two important properties of text: redundancy and diversity. the former relates to how repetitive concepts  or content words  are  the latter relates to how many different concepts there are in the text. while much of the prior work on summarization capitalize on redundancy to find important concepts in the text or its relevance to the query  few of them take an issue with the problem of diversity. one exception is carbonell and goldstein   who added the diversity component to a criterion for sentence selection  which they call maximal marginal relevance or mmr. mmr selects a sentence in such a way that it is both relevant to the query and has the least similarity to sentences selected previously. mani et al.  report that mmr-based summarization ranks among the best in the 1 summac conference. radev  also makes use of some dissimilarity measure for ranking sentences for multi-document summarization.
1 the method
　the above discussion motivates a summarization strategy which takes seriously diversity as well as redundancy of concepts in text. we will show how to construct a generic single-document summarizer along this direction. roughly  the summarizer consists of the following two operations:
1. find-diversity: find diverse topic areas in text.
1. reduce-redundancy: from each topic area  identify the most important sentence and take that sentence as a representative of the area.
a summary is then a set of sentences generated by reduceredundancy. below  we will look at each of the operations in details.
1.1 find-diversity
　find-diversity is built upon the k-means clustering algorithm extended with minimum description length principle  mdl   1  1 . the algorithm presented here is an mdlversion of x-means . x-means itself is an extension of the popular k-means clustering algorithm with an added functionality of estimating k  the number of clusters which otherwise needs to be supplied by the user. we call our adaptation of x-means 'xm means.'
　for the remainder of the paper  borrowing in part notations from pelleg and moore   we denote by μj the coordinates of the centroid with the index j  and by xi the coordinates of the i-th data point.  i  represents the index of the centroid closest to the data point i. μ j   for example  denotes the centroid associated with the data point j. ci denotes a cluster with the index i.
　k-means is a hard clustering algorithm that produces a clustering of input data points into k disjoint subsets. it dynamically redefines a clustering by relocating each centroid to the center of mass of points associated with it and re-associating the centroid with points closest to it.
　k-means starts with some randomly chosen initial points. as noted in bradley  and pelleg and moore   a bad choice of initial centers can have adverse effects on performance in clustering. in experiments described later in the paper  following an advice by bradely   we repeatedly ran k-means with random initial points and selected a best clustering solution from solutions generated  on the basis of distortion  a measure for the tightness of a cluster. a best solution is one that minimizes distortion.
　we define distortion as the averaged sum of squares of euclidean distances between objects of a cluster and its centroid. thus for some clustering solution s = {c1 ... ck}  its distortion is given by:
 
where
v.
here ci denotes a cluster  xj is an object in ci  μ i  represents the centroid of ci  and | ， | is the cardinality function.
　one problem with k-means is that the user has to supply the number of clusters  and it is known that it is prone to searching local minima . x-means overcomes these problems by globally searching the space of centroid locations to find the best way of partitioning the input data. x-means resorts to a model selection criterion known as the baysian information criterion  bic  to decide whether to split a cluster. the splitting happens when the information gain from splitting a cluster as measured by bic is greater than the gain for keeping that cluster as it is.
　let us graphically illustrate this situation. figure 1 shows a k-means solution with four centroids  large dots   which cover four distinct regions of the data space. the split operation examines each of the four clusters  breaks each of them in two  running the regular k-means on each local cluster region with k = 1  and decides whether the splitting is worthwhile in terms of bic. as mentioned above  each call to k-means involves repeated runs of itself with randomly chosen initial centers and selecting the best clus-

figure 1: the initial state with four regions.

figure 1: each local cluster splits into two subregions.

figure 1: bic determines that some of sub-regions are not worth keeping.
table 1: the xm-means clustering algorithm
xm-means c1 kmax  begin
c = φ
 c1 c1  = 1-means c1  c = c “ {c1 c1} k = 1
while k   kmax and k does not converge begin
s = {c : c （ c mdl 1-means c     mdl c } if s is not empty then
cbest=arg min mdl 1-means c   c = c {cc（s} “ {ck1 ck1}cbest
             best k = k + 1
endif end
end

tering solution from those runs.  in experiments described later  we performed 1 runs of k-means and selected from those runs a solution with the least distortion.  figure 1 shows a state where each local cluster splits into two subregions by k-means. in figure 1  bic determines that the upper left and upper right regions are not worth breaking up  leavings us with six clusters.
　our modification to x-means consists in replacing bic by minimum description length principle or mdl  a wellstudied general criterion for model selection.
　in general  given the data x1 ... xm  mdl claims that a model that allows a shortest description for the data is most likely to have given rise to them.  a model here is thought of as the probability distribution of a variable x  where x = xi.  in mdl  the length of a description of data is given as the sum of bits required to encode a model and bits required to encode the data given the model. the best hypothesis  i.e. a model  h for xm = x1 ... xm is then expressed as follows:
hbest xm  = arg min l xm : m   m（m
where l ym : m  = l xm | m  + l m   m is the set of possible models  and l x  is a description-length of x. l xm | m  denotes the description length of data  given the model m  which is the sum of the maximum log-likelihood estimate of p xm | m  and the coding length of parameters involved.
　let us assume identical hyperspherical gaussian distributions for input data. also let each data point represent a multi-dimensional encoding of the sentence in text  i.e. a vector of weights of index terms in the sentence. then the probability that a given data point xi belongs to a cluster c i  can be defined as the product of the probability of observing c i  and the multivariate normal density function of xi  with the covariance matrix Σ = σ1i.1 therefore we have
 
whereis the euclidean norm  r is the total number of input data points  u is the number of dimensions  and  σ1 is the maximum likelihood estimate of the variance such that:
.
therefore the maximum log-likelihood of the cluster cj is:
 
which is equivalent to:

now define the probability of observing the model m by:
.
then the description-length of the model m is: l m  =  log|m|  where |m| = kmax in our case. but we can drop it from the mdl formula since it is invariant for any choice of m. this brings us to the final form of mdl:
mdl
where m is a model  d is a set of data points in the input space  ci is a region or cluster as demarcated by m  r is the size of the input data  and k is the number of free parameters in m. the number of parameters here is simply the sum of k   1 cluster probabilities  u ， k centroid coordinates  and one variance estimate.
　table 1 shows an algorithm for xm means. it takes as input the entire data region  represented by a single cluster c1  and the maximum number kmax of clusters one likes to have. it begins then by running 1-means  i.e. k-means with k = 1  on the entire region  giving birth to two subregions  clusters  and . add those to c. set k  the number of clusters in c  to 1. if k   kmax and there is no convergence  then run 1-means on each local region found in c and test if the splitting produces a pair of child regions whose mdl is smaller than that of the parent. if that is the case  store that information in s. find a region in s whose splitting results in the smallest mdl. replace the region with those of children. increment k by 1. again test if the stopping conditions are met. if not  repeat the whole process. the idea here is that regions which are not represented well by current centroids will receive more attention with an increased number of centroids in them. the algorithm searches the entire data space for the best region to split.
1.1 reduce-redundancy
　for reduce-redundancy  we will use a simple sentence weighting model by zechner   call it the z-model   where one takes the weight of a given sentence as the sum of tf，idf values of index terms in that sentence. let the weight of sentence s  or w s  be given by:

where x denotes a index term  tf x  is the frequency of term x in document  idf x  is the inverse document frequency of x. the term weighting formula here comes from work by manning and schu：tze . in the z-model  sentence selection proceeds by: determining the weights of sentences in the text; sorting them in a decreasing order; and finally selecting top sentences. our implementation of z-model further normalized sentence weight for length.
　reduce-redundancy applies the z-model to each one of the clusters identified by find-diversity and finds out a sentence with the best w s  score. it then takes that sentence as a representative of the cluster. note that this strategy of picking up best scoring sentences is intended to minimize the loss of the resulting summary's relevance to a potential query. this is in contrast to an approach by radev   where the centroid of a cluster is selected instead.
　recall that our goal of summarization is to make an extract from a text in such a way that it best simulates its fulllength source in usual ir tasks such as document retrieval and text categorization. however  the summarization process as a rule does not preserve statistical properties of a source text such as frequencies of index terms  which are often left statistically indistinguishable after the process. one way to go about the problem is to extrapolate frequencies of index terms in extracts in order to estimate their true frequencies in source texts.
　in particular  we use the following formula from katz  for extrapolation.
		 1 
where pr denotes the probability of a given word occurring r times in the document and m − 1. formula 1  =  1  in   estimates the average number of occurrences of a word in the documents  each of which contains at least m occurrences of that word. with formula 1 it is possible to estimate the average frequency in the source of a word observed in a summary. for example  if we observe m occurrences of a word w in a summary  its expected frequency in its source text is given as e k | k − m .
　in our experiments  we restricted ourselves to index terms with two or more occurrences in the document  so their extrapolated estimates would be e k | k − 1 .
　the df values of index terms in a summary are obtained directly from a pool of summaries.
1. testdataandevaluationprocedure
table 1: the classification of queries in bmir-j1. figures under the primary  secondary  heading indicates the number of occurrences of documents relevant to a query of each type.
typeexplanationprimarysecondaryamorphological analysis  the use of thesaurus1bthe analysis of collocation involving numbers  e.g.
rangeinequality 1csyntactic analysis1dsemantic  discourse analysis1eworld/common sense knowledge1fsemantic  world/common sense knowledge  reasoning11 bmir-j1
　bmir-j1  benchmark for japanese ir system version 1  developed jointly by a japanese academic society and a government-funded research consortium represents a test collection of 1 news articles in japanese  all of them published in 1. articles were collected from such diverse domains as economy  engineering  and industrial technology  but they all came from a single news paper source. the collection comes with a set of 1 queries and an associated list of answers indicating which article is relevant to which query to what degree. the degree of relevance falls into one of three categories: a  b  and c. a indicates a perfect match to the query  b some relevance and c no relevance. all of the articles were manually labeled for relevance  and labelings were reviewed by the project committee  comprising researchers from industry and academia. the collection also features a classification of queries based on sorts of language technologies potentially required to process them  table 1 . as can be seen from the table  the classification gives us a general idea of how difficult the task of retrieval using a given query is. for instance  to properly identify documents relevant to a query of type a requires the morphological analysis of the query  which involves tokenization  lemmatization and assignments of grammatical categories  and also the possible use of a thesaurus  and to deal with a query of type b  one needs some way of analyzing collocation involving numbers  which is more difficult than an a-type query. to find documents relevant to a query of type f  one has to make reference to common sense or knowledge about the world and also some reasoning  which is the hardest of all. c  d  and e-type queries come in between.
　moreover  a set of queries prepared by bmir-j1 comprises a primary set of 1 queries  for each of which there are five or more relevant documents  and a secondary set of 1 queries  each having one to four relevant documents. the description of a query in bmir-j1 contains two types of information; query words/phrases used for the retrieval of documents and a short explanation of search needs the query is intended to fulfill. in experiments  we used the primary set of queries.
1 experiment setup and procedure
　we have conducted experiments using bmir-j1. our interest was in finding out whether the diversity-based summarization as formulated here is superior to relevance-based summarization  which represents a class of summarization methods broadly characterized as creating a summary from a list of sentences in the document ranked according to their salience or their likelihood of being included in a summary. as a representative of the class  we used z-model described earlier  which can be thought of as a baseline model for relevance-based approaches. another point of using z-model is that since the model is already part of our diversity-based scheme  experiments would reveal true effects of the diversity component on performance. we could see how much the diversity component contributes to performance by comparing it to an approach without one.
　we treated summaries as if they were stand-alone documents  and performed usual document retrieval sessions using them: which is to retrieve documents for a particular query and rank them according to the cosine similarity to the query. we scored performance in f-measure where for given p precision  and r  recall  
.
　we performed two sets of experiments which differ in what relevance scheme is employed. one scheme  which we call the strict relevance scheme  srs   takes only a-labeled documents as relevant to the query  while another  called the moderate relevance scheme  mrs   takes both a- and blabeled documents as relevant. bmir-j1 recommends the latter scheme.
　in each experiment  we tested three summarization methods set at a particular compression rate  which include zmodel  a diversity-based summarizer with the standard kmeans  hereafter  dbs/k   and a diversity-based summarizer with xm-means  hereafter  dbs/xm . dbs/k  which is identical to dbs/xm except for the diversity component  was introduced to determine the effects of mdl on kmeans. to obtain a given compression level α for dbs/xm  we set kmax to the corresponding number of sentences in text: e.g. for the text of 1 sentences  kmax = 1 for α = 1%. similarly for dbs/k.
　one feature specific to the present test domain is the use of a japanese tokenizer chasen   which breaks up sentences into words marked with some morphological information such as parts of speech. chasen is reported to have the accuracy of over 1% . for index terms  we used everything except for punctuation marks  non-linguistic symbols  particles such as case marker. furthermore  we did not use any stoplist except for those elements already excluded from the set of index terms.
　based on a finding by morris  that 1% and 1% extracts are reasonably informative and comparable to the fulllength text in the reading comprehension setting  we decided to use compression rates ranging from 1% to 1%. a test sequence consisted of:
1. at each compression rate  run z-model  dbs/k and dbs/xm on the entire bmir-j1 collection  to produce respective pools of extracts.
1. for each query from bmir-j1  perform a search oneach pool generated  and score performance with the uninterpolated average f-measure.
since we have two relevance schemes to consider  we did the sequence under each scheme  which brought the total of retrieval sessions to some 1.
1. results and discussion
　figure 1 and figure 1 give us a detailed picture of how dbs/xm compares in performance to the baseline z-model under the respective relevance scheme.  in the following  we mostly discuss results of dbs/xm and z-model due to space limitation.  figure 1 show results under srs  and figure 1 results under mrs. in each figure  a panel displays a query-wise plotting of performance of a model on x-axis against that on y-axis at compression rates from 1% to 1%. if a given point on the graph  which represents a query  appears above the diagonal line  a model on y-axis performs better for that point than that on x-axis  or worse if a point appears below the line. they break even when a point appears on the line.
　figure 1 compares performance of z-model  relevancebased   and dbs/xm under srs  showing how z-model performs against dbs/xm at the increasing compression rate. at compression rates between 1% and 1%  despite some outliers  dbs/xm performs better for the majority of queries than z-model  but breaks even at 1%. notice that as the compression rate advances  the text breaks up into ever smaller segments or clusters; at compression rate of 1%  it could break up into itself  i.e. into a set of singleton clusters  each containing just one sentence.
　table 1 shows results of statistical significance tests. the null hypothesis is that there is no difference in performance between z-model and dbs/xm under either relevance scheme. the hypothesis testing was done with the two-tailed paired t-test. p1 denotes a p-value under srs determined on the basis of results at a particular compression rate. since the results had one extreme outlier  a query at 1 on x-axis  we computed p-values  p1  for data without it for the purpose of comparison. p1 stands for p-values at each compression rate for the results under mrs  no points removed . the underlined figures indicate that they break 1% significance level.
　although p1 values from 1% to 1% are moderately significant  with no significant difference at 1% level  a clear difference emerges when we look at p1 values. without an outlier  dbs/xm is significantly different from z-model at 1% and 1%. however no significant difference was found at 1% and 1%. on the other hand  under mrs  dbs/xm turned out to be significantly different from zmodel at every compression rate examined. the superiority of dbs/xm over z-model is more pronounced under mrs than under srs.
　a possible reason for this is that: since z-model selects from a list of sentences globally ranked according to some importance measure  it fails to discover sentences which  though relevant to a query  do not talk about the primary topic of the text. it is reasonable to presume that b-labeled documents typically contain sentences which are relevant to a potential query  though of little or marginal relevance to a primary concern of the text. in contrast to z-model  dbs/xm selects from locally ranked groups of sentences and table 1: results of significance tests.
compression rate
	1%	1	1
1% 1 1 1% 1 1
	1%	1	1	1

table 1: the break down of average performance of dbs/xm by query type under srs.
query type1%1%a11b11c11d11e11f11
thus is able to identify an important sentence even though it lacks global relevance.
　another point about dbs/xm and z-model is that their differences in performance become smaller in proportion to the compression rate. this is because the z-model selects more of the diverse sentences in the text  as the compression rate increases.
　table 1 and table 1 break down performance of dbs/xm by query type under srs and mrs  respectively. the general picture is that in either scheme  dbs/xm performs best on average for queries of type b  and moderately well for queries of type a and c. the results are somewhat contrary to our expectation since a retrieval with a query of type b is supposedly more difficult than with an a-type query. although we have been investigating a possible cause for this  we do not have a definite answer yet.
　finally  we will take a look at how close dbs/xm and other summarization models come to a retrieval system using full length texts  call it the target retrieval system . figure 1 compares performance of z-model  filled diamonds  and dbs/xm  white diamonds  against that of the target retrieval system  diagonal line . the top row gives results for srs  the bottom row for mrs. we take a point  query  above the diagonal as indicating that a particular method associated with the point performs superior to the target system  and similarly for other cases.
turning to the top row of figure 1  we see at 1% level a
table 1: the break down of average performance of dbs/xm by query type under mrs.
query type1%1%a11b11c11d11e11f11
　

	relevance-based	relevance-based	relevance-based	relevance-based
figure 1: performance under srs.

	relevance-based	relevance-based	relevance-based	relevance-based
figure 1: performance under mrs.

	summarizer	summarizer	summarizer	summarizer

	summarizer	summarizer	summarizer	summarizer
m
figure 1: comparing performance of z-model and dbs/x	against the target retrieval system  which uses full-length documents.
　
table 1: average performance of z-model  dbs/k  and dbs/xm compared against target retrieval system  under srs. 'full' denotes the target system. figures are in f-measure.
cmp. ratefullzdbs/kdbs/xm1%11111%11111%11111%1111
table 1: average performance of z-model  dbs/k  and dbs/xm compared against target retrieval system  under mrs. 'full' denotes the target system. figures are in f-measure.
cmp. ratefullzdbs/kdbs/xm1%11111%11111%11111%1111
group of white diamonds flocking along the diagonal more tightly than that of filled diamonds  indicating that dbs/xm comes nearer in performance to the target system. however  as the compression rate increases  z-model  dbs/xm and the target system become increasingly harder to distinguish. the results from the bottom row of figure 1 appear to repeat patterns observed in the upper panels. dbs/xm is more distinguishable from z-model at 1% and 1%.
　table 1 gives figures on average performance under srs  and table 1 under mrs. f-scores here were averaged across scores for the primary set of queries. as the underlined figures show  dbs/xm has two wins and two losses against z-model in terms of average f-score in table 1  but no losses in table 1. dbs/xm performs consistently better than z-model under mrs. also it is worth noting that at compression rate of 1%  the difference between dbs/xm and z-model is greater under mrs than under srs.
1. conclusions and future work
　we have presented a new summarization scheme where evaluation does not rely on matching extracts against humanmade summaries but measuring the loss of information in extracts in terms of retrieval performance. under this scheme  diversity-based summarization  dbs/xm  was found to be superior to relevance-based summarization  z-model . since the former differs from the latter only in the use of the diversity component  the difference in performance could be attributed to the sole effects of selecting diverse sentences. we have seen that the improvement by the diversity component is more prominent under the moderate relevance scheme  mrs  than the strict relevance scheme  srs . but this is just what is expected of performance of the diversity-based approach  because under the moderate relevance scheme  the system has to be more sensitive to marginally relevant sentences.
　we were not particularly concerned about generally low performance rates  since our interest here was in finding out effects of the diversity component on performance. however  we expect that performance would be improved by exploiting a better indexing model such as one used in smart  query expansion techniques  or by improving a tokenizer program.
　for future work  we are interested in extending the current dbs framework to deal with multi-document summarization and speech summarization with audio input and output. also  how far the present results carry over to text categorization would be an interesting question.
