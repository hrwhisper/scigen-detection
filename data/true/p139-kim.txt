recent studies have shown that cache-conscious indexes such as the csb+-tree outperform conventional main memory indexes such as the t-tree. the key idea of these cache-conscious indexes is to eliminate most of child pointers from a node to increase the fanout of the tree. when the node size is chosen in the order of the cache block size  this pointer elimination effectively reduces the tree height  and thus improves the cache behavior of the index. however  the pointer elimination cannot be directly applied to multidimensional index structures such as the r-tree  where the size of a key  typically  an mbr  minimum bounding rectangle   is much larger than that of a pointer. simple elimination of fourbyte pointers does not help much to pack more entries in a node.
　this paper proposes a cache-conscious version of the r-tree called the cr-tree. to pack more entries in a node  the cr-tree compresses mbr keys  which occupy almost 1% of index data in the two-dimensional case. it first represents the coordinates of an mbr key relatively to the lower left corner of its parent mbr to eliminate the leading 1's from the relative coordinate representation. then  it quantizes the relative coordinates with a fixed number of bits to further cut off the trailing less significant bits. consequently  the cr-tree becomes significantly wider and smaller than the ordinary r-tree. our experimental and analytical study shows that the two-dimensional cr-tree performs search up to 1 times faster than the ordinary r-tree while maintaining similar update performance and consuming about 1% less memory space.
1. introduction
as the price of memory continues to drop below $1/gb  it is now feasible to place many of the database tables and indexes in main memory. with such memory-resident tables and indexes  the traditional bottleneck of disk access almost disappears  especially for search transactions. instead  memory access becomes a new bottleneck . a recent study with commercial dbmss has shown that half the execution time is spent on memory access when the whole database fits in memory . since the speed in dram chips has been traded off for the capacity  the gap between the cpu speed and the dram speed has grown significantly during the past decade . in today's computer systems  each memory access costs tens of processor cycles. to overcome this gap  modern processors adopt up to several megabytes of sram as the cache  which can be accessed in just one or two processor cycles.
　recognizing the widening gap between the cpu speed and the dram speed  rao and ross recently addressed the importance of the cache behavior in the design of main memory indexes and showed that the cache-conscious search tree  css-tree  performs lookups much faster than the binary search tree and the t-tree in the read-only olap environment . they also observed the reasonably good cache behavior of the b+-tree and proposed its cache sensitive variants . called csb+-tree  these b+-tree variants store child nodes contiguously in memory to eliminate most child pointers except the first one. the location of the i-th child node is computed from that of the first child. providing more room for keys in the node  this pointer elimination approach effectively doubles the fanout of the b+-tree. given the node size in the order of the cache block size  the fanout doubling reduces the height of b+-tree  and thus incurs less cache misses during the tree traversal than the b+-tree. note that such a pointer elimination technique does not provide much benefit in diskresident indexes where the fanout is typically a few hundreds and doubling the fanout does not lead to the immediate reduction in the tree height  e.g.   log1  =  log1  =1 .
　the pointer elimination technique cannot be directly applied to multidimensional index structures such as the r-tree   because multidimensional keys  typically  mbrs  minimum bounding rectangles   are much larger than pointers. thus  pointer elimination alone cannot widen the index tree significantly. for example  when the 1-byte mbr is used for the two-dimensional key  the simple elimination of a 1-byte pointer provides at most 1% more room for the keys  and this increase is not big enough to make significant difference in the tree height for the improved cache behavior.
　recognizing that mbr keys occupy most of index data in the multidimensional index  for example  almost 1% for the 1d rtree  this paper focuses on inexpensive compression of mbr keys
　
permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  to copy otherwise  to republish  to post on servers  or to redistribute to lists  requires prior specific permission and/or a fee. 
acm sigmod 1 may 1  santa barbara  california usa copyright 1 acm 1-1/1...$1 
 
r1  1
r1  1
1  1
r1r1  1
1  1
1  1r1
1  1
r1  1  1
r1  1
1  1
r1  1	1  1  1
1  1  1
	 a  absolute coordinates of r1~r1	 b  relative coordinates of r1~r1
 c  quantized relative coordinates
to the lower left corner of r1
figure 1: qrmbr techniqueto improve the index cache behavior. called cr-tree  cacheconscious r-tree   it takes advantage of the fact that the child nodes are grouped into a parent node such that each node occupies a small portion of the data space of its parent node . thus  if we represent an mbr relatively to its parent mbr  the coordinates of the resultant relative mbr have a fewer number of significant bits with many leading 1's. to further reduce the number of bits per mbr  the cr-tree cuts off trailing insignificant bits by quantization. our analysis and experiment show that this compression technique can reduce the mbr size to less than a fourth  thereby increasing the fanout by more than 1%. a potential problem with the proposed technique is that the information loss by quantization may increase false hits  which have to be filtered out through a subsequent refinement step in most multidimensional indexes . however  we can keep the number of false hits negligibly small by the proper choice of the quantization level so that the cost of filtering out false hits can be paid off by the significant savings in cache misses.
　this paper also explores several options in the design of crtree including whether to use the pointer elimination technique of the csb+-tree  whether to apply the proposed compression technique to leaf nodes or not  the choice of quantization levels  and the choice of node size. our experimental study shows that all the resultant cr-tree variants significantly outperform the r-tree in terms of the search performance and the space requirement. the basic cr-tree that uses only the proposed technique performs search operations up to 1 times faster than the r-tree while performing update operations similarly to the r-tree and using about 1% less memory space. compared with the basic cr-tree  most of cr-tree variants use less memory with algorithmic overhead. our analysis of the proposed technique and various indexes used in our experiment coincides with the experimental result.
　this paper is organized as follows. section 1 presents the basic idea of this paper and formulates our problem. section 1 presents the proposed mbr compression scheme  and the section 1 describes the proposed cr-tree. section 1 analytically compares the cr-tree with the ordinary r-tree  and section 1 presents the result of the experiment conducted to compare the cr-tree with
l1 cachel1 cachememoryblock size1b1b1kbsize1kb1kb~1mb~1gbhit time1 clock cycle1 clock cycles1 clock cyclesbacking storel1 cachememorydisksmiss penalty1 clock cycles1 clock cycles~1m clock cyclestable 1: summary of current memory hierarchy
the r-tree. section 1 finally concludes this paper.
1. motivation
1 memory hierarchy
table 1 summarizes the properties of the memory hierarchy observed in sun ultrasparc ii and intel xeon platforms. in ultrasparc ii  the block size is 1 bytes for the l1 cache and 1 bytes for the l1 cache . typically  the l1 cache can be accessed in one clock cycle  and the l1 cache can be accessed in two clock cycles. the memory access time depends on the dram type. when edo dram is used  each memory access takes 1 ns on average. when a cache miss occurs in the l1 cache and the l1 cache  a victim is selected. the miss penalty is the cost of selecting a victim and accessing the backing store. in ultrasparc ii  each l1 cache miss incurs two accesses to the l1 cache  and each l1 cache miss incurs four accesses to main memory.
1 basic idea
the idea in this paper is to make the r-tree cache-conscious by compressing mbrs. figure 1 illustrates the compression scheme used in this paper. figure 1 a  shows the absolute coordinates of r1~r1. figure 1 b  shows the coordinates of r1~r1 represented relatively to the lower left corner of r1. these relative coordinates have a less number of significant bits than absolute coordinates. figure 1 c  shows the coordinates of r1~r1 quantized into 1 levels or four bits by cutting off trailing insignificant bits. we call the resultant mbr qrmbr  quantized relative representation of mbr . note that qrmbrs can be slightly bigger than original mbrs.

figure 1: data structure of the cr-tree　the cr-tree is a cache-conscious r-tree that uses qrmbrs as index keys. for the sake of simplicity  the quantization levels are made the same for all nodes. figure 1 shows the structure of a cr-tree node that can contain up to m entries. it keeps a flag indicating whether it is a leaf or not  the number of stored entries  and the reference mbr that tightly encloses its entire child mbrs. the reference mbr is used to calculate the qrmbrs stored in the node. internal nodes store entries of the form  qrmbr  ptr   where ptr is the address of a child node and qrmbr is a quantized relative representation of the child node mbr. leaf nodes store entries of the form  qrmbr  ptr   where ptr refers to an object and qrmbr is a quantized relative representation of the object mbr. in most of our experiments  we quantize each of x and y coordinates into 1 levels or one byte.
1 problem formulation
our goal is to reduce the multidimensional index search time in main memory databases.
observation 1. let c be the node size in the number of cache blocks  and nnode access be the number of nodes accessed during search. the main memory indexes need to be designed to minimize c ， nnode access .
　in main memory  the index search time mainly consists of the key comparison time and the memory access time incurred by cache misses. if a cache miss occurs  the cpu has to wait until the missing data are cached. a cache miss can occur for three reasons: missing data  missing instructions  and missing tlb  table lookaside buffer  entries. therefore  we can roughly express our goal as minimizing
         tindex search   tkey compare + tdata cache + ttlb cache where tkey compare is the time spent comparing cached keys  tdata cache is the time spent caching data  and ttlb cache is the time spent caching tlb entries. for simplicity  we omit the time for caching missing instructions because the number of instruction misses mostly depends on the compiler and we can hardly control it.
　let ckey compare be the key comparison cost per cache block  ccache miss be the cost of handling a single cache miss  and ctlb miss be the cost of handling a single tlb miss. when the node size is smaller than that of a memory page  each access to a node incurs at most one tlb miss. for simplicity  we assume that nodes have been allocated randomly and that no node and no tlb entry are cached initially. then 
tindex search = c ， ckey compare ， nnode access
+ c ， ccache miss ， nnode access
+ ctlb miss ， nnode access
= c ， nnode access ，  ckey compare + ccache miss + ctlb miss / c 
　since ccache miss and ctlb miss are constant for a given platform  we can control three parameters: c  ckey compare  and nnode access. among them  we cannot expect to reduce ckey compare noticeably because the key comparison is generally very simple. in addition  ctlb miss and ccache miss typically have similar values. therefore  the index search time mostly depends on c ， nnode access.
observation 1. the amount of accessed index data can be best reduced by compressing index entries.
　the term c ， nnode access can be minimized in three ways: changing the node size such that c ， nnode access becomes minimal  packing more entries into a fixed-size node  and clustering index entries into nodes efficiently. the second is often termed as compression and the third as clustering .
　the optimal node size is equal to the cache block size in onedimensional case. in one-dimensional trees such as the b+-tree  since exactly one internal node is accessed for each level  the number of visited internal nodes decreases logarithmically the node size. on the other hand  the number of visited leaf nodes decreases linearly with the node size  and c increases linearly with the node size. therefore  c ， nnode access increases with the node size  and thus it is minimal when c is one.
　in multidimensional indexes  more than one internal nodes of the same level can be accessed even for the exact match query  and the number of accessed nodes of the same level decreases as the node size increases. since this decrease is combined with the log scale decrease of tree height  there is a possibility that the combined decrease rate of node accesses exceeds the linear increase rate of c. we will show analytically in section 1 that the optimal node size depends on several factors like the query selectivity and the cardinality.
　compressing index entries is equivalent to increasing the node size without increasing c. in other words  it reduces nnode access while keeping c fixed. thus  it is highly desirable. compression has been addressed frequently in disk-based indexes because it can reduce the tree height  but there is little dedicated work  especially in multidimensional indexes. the following simple analysis shows that why the compression in disk-resident indexes does not provide as significant gain as in main memory indexes.
　suppose that the tree a can pack f entries on average in a node and the tree b can pack 1f entries in a node using a good compression scheme. then  their expected height is logf n and log1f n  respectively. thus  the height of b is 1/log1f +1  = logf n / log1f n  times smaller than that of a. in disk-based indexes  the typical node size varies from 1kb to 1kb. assuming that the node size is 1kb and nodes are 1% full  f is 1    1〜1/1  for a b+-tree index and about 1    1〜1/1  for a twodimensional r-tree. thus  1/log1f is typically around 1. on the other hand  the node size is small in main memory indexes . with a node occupying two cache blocks or 1b  f is about 1 for a b+-tree and about 1 for a two-dimensional r-tree. thus  1/log1f is 1 for the b+-tree and 1 for the r-tree. in summary  node compression can reduce the height of main memory indexes significantly because the size of nodes is small.
　clustering has been studied extensively in disk-based index structures. in terms of clustering  the b+-tree is optimal in onedimensional space  but no optimal clustering scheme is known for the multidimensional case. instead  many heuristic schemes have been studied in various multidimensional index structures   . our work can be used with most of these clustering schemes.
1. mbr compression
here are two desirable properties of the mbr compression scheme that we seek.
overlap check without decompression: a basic r-tree operation is to check whether each mbr in a node overlaps a given query rectangle. checking the overlap of two mbrs should be done directly with the compressed mbrs stored in the nodes  without decompressing them. this property enables the basic r-tree operation to be processed with the one-time compression of the query rectangle instead of the decompression of all the compressed mbrs in the encountered nodes.
simplicity: compression and decompression should be computationally simple and can be performed only with already cached data. conventional lossless compression algorithms such as the one used in the gnu gzip program are expensive in terms of both computation and memory access because most of them maintain an entropy-based mapping table and look up the table for compression and decompression . thus  although they may be useful for disk-resident indexes  they are not adequate for main memory indexes.
1 rmbr
an obvious compression scheme is to represent keys relatively within a node . if we represent the coordinates of an mbr relatively to the lower left corner of its parent mbr  the resultant relative coordinates have many leading 1's. by cutting off these leading 1's and recording the number of bits cut off  we can effectively reduce the size of an mbr.
definition 1.  relative representation of mbr or rmbr  let p and c be mbrs  that are represented by their lower left and upper right coordinates  xl  yl  xh  yh   and let p enclose c. then  the relative representation of c with respect to p has the coordinates relative to the lower left corner of p.
rmbrp c  =  c.xl - p.xl  c.yl - p.yl  c.xh - p.xl  c.yh - p.yl 
　however  the following simple analysis shows that the rmbr technique can save only about 1 bits per mbr. for simplicity  we assume that the coordinates of mbr are uniformly distributed in their domain and that r-tree nodes of the same height have square-like mbrs roughly of the same size . without loss of generality  we assume that the domain of x coordinates has the unit length and consists of 1 different values equally spaced. let f be the average fanout of leaf nodes  and let n be the total number of data objects. then  there are roughly n/f leaf nodes  whose
mbrs have the area of f/n and the side length of f / n along each axis. since there are 1 different values in the unit interval along each axis  there are 1 f / n different values in the interval with the length of f / n . therefore  we can save 1- log1  1 f / n   bits or log1 n / f bits for each x coordinate
value. when n is one million and f is 1  about 1 bits are saved. by multiplying 1  we can save about 1 bits per mbr. note that the number of saved bits does not depend on the original number of bits as long as the former is smaller than the latter.
　we can easily extend this analysis result such that the number of bits saved is parameterized further by the dimensionality. the extended result is log1 d n / f or
	 log1 n  log1 f  /d	 1 
　the formula  1  increases logarithmically with n  decreases logarithmically with f  but decreases linearly with d. therefore  the number of saved bits mainly depends on the dimensionality. in one-dimensional space  the relative representation technique can save almost 1 bits for each scalar  but it becomes useless as the dimensionality increases.
1 qrmbr
since we cannot obtain a sufficient compression ratio from the rmbr technique alone  we introduce the additional quantization step. this step cuts off trailing insignificant bits from an rmbr while the rmbr technique cuts off leading non-discriminating bits from an mbr. after defining qrmbr  we show that quantizing an rmbr does not harm the correctness of index search and its small overhead by quantization is paid off by the significant savings in cache misses.
definition 1.  quantized relative representation of mbr or qrmbr  let i be the reference mbr  and let l be the desired quantization level. then  the corresponding quantized relative representation of an mbr c is defined as
qrmbri l  c  =
 φi.xl i.xh l  c.xl  φi.yl i.yh l  c.yl   Φi.xl i.xh l  c.xh   Φi.yl i.yh l  c.yh   where φa b l : r ★{1 ... l  1} and Φa b l : r ★ {1 ... l} are
φa b l r  =  1
 
  l  1
  l r   a / b   a  
  if r ＋ a
 if r − b
  otherwiseΦa b l r  =  1
 
  l if r ＋ a
 if r − b    l r   a / b  a    otherwise
computational cost. lemma 1 says that qrmbr satisfies the first of two desirable properties mentioned at the beginning of this section. therefore  the computational overhead of qrmbr technique is the cost of compressing the query rectangle into a qrmbr for each visited node. in our implementation  compressing an mbr into a qrmbr consumes at most about 1 instructions  which corresponds to less than 1 ns on a 1 mhz processor because of pipelining. in addition  it incurs no memory access as long as the query mbr and the mbr of the node on immediate access are cached.
lemma 1. let a and b be mbrs. for any mbr i and integer l  it holds that if qrmbri l a  and qrmbri l b  do not overlap  a and b also do not overlap.
proof. see appendix a. ＊
correctness. since it is generally not possible to recover the original coordinates of an mbr from its qrmbr  there is the possibility of incorrectly determining the overlap relationship between two mbrs. lemma 1 guarantees that there is no possibility of saying two actually overlapping mbrs do not overlap. thus  the qrmbr technique does not miss an object that satisfies a query.
　however  there is the possibility of concluding that two actually non-overlapping mbrs overlap. this means that the result of index search may contain false hits that have to be filtered out through a subsequent refinement step. however  this refinement step is needed for most multidimensional index structures because mbrs are typically approximations of objects . thus  requiring the refinement step itself is not an overhead  but the number of false hits can be. section 1 shows that the number of false hits can be made negligibly small  typically fewer than one  by choosing the quantization level properly.
1. cr-tree
1 algorithms
1.1 searching
the search algorithm is similar to the one used in other r-tree variants. the only difference is that the cr-tree compares a query rectangle with qrmbrs. instead of recovering mbrs from qrmbrs  the cr-tree transforms the query rectangle into the corresponding qrmbr using the mbr of each node as the reference mbr. then  it compares two qrmbrs to determine whether they overlap.
algorithm search. given a cr-tree and a query rectangle q  find all index records whose qrmbrs overlap q.
1. push the root node to the initially empty stack
s
1. if s is empty  stop
1. pop a node n from s and set r to be
qrmbrn.mbr l q 
1. if n is not a leaf  check each entry e to determine whether e.qrmbr overlaps r. if so  push e.ptr to s
1. if n is a leaf  check each entry e to determine whether e.qrmbr overlaps r. if so  add e.ptr to the result set
1. repeat from step 1
1.1 insertion
to insert a new object  the cr-tree traverses down from the root choosing the child node that needs the least enlargement to enclose the object mbr. when visiting an internal node to choose one of its children  the object mbr is first transformed into the qrmbr using the reference mbr. then  the enlargement is calculated between a pair of qrmbrs. when a leaf node is reached  the node mbr is first adjusted such that it encloses the object mbr. then  an index entry for the object is created in the node. if the node mbr has been adjusted  the qrmbrs in the node are recalculated because their reference mbr has been changed. if the node overflows  it is split and the split propagates up the tree.
algorithm insert. insert a new object o whose mbr is c into a cr-tree by invoking chooseleaf and install. the algorithms splitnode and adjusttree can also be invoked if needed. this algorithm is same as that of other r-tree variants.
algorithm chooseleaf. select a leaf node to insert a new mbr c  descending a cr-tree from the root. this algorithm is same as that of other r-tree variants.
algorithm install. install a pair of an mbr c and an object pointer p in a node n.
1. enlarge n.mbr such that it encloses c
1. make an entry of  qrmbrn.mbr l c   p  and append it to n
1. if n.mbr has been enlarged  recalculate all the qrmbrs in n by accessing their actual mbrs and invoke adjusttree passing n
algorithm splitnode. the cr-tree can use the split algorithms used in other r-tree variants including the r-tree and the r*-tree . in our experiment  the linear-cost split algorithm of the original r-tree was used. after splitting a node into two  the qrmbrs in the nodes are recalculated according to their mbr.
algorithm adjusttree. ascend from a leaf node l up to the root  adjusting mbrs of nodes and propagating node splits as necessary. when a node mbr has been adjusted  recalculate the qrmbrs in the node.
1.1 deletion
algorithm delete. remove index record e from a cr-tree. the cr-tree can use any of the deletion algorithms used in the r-tree and the r*-tree. however  the condensetree algorithm invoked by the delete algorithm needs a slight modification.
algorithm condensetree. given a leaf node l from which an entry has been deleted  eliminate the node if it has too few entries and relocate its entries. propagate node elimination upward as necessary. adjust all mbrs of the nodes on the path to the root  making them smaller if possible. when a node's mbr has been adjusted  recalculate the qrmbrs in the node. this last step is what is different from other r-tree variants.
1.1 bulk loading
bulk loading into a cr-tree is not different from that into other rtree variants. as long as qrmbrs are correctly maintained  existing bottom-up loading algorithms can be used directly
.
1 variants and space comparison
this paper also considers three variants of the cr-tree: pe  pointer-eliminated  cr-tree  se  space-efficient  cr-tree  and ff  false-hit free  cr-tree.
　the pe cr-tree eliminates most pointers to child nodes from internal nodes as in the csb+-tree. this extension can widen the cr-tree significantly because the key size of the cr-tree is now small unlike the r-tree. for example  when the size of qrmbr is four bytes  this pointer elimination doubles the fanout of internal nodes. however  it is just a minor improvement in most cases because pointers to data objects stored in leaf nodes can rarely be eliminated. when the average fanout of both internal and leaf nodes is 1  the number of internal nodes is about a ninth of that of leaf nodes. therefore  the overall increase of fanout is only about 1%. on the other hand  as in the csb+-tree  node split becomes expensive. the new node created by a split has to be
maximum fanoutnode spacetypical index sizeinternalleafinternalleafr-treemmns/1m 1m-1 ns/1m1 mbpe r-tree1mmns/1m 1m-1 ns/1m1 mbcr-tree1m-1.1m-1ns/ 1m-1  1m-1 ns/ 1m-1 1 mbpe cr-tree1m-1.1m-1ns/ 1m-1  1m-1 ns/ 1m-1 1 mbse cr-tree1m-1.1m-1ns/1m 1m-1 ns/ 1m-1 1 mbff cr-tree1m-1mns/1m 1m-1 ns/1m1 mbtable 1: space analysis  n: the number of leaf node entries  s: the node size in bytes; typical sizes are given when n=1 1 and s=1 stored consecutively with its siblings  and this often requires allocating a new space and moving the siblings.
　the se cr-tree removes the reference mbr from nodes of the pe cr-tree. this is possible because the reference mbr of a node can be obtained from the matching entry in its parent node. this extension increases the fanout of internal nodes by four and that of leaf nodes by two when the mbr size is 1 bytes and the qrmbr size is 1 bytes. this increase can be larger than the increase obtained in the pe cr-tree when the node size is as small as one or two cache blocks.
　while the above two extensions increase the fanout  the third extension to the cr-tree decreases the fanout of leaf nodes. since the qrmbr technique is a lossy compression scheme  the search result can be a superset of the actual answer for a given query. this can be avoided if we apply the qrmbr technique only to internal nodes and store actual mbrs in leaf nodes. called the ff cr-tree  this extension is useful when the subsequent refinement step is extremely expensive.
　table 1 shows the space requirements of the various index structures used in this paper  assuming all the nodes are 1% full. we assume that the size of mbr is 1 bytes  the size of qrmbr is 1 bytes  and the size of pointer is 1 bytes. the internal node space is calculated by dividing the leaf space by the average fanout of internal nodes minus one. this analysis shows that the pe cr-tree is not so different from the cr-tree in terms of the space requirement and the pe r-tree is no different from the rtree.
1. analysis
without loss of generality  we assume the data domain of unit hyper-square. for simplicity  we assume that data objects are uniformly distributed in the domain  and the query mbrs are hyper-squares. we further assume that the r-tree nodes of the same height have square-like mbrs roughly of the same size as in other analytical work .
1 number of accessed nodes
let h denote the height or level of a node assuming that the height of leaf nodes is one. let mh denote the number of nodes at the height of h. then  from the above assumption 
  n  
m h =   h   .
  f  
　let ah denote the average area that a node of height h covers. then  ah is 1/mh. using the minkowski sum technique   the probability that a node of height h overlaps a given query rectangle is  d s + d ah  d   where s denotes the size of the query rectangle. then  the number of height-h nodes that overlap the query rectangle is m h d s + d ah  d or
	  	  n  	  d
   1+ d    f h     s     .
　by summing this equation from the leaf to the root  the total number of node accesses in r-trees is
	 log f n  1 	  n  	  d
 
1+ ‘   1+ d    f h     s     .  1  h=1  
　the cr-tree accesses slightly more nodes than the r-tree because the qrmbr is bigger than the original mbr by the quantization error.
　let l denote the quantization level. then  each node has ld quantization cells  and the side length of each cell is d ah /l   where h denotes the height of the node. since whether to visit a child node is determined by comparing the qrmbr of the query rectangle and the stored qrmbr of the child node  the probability to visit a child node is
 d s + d ah /l + d ah 1 + d ah /l d . by multiplying by mh and summing from the leaf to the root  the total number of node accesses in cr-trees is
	 log f n  1  	  n  	  n  	  d
1+ ‘   1+ d    f h     s + d    f h+1     s /l    .  1  h=1  
　figure 1 compares equations  1  and  1  when the cardinality is one million and the query selectivity is 1%. here  we assumed that the pointer size is 1 bytes and that each node is 1% full. the mbr size is 1 bytes in 1d and increases linearly with dimensions. the qrmbr size is a one-fourth of the mbr size. in

	node size  bytes 	node size  bytes 
	 a  r-tree	 b  cr-tree
figure 1: number of node accesses in r-trees and cr-trees  n = 1m  s = 1%  mbr: qrmbr = 1 

node size  bytes 
 a  r-treenode size  bytes 
 b  cr-treefigure 1: number of cache misses in r-trees and cr-trees  n = 1m  s = 1%  mbr:qrmbr = 1 this figure  the number of node accesses decreases with the node size. the decrease rate is initially large  but it becomes smaller as the node size increases. for all the node sizes and all the three dimensionalities  the cr-tree surpasses the r-tree by more than twice.
1 number of cache misses
the number of cache misses can be easily obtained by multiplying equations  1  and  1  by the number of cache misses that one node access incurs. figure 1 shows the analyzed number of cache misses. it shows that as the node size grows  the number of cache misses approaches quickly to the minimum  and then increases slowly. in terms of cache misses  the cr-tree outperforms the rtree significantly  by up to 1 times. to obtain this figure  the equations  1  and  1  were multiplied by s/1  where s is the node size in bytes and 1 is the l1 cache block size.
　figure 1 a  shows a saw-like pattern that the number of cache misses decreases abruptly at certain node sizes while generally increasing with the node size. such bumps occur when the height of tree becomes smaller. for example  the 1d r-tree has the height of 1 when the node size is 1 or 1 bytes  but its height becomes 1 when the node size is 1 bytes. in other words  such bumps occur when the gain by the decrease of height surpasses the overhead associated with the increase of node size.
　although the optimal one-dimensional node size in terms of the number of cache misses is shown to be the cache block size in section 1  figure 1 shows that this choice of node size is not optimal in multidimensional cases as discussed in section 1. figure 1 shows the number of cache misses computed changing the query selectivity. the observation on this figure is that the optimal node size increases with the query selectivity in both the r-tree and the cr-tree. figure 1 a  shows that the optimal node size increases in the order of 1  1  1  1  and 1 bytes as the selectivity increases. figure 1 b  shows that the optimal node size increases in the order of 1  1  1  1  and 1 bytes as the selectivity increases. although we do not visualize because of the space limitation  the optimal node size increases in the same way as the cardinality and the dimensionality increase.
1 ratio of false hits by quantization
1	
node size  bytes 
 a  r-treenode size  bytes 
 b  cr-treefigure 1: increase of optimal node size with selectivity in 1d r-trees and cr-trees

	node size  bytes 	node size  bytes 
	 a  d =1	 b  qrmbr = 1b
figure 1: false hit ratio by qrmbr size and dimensionality  n = 1m  s = 1% following the same steps as in section 1  each quantization cell of a leaf node has the area of f /ld n and the side length of d f /ldn along each axis  and the probability that the qrmbrs of	the	query	mbr	and	the	object	mbr	overlap
is  d s +d a + 1d f /ldn   d .
  
therefore 	the	probability	that	a	false	hit	occurs	is
  d +d a + 1d f /ldn   d -  d s + d a d . dividing by  d s + d a d   s
 	 
the ratio of false hits incurred by quantization to actual answers is
	  1+ 1d f /ld n / d s + d a   d  1.	 1 
	 	 
　figure 1 shows the ratio when the cardinality is one million and the query selectivity is 1%. here  we assume that the pointer size is 1 bytes and that each node is 1% full. figure 1 a  shows the false hit ratio in the 1d cr-tree for three different qrmbr sizes: 1 bytes  1 bytes  and 1 bytes  and figure 1 b  shows the false hit ratio for three different dimensionality. the false hit ratio increases with both the node size and the dimensionality. using qrmbrs of 1 bytes incurs around one false hit in this configuration  but it saves tens or hundreds of cache misses as shown in figure 1.
1. experimental evaluation
to assess the merit of the proposed cr-tree and its variants  we conducted a series of experiments on a sun ultrasparc platform  1mhz cpu with 1mb l1 cache  running solaris 1.
　we implemented six index structures on 1d: the ordinary rtree  the pe r-tree  the cr-tree  the pe cr-tree  the se cr-tree  and the ff cr-tree. we also implemented a bulk-loading algorithm . we changed the size of nodes from 1 bytes to 1 bytes for the implemented index structures. we used 1-byte mbrs and changed the size of qrmbrs from 1 bytes to 1 bytes. if not specified  the default size of qrmbrs is 1 bytes  and the nodes are 1% full.
　we generated two synthetic data sets consisting of one million small rectangles located in the unit square. one is uniformly distributed in the unit square and the other has the gaussian distribution around the center point  1  1  with the standard deviation of 1. we set the average side length of rectangles to be 1.

	1	1	1	1	1
	node size  bytes 	node size  bytes 
	 a  s = 1%	 b  s = 1%
figure 1: search performance of bulk-loaded 1d indexes with uniform data set

node size  bytes   a  insertion timenode size  bytes   b  deletion timefigure 1: update performance on bulk-loaded 1d indexes with uniform data set1 search performance
in the first experiment  we compare the search performance of various indexes in terms of the wall-clock time spent processing a two-dimensional region query. we generated 1 different query rectangles of the same size  whose center points are uniformly distributed. we changed the size of query rectangles from 1% of the data space to 1%. since the data space is the unit square  the query selectivity is roughly same as the size of a query rectangle.
　figure 1 shows the elapsed time spent searching various indexes bulk-loaded with the uniform data set such that each node is 1% full. the observations on this figure are:
  as the node size grows  the search time approaches quickly to the minimum  and then increases slowly. the minimum shifts to the right as the selectivity increases. this trend holds for all the six trees  and it coincides with the analytical results presented in section 1.
  the cr-tree  the pe cr-tree  and the se cr-tree form the fastest group. the r-tree and the pe r-tree form the slowest group. the ff cr-tree lies between the two groups.
  although the se cr-tree is wider than both the cr-tree and the pe cr-tree  it performs worse. this is because the se crtree calculates the reference mbr of a node from the matching entry in its parent node. in our implementation  this calculation involves about 1 instructions and 1 bytes of memory write.
　we conducted the same experiment for the skewed data set. we could not find any noticeable difference from figure 1. in other words  all the six trees are robust to the skew for any node size.

	node size  bytes 	node size  bytes 	node size  bytes 
	 a  selectivity = 1%	 b  selectivity = 1%	 c  selectivity = 1%
figure 1: ratio of false hits incurred by quantization

　node size  bytes   a  selectivity = 1%　node size  bytes   b  selectivity = 1%node size  bytes   c  selectivity = 1%figure 1: search time with varying quantization levels1 update performance
to measure the update performance  we inserted 1 objects into indexes bulk-loaded with the one million uniform data set  then removed 1 randomly selected objects from the indexes.
　figure 1 a  and  b  show the measured elapsed time per insertion and deletion  respectively. for a given node size  the cr-tree consumes about 1% more time than the r-tree for insertion. however  when the fanout is same  for example  the cr-tree with the node size of 1 bytes and the r-tree with the node size of 1 bytes   the cr-tree performs similarly to or better than the r-tree. this can be explained in the following way.
　when descending a tree for insertion  the child node that needs to be enlarged least is selected. since the enlargement calculation consumes about 1 instructions in our implementation  it becomes more expensive than the cache miss in the cr-tree and its variants. since a single cache block contains about 1 qrmbrs in the cr-tree  the enlargement calculation cost is about 1 instructions per cache block  but a cache miss consumes about 1 processor cycles on 1mhz ultrasparc ii. on the other hand  since insertion accesses only one node for each height  the number of accessed nodes decreases logarithmically with the fanout  but the number of enlargement calculations for each node increases linearly with the fanout. thus  the total number of enlargement calculations increases with the fanout.
　the pe r-tree performs slightly worse than the r-tree because it increases the fanout by less than 1%. since the fanout of the cr-tree is about 1% larger than that of the r-tree  it performs worse than the r-tree for a given node size. since the fanout of the pe cr-tree is about 1% larger than that of the r-tree  it performs significantly worse than the r-tree for a given node size. on the other hand  when the fanout is same  the ranking of the cr-tree is determined by the saving in cache misses and the overhead of updating qrmbrs when the node mbr grows or shrinks.
　figure 1 b  shows that the rankings for deletion are slightly different from those for insertion. deletion is a combination of highly selective search and node update. as you can expect from figure 1  the cr-tree performs similarly to the r-tree as the selectivity decreases. on the other hand  node update becomes more expensive as the node size increases because the cost of updating qrmbrs increases. therefore  the cr-tree outperforms the r-tree when the node size is small  but they cross over as the node size increases.

1	1
node size  bytes 
 a  accessed index data1	1	1
node size  bytes 
 b  number of cache misses	1	1
node size  bytes 
 c  number of key comparisonsfigure 1: breakdown of search performance  s = 1% 1 impact of quantization levels
to assess the impact of quantization levels  we measured the ratio of false hits incurred by quantization and the search time for three different quantization levels  1  1  and 1. these correspond to qrmbrs of 1 bytes  1 bytes  and 1 bytes  respectively. in this experiment  we used the trees bulk-loaded with the 1m uniform data set.
　figure 1 shows the ratio of false hits measured varying the quantization level. in section 1  we have shown that the ratio of
false hits can be estimated by   1+ 1	f /l1n /  s + a   1  1 .
  
this ratio increases with the fanout or the node size  and decreases with the increasing quantization level and selectivity. figure 1 is consistent with the analytical result. with the 1-bit quantization  the result of the cr-tree search is almost the same as that of the r-tree search. with the 1-bit quantization  the cr-tree search result contains at most 1% more objects than the r-tree result. with the 1-bit quantization  the ratio of the false hits increases steadily with the node size  up to 1% when the node size is 1 bytes and the selectivity is 1%. when the selectivity is high  the graph shows a similar slope with respect to the selectivity but the ratio of the false hits is contained within a few percents. so the 1-bit quantization becomes useful as the selectivity increases.
　figure 1 shows the effect of the quantization on the search time. the time for filtering out the false hits is not counted. the figure shows that the 1-bit quantization performs the best when the selectivity is 1%. the 1-bit quantization with 1% selectivity performs well when the node size is small but becomes the worst as the node size grows. however  the 1-bit quantization performs the best regardless of the node size when the selectivity is high. this is because the number of false hits becomes relatively insignificant as the node size grows.
1 breakdown of search performance
to better understand the search performance of the indexes used in our experiment  we measured the amount of accessed index data  the number of l1 cache misses  and the number of key comparisons for the experiment reported in figure 1.
　figure 1 a  shows the amount of accessed index data  which is the number of l1 cache misses when no index data is cached initially or the worst-case cache misses. in terms of the worst-case cache misses  the six trees are clearly ranked by their fanout or in the order of the se cr-tree  the pe cr-tree  the cr-tree  the ff cr-tree  the pe r-tree  and the r-tree  from the best to the worst. the first three form one group  and the last two form another group as in figure 1. this result coincides with figure 1.
　figure 1 b  shows the measured number of l1 cache misses using the perfmon tool . the ultrasparc processors provide two registers for measuring processor events. we used the perfmon tool to make these registers count l1 cache misses and to read the values stored in them. the number of l1 cache misses is slightly different from the amount of accessed index data because of cache hits and missing instructions. instruction cache misses explains why the number of measured cache misses can be larger than that of the worst-case cache misses in figure 1 a  when both the node size and the selectivity are small.
　another observation on figure 1 b  is that the cache hit ratio increases with the node size. this has to do with the typical cache replacement policy based on the circular mapping of memory blocks to cache blocks. namely  the memory block with the address a is cached into the cache block whose address is determined by the cache size modulo of a. with this policy  a node consuming multiple memory blocks is placed consecutively in the cache. as the node size increases  the probability that the concurrently needed memory blocks are mapped to the conflicting location of the cache decreases.
　figure 1 c  shows that the qrmbr technique increases the number of key comparisons slightly. since the overlap test between two mbrs consumes less than 1 instructions on average in our implementation  saving an l1 cache miss is worth saving at least 1 overlap tests. the r-tree and the pe r-tree have similar fanouts and form one group. the pe cr-tree and the se cr-tree also have similar fanouts and form another group.
1. conclusion
there has been much research on multidimensional indexes. this paper addressed the problem of optimizing the cache behavior of multidimensional indexes for use in the main memory database environment. to pack more entries in the node whose size is given in multiples of cache blocks  we have proposed an efficient mbr compression scheme called the quantized relative representation of mbr or qrmbr which represents the coordinates of child nodes relatively to the mbr of the parent node and quantizes the resultant relative mbr using a fixed number of bits. the cr-tree based on qrmbr effectively increases the fanout of the r-tree and decreases the index size for the improved cache behavior.
　our extensive experimental study combined with analytical one shows that the 1d cr-tree and its three variants outperform the ordinary r-tree up to 1 times in the search time and use about 1% less memory space. to see the practical impact of the crtree  we are currently integrating the cr-tree into p*time  a prototype transact in memory engine under development.
