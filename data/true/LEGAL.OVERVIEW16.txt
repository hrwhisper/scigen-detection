   trec 1 was the second year of the legal track  which focuses on evaluation of search technology for discovery of electronically stored information in litigation and regulatory settings. the track included three tasks: ad hoc  i.e.  single-pass automatic  search  relevance feedback  two-pass search in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass  and interactive  in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback . this paper describes the design of the three tasks and analyzes the results.
1 introduction
the use of information retrieval techniques in law has traditionally focused on providing access to legislation  regulations  and judicial decisions. searching business records for information pertinent to a case  or  discovery   has also been important  but searching records in electronic form was until recently the exception rather than the norm. the goal of the legal track at the text retrieval conference  trec  is to assess the ability of information retrieval technology to meet the needs of the legal community for tools to help with retrieval of business records  an issue of increasing importance given the vast amount of information stored in electronic form to which access is increasingly desired in the context of current litigation. ideally  the results of a study of how well comparative search methodologies perform when tasked to execute types of queries that arise in real litigation will serve to better educate the legal community on the feasibility of automated retrieval as well as its limitations. the trec legal track was held for the first time in 1  when 1 research teams participated in an ad hoc retrieval task. this year  1 research teams participated in the track  which consisted of three tasks: 1  ad hoc  1  interactive  and 1  relevance feedback.
   the results of the legal track are especially timely and important given recent changes in the u.s. federal rules of civil procedure that went into effect on december 1  1. the amended rules introduce a new category of evidence  namely   electronically stored information   esi  in  any medium   intended to stand on an equal footing with existing rules covering the production of  documents.  against the backdrop of the federal rules changes  the status quo in the legal profession  even in large and complex litigation  is continued reliance on free-text boolean searching to satisfy document  and now esi  production demands . an important aspect of e-discovery and thus of the trec legal track is an emphasis on recall over precision. in light of the fact that a large percentage of requests for production of documents  and now esi  routinely state that  all  such evidence is to be produced  it becomes incumbent on responding parties to attempt to maximize the number of responsive documents found as the result of a search.
   the key goal of the trec legal track is to apply objective benchmark criteria for comparing search technologies  using topics that approximate how real lawyers would go about propounding discovery in civil litigation  and a large  representative  unstructured and heterogeneous  document collection. given the reality of the use of boolean search in present day litigation  comparing the efficacy of boolean search using negotiated queries with alternative methods is of considerable interest. the legal track has shown that alternative methods do identify many relevant documents that were missed by a reference implementation of a boolean search  though no single alternative method has yet been shown to consistently outperform boolean search without increasing the number of documents to review.
   the remainder of this paper is organized as follows. section 1 describes the ad hoc task  section 1 describes the interactive and relevance feedback tasks  section 1 lists the individual topic results  section 1 summarizes the workshop discussions and analysis conducted after the conference  and section 1 concludes the paper.
1 ad hoc task
in the ad hoc task  the participants were given requests to produce documents  herein called  topics   and a set of documents to search. the following sections provide more details  but an overview of the differences from the previous year is as follows:
  at the time of topic release  the b value  the number of documents matching the final negotiated boolean query  was provided for each topic in 1  along with an alphabetical list  by document-id  of the documents matching the boolean query  the  refl1b  run  for optional use by participants.
  a new evaluation measure  estimated recall b  where b is the number of documents matching the boolean query  was established as the principal measure for the track  although other measures are also reported . the legal community is interested in knowing whether additional relevant documents  those missed by a boolean query  can be found for the same number of retrieved documents.
  a new sampling method  herein called  l1   was used to produce estimates of the main measure for each topic for all submitted runs. all runs submitted to the ad hoc task were pooled this year  and all pooled runs were treated equally by the sampling procedure.
  the new topics were vetted to ensure that the b value for any topic was in the 1 to 1 range.  in 1  b ranged from 1 to 1. 
  participating teams were allowed to submit up to 1 documents for each topic  up from 1 in 1 .
  to facilitate cross-site comparisons  a  standard condition  run which just used the  typically onesentence  request text field was requested from all groups. additional runs which used other topic fields were also welcome  and encouraged.
  three different boolean queries were provided for each topic  defendant  plaintiff and final . in 1  the plaintiff and final queries had  usually  been the same.
1 document collection
the 1 legal track used the same collection as the 1 legal track  the iit complex document information processing  cdip  test collection  version 1  referred to here as  iit cdip 1   which is based on documents released under the tobacco  master settlement agreement   msa . the msa settled a range of lawsuits by the attorneys general of several us states against seven us tobacco organizations  five tobacco companies and two research institutes . one part of this agreement required those organizations to make public all documents produced in discovery proceedings in the lawsuits by the states  as well as all documents produced in a number of other smoking and health-related lawsuits. notable among the provisions is that the organizations were required to provide to the national association of attorneys general  naag  a copy of metadata and the scanned documents from the websites  and are forbidden from objecting to any subsequent distribution of this material.
   the university of california san francisco  ucsf  library  with support from the american legacy foundation  has created a permanent repository  the legacy tobacco documents library  ltdl   for tobacco documents . the iit cdip 1 collection is based on a snapshot  generated between november 1 and january 1  of the msa subcollection of the ltdl. the snapshot consisted of 1 tb of scanned document images  as well as metadata records and optical character recognition  ocr  produced from the images by ucsf. the iit cdip project subsequently reformatted the metadata and ocr  combined the metadata with a slightly different version obtained from ucsf in july 1  and discarded some documents with formatting problems  to produce the iit cdip 1 collection . the iit cdip 1 collection consists of 1 1 document records in the form of xml elements.
   iit cdip 1 has had strengths and weaknesses as a collection for the legal track. among the strengths are the wide range of document genres  including letters  memos  budgets  reports  agendas  minutes  plans  transcripts  scientific articles  and email  and the large number of documents. among the weaknesses are that the documents themselves were released as a result of tobacco-related discovery requests  and thus may exhibit a skewed topic distribution when compared with the larger collections from which they were initially selected. see the 1 trec legal track overview paper for additional details about the iit cdip 1 collection .
1 topics
topic development in 1 continued to be modeled on u.s. civil discovery practice. in the litigation context  a  complaint  is filed in court  outlining the theory of the case  including factual assertions and causes of action representing the legal theories of the case. in a regulatory context  often formal letters of inquiry serve a similar purpose by outlining the scope of the proposed investigation. in both situations  soon thereafter one or more parties create and transmit formal  requests for the production of documents  to adversary parties  based on the issues raised in the complaint or letter of inquiry. see the trec 1 legal track overview for additional background .
   a survey of case law issued subsequent to the adoption of the new federal rules of civil procedure in december 1 suggests that increasing attention is being paid by judges and lawyers to the idea of adversaries in litigation negotiating some form of  search protocol   including coming to consensus on what keywords will be used to search for relevant documents. in one reported case  a judge suggested to the parties that they reach consensus on what form of boolean queries should be used . in another case  a judge urged the parties to reflect upon recent scholarship discussing the use of  concept searches  to supplement traditional  keyword  searching  1  1 . although it remains unclear whether and to what extent lawyers are fully incorporating boolean and other operators  e.g.  proximity operators  in their proposed searches  as an example of best practices the trec 1 legal track chose to highlight the importance of negotiating boolean queries by including for each newly created topic a three-stage boolean query negotiation  consisting of  i  an initial boolean query1 as proposed by the receiving party on a discovery request  usually reading the request narrowly;  ii  a  counter -proposal by the propounding party  usually including a broader set of terms; and  iii  a final  negotiated  query  representing what was deemed the consensus arrangement as agreed to by the parties without resort to further judicial intervention.
   for the trec 1 legal track  four new hypothetical complaints were created by members of the sedona conference r working group on electronic document production  a group of lawyers who play a leading role in the development of professional practices for e-discovery. these complaints described:  1  a wrongful death and product liability action based on the use of a certain type of radioactive phosphate resulting in contaminated candy and drinking water;  1  a patent infringement action on a device going by the name  suck out the bad  blow in the good   designed to ventilate smoke;  1  a shareholder class action suit alleging securities fraud and false advertising in connection with a fictional  smoke longer  feel younger  campaign relying on  1s-era folk music;  and  1  a fictional justice department antitrust investigation looking in to a planned merger and acquisition of a casualty and property insurance company by a tobacco company. as in 1  in using fictional names and jurisdictions  the track coordinators attempted to ensure that no third party would mistake the academic nature of the trec legal track for an actual lawsuit involving real-world companies or individuals  and any would-be link or association with either past or present real litigation was entirely unintentional.
   for each of these four complaints  a set of topics  formally   requests to produce   were initially created by the creator of the complaint  and revised by the track coordinators. the final topic set contained 1 topics  numbered from 1 to 1. an xml formatted version of the topics  fulll1 v1.xml  was created for  potentially automated  use by the participants.
1 participation
1 research teams participated in this year's ad hoc task. the teams experimented with a wide variety of techniques including the following:
  carnegie mellon university: structured queries  indri operators  dirichlet smoothing  okapi bm1  boolean constraints  wildcards.
  dartmouth college: combination of expert opinion  ceo  algorithm  lemur/indri  lucene.
  fudan university: indri 1  yatata  word distribution model  corpus pre-processing methods  query expansion  query shrink.
  open text corporation: negotiated boolean queries  defendant boolean  plaintiff boolean  word proximity distances  vector query runs  blind feedback  fusion.
  sabir research  inc.: smart 1  statistical vector space model  ltu.lnu weighting  rocchio feedback weighting.
  university of amsterdam: query formulations  run combinations  lucene engine version 1  vectorspace retrieval model  parsimonious language modeling techniques.
  the university of iowa  eichmann : analysis of ocr  1 ngram analysis  translation of boolean query  pseudo-relevance feedback on persons  authors  recipients and mentions  and production boxes.
  the university of iowa  srinivasan : lucene library  okapi reranking  metadata  wildcard expansion  blind feedback  query reduction.
  university of massachusetts  amherst: indri  term dependence  markov random field  mrf  model  pseudo-relevance feedback  latent concept expansion  lce   phrase dictionaries  synonym classes  proximity operators.
  university of missouri  kansas city: query formulations  vector space model  language model  lucene  query expansion model  conceptual relevance framework.
  university of waterloo: wumpus search engine  cover density ranking  okapi bm1 ranking  boolean terms  character 1-grams  pseudo-relevance feedback  logistic regression  fusion  combmnz combination method  proximity-ranked boolean queries  relaxed boolean.
  ursinus college: document normalization  log normalization  power normalization  cosine normalization  enhanced ocr error detection  generalized vector space retrieval  query pruning.
   the teams submitted a total of 1 experimental runs by the aug 1  1 deadline  each team could submit a maximum of 1 runs . please consult the individual team papers in the trec proceedings for the details of the experiments conducted. also  please check the track web site  for the slides of many of the participant presentations at the conference  along with links to the aforementioned individual team members' papers in the trec proceedings.
1 evaluation
1.1 background on estimating precision and recall
the most straightforward way to produce an unbiased estimate of the number of relevant documents retrieved would be to use simple random sampling  i.e.  sampling in which all samples have an equal chance of being selected . unfortunately  for our purpose  the individual estimates would usually be too inaccurate. for example  suppose the target collection has 1 million documents  and for a particular topic 1 of these are relevant. suppose further that we have the resources to judge 1 documents. if we pick those 1 documents from a simple random sample of the collection  most likely 1 of the documents will be judged relevant  producing an estimate of 1 relevant documents  which is far too low. if 1 of the documents were to be judged relevant  then we would produce an estimate of 1 relevant documents  which is far too high.
   trec evaluations have typically dealt with this issue by using an extreme variant of stratified sampling. the primary stratum  known as the pool  is typically the set of documents ranked in the top-1 for a topic by the participating systems. traditionally  all of the documents in the pool are judged. contrary to the usual approach to stratified sampling  typically none of the unpooled documents are judged  these documents are just assumed non-relevant . for the older trec collections of about 1 documents   found that the results for comparing retrieval systems are reasonably reliable  even though that study also found that probably only 1%-1% of relevant documents for a topic were assessed  on average.
   traditional pooling can be too shallow for larger collections. as the judging pools have become relatively shallower  either from trec collections becoming larger and/or the judging depth being reduced  concerns have been expressed with the reliability of results. for example   recently reported bias issues with depth-1 judging for the 1 million-document aquaint corpus  and  estimated that fewer than 1% of the relevant documents were judged on average for the 1 million-document trec 1 legal track test collection. the trec 1 terabyte track  experimented with taking simple random samples of 1 documents from  up to  depth-1 pools  and estimated the average precision score for each run based on this deeper pooling by using the  inferred average precision   infap  measure suggested by . they found that infap scores were highly correlated with mean average precision  map  scores based on traditional depth-1 pooling.
1.1 the l1 method
the l1 method for estimating recall and precision was based on how the recall and precision components are estimated in the infap calculation. what distinguishes the l1 method is support for much deeper pooling by sampling higher-ranked documents with higher probability. for legal discovery  recall is of central concern. it was found last year by  that marginal precision exceeded 1% on average even at depth 1 for standard vector-based retrieval approaches. hence we used depth-1 pooling this year to get better coverage of the relevant documents. a simple random sample of a depth-1 pool  however  would be unlikely to produce accurate estimates for recall at less deep cutoff levels. hence we sampled higher-ranked documents with higher probability in such a way that recall estimates at all cutoff levels up to max 1 b  should be of similar accuracy.  details are provided in the following sections. 
   the l1 method was developed independently from the similar  statap  method evaluated by northeastern university in the trec 1 million query track .  the common ancestor was the infap method  which also came from northeastern.  both methods associate a probability with each document judgment. the differences are in how the probabilities are assigned  which should not matter on average  and in the measures being estimated  we are estimating the recall and precision of a set  whereas statap is estimating the  average precision  measure which factors in the ranks of the relevant documents . the l1 formulas are provided below  but please consult the northeastern work for a more thorough discussion of the theoretical underpinnings of measure estimation than we aim to provide here.
1.1 ad hoc task pooling
as stated earlier  a total of 1 runs were submitted by the 1 research teams for the ad hoc task by the aug 1  1 deadline. each run included as many as 1 documents  sorted in a putative best-first order  for each of the 1 topics. all submitted runs  plus a 1th run described below  were pooled to depth 1 for each topic and then each pool was sampled. the pool sizes before sampling ranged from 1  for topic 1  to 1  for topic 1 .  the pool sizes for all of the topics are listed in section 1. 
   the initial plan  given in the ad hoc task guidelines  was to assign judging probability p d  = min c / hirank d   1  to each submitted document d  where hirank d  is the highest  i.e.  best  rank at which any submitted run retrieved document d  and c is chosen so that the sum of all p d   for all submitted documents d  was the number of documents that could be judged  typically 1 . it was hoped that c would be at least 1 for all topics  so that we would have the accuracy of at least 1 simple random sample points for estimates at all depths. after the runs came in  it turned out the c values would range from only 1 to 1 if judging only 1 documents  substantially limiting the accuracy of the estimates of all measures.
   running some experiments  it turned out for specific depths we could get greater accuracy. for example  if all resources went to a simple random sampling for estimating precision at depth-b  we could get the accuracy of at least 1 sample points for each topic. if instead all resources were directed to just depth1  we could get at least 1 sample points for each topic. of course  if we targeted just one deep measure  we wouldn't have a lot of top-documents for training future systems or for contrasting our measure with traditional rank-based ir measures. experiments also found that if we just did traditional depth-k pooling  we could only go to at least depth-1 for each topic. but if all resources went to top-1 documents  we wouldn't have the ability to estimate deeper measures.
   the sampling process that we ultimately adopted was a hybrid of all of the above. the final p d  formula for the probability of judging each submitted document d was as follows:
if  hirank d   = 1  { p d  = 1; }
else if  hirank d   = b  { p d  = min 1    1/b + c/hirank d    ; }
else { p d  = min 1    1 + c/hirank d    ; }
   this formula causes the the first judging bin of 1 documents to contain the top-1 documents from each run  and it causes measures at depths b and 1 to have the accuracy of approximately 1+c simple random sample points. measures at other depths will have the accuracy of approximately  at least  c simple random sample points. if c is set to the largest multiple of 1 which produces a bin of at most 1 documents  c ranges from 1  topic 1  to 1  topic 1 . so by just dropping c by approximately 1 compared to the original plan  we gained more top document judging and at least 1-sample accuracy for depth-b and depth-1.
   to allow for the possibility that some assessors could judge more than 1 documents  the above process was adapted to have a first bin of approximately 1 documents and 1 additional bins of approximately 1 documents each  using the following approach. the c values were set so that the p d  values would sum to 1  and an initial draw of approximately 1 documents was done. then the c values were set so that the p d  values would sum to 1  and approximately 1 documents were drawn from the initial draw of 1  using the ratio of the probabilities ; the approximately 1 documents that were not drawn became  bin 1 . this process was repeated to create  bin 1    bin 1    bin 1  and  bin 1 . the approximately 1 documents drawn in the last step became  bin 1 .
   when the judgments were received from the assessors  as described in the next section   the final p d  values were based on how many bins the assessor had completed  e.g.  if 1 bins had been completed  then the p d  values from choosing c so that the p d  sum to 1 were used . if there had been partial judging of deeper bins  the judged documents from these bins were also kept  but with their p d  reset to 1. note that if the 1st bin was not completed  the topic had to be discarded. for each completed topic  the final number of assessed documents and corresponding c values are listed in section 1.
   two  runs  deserve special mention. first  the reference boolean run  refl1b   which would have been the 1th run  was not included in the pooling because it had been created by simply resorting one of the pooled runs  otl1fb  alphabetically by docno. instead  a 1th run called randoml1 was created  which for each topic had 1 randomly chosen documents that were not retrieved by any of the other 1 runs for the topic. we only included 1 random documents per topic  not 1  to reduce the number of judgments taken away from submitted runs. after the draw  it turned out that the 1st bin of 1 documents to be judged contained between 1 and 1 random documents  average 1 .
1 relevance judgments
for the trec 1 legal track  the track coordinators primarily sought out second-year and third-year law students who would be willing to volunteer as assessors in order to fulfill a law school requirement or expectation to perform some form of pro bono service to the larger community. based on a nationwide solicitation in mid-august 1  we received an enthusiastic response from students at a variety of u.s. law schools. all 1 new ad hoc task topics for the second year were assigned to assessors  but judgments for 1 topics were not available in time for use in the evaluation.1 most of the assessors  1  were law students from a wide variety of institutions: loyola-l.a.  1 volunteers   university of indiana-indianapolis  1   george washington  1   case western reserve  1   loyola-new orleans  1   boston university  1   university of dayton  1   university of maryland  1   and university of texas  1 . additionally  one justice department attorney and one archivist on staff in nara's office of the general counsel participated.
   this year  the assessors used a web-based platform developed by nist that was hosted at the university of maryland to view scanned documents and to record their relevance judgments. assessors found the interface easy to navigate  with the only reported problem being a technical one involving an inability to read or advance screens properly  due to use of a web browser other than firefox  the only one supported . each assessor was given a set of approximately 1 documents to assess  which was labeled  bin 1.  additional bins 1 through 1  each consisting of 1 documents  were available for optional additional assessment  depending on willingness and time.  it turned out that 1 of the assessors completed at least 1 of the optional bins  and 1 assessors completed all 1 optional bins.  in total  1 judgments were produced for the 1 topics. the assessment phase extended from august 1  1 through september 1  1.
   as in 1  we provided the assessors with an updated  how to guide  that explained that the project was modeled on the ways in which lawyers make and respond to real requests for documents  including in electronic form. assessors were told to assume that they had been requested by a senior partner  or hired by a law firm or another company  to review a set of documents for  relevance.  no special  comprehensive knowledge of the matters discussed in each complaint was expected  e.g.  no need to be an expert in federal election law  product liability  etc. . the heart of the exercise was to look for relevant and nonrelevant documents within a topic. relevance  consistent with all known legal definitions from wigmore to wikipedia  was to be defined broadly. special rules were to be applied for any document of over 1 pages. the same process was used for assessment for the interactive and relevance feedback tasks  which had different topics  as described below . see the trec 1 legal track overview for additional background  including a discussion of inter-assessor agreement which was measured in 1 but not in 1  .
on the whole  there was less confusion reported by assessors as to the definitional scope of the assigned topics in 1 than in 1  although some questions did arise. for example  for topic 1   all documents that memorialize any statement or suggestion from an elected federal public official that further research is necessary to improve indoor environmental air quality    the assessor questioned whether  memorialize  would be broad enough to include a mere reference to a superfund bill  without a quotation as such from the official. we responded that a quotation or allusion to an actual statement made by an official was necessary for the document to be responsive. on the same topic  the assessor wondered if a quote from an appointed federal official  e.g.  from the epa  would qualify  in light of the fact that the negotiated boolean contained the term  public official  without further qualification. we responded that the topic  not the boolean string  controlled interpretation  and that the topic contained the additional condition  elected   hence a mere quote from an epa official without more would not be responsive.
   in the case of topic 1  involving press releases concerning water contamination related to irrigation  the assessor reported afterwards that in performing the evaluation  it was sometimes difficult to determine what constituted a press release.  another post-assessment comment stated that because  assessments for responsiveness were done in different sessions  the triggers for responsiveness may not have been consistent   i.e.   sometimes a single word  convinced this assessor that the document was relevant   while at other intervals i read on to see whether  a finding of relevance  would make more sense in the narrower context of the complaint. 
   the assessor of topic 1 found it difficult to determine if certain types of radio and magazine advertising were sufficiently clear so as to say that the document made  a connection between folk songs and music and the sale of cigarettes   as the topic required. in the words of the assessor:  while it was easy to identify a connection when a music magazine contained a cigarette ad or when a cigarette magazine contained a music article  other magazines were less obvious. an outdoor magazine that contains an interview with a musician as well as a cigarette ad  for example. or a general interest magazine that contains a cigarette ad near its music section.  in wondering  how close  the connection had to be  the assessor went on to conclude that  ultimately  unless the cigarette ad was on the same page as the music section  or in the middle of it  i had to say there was no connection. 
