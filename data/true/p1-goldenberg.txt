improvements in data collection and the birth of online communities made it possible to obtain very large social networks  graphs . several communities have been involved in modeling and analyzing these graphs. usage of graphical models  such as bayesian networks  bn   to analyze massive data has become increasingly popular  due to their scalability and robustness to noise. in the literature bns are primarily used for compact representation of joint distributions and to perform inference  i.e. answer queries about the data. in this work we learn bayes nets using the previously proposed sbns algorithm . we look at the learned networks for the purpose of analyzing the graph structure itself. we also point out a few improvements over the sbns algorithm. the usefulness of bayes net structures to understand social networks is an open area. we discuss possible interpretations using a small subgraph of the medline publications and hope to provoke some discussion and interest in further analysis.
keywords
bayesian networks  structural learning  massive data  graph analysis  co-authorship networks
1. introduction
모the statistical literature on modeling social networks assumes that there are n entities called actors and that there exists information about binary relations between them. binary relations are represented as an n 뫄 n matrix y   where yij is 1  if actor i is somehow related to j and is 1 otherwise. for example  yij = 1 if  i considers j to be a friend . the entities are usually represented as nodes and the relations as arrows between the nodes. if matrix y is symmetric  then the relations are represented as undirected arrows. more generally yij can be real valued and not just binary  representing the strength of the relationship between actors i and j . in addition  each entity can have a set of characteristics xi such as their demographic information. then
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
linkkdd '1 august 1  1  chicago  il  usa copyright 1 acm 1-1... $1.
the n dimensional vector x = x1 ... xn is fully observed covariate data that is taken into account in the model .
모in our work  we assume that there are observations  particularly events relating entities  each paper is an event in the co-authorship dataset . however  the true underlying structure of relations between entities is not observed. we are not claiming to find the true underlying graph connecting the entities. by probabilistically modeling dependencies from the events data we aim to learn the relations robust to noise  the dependency structure and in the future predict the entities' further actions  using inference . in other words  based on the known information about simultaneous participation of entities in observed events  we construct a probabilistic model that would describe those events.
모studies on gene expression data  and social networks in particular suggest that correlations of entities on a local level are very important and in fact are what make up the global network  1  1 . the sbns algorithm  used here to learn the structure of bayes nets precisely makes use of that idea. the scalability of sbns is achieved by exhaustively searching over structures only on the local level for a large set of small subsets of variables. the advantage of such a
모structural learning algorithm is that the optimization never needs to be carried out on the global scale. so  along with being computationally practical  bayesian networks created by our algorithm have a very natural motivation stemming from those important domains.
모in this work we turn our attention to the question - how valuable are the graph structures of the bayes nets themselves  the resulting learned structures look like directed social networks  but the semantics behind links are different and one needs to be careful interpreting the results. in our experiments section we show two subgraphs built for the same authors of the medline dataset that exhibit different characteristics. we observe that by learning probabilistic models we are able to draw conclusions that were not possible by simply connecting co-authors together. in fact  we believe that probabilistic graphical models that learn dependencies between entities provide a very rich structure for analysis. this work is just the beginning of the exploration in this area.
모this paper is structured as follows. first we introduce notation and concepts essential to understanding the sbns algorithm. we then provide a shorter more intuitive description of the sbns indicating improved heuristics where applicable. further  we give an example of a possible interpretation of the bayes nets in terms of the social relations in the co-authorship publications. finally we discuss related
literature and conclude with our thoughts on future work.
1. background
모in this section we introduce the terms and concepts that are most relevant to our learning algorithm. it has to be noted that the proposed algorithm readily applies only to binary  otherwise known as transactional  data. thus  first we would like to introduce the general scenario where the algorithm can be applied.
1 data
모assume our training data is a collection of m records of observations of n binary variables x1 ...xn. write xji as the value of xi in the jth record where 1 뫞 i 뫞 n and 1 뫞 j 뫞 m. intuitively  each record denotes a collection of entities that participated in an  event . we use the words entity and actor  as in  social actor   interchangeably throughout the paper. the state of xi is 1 when actor i has participated in a given event and is 1 otherwise. for example  for a citation database  if two people i and k have co-authored a paper together  then for this event  co-authorship of a given paper  their states are xi = 1 and xk = 1 and the states of all other variables in the database for this event are 1  xt = 1  t =1 i k . examples of co-authorship datasets are the online library of computer science publications citeseer  the index of online library of medical publications medline and others  where each record is a list of co-authors of a particular paper.

figure 1: an example of representation  on the left  of the data  on the right . nodes in the network are people. rectangles are events relating them.
모these datasets have one important property in common. each record in these large datasets consists mostly of zeros: they are extremely sparse. sparseness has been considered hazardous in statistics as it may give rise to degeneracy in models. in fact  sparseness has many advantages that are very important for computational scalability. while the problems of degeneracy arise when attempting to build a global model  sparseness is helpful to quickly identify significant local models that can later be combined into a global model. it also is instrumental in greatly improving the speed of counting that is essential in obtaining sufficient statistics.
1 frequent sets
let the n variables be represented by integers {1 ...n}.
let the co-occurrence frequency of a set of attributes s   {1 ...m} be the number of records in which all the attributes in s are simultaneously set to 1.
	freq s  = |{i :  j 뫍 s xij = 1}|	 1 
given s 뫟 1 we say s is a frequent set of m attributes if s contains exactly m attributes and freq s  뫟 s. threshold s is called support in the data mining literature. given sparse data and a support s greater than about 1  it is surprisingly easy to compute all frequent sets . there is an abundance of literature on frequent sets as their collection is an essential part of the association rules algorithms  1  1  1  widely used in commercial data mining.
1 bayes nets
모bayesian network  bn  is a set {g 붿} where g is a directed acyclic graph {v e}  v is a set of nodes and e is a set of edges  and 붿 is a set of parameters obtained by maximizing a bayesian score  which is usually likelihood penalized for complexity. bns are factored probabilistic graphical models  where the joint distribution is determined by a product of conditional probabilities  i.e.
	p x1 ...xn  = yp xi|pa xi  	 1 
i
where pa xi  뫍 x is a set of parents of the variable xi in the dag. graphically  bns are represented using directed edges from parents pa xi  to children xi  for each i = 1...n. acyclicity of the dag guarantees the product in equation 1 is a coherent probability distribution. more information on bayesian networks can be found in  1  1 . note that directed arrows in the graph represent direct dependency of the outcome of variable xi on its parents pa xi . the dependencies can only be described in terms of the observed data  for example in a citation database case  a relation xi 뫸 xj  where pa xj  = xi  means that author xj is likely to appear as a co-author of the paper if xi is one of the co-authors. the dependence can also represent a negative correlation  i.e. in the above case knowing that xi is one of the authors  would make xj unlikely to be one of the co-authors.
1. sbns algorithm
모here we give a brief description of the sbns  screenbased bayes net structure search  and for details we refer the reader to .
모the sbns algorithm is a two stage process. during the first stage  which we will call local screening  sbns performs a bayes net structural search on each of the small subsets of variables defined by frequent sets. the resulting local structures comprise the restrictive pool of edges from which the global bayes net will be constructed at the second stage.
1 local screening
모the intuitive idea behind the local search stage is that we do a full structural search on very small subsets of variables. one of the ways to identify the  not necessarily disjoint  subsets is to use frequent sets.
모screening the frequent sets. suppose we have a collection of frequent sets {x : |x| = m m 뫟 1}. we call screening the process of finding the optimal bayes net structure for each of the frequent sets.
모first  we screen the pairs to find pairwise correlations. we add an edge between two variables to the edgedump if and only if a model that has an edge in either direction was found to have a higher score than a complete independence model between the two variables in the pair. we then in turn screen frequent sets of size 1  1  etc.
모it is possible that the dependencies in the frequent set s of size m   1 are already well-explained by interactions of order less than m. for example  suppose variables xi  xj and xk co-occur frequently  but their co-occurence is well explained by the local bayesian network dag structure of xi 뫹 xj 뫸 xk. in that case when searching through pairs   xi xj    xj xk    xi xk   the two-way interactions will already explain all dependencies of s. in fact  only dags that contain a node with m   1 parents could be missed by not considering an m-size tuple.
모we implement a screening test by searching over all possible dag structures for s and finding whether the best scoring structure has an m   1-parent node  we call it an m-way interaction . we thus allow s to pass the screening test if and only if s is best explained by a dag structure containing an m-way interaction. if s passes the screening test  all edges of the highest scoring dag are added to the edgedump - the set of edges that will eventually be considered for addition to the global bayes net.
1 stage 1: global bayes net
모once the edgedump is created  there are several ways to construct the global bayes net. in this work we use the following heuristic: prioritize the edges by the score of the highest scoring m-way interaction in which they participated; create the global bayes net by adding the highest correlated variables first. not all edges in the edgedump will be added due to the acyclicity property of bns. note that this approach is different from the heuristic originally proposed in  and seemed to have resulted in higher scoring bayes nets in practice. we start with an empty  edgeless  global bayesian network and iterate through the ordered contents of the edgedump  allowing each edge in turn to be added if and only if it improves the current score and avoids cycles. if the algorithm fails to add an edge with the direction stored in the edgedump  it tries to add the reversed edge to avoid cycles.
모the proposed deterministic approach for creating a global bayes net is fast and performs better on average than if the edges were added randomly. however it is a simple heuristic that imposes an ordering on the variables that is not necessarily optimal.
1. negative correlation
모in the previous section we pointed out that frequent sets allow the algorithm to consider only interactions that cause co-occurrence  and thus most likely positive correlations . due to the sparse nature of the data we are not omitting the strongest correlations in general. there is  however  still a danger that if a few variables have relatively high univariate marginal probability  they could cause significant negative correlations that we would miss. fortunately  such negative pairwise correlations can be detected cheaply by looking at a fraction of the pairs that have never occured together. we reduce the total number of entities significantly by only considering ones that occurred more than support s times in the dataset. this step is statistically justified because fewer occurrences mean lower possible mutual information. we then look at the pairs starting with the highest frequencies first.
모there are two possibilities for introducing the negatively correlated pairs. one is to introduce the edges to the edgedump from which the dag will be constructed. another possibility is to augment the dag created from positive correlations. each of the approaches has its own biases.
모when we decide whether to add an edge between possibly negatively correlated variables x and y to the edgedump before the dag is created  we compare the scores of the model x  y vs x y and add an edge if the former scores higher  note: the direction of the edge does not matter if the scoring metric is structurally equivalent . this approach has the disadvantage of not taking into account other dependencies that may already be modeled by the existing edges in the edgedump. it also might result in considering too many edges. however  the advantage of this approach is that when building a dag  the pool of dependencies is more complete. the second approach is to add edges between negatively correlated variables to the constructed dag. in this case  we add an edge only if it does not cause a cycle and improves the score. notice that neither of these conditions exist prior to building the dag and are thus impossible to verify in the alternative approach describe above. the advantage of this approach is that we are likely to consider fewer pairs and thus it may be more appealing for larger networks.
1. evaluation criterion
모there are several standard bayesian scoring functions that are often used in the literature to evaluate structural learning algorithms. the structures learned were evaluated based on one of the most often used: bdeu  with an equivalent sample size of 1. the general form for the bdeu scoring function is presented in equation 1. the bde score was originally suggested by . the u in bdeu just means a uniform prior over structures. the different scoring metrics are described in detail by .

모where i is the ith variable  qi - the number of states of the parents of xi and ri - the two states  true/false  of xi  in our case of binary variables. thus nijk is the number of records in our data where xi = k and pa xi  are in the jth state.
1 datasets
모we have applied the bayes net structural learning algorithm to several co-authorship datasets  sizes are in table 1 .
table 1: datasets and their sizes
	datasets	entities	records

	institute	1
	nips	1
	medline	1
	citeseer	1
1. the institute data is a set of records of collaborations between professors and students collected from publicly available web pages listed on the carnegie mellon university robotic institute's web site.
1. the nips data set contains co-authorship information of the neural information processing systems conference  nips  contained in proceedings 1  the preelectronic submission era 1.
1. the medline data is a sample of the co-authorship information of the publically available medical publication database medline.
1. the citeseer data is a set of co-publication records from the citeseer online library and index of computer science publications. since the entities are represented by first initial and last name  a single name might correspond to several people.
모one of the key reasons why the algorithm we propose is computationally feasible is the natural tendency of large social networks to be very sparse. in other words  most of the authors tend to co-author papers with only a handful of the others considered  while very few authors co-author with a large number of others. this effect of social nets has been extensively discussed in the social networks literature . since the co-authorship data can be interpreted as a bi-partite graph  where one type of nodes are authors and the other is publications  it is interesting to note that the number of people per publication also exhibits a power law property: there are few publications that have many authors and majority of publications have just a few authors. in figure 1 we provide the frequency plots for each of the datasets for both papers-per-author and authors-perpaper frequency distributions. from the plots it is apparent that co-authorship data is indeed distributed similarly to a power law  though some datasets tend to be particularly sparse  medline  and some datasets tend to have heavier tails  institute .

figure 1: marginal frequency distribution plots

1
 this dataset was made available by sam roweis and can be downloaded from
http://www.cs.toronto.edu/몲roweis/data.html
1 network interpretation
모usually in social science  1  1  it is assumed that the connections are given  for example if two people have coauthored a publication they are connected by an edge in the graph. in this case the connectivity in the graph can be easily interpreted in terms of original data and the research in these fields focuses mostly on modeling the generative mechanisms and understanding the global properties of the graph. even though the networks that we learn using the sbns algorithm are represented as graphs connecting the same nodes  they have different semantics and one should be careful when interpreting them. we claim however that the graph structure of the learned bayes nets can be used effectively to gain a different view of the relations between entities  actors  people  authors .
모what does the presence/absence of a link in the graph structure of the learned bayes net mean  first of all  the presence of a directed edge x 뫸 y means that if the author x is known to be one of the co-authors of a paper  we can infer something about the presence of y . by further inspection of the corresponding conditional probability table  cpt   we can say whether y is more or less likely to be an author if x is already an author. this is a standard bayes net analysis. it is interesting to note  that many edges in a bayes net correspond to the edges in the social network  i.e. some of the edges in the social network represent significant statistical dependence between the authors. also  due to the fact that sbns models negative correlations as well  we can gain additional information into the set of relations that normally cannot be inferred from the social networks. for example  two doctors  from the medline database  never co-author a paper together  but co-author quite often by themselves or with others. knowing a few of the  negative relations  might help the network analysts to discover polarity in opinions of the corresponding doctors.
1 example
모to illustrate how bayes nets help to improve understanding about relations among doctors  we give an example of analyzing connections of a random author from the medline publication dataset. the part of the network shown is obtained by learning the bayes net only on the publications that had the key word  overactive bladder   the support was set to 1 and the maximum tuple size was 1. the number of authors were consequently 1 and the number of corresponding publications is 1. sbns took 1 second to learn the network. figure 1 represents relations of the 1 levels of predecessors and successors of alan j wein in the learned bayes net.
모from the part of the corresponding probability table shown in table 1 it is evident that the presence of christopher r chapple is negatively correlated with the target alan j wein and that the presence of eric s rovner by himself is not as strong evidence for the presence of alan j wein as the presence of both eric s rovner and flavio e trigo-rocha.
모we also provide a social network graph where each link means co-authorship also starting with alan j wein as the main actor. we limit ourselves in this case to just people that have co-authored with alan j wein directly since the network grows very fast. each link has a weight which
represents how many publications the pair have appeared on as co-authors. the graph presented on figure 1 appears much more interconnected with a few fully connected

figure 1: a part of a bayesian network learned from medline publications with the keyword  overactive bladder 
alan j weinchristopher
r chappleeric	s
rovnerflavio
e	trigo-
rocha111.1.111111.1.1111table 1: part of a conditional probability table
 cpt  for alan j wein from the bayes net learned using sbns
cliques. there are also several people that were not appearing in our bayes net. note that the links with weights higher than 1 appear in the bayes net. most links in the presented social network however have a weight of 1  meaning that there is not enough evidence to claim a strong dependency between co-authors. thus  given the same data  even without increasing the support parameter  our bayes net learning algorithm is able to bring more clarity into the picture of relations.
1 dangers in interpretation of the bayes net
모there are certain things one should keep in mind when interpreting the bayes net graphs. here we list three issues that one must be aware of  but the list might not be complete.
1. if the two nodes are not linked  it doesn't mean theyare independent. it means that they are conditionally independent given their parents. thus one must not ignore the structure of the graph when reasoning about any two nodes.
1. proximity and number of hops in the network may notnecessarily translate into the strength of a relationship as might be done in social networks. for example  in the case of the two small subgraphs presented here 

figure 1: a part of a social network learned from medline publications with the keyword  overactive bladder  where each link represents co-authorships and weights represent the number of co-authored publications
from the social network on figure 1 we see that deborah lightner has co-authored with alan j wein once and john p heessakkers also co-authored with alan j wein once. in our bayes net on figure 1 however deborah depends on alan through john and another parent. this does not translate into deborah is less likely to co-author with alan than with john  however it does tell us that if we know that john was one of the authors  knowing about alan will not affect our belief in deborah's presence as a co-author.
1. our networks do not necessarily imply the causalitywhich is usually associated with bayes nets. causality needs to be tested by perturbing the evidence and seeing whether the outcome changes. we do not perform any such tests and thus in general we cannot say that the presence of x causes y to be present  we can state however that the presence of x makes y s presence more likely and vice versa  if that is what our conditional probability tables tell us.
1 global graph properties of the bayes nets
모in terms of the global properties of the graph  we also show the graph of degree distributions for the global social network for the overactive bladder and the indegree and outdegree of the learned bayes nets in figure 1. the bayes net structure seems to follow a power law as well. the top indegree nodes do not correspond to the top outdegree nodes. from the graph we can see that there are a few nodes with higher outdegree than the number of publications per person in the data  the social network degree distribution corresponds to precisely that . this is caused by a few of the negative correlations added  i.e. the doctors who are popular  having a high number of publications with other authors  tend to have extra edges corresponding to doctors with high number of publications whom they have never co-authored with.

figure 1: degree distributions for the social network and the indegree and outdegree for the learned bayes net for the publications from the medline data with the keyword  overactive bladder 
1 maximum frequent set size
모in our experiments we tried different maximum frequent set sizes:  mfss = 1...1 . the lower bound mfss = 1 means that we consider only pairs of entities and thus the structure learned is based solely on two-way marginal counts. our experiments on large datasets such as citeseer have shown that there is an obvious loss in accuracy when high order interactions are not taken into account. beyond a maximum frequent set size of 1 the number of frequent sets does not increase substantially in these datasets and hence the behavior of sbns changes little.
모we have to note here  that there is a natural upper bound on the maximum tuple size due to the sparsity of the datasets. for example  there are 1 publications in the citeseer database that have 1 authors and only 1 that have exactly 1 authors. the potential number of publications that have 1 authors  given the total number of authors in the database is 1 뫄 1  so the empirical number is only  1뫄1 % of the total. hence  we cannot expect a great improvement in the score of the bayes net when increasing the maximum tuple size  since there is not enough support for larger tuples.
1 support
모lowering support greatly increases the number of frequent sets to be considered during screening. however  it also introduces quite a few interactions between variables that have low marginal counts. model fitting in contingency tables in general is sensitive to very low marginal counts even if they are not zero . here we use bdeu  which is less sensitive to low counts. despite this  it seems to be a good idea to keep support relatively large in the case of very large datasets. we have tested several support sizes on smaller datasets and found that on very sparse datasets we can use support s = 1 without significant overfitting. however for large datasets such as citeseer we used support s = 1 to reduce computational cost without affecting the bdeu score too much. we also have to note that if s = 1 support is used  we cannot use the approach of adding negative correlations before constructing the dag  this approach becomes too costly. the addition of negative correlation after the construction of the dag has showen to improve the score  while keeping the computation costs low.
1 other datasets
모we have tried our algorithm on a variety of other datasets  for example imdb  the internet movie database  and iobdb  the off-broadway shows database . these datasets exhibit different properties than the publication data since it is more typical of plays and movies to have many actors. thus  the distribution of the entities per event is different. the sbns algorithm learns bayes nets that fit the data better  score higher  than the networks found by random hillclimbing. sbns however is more time consuming since on average the data is somewhat less sparse. we are planning to do the graph analysis of these domains in the future.
1. related work
모using frequent sets when learning bayes nets on the local scale was also explored in . the goal of this work was to answer probabilistic queries on a subset of variables  thus there was no need to combine local information to obtain the joint distribution once the query size was estimated. the authors have explored frequent sets for quick computations of the cpts and have noted that it is enough to look at all pairs to compute the triples without having to scan the dataset directly. the performance of bayes nets learned from a selection of variables was reported to be worse though close in accuracy to the inferences drawn from a bayes net learned on a full dataset. in  it has been proposed to integrate frequent sets as a local methodology when modelling joint distributions. this work has shown that mixture models obtained from frequent sets using maximum entropy are more accurate  thus supporting our claim that frequent sets contain important local information when modelling joint distributions.
모one approach to speed up structural search in bayes nets for massive datasets has been to restrict the possible parents. the full sparse candidate algorithm is presented in . in its original form it is a method to speed up hillclimbing at the cost of lower performance  though in practice the performance loss was shown to be insignificant for some of the small datasets. this work is yet another motivation for us  since structural search on the local scale inadvertently restricts the number of parents. however  since on the global scale the number of parents in our bayesian network is not limited we perceive it as an improvement on the original sparse candidate algorithm.
모the idea of augmenting bayes nets with high mutual information edges is based on the fact that such dependencies could not be accounted for in frequent sets. the fast computation used in this work is based on .
1 statistical network modeling
모the social network literature focuses predominantly on modeling p y-x   i.e. on probabilistically describing relations among actors as functions of their covariates and also properties of the graph  such as indegree and outdegree of individual nodes. a complete list of the graph-specific properties that are being modeled can be found in . thus  the models are geared to probabilistically explains the patterns of observed links and their absence between n given entities.
모several useful properties of stochastic models are listed in a brief survey work . some of them are:
  the ability to explain important properties between entities that often occur in real life such as reciprocity: if i is related to j then j is more likely to be somehow related to i; and transitivity: if i knows j and j knows k  it is likely that i knows k.
  inference methods for handling systematic errors in the measurement of links 
  general approaches for parameter estimation and model comparison using markov chain monte carlo methods
 e.g.  
  taking into account individual variability  and properties  covariates  of actors 
  an ability to handle groups of nodes with equivalent statistical properties .
모there are several problems with existing models such as degeneracy  analyzed by   and scalability  mentioned by several sources  1  1 . the new specifications for the exponential random graph models proposed in  attempt to find a solution for unstable likelihoods by proposing a slightly different parametrization of the models than used previously. experiments show that the parameters estimated using the new approach yield a smoother likelihood surface that is more robust and is less susceptible to the degeneracy problem. scalability remains to be a major issue. datasets with hundreds of thousands of entities are not uncommon in the internet and co-authorship based domains. to our knowledge  there are no statistical models in the social networks literature that would scale to thousands or more actors. parameter estimation for markov random fields is well-known to be intractable in general for large number of variables due to the computational complexity of the normalization constant which requires summation over all possible graphs with n nodes. the scalability problem has also been attributed to the tendency of the models to be global  i.e. most operate on the full covariance matrices . the use of mcmc approaches that tend to have slow convergence rate may also hinder computational speed of the parameter estimation in high dimensions.
모one of the more recent directions is latent variable models. those may be able to avoid the problems related to the use of markov random graphs. for example  the work of  proposes a model in which it is assumed that each actor i has an unknown position zi in a latent space. the links between actors in the network are then assumed to be conditionally independent given those positions. the probability of a link is a probabilistic function of the positions and actors' covariates. the latent positions are estimated from data using logistic regression. the general form of the model is: logodds yij = 1|zi zj xij 붸 붹  = 붸+ 붹txij +d zi zj   1  where d zi zj  is a distance between positions of the actors in latent space. while this model is promising  it also suffers from a lack of scalability of the parameter estimation.
1.1 network modeling in physics
모the graph theoretic area of physics that studies complex systems is directly applicable to social network modeling. though modeling of complex systems has developed seemingly in parallel to statistical modeling of social networks in social science  the findings in this area help to understand further the phenomenon of real networks organization and structure. the assumptions are the same: there are n actors  nodes  and there are m links between those nodes representing relationships among actors. the goal is also to understand and model structural properties of naturally occurring networks. the base model describing random graphs was developed by   where the expected number of edges in the graph is   where p is the probability of having any edge  and the probability of obtaining the observed graph is. however  it was noted that the degree distribution in random graphs does not follow power law p k  몲 k 붺 common in realistic networks. thus  scale-free networks  were introduced  1  1 .  have developed a generalized random graph model where the degree distribution is given as an input parameter. research in the field of physics gives more insight into graph growth  clusterability  graph diameter and the formation of a large component. a good summary of past and ongoing work and its relation to statistical physics is given in .
1. conclusion
모recent work has made it computationally possible to learn bayesian networks from very large datasets. one of the areas where such models could be of use is social science. in particular  in this work we focus on the connection between bayes nets and social networks and illustrate potential interpretations of the graphical structure learned. our simple example shows that bayes nets  while providing a compact representation  are a potential source for much deeper understanding of the data  such as learning about negative interactions among actors. this work is just the beginning of exploratory analysis using bayesian networks to model the structure of social networks themselves. we are currently collaborating with our colleagues at pfizer to gain deeper understanding into the usefulness of this representation.
1. acknowledgements
모we would like to thank ira haimowitz and pinaki karr from pfizer for helping with data and model interpretations. we would also like to thank jens nielsen and ricardo silva for insightful discussions.
