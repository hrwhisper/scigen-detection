given the ranked lists of documents returned by multiple search engines in response to a given query  the problem of metasearch is to combine these lists in a way which optimizes the performance of the combination. this paper makes three contributions to the problem of metasearch:  1  we describe and investigate a metasearch model based on an optimal democratic voting procedure  the borda count;  1  we describe and investigate a metasearch model based on bayesian inference; and  1  we describe and investigate a model for obtaining upper bounds on the performance of metasearch algorithms. our experimental results show that metasearch algorithms based on the borda and bayesian models usually outperform the best input system and are competitive with  and often outperform  existing metasearch strategies. finally  our initial upper bounds demonstrate that there is much to learn about the limits of the performance of metasearch.
1. introduction
　numerous search systems have been developed both in academia  and in industry  google  alta vista  lycos  hotbot  etc. . in practice  no one system performs  better  than each of the others under all circumstances  and the  best  system for a particular task may not be known a priori. this being the case  metasearch engines  such as metacrawler  profusion  savvysearch  metaferret  infind  etc.  have been introduced which query a number of search engines  merge the returned lists of pages  and present the resulting ranked list to the user.
　this after-the-fact combining of  complete  search engines can be called external metasearch. alternatively  metasearch offers a systematic way of internally incorporating all of the various types of evidence available within a single search engine. for example  in the context of web

 this work generously supported by nsf grants eia1  bcs-1  and career award ccr 1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  september 1  1  new orleans  louisiana  usa..
copyright 1 acm 1-1/1 ...$1.
page retrieval  many sources of information exist: each page has text  in-links  out-links  images  tags  keywords  and structural information. for each of these elements  numerous indexing and retrieval algorithms may exist. metasearch can be used to easily and automatically take advantage of the information provided by these disparate retrieval components.
potential benefits of metasearch include:
improved recall: recall is the ratio of retrieved relevant documents to total relevant documents. fusion can improve recall as long as the input systems are retrieving different relevant documents .
improved precision: precision is the ratio of retrieved relevant documents to retrieved documents. lee  argues that an  unequal overlap property  holds in ranked list fusion: different retrieval algorithms retrieve many of the same relevant documents  but different irrelevant documents. ng and kantor  conclude with lee that if this is true  any fusion technique that more heavily weights common documents should improve precision. vogt  calls this the  chorus effect. 
consistency: selberg and etzioni  show that current web search engines often respond to the same query very differently over time. inasmuch as fusing is an averaging procedure  we can expect the idiosyncrasies of any one system to be smoothed out  providing more reliable behavior . modular architecture: from the system design perspective  metasearch techniques allow a large  monolithic search engine to be decomposed into smaller  more specialized modules which can be queried in parallel and then fused.
　in short  metasearch holds the promise of obtaining better results than the best underlying retrieval system.
　current metasearch techniques can be characterized by the data they require: whether they need relevance scores or only ranks  and whether they require training data or not.  see figure 1.  a number of researchers have attempted to characterize when metasearch will yield good performance. the work of ng and kantor  1  1  in this area  as well as vogt et al.  1  1   essentially support the conclusion of croft : the systems being combined should  1  have compatible outputs  e.g.  on the same scale    1  each produce accurate estimates of relevance  and  1  be independent of each other.
　in this paper  we describe and investigate two models for the problem of metasearch  one based on democratic election strategies and another based on bayesian inference. unlike most existing metasearch algorithms  our techniques require rank information alone; the relevance scores assigned to re-
1
no training datatraining data
borda fusebayes fuse 
weighted
borda fusecombmnz　linear combination
figure 1: metasearch techniques may or may not require relevance scores  and may or may not require training. we discuss each of the given methods in the paper.
trieved documents by the underlying retrieval systems are not required. in many real-world situations  these relevance scores are not available  consider most search engines on the world wide web . we also describe and investigate a model for obtaining upper bounds on the performance of metasearch strategies.
　the first model we shall discuss  borda-fuse  is based on an optimal voting procedure  the borda count. metasearch algorithms based on the borda count have a number of advantages:  1  they require no training data;  1  they do not require relevance scores; and  1  they are surprisingly simple and efficient. our experiments with borda-fuse show that its performance is competitive with even the best existing metasearch strategies which typically require access to relevance scores which are not often available. we also describe a weighted version of this algorithm which requires minimal training but whose performance improves.
　the second model we shall discuss  bayes-fuse  is based on a bayesian analysis of the odds of document relevance. metasearch algorithms based on this model require training data  but they have the following advantages:  1  they do not require relevance scores;  1  they are based on a rigorous  probabilistic model of the problem; and  1  they can be implemented in both a simple  naive manner that assumes search engine independence and in a sophisticated manner that accounts for dependencies. our experiments with bayes-fuse show that its performance is competitive with even the best existing metasearch strategies which typically require access to relevance scores and that its performance can significantly exceed the best existing strategies when combining the results of disparate systems.
finally  we propose a model for obtaining upper bounds
on the performance of metasearch algorithms based on the idea of a constrained oracle. given a set of ranked lists to merge  we assess the performance of an omniscient metasearch oracle that knows which documents are relevant and which are not but is constrained in how it may rank these documents. the constraints are chosen so as to capture natural limitations of any reasonable metasearch algorithm. our experiments reveal a sizable gap between the oracle's performance and the performance of current metasearch algorithms.
　in the remainder of this work  we first review the relevant literature  noting especially the various metasearch algorithms that have been proposed in the past. we then present the borda-fuse  bayes-fuse  and upper bound models  describe a number of implementation issues  and detail the results of a number of experiments on trec data. finally  we conclude with directions for future research.
1. related work
　the use of data fusion to combine document retrieval results has received considerable attention in the past few years: it has been the subject of a number of doctoral dissertations  1  1  1  1   journal articles  1  1  1   and conference papers  1  1  1  1  1  1  1   being especially used in the trec competitions  1  1  1 . in this section we review the results of these publications as they relate to our work.
1 ceo model
　thompson  1  1  proposes a bayesian model that he calls the combination of expert opinion  ceo  model. it bears some resemblance to our bayesian model  but also differs in a number of ways. ceo is not really a general-purpose metasearch algorithm  but is a complete search system; it has two basic search components  each driven by bayesian updating of probabilities  that are fused with a bayesian formula. the fusion algorithm is not based on odds. and  it requires a full probability distribution to be specified for each document  not simply a probability of relevance . as far as we know  the ceo system has never been implemented.
1 min  max  and sum models
　fox and shaw  fuse based on the unweighted min  max  median  or sum of each document's normalized1 relevance scores over the constituent systems. fox and shaw also try to more or less heavily weight the number of systems that returned a given document  nd  by using the formula

for γ （ { 1 1}. the sum is over the constituent systems. when γ =  1  the system is equivalent to the average similarity over systems that returned d   combanz . when γ = 1  the result is the sum of the similarities   combsum   assuming  as they do  a similarity of zero for documents not returned  this is equivalent to the average similarity over all systems-even those that did not return d . and when γ = 1  the result is the formula they call  combmnz   multiply-by-number-non-zero . they found combsum to be slightly more effective than combmnz.
　lee  performed experiments with fox and shaw's algorithms  arguing that  different runs retrieve similar sets of relevant documents but retrieve different sets of non-relevant
1
documents.  he further argues that the combmnz combination rule appropriately takes advantage of this observation by heavily weighting common documents.
　in our own work  we have found that combmnz is the most effective of these schemes  and as it has become something of a standard  we shall use it as a baseline for experimental comparison.
1 averaging models
　in the context of the filtering problem  hull et al.  try averaging. the four systems they fuse each output estimates for the probability of relevance of each document  so they try both directly averaging these probabilities as well as averaging the log-odds ratios  log  a technique related to our own log-odds formulation. they find that averaging the logodds does yield improvement and in fact works better than more complicated regression and weighting techniques.
　manmatha et al.  also average probabilities of relevance  pointing out that this minimizes the bayes' error if the search engines are viewed as independent classifiers. without training data they transform arbitrary relevance scores into probabilities by fitting a negative exponential and gaussian mixture model to the scores  from which they infer probabilities of relevance.
1 logistic regression model
　savoy et al.  train a logistic regression model and find that over one trec collection  wsj1   it achieves performance slightly superior to a linear combination fusion model  improving on the best input system's performance by almost 1%.
1 linear combination model
　bartell   vogt  1  1  1  1   and others experiment with linearly combining the normalized relevance scores given to each document. that is 

where the sum is over the constituent systems. note that this fusion model requires both relevance scores and training data to determine the weight αi given to each input system. although good results are achieved in specific cases  this technique has not yet been shown to produce reliable improvement.
1. borda-fuse
　in this section  we describe our first model for the metasearch problem which is based on election strategies. we begin by describing the corresponding problem of voting.
1 a voting model
