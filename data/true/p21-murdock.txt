contextual advertising is a growing category of search advertising. it presents a particular challenge to ad placement systems because of the sparseness of the language of advertising. we present a system that is language independent and knowledge free based on svm ranking. we evaluate it on a large number of advertisements appearing on real web pages. our contribution is two new classes of features of similarity between ads and web pages based on machine translation technologies. we show that our features significantly improve performance over baseline techniques.
1. introduction
　search advertising  of which contextual advertising is one category  produces the lion's share of advertising revenues on the internet . in contextual advertising  the ad is represented by a url and a short textual description. the ad is placed in a target web page based on similarity between the content of the target page and the ad description provided by the advertiser. the user views the ad as a hyperlink and a short description  usually a sentence or fragment . a click on the hyperlink takes the user to the ad landing page. as with sponsored search  advertisers pay for each click on the ad url. a key difference is that contextual advertising appears in a web page and a user may view the ad without ever having visited a search engine  whereas sponsored search advertising appears in the list of search engine results in response to a user's query. figure 1 shows an example of contextual advertising. note that the banner ad in the rightmost column is not an example of a contextual ad.
　traditional advertising makes extensive use of imagery and symbolism. contextual advertising on the internet uses only text. the advertiser is not at liberty to choose the target page  and ad placement systems are ill-equipped to place ads based on the image content of the target page. while the imagery and text surrounding the ad on the web page may change the effectiveness of the ad  this is beyond the control of both the advertiser and the ad placement system.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
adkdd '1 august 1  1  san jose  california  usa.
copyright 1 acm 1-1-1/1 ...$1.
　the language of advertising has evolved to communicate the maximum information in the fewest possible words. ads are designed to be memorable  to elicit emotions or associations  to provide key information  and to imply meaning. because of the brevity of ads  words are chosen so as to imply information without necessarily stating it directly. for example  the slogan  i can't believe it's not butter!  implies that butter is preferable  and that this product is indistinguishable from butter. furthermore  advertisers make use of slogans or cultural associations to carry their message  such as the slogan  got milk  .
　contextual advertising presents a particular challenge because the information carried in the ad must be represented wholly by the textual description  and the text must be brief. the challenge to advertisers is to design an ad that drives traffic to the landing page. the challenge to the search engine is to provide appropriate context for the ad to maximize the number of user clicks on the ad url. yoo  showed that the effectiveness of an ad is correlated to its level of congruency with the surrounding context  even when the ad does not elicit any conscious response  which supports the assumption that ads should be topically relevant to the surrounding context.
　because there are few terms in the ad representation  an ad may not contain any terms directly identifying the product or product category. in fact  the ad representation may not contain any content terms. traditional information retrieval techniques fail in these cases because of the inherent compactness and sparsity of the ad representation.
　in this paper we present a system that re-ranks ad candidates based on the noisy-channel model using features derived from machine translation technologies. our system assumes that each web page has been provided with several ad candidates of varying quality. we seek to re-rank the candidates so that the best ads are at the top of the list. our primary contribution is two classes of features that show statistically significant improvement over baseline features. in addition  our study is the largest study to date  with 1 target pages  and 1 ad-page pairs. the proposed system is language independent  and requires no external resources. the paper is organized as follows. in section 1 we summarize previous work in contextual advertising. in sections 1 and 1 we describe the data  evaluate the relevance assessment  and present the features. we describe the experimental setting in section 1. section 1 presents the results  followed by a discussion in section 1. finally  we present our conclusions and future work in section 1.

figure 1: an example of contextual advertising. the links in the center of the page are contextual ads. the links in the left column are not contextual ads. the banner ad in the right column is also not a contextualad.
1. related work
　one approach to contextual advertising is to represent the target page by its key terms  and then place ads in the target page matching the key terms. yih et al.  propose a method for extracting keywords from pages  using a supervised approach and a corpus of pages whose keywords are manually identified. they show that a model using logistic regression outperforms traditional vector models with tf.idf term weights for the task of keyword extraction. they stop short of demonstrating the technique in a contextual advertising setting.
　broder et al.  classify both target pages and ads into an extensive taxonomy  and match ads to target pages falling in the same node in the taxonomy. each node in the taxonomy is represented by a set of bid phrases or queries corresponding to a concept. to build the taxonomy  the nodes are seeded by a small number of examples  and then human editors populate the nodes with around one hundred additional queries. the resulting system improves accuracy over a system that simply matches keywords in ad descriptions to target pages  for a test set of 1 target pages. such a taxonomy should be sufficiently extensive to provide meaningful matches and coverage. extending the system to other languages requires constructing a new taxonomy or translating the existing one. an advantage of this system is that it can be used in conjunction with more general approaches to improve accuracy.
　ribeiro-neto et al.  focus on the vocabulary mismatch problem  noting that the lack of overlap in the vocabulary of the ads and the vocabulary of the target page degrades accuracy. they refer to this as the vocabulary impedance problem. to overcome this limitation they expand the vector representation of the target page to increase the chance of matching some of the terms in the ad. we take a similar approach  but rather than expand the representation of the target page  we consider that the ad and the target page have different vocabularies  and we learn a  translation  between the two. in addition  their experimental setup is different. they evaluate 1 web pages  and retrieve ads for each page from a large pool of ads. we evaluate 1 web pages  and assume that we are re-ranking ad candidates.
　as a follow-on to ribeiro-neto et al.  lacerda et al.  use machine learning to find good ranking functions for ads  and apply a genetic programming algorithm to select a ranking function that maximizes the average precision on the training data. the resulting ranking function is a non-linear combination of simple components based on the frequency of ad terms in the target page  document frequencies  document length and size of the collections. they find that the ranking functions selected in this way are more accurate than the baseline proposed in ribeiro-neto et al. . they use 1 web pages  and retrieve ads for each page  as described in ribeiro-neto et al.   partitioned into test and training sets.
　our system uses features derived from statistical machine translation models. machine translation has been used in several other information retrieval tasks  such as document retrieval   sentence retrieval for question answering  and novelty detection   as well as community question answering . in each of these cases  the retrieval model itself includes a translation probability. in our case  we are using translation probabilities as features in a learned ranking function.
our assessmentsrelevantnonrelevanttotalrelevant11nonrelevant11total11table 1: the inter-assessor agreement between our relevance assessments and the binary relevance assessment based on the composite relevance score.
1. data
　our data originally comprised more than 1 target page-ad pairs. pairs for which no landing page was available were excluded. we also excluded target pages for which there was only one candidate ad  and target pages for which all add candidates were assigned the same relevance score by the assessors  because they are not useful for learning ranking functions. after filtering these examples from the data  we were left with 1 pairs  corresponding to 1 target pages  where the average number of ads per target page is 1. all 1 target pages were used for evaluation.
　each pair was evaluated by assessors who were experts in content match evaluation  on a three-point scale. a score of one indicates the ad was relevant to the target page  two indicates the ad was somewhat relevant  and three indicates the ad is not relevant to the target page. the assessors' scores were converted to a binary relevance judgment by averaging the raw scores and then assuming that pairs whose average score was 1 or higher were not relevant. we set the cutoff at 1 because an ad receives a composite score of 1 when the sum of three assessors scores is 1  which corresponds to individual scores of  1  1  1  or  1  1  1 . we estimated inter-assessor agreement to empirically justify our choice of 1 as a cutoff.
　we did not have access to the original relevance judgments  only to the averaged score   so we could not evaluate interassessor agreement directly. to estimate this  we randomly sampled 1 target pages from the complete corpus  and then evaluated the agreement between our assessments and the binary score derived from the averaged scores. table 1 shows the results. in all  we judged 1 target page-ad pairs. the cohen's kappa  between our assessments and the binary relevance score based on the original assessments was 1. this indicates a high level of agreement between our assessment and the composite binary judgment. we experimented with various cutoffs for the composite score  and cohen's kappa is highest when the cutoff is 1  which confirms our intuition about the relationship between the composite score and the relevance of an ad for a target page.
1. features
　the noisy channel model refers to the notion that a sentence spoken in one language is a garbled  noisy version of the same sentence spoken in another language. we view a description of an ad as a noisy version of the text in the ad landing page. we propose that we can  translate  between the ad description and its actual text to overcome the vocabulary mismatch between the two. this is consistent with findings in ribeiro-neto et al  that the ad landing page provides useful information for matching pages. our features fall into three categories: baseline features based on cosine similarity and term overlap between the ad and the target page  features based on statistical machine translation  and features based on machine translation evaluation. in all  we considered fourteen features.
　baseline features based on cosine similarity measure the lexical overlap between the target page and the ad. the translation features bridge the gap between the language of the target page and the language of the ad by providing a mapping between the two vocabularies. the translation evaluation features also measure lexical overlap between the target page and the ad  but whereas cosine similarity only considers unigrams  translation evaluation features consider higher order n-grams.
1 cosine similarity features
　we designed the baseline features to capture the best performing system from ribeiro-neto et al. . we compute five features of the cosine similarity: the cosine similarity between the target page and the ad  where all ad fields are treated as a bag of words  and one feature for each ad field  title  keyword  summary  and landing page  that is the cosine similarity between the ad field and the target page.
　in ribeiro-neto et al. they show significant improvement by constraining the placement of ads such that ads are only placed in target pages that contain all of the ad keywords. this filter is too strict for our data  because we are reranking ads that have been selected by a retrieval system. instead we encode this constraint as a pair of binary features  rather than as a filter. the first feature is  one  if all of the keywords are present in the page  and zero otherwise. the second feature is the complement of the first.
1 machine translation features
　statistical machine translation systems estimate the conditional probability that a source sentence is a translation of a target sentence  given an alignment between the two. in ibm model 1  all alignments are considered equally likely  and the probability of an ad landing page  l  given an ad description  d  is:

where p l|d  is maximized subject to the constraint
x
	p l|d  = 1	 1 
l
　in the above equations  d is the length of the ad description  m is the length of the ad landing page  az is the alignment between a word in the ad description to a word z in the ad landing page. details of all ibm translation models are given in brown et al. . we used the implementation of ibm model 1 in giza++ 1 to construct a translation table of terms in the ad description. we computed four features of translations between ad descriptions and target pages:
translation probabilities
the average of the translation probabilities of all words in the target page  translated  to all words in the ad description. this feature is a real number between zero and one.

1
giza++	can	be	downloaded	from
http://www.fjoch.com/giza++.html  may 1 
keyword translation probabilities
the average of the translation probabilities of all words in the target page  translated  to all words in the keywords for the ad. this feature is a real number between zero and one.
translation proportion
the total number of translations of ad terms appearing in the target page  divided by the number of ad terms. this is a real number between zero and infinity  with typical values being between zero and five.
keyword translation proportion
the total number of translations of ad keywords appearing in the target page  divided by the number of ad keywords. this is a real number between zero and infinity  with typical values being between zero and five.
1 translation evaluation features
　one issue in evaluating translations is that there are many ways to say the same thing correctly. machine translation systems are typically evaluated by measuring the overlap between the translation and a set of gold standard translations. the bleu metric  takes the geometric mean of the n-gram precision between the gold standard and the translation  with a penalty for very short translations. it has been shown to correlate with human assessments of translation quality.

　where n = 1  and wn = n 1  lref is the number of words in the target page title  lsys is the number of words in the ad title  or landing page   and pn is defined as:
		 1 
　where cim is the number of n-grams in segment i in the ad title that have a matching cooccurrence in segment i in the target page title  and ci is the number of n-grams in segment i in the ad title.
　a more lenient measure of translation quality is the nist score   which is a variant of bleu.

 1 
　whereis an n-gram of length k co-occurring in both sentences   is an n-gram of length k in the ad title  β is a constant that regulates the brevity penalty  n = 1  lref is the average number of words in the target page title  and lsys is the average number of words in the ad title  or landing page .  info  is defined as:
		 1 
in our data  the bleu score between the ad titles and the target page title is zero for nearly every example. all data were stemmed using a krovetz stemmer  and lowercased before computing the score. we compute three features based on bleu and nist:
nist
the nist score between the ad title and the target page title. the motivation for this is that the titles are more similar to each other in length and character. the analogous feature for bleu scores was not used because it was nearly always zero.
nist landing
the nist score between the ad landing page and the target page title.
bleu landing
the bleu score between the ad landing page and the target page title.
1. experimentation
　a small number of ads are usually placed in a target page - typically between one and five ads - thus we are mostly concerned with precision at the top of the list  rather than the quality of the ranking. for this reason we evaluate the precision at ranks one  three and five. that said  we can construct a ranking of ads based on the average of the raw relevance scores assigned by the assessors. to give an approximation of the quality of the ranked list  and also to evaluate how well our model is learning to rank  we evaluate the kendall's τ  between the ranking given by the average raw scores and the ranking given by the model. as there may be ties in the scores  we use a version of kendall's τ that accounts for arbitrary ties in both rankings :

where c is the number of concordant pairs  the same ad at the same rank in both ranked lists   d is the number of discordant pairs  n is the total number of ads ranked  t1 is the number of ties in the first ranked list  and t1 is the number of ties in the second ranked list.
　our baseline is cosine similarity with tf.idf term weights  which represents a standard information retrieval approach. we investigated the bm1 retrieval model but found that it did not perform significantly better  and cosine similarity has the advantage that there are no parameters to tune. we used the implementation of cosine similarity in the lemur toolkit . the ads and target pages were stemmed using a krovetz stemmer   and stop words from the list of 1 words in the terrier retrieval platform  were removed.
　we would like to be able to incorporate arbitrary features of target pages and ads  and it is not straightforward how to do this in standard information retrieval settings. instead we apply machine learned ranking methods in order to learn a ranking function which weights each feature individually. specifically  we used a support vector machine  svm  which optimizes a function directly related to kendall's tau   the number of concordant pairs in the candidate ranking and the gold standard ranking. we used
retrievalp 1p 1p 1τbad1111landing1111svmp 1p 1p 1τbad1111landing1111ad  individual fields 1111table 1: comparing ranking ads using a traditional information retrieval system based on cosine similarity with a ranking svm using the cosine similarity as a feature. the best results are shown in bold.
the implementation of a ranking svm  in svm-light1. in all experiments  we learned parameters using ten-fold crossvalidation. statistical significance is calculated with a twotailed t-test for p   .1.
1. results
　contextual advertising can be framed as a retrieval problem: the target page is a query  and it retrieves from an index of ads. each ad has several fields that can be indexed separately. in table 1 the rows indicated  ad  reflects the results when all fields of the ad  including the landing page  are treated as a unit. the row indicated  landing  reflects the retrieval results when the ad is represented only by its landing page. the final row of table 1 shows the result of computing the cosine similarity with each ad field individually and creating a separate feature for each cosine similarity value. as expected  there is no real difference between retrieval and a ranking svm when the similarity function is the same  as can be seen comparing the two rows labeled  ad . computing a separate feature for each ad field offers a small incremental improvement over the other methods. all further experiments in this paper use a ranking svm with separate cosine similarity features computed over the ad fields.
　ribeiro-neto et al.  found that using cosine similarity between the ad landing page  with the additional constraint that the ad keywords must appear in the target page produced the best results. to capture this constraint  we added a pair of binary features described in section 1. in our data  almost none of the target pages contain all of the ad keywords  so we made it a feature rather than a filter. the row labeled  ad + keywords  in table 1 shows the result of adding these features to the best performing model from table 1. all results following use this base set of features  in addition to the features reported.
　although the cosine similarity between the target page and the landing page picks up vocabulary not used in the description of the ad  matching terms are weighted according to their frequency. in a translation framework  words may or may not match. for example  the translation model may learn that  mortgage  and  home  are associated with each other. furthermore  since it trains on the entire corpus of ads  related words appearing in the target page and the landing page that don't appear in the ad description will contribute to the similarity between the target page and the ad. while cosine similarity estimates the contribution

1
 open source ranking svm software is available from http://svmlight.joachims.org/  may 1 
svmp 1p 1p 1τbad1111ad + keywords1111key trans prob1111key trans prop1111all keyword trans1*111trans prob1*111trans prop1*111all translation1*111table 1: features based on statistical translation models.  prob  stands for  probability  and  prop  stands for  proportion  as described in section 1. the best results are shown in bold. results with an asterisk are statistically significant with respect to the  ad  basline.
of a matching pair based on their term frequency with a discounting factor for the inverse document frequency for a given ad-target page pair  the translation model learns a probability distribution over all word pairs  and may provide a more appropriate similarity score.
　table 1 shows the results of using the four translation features. as stated in section 1 the  trans key  features compute the average translation scores for the target page and the ad keywords. the other translation features are computed over the ad description and the target page  and these features produce statistically significant improvements in precision at rank one. the best feature from this set is the average translation probability computed over the ad description  which includes the keywords  the title  and the summary . combining the keyword features with the ad features produces results slightly worse than using no translation features.
　while the translation features capture related terms  the metrics nist and bleu give a summary of the degree to which two segments of text overlap exactly. the bleu metric measures the overlap of 1-grams. nist relaxes the restriction to allow for matching bigrams and trigrams. since the ad and the target page are both in english  albeit different forms of english  we would expect matching n-grams to be a good indicator of topical relatedness.
　in our data  the bleu score between the ad title and the target page title was zero for nearly every example. for this reason  we computed the bleu score between the ad landing page and the target page title. as it happened  although more examples had non-zero bleu scores when computed this way  a large proportion were zero  and thus the bleu score did not affect the ranking function  as shown in table 1. the nist score  which is considerably more lenient  proved to be a good indicator of similarity between ads and target pages  and all of the features based on the nist score produce statistically significant results in precision at rank one.
　the best results are produced with a combination of the ad translation probability  the keyword translation proportion and the nist and nist landing features  as shown in table 1. using all features also produces statistically signficant improvements. the features resulting in significant improvements in precision are the pair of binary features that indicate the presence of all of the keywords in the target page  the features based on nist scores  the features
svmp 1p 1p 1τbad1111ad + keywords1111bleu landing1111nist1*111nist landing1*111nist + nist landing1*111table 1: features based on translation evaluation metrics. the best results are shown in bold. results with an asterisk are statistically significant with respect to the  ad  baseline.
svmp 1p 1p 1τbad1111ad + keywords1111all features1*111best trans + nist + nist landing1*111table 1: comparing the best performing feature combinations. the  best trans  features are translation probability features and keyword translation proportion features from table 1. results with an asterisk are statistically significant with respect to the  ad  baseline.
based on the ad translation probabilities  and the proportion of keywords that have translations in the target page.
1. discussion
　we have demonstrated statistically significant improvements for precision at rank one. we found that the nist features  which measure the degree of exact overlap  and the translation features  which measure overlap of related terms  in combination produce the best results. although we are using these features in a re-ranking context  we would expect the features to produce greater improvements in a retrieval context  where the vocabulary is broader because the pool of candidate ads is larger.
　the results of re-ranking are biased by the initial retrieval. if the original retrieval algorithm ranks ads based on their textual overlap with the target page  then there will be few ads that exhibit a lexical mismatch. in this case  the best features indicate a high degree of lexical overlap with the system. the fact that the nist score  which favors matching n-grams  improves results over the baseline system  supports our supposition that the ad candidates were selected precisely because they contain the same vocabulary.
　although the re-ranking of ads is less than ideal because it is biased by the initial retrieval  it is not unreasonable as a real-world approach. contextual advertising systems typically have an enormous number of ads to retrieve from  and it is not unreasonable that an efficient retrieval pass might be used to narrow the possible ad candidates  followed by a more expensive refinement in the form of a learned ranking of the most likely ad candidates.
　in this paper we report statistically signficant improvements for precision at rank one. not all of the target pages have at least five ads to be re-ranked. and for target pages that have at least five ads to be re-ranked  not all of the ads are relevant. therefore  as the system is improved by the features  we would expect to see most of the gains in precision at rank one  and lesser gains in precision at ranks three and five. the fact that the features show improvement suggests they may be more effective in a different experimental setting with a larger pool of ads.
　the τb is a less reliable indicator of the quality of the ranking. the reason for this is that the  gold standard  ranking is the result of the average of the assessors scores. it is possible that the same average score could be produced by different raw scores. for example  if an ad is ranked  relevant  by two assessors and  non relevant  by a third assessor  it will recieve the same composite score as an ad ranked  relevant  by one assessor and  somewhat relevant  by two assessors. the editorial judgments were not designed to give a ranking. we do not have access to the original raw scores  and we do not know how many assessors judged each ad. it is possible that some of the ads were judged by only one assessor  and that some were judged by two or three assessors. we are confident that the binary relevance judgment derived from the composite score is reliable because we assessed its reliability  and the results for precision are a reasonable assessment of the quality of the ad placement.
　we would not expect the result for the ranking svm trained on cosine similarity features to perform significantly better than the retrieval model using the same features. the retrieval system ranks according to a similarity score  the ranking svm learns a ranking that is a function of the same similarity score. we would expect comparable results. the advantage of the svm over the retrieval model is that the svm allows us to learn a ranking that is a function of many different forms of evidence. in this paper we demonstrate that we can improve performance using evidence derived from machine translation technologies  but any features of the similarity between ads and pages that can be represented as a real number can be used as evidence.
　we use the svm to learn separate weights for each field in the advertisement  allowing us to take advantage of the structure of both the ad and the target page. there are information retrieval models that allow retrieval using document fields  but it is unclear how to incorporate other  arbitrary  real-valued features in these models. retrieval systems - with or without fields - can only use information integral to the document. it is not clear how to incorporate other features into the system.
　the cosine similarity between the ad landing page and the target page favors pages that contain the vocabulary of the landing page  even if they don't contain the vocabulary of the ad description. if multiple ads all match the description vocabulary  ads matching the landing page vocabulary will be ranked more highly. cosine similarity finds matches at the n-gram level  where n = 1. nist favors a more precise match because it gives a higher score to longer matching n-grams.
　the translation model also associates the vocabulary of the target page and the vocabulary of the landing page. the benefit of the translation model is that it uses evidence from the complete pool of ads and target pages. words that co-occur in a given ad-target page pair will be given more weight if they co-occur in many other pairs. this allows the translation model to down-weight co-occurring words that are less meaningful. by contrast  cosine similarity is only computed over the current pair.
normally translation models require an external parallel training corpus. we constructed the translation table using no external resources. the rationale for this is that a self-contained system is more likely to be portable to other languages and other markets  making the system more flexible. if language-dependent resources are available  such as wordnet    or if there exists an external training corpus of ads and target pages known to be good matches  the translation model provides a principled way to incorporate the information.
1. conclusions and future work
　in this paper we confirm findings presented in ribeironeto et al.  that the ad landing page provides valuable evidence for the quality of a match. we demonstrate this in the task of re-ranking ad candidates. furthermore  we demonstrate that using a different framework  in our case  a ranking svm  does not harm performance. we show that using features based on the nist score of the ad title and the target page title improves results  as well as using the nist score between the ad landing page and the target landing page. we showed that a translation table can be constructed using no external resources  and demonstrated features based on this translation table that improve precision.
　although we have presented a language-independent framework for contextual advertising  the features could be extended to incorporate language-specific features to improve precision. there is no reason the proposed system could not be integrated with other types of systems  such as the ontology approach proposed by broder et al. .
　the universe of possible similarity features is enormous  and we have investigated a tiny corner of it. in the future we would like to investigate better features of similarity between ads and target pages. we demonstrated improvements for the task of re-ranking ad candidates  but because our results are biased by the initial ad retrieval algorithm  in future work we would like to evaluate our techniques by retrieving ads from a large pool of ads  rather than re-ranking candidate ads. this would also allow us to investigate treating contextual matching as a classification problem rather than a ranking problem  which is a more intuitive approach for this task. finally  we address topical relevance  but all relevant ads are not necessarily appropriate. future work in this area should include an investigation of the appropriateness of the placement of a relevant ad.
