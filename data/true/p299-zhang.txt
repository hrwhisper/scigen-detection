monitoring aggregates on ip traffic data streams is a compelling application for data stream management systems. the need for exploratory ip traffic data analysis naturally leads to posing related aggregation queries on data streams  that differ only in the choice of grouping attributes. in this paper  we address this problem of efficiently computing multiple aggregations over high speed data streams  based on a two-level lfta/hfta dsms architecture  inspired by gigascope.
　our first contribution is the insight that in such a scenario  additionally computing and maintaining fine-granularity aggregation queries  phantoms  at the lfta has the benefit of supporting shared computation. our second contribution is an investigation into the problem of identifying beneficial lftaconfigurations of phantoms and user-queries. we formulate this problem as a cost optimization problem  which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the lfta. we formally show the hardness of determining the optimal configuration  and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. our final contribution is a thorough experimental study  based on real ip traffic data  as well as synthetic data  to demonstrate the effectiveness of our techniques for identifying beneficial configurations.
1.	introduction
　the phenomenon of data streams is real. in data stream applications  data arrives very fast and the rate is so high that one may not wish to  or be able to  store all the data; yet  the need exists to query and analyze this data.
　the quintessential application seems to be the processing of ip traffic data in the network  see  e.g.   1  1  . routers forward ip packets at great speed  spending typically a few hundred nanoseconds per packet. processing the ip packet data for a variety of monitoring tasks  e.g.  keeping track of statistics  and detecting network attacks  at the speed at which packets are forwarded is an illustrative example of data stream processing. one can see the need for aggregation queries in this scenario: to provide simple statistical summaries of the traffic carried by a link  to identify normal activ-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  baltimore  maryland  usa. copyright 1 acm 1-1/1 $1.
ity vs activity under denial of service attack  etc. for example  a common ip network analysis query is  for every source ip and 1 minute interval  report the total number of packets  provided this number of packets is more than 1 . thus  monitoring aggregates on ip traffic data streams is a compelling application.
　there has been a concerted effort in recent years to build data stream management systems  dsmss   either for general purpose or for a specific streaming application. many of the dsmss are motivated by monitoring applications. example dsmss are in  1  1  1  1  1 . of these dsmss  gigascope  appears to have been tailored for processing high speed ip traffic data. this is  in large measure  due to gigascope's two layer architecture for query processing. the low level query nodes  or lftas1  perform simple operations such as selection  projection and aggregation on a high speed stream  greatly reducing the volume of the data that is fed to the high level query nodes  or hftas . the hftas can then perform more complex processing on the reduced volume  and speed  of data obtained from the lfta.
　the need for exploratory ip traffic data analysis naturally leads to posing related aggregation queries on data streams  that differ only in the choice of grouping attributes. for example   for every destination ip  destination port and 1 minute interval  report the average packet length   and  for every source ip  destination ip and 1 minute interval  report the average packet length . an extreme case is that of the data cube  i.e.  computing aggregates for every subset of a given set of grouping attributes; more realistic is the case where specified subsets of the grouping attributes  such as  source ip  source port    destination ip  destination port  and  source ip  destination ip   are of interest. in this paper  we address this problem of efficiently computing multiple aggregations over high speed data streams  based on the lfta/hfta architecture of gigascope  and make the following contributions:
our firstcontribution is the insight that when computing multiple aggregation queries that differ only in their grouping attributes  it is often beneficial to additionally compute and maintain phantoms at the lfta.
these are fine-granularity aggregation queries that  while not of interest to the user  allow for shared computation between multiple aggregation queries over a high speed data stream.
our second contribution is an investigation into the problem of identifying beneficial configurations of phantoms and user-queries in gigascope's lfta.
we formulate this problem as a cost optimization problem  which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for hash tables

1
fta stands for  filter  transform  aggregate .
in the lfta amongst a set of phantoms and user queries. we formally show the hardness of determining the optimal configuration  and propose cost greedy heuristics for both the sub-optimization problems based on detailed analyses.
our final contribution is a thorough experimental study  based on real ip traffic data  as well as synthetic data  to understand the effectiveness of our techniques for identifying beneficial configurations.
we demonstrate that the heuristics result in near optimal configurations  within 1% most of the time  for processing multiple aggregations over high speed streams. further  choosing a configuration is extremely fast  taking only a few milliseconds; this permits adaptive modification of the configuration to changes in the data stream distributions.
　the rest of the paper is organized as follows. we first motivate the problem and our solution techniques in section 1. we then give a formal definition of the problem  formulate the cost model for it  and present algorithms for choosing phantoms in section 1. section 1 presents our collision rate model at the lfta  which is a key component of the cost model. section 1 analyzes space allocation schemes. section 1 presents our experimental study. related work is summarized in section 1. we finally conclude in section 1.
1.	motivation
　in this section  we describe the problem of efficiently processing multiple aggregations over high speed data streams  based on the architecture of gigascope  and motivate our solution techniques  in an example driven fashion.
1	gigascope's two level architecture
　gigascope splits a  potentially complex  query over high speed tuple data streams into two parts   i  simple low-level queries  at the lfta  over high speed data streams  which serve to reduce data volumes  and  ii   potentially complex  high-level queries  at the hfta  over the low speed data streams seen at the hfta. lftas can be processed on a network interface card  nic   which has both processing capability and limited memory  a few mbs . hftas are typically processed in a host machine's main memory  which can be hundreds of mb to several gb .
1	processing a single aggregation
　let us first see how a single aggregation query is processed in gigascope. consider a data stream relation r  e.g.  ip packet headers   with four attributes a  b  c  d  e.g.  source ip  source port  destination ip  destination port   in
addition to a time attribute. suppose the user defines the following aggregation query:
q1: select a  tb  count *  as cnt from r group by a  time/1 as tb
　figure 1 is an abstracted model of gigascope. corresponds to the lfta  and corresponds to the hfta. q1 is processed in gigascope as follows. when a data stream record in r arrives  it is observed at . maintains a hash table consisting of a specified number of entries  and each entry is a group  count pair. group identifies the most recently observed group that hashes to this entry and count keep track of the number of times that group has been recently observed without observing other groups that hash to the same entry.
hftasmh

figure 1: single aggregation in gigascope
　when a new record hashes to entry   gigascope checks if belongs to the same group as the existing group in . if yes  the count is incremented by 1. otherwise  a collision is said to occur. in this case  first the current entry in is evicted to . then  a new group corresponding to is created in in and the count of the group corresponding to is set to 1.
　query q1 is processed by gigascope in an epoch by epoch fashion  for an epoch of length 1 minute  i.e.  time/1 . this means that at the end of every minute  all the entries in would be evicted to to compute the aggregation results for this epoch at the hfta. at the hfta  multiple tuples for the same group in the same epoch may be seen because of evictions  and these are combined to compute the desired query answer.
　consider an example stream with the following prefix: 1  1  1  1  1  1  1  1. at the lfta  suppose we use a simple hash function which is the remainder modulo 1. the first item in the stream  1  hashes to a certain entry. we check the entry in the hash table  which is empty in the beginning  and so we add the entry to the hash table. then we see 1  which hashes to a different entry of the hash table. similarly an entry is added. the third item is 1. we check the hash table and find that the existing entry where 1 hashes to contains the same group 1  so we just increment the count of the entry by 1  resulting in . the rest of the items are similarly processed one by one. after we processed the 1 item  which is 1  the status of the hash table is shown in figure 1. the next item 1 hashes to the same entry as 1. when we check the existing entry in the hash table  we find the new group is different from the existing one. in this case  we evict the existing entry to and set the entry to .
　gigascope is especially designed for processing network level packet data. usually this data exhibits a lot of clusteredness  that is  all packets in a flow have the same source/destination ip/port. therefore  the likelihood of a collision is very low until many packets have been observed. in this fashion  the data volume fed to is greatly reduced.
1	cost of processing a single aggregation
　since has much more space and a much reduced volume of data to process  processing at does not dominate the total cost. the overall bottlenecks are:
the cost of looking up the hash table in   and possible update in case of a collision. this whole operation  called a probe  has a nearly constant cost .
the cost of transferring an entry from to . this operation  called an eviction  has a nearly constant cost .
usually 	is much higher than	because the transfer from to	is more expensive than a probe in	.
hftasmh

figure 1: multiple aggregations using phantoms
　the total cost of query processing thus depends on the number of collisions incurred  which is determined by the number of groups of the data and collision rate of the hash table. the number of groups depends on the nature of the data. the collision rate depends on the hash function  size of the hash table  and the data distribution. therefore  generally  what we can do is to devise a good hash function and allocate more space  within space and peak load constraints  as we will discuss more later  to the hash table in order to minimize the total cost.
1	processing multiple aggregations naively
　given the method to process single aggregation queries  and its cost model  based on the gigascope architecture  we now examine the problem of evaluating multiple aggregation queries. suppose the user is interested in the following three aggregation queries:
q1: select a  count *  from r group by a
q1: select b  count *  from r group by b
q1: select c  count *  from r group by c
　a straightforward method is to process each query separately using the above single aggregation query processing algorithm  so we maintain  in   three hash tables for a  b  and c separately. for each incoming record  we need to probe each hash table  and if there is a collision  some entry gets evicted to .
1	processing multiple aggregations using phantoms
　since we are processing multiple aggregation queries  we may be able to share the computation that is common to each one and thereby reduce the overall processing cost. for example  we can additionally maintain a hash table for the relation abc in as shown in figure 1. if we have the counts of each group in abc  we can derive the counts of each group of a  b and c from it. the intuition is that  when a new record arrives  instead of probing three hash tables a  b and c  we only probe the hash table abc. we would delay the probes on a  b and c  we omit  hash tables  when the context is clear  until the point when an entry is evicted from abc.
　since the aggregation queries of a  b and c are derived from abc  we say that abc feeds a  b and c. although abc is not of

abbc bdcdabbc bdcdabbc bdcd a  b  c figure 1: choices of phantoms

figure 1: feeding graph for the relations
interest to the user  its maintenance could help reduce the overall cost. we call such a relation as a phantom. while for a  b and c  whose aggregate information is of user interest  we call each of them as a query. both queries and phantoms are called relations.
　now we examine figure 1 to illustrate how the instantiation of a phantom can benefit the total evaluation cost. to be fair  the total space used for the hash tables should be the same with or without the phantoms. so when we add the phantom abc  the size of the hash tables for a  b and c need to be reduced. suppose the total space allocated for the three queries is . while we have many choices of space allocation between the hash table instantiations  let us allocate equal space to each instantiation  for simplicity of exposition. without phantoms  we allocate to each hash table. with the phantom abc  we allocate to each hash table. also assume that a  b and c have the same collision rate. without the phantom  their collision rate is denoted ; with the phantom  their collision rate is denoted . since the hash table size of a  b and c is smaller in the presence of the phantom  should be larger than . let the collision rate of phantom abc be .
　consider the cost for processing records. without the phantom  we need to probe 1 hash tables for each incoming record  and there are evictions from each table. therefore the total cost is:
 1 
with the phantom  we probe only abc for each incoming record and there would be evictions. for each of these evictions  we probe a  b and c  and hence there are evictions from each of them. the total cost is:
 1 
comparing equations 1 and 1  we can get the difference of	and as follows
 1 
if	is small enough so that both	and
are larger than 1  then will be smaller than   and therefore instantiation of the phantom benefits the total cost. if is not small enough so that one of and is larger than 1 but the other is less than 1  then depends on the relationship of and . if is so large that both and are less than 1  then the instantiation of the phantom
increases the cost and therefore we should not instantiate it.
1	choice of phantoms
　in the above example  we have only considered one phantom. in fact  we can have many phantoms and multiple levels of phantoms.
again  consider stream relation r with four attributes a  b  c  d. suppose the queries are ab  bc  bd and cd. we could instantiate phantom abc  which feeds ab and bc as shown in figure 1 a   a shaded box is a phantom and a non-shaded box is a query ; or we could instantiate phantom bcd  which feeds bc  bd and cd as shown in figure 1 b ; or we could instantiate bcd and abcd  where abcd feeds ab and bcd as shown in figure 1 c . we only list three choices here  although there are many other possibilities.
　it is easy to prove that a phantom that feeds less than two relations is never beneficial. so by combining two or more queries  we can obtain all possible phantoms and plot them in a relation feeding graph as in figure 1. each node in the graph is a relation and each directed edge shows a feed relationship between two nodes  that is  the parent feeds the child. note that this feed relationship can be  short circuited   that is  a node can be directly fed by any of its ancestors in the graph. for example  ab could be fed directly by abcd without having abc or abd instantiated.
　given the relation feeding graph  and a set of user queries that are instantiated in the lfta  one optimization problem is to identify the phantoms that we should instantiate to minimize the cost. the exhaustive method is obvious  that is  we try all possible combinations of the phantoms and calculate the cost of each combination as in section 1. then we choose the one with the minimum cost. however  the exhaustive method is too expensive  especially for data stream systems where a fast response is essential.
　in our example in section 1  we assumed that each hash table has the same size for simplicity of exposition. however  given a set of phantoms and queries to instantiate in   how does the allocation of space to each hash table affect the collision rate and hence the cost  therefore  another optimization problem is that  given a set of relations to instantiate  how to allocate space to them so that the cost is minimized.
　in summary  our cost optimization problem consists of two suboptimization problems: how to choose phantoms and how to allocate space. we formulate the cost model for the multiple aggregation problem and propose a cost greedy algorithm to choose phantoms. in addition  for a given set of relations to instantiate  we analyze which space allocation gives the minimum cost; in case the optimal space allocation cannot be calculated  we propose heuristics which can approximate the optimal solution very well.
1.	problem formulation
　in this section  we formulate our cost model  and give a formal definition of our optimization problem. we present hardness results  motivating the greedy heuristic algorithms for identifying optimal configurations.
1	notation
　when we have chosen a set of phantoms to instantiate in the lfta  we call the set of instantiated relations  i.e.  the chosen phantoms and user queries  as a configuration. for example  figure 1 shows three configurations for an example query. while the feeding graph is a dag  a configuration is always a tree  consistent with the path structure of the feeding graph. if a relation in a configuration is directly fed by the stream  we call it a raw relation. for example  abc  bd  cd are raw relations in figure 1 a ; and abcd is the only raw relation in figure 1 c . if a relation in a configuration has no child  then it is called a leaf relation or just leaf. user queries are always instantiated in the lfta  therefore only queries are leaves. for all the configurations in figure 1  queries ab  bc  bd and cd are the leaves. note that raw relations and leaf relations need not be mutually exclusive. for example  bd and cd are both raw and leaf relations in figure 1 a .
　we next develop our cost model  which determines the total cost incurred during data stream processing of a configuration. we then formalize the optimization problem studied in this paper.
1	cost model
　recall that aggregation queries usually include a specification of temporal epochs of interest. for example  in the query  for every destination ip  destination port and 1 minute interval  report the average packet length   the  1 minute interval  is the epoch of interest. during stream processing within an epoch  e.g.  a specific 1 minute interval   the aggregation query hash tables need to be maintained  for each record in the stream. at the end of an epoch  all the hash tables of the user queries at the lfta need to be evicted to the hfta to complete the user query computations. thus  there are two components to the cost: intra-epoch cost  and end-of-epoch cost. we discuss each of these next.
1.1	intra-epoch cost
　let be the maintenance cost of all the hash tables during an epoch  the maintenance cost for short . it includes updating all hash tables for the raw relations when a new record in the stream is processed. if  and only if  there is collision in hash tables for the raw relations  the hash tables of the relations they feed are updated. this process recurses until the hash tables for the leaf level. each of these updates has a cost of .
　if there are collisions in the hash tables for the leaf  user  queries  evictions to the hftasare incurred  each with the cost of . therefore  the total maintenance cost is
 1 
where is a configuration  is the set of all leaves in   is the number of tuples fed to relation during epoch   and is the collision rate of the hash table for . is derived as follows.
if
　　　　　　　　　　　 1  else
where is the set of all raw relations  is the number of tuples observed in   is the number of tuples fed to the parent of in
  and	is the collision rate of the hash table for the parent of
in . if we further define and   when is a raw relation  equation 1 can be rewritten as follows.
 1 
where is the set of all ancestors of in . is determined by the data stream and is not affected by the configuration. hence  the per record cost is:
 1 
where and are constants determined by the lfta/hfta architecture of the dsms. therefore  the cost is only affected by the feeding relationship and collision rates of the hash tables.
1.1	end-of-epoch cost
　denote the update cost at the end of epoch as  the update cost for short . it includes the cost of the following operations. from the raw level to the leaf level of the feeding graph of the configuration  we scan each hash table and propagate each item in the hash table to hash tables of the lower level relations they feed. finally  we scan the leaf level hash table and evict each item in it to the hfta  . using an analysis similar to the one for intraepoch costs  taking the possibilities of collisions during this phase into account  the update cost can be expressed as follows.
 1 
where is the size of the hash table of relation   and w is the set of all raw relations.
1	our problem
　intuitively  the lower the average per-record intra-epoch cost  the lower is the load at the lfta  increasing the likelihood that records in the stream are not dropped during query processing. we also want to ensure that the total cost of the end-of-epoch processing is manageable. this leads to the multiple aggregation  ma  optimization problem studied for the two-level lfta/hfta architecture in this paper.
　consider a set of aggregation queries over a data stream that differ only in their grouping attributes      and memory limit in . determine the configuration   of relations in the feeding graph of to instantiate in  the lfta  and also the allocation of the available memory to the hash tables or the relations so that the per-record intra-epoch cost  equation 1  for answering all the queries is minimized  subject to the end-of-epoch cost being less than peak load cost .
for the ma optimization problem we can show the following:
　theorem 1. let be the number of possible group-by queries in the feeding graph of . if   for every every polynomial time approximation algorithm for the ma problem will have a performance ratio of at least .
　moreover  the same is true for the corresponding maximization problem. define the  benefit  of an aggregate in the feeding graph of   as the cost improvement with respect to the solution that computes all aggregates in the lfta. the maximization problem aims to identify the solution with the maximum benefit. it is possible to show that there does not exist a polynomial time algorithm for this problem with performance ratio better than   if
.
　our cost optimization problem consists of two sub-problems: how to choose phantoms and how to allocate space. given the hardness result above  we next describe cost greedy algorithms to choose phantoms  based on the cost model presented earlier. the cost model critically depends on the collision rate model  which we discuss in detail in section 1. for a given set of relations to instantiate  we analyze which space allocation gives the minimum cost  in section 1.
1	algorithmic strategies
　the multi-aggregation  ma  problem has similarities to the view materialization  vm  problem . they both have a feeding graph consisting of nodes some of which can feed some others  and we need to choose some of them to instantiate. so one possibility is to adapt the greedy algorithm developed for vm to ma. however  there are two differences between these two problems. first  instantiation of any of the views in vm will add to the benefit; while in ma  instantiation of a phantom is not always beneficial. second  the space needed for instantiation of a view is fixed but the hash table size is flexible. therefore  in order to adapt the vm greedy algorithm  we need to have a space allocation scheme so that the hash tables must have low collision rate and therefore each instantiated phantom is beneficial. we discuss this next.
1.1	greedy by increasing space
　the more space that is allocated to a hash table  the lower is the collision rate. on the other hand  the more groups a relation has for a fixed sized hash table  the higher is the collision rate. let be the number of groups in a relation. one way of allocating hash table space to a relation is proportional to the number of groups in the table. thus  we can allocate space for a relation with groups. is a constant and we set it large so that the hash table is guaranteed to have a low collision rate. we will develop a model to estimate collision rate in section 1. we can then have a better sense of what value of might be good according to the analysis there.
　the greedy algorithm goes as follows. we calculate the benefit of each phantom according to the cost model  i.e.  the difference between the maintenance costs without and with this phantom. let us denote this benefit by . then we calculate the benefit per unit space for each phantom   . we choose the phantom with the largest benefit per unit space as the first phantom to instantiate. for the other phantoms  this process is iterated. the process ends when the benefit per unit space becomes negative  the space is exhausted  or all phantoms are instantiated.
　this approach has two drawbacks:  1  needs to be tuned to find the best performance. a bad choice can result in suboptimal performance.  1  by allocating space to a relation proportional to the number of its groups  we make the collision rates of all the relations the same. as we shall show later  this is not a good strategy.
1.1	greedy by increasing collision rates
　here  we present a different greedy algorithm for allocating space to hash tables of the relations in the lfta. instead of allocating a fixed amount of space to each phantom progressively  we always allocate all available space to the current configuration  how to allocate space among relations in a configuration is analyzed in section 1 . so as each new phantom is added to a configuration  what changes is not the total space used  but the collision rate of each table. since the more the number of groups mapped to a fixed space  the higher the collision rate  the collision rate would increase as new phantoms are instantiated.
　the greedy algorithm is as follows. at first  the configuration only includes all the queries. we calculate the maintenance cost if a phantom is added to . by comparing with the maintenance cost when is not in   we can get the benefit. after we add this phantom to   we iterate with the other phantoms.
　as more phantoms are added into   the overall collision rate goes up and benefit decreases. we stop when the benefit becomes negative. this algorithm depends on estimating the collision rates. we derive a model to estimate the collision rate  in section 1.
1.	the collision rate model
　in this section  we develop a model to estimate the collision rate. we assume that the hash function randomly hashes the data  so each hash value is equally possible for every record. we first consider uniformly distributed data  and subsequently consider when the data exhibits clusteredness.
1	randomly distributed data
　let be the number of groups of a relation and the number of buckets in the hash table. if groups hash to a bucket  we say that this bucket has groups. let be the number of buckets having groups. if the records in the stream are uniformly distributed  each group has the same expected number of records  denoted by . so records will go to a bucket having groups. under the random hash assumption  the collision rate in this bucket is . therefore collisions happen in this bucket.
the overall collision rate is obtained by summing all the collisions and then dividing by the total number of records. therefore  we have collision rate
		 1 
begins from 1 because when 1 or 1 group hashes to a bucket  no
collision happens. in order to calculate it  we still need to know . this problem belongs to a class of problems called the occupancy problem.
　as we know  the expectation of	for each bucket is	. a rough estimation of	based on expectation would be
k=g/b k	g/b
substituting this for	in equation 1  we get
 1 
　however  in a real random process  the probability of each bucket having the same number of groups is small. in   chapter ii.1   an example when is given to calculate the probability of different distributions of groups. it is shown that probability of each of the 1 buckets having exactly 1 group is 1  which makes it extremely unlikely. therefore  we need to calculate based on probability. to the best of our knowledge  no study exists on estimating as we defined here.1 our derivation of is as follows.
	the probability of	groups out of	hashed to a given bucket is
 1 
note this holds for any bucket  which means each bucket has the chance of equation 1 to have groups. if we assume that all buckets are independent of each other  then statistically there are
 1 
buckets each of which has groups. substitute equation 1 for in equation 1 we have
		 1 
　our experiments on both synthetic and real data show that the actual distribution of matches equation 1 well  even though the buckets are not completely independent  they satisfy the equation
 .
1	validation of collision rate model
　we have measured experimentally the collision rates on both synthetic random datasets and real datasets. the results for the real datasets are shown in figure 1; the results for the synthetic datasets are very similar and omitted.

1
 if we use to denote the number of balls in the -th bucket  are called occupancy numbers. this problem has been studied before and the 's follow the multinomial distribution   chapter vi.1 . however  our definition of is different from . instead of the probability of a certain arrangement of the  balls  in the buckets  what we want is the distribution of the  balls .

figure 1: collision rates of real data
　the real datasets are extracted from the netflow dataset as described in section 1. we have assumed random data distribution for the above analysis  while the netflow dataset has a lot clusteredness of multiple packets in a flow. in order to validate our analysis using the real data  we grouped all packets of a flow into a single record  eliminating the effect of clusteredness.  we consider clusterness in a later subsection.  after eliminating clusteredness of the data  we extracted 1 datasets which have 1  1  1 and 1 attributes respectively. the number of groups in these datasets are 1  1  1  1 respectively.
　the  rough model  curve is plotted according to equation 1 and the  precise model  curve is plotted according to equation 1. collision rates of the real data match the precise model very well. in all the observed collision rates  more than 1% of the experimental results have less than 1% difference from the precise model. the rough model differs greatly from the precise model when is small but becomes similar as gets large. the reason is that the rough model only captures the expected case  which occurs with low probability. when becomes larger  the behavior gets closer to the average case  therefore the rough model gets close to the precise model.
　for the rough model  the collision rate is only dependent on the ratio of to as we can see from equation 1. we will show in section 1 that the precise model is also dependent on   though the function is different.
1	clustered data
　the above analysis was for randomly distributed data. however  real data streams  especially the packets in netflow data  which have exactly the same values for attributes such as source/destination ip/port   are clustered. although packets from different flows are interleaved with each other in the stream  the likelihood of these interleaved flows hashing to the same bucket is very small. therefore we can think of the packets in a flow going through a bucket without any collision until the end of the flow. to analyze collision rate for such clustered distributions  we should consider what happens at the per flow level. if we think of each flow as one record  then we can use the same formula as in the random distribution  equation 1  to calculate the total number of collisions as follows.
 1 
where is the number of flows in each group; is still calculated by equation 1. to obtain the collision rate  we divide by the total number of records    where is the average length of all the flows. then we have the collision rate for the data with a clustered distribution as follows.

figure 1: probability of collision vs. k
		 1 
　we can see that the difference of the collision rate on data with clusteredness from that of the random data is a linear relationship over average flow length . we can view the collision rate of the random data as a special case of clustered data with . the average flow length can be computed by maintaining the number of times hash table bucket entries are updated before being evicted.
1	on computing collision rates
　to calculate the collision according to equations 1 and 1  we need to compute a sum of about items  which can be hundreds to thousands of items. this computation is expensive. in this section  we show that actually we only need to sum up to a much smaller number of items. further  the collision rate is almost solely dependent on   therefore we can pre-compute the collision rates and store them as a function of . in this way  the computational effort of collision rates is greatly reduced.
　we computed the probability of collision of different 's according to equation 1 to see how much each component contributes to the overall collision rate. figure 1 shows the probability of collision as a function of when and . we can see that the contributions become almost 1 when becomes larger than 1 and the shape of the curve looks like a bell. it looks like the pdf of the gaussian distribution. examining equation 1  we find that except the part and which is a constant  the rest are the same as the pdf of the binomial distribution. and the binomial distribution can be approximated by the gaussian distribution. so the plots of the collision probability of different components can be viewed as a gaussian distribution with an amplitude of . that's why the plot mimics the gaussian distribution. given its similarity  we can understand many features of the plots according to characteristics of the gaussian distribution. the peak of the plot appears at approximately the mean  which is  the actual maximum in figure 1 appears at due to the effect of the amplitude  . also we know that the probability in the interval is 1%. so when we calculate the collision rate  we do not need to sum over all values of but up to is enough  where . in case of figure 1 

. in figure 1  the component at
　　　is as small as 1 already. it is not 1 yet due to the amplitude   so we can calculate up to several more say   which is 1 in our case. the components after 1 are almost 1  and 1 is much smaller than 1  the number of groups .
　the cost of computing collision rates can be further reduced. a gaussian distribution is determined by and . here we just want the sum  so we don't care about  the mean  but only   which equals . in the data stream case is usually several
11111variation % 1111111table 1: variations of the collision rate

figure 1: the collision rate curve
hundred to thousands  therefore is almost 1 and the sum is essentially determined by .
　as our sum is not exactly a gaussian distribution  some errors are expected. in the following  we experimentally evaluate how much the errors are. we let be certain constants and compute the collision rates according to equation 1  for varied from 1 to 1. note that once is chosen  is also determined for a given . the resulting collision rate  for varied from 1 to 1  is almost constant for the same . the maximum relative variations of collision rates as varies are listed in table 1. we observe that all the variations are less than 1%. therefore the collision rate only depends on and we can pre-compute collision rate and use regression to model the function.
　the collision rate curve as a function of is plotted in figure 1. we divided the whole curve into 1 intervals and used twodimensional regression to simulate the curve so as to achieve a maximum relative error of 1% in each interval. the average relative error is actually much lower  which is less than 1%.
　according to our previous analysis  the hash table must have a low collision rate if we want to benefit from maintaining phantoms. therefore  we examine the low collision rate part of this curve closely. a zoom in of the collision rate curve when collision rate is smaller than 1 as well as a linear regression of this part is shown in figure 1. we observe that this part of the curve is almost a straight line and the linear regression achieves an average error of
1%. the linear function for this part is
 1 
　expressing this part of the collision rate linearly is important for the space allocation analysis as we will see in section 1. in addition  since we now know how the collision rate is determined  we can suggest values of to use in the adapted greedy algorithm  by increasing space  of section 1. for example  could be a good choice  since it corresponds to a collision rate of about 1.

figure 1: the low collision rate part
1.	space allocation
　in this section  we consider the problem of space allocation  that is  given a configuration of certain relations  phantoms and queries  to be instantiated  how to allocate the available space to their hash tables so that the overall cost is minimized. we start with a simple two-level configuration in section 1  and identify the difficulties in analyzing more complex configurations. heuristics for space allocation are discussed in section 1.
1	a case of two levels
　we first study the case when there is only one phantom and it feeds all queries      ...  . let be the collision rate of the phantom      ...  be the collision rate of the queries. in order to benefit from maintaining a phantom  its collision rate must be low  therefore we only care about the low collision rate part of the collision rate curve. according to section 1  this part
of the curve can be expressed as a linear function   where =1 and =1.1 since is small  here we make a further approximation to let . we will discuss later how the results are affected when we consider . given the approximation    . the total size is   so
. the cost of this configuration is

 1 


　is a function of multiple variables  . to find out the minimum  we equate the partial derivatives of to 1. in the following  we calculate the partial derivative of over   .

	let 	  then


 is non-zero  so

		 1 

1
 actually  even if the collision rate for the optimal allocation is a little higher than 1  we can still use linear regression for that part. the values of and would be a little different  but experiments show that small variation in their values does not affect the result much.
for	.
　observe that left hand side of the equation is the same for any . so we have

that is 	is proportional to	 .

	let	 	. substituting this for	in equa-
tion 1  we have
		 1 
	this is a quadratic equation over	. solving it we have



         so only the one with     before the square root on the numerator is the solution. so






 1 
	where	.

 1 


　a key consequence of our analysis is that we should allocate space proportional to the square root of the number of groups in
order to achieve the minimum cost. another interesting point is that  the space allocated to the hash table of the phantom  always takes more than half the available space.
　while the 1-level case results in a quadratic equation  that is  equation 1   a similar analysis on the simplest 1-level case results in an equation of order 1. according to abel's impossibility theorem  equations of order higher than 1 cannot be solved algebraically in terms of a finite number of additions  subtractions  multiplications  divisions  and root extractions  in the sequel  we simply say  unsolvable  . more general multi-level configurations generate equations of even higher order which are unsolvable  therefore we would use heuristics to decide space allocation for the these unsolvable cases based on the analysis available. experiments show that our proposed heuristics based on the analysis are very close to optimal and better than other heuristics.
1	heuristics
　for unsolvable configurations  we propose heuristics to allocate space based on the analysis of the solvable cases and partial results we can get from the unsolvable cases. we observe that while   is a leaf relation  is proportional to   which is affected by its own number of groups  according to our analysis on the three level case    is a non-leaf relation  is proportional to
　　　　　　　 note that    where are the collision rates of tables for the children of . is affected not only by its own number of groups  but also its children's. the intuition is that we should care more about a relation when it has children in the feeding graph of the configuration. therefore we will consider the following space allocation schemes which add some weights to a relation when it has children to feed.
　heuristic 1: supernode with linear combination  sl . we start from the leaf level of the configuration. each phantom at the record level together with all its children are viewed as a supernode. the number of groups of this supernode is the sum of the number of groups of the phantom and all its children. then we view the supernode as a query and do the above compaction recursively until the configuration become all queries. for the all query configuration  we can allocate space optimally. after this allocation  each query  some may be supernodes  has some space. we decompose a supernode to a two-level structure and allocate space according to the analysis of section 1  that is  allocate space proportional to the square root of the number of groups. if there are still supernodes in the structure  we do the decomposition recursively.
　heuristic 1: supernodewith square root combination  sr . this heuristic is the same as sl except the calculation of the group of the supernode. since in the two level case we see that the space should be proportional to the square root of the number of groups  we can also let the square root of the number of groups of the supernode be the sum of the square roots of all its relations.
　note that both sl and sr give the optimal result for the case of one phantom feeding all queries. we will also try two other simple heuristics which are not based on our analysis as a comparison to the above two more well-founded heuristics.
　heuristic 1: linear proportional allocation  pl .this heuristic simply allocates space to each relation proportional to the number of groups of that relation.
　heuristic 1: square root proportional allocation  pr . this heuristic allocates space to each relation proportionally to the square root of the number of groups of that relation.
　although we cannot compute the optimal solution for space allocation of some cases  there does exist a space allocation which gives the minimum cost for each configuration. one way to find this optimal space allocation is to try all possibilities of allocation of space at certain granularity. for example  if the configuration is ab feeds a and b  and total space is 1  we can first allocate 1 to ab  1 to a  and 1 to b. then we try 1 to ab  1 to a  and 1 to b  and so on. by comparing the cost of all these space allocation choices we will find the optimal one. we call this method the exhaustive space allocation  es . obviously this strategy is too expensive to be practical  but we use it in our experiments to compare with the four space allocation schemes and see how much the heuristics differ from the optimal choice. the results of es are affected by the granularity of varying the space allocation. in our experiments  we found that using a granularity of 1% of is small enough to provide accurate results.
　the space allocation schemes are independent of the phantom choosing strategies  that is  given a configuration  a space allocation scheme will produce a space allocation no matter in what order the relations in the configuration are chosen. therefore we will evaluate space allocation schemes and phantom choosing strategies independently.
1	revisiting simplifications
　from the beginning of the analysis on space allocation  we have made an approximation on the linear expression of the collision rate  that is  we let equal instead of . we also did the analysis when we let . the result of the case with no phantom is the same. the case with one phantom feeding all queries results in a quartic equation which can be solved  so we can still get an optimal solution for this case. however  because solving a quartic equation is much more complex than a quadratic equation and it's more involved to decide which solution of the quartic equation is the one we want  we use the approximated linear expression  that is  for space allocation in our experiments. the results of the experiments show that they have good accuracy.
　we have made another simplification on the size of each hash table bucket entry in the analysis for ease of exposition. by using
           we have assumed that a hash table entry has the same size for all relations in the lfta. actually  the size of a hash table entry for different relations can vary a lot. suppose we use an int  1 byte  to represent each attribute or a counter. then a bucket for relation a takes 1 bytes and a bucket for abcd takes 1 bytes. if we denote the bucket entry size of relation as   then
　　　. in this case  the results of the analysis are similar. instead of allocating space proportional to   we should allocate space proportional to. we have used such variable sized buckets in our implementation  and experimental study  discussed next.
　for clustered data  collision rates should be divided by the average flow length . to consider this in space allocation  we should allocate space proportional to   where is the average flow length of relation .
1.	experimental study
1	experimental setup and datasets
　we prototyped this framework in c in order to evaluate the different techniques we developed. we use 1 bytes as our unit of space allocation. each attribute value and counter we instantiate has this size. in accordance to operational streaming data managers   we consider between 1 and 1 units of space  1 bytes each . the ratio of eviction cost to probe cost is is modeled as 1 in our experiments  which is also a ratio measured in operational data stream management systems .
　we used both synthetic and real datasets in our evaluation. the real dataset is obtained by tcpdump on a network server. we extracted tcpheaders obtaining 1 records with attributes source ip  destination ip  source port and destination port  each of size 1 bytes. the duration of all these packets is 1 seconds. there are 1 groups in this 1-attribute relation. for other relations we extracted in this way  the number of groups varies from 1 to 1. for the synthetic datasets  we generated 1 1 and 1 dimensional tuples uniformly at random with the same number of groups as those encountered in real data. all the experiments are run on a desktop with pentium1.1ghz cpu and 1gb ram.
　we adopt the following way to specify a configuration.  ab a b   is used to denote a phantom ab feeding a and b. we use this notation recursively. for example  the configuration in figure 1 c  can be expressed as  abcd ab bcd bc bd cd   .

figure 1: comparison of space allocation schemes
1	evaluation of space allocation strategies
　our first experiment aims to evaluate the performance of various space allocation strategies. in these experiments we derive our parameters from the real data set. our observations were consistent across a large range of real and synthetic datasets. we vary from 1 to 1 at steps of 1 and the granularity for increasing space while executing es is set at 1% of . in all experiments we compute the cost using equation 1 with a suitable model for collision rate  as described below.
1.1	solvable configurations
　we first experimentally validate the results of our analysis for the case of configurations for which we can analytically reason about the goodness of space allocation strategies.
　for the case with no phantoms   assuming as collision rate  we compared the cost of the exhaustive space allocation  es  with a scheme that allocates space according to our analytical expectations  namely  allocating space proportional to the square root of number of groups. for the case of real data  we tested all possible configurations with no phantoms. the cost obtained by the scheme performing space allocation as dictated by our analytical derivations incurred an error less than 1% compared to es. the small error comes from our approximation to the collision rate  especially the value of   which can be different from the value the optimal solution assumes.
　for the case with only one phantom feeding all queries  we use our optimal space allocation scheme derived based on the approximation of collision rate by . we again compare the accuracy of the space allocation scheme allocating space according to our analysis  to that of es and test all possible configurations for the case of the real data set. the average cost error  compared to es  of our scheme is usually less than 1% and the maximum observed was 1%. therefore even with this approximation     to the collision rate  the results are still quite accurate.
1.1	unsolvable configurations
　for unsolvable configurations  we evaluated several heuristics. we compared sl  sr  pl  pr as described in section 1 and es. we evaluated all possible configurations for the case of the real data set  four attributes . the relative errors of the heuristics compared to the cost of es are shown in figures 1 and 1 for 1 representative configurations. related results were obtained for other configurations; all those are summarized in table 1.
we observe that generally sl and sr are better than pl and pr.
thus  heuristics inspired by our analytical results appear beneficial.
except one case in figure 1 a  when   sl is always the best. pl and pr can have errors as large as 1% and although sr has smaller error  it is always less accurate than sl. in table 1  we show the average relative error of the four different heuristics compared to es. sl is the best for all values of .
in table 1  we accumulate statistics in order to show in all con-

 a   abcd abc a bc b c   d    b   abcd ab bcd bc bd cd   
figure 1: comparison of space allocation schemes
 thousand 111sl  % 11111sr  % 11111pl  % 11111pr  % 11111table 1: average error for the four heuristics
figurations tested how frequently sl is the heuristic yielding the minimum error. we preset the percentage of configurations tested in which sl yields minimum error  as well as for the cases that sl does not yield the minimum error  how far its error is from the error of the best heuristic  on the average.
　these results  which are representative of a large set of experiments conducted  attest that sl behaves very well across a wide range of configurations. even in the cases that it's not the best it remains highly competitive to the best solution. therefore we would choose sl for space allocation in our algorithms.
1	evaluation of the greedy algorithms
　we now turn to the evaluation of algorithms to determine beneficial configurations of phantoms. we will evaluate the greedy algorithm gs and our proposed greedy algorithm gc. gc makes use of the sl space allocation strategy; we refer to this combination as gcsl  algorithm gc using sl space allocation . for gs  we would add space of each time a phantom is added in the current configuration under consideration until there is not enough space for any additional phantom to be considered. at this point we allocate the remaining space to relations already in the configuration proportional to their number of groups. we also consider the following method to obtain the optimal configuration cost. we explore all possible combinations of phantoms and for each configuration we use exhaustive space  es  allocation to calculate the cost  choosing the configuration with the minimum overall cost. we will refer to this method as epes in the sequel. costs are computed using equation 1 and our approximation to the collision rate.
1.1	phantom choosing process
　we first look at the query set a  b  c  d on a 1-dimensional uniform random dataset with set as 1. since a good value of is not known a priori  we vary it and observe the trends. figure 1 presents the cost of the different algorithms. the costs are normalized by the cost of epes  the optimal cost . the cost of gs first decreases and then increases  as increases. if is too small  each phantom is allocated a small amount of space  at the expense of high collision rate. on the other hand  if is too large  each
 thousand 111sl being best  % 111relative error from the best  % 11111table 1: statistics on sl

figure 1: comparison of phantom choosing algorithms

figure 1: phantom choosing process
phantom has low collision rate  but each phantom takes too much space and prohibits addition of further phantoms  which could be beneficial. this alludes to a knee in the cost curve signifying the existence of an optimum value. for the gcsl algorithm  cost is lower than the cost of gs for any   because when we adjust the space allocation and calculate the cost each time a phantom is added  we are essentially adapting to a better value. the gap between the minimum point of the gs curve and gcsl is due to the space allocation scheme. using gc in conjunction with pl space allocation  yields a curve which precisely lower bounds gs. thus  gcsl benefits from both the way we choose phantoms and the way space is allocated in these phantoms.
　figure 1 presents the change in the overall cost in the above scenario as each phantom is chosen. we observe that the first phantom introduces the largest decrease in cost. the benefit decreases as more phantoms are added and for gs with   the cost goes up when adding the third phantom. note that the third phantom added by gs with is different from the third phantom added by gcsl due to the differences in space allocation. for gs with there is no space to add more than one phantom. 1.1 validating cost estimation framework
　with our next experiment we wish to validate our cost estimation framework against the real measured errors. we implemented the hash tables and we let a uniform random dataset pass through the phantoms and queries computing the desired aggregates. the phantoms are chosen and the corresponding space allocation is conducted  using our heuristics. we count the collisions in the hash tables and calculate the true cost of this configuration. we normalize the actual cost of gcsl and gs by the actual cost of the optimal  according to our cost model  configuration obtained by epes; the relative actual costs are shown in figure 1 a . for gs  we tried different values  and only the one with the lowest cost at each value of m is presented in the figure.
　we can see that the actual cost of gcsl is always much lower than that of gs  even we could always choose the best for gs  which is impossible in practice . when =1  the cost of gcsl is as low as 1% of the cost of gs. while gs can have cost as high as 1 times the optimal cost  gcsl is always within 1 times the optimal cost.

figure 1: comparison on synthetic dataset

figure 1: comparison on real dataset
　we conducted a large set of experiments quantifying the accuracy of our estimation framework against actual measurements. in general  difference between the predictions of the cost model and the actual cost becomes large as increases. the relative cost difference of gcsl compared to the optimal cost also increases as increases. this is due to two factors: first when is very large then collision rates are very small and become increasingly difficult to capture analytically. second  for large there are many phantom levels and as a result errors accumulate across multiple levels. however  despite certain inaccuracy  our technique results in a reasonable low cost compared to the optimal cost and outperforms gs considerably  for a variety of data sets  especially for low values of  which is the common case in practice .
　in order to validate the effectiveness of phantoms for computing multiple aggregates  we conducted the following experiment. we run the same queries without maintaining any phantoms and we compare the cost with the cost of gcsl. the results are presented in figure 1 b . it is evident that maintaining phantoms does reduce the cost greatly  more than an order of magnitude .
1.1	experiments with real data
　we repeated our validation experiment using real data this time and the query set ab  bc  bd  cd . again we let the real data set stream by the configuration we have obtained using our algorithms and report the resulting actual costs incurred. once again actual costs are normalized by the actual cost incurred by the epes strategy. flow length is derived temporally.
　figure 1 a  presents the results. it is evident that gcsl outperforms gs. once again we compare the cost of gcsl and the cost incurred without the maintenance of any phantoms. gcsl offers an improvement up to about 1 compared to the cost incurred without the use of phantoms.
1.1	peak load constraint
　the update cost at the end of epoch as described in section 1.1 can be calculated according to equation 1. this update cost must be within the peak load constraint . if the update cost exceeds   we can use two methods to resolve it: shrink and shift. the shrink method shrinks the space of all hash tables proportionally. the shift method shifts some space from queries to phantoms

figure 1: peak load constraint
since is much larger than and a major part of the update cost is incurred by queries. for the real dataset and the query set ab  bc  bd  cd   given a space allocation  we calculate its ; then we set to a percentage of and use the two methods to reallocate space. after the reallocation  we run the data through the configuration and we compute the cost when . the results are in figure 1. when is not much smaller than   the shift method performs better; while when is much larger than   the shrink method performs better. the reason is that when is close to   a small shift to reduce suffices. when
and differ by much  a major shift in space results in non optimal space allocation and thus shrink is better. similar behavior is observed when m is set as other values.
　in terms of the performance of our algorithms  the running time of gcsl in all configurations we tried was sub-millisecond; we don't expand further due to space limitations.
1.	related work
　our problem is closely related to the problem of multi-query optimization  i.e.  optimizing multiple queries for concurrent evaluation. this problem has been around for a long time  and several techniques for this problem have been proposed in the context of a conventional dbms  see  e.g.   . the basic idea of all these techniques is the identification of common query sub-expressions  whose evaluation can be shared among the query execution plans produced. this is also the basis for sharing of filters in pub-sub systems  see  e.g.   . our technique of sharing computation common to multiple aggregation queries is based on the same idea.
　our problem also has similarities to the view maintenance problem  which has been studied extensively in the literature  see  e.g.   . in this context  ross et al.  have studied the problem of identifying  in a cost-based manner  what additional views to materialize  in order to reduce the total cost of view maintenance. our idea of additionally maintaining phantoms  and choosing which phantoms to maintain  to efficiently process multiple aggregations is based on the same conceptual idea.
　many papers  see  e.g.   1  1  1   have highlighted the importance of resource sharing in continuous queries.  1  1  use variants of predicate indexes for resource sharing in filters in continuous query processing systems. in the context of query processing over data streams  dobra et al.  consider the problem of sharing sketches for approximate join-based processing.  1  1  consider the problem of resource sharing when processing large numbers of sliding window aggregates over data streams. however  none of these papers proposed the maintenance of additional queries to improve the feasibility of resource sharing.
1.	conclusions
　monitoring aggregates on ip traffic data streams is a compelling application for data stream management systems. evaluating multiple aggregations in a two level dsms architecture is an important practical problem. we introduced the notion of phantoms  finegranularity aggregation queries  that has the benefit of supporting shared computation. we formulated the ma optimization problem  analyzed its components and proposed greedy heuristics which we subsequently evaluated using real and synthetic data sets to demonstrate the effectiveness of our techniques.
　we are currently considering deploying this framework in a real dsms system. this raises important research questions at the system level  in terms of interaction of such algorithms with the current system  studying issues related to adaptivity and frequency of execution  etc. we hope to report such results in the near future.
