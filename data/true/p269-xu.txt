this work evaluates a few search strategies for arabic monolingual and cross-lingual retrieval  using the trec arabic corpus as the test-bed. the release by nist in 1 of an arabic corpus of nearly 1k documents with both monolingual and cross-lingual queries and relevance judgments has been a new enabler for empirical studies. experimental results show that spelling normalization and stemming can significantly improve arabic monolingual retrieval. character tri-grams from stems improved retrieval modestly on the test corpus  but the improvement is not statistically significant. to further improve retrieval  we propose a novel thesaurus-based technique. different from existing approaches to thesaurus-based retrieval  ours formulates word synonyms as probabilistic term translations that can be automatically derived from a parallel corpus. retrieval results show that the thesaurus can significantly improve arabic monolingual retrieval. for cross-lingual retrieval  clir   we found that spelling normalization and stemming have little impact.  
categories and subject descriptors 
h.1  information storage and retrieval : content analysis and indexing--dictionaries  indexing methods  linguistic processing  thesauruses; h.1  information storage and retrieval : information search and retrieval--relevance 
feedback  retrieval models 
general terms 
algorithms  measurement  performance  experimentation  
keywords 
arabic retrieval  stemming  spelling normalization  thesaurus  ngrams  broken plurals  parallel corpora  cross-lingual retrieval  
1 	introduction 
arabic is one of the most widely used languages in the world  yet there are relatively few studies on the retrieval of arabic documents in the literature. furthermore  the lack of a realistically large test corpus has been a problem in past studies on arabic retrieval. this work will explore a few strategies for the retrieval of arabic documents  using the recently available trec arabic corpus  voorhees  1  for evaluation.  
arabic is a challenging language for information retrieval  ir  for a number of reasons. first  orthographic variations are prevalent in arabic; certain combinations of characters can be written in different ways. for example  sometimes in glyphs combining hamza or madda with alef the hamza or madda is dropped  rendering it ambiguous as to whether the hamza or madda is present. second  arabic has a very complex morphology. third  broken plurals are common. broken plurals are somewhat like irregular english plurals except that they often do not resemble the singular form as closely as irregular plurals resemble the singular in english. because broken plurals do not obey normal morphological rules  they are not handled by existing stemmers. fourth  arabic words are often ambiguous due to the tri-literal root system. in arabic  a word is usually derived from a root  which usually contains three letters. in some derivations one or more of the root letters may be dropped  rendering many arabic words highly ambiguous with one another. fifth  short vowels are omitted in written arabic. sixth  synonyms are widespread  perhaps because variety in expression is appreciated as part of a good writing style by arabic speakers  noamany  1 . 
those problems make exact keyword match inadequate for arabic retrieval. we will explore a few search strategies to address the problems. two techniques  spelling normalization and stemming  are well-known techniques for ir. our experiments show that while these techniques can significantly improve retrieval  they are not adequate. the third technique  retrieval based on character n-grams  has been used by a few studies  darwish et al  1; mayfield et al  1; kwok et al  1 . we found that tri-grams from stems modestly improved retrieval on the test corpus  but the improvement is not statistically significant. to further improve arabic retrieval  we propose a statistical thesaurus to deal with the large number of broken plurals and synonyms in arabic. our approach differs from existing techniques in that it is probabilistically motivated and employs a parallel corpus rather than a monolingual corpus for determining word associations. experiments show that the thesaurus can significantly improve monolingual retrieval.  

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir '1  august 1  1  tampere  finland. 
copyright 1 acm 1-1/1...$1. 
 
we also studied the effect of spelling normalization and arabic stemming on cross-lingual retrieval  where english queries were used to retrieve arabic documents. interestingly  they had little impact on our clir experiments  as we had sufficient data to learn translations of each of the variants. 
1 	retrieval strategies 
1 spelling normalization 
arabic orthography is highly variable. for instance  changing the letter yeh     to alef maksura     at the end of a word is very common.  not surprisingly  the shapes of the two letters are very similar.  since variations of this kind usually result in an  invalid  word  in our experiments we detected such  errors  using a stemmer  the buckwalter stemmer  to be discussed later  and restored the correct word ending. 
 a more problematic type of spelling variation is that certain glyphs combining hamza or madda with alef  e.g.      and    are sometimes written as a plain alef      possibly because of their similarity in appearance. often  both the intended word and what is actually written are valid words. this is much like confusing  r¨¦sum¨¦  with  resume  in english. since both the intended word and the written form are correct words  it is impossible to correct the spellings without the use of context. in our experiments  we converted every occurrence of these glyphs to a plain alef.  
1 arabic stemming 
arabic has a complex morphology. most arabic words  except some proper nouns and words borrowed from other languages  are derived from a root. a root usually consists of three letters. we can view a word as derived by first applying a pattern to a root to generate a stem and then attaching prefixes and suffixes to the stem to generate the word  khoja and garside  1 . for this reason  an arabic stemmer can be either root-based or stem-based.  
experiments in this work used a stem-based stemmer  the buckwalter stemmer  buckwalter  1   for two reasons. first  it is a simple algorithm and can be easily re-implemented in a way usable in our retrieval system. second  judging from the published results in the trec 1 proceedings  voorhees  1   it is at par with other stemmers for retrieval purposes. the algorithm is table-driven  employing a number of tables that define all valid prefixes  stems  suffixes  and their valid combinations. given an arabic word w  the stemmer tries every segmentation of w into three sub-strings  w=x+y+z. if x is a valid prefix  y a valid stem and z a valid suffix  and if the combination is valid  then y is considered a stem. if several valid combinations are found  it returns all of the stems. we re-implemented the stemmer to make it faster and compatible with the utf1 encoding. we also modified it so that if no valid combination of prefix-stem-suffix is found  the word itself is returned as the stem. 
1 character n-grams 
broken plurals  analogous to irregular nouns in english  e.g.  woman/women    are very common in arabic. there is no existing rule-based algorithm to reduce them to their singular forms  and it seems that it would be not be straight-forward to create such an algorithm. as such  broken plurals are not handled by current arabic stemmers.  
one technique to address this problem is to use character n-grams. although broken plurals are not derived by attaching word affixes  many of the letters in broken plurals are the same as in the singular forms  though sometimes in a different order . if words are divided into character n-grams  some of the n-grams from the singular and plural forms will probably match. this technique can also handle words that have a stem but cannot be stemmed by a stemmer for various reasons. for example  the buckwalter stemmer employs a list of valid stems to ensure the validity of the resulting stems. although the list is quite large  it is still not complete. n-grams in this case provide a fallback where exact word match fails.  
in this work  we have experimented with n-grams created from stems as well as n-grams from words. n-grams were created by applying a shifting window of n characters over a word or stem. if the word or stem has fewer than n characters  the whole word or stem was returned. 
1 deriving an arabic thesaurus from a parallel corpus 
in all natural languages  synonyms present a challenge to ir. as discussed before  this problem is especially serious for arabic. a common technique to address synonyms is to use a thesaurus. here we discuss how to automatically derive an arabic thesaurus from a parallel corpus  based on the intuition that synonyms in one language tend to be translated to the same words in the other language.  
we treat synonyms as probabilistic translations between words. our approach therefore attempts to estimate p b|a   the translation probability from one arabic word a to another arabic word b. we can imagine the user first translates a to some english word x and then translates x to b. theoretically any english word could be the intermediate translation x. therefore  p b|a  can be expressed as: pthesaurus  b | a  = ¡Æ p x | a p b | x   
english words x
to overcome the problem of data sparseness  the probability was smoothed using a mixture model:  
 
p b | a  =¦Âpdiag  b | a  +  1 ¦Â  pthesaurus  b | a  
 
where pdiag b|a =1 if a=b and 1 otherwise. the smoothing parameter ¦Â controls how much confidence we have in the original word and how much confidence we have in the thesaurus.  in our experiments  the probability estimates p x|a  and p b|x  were estimated from a parallel corpus using giza++   och and ney  1 . giza++ is a freely available statistical machine translation toolkit whose theory was based on the statistical translation work pioneered by  brown et al  1 . giza++ implemented several models proposed by brown for estimating term translation probabilities from sentence aligned parallel corpora; model 1 was used in this work for its efficiency. 
the parallel corpus used in our experiments was obtained from the united nations  un . the un website 
 http://www.ods.un.org  publishes all un official documents under a document repository  which is accessible by paying a monthly fee. we extracted around 1 document pairs from the un archive  with over 1 million english words and a similar number of arabic words. an algorithm developed in-house was used to align the corpus at the sentence level. 
while thesaurus-based retrieval has been extensively studied over the decades  spark jones  1; deerwester et al  1; jing and croft  1; sch¨¹tze and pedersen  1   our approach is different from existing ones in two ways. our approach extracts word associations from a parallel corpus  while existing techniques employed a monolingual corpus. we believe that a parallel corpus contains stronger semantic clues and therefore can result in more reliable word associations than a monolingual corpus. second  word associations in our technique have a welldefined probabilistic interpretation. this enables a principled integration of the thesaurus model and a probabilistic retrieval model. an effective thesaurus-based technique must deal with the problem of word polysemy or ambiguity  which is particularly serious for arabic retrieval. for words with multiple meanings  a probabilistic technique like ours can emphasize probable meanings with high probabilities and discount unlikely ones with low probabilities. this curbs the impact of spurious word associations on retrieval.  
1 	 retrieval system 	 
our retrieval system was based on the probabilistic generative model described in  xu  weischedel and ngyuen  1 . it ranks documents according to the probability that a query q is generated from a document d:  
p q|d  =¡Ç  ¦Áp tq|gl  +  1 ¦Á  ¡Æ p td | d p tq | td    f  tq  q 
	tq in q	td in d
 
where tq's are query terms  td's are terms in the document  p tq|td  is the translation probability from td to tq and f tq  q  is the number of occurrences of tq in q. gl is a background corpus of the query language. the mixture weight ¦Á is fixed to 1. we estimate p tq|gl  and p td|d  as: p tq | gl  = frequencysize ofof gltq in gl  p td | d  = frequencysize ofof dtd in d
the retrieval model was originally proposed for clir. since monolingual retrieval is a special case of clir  where the query terms and document terms happen to be of the same language  e.g. arabic   the same retrieval system was also used for monolingual experiments. for simple monolingual ir  the lexicon used for term  translation  is an identity matrix  where p a|b =1 if a=b and 1 otherwise. for thesaurus-based monolingual retrieval  the translation probabilities were calculated from the un parallel corpus as discussed in the previous section. 
for clir  translation probabilities were estimated from the same un parallel corpus and were combined with a manual bilingual lexicon  the buckwalter lexicon  buckwalter  1   with around 1 arabic-english word pairs. we assume that translation probabilities in the manual lexicon are uniformly distributed. that is  if an arabic term has n english translations  each translation gets 1/n probability. the two lexical resources were combined using a mixture model with equal weights to produce a single probabilistic bilingual lexicon for term translation.  
in our monolingual experiments  the background corpus gl is the trec 1 arabic corpus. in our clir experiments  the background corpus consists of newspaper articles in trec english disks 1. 
1 	experimental setup 
our experiments were performed on the trec 1 arabic corpus  voorhees  1 . that corpus has 1 arabic documents from agence france presse  afp  with 1 test topics. each topic has three versions  arabic  english and french. the arabic topics were used in our monolingual experiments and the english topics in our clir experiments. only the title and description fields of the topics were used in query formulation. retrieval performance was measured using the trec noninterpolated average precision  voorhees  1 .  
in addition to average precision  standard t-test  hull  1  was used to determine the statistical significance of the retrieval difference between two retrieval runs. a difference is considered to be statistically significant if the p value is less than 1.  
we used the arabic stop word list compiled by yaser al-onaizan  http://www.isi.edu/~yaser/arabic/arabic-stop-words.html . that list was augmented with a handful of manually selected high frequency words from the afp corpus. in our experiments  english words were stemmed using the porter stemmer  porter  1 . 
1 	baseline for arabic monolingual retrieval 
experiments in this section will establish a baseline for arabic monolingual retrieval. since spelling normalization and stemming are well-studied ir techniques  they were employed in the baseline. 
ambiguities arise when the buckwalter stemmer returns several stems for a word. we considered two alternatives  sure-stem and all-stems. with sure-stem  we only stemmed a word if it has exactly one possible stem. otherwise  the word was left alone. both the documents and the queries were processed in the same manner. with all-stems  we did not stem the words in the documents but instead probabilistically  translated  them to stems. the query words were stemmed though  by replacing each word by its possible stem s . in the absence of training data  we assume that all possible stems are equally-probable. that is  if a word w has n possible stems s1  s1  ...sn  then p si|w =1/n. the advantage of sure-stem is that it does not introduce additional ambiguity  while the advantage of all-stems is that it always finds a stem for a word when one exists.  
to show the impact of spelling normalizations  see section 1  and stemming  four retrieval runs were carried out:  
1. there was no text processing except for the removal of the stop words from the documents and the queries.  
1. spelling normalization was used in addition to stop word removal 
1. sure-stem was used in addition to stop word removal and spelling normalization 
1. all-stems was used in addition to stop word removal and spelling normalization 
in 1  translation probability p tq|td =1 if tq=td and 1 otherwise. that is  a term was only translated to itself.  
results in table 1 show that spelling normalization produced a 1% relative improvement in retrieval performance. the improvement is statistically significant  p value=1 . this is not surprising because spelling variations are prevalent in the test corpus. statistics from the test corpus indicate that about 1% of the words containing a glyph combining hamza or madda with alef also have a variant with just a plain alef in the corpus. a human assessment of a few hundred sentences indicates that if two words exist which are only different in that one has such a glyph and the other has a plain alef  most occurrences of the word with a plain alef would be written as the word containing the glyph under a strict writing standard.  
sure-stem stemming produced a remarkable 1% relative improvement in performance. the improvement is statistically significant  p value=1 . the impact of stemming on arabic retrieval is far greater than the impact on english retrieval  harman  1 . the complex morphology of arabic causes a high level of synonymy in its vocabulary. for example  the trec arabic corpus has over 1 unique unstemmed words. in comparison  an english corpus of comparable size  wall street journal in trec disks1  has about 1 unstemmed words. many arabic words can be conflated to the same stem. statistics collected from the test corpus indicate that 1 stems have 1 or more unstemmed words  and 1 stems have over 1 unstemmed words. given such data  it is understandable that stemming has such a big impact on retrieval.  
there is little difference between the retrieval scores of sure-stem and all-stems. the difference is not statistically significant. statistics show that 1% of words in the test corpus have two or more possible stems. the percentage is probably too small to have an impact on retrieval. we will use the sure-stem result as our monolingual baseline.  
table 1: impact of spelling normalization and stemming on 
arabic monolingual retrieval 
stop words removal normalization  sure-stem all-stems 1 1 1 1  
1 	improving on the 
monolingual baseline 
1 n-gram-based retrieval 
two methods of creating n-grams were tried: from words and from stems. retrieval scores in table 1 show that stem-based ngrams are better than word-based n-grams for retrieval. the probable reason is that some of the word-based n-grams are prefixes or suffixes  which can cause false matches between documents and queries. the best results were obtained with trigrams  suggesting that bigrams carry too little contextual information while 1-grams and longer ones simply simulate word or stem-based retrieval.  
trigrams from stems produced the best result  with a 1% relative improvement over the baseline  from 1 to 1 . however  the improvement is not statistically significant  meaning the benefit of using n-grams is not conclusive.  
table 1 : retrieval results using n-grams 
 bigrams trigrams 1-grams words 1 1 1 stems 1 1 1  
1 thesaurus-based retrieval 
table 1 compares the retrieval performance of the statistical thesaurus described in section 1 with the baseline and the trigram results. the smoothing parameter ¦Â in the mixture model was set to 1 in the experiment. the relative improvement over the baseline is 1%. the improvement over trigrams is 1%. the improvements in both cases are statistically significant  p value=1 and 1 respectively . the results clearly show that the thesaurus is a better technique than the use of n-grams for improving on the monolingual baseline. while it appears that both broken plurals and general synonyms have contributed to the improved retrieval  a breakdown of the two factors is not available because we do not have a human assessment on the word pairs in the thesaurus. this is left for future work. 
table 1 : comparing baseline  trigrams and thesaurus for 
arabic monolingual retrieval 
baseline trigrams thesaurus 1 1 1  
figure 1 shows the retrieval performance as a function of the smoothing parameter ¦Â. as we discussed in section 1  a larger ¦Â places more confidence in the original terms while a smaller ¦Â places more confidence in the translations learned from the parallel corpus. retrieval performance peaks when ¦Â=1. overall  retrieval performance is not sensitive to the choice of ¦Â: any value between 1 and 1 works fine. 

figure 1:  the effect of smoothing parameter ¦Â on thesaurusbased retrieval. 
using a thesaurus can be viewed as a query expansion technique. another commonly used query expansion technique in ir is local feedback  buckley et al  1 . local feedback selects terms from top retrieved documents and adds them to the initial query. the expanded queries usually significantly improve retrieval performance. one may wonder whether thesaurus and local feedback overlap  and whether using one eliminates the need for the other.  
to address that concern  we compared two versions of local feedback. in one  the statistical thesaurus was used in both the initial retrieval and the final retrieval with the expanded queries. 
in the other  the thesaurus was not used at all. in our experiments  local feedback selected 1 terms from 1 top retrieved documents based on their total tf¡Áidf weight in the top documents. the expansion terms and the original query terms were re-weighted. in the probabilistic retrieval model used in this work  we interpret the weight of a query term to be the frequency of the term being generated in query generation. as described in section 1  the frequency is used as an exponent in the retrieval function. with local feedback  the frequency of a term t in a query q is calculated:  
 f  
where di is a top retrieved document  tfidf t  di  is the tf¡Áidf weight of term t in di   fold t  q  and f t  q  are the old and new frequencies of t in the query. the tf and idf functions were based on the ones described in  allan et al  1 .  
results in table 1 show that using local feedback and the thesaurus together is 1% better than local feedback alone. this is comparable to the 1% improvement produced by the thesaurus when local feedback was not used. the improvement is statistically significant  p value=1 . the results suggest that local feedback and the thesaurus are two different types of query expansion techniques.  
table 1: the impact of the thesaurus when used together with local feedback 
local feedback only local feedback + thesaurus 1 1  
1 	cross-lingual experiments 
to explore the impact of spelling normalization and arabic stemming on clir  we have compared three versions of bilingual lexicon creation for term translation. all three were formed from the un parallel corpus and the buckwalter lexicon using the same procedure described in section 1. the only difference is that the arabic terms are non-normalized words in the first lexicon  are non-normalized stems in the second  and are normalized stems in the third.  
results in table 1 show that the differences between the three versions of lexicon are very small. the differences are not statistically significant. this is different from monolingual ir  where spelling normalization and stemming had a very big impact. the explanation is that the un parallel corpus  with over 1 million words in each language  has enough data to enable giza++ to reliably learn the english translations for most arabic words. it appears that the buckwalter lexicon also lists the most common arabic spelling variants. therefore  the advantage of translating words by groups over translating them individually is very small. besides  stemming and normalization invariably introduce ambiguity. apparently  the small benefit of stemming and spelling normalization was canceled by the introduced ambiguity. although their impact on clir performance is small  spelling normalization and stemming are still useful because they reduce the need for memory because there are fewer entries in the lexicon and they improve the retrieval speed by simplifying the score computation.  
table 1: the effect of spelling normalization and stemming of arabic words on clir 
translation of nonnormalized words translation of non-normalized stems translation of normalized stems 1 1 1 one might wonder whether we can use the arabic monolingual thesaurus to improve clir. the assumption is that the thesaurus would be useful for cases where we do not know how to translate a word but we do know how to translate its synonyms. we did not run such an experiment because it is computationally prohibitive  requiring multiplying three large matrices. but based on a similar argument to the one we just made  it is unlikely such an experiment would produce better retrieval results given that we already have enough resources for effective term translation.  
1 	related work 
a key enabling element for our work has been the release by nist of a large arabic corpus with relevance judgments for both monolingual and cross-lingual retrieval  voorhees  1 . arabic stemming algorithms can be roughly classified as either stembased or root-based. while stem-based algorithms such as the buckwalter stemmer  buckwalter  1  remove prefixes and suffixes from arabic words  root-based algorithms  beesley 1; khoja and garside  1  further reduce stems to roots. a study reported better results using roots than stems on a very small test corpus  abu-salem et al  1 . however  a later study on the trec corpus showed that stem-based retrieval is more effective than root-based retrieval  aljlayl et al  1 . the idea of using ngrams for arabic retrieval appeared in several studies  darwish et al  1; mayfield et al  1; kwok et al  1 .  the use of thesauri in ir has been extensively studied under a variety of names  such as keyword clustering  spark jones  1   co-ocurrence thesauri  jing and croft 1; sch¨¹tze and pedersen  1  and latent semantic indexing  deerwester et al  1 . one difference from existing approaches is that our thesaurus was derived from a parallel corpus instead of a monolingual corpus. another difference is that our thesaurus is probabilistically motivated. a similar idea was proposed by  berger and lafferty  1 . but that work used artificially synthesized training data while ours used a parallel corpus. the use of parallel corpora for cross-lingual ir has been well studied  sheridan and ballerini  1; nie et al  1; j. mccarley  1 . the use of probabilistic generative models for ir appeared in a number of studies  ponte and croft  1; miller et al  1; hiemstra and de jong  1 .  
1 	conclusions and future work 
this work evaluated a number of search strategies for the retrieval of arabic documents  using the trec arabic corpus as the test bed. for arabic monolingual retrieval  spelling normalization significantly improved retrieval performance by 1%. stemming is critical  improving retrieval performance by 1%. two techniques were explored to further improve retrieval. tri-grams from stems produced a modest improvement in retrieval  but the improvement is not statistically significant. in comparison  a sophisticated statistical thesaurus boosted retrieval performance by 1%. we also studied the impact of spelling normalization and stemming on arabic clir. retrieval results show that their impact on clir is very small.  
there are a number of areas for future work. first  we would like to compare our stemming algorithms with other arabic stemming algorithms. second  we would like to compare our thesaurusbased technique with similar techniques such as latent semantic indexing. third  we would like to apply our thesaurus-based technique to other languages. for example  by simply swapping the roles of arabic and english  we can induce an english thesaurus instead of an arabic one from the un parallel corpus. fourth  we would like to classify the word pairs in our thesaurus and investigate which category  broken plurals or general synonyms  has the largest impact on retrieval. fifth  we would like to validate our techniques on more test corpora when they are available. such a validation is necessary since the problem of incomplete relevance judgments is potentially severe for the trec 1 arabic corpus  gey and oard  1 .  
