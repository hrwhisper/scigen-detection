exact computation for aggregate queries usually requires large amounts of memory - constrained in data-streaming - or communication - constrained in distributed computation - and large processing times. in this situation  approximation techniques with provable guarantees  like sketches  are the only viable solution. the performance of sketches crucially depends on the ability to efficiently generate particular pseudo-random numbers. in this paper we investigate both theoretically and empirically the problem of generating k-wise independent pseudo-random numbers and  in particular  that of generating 1 and 1-wise independent pseudorandom numbers that are fast range-summable  i.e.  they can be summed up in sub-linear time . our specific contributions are:  a  we provide an empirical comparison of the various pseudo-random number generating schemes   b  we study both theoretically and empirically the fast rangesummation practicality for the 1 and 1-wise independent generating schemes and we provide efficient implementations for the 1-wise independent schemes   c  we show convincing theoretical and empirical evidence that the extended hamming scheme performs as well as any 1-wise independent scheme for estimating the size of join using ams-sketches  even though it is only 1-wise independent. we use this generating scheme to produce estimators that significantly outperform the state-of-the-art solutions for two problems - size of spatial joins and selectivity estimation.
1. introduction
¡¡exact computation for aggregate queries usually requires large amounts of memory - constrained in data-streaming - or communication - constrained in distributed computation - and large processing times. in this situation  approximation techniques with provable guarantees that can be maintained over data-streams or that can be used for estimations in distributed environments are the only viable solu-

 
 material in this paper is based upon work supported by the national science foundation under grant no. 1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
tion. due to their linearity  ams-sketches  have all these properties and they have been successfully used for the estimation of aggregates like the size of join over data-streams  1  1  and for computations in distributed environments like sensor networks . ams-sketches depend on the ability to efficiently generate large families of random variables with particular properties: limited degree of independence and  for applications in which the input is specified as a set of intervals  fast range-summation  i.e.  the ability to sketch an interval in time sub-linear in the size of the interval . as we show in this paper  the last property is especially useful when ams-sketches are applied for solving problems like size of spatial joins   l1-difference of two vectors   and selectivity estimation for the dynamic construction of histograms  over data-streams or in a distributed environment.
¡¡in this paper we investigate both theoretically and empirically the known methods for generating random variables used by the ams-sketches with the goal of identifying the generating schemes that are practical  both for the traditional application of ams-sketches  i.e.  aggregate computation over streaming data  and for applications that involve interval inputs. more specifically  our contributions are:
  we provide an empirical comparison of the various generating schemes with the goal of identifying the efficient ones. to this  we explain how the schemes can be implemented on modern processors and we use such implementations to empirically evaluate the generating schemes. the main conclusion of this study is that the schemes based on bch codes need the smallest seeds and have the most efficient implementations.
  we provide a detailed study of the practicality of fast range-summation for the known generating schemes. we show that no 1-wise independent generating scheme is practical  even though the scheme based on reedmuller codes can be theoretically range-summed in
sub-linear time . two of the 1-wise independent schemes  the 1-wise bch scheme and the extended hamming scheme   are practical. for both  we explain how they can be efficiently implemented and we conduct an empirical study to determine their performance.
  we show  both theoretically and empirically  that the extended hamming scheme  eh1   is as good  and sometimes much better  than any 1-wise independent generating scheme for estimations using ams-sketches. the fact that eh1 gets within a constant factor of the error of the 1-wise independent schemes for the problem of computing the l1-difference of two streaming vectors was theoretically proved by feigenbaum et al. . here we show a significantly stronger result  namely that eh1 can always replace the 1wise independent schemes for estimations using amssketches without sacrificing accuracy. moreover  eh1 has two clear advantages: it can be implemented more efficiently and  more important  it is practically fast range-summable. we show that the estimation of the size of spatial joins using eh1 has significantly smaller errors than the solution proposed in .
¡¡in the rest of the paper  we first give some introductory notions in section 1  then we discuss the known generating schemes for random variables with limited independence in section 1. in section 1 we investigate which generating schemes are fast range-summable from a practical point of view. in section 1 we provide theoretical proof that the extended hamming generating scheme  eh1  works as well as the 1-wise independent schemes with the added benefit that it is fast range-summable. we provide empirical evidence of this fact in section 1 together with a thorough comparison of the estimations using eh1 and the 1-wise schemes on two applications with previously proposed solutions. we conclude in section 1.
1. preliminaries
¡¡in this section we give some preliminaries that are useful for understanding the rest of the paper.
1 random sketches
¡¡sketches  are randomized schemes for approximating aggregates such as the size of join. they are particularly suited when the computation is memory restricted - in the case of data-streams - or communication restricted - in the case of distributed systems such as sensor networks.
¡¡to introduce a sketch-based solution  consider the problem of computing the size of the natural join of two relations  r and s  each with a single attribute a  |r 1a s|. if we let i to be the domain of the attribute a and ri and si to be the frequency of the value i ¡Ê i in r and s  respectively  the size of join problem is to estimate the quantity |r 1a s| = pi¡Êi risi. the straightforward solution to this problem is to maintain the frequency vectors ~r and ~s and then to compute the size of join. such a solution would not work if the amount of memory or the communication bandwidth is smaller than |i|. the solution based on sketches is defined as follows:
1. start with a family of 1-wise independent ¡À1 random variables ¦Îi i ¡Ê i  i.e.  any four random variables in the family are independent .
1. define the sketches xr = pi¡Êi ri¦Îi = pt¡Êr ¦Ît.a and  similarly  xs = pi¡Êi si¦Îi = pt¡Ês ¦Ît.a.
1. define the random variable x = xrxs. x has the properties that it is an unbiased estimator for the size of join |r 1a s| and that it has small variance. an estimator with relative error at most  with probability at least 1   ¦Ä can be obtained by taking medians of averages of multiple independent copies of the random variable x; the number of medians is proportional to log 1¦Ä  while the number of averaged random variables is proportional to .
¡¡sketches are perfectly suited for both data-streaming and distributed communication since they can be updated on pieces. for example  if the tuples in the relation r are streamed one by one  xr can be computed by simply adding the value ¦Îi  where i is the value of the current item. for distributed computation  each party can compute the sketch of the data it owns; by exchanging only the value of the sketch with the other parties and simply adding up the sketches  the sketch of the entire data can be obtained.
¡¡the type of sketch described here uses ¡À1 random variables. ¡À1 values can be obtained by simply generating random bits with required properties and then interpreting the {1} values as ¡À1. the minimum requirement for this family of random variables is to be 1-wise independent  which ensures that x is an unbiased estimator for the size of join. the stronger 1-wise independence property is usually required in order to make the variance as small as possible  which reduces the number of copies of x that need to be averaged in order to achieve a given precision.
¡¡the above sketches using ¡À1 random variables with limited independence can be extended so that results of large classes of queries can be approximated. for example  dobra et al.  show how to extend the sketches to compute complex aggregates over general equi-joins  das et al.  show how to approximate the size of spatial joins  ganguly et al.  show how to compute aggregates over expressions involving set operators. what all these schemes have in common is the dependency on ¡À1 random variables and the fact that a certain amount of independence is required in order to keep the variance small. for applications such as spatial joins an extra property is required for the random variables: fast range-summation  i.e.  the ability to compute pi¡Ê ¦Á ¦Â  ¦Îi for a range  ¦Á ¦Â  in time sub-linear in the size of the range.
1 abstract algebra
¡¡as mentioned in the previous section  ¡À1 random variables with limited independence can be obtained by generating {1} random variables and mapping them to ¡À1. since {1} are the only elements of the galois field with order 1  denoted by gf 1   abstract algebra is the ideal framework in which to talk about the generation of families of random variables with limited independence. the field gf 1  has two operations: addition  boolean xor  and multiplication  boolean and . abstract algebra provides two ways to extend the field gf 1 : vector spaces and extension fields. both these extensions are useful for generating limited independence ¡À1 random variables.
¡¡gf 1 k vector spaces are spaces obtained by bundling together k dimensions  each with a gf 1  domain. the only operation we are interested in here is the dot product between two vectors v and u  defined as: v.
for gf 1 k vector spaces this corresponds to and-ing the arguments and then xor-ing all the resulting bits.
¡¡gf p  prime fields are fields over the domain {1 ... p  1} with both the multiplication and the addition defined as the arithmetic multiplication and addition modulo the prime
p.
¡¡gf 1k  extension fields are fields defined over the domain {1 ... 1k   1} that have two operations: addition  +  with zero element 1  and multiplication  ¡¤  with unity element 1. both addition and multiplication have to be associative and commutative. also  multiplication is distributive over addition. all the elements  except 1  must have an inverse with respect to the multiplication operation. the usual representation of the extension fields gf 1k  is as polynomials of degree k   1 with the most significant bit as the coefficient for xk 1 and the least significant as the constant term. the addition of two elements is simply the addition  term by term  of the corresponding polynomials. the multiplication is the polynomial multiplication modulo an irreducible polynomial of degree k that defines the extension field. with this representation  the addition is simple  just xor the bit representations   but the multiplication is more intricate since it requires both polynomial multiplication and division.
1 dyadic intervals
¡¡as mentioned in section 1  there exist applications requiring the ¡À1 limited independence random variables to be range-summable. one strategy to design such random variables is first to achieve range-summation for special intervals and then to extend it to general intervals. dyadic intervals  are particularly useful for this purpose. given a domain i of size |i| = 1n  its dyadic intervals are all the intervals of the form  q1j  q + 1j  with 1 ¡Ü j ¡Ü n and 1 ¡Ü q ¡Ü 1n j.
a graphical representation of the dyadic intervals defined over the domain i = {1 ... 1} is depicted in figure 1.

figure 1: the set of dyadic intervals over the domain i = {1 ... 1}
¡¡any interval  ¦Á ¦Â  on i has a unique minimal decomposition - minimal dyadic cover - into at most 1n 1 dyadic intervals. if a fast range-summable algorithm exists for dyadic intervals  then it can be extended to arbitrary intervals by simply summing up the contribution of each dyadic interval; the running time increases by at most a logarithmic factor. the minimal dyadic cover of  ¦Á ¦Â  can be determined straightforwardly from the binary representation on n bits of the end-points of the interval. this allows an efficient implementation of the algorithms that use dyadic interval decomposition on modern processors.
1. generating schemes
¡¡based on the published literature   ams-sketches described in section 1 need ¡À1-wise independent random variables that can be generated in small space in order to
produce correct estimations. we address the problem of whether the 1-wise independence is actually required later in the paper  but the 1-wise independence is a minimum requirement since otherwise the estimator is not even unbiased. in this section we review a number of generating schemes that are either 1-wise1 or 1-wise independent and we discuss how they can be implemented on modern processors. in the subsequent sections we refer back to these generating schemes.
¡¡in order to be space efficient  all the schemes have the following form:
	¦Îi s  =   1 f s i  	i ¡Ê i	 1 
where s is a random seed from some space  to be specified later  and function f can be efficiently computed from i and s  polynomial in the representation . since a ¦Îi is required for every different value in a large domain  e.g.  i = {1 ... 1   1}  generating methods that depend on small seeds are crucial for the success of sketching methods. fortunately  there are several methods to generate ¦Î families from small seeds. an important characteristic of a generating scheme is the degree of independence. since all the schemes are required to produce the values ¡À1 with the same probability  the k-wise independence requirement of a generating scheme can be expressed by the following definition :
	definition 1	 uniform k-wise independent family .
a family ¦Î of ¡À1 random variables defined over the sample space i is k-wise uniform independent if for any k different instances of the family  ¦Îi1 ¦Îi1 ... ¦Îik  and any k ¡À1 values  v1 v1 ... vk  we have:
	pr ¦Îi1 = v1 ¡Ä ¦Îi1 = v1 ¡Ä ¡¤¡¤¡¤ ¡Ä ¦Îik = vk  = 1 k	 1 
in order to show that a scheme defined by a particular function f s i  is uniform k-wise independent  it is enough to show this property holds for the bits generated by f s i   which simplifies the exposition. this is the case since   1 x maps 1 into 1 and 1 into  1.
¡¡throughout this section we make extensive use of the notation in section 1  in particular the dot product over vector spaces. before we introduce the known generating schemes  we identify two common points of the majority of the schemes. first  a large number of schemes use dot products in vector spaces. dot products can be implemented by simply and-ing the two vectors and xor-ing the resulting bits. while and-ing entire words  integers  on modern architectures is extremely fast  xor-ing the bits of a word  which has to be performed eventually  is problematic since no high-level programming language supports such an operation  this operation is actually the parity bit computation . to speed up this operation  which is critical  we implemented it in assembly for pentium processors to take advantage of the supported 1-bit parity computation. the second point we make is that all the schemes need uniform random seeds from spaces of the form {1 ... 1n 1}. these seeds can be generated by simply choosing n uniformly independent bits and combining them to form the binary representation of the seed.
¡¡for all the schemes  we assume the domain to be i = {1 ... 1n 1} for a generic n  the description of the schemes depends on n . we also make the convention that  a b  is equivalent with the vector obtained by concatenating the vectors a and b. the size of a  b  and  a b  will be clear from the context.
1 bch scheme
¡¡the bch scheme was first introduced in  and it is based on bch codes. this scheme can generate k-wise independent random variables by using uniformly random seeds s that are kn + 1 bits in size and has the function f s i  defined as:
	f s i  = s ¡¤  1 i ... i1k 1 	 1 
where 1 is a bit and i1k 1 is computed in the extension field gf 1n . this scheme comes close to the theoretical bound based on rao's inequalities  on how small the seed space can be - it is the scheme with the smallest seed requirement amongst all the known schemes. the proof that this scheme produces  1k+1 -wise independent families can be found in
.
¡¡implementing i1k 1 over finite fields is problematic on modern processors if speed is paramount  but in the special case when only 1-wise independence is required this problem is avoided  see the speed comparison at the end of the section . since the 1-wise independent version of this scheme  called bch1 throughout the paper  is important in latter developments  we provide here its particular form:
	f s i  = s ¡¤  1 i 	 1 
the 1-wise independence required by the ams-sketches can be obtained with f s i  = s ¡¤ 1 i i1  and requires  1n+1 bit seeds  compared with n + 1 for bch1.
1.1 extended hamming 1-wise scheme  eh1 
¡¡the extended hamming 1-wise scheme is a modification of bch1 and it was introduced in . it requires seeds s of size n + 1 and its generating function is defined as:
	f s i  = s ¡¤  1 i  ¨’ h i 	 1 
where h i  is a nonlinear function of the bits of i. a possible form of h is:
	h i  = i1 ¡Å i1 ¨’ ¡¤¡¤¡¤ ¨’ in 1 ¡Å in 1	 1 
function h does not change the amount of independence  thus  from the traditional ams-sketches theory  it is not as good as a 1-wise independent scheme  but  as proved theoretically by feigenbaum et al.   it produces comparable errors. we discuss the importance of the function h in section 1.
¡¡from the point of view of a fast implementation  only a small modification has to be added to the implementation of bch1 - the computation of function h i . as the experimental results show  there is virtually no running time difference between these schemes if a careful implementation is deployed on modern processors.
1 reed-muller scheme
¡¡the bch schemes require computations over extension fields for degrees of independence greater than 1. since the ams-sketches need 1-wise independence  alternative schemes that require only simple computations might be desirable.
the reed-muller scheme  generalizes the bch codes in a different way in order to obtain higher degrees of independence. seeds of size 1 + are required to obtain a degree of independence of k = 1t+1   1  t   1. we introduce here only the 1-wise independent version of the scheme that requires 1 + seed bits:
wheref s i  = s ¡¤  1 i i 1   1 	 	 1 
1 polynomials over primes scheme
¡¡the generating schemes in the previous sections are derived from error-correcting codes. in this section  we present a different method of generating k-wise independent random variables that uses polynomials over a prime number field
.
	theorem 1	  . let p ¡Ý n be a prime.	choose
a1 a1 ... ak 1 uniformly and independently at random from zp  and let xj = a1 + a1j + a1 + ¡¤¡¤¡¤ + ak 1jk 1 mod p  for 1 ¡Ü j   p. then x1 x1 ... xp 1 are uniform k-wise independent.
¡¡this scheme generates k-wise independent random variables that have p values. in order to restrict the domain to two values  we consider the binary representation of the variables and take into consideration only one bit. the resulting two-valued random variables are not uniformly distributed - they are slightly biased. however  for large primes  e.g.  p = 1  1  the bias between the variables with value 1 and those with value 1 is negligible  i.e.  .
¡¡the seed of this scheme consists of the k coefficients aj  1 ¡Ü j   k  each represented on dlogpe bits. note that the size of the seed almost doubled compared to the seed needed for the corresponding bch generating scheme.
1 performance evaluation
schemetime  ns seed sizebch1.1n + 1eh1.1n + 1massdal1.1nbch1.1n + 1massdal1.1nrm1 1 + n + n n 1 1
table 1: generation time and seed size
¡¡we implemented the 1-wise independent bch  bch1   the 1-wise independent bch  bch1   the extended hamming  eh1   and the 1-wise independent reed-muller  rm1  schemes  and we used massdal  for the implementation of the polynomials over primes scheme. we ran our experiments on a two-processor xeon 1 ghz machine  each with a 1 kb cache. the system had 1 gb available memory and used a fedora core 1 operating system. as a comparison for our results  the time to read a word from a memory location that is not cached in either l1 or l1 cache takes about 1 ns on this machine. we used the special assembly implementation of the dot product for all the

1
 bch1 was implemented by performing the i operation arithmetically not in an extension field. this does not change the performance of bch1 for domains not too large. methods. experiments consisted in generating 1 variables i and 1 seeds and then computing all possible combinations of random variables  i.e.  1 1. each experiment was run 1 times and the average of the results is reported. the relative error was in general under 1%  with a maximum of 1%. since the results are so stable  we do not report the actual error for individual experiments. the generation time  in nanoseconds per random variable  is reported in table 1. when compared to memory random access time  all the schemes  except rm1  are much faster. indeed  eh1 is at least as fast as bch1  we believe it is actually faster since the extra operations maintain a better flow through the processor pipelines  and both are significantly faster than massdal. out of the 1-wise or higher independence schemes  clearly bch1 is the fastest  while rm1 is almost 1 times slower.
¡¡table 1 also contains the size of the seed for the given schemes. n represents the number of bits the domain i can be represented on. for the polynomials over primes scheme  n is the smallest power of 1 for which 1n ¡Ý p. as noted  the bch schemes have the seeds with the smallest size  while the reed-muller scheme needs the largest seed. for the same degree of independence  the polynomials over primes scheme requires a seed double in size compared to bch.
1. fast range-summable schemes
¡¡when at least one of the input relations to the size of join problem in section 1 is given as a union of intervals  the ¡À1 family of random variables is required to be fast range-summable. the fast range-summation property is the ability to compute the sum of random variables in an interval in time sub-linear in the size of the interval - the alternative is to generate and sum up the values ¦Îi for each i in the interval. formally  this property is defined as:
¡¡definition 1  bitwise range-summation  . a generating scheme for two-valued k-wise independent random variables is called bitwise range-summable if there exists a polynomial-time function g such that
	g  ¦Á ¦Â  s  = x f s i 	 1 
¦Á¡Üi¡Ü¦Â
where ¦Á  ¦Â  and i are vectors over the domain {1}n.
¡¡computing the function g over general  ¦Á ¦Â  intervals is usually not straightforward. the task is easier for dyadic intervals  see section 1  due to their regularity. fortunately  any scheme that is bitwise range-summable for dyadic intervals can be extended to general intervals  ¦Á ¦Â  by simply determining its minimal dyadic cover  computing the function g over each dyadic interval in the cover  and then summing up these results. since the decomposition of any  ¦Á ¦Â  interval contains at most a logarithmic number of dyadic intervals  fast range-summable algorithms for dyadic intervals remain fast range-summable for general intervals.
¡¡in this section  we study the bitwise range-summation property of the generating schemes presented in section 1. for the 1-wise independent random variables  both the bch1 scheme and its eh1 variant are fast range-summable. previous work show that there exist other schemes that are fast range-summable for the 1-wise case. for example  the scheme based on the toeplitz family of hash functions is shown to be fast range-summable in . a related algorithm for p-valued 1-wise independent random variables generated using the polynomials over primes scheme is introduced in . for the 1-wise case  the reed-muller generating scheme is the only scheme known to be fast range-summable  1  1 . reducing the range-summing problem to determining the number of boolean variables assignments that satisfy an xor-and logical expression and using the results in   we obtain a simple method to determine if a generating scheme is fast range-summable. we apply this method to show that the 1-wise bch and the polynomial schemes are not fast range-summable.
1 eh1 scheme
¡¡although feigenbaum et al.  show that the random variables generated using the extended hamming scheme  eh1  are fast range-summable  the algorithm contained in the proof is abstract and not appropriate for implementation purposes. we propose a practical algorithm for the fast range-summation of the eh1 random variables. it is an extension of our constant-time algorithm for range-summing bch1 random variables  that is not included here due to lack of space.
¡¡the following theorem provides an analytical formula for computing the range-sum function g. note that only one computation of the generating function f is required in order to determine the value of g over any dyadic interval. the proof can be found in the extended version of the paper .
¡¡theorem 1. let  q1j  q+1j  be a dyadic interval with size at least 1  j ¡Ý 1. the range-sum function g  q1j  q +
j
1defined for the extended hamming 1-wise scheme  eh1  is equal to:
	g  q1j  q + 1j  s  =   1 #zero ¡¤ 1j ¡¤ f s q1j 	 1 
where f is the ¡À1 generating function and #zero represents the number of two adjacent pair bits that or to 1.
based on the results in theorem 1  algorithm h1interval computes function g  ¦Á ¦Â  s  = p¦Á¡Üi¡Ü¦Â f s i  for any interval  ¦Á ¦Â . first  the minimal dyadic cover of  ¦Á ¦Â  is determined  then the sum over each dyadic interval is computed using  1 . note that these two steps can be combined  the computation of g being performed while determining the minimal dyadic cover of  ¦Á ¦Â . the minimal dyadic cover can be efficiently determined from the binary representation of ¦Á and ¦Â. since any interval can be decomposed into a logarithmic number of dyadic intervals  algorithm h1interval computes function g in o log ¦Â   ¦Á   steps.
algorithm 1 h1interval  ¦Á ¦Â   s =  s1 s1  
1. let d = {¦Ä1 ... ¦Äm} be the minimal dyadic cover of
 ¦Á ¦Â   where each ¦Ä has the form  q1j  q + 1j 
1. sum ¡û 1
1. for ¦Äk ¡Ê d  1 ¡Ü k ¡Ü m do
1. sum ¡û sum +   1 #zero ¡¤ 1j ¡¤ f s q1j 
1. end for
1. return sum

¡¡example 1. we show how algorithm h1interval works for the interval  1  and the seed s =  s1 s1  =  1 =  1 . the minimal dyadic cover of  1  is
d  1   = { 1   1   1  
 1   1 }
#zero is equal with 1 for the given s1  the only pair oring to 1 being the pair at the end. it affects the dyadic intervals with the power greater than 1.
g  1  s  = g  1  s  + g  1  s 
+ g  1  s  + g  1  s 
+ g  1  s 
=  1 ¡¤ f s 1    1 ¡¤ f s 1 
  1 ¡¤ f s 1  + 1 ¡¤ f s 1 
+ 1 ¡¤ f s 1 
= 1 + 1 + 1 + 1   1 = 1
1 four-wise independent schemes
¡¡in this section we investigate the bitwise range-summation property of the 1-wise independent generating schemes presented throughout this paper  namely bch and polynomials over primes. the discussion regarding the reed-muller scheme is deferred to the next section.
¡¡the main idea in showing that some of the schemes are not fast range-summable is to use the result due to ehrenfeucht and karpinski  on the problem of counting the number of times a polynomial over gf 1   written as xor of ands  sums of products with operations in gf 1    takes each of the two values in gf 1 . the result states that the problem is #p-complete if any of the terms of the polynomial written as an xor of ands contains at least three variables. for our application  to show that a scheme is not fast range-summable  it is enough to prove that for some seed s the generating function f s i   written as an xor of ands polynomial in the bits of i  contains at least one term that involves three or more variables. the following results  whose proof is omitted due to scarcity of space  use this fact to show that the bch1 and the polynomials over primes schemes are not fast range-summable.
¡¡theorem 1. the k-wise independent bch schemes are not fast range-summable for k ¡Ý 1 and n ¡Ý 1.
the bch1 scheme is not covered by this theorem and  indeed  it is fast range-summable. in fact  by exploiting special properties of the bch1 scheme  a fast range-summable algorithm can be implemented in o 1  average time  with respect to random seeds  if arithmetic operations and dotproducts are considered o 1  operations. the reason for this is the fact that  when computing the sum over any interval  ¦Á ¦Â   only the last bits of ¦Á and ¦Â that correspond to zero bits in the seed have to be processed before the result of the summation can be computed with a simple arithmetic formula. since the number of contiguous zero bits at the end of a uniformly random seed can be shown to be approximately 1 on average  the entire computation can be finished in o 1  time. a detailed presentation and proofs of these facts are deferred to the full version of the paper .
¡¡theorem 1. let n = dlogpe be the number of bits the prime p   1 can be represented on and  be a dyadic interval with l ¡Ý 1. then  the function f s i  =   a1 + a1i  mod p  mod 1 is not fast range-summable over the interval  q1l  q + 1l .
theorem 1 shows that for the polynomials over primes scheme with k = 1 there exist values for the coefficients a1 and a1 that make the scheme not fast range-summable for dyadic intervals with size greater or equal than 1 = 1. since the schemes for k   1 can be reduced to   a1 + a1i  mod p  mod 1 by making a1 = ¡¤¡¤¡¤ = ak 1 = 1  it results that the polynomials over primes scheme is not fast range-summable when k ¡Ý 1.
1 rm1 scheme
¡¡together with the negative result about the hardness of counting the number of times an xor of ands polynomial with terms containing more that three variables anded  ehrenfeucht and karpinski  provided an algorithm for such counting for formulae that contain only at most two variables anded in each term. this algorithm  that we refer to as 1xor-and  can be readily used to produce a fast range-summable algorithm for the 1-wise independent reed-muller  rm1  scheme. an algorithm for this scheme based on the same ideas was proposed in  1  1 . we focus our discussion on the 1xor-and algorithm  but the same conclusions are applicable to the algorithm in  1  1 .
¡¡the observation at the core of the 1xor-and algorithm is the fact that polynomials with a special shape are fast range-summable. these are polynomials with at most two variables anded in any term and with each variable participating in at most one such term. the other cases can be reduced to this case by introducing new variables that are linear combinations of the old ones. to determine these linear combinations in the general case  systems of linear equations have to be constructed and solved  one for each variable. the overall algorithm is o n1   with n the number of variables  if the summation is performed over a dyadic interval.
¡¡the 1xor-and algorithm can be used to fast rangesum random variables produced by the 1-wise reed-muller scheme since in the xor of ands representation of this scheme as a polynomial of the bits of i  which is the representation used in section 1  only terms with ands of at most two variables appear. using the 1xor-and algorithm for each dyadic interval in the minimal dyadic cover of a given interval  the overall running time can be shown to be o n1  where the size of i  the domain  is 1n. while this algorithm is clearly fast range-summable using the definition  in practice it might still be too slow to be useful. indeed this is the case  as it is shown in the next section where we provide running time comparisons of the fast range-summable algorithms.
¡¡since this fast range-summable algorithm is not practical and fast range-summable algorithms for bch1 and polynomials over primes schemes do not exist  it does worth to investigate approximation algorithms for the 1-wise case. while such approximations are possible  1  1   they are not more practical than the exact algorithm for rm1. a detailed theoretical and empirical evaluation of such approximation schemes can be found in the extended version of the paper
.
1 empirical evaluation
¡¡we implemented the fast range-summable algorithms for the bch1  eh1 and rm1 schemes and we empirically evaluated them with the same experimental setup as in section 1. the performance evaluation is based on 1 experiments that use a number of randomly generated intervals and an equal number of sketches chosen for each method such that the overall running time is in the order of minutes in order to obtain stable estimates of the running time per sketch. the results  depicted in table 1  are the average of the 1 runs and have errors of at most 1%. notice that the execution time of bch1 for ranges is merely 1 times larger than the execution time for a single sketch  refer to table 1 for the running times of individual sketches  - this happens since  as we mentioned earlier  our algorithm for bch1 is essentially o 1 . the extended hamming scheme eh1 has an encouraging running time of approximately 1 ¦Ìs  thus about 1 such computations can be performed per second on a modern processor. the reed-muller fast range-summable algorithm is completely impractical since only about 1 computations can be performed per second. this is due to the fact that the algorithm is quite involved  a significant number of systems of linear equations have to be formed and solved . even if special techniques are used to reduce the running time  at most a 1 factor reduction is possible and the scheme would be still impractical.
¡¡the net effect of these experimental results and of the theoretical discussions in the previous section is that there is no practical fast range-summable algorithms for any of the known 1-wise generating schemes. fortunately  as we show in the next section  the eh1 scheme can successfully replace the 1-wise independent generating schemes in applications using ams-sketches.
schemetime  ns bch1.1eh1 1rm1.1 ¡Á 1table 1: sketching time per interval
1. streams and random sketches
¡¡the ams-sketches introduced in section 1 are a versatile approximation method that can be applied to numerous estimation problems and that can accommodate multiple types of input. in this section we investigate applications that require fast range-summable random variables and explain how the generating schemes introduced earlier in the paper can be used for successful estimation. as mentioned in section 1  the basic problem the ams-sketches are solving is estimating the size of join of two relations when they are specified tuple-by-tuple over a data-stream. here we consider a variation of this problem in which one of the relations is specified as a stream of intervals  this is equivalent to specifying every point inside the interval . first we introduce three practical applications for the size of join problem with interval-input data and then we explain how the fast range-summable generating schemes can be used to solve this problem. we also point out other solutions.
1 applications of interval-input sketches
¡¡we introduce three problems - spatial joins   l1 estimation   and the construction of multidimensional histograms  - for which solutions using ams-sketches have been proposed. since all these problems can be reduced to the computation of the size of join where one of the relations is specified as a sequence of intervals  solutions based on fast range-summable random variables can be provided. for each of these applications we show the reduction to the size of join problem and how the fast range-summable amssketches provide improved solutions.
	application 1	 spatial size of join  .
given two sets of line segments in an unidimensional space  the spatial size of join problem is to compute the number of segments in the two sets that intersect. the approach in   even though not explicitly stated  expresses the solution as the average of two size of join estimators: the size of join of the line segments from the first relation and the segment end-points from the second relation and  symmetrically  the size of join of the segment end-points from the first relation and the segments from the second relation. in order to avoid the range-summation of the intervals  the proposed solution maps the initial domain into the domain of all possible dyadic intervals that can be defined over it. by doing this  the size of each initial interval reduces to at most a logarithmic number of points in the new domain  i.e.  the number of sketch updates reduces from linear to logarithmic in the size of the interval. at the same time  each segment end-point maps to a logarithmic  in the size of the initial domain  number of points in the new domain  increasing the number of sketch updates from one to a logarithmic factor. the problem can be generalized to multiple dimensions  see   by defining estimators over all possible combinations of full segments and end-points in each dimension. a more detailed discussion of this algorithm and our solution that uses fast range-summable random variables are presented in section 1.
	application 1	 l1-difference  .
given two vectors ~a and ~b with elements ai and bi  respectively  i ¡Ê i  where i is a large domain  compute in small space the l1-difference of the two vectors defined as pi |ai   bi|  when the vector elements are streamed in random order as tuples of the form  i ai  and  i bi . the reduction to the size of join problem is obtained by first fixing a maximum value  m  for the elements in the vectors ~a and
~b.	two virtual relations can be introduced  ra for ~a and
rb for ~b  each containing tuples of the form  i j  for every i ¡Ê i and every j ¡Ê {1 ... ai}  j ¡Ê {1 ... bi}  respectively. it can be shown that pi |ai   bi| is the self-join size of the symmetric difference of ra and rb. to see why this is the case  notice that for every tuple in the symmetric difference the contribution to the self-join size is 1. there are exactly |ai   bi| such contributions for each i ¡Ê i since this is the size of the symmetric difference between the parts of ra and rb that have the first attribute value i. the relations ra and rb are specified by a number of ranges  one for each ai and bi  respectively. the solution to the self-join size problem has to accommodate the easy computation of the symmetric difference and has to have the input specified as intervals.
	application 1	 dynamic histograms  .
any histogram construction algorithm  such as the one in   is based on evaluating the average frequency for any rectangular region  a potential bucket . the computation of the average frequency can be performed by computing the sum of the frequencies in the region and dividing by the size of the region. if two virtual relations are introduced  one that specifies the data for which the histogram is constructed and one that specifies the region by enumerating all the points in the region  it is easy to see that the sum of the frequencies in the region is exactly the size of the join of these two relations. the second relation is specified by a hyper-rectangle  thus solutions to the size of join problem that accommodate interval inputs are required.
¡¡the common point of the above three applications is the fact that they can be reduced to efficiently computing the size of join of two relations that accommodate interval-input data for at least one of the arguments. solutions using ams-sketches based on random variables generated with fast range-summable schemes fit perfectly these requirements. for two of the applications  spatial joins and dynamic construction of histograms  an alternative solution based also on the use of ams-sketches is possible . we provide the description of this solution  that we call dmap  in the next section.
1 dyadic mapping  dmap 
¡¡in their work on spatial joins   das et al. presented a solution based on ams-sketches and dyadic intervals. the solution can be decomposed1 in two parts: the reduction of the problem to the size of join problem when one of the input relations is specified as a stream of intervals and an approximation scheme for the size of join problem. the solution to the reduction problem was outlined in application 1. in this section we focus on the solution to the size of join problem for interval-input relations. we call this solution dyadic mapping  dmap  for reasons that will be clear shortly. we present here only the main idea of the method; details and proofs can be found in .
¡¡let r and s be the two relations with a common attribute a for which the size of join is to be computed. assume  without loss of generality  that the relation r is specified as a set of intervals  while the relation s as a set of points. in order to efficiently compute the size of join  dmap generates two virtual relations  one for r and one for s  by mapping both the intervals in r and the points in s to the space of all dyadic intervals defined over the domain of the attribute a  for an introduction to dyadic intervals see section 1 . the tuples of the new relations consist of a single dyadic interval attribute over i  the original domain of the attribute a. each interval  ¦Á ¦Â  in r is decomposed into its minimal dyadic cover and a tuple for each member of the cover is introduced in the virtual relation rd. for each point i in s  all the dyadic intervals containing the point  there are log|i| such intervals  are included in sd  the virtual relation corresponding to s. the size of join of the two relations r and s is estimated through the size of join of rd and sd. it is easy to see that this gives the correct result since for each point i ¡Ê  ¦Á ¦Â  there exists exactly one dyadic interval that contains i and that is included in the minimal dyadic cover of  ¦Á ¦Â . the size of join of rd and sd can be estimated using the ams-sketches and  thus  the size of join of the original relations r and s is obtained. the sketches of the relations rd and sd can be efficiently computed since both for an interval in r and a point in s at most log|i| dyadic intervals have to be sketched. notice that  by mapping to the space of dyadic intervals  dmap avoids the linear update time for the sketching of intervals.
¡¡to get an idea of how efficient dmap is  we timed an implementation of this scheme using the same setup as in section 1. the average execution time we obtained is 1 ns  which is slightly faster than the time for eh1  1 ns . the time to sketch a point is only 1 ns for eh1  but it is 1 ns  1 ¡Á 1 for |i| = 1  for dmap. as we show in section 1  the overall running times of eh1 and dmap algorithms are comparable  with a slight advantage for dmap.
1 size of join using sketches
¡¡as we pointed out earlier  an alternative solution to the size of join problem when at least one of the input relations is given as a set of intervals is to use fast range-summable families of random variables to speed up the sketching of the intervals. using the current understanding of the amssketches applied to the size of join problem  the 1-wise independence of the random variables is required in order to keep the variance small  we show latter in the section why . since  as it is shown in section 1  there is no practical fast range-summable 1-wise independent generating scheme  it looks like dmap is the only feasible alternative  given the state-of-the-art. what we show in this section is that the extended hamming 1-wise scheme  not only can be used as a reasonable replacement of the 1-wise independent schemes  this is the theoretical result proved in  for application 1   but it is usually as good  in absolute big-o notation terms  and in certain situations significantly surpasses the 1-wise independent schemes for ams-sketches solutions to the size of join problem. we provide here the theoretical support for this statement  but defer the empirical evidence to section 1.
¡¡we proceed by taking a close look at the estimation of the size of join of two relations using ams-sketches. let r and s be two relations with a common attribute a whose domain is i. for a given value i ¡Ê i  we denote by ri and si the frequency of the value i in r and s  respectively. with this  the size of join |r 1a s| is defined as |r 1a s| = pi¡Êi risi. to estimate this quantity  we use a ¡À1 family of random variables {¦Îi|i ¡Ê i} that are at least 1-wise independent  but not necessarily more . we define the sketches  one for each relation  as xr = pi¡Êi ri¦Îi and xs = pi¡Êi si¦Îi. the random variable x = xrxs estimates  on expectation  the size of join |r 1a s|. it is easy to show that  due to the 1wise independence  e x  = pi¡Êi risi  which is exactly the size of join. since all the generating schemes published in the literature  see section 1 for a review of the most important ones  are 1-wise independent  they all produce estimates that are correct on average. the main difference between the schemes is the variance of x  var x . the smaller the variance  the better the estimation; in particular  the error of the estimate is proportional with.
¡¡let us now analyze the variance of x. following the technique in   we first compute e  x1  since var x  = e  x1    e  x 1. we have:
	  	!1#
e  x1  = e xri¦Îi xsi¦Îi i¡Êi	i¡Êi
= xxxxrirjsksle  ¦Îi¦Îj¦Îk¦Îl 
i¡Êi j¡Êi k¡Êi l¡Êi
the expression e  ¦Îi¦Îj¦Îk¦Îl  is equal to 1 if groups of two variables are the same  i.e.  i = j ¡Ä k = l or i = k ¡Ä j = l or i = l ¡Ä j = k  since ¦Îi1 = 1 irrespective of the actual value of ¦Îi  1 = 1 and   1 = 1 . e  ¦Îi¦Îj¦Îk¦Îl  is equal to 1 if two variables are identical  say i = j  and two are different  since then e  ¦Îi¦Îj¦Îk¦Îl  = e  1 ¡¤ ¦Îk¦Îl  = 1  using the 1-wise independence property. the same is true when three of the variables are equal  say i = j = k  but the fourth one is not  since e  ¦Îi¦Îj¦Îk¦Îl  = e  ¦Îi1¦Îl  = 1  we use the fact that ¦Îi1 = ¦Îi . the above observations are not dependent on what generating scheme is used  as long as it is at least 1-wise independent. the contribution of the 1 values to e  x1  adds up to:

in the expression of the variance var x   the middle term has the coefficient 1  instead of 1  since e  x 1 is subtracted. the difference between the various generating schemes consists in what other terms have to be added to the above formula. the additional terms correspond only to the values of i j k l that are all different  otherwise the contribution is 1 or 1 irrespective to the scheme  as explained before . we explore what are the additional terms for three of the schemes  namely bch1  bch1  and eh1.
1.1 variance for bch1
¡¡the bch1 scheme is 1-wise independent  which means that for i 1= j 1= k 1= l  we have:
e  ¦Îi¦Îj¦Îk¦Îl  = e  ¦Îi  ¡¤ e  ¦Îj  ¡¤ e  ¦Îk  ¡¤ e  ¦Îl  = 1
since all the ¦Îs are independent and e  ¦Îi  = 1 for all generators. this means that no other terms are added to the above variance. the following formula results:
var x bch1 
1.1 variance for bch1
¡¡bch1 is only 1-wise independent  thus clearly it is not the case that  for all i 1= j 1= k 1= l  e  ¦Îi¦Îj¦Îk¦Îl  = 1. to characterize the value of the expectation for different variables  we need the following result:
¡¡proposition 1. let the function f on n binary variables x1 x1 ... xn be defined as:

where c ¡Ê {1} is a constant and sk ¡Ê {1}  for 1 ¡Ü k ¡Ü n  are parameters. if there exists at least one sk that is equal to 1  function f takes the values 1 and 1 equally often  each value appearing 1n 1 times. otherwise  function f evaluates to the constant c for all the possible combinations of x1 x1 ... xn.
the proof of this proposition is omitted due to scarcity of space  but it can be found in the extended version of the paper . the result is intuitive though  since if any of the parameter bits sk is nonzero  its corresponding variable xk causes the function f to take the values 1 and 1 in exactly half of the cases  irrespective to the constant c. now we can prove the result for bch1:
¡¡proposition 1. assume the ¦Îs are generated using the bch1 scheme. then  for i 1= j 1= k 1= l  e  ¦Îi¦Îj¦Îk¦Îl  = 1 if i ¨’ j ¨’ k ¨’ l 1= 1  and e  ¦Îi¦Îj¦Îk¦Îl  = 1 if i ¨’ j ¨’ k ¨’ l = 1  where ¨’ is the bit-wise xor.
¡¡proof. let s =  s1 s1  be the  n + 1 -bits random seed with s1 its first bit and s1 the last n bits. using the notations and the definition of bch1 in section 1  we have:
¦Îi =   1 s1¨’s1¡¤i
with this  we obtain:
e  ¦Îi¦Îj¦Îk¦Îl 
= e h  1 s1¨’s1¡¤i ¡¤   1 s1¨’s1¡¤j ¡¤   1 s1¨’s1¡¤k ¡¤   1 s1¨’s1¡¤li
= e h  1 s1¡¤i¨’s1¡¤j¨’s1¡¤k¨’s1¡¤li
= e h  1 s1¡¤ i¨’j¨’k¨’l i
using the result in proposition 1 with c = 1 and s1 ... sn set as the last n bits of the seed s  we know that the expression s1 ¡¤  i ¨’ j ¨’ k ¨’ l  takes the values 1 and 1 equally often for random seeds s1 when i ¨’ j ¨’ k ¨’ l 1= 1 - this immediately implies that e  ¦Îi¦Îj¦Îk¦Îl  = 1. when i ¨’ j ¨’ k ¨’ l = 1  s1 ¡¤  i ¨’ j ¨’ k ¨’ l  = 1 irrespective of
s1  giving e  ¦Îi¦Îj¦Îk¦Îl  = 1.	
this means that the variance for bch1 contains an additional term besides the ones that appear in the variance formula for bch1. the extra term has the following form:
 var bch1  = x x x rirjsksi¨’j¨’k
i¡Êi j¡Êi j1=ik¡Êi k1=i j
where i ¨’ j ¨’ k ¨’ l = 1 implies l = i ¨’ j ¨’ k. the additional term in the bch1 variance can be significantly large  implying a big increase over the variance for bch1.
1.1 variance for eh1
¡¡as bch1  the extended hamming scheme  eh1  is also 1-wise independent  which might suggest that the variance of eh1 is similar to the variance of bch1  extra terms not in the variance of bch1 have to appear  otherwise the scheme would be 1-wise independent . as we show next  even though only positive terms appeared in the variance for bch1  in the eh1 variance negative terms appear as well. these negative terms  in certain circumstances  can compensate completely for the positive terms and give a variance that approaches to zero.
proposition 1. assume the ¦Îs are generated using the
eh1 scheme. then  for i 1= j 1= k 1= l 
1 
 
 if i ¨’ j ¨’ k ¨’ l 1= 1 if i ¨’ j ¨’ k ¨’ l = 1 ¡Ä
h i  ¨’ h j  ¨’ h k  ¨’ h l  = 1	 	if i ¨’ j ¨’ k ¨’ l = 1 ¡Ä
	:	h i  ¨’ h j  ¨’ h k  ¨’ h l  = 1
where ¨’ is the bit-wise xor.
proof. we know that
¦Îi =   1 s1¨’s1¡¤i¨’h i 
for random variables generated using the eh1 scheme. replacing this form into the expression for e  ¦Îi¦Îj¦Îk¦Îl  and applying the same manipulations as in the proof of proposition 1  we get:
e  ¦Îi¦Îj¦Îk¦Îl  = e h  1 s1¡¤ i¨’j¨’k¨’l ¨’ h i ¨’h j ¨’h k ¨’h l  i
if we use again proposition 1 with c = h i ¨’h j ¨’h k ¨’ h l  and s1 ... sn set as the last n bits of the seed s  we first observe that the expectation is 1 when i ¨’ j ¨’ k ¨’ l 1= 1. when i ¨’ j ¨’ k ¨’ l = 1  the expected value is always   1 c. for bch1 the constant c always took the value 1  thus the expectation in that case was always 1. for eh1  the value of c depends on the function h - it is 1 when h i  ¨’ h j  ¨’ h k  ¨’ h l  = 1. this implies the value  1 for
e  ¦Îi¦Îj¦Îk¦Îl .	
using the above result  we observe that e  ¦Îi¦Îj¦Îk¦Îl  =  1 when i ¨’ j ¨’ k ¨’ l = 1 and h i  ¨’ h j  ¨’ h k  ¨’ h l  = 1. this means that  even though eh1 can have all the 1 terms bch1 has  it can also have  1 terms  thus  it can potentially compensate for the 1 terms. indeed  the following results show that this is exactly what happens under certain independence assumptions.
¡¡in the worst case  an example can be built in which the  1 terms do not appear with nonzero coefficients  but the 1 terms do. in this case the performance of eh1 is equivalent to the performance of bch1. these are pathological cases  though. an average analysis would me more useful to predict the performance of eh1. to obtain such an average analysis  consider first the proposition:
¡¡proposition 1. for i j k taking all the possible values over the domain i = {1 ... 1n   1}  n   1  the function g i j k  = h i  ¨’ h j  ¨’ h k  ¨’ h i ¨’ j ¨’ k  takes the value 1 zn times and the value 1 yn times  where zn and yn are given by the following recursive equations:
	z1 = 1 	y1 = 1
zn = 1 ¡¤ zn 1 + 1 ¡¤ yn 1 yn = 1 ¡¤ zn 1 + 1 ¡¤ yn 1
¡¡proof. the base case  n = 1  can be easily verified by hand. the recursion is based on the observation that the groups of two bits from different h functions interact independently and give 1 zero values and 1 one values. when the results of two groups are xor-ed  a zero is obtained if both groups are zero or both are one; a one is obtained if a group equals zero and the other group equals one. 
we have to characterize the behavior of function g i j k  for i 1= j 1= k  when at least two of these variables are equal  we obtain the special case of the variance for bch1 that we have already considered . the number of times at least two out of the three variables are equal is eqn = 1¡¤ 1n 1  1¡¤1n  which allows us to determine the desired quantities  i.e.  the number of zeros is zn   eqn  while the number of ones is yn. to determine the average behavior of eh1  we assume that neither the frequencies in r are correlated with the frequencies in s nor the frequencies in s are correlated amongst themselves. this allows us to model the quantity l = i ¨’ j ¨’ k as a uniformly distributed random variable l over the domain i. moreover  due to the same independence assumptions  function g i j k  can be modeled as a random variable g  independent of l  that has the same macro behavior as g i j k   i.e.  it takes the values 1 and 1 the same number of times. with these random variables  we get:

and

the expected value of the additional terms that appear in the variance of eh1 is given by:
e   var eh1   =
overall  the variance of the eh1 generating scheme is:
var x eh1 
+ var x bch1
 1 
notice that the additional term over the variance for bch1 is inversely proportional with the size of the domain i. also  the last factor in the additional term takes small sub-unitary values. the combined effect of these two is to drastically decrease the influence of the extra term on the eh1 variance  making it close to the bch1 variance. actually  there exist situations for which the eh1 variance is significantly smaller than the bch1 variance. it can even become equal to zero. the following proposition states this surprising result:
¡¡proposition 1. if ri = r and si = s  i ¡Ê i  with r and s constants  and |i| = 1n  then:
var x eh1 = 1
the reason the variance of eh1 is zero when the distribution of both r and s is uniform is the fact that the  1 terms cancel out entirely the 1 terms. for less extreme cases  when the distribution of the two relations is close to a uniform distribution  eh1 significantly outperforms bch1. this intuition is confirmed by the experimental results in section 1.
¡¡given the theoretical results in this section  the experimental results in the following section  and the fact that eh1 is fast range-summable and can be implemented more efficiently than bch1  we recommend the exclusive use of the eh1 random variables for estimations using ams-sketches.
1. empirical evaluation
¡¡the purpose of the empirical evaluation is threefold. first  we want to validate the theoretical models in section 1  especially the average behavior of eh1. second  we want to compare the eh1 and bch1 generating schemes for estimations using ams-sketches. third  we want to compare eh1 with dmap   see section 1  on two of the applications introduced in section 1  namely  spatial joins and selectivity estimation for histograms construction. the reason we do not provide a comparison for the computation of the l1-difference is the fact that dmap cannot perform estimations when both relations are specified as sets of intervals. furthermore  the comparison with bch1 is omitted since its error is significantly higher when compared to eh1 or dmap. we do not perform comparisons with the fast range-summable version of the reed-muller scheme  rm1  since its throughput is not higher than 1 sketch computations per second  eh1 is capable of performing more than 1 sketch computations per second as shown in section 1 .
¡¡the main findings of our experimental study can be summarized as follows:
  validation of the theoretical model for the eh1 generating scheme. our study shows that the behavior of the eh1 generating scheme is well predicted by the theoretical model in section 1.
  eh1 vs bch1. eh1 has approximately the same error  or  in the case of low skew distributions  a significantly better error than the bch1 scheme. this justifies our recommendation to always use eh1 instead of bch1.
  eh1 vs dmap. our study shows that both for real and synthetic data applications  eh1 significantly outperforms dmap  sometimes by as much as a factor of 1  with the same memory usage and smaller running time.
we performed the experiments using the setup in section 1. we give detailed descriptions of both the datasets and the comparison methodology used for each group of experiments.
1 validation of the eh1 model
¡¡to validate the average error formula in equation 1  we generated zipf distributed data with the zipf coefficient ranging from 1 to 1 over domains of various sizes in order to estimate the self-join size. the prediction is performed using ams-sketches with only one median  i.e.  only averaging is used to decrease the error of the estimate. in figure 1 we depict the comparison between the average error of the eh1 scheme and the theoretical prediction given by equation 1 for a domain with the size 1 and a relation containing 1 tuples. notice that  when the value of the zipf coefficient is larger than 1  the prediction is accurate. when the zipf coefficient is between 1 and 1  the error of eh1 is much smaller  it is zero for a uniform distribution . this is explained in proposition 1  which states that the variance of eh1 is zero when the distribution of the data is uniform and the size of the domain is a power of 1.
1 eh1 vs bch1
¡¡we performed the same experiments as in the previous section both for eh1 and bch1  except that the number of medians was fixed to 1. the results of the experiments are depicted in figure 1. note that the error for eh1 and bch1 is virtually the same for zipf coefficients greater than 1  but for eh1 it is significantly smaller for zipf coefficients lower than 1. when compared to the results in figure 1  the errors are smaller by a factor of 1. this is due to the fact that 1 medians were used instead of only 1 and the medians have almost the same effect in reducing the error as the averages - the same observation was made in .
1 eh1 vs dmap for spatial joins
¡¡we used the same experimental setup as in  to compare eh1 and dmap for approximating the size of spatial joins. three datasets are used  lando  describing land cover ownership for the state of wyoming and containing 1 objects; landc  describing land cover information such as vegetation types for the state of wyoming and containing 1 objects; and soil  representing the wyoming state soils at a 1 : 1 scale and containing 1 objects. the average error for estimating the size of spatial joins for both
eh1 and dmap is depicted in figure 1  1  and 1. the sketch size varies between 1 and 1 k words of memory. notice that in all the experiments eh1 significantly outperforms dmap by as much as a factor of 1. this means that dmap needs as much as 1 times more memory in order to achieve the same error guarantees.
1 eh1 vs dmap for selectivity estimation
¡¡to compare eh1 and dmap on the task of selectivity estimation  we used the synthetic data generator from . it generates multi-dimensional data distributions consisting in regions  randomly placed in the two-dimensional space  with the number of points in each region zipf distributed and the distribution within each region zipf distributed as well. for the experiments we report here  we generated twodimensional datasets with the domain for each dimension having the size 1. a dataset consists of 1 regions of points. the distribution of the frequencies within each region has a variable zipf coefficient  as shown in figure 1. notice that for small zipf coefficients eh1 outperforms dmap by a factor of 1. when the zipf coefficient becomes larger  the gap between dmap and eh1 shrinks considerably  but eh1 still outperforms dmap by a large margin.
1. conclusions
¡¡in this paper we conducted both a theoretical as well as an empirical study of the various schemes used for the generation of the random variables that appear in the amssketches based estimations. our primary focus was the identification of the fast range-summable schemes that can sketch intervals in sub-linear time. we explain how the fast range-summable versions of two of the 1-wise independent schemes  bch1 and eh1   can be implemented efficiently and we provide an empirical comparison with the only known 1-wise independent fast range-summable scheme  rm1   that reveals that only bch1 and eh1 are practical. moreover  we provide theoretical and empirical evidence that eh1 can replace the 1-wise independent schemes for the estimation of the size of join using ams-sketches. the eh1based solutions significantly outperform the state-of-the-art algorithms for applications such as the size of spatial joins and the dynamic construction of histograms.
¡¡the main recommendation of this paper is to always use the eh1 random variables for ams-sketches estimations of the size of join since they can be generated more efficiently and use a smaller seeds than any of the 1-wise independent schemes. at the same time  the error of the estimate is as good or  in the case when the distribution has low skew  better than the error provided by a 1-wise independent scheme.
