many variants of language models have been proposed for information retrieval. most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. in this paper  we propose and study a new family of query generation models based on poisson distribution. we show that while in their simplest forms  the new family of models and the existing multinomial models are equivalent  they behave differently for many smoothing methods. we show that the poisson model has several advantages over the multinomial model  including naturally accommodating per-term smoothing and allowing for more accurate background modeling. we present several variants of the new model corresponding to different smoothing methods  and evaluate them on four representative trec test collections. the results show that while their basic models perform comparably  the poisson model can outperform multinomial model with per-term smoothing. the performance can be further improved with two-stage smoothing.
categories and subject descriptors: h.1  information search and retrieval : retrieval models
general terms: algorithms
keywords: language models  poisson process  query generation  formal models  term dependent smoothing
1. introduction
모as a new type of probabilistic retrieval models  language models have been shown to be effective for many retrieval tasks  1  1  1  1 . among many variants of language models proposed  the most popular and fundamental one is the query-generation language model  1  1   which leads to the query-likelihood scoring method for ranking documents. in such a model  given a query q and a document d  we compute the likelihood of  generating  query q with a model estimated based on document d  i.e.  the conditional prob-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
ability p q|d . we can then rank documents based on the likelihood of generating the query.
모virtually all the existing query generation language models are based on either multinomial distribution  1  1  1  or multivariate bernoulli distribution  1  1 . the multinomial distribution is especially popular and also shown to be quite effective. the heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition  where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text. compared with multivariate bernoulli  multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast  multivariate bernoulli only models the presence and absence of query terms  thus cannot capture different frequencies of query terms. however  multivariate bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution  the probabilities of all the terms must sum to 1  making it hard to accommodate per-term smoothing  while in a multivariate bernoulli  the presence probabilities of different terms are completely independent of each other  easily accommodating per-term smoothing and weighting. note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.
모in this paper  we propose and study a new family of query generation models based on the poisson distribution. in this new family of models  we model the frequency of each term independently with a poisson distribution. to score a document  we would first estimate a multivariate poisson model based on the document  and then score it based on the likelihood of the query given by the estimated poisson model. in some sense  the poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate bernoulli in accommodating per-term smoothing. indeed  similar to the multinomial distribution  the poisson distribution models term frequencies  but without the constraint that all the term probabilities must sum to 1  and similar to multivariate bernoulli  it models each term independently  thus can easily accommodate per-term smoothing.
모as in the existing work on multinomial language models  smoothing is critical for this new family of models. we derive several smoothing methods for poisson model in parallel to those used for multinomial distributions  and compare the corresponding retrieval models with those based on multinomial distributions. we find that while with some smoothing methods  the new model and the multinomial model lead to exactly the same formula  with some other smoothing methods they diverge  and the poisson model brings in more flexibility for smoothing. in particular  a key difference is that the poisson model can naturally accommodate perterm smoothing  which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model. we exploit this potential advantage to develop a new term-dependent smoothing algorithm for poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either poisson or multinomial model. this advantage is seen for both one-stage and two-stage smoothing. another potential advantage of the poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula. this new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.
모the rest of the paper is organized as follows. in section 1  we introduce the new family of query generation models with poisson distribution  and present various smoothing methods which lead to different retrieval functions. in section 1  we analytically compare the poisson language model with the multinomial language model  from the perspective of retrieval. we then design empirical experiments to compare the two families of language models in section 1. we discuss the related work in 1 and conclude in 1.
1. query generation with poisson process
모in the query generation framework  a basic assumption is that a query is generated with a model estimated based on a document. in most existing work  1  1  1  1   people assume that each query word is sampled independently from a multinomial distribution. alternatively  we assume that a query is generated by sampling the frequency of words from a series of independent poisson processes .
1 the generation process
모let v = {w1 ... wn} be a vocabulary set. let w be a piece of text composed by an author and hc w1  ... c wn i be a frequency vector representing w  where c wi w  is the frequency count of term wi in text w. in retrieval  w could be either a query or a document. we consider the frequency counts of the n unique terms in w as n different types of events  sampled from n independent homogeneous poisson processes  respectively.
모suppose t is the time period during which the author composed the text. with a homogeneous poisson process  the frequency count of each event  i.e.  the number of occurrences of wi  follows a poisson distribution with associated parameter 뷂it  where 뷂i is a rate parameter characterizing the expected number of wi in a unit time. the probability density function of such a poisson distribution is given by

without losing generality  we set t to the length of the text w  people write one word in a unit time   i.e.  t = |w|.
모with n such independent poisson processes  each explaining the generation of one term in the vocabulary  the likelihood of w to be generated from such poisson processes can be written as

where 붦 = {뷂1 ... 뷂n} and  . we refer to these n independent poisson processes with parameter 붦 as a poisson language model.
모let d = {d1 ... dm} be an observed set of document samples generated from the poisson process above. the maximum likelihood estimate  mle  of 뷂i is

note that this mle is different from the mle for the poisson distribution without considering the document lengths  which appears in  1  1 .
모given a document d  we may estimate a poisson language model 붦d using d as a sample. the likelihood that a query q is generated from the document language model 붦d can be written as
y
	p q|d  =	p c w q |붦d 	 1 
w뫍v
this representation is clearly different from the multinomial query generation model as  1  the likelihood includes all the terms in the vocabulary v   instead of only those appearing in q  and  1  instead of the appearance of terms  the event space of this model is the frequencies of each term.
모in practice  we have the flexibility to choose the vocabulary v . in one extreme  we can use the vocabulary of the whole collection. however  this may bring in noise and considerable computational cost. in the other extreme  we may focus on the terms in the query and ignore other terms  but some useful information may be lost by ignoring the nonquery terms. as a compromise  we may conflate all the non-query terms as one single pseudo term. in other words  we may assume that there is exactly one  non-query term  in the vocabulary for each query. in our experiments  we adopt this  pseudo non-query term  strategy.
모a document can be scored with the likelihood in equation 1. however  if a query term is unseen in the document  the mle of the poisson distribution would assign zero probability to the term  causing the probability of the query to be zero. as in existing language modeling approaches  the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p 몫|d .
1 smoothing in poisson retrieval model
모in general  we want to assign non-zero rates for the query terms that are not seen in document d. many smoothing methods have been proposed for multinomial language models 1  1  1 . in general  we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words. in poisson language models  however  we do not have the same constraintp as in a multinomial model  i.e.  w뫍v p w|d  = 1 . thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word. instead  wep only need to guarantee that k=1 1 ... p c w d  = k|d  = 1. in this section  we introduce three different strategies to smooth a poisson language model  and show how they lead to different retrieval functions.
1.1 bayesian smoothing using gamma prior
모following the risk minimization framework in   we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model  which essentially consists of a vector of poisson rates for each term  i.e.  붦d = h뷂d 1 ... 뷂d |v |i.
모a document is assumed to be generated from a potentially different model. given a particular document d  we want to estimate 붦d. the rate of a term is estimated independently of other terms. we use bayesian estimation with the following gamma prior  which has two parameters  붸 and 붹:

모for each term w  the parameters 붸w and 붹w are chosen to be 붸w = 뷃   뷂c w and 붹w = 뷃  where 뷃 is a parameter and 뷂c w is the rate of w estimated from some background language model  usually the  collection language model . the posterior distribution of 붦d is given by
모모모모모모y  뷂w |d|+뷃 뷂cw w d +뷃뷂c w 1 p 붦d|d c  뫚	e
모모모모모모모모모모모모모모w뫍v which is a product of |v | gamma distributions with parameters c w d  + 뷃뷂c w and|d| + 뷃 for each word w. given that the gamma mean is   we have
뷂 d w =
모this is precisely the smoothed estimate of multinomial language model with dirichlet prior .
1.1 interpolation  jelinek-mercer  smoothing
모another straightforward method is to decompose the query generation model as a mixture of two component models. one is the document language model estimated with maximum likelihood estimator  and the other is a model estimated from the collection background  p 몫|c   which assigns non-zero rate to w.
모for example  we may use an interpolation coefficient between 1 and 1  i.e.  붻 뫍  1  . with this simple interpolation  we can score a document with
x
score d q  =	log  1   붻 p c w q |d  + 붻p c w q |c  	 1  w뫍v
using the maximum likelihood estimator for p 몫|d   we
have  thus equation 1 becomes

 we can also use a poisson language model for p 몫|c   or use some other frequency-based models. in the retrieval formula above  the first summation can be computed efficiently. the second summation can be actually treated as a document prior  which penalizes long documents.
모as the second summation is difficult to compute efficiently  we conflate all non-query terms as one pseudo  non-queryterm   denoted as  n . using the pseudo-term formulation and a poisson collection model  we can rewrite the retrieval formula as

		 1 
p
모모모모모모모모모모|d| 	w뫍q c w d  where 뷂d n =	|d|	and	.
1.1 two-stage smoothing
모as discussed in   smoothing plays two roles in retrieval:  1  to improve the estimation of the document language model  and  1  to explain the common terms in the query. in order to distinguish the content and non-discriminative words in a query  we follow  and assume that a query is generated by sampling from a two-component mixture of poisson language models  with one component being the document model 붦d and the other being a query background language model p 몫|u . p 몫|u  models the  typical  term frequencies in the user's queries. we may then score each document with the query likelihood computed using the following two-stage smoothing model:
 p c w q |붦d u  =  1   붻 p c w q |붦d  + 붻p c w q |u   1  where 붻 is a parameter  roughly indicating the amount of  noise  in q. this looks similar to the interpolation smoothing  except that p 몫|붦d  now should be a smoothed language model  instead of the one estimated with mle.
모with no prior knowledge on p 몫|u   we could set it to p 몫|c . any smoothing methods for the document language model can be used to estimate p 몫|d  such as the gamma smoothing as discussed in section 1.1.
모the empirical study of the smoothing methods is presented in section 1.
1. analysis of poisson language model
모from the previous section  we notice that the poisson language model has a strong connection to the multinomial language model. this is expected since they both belong to the exponential family . however  there are many differences when these two families of models are applied with different smoothing methods. from the perspective of retrieval  will these two language models perform equivalently  if not  which model provides more benefits to retrieval  or provides flexibility which could lead to potential benefits  in this section  we analytically discuss the retrieval features of the poisson language models  by comparing their behavior with that of the multinomial language models.
1 the equivalence of basic models
모let us begin with the assumption that all the query terms appear in every document. under this assumption  no smoothing is needed. a document can be scored by the log likelihood of the query with the maximum likelihood estimate:
		 1 
using the mle  we have. thus

this is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate. indeed  even with gamma smoothing  when
plugging and 뷂c w = c |w cc|   into equation 1  it is easy to show that

which is exactly the dirichlet retrieval formula in . note that this equivalence holds only when the document length variation is modeled with poisson process.
모this derivation indicates the equivalence of the basic poisson and multinomial language models for retrieval. with other smoothing strategies  however  the two models would be different. nevertheless  with this equivalence in basic models  we could expect that the poisson language model performs comparably to the multinomial language model in retrieval  if only simple smoothing is explored. based on this equivalence analysis  one may ask  why we should pursue the poisson language model. in the following sections  we show that despite the equivalence in their basic models  the poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features  which could not be achieved with multinomial language models.
1 term dependent smoothing
모one flexibility of the poisson language model is that it provides a natural framework to accommodate term dependent  per-term  smoothing. existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are.  also predicted that different terms should have a different smoothing weights. with multinomial query generation models  people usually use a single smoothing coefficient to control the combination of the document model and the background model  1  1 . this parameter can be made specific for different queries  but always has to be a constant for all the terms. this is mandatory since a multinomial language model has thep constraint that w뫍v p w|d  = 1. however  from retrieval perspective  different terms may need to be smoothed differently even if they are in the same query. for example  a non-discriminative term  e.g.   the    is   is expected to be explained more with the background model  while a content term  e.g.   retrieval    bush   in the query should be explained with the document model. therefore  a better way of smoothing would be to set the interpolation coefficient  i.e.  붻 in formula 1 and formula 1  specifically for each term. since the poisson language model does not have the  sum-to-one  constraint across terms  it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models. below we present a possible way to explore term dependent smoothing with poisson language models.
모essentially  we want to use a term-specific smoothing coefficient 붻 in the linear combination  denoted as 붻w. this coefficient should intuitively be larger if w is a common word and smaller if it is a content word. the key problem is to find a method to assign reasonable values to 붻w. empirical tuning is infeasible for so many parameters. we may instead estimate the parameters    = {붻1 ... 붻|v |}  by maximizing the likelihood of the query given the mixture model of p q|붦q  and p q|u   where 붦q is the  true  query model to generate the query and p q|u  is a query background model as discussed in section 1.1.
with the model p q|붦q  hidden  the query likelihood is
p q|  u  =
z y
  1   붻w p c w q |붦q  + 붻wp c w q |u  p 붦q|u d붦q
붦q w뫍v
if we have relevant documents for each query  we can approximate the query model space with the language models of all the relevant documents. without relevant documents  we opt to approximate the query model space with the models of all the documents in the collection. setting p 몫|u  as
p 몫|c   the query likelihood becomes
	x	y
p q|  u  =	뷇d	  1 붻w p c w q |붦 d +붻wp c w q |c  
	d뫍c	w뫍v
where 뷇d = p 붦 d|u . p 몫|붦 d  is an estimated poisson language model for document d.
모if we have prior knowledge on p 붦 d|u   such as which documents are relevant to the query  we can set 뷇d accordingly  because what we want is to find   that can maximize the likelihood of the query given relevant documents. without this prior knowledge  we can leave 뷇d as free parameters  and use the em algorithm to estimate 뷇d and  . the updating functions are given as

and

모as discussed in   we only need to run the em algorithm for several iterations  thus the computational cost is relatively low. we again assume our vocabulary containing all query terms plus a pseudo non-query term. note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term. in our experiments  we set it to the average over 붻w of all query terms.
모with this flexibility  we expect poisson language models could improve the retrieval performance  especially for verbose queries  where the query terms have various discriminative values. in section 1  we use empirical experiments to prove this hypothesis.
1 mixture background models
모another flexibility is to explore different background  collection  models  i.e.  p 몫|u   or p 몫|c  . one common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models  1  1 . similarly  we can also make the assumption that the collection model is a poisson language model  with the rates. however  this assumption usually does not hold  since the collection is far more complex than a single document. indeed  the collection usually consists of a mixture of documents with various genres  authors  and topics  etc. treating the collection model as a mixture of document models  instead of a single  pseudo-document model  is more reasonable. existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance  such as clusters  1  1   neighbor documents   and aspects  1  1 . all the approaches can be easily adopted using poisson language models. however  a common problem of these approaches is that they all require heavy computation to construct the background model. with poisson language modeling  we show that it is possible to model the mixture background without paying for the heavy computational cost.
모poisson mixture  has been proposed to model a collection of documents  which can fit the data much better than a single poisson. the basic idea is to assume that the collection is generated from a mixture of poisson models  which
has the general form of
               z p x = k|pm  =	p 뷂 p x = k|뷂 d뷂
뷂
p 몫|뷂  is a single poisson model and p 뷂  is an arbitrary probability density function. there are three well known poisson mixtures : 1-poisson  negative binomial  and the katz's k-mixture . note that the 1-poisson model has actually been explored in probabilistic retrieval models  which led to the well-known bm1 formula .
모all these mixtures have closed forms  and can be estimated from the collection of documents efficiently. this is an advantage over the multinomial mixture models  such as plsi  and lda   for retrieval. for example  the probability density function of katz's k-mixture is given as

where 붾k 1 = 1 when k = 1  and 1 otherwise.
모with the observation of a collection of documents  붸w and 붹w can be estimated as

where cf w  and df w  are the collection frequency and document frequency of w  and n is the number of documents in the collection. to account for the different document lengths  we assume that 붹w is a reasonable estimation for generating a document of the average length  and use 붹1 = avdl붹w |q| to generate the query. this poisson mixture model can be easily used to replace p 몫|c  in the retrieval functions 1 and 1.
1 other possible flexibilities
모in addition to term dependent smoothing and efficient mixture background  a poisson language model has also some other potential advantages. for example  in section 1  we see that formula 1 introduces a component which does document length penalization. intuitively  when the document has more unique words  it will be penalized more. on the other hand  if a document is exactly n copies of another document  it would not get over penalized. this feature is desirable and not achieved with the dirichlet model . potentially  this component could penalize a document according to what types of terms it contains. with term specific settings of 붻  we could get even more flexibility for document length normalization.
모pseudo-feedback is yet another interesting direction where the poission model might be able to show its advantage. with model-based feedback  we could again relax the combination coefficients of the feedback model and the background model  and allow different terms to contribute differently to the feedback model. we could also utilize the  relevant  documents to learn better per-term smoothing coefficients.
1. evaluation
모in section 1  we analytically compared the poisson language models and multinomial language models from the perspective of query generation and retrieval. in this section  we compare these two families of models empirically. experiment results show that the poisson model with perterm smoothing outperforms multinomial model  and the performance can be further improved with two-stage smoothing. using poisson mixture as background model also improves the retrieval performance.
1 datasets
모since retrieval performance could significantly vary from one test collection to another  and from one query to another  we select four representative trec test collections: ap  trec1  trec1  and wt1g web . to cover different types of queries  we follow  1  1   and construct short-keyword  sk  keyword title   short-verbose  sv  one sentence description   and long-verbose  lv  multiple sentences  queries. the documents are stemmed with the porter's stemmer  and we do not remove any stop word. for each parameter  we vary its value to cover a reasonably wide range.
1 comparison to multinomial
모we compare the performance of the poisson retrieval models and multinomial retrieval models using interpolation  jelinekmercer  jm  smoothing and bayesian smoothing with conjugate priors. table 1 shows that the two jm-smoothed models perform similarly on all data sets. since the dirichlet smoothing for multinomial language model and the gamma smoothing for poisson language model lead to the same retrieval formula  the performance of these two models are jointly presented. we see that dirichlet/gamma smoothing methods outperform both jelinek-mercer smoothing methods. the parameter sensitivity curves for two jelinek-mercer

figure 1: poisson and multinomial performs similarly with jelinek-mercer smoothing smoothing methods are shown in figure 1. clearly  these two methods perform similarly either in terms of optimality
dataqueryjm-multinomialjm-poissondirichlet/gammaper-term 1-stage poissonmapinitprpr 1dmapinitprpr 1dmapinitprpr 1dmapinitprpr 1dap1sk111111111111sv1111111111*11lv1111111111*11trec1sk111111111111sv1111111111*11lv1111111111*11trec1sk111111111111sv1111111111*11lv1111111111*11websk111111111111sv1111111111*11lv1111111111*11table 1: performance comparison between poisson and multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves poisson
an asterisk  *  indicates that the difference between the performance of the term dependent two-stage smoothing and that of the dirichlet/gamma single smoothing is statistically significant according to the wilcoxon signed rank test at the level of 1.or sensitivity. this similarity of performance is expected as we discussed in section 1.
모although the poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods  the poisson model has great potential and flexibility to be further improved. as shown in the rightmost column of table 1  term dependent two-stage poisson model consistently outperforms the basic smoothing models  especially for verbose queries. this model is given in formula 1  with a gamma smoothing for the document model p 몫|d   and 붻w  which is term dependent. the parameter 뷃 of the first stage gamma smoothing is empirically tuned. the combination coefficients  i.e.      are estimated with the em algorithm in section 1. the parameter sensitivity curves for dirichlet/gamma and the per-term two-stage smoothing model are plotted in figure 1. the per-term two-stage smoothing method is less sensitive to the parameter 뷃 than dirichlet/gamma  and yields better optimal performance.

figure 1: term dependent two-stage smoothing of poisson outperforms dirichlet/gamma
모in the following subsections  we conduct experiments to demonstrate how the flexibility of the poisson model could be utilized to achieve better performance  which we cannot achieve with multinomial language models.
1 term dependent smoothing
모to test the effectiveness of the term dependent smoothing  we conduct the following two experiments. in the first experiment  we relax the constant coefficient in the simple jelinek-mercer smoothing formula  i.e.  formula 1   and use the em algorithm proposed in section 1 to find a 붻w for each unique term. since we are using the em algorithm to iteratively estimate the parameters  we usually do not want the probability of p 몫|d  to be zero. we then use a simple laplace method to slightly smooth the document model before it goes into the em iterations. the documents are then still scored with formula 1  but using learnt 붻w. the results are labeled with  jm+l.  in table 1.
dataqjmjmjm+l.1-stage1-stage map pt:noyesyesnoyesapsk11111*sv111*11*trec1sk11111sv111*11trec1sk111*11sv111*11*websk111*11*sv111*11*table 1: term dependent smoothing improves retrieval performance
an asterisk  *  in column 1 indicates that the difference between the  jm+l.  method and jm method is statistically significant; an asterisk  *  in column 1 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; pt stands for  per-term .
모with term dependent coefficients  the performance of the jelinek-mercer poisson model is improved in most cases. however  in some cases  e.g.  trec1/sv   it performs poorly. this might be caused by the problem of em estimation with unsmoothed document models. once non-zero probability is assigned to all the terms before entering the em iteration  the performance on verbose queries can be improved significantly. this indicates that there is still room to find better methods to estimate 붻w. please note that neither the perterm jm method nor the  jm+l.  method has a parameter to tune.
모as shown in table 1  the term dependent two-stage smoothing can significantly improve retrieval performance. to understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework  we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in . their method managed to find coefficients specific to the query  thus a verbose query would use a higher 붻. however  since their model is based on multinomial language modeling  they could not get per-term coefficients. we adopt their method to the poisson two-stage smoothing  and also estimate a per-query coefficient for all the terms. we compare the performance of such a model with the per-term two-stage smoothing model  and present the results in the right two columns in table 1. again  we see that the  per-term  two-stage smoothing outperforms the  per-query  two-stage smoothing  especially for verbose queries. the improvement is not as large as how the perterm smoothing method improves over dirichlet/gamma. this is expected  since the per-query smoothing has already addressed the query discrimination problem to some extent. this experiment shows that even if the smoothing is already per-query  making it per-term is still beneficial. in brief  the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method.
1 mixture background model
모in this section  we conduct experiments to examine the benefits of using a mixture background model without extra computational cost  which can not be achieved for multinomial models. specifically  in retrieval formula 1  instead of using a single poisson distribution to model the background p 몫|c   we use katz's k-mixture model  which is essentially a mixture of poisson distributions. p 몫|c  can be computed efficiently with simple collection statistics  as discussed in section 1.
dataqueryjm. poissonjm. k-mixtureapsk11sv11*trec-1sk11sv11*trec-1sk11sv11*websk11sv11*table 1: k-mixture background model improves retrieval performance
모the performance of the jm retrieval model with single poisson background and with katz's k-mixture background model is compared in table 1. clearly  using k-mixture to model the background model outperforms the single poisson background model in most cases  especially for verbose queries where the improvement is statistically significant.
모figure 1 shows that the performance changes over different parameters for short verbose queries. the model using k-mixture background is less sensitive than the one using single poisson background. given that this type of mixture

figure 1: k-mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost  it would be interesting to study whether using other mixture poisson models  such as 1-poisson and negative binomial  could help the performance.
1. related work
모to the best of our knowledge  there has been no study of query generation models based on poisson distribution.
모language models have been shown to be effective for many retrieval tasks  1  1  1  1 . the most popular and fundamental one is the query-generation language model  1  1 . all existing query generation language models are based on either multinomial distribution  1  1  1  1  or multivariate bernoulli distribution  1  1  1 . we introduce a new family of language models  based on poisson distribution. poisson distribution has been previously studied in the document generation models  1  1  1  1   leading to the development of one of the most effective retrieval formula bm1 .  studies the parallel derivation of three different retrieval models which is related to our comparison of poisson and multinomial. however  the poisson model in their paper is still under the document generation framework  and also does not account for the document length variation.  introduces a way to empirically search for an exponential model for the documents. poisson mixtures  such as 1-poisson   negative multinomial  and katz's kmixture  has shown to be effective to model and retrieve documents. once again  none of this work explores poisson distribution in the query generation framework.
모language model smoothing  1  1  1  and background structures  1  1  1  1  have been studied with multinomial language models.  analytically shows that term specific smoothing could be useful. we show that poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model  and is able to efficiently better model the mixture background  both analytically and empirically.
1. conclusions
모we present a new family of query generation language models for retrieval based on poisson distribution. we derive several smoothing methods for this family of models  including single-stage smoothing and two-stage smoothing. we compare the new models with the popular multinomial retrieval models both analytically and experimentally. our analysis shows that while our new models and multinomial models are equivalent under some assumptions  they are generally different with some important differences. in particular  we show that poisson has an advantage over multinomial in naturally accommodating per-term smoothing. we exploit this property to develop a new per-term smoothing algorithm for poisson language models  which is shown to outperform term-independent smoothing for both poisson and multinomial models. furthermore  we show that a mixture background model for poisson can be used to improve the performance and robustness over the standard poisson background model. our work opens up many interesting directions for further exploration in this new family of models. further exploring the flexibilities over multinomial language models  such as length normalization and pseudo-feedback could be good future work. it is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost.
1. acknowledgments
모we thank the anonymous sigir 1 reviewers for their useful comments. this material is based in part upon work supported by the national science foundation under award numbers iis-1 and 1.
