prefetch can be used to reduce the query latency and improve the bandwidth utilization of cache invalidation schemes. however  prefetch consumes power. in this paper  we propose a power-aware cache management to address this issue. based on a novel prefetch-access ratio concept  the proposed scheme can dynamically optimize performance or power based on the available resources and performance requirements. simulation results show that our solution not only improves the cache hit ratio  the throughput  and the bandwidth utilization  but also reduces the query delay and the power consumption.
keywords
power-aware  invalidation report  caching  mobile computing.
1. introduction
　caching frequently accessed data items on the client side is an effective technique to improve performance in a mobile environment. classical cache management strategies may not be suitable for mobile environments due to the disconnection and mobility of the mobile clients. barbara and imielinski  proposed a cache solution which is suitable for mobile environments. in this approach  the server periodically broadcasts an invalidation report  ir  in which the changed data items are indicated. rather than querying the server directly regarding the validation of cached copies  the clients can listen to these irs over the wireless channel  and use them to validate their local cache. the ir-based solution is attractive because it can scale to any number of clients who listen to the ir. however  the ir-based solution has some major drawbacks such as long query latency and low bandwidth utilization. in our previous work   we addressed the first problem with a uir-based approach. in this approach  a small fraction of the essential information  called updated invalidation report  uir   related to cache invalidation is replicated several times within an ir interval  and hence the client can answer a query without waiting until the next ir. however  if there is a cache miss  the client still needs to wait for the data to be delivered. to increase the cache hit ratio and reduce the bandwidth consumption  clients intelligently prefetch the data that are most likely used in the future. however  prefetch consumes power. in this paper  we propose a power-aware cache management to address this issue. based on a novel prefetch-access ratio concept  the proposed scheme can dynamically optimize performance or power based on the available resources and performance requirements. compared to previous schemes  our solution not only improves the cache hit ratio  the throughput  and the bandwidth utilization  but also reduces the query delay and the power consumption.
1. power-awarecachemanagement
1 efficiently utilize the bandwidth
　to improve the cache hit ratio  clients prefetch data that may be used in the near future. to save power  clients may only wake up during the ir broadcasting period  and then how to prefetch data becomes an issue. as a solution  after broadcasting the ir  the server first broadcasts the list of the data items whose data values will be broadcast next  and then broadcasts the data values of the data items in the list. each client should listen to the ir if it is not disconnected. at the end of the ir  a client downloads the list and finds out when the interested data will come and wakes up at that time to download the data. with this approach  power can be saved since clients stay in the doze mode most of the time; bandwidth can be saved since the server may only need to broadcast the updated data once. since prefetching also consumes power  it is very important to identify which data should be included in the list. since the server does not maintain any information about the clients  it is very difficult  if not impossible  for the server to identify which data is hot. to save broadcast bandwidth  the server does not answer the client requests immediately; instead  it waits for the next ir interval. after broadcasting the ir  the server broadcasts the list of the data items that have been requested during the last ir interval. in addition  the server broadcasts the values of the data items in the list.
1 an adaptive prefetch approach
　since prefetching also consumes power  we investigate the tradeoff between performance and power  and propose an adaptive scheme to efficiently utilize the power.

figure 1: the effects of　each client may have different available resources and performance requirements  and these resources such as power may change with time. for example  suppose the battery of a laptop lasts three hours. if the user is able to recharge the battery within three hours  power consumption may not be an issue  and the user may be more concerned about the performance aspects such as the query latency. however  if the user cannot recharge the battery within three hours and wants to use it a little bit longer  then power consumption becomes a serious concern. to address this issue  the system monitors the power level. when the power level drops below a threshold  power consumption becomes the primary concern. if query latency is more important than power consumption  the client should always prefetch the interested data. however  when the power drops to a threshold  the client should be cautious about prefetching.
　there are two solutions to reduce the power consumption. as a simple solution  the client can reduce its cache size. with a smaller cache  the number of invalid cache entries reduces  and the number of prefetches drops. although small cache size reduces prefetch power consumption  it may also increase the cache miss ratio  thereby degrading performance. in a more elegant approach  the client marks some invalid cache entries as non-prefetch and it will not prefetch these items. intuitively  the client should mark those cache entries that need more power to prefetch  but are not accessed too often.
the adaptive prefetch approach: in order to implement the idea 

for each cached item  the client records how many times it accessed the item and how many times it prefetched the item during a period of time. the prefetch-access ratio  par  is the number of prefetches divided by the number of accesses. if the is less than 1  prefetching the data is useful since the prefetched data may be accessed multiple times. when power consumption becomes an issue  the client marks those cache items which have as non-prefetch  where is a system tuning factor. the value of can be dynamically changed based on the power consumption requirements. for example  with a small   more energy can be saved  but the cache hit ratio may be reduced. on the other hand  with a large   the cache hit ratio can be improved  but at a cost of more energy consumption. note that when choosing the value of   the uplink data request cost should also be considered.
	when the data update rate is high  the	may always be
larger than   and clients cannot prefetch any data. without prefetch  the cache hit ratio may be dramatically reduced and resulting in poor performance. since clients may have a large probability to access a very small amount of data  marking these data items as pre-fetch may improve the cache hit ratio and does not consume too much power. based on this idea  when   the client marks number of cache entries which have high access rate as
.
　since the query pattern and the data update distribution may change over time  clients should measure their access rate and periodically and refresh some of their history information. assume is the number of access times for a cache entry . assume is the number of access times for a cache entry in the current evaluation cycle. the number of access times is calculated by

　where is a factor which reduces the impact of the old access frequency with time. similar formula can be used to calculate
.
1. performance evaluation
　in order to evaluate the efficiency of various invalidation algorithms  we develop a model which is similar to that employed in  1  1 . it consists of a single server that serves multiple clients. the database can only be updated by the server whereas the queries are made on the client side. figure 1 shows the effects of on the number of prefetches and the cache hit ratio. as can be seen  the no-prefetch scheme  which does not prefetch data  has the lowest cache hit ratio  whereas the no-nonprefetch scheme has the highest cache hit ratio  but the highest number of prefetches. as changes  the number of data items to be marked as prefetch changes  and resulting in a tradeoff between cache hit ratio  delay  and the number of prefetches  power .
1. conclusions
　based on a novel prefetch-access ratio concept  we presented an adaptive power-aware cache management scheme for mobile environments. simulation results verified that our scheme can keep the advantage of prefetch with low power consumption.
