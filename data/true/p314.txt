recently  there have been a number of algorithms proposed for analyzing hypertext link structure so as to determine the best  authorities  for a given topic or query. while such analysis is usually combined with content analysis  there is a sense in which some algorithms are deemed to be  more balanced  and others  more focused . we undertake a comparative study of hypertext link analysis algorithms. guided by some experimental queries  we propose some formal criteria for evaluating and comparing link analysis algorithms.
keywords
link analysis  web searching  hubs  authorities  salsa  kleinberg's algorithm  threshold  bayesian
1. introduction
　in recent years  a number of papers  1  1  1  1  1  have considered the use of hypertext links to determine the value of different web pages. in particular  these papers consider the extent to which hypertext links between world wide web documents can be used to determine the relative authority values of these documents for various search queries.

 department of computer science  university of toronto  toronto  ontario  canada m1s 1 and gammasite  hertzilya  israel. e-mail: bor cs.toronto.edu. web: http://www.cs.utoronto.ca/dcs/people/faculty/bor.html.  department of mathematics and statistics  lancaster university  lancaster  u.k. la1yf. e-mail: g.o.roberts lancaster.ac.uk. web: http://www.maths.lancs.ac.uk/dept/people/robertsg.html.  department of statistics  university of toronto  toronto  ontario  canada m1s 1. supported in part by nserc of canada. e-mail: jeff math.toronto.edu. web: http://markov.utstat.toronto.edu/jeff/. ′department of computer science  university of toronto  toronto  ontario  canada m1s 1. e-mail:
tsap cs.toronto.edu. web:
http://www.cs.toronto.edu/゛tsap/.
copyright is held by the author/owner.
www1  may 1  1  hong kong.
acm 1-1/1.
　we consider some of the previously published algorithms as well as introducing some new alternatives. one of our new algorithms is based on a bayesian statistical approach as opposed to the more common algebraic/graph theoretic approach. while link analysis by itself cannot be expected to always provide reliable rankings  it is interesting to study various link analysis strategies in an attempt to understand inherent limitations  basic properties and  similarities  between the various methods. to this end  we offer definitions for several intuitive concepts relating to  link analysis  ranking algorithms and begin a study of these concepts.
　we also provide some new  comparative  experimental studies of the performance of the various ranking algorithms. it can be seen that no method is completely safe from  topic drift   but some methods do seem to be more resistant than others. we shall see that certain methods have surprisingly similar rankings as observed in our experimental studies  however they cannot be said to be similar with regard to our formalization.
1. previous algorithms
1 the pagerank algorithm
　one of the earliest and most commercially successful of the efforts to use hypertext link structures in web searching is the pagerank algorithm used by brin and page  in the google search engine .
　the page rank of a given web page i  denoted pr i   is defined recursively according to the equation
 
where the sum is taken over all pages j which have a link to page i  n j  is the total number of links originating from page j  d is a number between 1 and 1  and d is a probability distribution  e.g. uniform  over all web pages.
　brin and page  note that the value of pr i  is equivalent to the limiting fraction of time spent on page i by a random walk which proceeds at each step as follows: with probability d it jumps to a sample from the distribution d ，   and with probability 1   d it jumps uniformly at random to one of the pages linked from the current page. this idea is also used by rafiei and mendelzon  for computing the  reputation  of a page. intuitively  the value of pr i  is a measure of the importance or authority of the web page i. this ranking is used as one component of the google search engine  to help determine how to order the pages returned by a web search query.
1 kleinberg's algorithm
　independent of brin and page  kleinberg  proposed a more refined notion for the importance of web pages. he suggested that web page importance should depend on the search query being performed. furthermore  each page should have a separate  authority  rating  based on the links going to the page  and  hub  rating  based on the links going from the page . kleinberg proposed first using a text-based web search engine  such as altavista   to get a  root set  consisting of a short list of web pages relevant to a given query. second  the root set is augmented by pages which link to pages in the root set  and also pages which are linked to pages in the root set  to obtain a larger 'base set  of web pages. if n is the number of pages in the final base set  then the data for kleinberg's algorithm consists of an n 〜n adjacency matrix a  where aij = 1 if there are one or more hypertext links from page i to page j  otherwise
aij = 1.
　kleinberg's algorithm assigns to each page i an authority weight ai and a hub weight hi. let a =  a1 a1 ... an  denote the vector of all authority weights  and h =  h1 h1 ... hn  the vector of all hub weights. initially both authority and hub vectors are set to u=  1 ... 1 . at each iteration the operations i   in   and o   out   are performed. the operation i sets the authority vector to a = at h. the operation o sets the hub vector to h = a a. a normalization step is then applied  so that the vectors a and h become unit vectors in some norm. kleinberg proves that after a sufficient number of iterations the vectors a and h converge to the principal eigenvectors of the matrices ata and aat  respectively. the above normalization step may be performed in various ways. indeed  ratios such as ai/aj will converge to the same value no matter how  or if  normalization is performed.
　kleinberg's algorithm  and some of the other algorithms we are considering  converge naturally to their principal eigenvector  i.e. to the eigenvector of their transition matrix which corresponds to the largest eigenvalue. kleinberg  makes an interesting  though non-precise  claim that the subsequent non-principal eigenvectors  or their positive and negative components  are sometimes representative of  subcommunities  of web pages. it is easy to construct simple examples which show that subsequent eigenvectors sometimes are  but sometimes are not  indicative of sub-communities; we present a few indicative such examples in the full version of this paper. the significance of non-principal eigenvectors is an important topic that we intend to pursue further.
1 the salsa algorithm
　an alternative algorithm  salsa  was proposed by lempel and moran . like kleinberg's algorithm  salsa starts with a similarly constructed base set. it then performs a random walk by alternately  a  going uniformly to one of the pages which links to the current page  and  b  going uniformly to one of the pages linked to by the current page. the authority weights are defined to be the stationary distribution of the two-step chain doing first step  a  and then  b   while the hub weights are defined to be the stationary distribution of the two-step chain doing first step  b  and then  a .
　formally  let b i  = {k : k ★ i} denote the set of all nodes that point to i  that is  the nodes we can reach from i by following a link backwards  and let f i  = {k : i ★ k} denote the set of all nodes that we can reach from i by following a forward link. the markov chain for the authorities has transition probabilities
.
　assume for a moment that the markov chain is irreducible  that is  the underlying graph consists of a single connected component. the authors prove that the stationary distribution a =  a1 a1 ... an  of the markov chain satisfies  is the set of all  backward  links. similarly  the markov chain for the hubs has transition probabilities
 
and the stationary distribution h =  h1 h1 ... hn  satisfies   where  is the set of all
 forward  links.
　salsa does not really have the same  mutually reinforcing structure  that kleinberg's algorithm does. indeed  since ai = |b i |/|b|  the relative authority of site i within a connected component is determined from local links  not from the structure of the component.  see also the discussion of locality in section 1.  we also note that in the special case of a single component  salsa can be viewed as a onestep truncated version of kleinberg's algorithm. that is  in the first iteration of kleinberg's algorithm  if we perform the i operation first  the authority weights are set to a = at u  where u is the vector of all ones. if we normalize in the l1 norm  then  which is the stationary distribution of the salsa algorithm. a similar observation can be made for the hub weights.
　if the underlying graph of the base set consists of more than one component  then the salsa algorithm selects a starting point uniformly at random  and performs a random walk within the connected component that contains that node. formally  let j be a component that contains node i  let nj denote the number of nodes in the component  and bj the set of  backward  links in component j. then  the authority weight of node i is
 .
　led astray by the simplifying assumption of a single component  we considered a simplified version of the salsa algorithm where the authority weight of a node is the ratio |b i |/|b|. this corresponds to the case that the starting point for the random walk is chosen with probability proportional to the  popularity  of the node  that is  the number of links that point to this node. we will refer to this variation of the salsa algorithm as psalsa  popularity salsa 1. we will consider the original salsa algorithm as defined in  in the full version of our paper.
　an interesting generalization of the salsa algorithm is considered by rafiei and mendelzon . they propose an algorithm for computing reputations that is a hybrid of the salsa algorithm  and the pagerank algorithm. at each step  with probability d  the rafiei and mendelzon algorithm jumps to a page of the collection chosen uniformly at random  and with probability 1   d it performs a salsa step.
1 the phits algorithm
　cohn and chang  propose a statistical hubs and authorities algorithm  which they call the phits algorithm. they propose a probabilistic model in which a citation c of a document d is caused by a latent  factor  or  topic   z. it is postulated that there are conditional distributions p c|z  of a citation c given a factor z  and also conditional distributions p z|d  of a factor z given a document d. in terms of these conditional distributions  they produce a likelihood function.
　cohn and chang then propose using the em algorithm of dempster et al.  to assign the unknown conditional probabilities so as to maximize this likelihood function l  and thus best  explain  the proposed data. their algorithm requires specifying in advance the number of factors z to be considered. furthermore  it is possible that their em algorithm could get  stuck  in a local maximum  without converging to the true global maximum.
1. random walks and the kleinberg algorithm
　the fact that the output of the first  half  step of the kleinberg algorithm can be seen as the stationary distribution of a certain random walk on the underlying graph  poses the natural question of whether other intermediary results of kleinberg's algorithm  and ultimately the output of the algorithm itself  can also be seen as the stationary distribution of a random walk. we show that this is indeed the case.
　theorem 1. there exist sequences  and of markov chains  such that  for each n − 1  the stationary distribution of mna is equal to the authority vector after the nth iteration of kleinberg's algorithm  and the stationary distribution of mnh is equal to the hub vector after the nth iteration of kleinberg's algorithm.
　proof. we first introduce the following notation. we say that we follow a b path if we follow a link backwards  and we say we follow an f path if we follow a link forward. we can combine these to obtain longer paths. for example a bf path is a path that first follows a link backwards  and then a link forward. now  let  bf n i j  denote the set of  bf n paths that go from i to j   bf n i  the set of  bf n paths that leave node i  and  bf n the set of all possible  bf n paths. we can define similar sets for the  fb n paths.
by definition of the  ata n  and  aat n matrices  we
j
 . after the nth operation of
the kleinberg algorithm the authority vector a  and hub vector h are the unit vectors in the direction of  ata nu and  aat nu  respectively.  this actually assumes that in order to compute the authority weights we switch the order of the operations i and o  but asymptotically this does not make any difference . if we take the unit vectors under the l1 norm  then we have
	 	and	
　now  we define the undirected weighted graph g bf n as follows. the vertex set of the graph is the set of nodes in the base set. we place an edge between two nodes i and j if there is a  bf n path between these nodes. the weight of the edge is | bf n i j |  the number of  bf n paths between i and j. we perform a random walk on graph g bf n. when at node i  we move to node j with probability proportional to the number of paths between i and j. the corresponding markov chain m bf n has transition probabilities
.
from a standard theorem on random walks on weighted graphs  see  e.g.  p. 1 of  for the corresponding result on unweighted graphs   the stationary distribution of m bf n is the same as the vector a in equation  1 . similarly  we can define the graph g fb n  and the corresponding markov chain m fb n  for the hubs case. setting mna to m bf n  and mnh to m fb n concludes the proof. 
1. somemodificationstothekleinberg and salsa algorithms
　while kleinberg's algorithm has some very desirable properties  it also has its limitations. one potential problem is the possibility of severe  topic drift . roughly  kleinberg's algorithm converges to the most  tightly-knit  community within the base set. it is possible that this tightly-knit community will have little or nothing to do with the proposed query topic.
　a striking example of this phenomenon is provided by cohn and chang    p. 1 . they use kleinberg's algorithm with the search term  jaguar   an example query suggested by kleinberg    and converge to a collection of sites about the city of cincinnati! they determine that the cause of this is a large number of on-line newspaper articles in the cincinnati enquirer which discuss the jacksonville jaguars football team  and all link to the same standard cincinnati enquirer service pages. interestingly  in a preliminary experiment with the query term  abortion   another example query suggested by kleinberg    we also found the kleinberg algorithm converging to a collection of web pages about the city of cincinnati!
　now  in both these cases  we believe it is possible to eliminate such errant behavior through more careful selection of the base set  and more careful elimination of intra-domain hypertext links. nevertheless  we do feel that these examples point to a certain  instability  of kleinberg's algorithm.
1 the hub-averaging-kleinberg algorithm
　we propose here a small modification of kleinberg's algorithm to help remedy the above-mentioned instability. for motivation  consider the following. suppose there are m +1 authority pages  and m + 1 hub pages  with m large. the first m hubs link only to the first authority  while the final hub links to all m + 1 authorities. in such a set-up  we would expect the first authority to be considered much more authoritative than all the others  and kleinberg's algorithm does indeed do this. on the other hand  it seems that the final hub should be worse than the others  since in addition to linking to a good authority  the first authority   it also links to many bad authorities. however  according to kleinberg's algorithm  it is the best hub  because linking to more things can only improve your hub rating.
　inspired by such considerations  we propose an algorithm which is a  hybrid  of the kleinberg and salsa algorithms. namely  it does the authority rating updates i just like kleinberg  i.e.  giving each authority a rating equal to the sum of the hub ratings of all the pages that link to it   but does the hub rating updates o by instead giving each hub a rating equal to the average of the authority ratings of all the pages that it links to. with this modified  hub-averaging  algorithm  a hub is better if it links to only good authorities  rather than linking to both good and bad authorities.
1 the threshold-kleinberg algorithms
　we propose two different  threshold  modifications to kleinberg's algorithm. the first modification  hub-threshold  is applied to the in-step i. when computing the authority weight of the ith page  this algorithm does not take into account all hubs that point to page i. it only counts those hubs whose hub weight is at least the average hub weight over all the hubs that point to page i  computed using the current hub weights for the nodes. this corresponds to saying that a site should not be considered a good authority simply because a lot of very poor hubs point to it.
　the second modification  authority-threshold  is applied to the out-step o. when computing the hub weight of the ith page  this algorithm does not take into account all authorities pointed to by page i. it only counts those authorities which are among the top k authorities  judging by current authority values. the value of k is passed as a parameter to the algorithm. this corresponds to saying that a site should not be considered a good hub simply because it points to a number of  acceptable  authorities; rather  to be considered a good hub the site must point to some of the best authorities. this is inspired partially by the fact that  in most web searches  a user only visits the top few authorities.
　we also consider a full-threshold algorithm  which makes both the hub-threshold and authority-threshold modifications to kleinberg's algorithm.
1 the breadth-first-search algorithm: a normalized n-step variant
　when the psalsa algorithm computes the authority weight of a page  it takes into account only the popularity of this page within its immediate neighborhood  disregarding the rest of the graph. on the other hand  the kleinberg algorithm considers the whole graph  taking into account more the structure of the graph around the node  than the popularity of that node in the graph. specifically  after n steps  the authority weight of the ith authority is | bf n i |/| bf n|  where | bf n i | is the number of  bf n paths that leave node i. another way to think of this is that the contribution of a node to the weight of i is equal to the number of  bf n paths that go from i to j. therefore  if a small bipartite component intercepts the path between node j and i  the contribution of node j will increase exponentially fast. this may not always be desirable  especially if the bipartite component is not representative of the query.
　we propose the breadth-first-search  bfs  algorithm  as a generalization of the psalsa algorithm  and a restriction of the kleinberg algorithm. the bfs algorithm extends the idea of popularity that appears in psalsa from a one link neighborhood to an n-link neighborhood. however  instead of considering the number of  bf n paths that leave i  it considers the number of  bf n neighbors of node i. we let  bf n i  denote the set of nodes that can be reached from i by following a  bf n path. the contribution of node j to the weight of node i depends on the distance of the node j from i. we adopt an exponentially decreasing weighting scheme. therefore  the weight of node i is determined as follows: ai = 1n 1|b i |+1n 1|bf i |+1n 1|bfb i |+...+| bf n i |.
　the algorithm starts from node i  and visits its neighbors in bfs order. at each iteration it takes a backward or a forward step  depending on whether it is an odd  or an even iteration   and it includes the new nodes it encounters. the weight factors are updated accordingly. note that each node is considered only once  when it is first encountered by the algorithm.
1. a bayesian algorithm
　a different type of algorithm is given by a fully bayesian statistical approach to authorities and hubs. suppose there are m hubs and n authorities  which could be the same set . we suppose that each hub i has an  unknown  real parameter ei  corresponding to its  general tendency to have hypertext links   and also an  unknown  non-negative parameter hi  corresponding to its  tendency to have intelligent hypertext links to authoritative sites . we further suppose that each authority j has an  unknown  non-negative parameter aj  corresponding to its level of authority.
　our statistical model is as follows. the a priori probability of a link from hub i to authority j is given by
	p  	 1 
with the probability of no link from i to j given by
	p .	 1 
　this reflects the idea that a link is more likely if ei is large  in which case hub i has large tendency to link to any site   or if both hi and aj are large  in which case i is an intelligent hub  and j is a high-quality authority .
　to complete the specification of the statistical model from a bayesian point of view  see  e.g.  bernardo and smith    we must assign prior distributions to the 1m +n unknown parameters ei  hi  and aj.  these priors should be general and uninformative  and should not depend on the observed data. for large graphs  the choice of priors should have only a small impact on the results.  to do this  we let μ =  1 and σ = 1 be fixed parameters  and let each ei have prior distribution n μ σ1   a normal distribution with mean μ and variance σ1. we further let each hi and aj have prior distribution exp 1   since they have to be non-negative   meaning that for x − 1  p hi − x  = p aj − x  = exp  x . the  standard  bayesian inference method then proceeds from this fully-specified statistical model  by conditioning on the observed data  which in this case is the matrix a of actual observed hypertext links in the base set. specifically  when we condition on the data a we obtain a posterior density π : r1m+n ★  1 ±  for the parameters  e1 ... em h1 ... hm a1 ... an . this density is defined so that
p
		 1 
for any  measurable  subset s   r1m+n  and also
e

r
π e1 ... em h1 ... hm a1 ... an 
de1 ...demdh1 ...dhmda1 ...dan
for any  measurable  function g : r1m+n ★ r. an easy computation gives the following.
　lemma 1. for our model  the posterior density is given  up to a multiplicative constant  by
.
　our bayesian algorithm then reports the conditional means of the 1m + n parameters  according to the posterior denfor example
sity π. that is  it reports final values aj  hi  and ei  where 
	ajπ e1 ... em h1 ... hm a1 ... an 
r
de1 ...demdh1 ...dhmda1 ...dan.
　to actually compute these conditional means is non-trivial. to accomplish this  we used a metropolis algorithm.  the metropolis algorithm is an example of a markov chain monte
carlo algorithm; for background see  e.g.  smith and roberts ; tierney ; gilks et al. ; roberts and rosenthal  .
　there is  of course  some arbitrariness in the specification of this bayesian algorithm  e.g.  in the form of the prior distributions and in the precise formula for the probability of a link from i to j. however  the model appears to work well in practice  as our experiments show. we note that it is possible that the priors for a new search query could instead depend on the performance of hub i on different previous searches  though we do not pursue that here.
　this bayesian algorithm is similar in spirit to the phits algorithm of cohn and chang  described earlier  in that both use statistical modeling  and both use an iterative algorithm to converge to an answer. however  the algorithms differ substantially in their details. firstly  they use substantially different statistical models. secondly  the phits algorithm uses a non-bayesian  i.e.  classical  or  frequentist   statistical framework  as opposed to the bayesian framework adopted here.
1 a simplified bayesian algorithm
　it is possible to simplify the above bayesian model  by replacing equation  1  with p i ★ j  =  ajhi / 1 + ajhi  and correspondingly replace equation  1  with 1/ 1 + ajhi . this eliminates the parameters ei entirely  so that we no longer need the prior values μ and σ.
　this leads to a slightly modified posterior density π ，   now given by π : rm+n ★ r−1 where

　this simplified bayesian algorithm was designed to be to similar to the original bayesian algorithm. surprisingly  we will see that experimentally it often performs very similarly to the psalsa algorithm.
1. experimental results
　we have implemented the algorithms presented here on various queries. because of space limitations we only report here  see appendix a  a representative subset of results; all of our results  including the queries  death penalty    computational complexity  and  gun control  which are not reported here  can be obtained at
http://www.cs.toronto.edu/゛tsap/experiments. the reader may find it easier to follow the discussion in the next section by accessing the full set of results. for the generation of the base set of pages  we follow the specifications of kleinberg  described earlier. for each of the queries  we begin by generating a root set that consists of the first 1 pages returned by altavista on the same query. the root set is then expanded to the base set by including nodes that point to  or are pointed to  by the pages in the root set. in order to keep the size of the base set manageable  for every page in the root set  we only include the first 1 pages returned from altavista that point to this page. we then construct the graph induced by nodes in the base set  by discovering all links among the pages in the base set  eliminating those that are between pages of the same domain1.
　for each query  we tested nine different algorithms on the same base set. we present the top ten authority sites returned by each of the algorithms. for evaluation purposes  we also include a list of the url and title  possibly abbreviated  of each site which appears in the top five of one or more of the algorithms. for each page we also note the popularity of the page  denoted pop in the tables   that is  the number of different algorithms that rank it in the top ten sites. the pages that seem  to us  to be generally unrelated with the topic in hand appear bold-faced. we also present an  intersection table  which provides  for each pair of algorithms  the number of sites which were in the top ten according to both algorithms  maximum 1  minimum 1 .
　in the tables  sbayesian denotes the simplified bayesian algorithm  hubavg denotes the hub-averaging kleinberg algorithm  athresh denotes the authority-threshold algorithm  hthresh denotes the hub-threshold algorithm  and fthresh denotes the full-threshold algorithm. for the authority-threshold and full-threshold algorithms  we  arbitrarily  set the threshold k = 1.
1 discussion of experimental results
　we observe from the experiments that different algorithms emerge as the  best  for different queries  while there are queries for which no algorithm seems to perform well. one prominent such case is the query on  net censorship   also on  computational complexity   where only a few of the top ten pages returned by any of the algorithms can possibly be considered as authoritative on the subject. one possible explanation is that in these cases the topic is not well represented on the web  or there is no strong interconnected community. this reinforces a common belief that any commercial search engine cannot rely solely on link information  but rather must also examine the text content of sites to prevent such difficulties as  topic drift . on the other hand  in cases such as  death penalty   not shown here   all algorithms converge to almost the same top ten pages  which are both relevant and authoritative. in these cases the community is well represented  and strongly interconnected.
　the experiments also indicate the difference between the behavior of the kleinberg algorithm and psalsa  first observed for the salsa algorithm in the original paper of lempel and moran . specifically  when computing the top authorities  the kleinberg algorithm tends to concentrate on a  tightly knit community  of nodes  the tkc effect   while psalsa  like salsa  tends to mix the authorities of different communities in the top authorities. the tkc effect becomes clear in the  genetic  query  where the kleinberg algorithm only reports pages on biology in the top ten while psalsa mixes these pages with pages on genetic algorithms. it also becomes poignantly clear in the  movies  query  and also in the  gun control  and the  abortion  query   where the top ten pages reported by the kleinberg algorithm are dominated by an irrelevant cluster of nodes from the about.com community. a more elaborate algorithm for detecting intra-domain links could help alleviate this problem. however  these examples seem indicative of the topic drift potential of the principal eigenvector in the kleinberg algorithm.
　on the other hand  the limitations of the psalsa algorithm become obvious in the  computational geometry  query  where three out of the top ten pages belong to the unrelated w1.com community. they appear in the top positions because they are pointed to by a large collection of pages by acm  which point to nothing else. a similar phenomenon explains the appearance of the  yahoo!  page in the  genetic  query. we thus see that the simple heuristic of counting the in-degree as the authority weight is also imperfect.
　we identify two types of characteristic behavior: the kleinberg behavior  and the psalsa behavior. the former ranks the authorities based on the structure of the entire graph  and tends to favor the authorities of tightly knit communities. the latter ranks the authorities based on their popularity in their immediate neighborhood  and favors various authorities from different communities. to see how the rest of the algorithms fit within these two types of behaviors  we compare the behavior of algorithms on a pairwise basis  using the number of intersections in their respective top ten authorities as an indication of agreement.
　the first striking observation is that the simplified bayesian algorithm is almost identical to the psalsa algorithm. the psalsa algorithm and the simplified bayesian have at least 1% overlap on all queries. one possible explanation for this is that both algorithms place great importance on the in-degree of a node when determining the authority weight of a node. for the psalsa algorithm we know that it is  local  in nature  that is  the authority weight assigned to a node depends only on the links that point to this node  and not on the structure of the whole graph. the simplified bayesian seems to possess a similar  yet weaker property; we explore the locality issue further in the next section. on the other hand  the bayesian algorithm appears to resemble both the kleinberg and the psalsa behavior  leaning more towards the first. indeed  although the bayesian algorithm avoids the severe topic drift in the  movies  and the  gun control  queries  but not in the  abortion  case   it usually has higher intersection numbers with kleinberg than with psalsa. one possible explanation for this observation is the presence of the ei parameters in the bayesian algorithm  but not the simplified bayesian algorithm   which  absorb  some of the effect of many links pointing to a node  thus causing the authority weight of a node to be less dependent on its in-degree.
　another algorithm that seems to combine characteristics of both the psalsa and the kleinberg behavior is the hub-averaging algorithm. the hub-averaging algorithm is by construction a hybrid of the two since it alternates between one step of each algorithm. it shares certain behavior characteristics with the kleinberg algorithm: if we consider a full bipartite graph  then the weights of the authorities increase exponentially fast for hub-averaging  the rate of increase  however  is the square root of that of the kleinberg algorithm . however  if the component becomes infiltrated  by making one of the hubs point to a node outside the component  then the weights of the authorities in the component drop. this prevents the hub-averaging algorithm from completely following the drifting behavior of the kleinberg algorithm in the  movies  query. nevertheless  in the  genetic  query  hub-averaging agrees strongly with kleinberg  focusing on sites of a single community  instead of mixing communities as does psalsa1. on the other hand  hub-averaging and psalsa share a common characteristic  since the hub-averaging algorithm tends to favor nodes with high in-degree: if we consider an isolated component of one authority with high in-degree  the authority weight of this node will increase exponentially fast. this explains the fact that the top three authorities for  computational geometry  are the w1.com pages that are also ranked highly by psalsa  with hub-averaging giving a very high weight to all three authorities .
　for the threshold algorithms  since they are modifications of the kleinberg algorithm  they are usually closer to the kleinberg behavior. this is especially true for the hubthreshold algorithm. however  the benefit of eliminating unimportant hubs when computing authorities becomes obvious in the  abortion  query. if one looks further than the first ten pages returned by kleinberg  one observes that after the first nine pages  which belong to the amazon.com community  the rest of the pages are on topic. the hub-threshold algorithm escapes this cluster  and moves directly to the relevant pages. the intersection between the top ten pages of hub-threshold  and the set of pages in the positions 1 to 1 in the kleinberg algorithm is 1%.
　the authority-threshold often appears to be most similar with the hub-averaging algorithm. this makes sense since these two algorithms have a similar underlying motivation. the best moment for authority-threshold is the  movies  query  where it reports the most relevant top ten pages among all algorithms. the full-threshold algorithm combines elements of both the threshold algorithms; however  usually it reports in the top ten a mixture of the results of the two algorithms  rather than the best of the two.
　finally  the bfs algorithm is designed to be a generalization of the psalsa algorithm  that combines some elements of the kleinberg algorithm. its behavior resembles both psalsa and kleinberg  with a tendency to favor psalsa. in the  genetic  and  abortion  queries it demonstrates some mixing  but to a lesser extent than that of psalsa. the most successful moments for bfs are the  abortion  and the  gun control  queries where it reports a set of top ten pages that are all on topic. an interesting question to investigate is how the behavior of the bfs algorithm is altered if we change the weighting scheme of the neighbors.
1. theoretical analysis
　the experimental results of the previous section suggest that certain algorithms seem to share similar properties and ranking behavior. in this section  we initiate a  preliminary  formal study of fundamental properties and comparisons between ranking algorithms. for the purpose of following analysis we need some basic definitions and notation. let gn be a collection of graphs of size n. one special case is to let gn be the set of all graphs of size n  hereafter denoted. we define a link analysis algorithm a as a function that maps a graph g （ gn to an n-dimensional vector. we call the vector a g  the weight vector of algorithm a on graph g. the value of the entry a g  i  of vector a g  denotes the authority weight assigned by the algorithm a to the page i. we can normalize the weight vector a g  under some chosen norm. the choice of normalization affects the definition of some of the properties of the algorithms  so we discriminate between algorithms that use different norms. for any norm l  we define the l-algorithm a to be the algorithm a  where the weight vector of a is normalized under l. we also examine unnormalized algorithms  where no normalization is performed at any stage of the algorithm. for example  the unnormalized psalsa algorithm assigns a weight to page i equal to the in-degree of page i. for the following discussion  when not stated explicitly  we will assume that the weight vectors of the algorithms are normalized under the l± norm  i.e. each weight is divided by the maximum weight ; this gives weight 1 to the top authority  with other weights given as a fraction of the top weight. due to space constraints  many proofs have been omitted in the following sections.
1 monotonicity
　definition 1. an algorithm a is monotone if it has the following property: if j and k are two different nodes in a graph g  such that every hub which links to j also links to k  then a g  k  − a g  j .
　monotonicity appears to be a  reasonable  property that any sensible link-analysis algorithm should satisfy.
　theorem 1. the algorithms we consider in this paper  including the salsa algorithm  are all monotone.
1 similarity
　let a1 and a1 be two algorithms on gn. we shall consider the distance d a1 g  a1 g   between the weight vectors of
a1 g  and a1 g   for g （ gn  where d : rn 〜 rn ★
r is some function that maps the weight vectors a1 g  and a1 g  to a real number d a1 g  a1 g  . we shall consider the manhattan distance d1  that is  the l1 norm of the difference of the weight vectors  given by d1 w1 w1  =
.
　for this distance function  we now define the similarity between two l±-algorithms as follows1.
　definition 1. two l±-algorithms a1 and a1 are similar on {gn}  if  as n ★ ± 
max d1 a1 g  a1 g   = o n  . g（gn
　we also consider another distance function that attempts to capture the similarity between the ordinal rankings of two algorithms. the motivation behind this definition is that the ordinal ranking is the usual end-product seen by the user. let w1 = a1 g  and w1 = a1 g  be the weight vectors of two algorithms a1 and a1. we define the indicator function
iw1 i j  as follows
	1	if w1 i    w1 j  and w1 i    w1 j 
i
	1	otherwise
we note that iw1 i j  = 1 if and only if w1 i    w1 j    w1 i  ＋ w1 j . iw1 i j  becomes one for each pair of nodes that are ranked differently. we define the  ranking distance  function dr as follows.
 .
note that  unlike d1  the distance between two weight vectors under dr does not depend upon the choice of normalization.
　definition 1. two algorithms  a1 and a1  are rank matching on {gn}  if for every n  and every graph g （ gn  dr a1 g  a1 g   = 1 .
remark: we note that by the above definition  every algorithm is rank matching with the trivial algorithm that gives the same weight to all authorities. although this may seem somewhat bizarre  it does have an intuitive justification. for an algorithm whose goal is to produce an ordinal ranking  the weight vector with all weights equal conveys no information; therefore  it lends itself to all possible ordinal rankings. we also note that the dr distance does not satisfy the triangle inequality  since  e.g.  all algorithms have dr-distance 1 to the trivial algorithm. of course  it is straightforward to modify the definition of dr to avoid this; however  we find the definition used here to be most natural.
　proposition 1. the l±-hub-averaging algorithm  and the l±-kleinberg algorithm are neither similar  nor rank matching on.
　proof. consider a graph g on n = 1r nodes that consists of two disconnected components. the first component c1 consists of a complete graph on r nodes. the second component c1 consists of a complete graph c on r nodes  and a set of r  external  nodes e  such that each node in c points to a node in e  and no two nodes in c point to the same  external  node.
　let wk and wh denote the weight vectors of the kleinberg  and the hub-averaging algorithm  respectively  on graph g. it is not hard to see that the kleinberg algorithm allocates all the weight to the nodes in c1. after normalization  for all i （ c  wk i  = 1  for all j （ e  wk j  = m1   and for all k （ c1  wk k  = 1. on the other hand  the hubaveraging algorithm allocates all the weight to the nodes in c1. after normalization  for all k （ c1  wh k  = 1  and for all j （ c1  wh j  = 1.
　therefore  it is easy to see that which proves that the algorithms are not similar.
　the proof for rank dissimilarity follows immediately from the above. for every pair of nodes  i j  such that i （ c1 and j （ c1  wk i    wk j   and ws i    ws j . there are Θ n1  such pairs  therefore  dr wk wh  = Θ n . thus  the two algorithms are not rank matching. 
　proposition 1. the l±-psalsa algorithm and the l±kleinberg algorithm are neither similar  nor rank matching on.
　proof. consider a graph g on n = 1r nodes that consists of two disconnected components. the first component c1 consists of a complete graph on r nodes. thus  each node points to  and is pointed to by  r   1 nodes. the second component c1 consists of a bipartite graph with 1r hubs  and r authorities. without loss of generality assume that r is even  and enumerate all hubs and authorities. make all  odd  hubs point to all  odd  authorities  and all  even  hubs point to all  even  authorities. thus  each hub points to  authorities  and each authority is pointed to by r authorities.
　let wk and ws denote the weight vectors of the kleinberg  and the psalsa algorithm  respectively  on graph g. it is not hard to see that the kleinberg algorithm allocates all the weight to the nodes in c1. after normalization  for all i （ c1  wk i  = 1  while for all j （ c1  ws j  = 1. on the other hand  the psalsa algorithm distributes the weight to both components  allocating more weight to the nodes in c1. after the normalization step  for all j （ c1  w j  = 1  while for all.
therefore  it is easy to see that

which proves that the algorithms are not similar.
　the proof for rank dissimilarity follows immediately from the above. for every pair of nodes  i j  such that i （ c1
and j （ c1  wk i    wk j   and ws i    ws j . there are Θ n1  such pairs  therefore  dr wk wh  = Θ n . thus  the two algorithms are not rank matching.
　we note that a modification of this example can be used to prove the same result for the salsa and the kleinberg algorithms. 
　proposition 1. the l±-psalsa algorithm and the l±hub-averaging algorithm are neither similar  nor rank matching on .
　proof. consider a graph g on n = 1r + 1 nodes which are connected as follows. the graph consists of two sets of hubs x and y of size r and 1  respectively  and two sets of authorities a and b  each of size r  and a single  central  authority c. each hub in set x points to exactly one distinct authority in a  and both hubs in y point to all authorities in b. furthermore  all hubs in x and y point to c.
　let ws and wh be the weight vectors of psalsa and hub-averaging  respectively. the psalsa algorithm allocates the most weight to the central authority  then to the authorities in b  and then to the authorities in a. after normalization  ws c  = 1  for all  and for all.
　on the other hand  the hub-averaging algorithm considers each hub in x to be much better than each hub in y . hence  it will allocate highest weight to the authority c  nearly as high weight to the authorities in a  and much lower weight to the authorities in b. this shows that the two algorithms are neither similar nor rank matching.
　we note that the same example can be used to prove the dissimilarity between salsa and the hub-averaging algorithm. 
on the other hand  we have the following.
　definition 1. a link graph is  nested  if for every pair of nodes j and k  the set of in-links to j is either a subset or a superset of the set of in-links to k.
　let be the set of all size-n nested graphs.  of course  gn is a rather restricted set of size-n graphs. 
　theorem 1. if two algorithms are both monotone  then they are rank matching on gnnest.
corollary 1. the algorithms we consider in this paper
 including the salsa algorithm  are all rank matching on nest
gn	.
1 stability and locality
　in the previous section we examined the similarity of two different algorithms on the same graph g. in this section we are interested on how the output of a fixed algorithm changes  as we alter the graph. we would like small changes in the graph to have a small effect on the weight vector of the algorithm. we capture this requirement by the definition of stability. for the following  let e g  denote the set of all edges  i.e. links  in the graph g. we assume that e g  = ω 1   otherwise all properties that we discuss below are trivial. the following definition applies to unnormalized  and l±-algorithms1.
　definition 1. an algorithm a is stable on {gn} if for every fixed positive integer k  we have  as n ★ ±  max min d  a g  γ，a g {*  ... *k}   = o n 
where g {*1 ...*k} is the graph g with the edges *1 ... *k removed.
　definition 1. an algorithm a is rank stable on {gn} if for every k  we have  as n ★ ± 
.
　stability may be a desirable property. indeed  the algorithms all act on a base set which is generated using some other search engine  e.g. altavista   and the associated hypertext links. presumably with a  very good  base set  all the algorithms would perform well. however  if an algorithm is not stable  then slight changes in the base set  or its link structure  may lead to large changes in the rankings given by the algorithm. thus  stability may provide  protection  from poor base sets. we note that the parameter γ used in the definition of stability allows for an arbitrary scaling of the second weight vector  thus eliminating instability which is caused solely by different normalization factors.
	proposition 1. the	l±-kleinberg	and	l±-hub-
averaging algorithms are neither stable  nor rank stable.
　we now introduce the idea of  locality . the idea behind locality is that a change in the in-links of a node should have only a small effect on the weights of the rest of the nodes.
　definition 1. an algorithm a is local if for graph g  and every link * （ e g   |a g  i    a g   {*}  i | = 1 for all i （ g   {p }  where p  is the page pointed to by the link *.
　definition 1. an algorithm a is pairwise local if for every graph g  and every link a i a i for all i j （ g   {p }  where p  is the document linked to by the link *.
　definition 1. an algorithm a is rank local if for every graph g  and every link * （ e g   if w = a g  and a g   {*}   then i   = 1 for all i j （ g   {p }  where p  is the document linked to by the link *.
　we note that locality depends on the normalization used  but pairwise locality and rank locality do not. the following lemmas are direct consequences of the definitions.
　lemma 1. if an unnormalized algorithm is local  then the corresponding normalized algorithm is pairwise local  under any normalization .
　lemma 1. if an algorithm a is pairwise local  then it is rank local.
we have the following.
　theorem 1. if an algorithm is rank local  then it is rank stable  under any normalization .
　proof. let w be the weight vector of the algorithm on a graph g  and let w be the weight vector of the algorithm on the modified graph g   {*1 *1 ... *k}. let p =
               be the set of distinct pages pointed to by links *1 ... *k. since the algorithm is rank local  i
1 for all. therefore 
 .
but i	1 for all
m. therefore  the algorithm is rank stable. furthermore  rank locality is unaffected by normalization. 
　theorem 1. if u is an unnormalized algorithm that is stable on {gn}  and a is the corresponding normalized algorithm under norm ，  and  then a is stable on {gn}.
proof. let g （ gn be a graph  and let
be the modified graph. let u = u g   and   and let w = a g   and  . since u is stable 
 .
set. then

since    therefore a is stable.	
　theorem 1. the unnormalized psalsa algorithm is local.
　corollary 1. the psalsa algorithm  under any normalization  is both pairwise local and rank local.
　corollary 1. the psalsa algorithm  under any normalization  is rank stable.
　we originally thought that the bayesian and simplified bayesian algorithms were also local. however  it turns out that they are neither local nor pairwise local. indeed  it is true that conditional on the values of hi  ei  and aj  the conditional distribution of is unchanged upon removing a link from i to j. however  the unconditional marginal distribution of ak  and hence also its pos-
be changed upon removing a link fromto j.  indeed  we terior mean ak  or even ratios   may still
have computed experimentally that may change upon removing a link from 1 to 1  even for a simple example with just four nodes.  hence  neither the bayesian nor the simplified bayesian algorithm is local or pairwise local.
　theorem 1. the unnormalized psalsa algorithm is stable.
　corollary 1. the psalsa algorithm  under any normalization  is stable.
　finally  we use locality and  label-independence  to prove a uniqueness property of the psalsa algorithm.
　definition 1. an algorithm is label-independent if permuting the labels of the graph nodes only causes the authority weights to be correspondingly permuted.
　all of our algorithms are clearly label-independent  and we would expect this property of any reasonable algorithm. we have the following.
　theorem 1. suppose an algorithm a is local  monotone  and label-independent. then a and psalsa are rank matching on.
　proof. let g be any graph  and let i be a node in g. let wa = a g . since a is local  the value of wa g  i  is unchanged if we remove all links in g that do not point to i. therefore  wa i  depends solely on the set of in-links to
i. furthermore  since a is label-independent  it follows that wa i  depends only on the number of in-links to i.
　in particular  if two nodes i and j have the same number of in-links  then a assigns equal authority weight to the two nodes. assume now that j has fewer in-links than i. since a is local  we may modify the graph so that the nodes that point to j are a subset of those that point to i  without affecting wa i  or wa j . since a is monotone  this implies that wa j  ＋ wa i .
　therefore  if ws is the weight vector of the psalsa algorithm on g  then ws j    ws i    wa j  ＋ wa i . hence  iwswa i j  = 1  for all i and j in g. it follows that dr ws wa  = 1  as required.
　we note that any normalized  or unnormalized variant of a is also rank matching with psalsa.	
1 symmetry
　definition 1. an algorithm a is  symmetric  if inverting all the links in a graph simply interchanges the hub and authority values produced by the algorithm.
we have by inspection:
theorem 1. the psalsa  and salsa  algorithm  the
kleinberg algorithm  the threshold-kleinberg algorithms  the bfs algorithm  and the simplified bayesian algorithm are all symmetric. however  the hub-averaging-kleinberg algorithm and the bayesian algorithm are not symmetric.
1. summary
　we have considered a number of known and some new algorithms which use the hypertext link structure of world wide web pages to extract information about the relative ranking of these pages. in particular  we have introduced two algorithms based on bayesian statistical approach as well as a number of algorithms which are modifications of kleinberg's seminal hubs and authority algorithm. based on 1 different queries  1 presented here   we discuss some observed properties of each algorithm as well as relationships between the algorithms. we found  experimentally  that certain algorithms appear to be more  balanced   while others more  focused . the latter tend to be sensitive to the existence of tightly interconnected clusters  which may cause them to drift. the intersections between the lists of the topten results of the algorithms suggest that certain algorithms exhibit similar behavior and properties.
　motivated by the experimental observations  we introduced a theoretical framework for the study and comparison of link-analysis ranking algorithms. we formally defined  and gave some preliminary results for  the concepts of monotonicity and locality  as well as various concepts of distance and similarity between ranking algorithms.
　our work leaves a number of interesting open questions. the two bayesian algorithms open the door to the use of other statistical and machine learning techniques for ranking of hyper-linked documents. furthermore  the framework we defined suggests a number of interesting directions for the theoretical study of ranking algorithms  which we have just begun to explore in this work.
