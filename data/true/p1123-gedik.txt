in this paper  we present spade  the system s declarative stream processing engine. system s is a large-scale  distributed data stream processing middleware under development at ibm t. j. watson research center. as a front-end for rapid application development for system s  spade provides  1  an intermediate language for flexible composition of parallel and distributed data-flow graphs   1  a toolkit of type-generic  built-in stream processing operators  that support scalar as well as vectorized processing and can seamlessly inter-operate with user-defined operators  and  1  a rich set of stream adapters to ingest/publish data from/to outside sources. more importantly  spade automatically brings performance optimization and scalability to system s applications. to that end  spade employs a code generation framework to create highly-optimized applications that run natively on the stream processing core  spc   the execution and communication substrate of system s  and take full advantage of other system s services. spade allows developers to construct their applications with fine granular stream operators without worrying about the performance implications that might exist  even in a distributed system. spade's optimizing compiler automatically maps applications into appropriately sized execution units in order to minimize communication overhead  while at the same time exploiting available parallelism. by virtue of the scalability of the system s runtime and spade's effective code generation and optimization  we can scale applications to a large number of nodes. currently  we can run spade jobs on 「 1 processors within more than 1 physical nodes in a tightly connected cluster environment. spade has been in use at ibm research to create real-world streaming appli-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
cations  ranging from monitoring financial market feeds to radio telescopes to semiconductor fabrication lines.
categories and subject descriptors
h.1  database management : systems-distributed databases; h.1  database management : languages- data manipulation languages
general terms
design
keywords
distributed data stream processing
1. introduction
　on-line information sources are increasingly taking the form of data streams  that is time ordered series of events or readings. example data streams include live stock and option trading feeds in financial services  physical link statistics in networking and telecommunications  sensor readings in environmental monitoring and emergency response  and satellite and live experimental data in scientific computing. the proliferation of these sources has created a paradigm shift in how we process data  moving away from the traditional  store and then process  model of database management systems toward the  on-the-fly processing  model of emerging data stream processing systems  dspss . this paradigm shift has recently created a strong interest in dsps-related research  in academia  1  1  1  1  and industry  1  1  1  1  alike.
　in this paper we describe the design of spade  which is the declarative stream processing engine of the massively scalable and distributed system s   a large-scale stream processing middleware under development at ibm research. spade provides a rapid application development front-end for system s. concretely  spade offers:
1. an intermediate language for flexible composition of parallel and distributed data-flow graphs. this language sits in-between higher level programming tools and languages such as the system s ide or stream sql1 and the lower level system s programming apis. the spade language provides constructs such as loops  stream bundles  node pools  and operator partitions to ease the specification and configuration of flow graphs in various distributed environments.
1. a toolkit of type-generic built-in stream processing operators. spade supports all basic stream-relational operators with rich windowing and punctuation semantics. it also seamlessly integrates built-in operators with user-defined ones. one particularly powerful feature of built-in spade operators is their ability to operate on list types and their ability to intermix scalar and vectorized processing on lists.
1. a broad range of stream adapters. these adapters are used to ingest data from outside sources and publish data to outside destinations  such as network sockets  relational and xml databases  file systems  etc.
　spade leverages the existing stream processing infrastructure offered by the stream processing core  spc   component of system s. given an application specification in spade's intermediate language  the spade compiler generates optimized code that runs on spc as a native system s application. as a result of this code generation framework  spade applications enjoy a variety of services provided by the system s runtime  such as placement and scheduling  distributed job management  failure-recovery  and security. more importantly  this multi-layered framework creates opportunities for the spade compiler to perform various optimizations  so as to best map the higher level spade constructs into the lower-level ones that the system s runtime expects in order to efficiently run a distributed stream processing application. for instance  spade enables developers to structure their applications using fine granular stream operators without worrying about the performance implications that might exist in a distributed system. spade's optimizing compiler automatically maps applications into appropriately sized execution units in order to minimize the communication overhead  while at the same time exploiting available parallelism.
　spade's effective code generation and optimization framework enables it to fully exploit the performance and scalability of system s. it currently runs on approximately 1 processors within more than 1 physical nodes in a tightly connected cluster environment. spade has been in use at ibm research to create real-world data stream processing applications  ranging from processing financial market feeds to radio telescopes to semiconductor fabrication lines.
　in summary  spade improves over the current state-ofthe-art in the following aspects:
  sheer scale and performance: spade inherits its scalability from the system s stream processing core  and provides both language constructs and compiler optimizations to fully utilize and expose the performance and flexibility of spc. the distributed flow-graph composition constructs of spade offer an easy way to harness the power of system s  whereas the compiler optimizations deliver high-performance stream processing operators  which can be ideally partitioned into properly sized execution units to best match the runtime resources of system s.
  incremental application composition and deployment: spade applications are expected to be long-running continuous queries. these applications can be developed and deployed incrementally. in other words  a deployable application component  a spade job/query  can tap into existing streams that are generated by already deployed spade or non-spade system s jobs. such connections can optionally be determined dynamically at run-time  using spc's ability to discover source streams based on type compatibility.
  relational  non-relational  and mixed workloads: spade supports all fundamental stream-relational operators  with extensions to process list types. supporting list types and vectorized operations on them enables spade to handle  without performance penalty  mixed-workloads  such as those in signal processing applications that usually treat a list of samples as the basic unit of data processing  see sigsegs  .
　the rest of this paper is organized as follows. section 1 gives the architectural overview of spade and relevant aspects of system s. section 1 describes the spade language and operators. section 1 discusses compiler optimization opportunities within spade's code generation framework. section 1 introduces spade's optimizing partitioner. section 1 showcases an example spade application from the finance engineering domain. section 1 reports our ongoing work and future directions. finally  section 1 concludes the paper.
1. system overview
　in this section we briefly describe system s and provide relevant details of the spc runtime components utilized by spade. we conclude with an overview of spade's codegeneration framework.
1 system s overview

figure 1: system s from an application developer's perspective　system s is a large-scale distributed data stream processing middleware. it supports structured as well as unstructured data stream processing and can be scaled from one to thousands of compute nodes. system s runtime can execute a large number of long-running jobs  queries  that take the form of data-flow graphs. a data-flow graph consists of a set of processing elements  pes  connected by streams  where each stream carries a series of stream data objects  sdos . the pes implement data stream analytics and are basic execution units that are distributed over the compute nodes. the pes communicate with each other via their input and output ports  connected by streams. the pe ports as well as streams connecting them are typed. system s adopts the uima framework  for the type system. pes can be explicitly connected using hard-coded links  e.g.  input port 1 of pe a is connected to output port 1 of pe b  or through implicit links that rely on type compatibility  e.g.  input port 1 of pe a is connected to any output port that provides a superset of what it expects . the latter type of connections is dynamic and allows system s to support incremental application development and deployment. besides these fundamental functionalities  system s provides several other services  such as reliability  scheduling and placement optimization  distributed job management  storage services  and security  to name a few.
　system s provides several alternatives for a user or developer to craft data-flow graphs  as shown in figure 1. at one extreme  an experienced developer can use a programming language such as c++ or java to implement the desired stream analytics as pes  utilizing system s' pe apis. in this case  the developer also creates pe templates that describe each pe in terms of its input and output ports  and populates a configuration file that describes the topology of the data-flow graph. these activities could be simplified via the use of the system s ide.
　at the other extreme  a user with little or no expertise could pose natural language-like  domain-specific inquiries to the system. the inquiry services  inq  planner can use an existing set of pes developed for the particular domain at hand  together with their semantic descriptions and a domain ontology  to automatically create a data-flow graph that implements the user's high-level inquiry. for further details  we refer the reader to .
　in contrast  spade strikes a middle-ground between the aforementioned two alternatives  by providing a declarative processing front-end to the users  while still making it possible to integrate arbitrary user-defined or legacy code into the data-flow graph. developers interacting with spade use a set of well-defined  type-generic  and highly configurable operators and stream adapters to compose their applications. spade's intermediate language also provides several constructs to ease the development of distributed data-flow graphs  and exposes various knobs to influence their deployment. furthermore  it forms a common ground on top of which support for other interfaces can be build. for instance  the inq planner can potentially generate spade applications from high-level inquiries  or a streamsql query specification can be converted into a spade application.
1 stream processing core runtime
　since spc provides the execution and communication substrate for spade  the basics of how a data-flow graph is executed by the runtime is important in understanding spade's code generation and optimization framework. figure 1 shows the key architectural components of spc runtime.

figure 1: stream processing core  parts relevant to spade are shown 
　the dataflow graph manager  dgm  determines stream connections among pes  and matches stream descriptions of output ports with the flow specifications of input ports. the data fabric  df  is the distributed data transport component  comprising a set of daemons  one on each node supporting the system. upon guidance from the dgm  it establishes the transport connections between pes and moves sdos from producer pes to consumer pes. the resource manager  rm  collects runtime statistics from the df daemons and the pe execution containers. this information is used by the system s optimizer  a component called soda  for making global placement and scheduling decisions. the pe execution container  pec  provides a runtime context and access to the system s middleware. it acts as a security barrier  preventing the user-written applications from corrupting the system s middleware as well as each other. for further details on the spc the reader is referred to .
1 spade's code generation framework
　developers interact with spade through spade's intermediate language and the spade compiler. the spade compiler takes a query  job  specification in spade's intermediate language as input and generates all the artifacts commonly associated with a native system s application. figure 1 illustrates the details of this process. the spade compiler first generates code that implements the stream operator instances specified in the spade query  and then generates additional code to pack these operators into pes that form the basic execution units distributable over a system s cluster. this mapping can be optimized manually  by the user through language constructs  or automatically  by the compiler through learning  see section 1 . the compiler also generates pe templates  a type system specification  a pe topology that describes the connections among pes and pe-to-node assignments  and node pools that list the compute nodes to be used during execution. these are fed into the system s job description language  jdl  compiler to yield a complete job description. the operator and pe code are compiled into executable binaries  using traditional programming language compilers and linking against the spade and other system s libraries. the jdl file and the set of pe binaries form a readily deployable job on a system s cluster running the stream processing core.

figure 1: spade's code generation framework
　in order to support customizable data stream processing operators  spade relies on a code generation framework  instead of having type-generic operator implementations that employ some form of condition interpretation and type introspection. the reliance on code generation provides the means for the creation of highly optimized platform- and application-specific code. in contrast to traditional database query compilers  the spade compiler outputs code that is very tailored to the application at hand as well as systemspecific aspects such as: the underlying network topology  the distributed processing topology for the application  i.e.  where each piece will run   and the computational environment  including hardware and architecture-specific tuning. in most cases  applications created with spade are longrunning queries. hence the long runtimes amortize the build costs. nevertheless  the spade compiler has numerous features to support incremental builds as the application gets modified  greatly reducing the build costs as well.
1. programming model
　the spade programming model consists of a programming language and the ancillary support runtime libraries and tooling  e.g.  parser  code generators  and optimizer . the programming model was conceived with several goals in mind. on the one hand  we aimed at providing highlevel constructs where application and tool writers alike can quickly assemble their applications. on the other hand  we focused on creating a framework that enables the compiler to have direct access to the important optimization knobs such that applications are able to derive the best performance from the underlying runtime system. a longer discussion on these topics will come later. finally  the programming model was conceived such that the out-of-the-box constructs can be extended by adding new language operators and by extending the existing language operators with new capabilities. specifically  we designed the programming model as well as the tooling to support the addition of external edge adapters as well as new operators  enabling developers to incrementally add additional operators  forming new  and potentially shareable  toolkits.
1 guiding principles
　we believe that two particular design decisions we made were fundamental in achieving the goals stated above:  1  a stream-centric design  and  1  an operator-level programming model. the stream-centric design implies building a programming language where the basic building block is a stream. in other words  an application writer can quickly translate the flows of data she anticipates from a back-of-theenvelope prototype into the application skeleton  by simply listing the data flows. the second aspect  i.e.  operator-level programming  is focused on designing the application by reasoning about the smallest possible building blocks that are necessary to deliver the computation an application is supposed to perform. here it is important to note that  while it is hard to precisely define what an operator is  in most application domains  application engineers typically have a good understanding about the collection of operators they intend to use. for example  database engineers typically conceive their applications in terms of the operators available in the stream relational algebra  1  1 . likewise  matlab  programmers have several toolkits at their disposal  from numerical optimization to symbolic manipulation to signal processing  which  depending on the application domain  are appropriately used.
　the importance of an operator-centric view of applications is two fold. on one hand  it gently nudges application writers to think in terms of fine-granularity operations  that is  the fundamental processing pieces that need to be put together. on the other hand  it exposes multiple optimization opportunities  namely  the inner-workings of the operator as well as the operator boundaries  that are important for generating distributed  and parallel  code that will  ultimately  run efficiently on the computing resources. note that a side benefit of this approach is that  through a recompilation  one can typically obtain different versions of the same application which are specifically optimized for different computational platforms. for example  the runtime application layout as well as the internal operator implementation for a cluster of x1 nodes may not necessarily be the same as the one for a large symmetric multiprocessor box. the spade code generators were designed with such specializations in mind.
1 the spade programming language
　from the programming standpoint  spade's syntax and structure is centered on exposing the controls to the main tasks associated with designing applications. at the same time  it effectively hides the complexities associated with:  1  basic data streaming manipulations  e.g.  generic language support for data types and building block operations ;  1  application decomposition in a distributed computing environment  e.g.  how should the application be laid out in the computing environment ; and  1  the underlying computing infrastructure and data transport issues  e.g.  where to deploy each operator  how to best ingest external data and externalize the data being produced  etc . many of these aspects can be seen in the spade source code for a sample application  provided in the appendix and described in detail in section 1.
　the source code for an application written in the spade language is organized in terms of 1 main sections:
  application meta-information: this section lists the application name  followed optionally by the debug/tracing level desired.
  type definitions: this is where application designers must create a namespace for the types to be used by the application as well as  optionally  aliases to the types they intend to use. the type namespace provides type system-level isolation amongst system s applications that may be concurrently running on the system.
