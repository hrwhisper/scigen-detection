the management of uncertain  probabilistic data has recently emerged as a useful paradigm for dealing with the inherent unreliabilities of several real-world application domains  including data cleaning  information integration  and pervasive  multi-sensor computing. unlike conventional data sets  a set of probabilistic tuples defines a probability distribution over an exponential number of possible worlds  i.e.   grounded   deterministic databases . this  possible worlds  interpretation allows for clean query semantics but also raises hard computational problems for probabilistic database query processors. to further complicate matters  in many scenarios  e.g.  large-scale process and environmental monitoring using multiple sensor modalities   probabilistic data tuples arrive and need to be processed in a streaming fashion; that is  using limited memory and cpu resources and without the benefit of multiple passes over a static probabilistic database. such probabilistic data streams raise a host of new research challenges for stream-processing engines that  to date  remain largely unaddressed.
　in this paper  we propose the first space- and time-efficient algorithms for approximating complex aggregate queries  including  the number of distinct values and join/self-join sizes  over probabilistic data streams. following the possible-worlds semantics  such aggregates essentially define probability distributions over the space of possible aggregation results  and our goal is to characterize such distributions through efficient approximations of their key moments  such as expectation and variance . our algorithms offer strong randomized estimation guarantees while using only sublinear space in the size of the stream s   and rely on novel  concise streaming sketch synopses that extend conventional sketching ideas to the probabilistic streams setting. our experimental results verify the effectiveness of our approach.
categories and subject descriptors:
e.1  data : data structures; f.1  theory : analysis of algorithms
general terms: algorithms  performance  reliability keywords: data streams  uncertain data.

 
work done while at intel research  berkeley
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  beijing  china.
copyright 1 acm 1-1-1/1 ...$1.
1. introduction
　conventional database systems and query processing tools are designed around the idea of static collections of exact data tuples. unfortunately  the data generated by a diverse set of real-world applications is often uncertain and imprecise. for instance  data integration and record linkage tools can generate distinct degrees of confidence for output data tuples  based on the quality of the match for the underlying entities  ; structured information extractors typically assign different confidences to rules for identifying meaningful patterns in their  unstructured  input ; and  pervasive multi-sensor computing applications need to routinely handle noisy sensor/rfid readings . motivated by these new application requirements  recent research efforts on probabilistic data management aim to incorporate uncertainty and probabilistic information as  first-class citizens  of the database system.
　among the different approaches for managing uncertainty inside the database  tuple-level uncertainty models  that  essentially  associate independent existence probabilities with individual tuples  have seen wide adoption both in research papers as well as system prototypes. this is due to both their simplicity of representation in current relational systems  i.e.  just adding an  existence probability  column   as well as their simple and intuitive query semantics. in a nutshell  a probabilistic database is a concise representation for a probability distribution over an exponentiallylarge collection of possible worlds  each representing a possible  grounded   deterministic  instance of the database  e.g.  by flipping appropriately-biased independent coins to select each uncertain tuple . this  possible-worlds  semantics also implies clean semantics for queries over a probabilistic database - essentially  the result of a probabilistic query defines a probability distribution over the space of possible query results across all possible worlds .
　unfortunately  despite its simple  intuitive semantics  the paradigm shift towards tuple-level uncertainty also appears to imply a huge jump in the computational complexity of simple query processing operations: as demonstrated by dalvi and suciu   correctly evaluating the resulting tuple probabilities for duplicate eliminating projections over a simple three-way join can give rise to problems with #p-hard data complexity  i.e.  exponential in the number of database tuples . query-processing efficiency issues are  of course  further exacerbated by the streaming nature of several target applications for probabilistic data-management tools. for example  large-scale process and environmental monitoring tools rely on the continuous collection and processing of large amounts of noisy  uncertain readings from numerous sensor modalities  and recent work has already suggested attaching tuple-level probabilities as explicit indicators of data quality . due to the continuous  online nature of these applications and the large data volumes involved  these probabilistic data tuples arrive continuously and need to be processed in a streaming fashion; that is  query results must be available in real-time  using only limited memory and cpu resources  and without the benefit of several passes over a static probabilistic database. efficient stream-processing algorithms and system architectures for deterministic tuple streams have formed a very active research area in recent years. as in the static database case  however  such probabilistic data streams raise a host of new  difficult research challenges for stream-processing engines that mandate novel algorithmic approaches.
prior work. several efficient algorithms have been developed for processing different classes of complex queries over massive data streams; examples include computing quantiles   estimating distinct-value counts   counting frequent elements  i.e.   heavy hitters    1  1   approximating large haar-wavelet coefficients   and estimating join sizes and stream norms  1  1  1 . none of these works consider the issues raised by uncertain streaming tuples.
　recent work on probabilistic database systems has focused on different aspects of managing uncertain data tuples in relational dbms architectures  including the complexity and algorithmic problems of query evaluation   data modeling issues   and managing lineage for probabilistic query results . the issue of efficient aggregate query evaluation under the stringent constraints of the streaming model was not considered in any of these papers. khoussainova et al.  propose an architecture for cleaning sensor readings by attaching explicit  correctness  probabilities - their techniques are essentially complementary to our work. more recently  jayram et al.  have studied the problem of evaluating simple aggregate functions  focusing  in particular  on average  over streams of uncertain data. their model of uncertainty is actually richer than ours  and can capture value-level uncertainty as well as the probability of existence. still  even for very simple aggregates like average  their techniques and analyses rely on sophisticated mathematical tools  e.g.  generating functions   and it is very unclear if they can be extended to the broader  more complex class of aggregate queries considered here. independently and concurrent with this work  jayram et al.  showed some results for the expectation versions of f1  f1 and quantiles problems studied here  and improve their results for average.
our contributions. in this paper  we initiate the study of spaceand time-efficient techniques for approximating a broad class of complex aggregate queries over continuous probabilistic tuple streams. by possible-worlds semantics  such probabilistic aggregates define probability distributions over an  exponentially-large  space of possible results; thus  our goal is to characterize such distributions through efficient  streaming approximations of their key moments  such as expectation and variance  over the possible-worlds collection. we propose novel  randomized sketching synopses and estimation algorithms for probabilistic data streams; as with conventional data streaming  our methods employ only sublinear space  in both the size and domain of the probabilistic stream  and offer strong randomized estimation guarantees for the key moments of the underlying complex aggregate. more concretely  our contributions are summarized as follows.
  generic streaming probabilistic aggregate estimator based on possible-worlds sampling. we present a universal aggregate estimation algorithm for probabilistic data streams based on the intuitive idea of sampling possible-worlds  i.e.  deterministic streams  from the input  and running conventional streaming estimators over the sampled streams. the obvious appeal of such a scheme is that it allows us to directly leverage existing space-efficient algorithms for deterministic data streams in the probabilistic setting. unfortunately  as our analysis and experimental results show  this approach has severe limitations when it comes to estimating the moments of complex aggregates over streaming probabilistic data.
  probabilistic fm  pfm  sketch synopses and estimators for probabilistic count-distinct aggregates. while count-based aggregates  e.g.  expected tuple counts or heavy-hitters  allow for simple solutions  due to their linearity   that is not the case for more complex aggregate queries over the probabilistic stream. we initially focus on the class of count-distinct queries and introduce a novel  randomized estimation algorithm for probabilistic data streams. our algorithm relies on a new  hash-based sketch synopsis structure for streaming probabilistic data  termed probabilistic fm  pfm  sketch   inspired by the well-known flajolet-martin  fm  sketch  for counting distinct deterministic values. we introduce and analyze pfm-sketch-based estimators for probabilistic count-distinct  and demonstrate strong error guarantees while using only small space to sketch a large probabilistic data stream.
  streaming estimators for probabilistic self-join/join sizes and higher frequency moments based on ams techniques. for the second moment of a probabilistic data stream  corresponding to the expected self-join size  and the related quantity of the join size of two independent probabilistic streams  we develop new insights based on expressing the expectation and variance in terms of the cumulants of appropriate distributions. these cumulants can be computed easily for each component of the stream  and we show how to represent them all compactly  and manipulate them  using variations of the alon-matias-szegedy  ams  sketch data structure . we show the accuracy and power of this approach  as we are able to track yet higher moments using cumulant analysis.
  experimental results validating our approach. we perform a thorough evaluation of our techniques on a mixture of real and synthetic data. we observe that our methods can be highly practical: they typically obtain a small    1%  error on streams of millions of items  using only tens of kilobytes and taking only seconds of cpu time. we show that simple techniques often suffice for estimating expectations  but to understand the higher moments our more involved algorithms are a necessity.
due to space constraints  several proofs and detailed technical arguments are omitted; complete details are deferred to the full version.
1. probabilistic data stream model
　we consider a simple model of probabilistic data streams  where each stream renders a multi-set of relational tuples with independent tuple-level uncertainties  i.e.  existence probabilities . as mentioned earlier  such independent probabilistic tuples form the basis of several recent studies on probabilistic data management  e.g.     since they can be naturally represented in relational systems  and allow for clean and intuitive query semantics; furthermore  even such simple models of uncertainty raise intractable computational problems for probabilistic query processing .
　more formally  we define a probabilistic data stream as a sequence of n uncertain tuples ht pi  with the semantics that tuple t occurs in an instance of the database with probability p （  1   independently of all other tuples in the database .1 tuples t are drawn from a finite domain of size m that  without loss of generality  is assumed to be the integer domain  m  = {1 ...  m}. as is typical in data stream analysis  we focus on the case when n and m are  large   and our stream query processor can observe the streaming tuples only once  in the fixed order of arrival  and can only use space/time that is sublinear in both n and m to maintain a concise synopsis of the probabilistic data stream. as our results show  relatively simple techniques can be used in the case where o min{n m}  space is available  but more advanced tools are needed under sublinear space constraints. this model is a special case of the more general model where each tuple encodes a compact probability distribution  pdf ; we comment that many of our results immediately apply to this more general setting  but for simplicity we concentrate our discussion on the simpler case.
aggregate estimation over probabilistic streams. following all earlier work on probabilistic databases  we view a probabilistic data stream as defining a probability distribution over a collection of possible worlds: implicitly  a probabilistic stream encodes exponentially many  up to 1n  conventional  deterministic data streams  each occurring with some probability  determined by the individual existence probabilities of its constituent tuples . we refer to these possible deterministic instantiations of a probabilistic data stream s as its grounded streams  denoted by grnd s  . specifically  consider a probabilistic stream s =  hti  pii : 1 ＋ i ＋ n  and let g i  denote one possible outcome of s comprising the sequence of all tuples with index in some subset i    n   i.e.  g i  =  ti : i （ i} ; then  by tuple independence  it is easy to see that the probability of this possible outcome g i  can be computed simply as pr g i   = qi（i pi ， qj1（i 1   pj . note  of course  that  since the tuples ti are not necessarily distinct  i.e.  a stream renders a bag of tuples   several distinct index subsets i can in fact map to the same grounded stream g （ grnd s ; thus  grnd s  ＋ 1n and the probability of a grounded stream instance g （ grnd s  is defined as pr g  = pi:g i =g pr g i  .  it is straightforward to show that pr g  defines a valid probability distribution over grnd s . 
example 1. consider the simple probabilistic stream s =
. this encodes 1 = 1 possible outcomes 
covering 1 distinct grounded stream instances:
       grnd s  = { x ; y ; x y ; y y ; x y y ;φ}  where φ denotes the empty stream . the probabilities for each possible distinct grounded stream can be easily computed as:
grounded stream g x  y  x y  y y  x y y φpr g 1
1
1
1
1
1
1　our focus here is on computing complex aggregates over probabilistic data streams. as mentioned earlier  such aggregates essentially define a probability distribution over the  exponentially large  space of aggregation results for all possible worlds. given the potentially enormous size and complexity of such distributions  our goal is instead to effectively characterize these probabilistic data aggregates through efficient streaming approximations of their key distribution moments  such as expectation and variance over the possible-worlds collection. more formally  consider a probabilistic data stream s  and let f s  denote the result of evaluating an aggregate function f   over s. then  f s  is a random variable ranging over all possible grounded streams g （ grnd s  with ex-
pectation and variance naturally defined aseg f s  =x
	pr g  ， f g 	and
g（grnd s varg f s  =eg f1 s     e1g f s  =	x	1
pr g  ， f  g    eg f s   
g（grnd s where we use the  g  subscript to denote the underlying probability space of all possible worlds g （ grnd s .
　naturally  a  naive  way to compute eg f s   and varg f s   in   1n  time and space  is by explicitly grounding s and computing pr g  and the  deterministic  aggregate value f g  for every g （ grnd s . however  our goal is much stronger: we aim to perform these computations in time exponentially less than this  essentially  in a single pass over the probabilistic tuples   and with space significantly sublinear  e.g.  poly-logarithmic  in n and m. these are the typical requirements for efficient query processing algorithms in the  deterministic  streaming model  1  1 .
　the specific class of aggregate queries of interest in this work are the frequency moments of a stream  as well as closely related streaming problems . such frequency moments have formed the basis of most algorithmic studies of  non-probabilistic  data streams  and the techniques developed for their computation are at the heart of many other algorithms for data-stream query processing  1  1  1  1  1  1 . formally  consider a grounded stream g and let ft denote the number of occurrences of element t （  m  in g.  we use  tuple  and  element  interchangeably in the remainder of the paper.  the kth frequency moment of g  fk g   k − 1   is defined as  where k − 1. we treat 1 = 1  which implies that f1 g  is the number of domain elements t （  m  such that ft is non-zero  i.e.  the number of distinct tuples in the stream g. similarly  f1 g  is simply the total number of tuples in g  i.e.  f1 g  = n   and f1 g  is the size of the self-join of g  over the attributes of the streaming tuples  .
　the kth frequency moment fk s  of a probabilistic stream s can now be defined naturally as a random variable over g （ grnd s   as earlier. of course  it is important to note here the distinction between the fk s  frequency moments  and the corresponding distribution moments  such as eg fk s   and varg fk s   over grnd s  - our goal in this work is to devise streaming algorithms for estimating such distribution moments in a single pass over s. as with most work on data-stream algorithmics  our approach is based on the design of efficient randomized schemes that  ideally  guarantee
-approximation bounds  1  1 ; that is  given a quantity x to be estimated over a stream  we aim to produce an estimate x  of x such that pr .
1. warm-up: basic stream estimates
　in this section  we start by describing a general aggregate estimation scheme for probabilistic data streams based on the idea of sampling possible worlds. then  we briefly discuss a simple streaming estimator for the first frequency moment of a probabilistic stream s  i.e.  f1 s    and its implications for other probabilistic aggregate queries  including  quantiles and heavy-hitters .
1 universal sampling-based algorithm
　based on our possible-worlds interpretation of probabilistic stream aggregates  a natural sampling-based scheme emerges for obtaining streaming  estimators for eg f s   and varg f s    for any aggregate f   that can be computed  or  accurately approximated  over a deterministic data stream.
　the idea is to randomly sample a small subset of possible worlds for an input probabilistic stream s  where a grounded stream g （ grnd s  is chosen with probability pr g . this is simple to implement: given a target sample size s  initialize gj = φ  j = 1  ...  s   and  for each incoming probabilistic tuple hti  pii  simply perform s independent biased coin flips  each with success probability pi  setting gj = gj “ {ti} if the jth coin flip succeeds. for our universal probabilistic-stream estimator for f    we do not store the possible worlds gj; instead  the tuples generated for these  grounded  streams are fed into s parallel instances of streaming estimation algorithms for f  . let {f  gj  : j = 1  ...  s} denote the outputs of these streaming estimators; then  we estimate eg f s   and varg f s   as the sample mean μ  and the sample variance σ   respectively  where:
		and	.
the following theorem establishes the accuracy properties of our universal probabilistic stream estimator.
　theorem 1. if f  g  is an unbiased streaming estimator for f g   then μ  and σ  are unbiased estimators for eg f s   and varg f s    respectively. moreover  using samples  μ  provides an -approximation of eg f s  .
in other words  by sampling s  possible worlds
and passing each to a separate instance of a streaming estimation algorithm for f   over deterministic data streams  our sample-mean estimator can guarantee -relative error with constant probability. we can amplify this to any user-defined success probability by simply repeating o log 1/δ   times and taking the median result; standard chernoff-bounds arguments then guarantee that the probability of failure is less than δ. by its sample-size requirement  it is easy to see that the estimation quality of our universal estimator depends crucially on the ratio of varg f s   = eg f1 s    e1g f s   to e1g f s  . we will see subsequently that for the frequency moments we consider in this paper  varg fk s   ＋
eg fk s   for k = 1  and varg f1 s   = o e1g/1 f1 s   . so this ratio will be small  and decreasing as eg fk s   increases.
　it is tempting to try to apply the same technique to derive bounds for the accuracy of σ 1 for approximating the distribution variance varg f s   of the probabilistic aggregate f   - after all  the variance is just another expectation. unfortunately  the bounds resulting from such an approach are considerably worse. observe that the variance computation is equivalent to computing the difference of two values: and μ 1. the first of these is an unbiased estimator for the quantity varg s  + e1g f s   and the second for the quantity e1g f s  . however  for the frequency moments that we study  e1g fk s   can be much larger than varg fk s  : we stated above that varg fk s   ＋ e fk s   for k = 1.
suppose we chose s large enough to ensure that eg fk s   is  approximated  then e	approximated.
because we the quantity we are estimating can be much smaller than this  we need a much more accurate approximation in order  approximate varg fk s  . thus we need to choose an
1 such that. using the bound from theorem 1  this means we need   eg fk s    samples for
varg f1 s   and varg f1 s    and   e1g/1 fk s    samples for varg f1 s  . the bounds from this analysis are so weak that they are of little practical use. we shall also see experimentally that the estimations of variance using the universal sampling algorithm are very poor in practice.
　in the remainder of the paper  we show several cases where more intelligent   aggregate-aware  streaming estimation algorithms can be developed that perform significantly better than the above universal sampling-based strategy. we start by discussing solutions and applications for the simple case of the first frequency moment.
1 f1: counts  quantiles  and heavy hitters
　for a conventional  deterministic data stream g  computing the first frequency moment f1 g  is trivial: it is just the count of elements in the stream  which can be computed exactly using a single counter. similarly  computing the  exact  distribution moments of f1 s  over a probabilistic stream s turns out to be quite straightforward. specifically  consider the probabilistic stream s =  hti  pii : 1 ＋ i ＋ n . for the purposes of first-moment estimation over s  each streaming uncertain tuple hti  pii can be seen as a bernoulli variable bi that takes the value 1  i.e . the tuple is in the possible world  with probability pi  and 1 otherwise. then  by linearity of expectation and the independence of probabilistic tuples in the stream:
	eg f1 s  	= pni=1 e xi  = pin=1 pi  	and
   varg f1 s   = pni=1 var xi  = pni=1 pi 1   pi   and hence varg f1 s   ＋ eg f1 s  . the above linearity immediately implies that we can compute the exact expectation and variance of f1 s  using only two variables. earlier work on probabilistic data has already made similar observations for such linear aggregates  at least for the case of computing the expectation  e.g.  see  . it is not difficult to see that  thanks to this linearity  we can also leverage known  deterministic  stream synopses and results to provide strong estimation guarantees for other important count-based aggregates over probabilistic data streams. we now summarize our key results in this setting.
　given a probabilistic data stream s and tuple t （  m   let ft denote the point frequency  i.e.  number of occurrences  of t in s - this is again a random variable over grnd s . if t （  m  is given in advance  we can easily compute eg ft  and varg ft  using two simple counters  as above. more interestingly  when t is not known beforehand  it is possible to build small-space streaming synopses over s to estimate such point-frequency moments to within  factors of the corresponding stream moments.  in other words  we can estimate point-frequencies accurately in small space as long as they are  large  with respect to the overall  expected  count of elements in the stream; these correspond to the typical streaming error guarantees for point estimation in the deterministic setting  1  1 . 
　lemma 1. using space of only  it is possible build a synopsis over a probabilistic data stream s that  given t （  m  
can return estimates μ t and σ t such that  with probability 1 
	e	and
.
proof:by linearity of the aggregate  it is not difficult to see that estimating eg ft  and varg ft  corresponds to a standard point query over a data stream containing fractional values - namely  the pi values for eg ft  and the pi 1   pi  values for varg ft . thus  one can immediately apply known streaming synopses  such as the count-min sketch . this provides estimates with errors proportional to  and  for expectation and variance  respectively   but with only probabilistic guarantees. well known techniques such as lossy counting  and misra-gries  do not naturally accommodate a stream of fractional values  they assume a stream of unitary updates . however  the algorithms of suri et al.  and metwally et al.  allow arbitrary update values  and can be applied to extract point estimates from a stream. they can be implemented so that each update takes expected time o 1   and uses space  amd  respectively. thus  we can provide the required estimates μ t and σ t in this time and space.	
　using standard streaming arguments  the above lemma directly implies space/time-efficient probabilistic-stream estimation algorithms for other interesting count-based aggregates  such as approximate heavy hitters and approximate quantiles. the development resembles that for the conventional streaming version of the problem - more details can be found  e.g.  in  1  1 .
　corollary 1. assume a constant φ （  1   and a desired accuracy guarantee    φ. we state the following results:
　1. approximate φ-heavy hitters: to return all elements t such that e  and no item t1 such that eg ft1  ＋  can be done in space.
　1. approximate φ-quantiles: to return all items t such that  φ+
 e can be done in space.
　the same estimation guarantees also hold for variance estimates  i.e.  replacing eg with varg .
1. estimating count distinct  f1 s  
　as noted in section 1  the case of of the first frequency moment f1 and related count-based estimation problems was easy to reduce to prior streaming work  due to linearity of the aggregate. unfortunately  other frequency moments are less straightforward. in this section  we consider the case of f1  the number of distinct items over the probabilistic stream s. it is important to note that computing eg f1  is very different from computing the number of distinct tuples seen in s; for instance  s could comprise many millions of tuples with distinct values  but if each pi is minuscule  say  of the order of 1   eg f1 s   could still be much less than one. also  the linearity property that allowed us to simply sum probabilities in the case of f1 is no longer valid - intuitively  this is because  for f1  the number of occurrences of a distinct tuple t （  m  in a stream is immaterial as long as the tuple appears at least once. thus  our streaming algorithms must track these probabilities of occurrence for distinct tuples in the domain. thanks to tuple independence  this turns out to be quite simple to do if one assumes linear space  o m  .
　lemma 1. using space o m   a streaming algorithm can compute the exact values of eg f1 s   and varg f1 s   over a probabilistic data stream s.	
proof:let pt denote the probability that element t （  m  is observed at least once in any grounded instance of the stream s =
 hti  pii : i = 1  ...  n ; that is  pt = pt（g g（grnd s  pr g . by tuple independence  it is not difficult to see that this probability can also be expressed as pt = 1  qi:ti=t 1   pi   i.e.  one minus the probability that none of the instances of t are actually materialized. this probability can be incrementally computed as tuples hti pii from s are streaming by. initially  set pt = 1 for all t = 1  ...  m. now  suppose the next tuple in s is hti pii  with ti = t; then  we update pt ○ pt 1   pi  + pi  to reflect the updated probability of seeing a t tuple. it is easy to verify that this rule correctly maintains the probability of occurrence for t as defined above. the expected number of distinct items is exactly
e. for varg f1 s    observe that  for countdistinct estimation  each t （  m  is just a bernoulli random variable with parameter pt  and  thus  its variance is simply pt 1 pt . since f1 s  is basically the summation of these bernoulli random variables  and by tuple independence  we have varg f1 s   =   and hence varg f1 s   ＋ eg f1 s  . 
example 1. consider our example probabilistic stream s =
. we have   and   so eg f1 s   = 1 and . we can verify these results against the probabilities for the grounded streams pr g   see example 1 . from these probabilities  we have eg f1 s   =
1 pr  x  +pr  y  +pr  y y   +1 pr  x y  +pr  x y y    =
. similarly  varg f1 s   =
e.
using the universal  sampling-based estimator for eg f1 s  . we show the guarantees that our universal estimation algorithm
 based on possible-worlds sampling  of section 1 can provide for f1 estimation over s. from theorem 1  we know that the universal estimator can guarantee a good approximation for eg f1 s  
by sampling grounded streams g （ grnd s   and using an efficient  guaranteed-error streaming estimator for f1 g  on each. from the above analysis based on independent
bernoulli random variables  we see that
.
e
in other words  the number of grounded-stream samples  and estimators  needed to guarantee small relative errors is inversely proportional to eg f1 s    the quantity that we want to estimate . this is actually quite intuitive: consider a probabilistic stream s comprising n distinct tuples ti  where all tuples have the same probability pi = p. if p is very small  which  of course  implies that eg f1 s   is small  then a large number of samples is needed to ensure a sampling estimate with small relative error; for instance  if n = 1 and p = 1  then at least   samples are needed  on average  just to sample a non-empty possible world  i.e.  give a non-zero estimate ! whereas  if p = 1  i.e.  eg f1 s   = n   then all tuples are deterministic  and a single instance of the f1 g  estimator  i.e.  s = 1 .
　as our experiments show  in practice  eg f1 s   is typically of moderate size  and a small sample size s is often sufficient to get a good approximation. on the other hand  as discussed in section 1  such error guarantees unfortunately do not carry over to the case of the f1-variance varg f1 s  .
1 the probabilistic fm  pfm  sketch
　we now introduce a novel algorithm for f1 estimation over probabilistic data streams. our algorithm guarantees randomized estimates for both eg f1 s   and varg f1 s    while using space that is only poly-logarithmic in the size of the stream s  and independent of the value of eg f1 s  .
　our technique is inspired by the popular flajolet-martin  fm  algorithm  for estimating the number of distinct elements over a deterministic data stream g. briefly  letting  m  denote the domain of the stream  as earlier   the fm algorithm employs a family h of hash functions h :  m   ★ {1 ... logm}  such that  for any x （  m  pr h x  = i  = 1 i  where probability is defined over the family of hash-function choices h . the basic stream synopsis maintained by the fm algorithm  known as an fm sketch  is a bitmap of size o logm  that is maintained using a specific hash function h   （ h. specifically  the sketch is initialized to all zeros and  for each incoming element x in the stream  the bit at location h x  is turned on. by the properties of h    we expect a fraction of 1 i of the distinct values in the stream to map to location i of the sketch; that is  we expectof the distinct values to land in bit 1 in bit 1  and so on. thus  if we let λ denote the highest bit location that has been turned on in the bitmap  then 1λ is a good indicator of the number of distinct elements in g. using several independent fm sketches  with different hash-function choices from h   it is possible to boost estimation accuracy and confidence to user-defined  levels  e.g.   .
　our proposed sketch synopsis for probabilistic data streams  termed probabilistic fm  pfm  sketch  is based on similar ideas. we employ the same class of hash functions h as the basic fm algorithm  and define a pfm sketch as an array pfm comprising o logm   real-valued  probability entries maintained using a hash function
procedure pfmestimate   sketchmatrix 1...s1...s1   
input: matrix of s1 〜 s1 independent probabilistic fm sketches for
probabilistic stream s  where.
output: -approximate estimate for eg f1 s  .
1. let d ij := basic pfm estimate of eg f1 s   using the sketch of s at sketchmatrix i j   see eqn.  1  
1. d := pi j d ij/ s1 ， s1 
1. k  := dlog 1d  e	// find inference level
1. for
1. j :=	i	sketchmatrix 	  	 	1
1. return
figure 1:  estimation algorithm for eg f1 s  .

h   （ h. the pfm array is initialized to all zeros  and  for each streaming pair hti pii in the probabilistic data stream s  we update the pfm h ti   entry by setting pfm h ti   ○ pfm h ti   1   pi  + pi.
　let h 1 j  the set of tuples in  m  that map onto bit j through h  ; that is h 1 j  = {t （  m  : h t  = j}. using a simple inductive argument  we can prove that  using the update rule above  pfm j  = 1   qti（s 1   pi  = pr h 1 j  =1 φ  
where the last equality follows from tuple independence. we
compute a basic estimate for eg f1 s   from the pfm array as
         o logm  o logm  d  = x 1jpfm j  y  1   pfm k  .  1 
	j=1	k=j+1
intuitively  the above formula computes an expectation over the 1j estimates for all possible bitmap locations j using the probability that j is the highest non-empty location in the bitmap. in a sense  this can be seen as an  probabilistic expectation  version of the original fm idea. our basic pfm estimate can be shown to guarantee constant estimation error with constant probability:
　theorem 1. using a single pfm sketch of size o logm   our basic pfm estimation algorithm outputs an estimate d for
eg f1 s   such that pr 	e.	
an -approximate estimator for eg f1 s  . we now give a streaming -approximate estimation algorithm for eg f1 s  . briefly  our algorithm employs a number of independent pfm sketches built over the s probabilistic stream. the basic idea behind our estimation process is to first use our constant-factor/probability basic estimator for eg f1 s    described above  in order to identify an appropriate inference level k  in the pfm sketch structures. our goal in determining this inference level is to ensure that the probability that more than one distinct elements map onto that level  over all possible worlds g and choices of hash function h    is small. we can achieve this by choosing k  to be a few levels higher than logd   where d is the estimate returned by averaging basic estimates across the individual pfm sketches. then  we use averaging and median selection over the probabilities at level k  across all pfm sketches in order to produce an accurate estimate of eg f1 s  . pseudocode for our  estimation algorithm is in figure 1.
　theorem 1. algorithm pfmestimate returns an -approximate estimate for eg f1 s   using independent pfm sketch synopses for the probabilistic data stream s. 
proof  sketch :we briefly discuss the main ideas behind our proof - the complete details  optimizations of constant factors  etc.  are deferred to the full paper. we start with some notation. fix a specific level k in the pfm sketch. let xk be an indicator random variable  rv   over all g and h    for the event that − 1 distinct stream elements map onto level k  and let zk denote a rv  over all g and h    for the number of distinct values in the stream mapping onto level k. now  fix a specific choice of hash function h   and let pk h  = pfm k  denote the  incrementally  computed probability at level k of the sketch; this is exactly the fraction of possible worlds g for which the kth bit of the corresponding fm sketch is on  i.e.  pk h  = eg xk h  . in other words  pk h  is a sample point  for a fixed h    from the distribution of xk - note that the expectation of xk has a highly non-linear relationship with f1; more specifically:
eg h xk  = pg pr g  ，  1    1   1 k f1 g  
　since each distinct element in g maps independently onto level k with probability 1k.  we use the g  h subscripts to denote expectation taken over the space of possible worlds and hash functions  respectively.  so  while we can use the sample points pk h  to estimate eg h xk   there is no obvious way to go from that to our target quantity eg f1 s   = pg pr g  ， f1 g .
　on the other hand  the expectation of zk has an  easy  linear relationship to eg f1 s    since
eg h zk = xpr g f1 g pr level=k =1 keg f1 s    1 
g
our goal is to pick out an inference level k  such that  with some constant probability  eg h xk  = eg h zk . we achieve that using our initial constant factor approximation to eg f1 s   - based on our analysis  and using an additional averaging step over m iid instantiations to reduce the variance  and  thus  the probability of error in the chebyshev bound  by a factor of m  we can get an estimate d of eg f1 s   such that 1eg f1    1cd    1eg f1  with probability at least . in what follows  we assume our approximation bounds on d hold and add to the probability of error. define k  = dlog 1cd  e. then  our bounds on d imply that
 or 
now  consider the rv zk . from equations  1 - 1   application of the markov inequality gives
pr.
　so at level k   we have zk  ＋ 1  i.e.  zk  （ {1}  with probability − 1. again  we assume zk  （ {1} and adjust the final probability of error by adding 1. in this case  at level k  
 k 
eg h zk   = eg h xk   = eh pk  h   = 1	eg f1 s  
　 where the last equality follows from equation  1  . now  over all possible choices of h    the rv pk  h  satisfies
varh pk  h   ＋ eh p1k  h   ＋ eh pk  h   = 1 k eg f1 s   
 since k  h  （  1  . assuming m iid instantiations  using independently chosen hash functions h1    h1    ...  hm    of our probabilistic fm sketch  we define the sample-average estimator
 for the expectation eh pk  h    line 1 in
our pfmestimate algorithm . from the chebyshev bound:
pr
 where the last inequality follows from the above equation .
 
　so 1k p is a randomized -relative error estimator of eg f1 s   with an overall error probability upper bounded by 
  where m is the number of iid pfm sketches used for averaging pk  hi  instances  and c is a constant   1. assuming a small   say  ＋ 1   we can now choose m and c to ensure that this error probability is upper bounded by a constant   1. we can now use o log 1/δ   iid instantiations of the above procedure to bring the overall error probability down to δ  lines 1 of pfmestimate .
thus  we have an  randomized estimator for eg f1 s   using  pfm sketch summaries.
estimating varg f1 s . computing varg f1 s   initially seems harder than computing eg f1 s  . as we demonstrate  however  the complexity of building a good estimator is no harder  by reduction to the expectation-estimation problem.
　lemma 1. given a method to  approximate eg f1 s    the quantity varg f1s   can be estimated  using the same space  with error at most  with probability at least 1   1δ.
proof:given a probabilistic stream s =  hti pii : i = 1 ... n   we define a new stream s1 =  hti 1pi   p1ii : 1 ＋ i ＋ n   and compute eg f1 s1  . we claim that eg f1 s1     eg f1 s   = varg f1 s  . we prove this claim by studying properties of pt  where pt denotes the probability that t does not occur in a ground
stream of s  i.e. pt = pt1（g g（grnd s  pr g  = qi ti=t 1 pi .
thus  e	and
e.
so eg f1 s1     eg f1 s   = pmt=1   p1t    pmt=1   pt 
=pmt=1 pt 1   pt  = varg f1 s   
since lemma 1 showed that varg f1 s   = pmt=1 pt 1   pt . we observe that 1   p1t = 1pt   p1t ＋ 1pt  and thus
	m	m
eg f1 s1   = x1   p1t ＋ 1xpt = 1eg f1 s  .
	t=1	t=1
thus  if we  approximate eg f1 s1    the error is at most  with probability at least 1   δ. combining this with an  approximation of eg f1 s   yields an approximation of varg f1 s   which is within  with probability at least 1   1δ by the union bound.
our proof argument demonstrates that we can compute varg f1 s   using two distinct pfm sketches: one for estimating eg f1 s   over the original stream s =  hai pii : 1 ＋ i ＋ n   and one for estimating eg f1 s1   over a  modified  stream s1 =  hai 1pi   p1ii : 1 ＋ i ＋ n . but we can be even more space efficient: observe that our pfm sketch summaries find pfm b  =pr h 1 b  1= φ in grnd s   =1   pr h 1 b  = φ in grnd s   = 1   y pt
t h t =b
if we built a pfm summary pfm1 of s1 using the same hash function h    we want to find pfm1 b  = 1   qt h t =b p1t. but 
 pfm b  1.
thus  we can actually estimate both eg f1 s   and eg f1 s1   using the information stored in a single pfm sketch data structure.
1. second moment and join size
　we next consider the complexity of computing the expectation and variance of the second frequency moment of a probabilistic stream. later  we will go on to study the related question of the expected join size between two probabilistic streams.
1 expectation of f1 using pams
　we introduce an estimation technique based on the randomized sketches of alon et al.   which we call  probabilistic ams   or pams for short. we proceed by reducing the problem from computing over exponentially many ground streams to tracking information about the up to m distinct tuples in the stream  then show how to approximate this information in sublinear space.
　theorem 1. we can compute an estimator f 1 for eg f1 s   such that with probability at least
1   δ in space
proof:we first analyze the problem to show an algorithm that computes the exact value of eg f1 s   in space linear in m. let st denote the substream of s corresponding only to tuples t  i.e. st = {hti pii （ s ti = t}. let xt be a random variable which captures the distribution of the occurrences of t  ft . since each ht pii （ st can be thought of as defining an  independent  bernoulli random variable  we can write e xt  = ft = pht pii（st pi. moreover  each bernoulli variable has variance equal to pi 1 pi  and by the summation of variances var xt  = pi pi 1   pi . now we have
e xt1  = var xt +e xt 1 = x pi 1 pi +  x pi 1
                                     hti pii（st	hti pii（st for a given t  this sum can be tracked in constant space: let vt = phti pii（st pi and wt = phti pii（st p1i. then we need to compute vt1   wt + vt. each of vt and wt are easy to update as each tuple from s  ht pii  is seen: we set vt ○ vt + pi and wt ○ wt + p1i. finally observe that
	m	m
eg f1 s   = xe xt1  = x vt1 + vt   wt  
　　　　　　　　　　　　　　t=1	t=1 showing that eg f1 s   can be found exactly in space o m .
　in order to compute eg f1 s   in small space  sublinear in the number of domain size  m   we define the random variable y =
. by linearity of expectation 
m
eg f1 s   = e y   = x  x pi   p1i  +   x pi 1 
	t=1 hti pii（st	hti pii（st
the first term is precisely varg f1 s    which we have already shown can be computed exactly in constant space by tracking the sum of all variances of each tuple  i.e. phti pii（s pi 1   pi . the last term is a streaming second frequency moment computation applied to the stream of probabilities s. we can treat s as a nonprobabilistic stream which defines the vector k1 such that
             k1 t  = vt = phti pii（s t=ti pi  and write this term as   the square of the l1 norm of the vector v . using the alon et al. sketching technique  we can find  which is an  estimator for kk1 in space
 1  1 . thus we define our estimator
.
　since varg f1 s   − 1 provided all 1 ＋ pi ＋ 1  the sum of the first and third terms above is a non-negative quantity; hence  summing a  estimator for kk1 with exact values for
varg f1 s   yields an estimator for eg f1 s   that is within a  factor with probability at least 1   δ. in other words  our probabilistic ams technique  pams  is a guaranteed  estimator for eg f1 s  . importantly  the space used is independent of m  the number of distinct items in the stream and |s| = n  the length of the stream.
example 1. we again use the stream 
. here we can compute over all ground streams
e.
by the above analysis  we have  and	and	.
thus  we can confirm that
e.
1 variance of f1 using pams
　to compute e f1 s    we made use of the facts that the expectation of the sum of independent random variables is equal to the sum of the expectations  linearity of expectation   and that the variance of the sum is equal to the sum of the variances  linearity of variance . these are two cases of more general properties of random variables based on the cumulants of the variables  denoted by κj x . the first cumulant of a random variable x  κ1 x   is just the mean of the distribution  e x . the second cumulant is the variance: κ1 x  = var x . higher cumulants can be expressed in terms of the central moments of the variable: for example  κ1 x  = e  x e x  1  and κ1 x  = e  x e x  1  1var x 1.
the important property of cumulants that we will make extensive use of is that  for independent random variables x and y  
	 j （ n.	κj x + y   = κj x  + κj y  
that is  cumulants generalize the linearity of expectation and variance to higher moments of independent variables.
　theorem 1. we can approximate varg f1 s   with additive error in space	.
proof:by expanding and rearranging  we can write
e
e
var x1  = e x1    e x1
= κ1 x  + 1κ1 x κ1 x  + 1κ1 x  + 1κ1 x κ1 x 
　writing b p  for a bernoulli random variable with parameter p  we have the following equalities:
κ1 b p   = e b p   = p κ1 b p   = var b p   = p 1   p  κ1 b p   =  1   1p p 1   p κ1 b p   =  1   1p + 1 p 1   p 
note that since 1 ＋ p ＋ 1  we have
1＋ κ1 b p   ＋κ1 b p   ＋ 1
 κ1 b p  ＋ κ1 b p   ＋κ1 b p  
 κ1 b p  /1＋ κ1 b p   = 1   1κ1 b p   κ1 b p   ＋ κ1 b p  
　thus κ1 b p   and κ1 b p   may be negative  but for any random variable var x1  must be non-negative  indeed  for a bernoulli random variable  b p  = b p 1 = b p 1 and so var b p 1  = var b p   = κ1 b p   . by summation of cumulants  for an rv
xt corresponding to the number of occurrences of t in s  we have κk xt  = phti pii（st κk b pi  .
　we will write vectors kj such that kj t  = κj xt   hence  k1 is as before . by summation of variance  we can write
m
varg f1 s   = xk1 t +1 t k1 t +1 t +1 t k1 t  t=1
　the first term  which we denote κ1 s   can be computed exactly in constant space  as it is a sum of a function of each pi in turn. the other three terms are more complex  but can be dealt with using techniques from computing over deterministic streams. the second and third terms are both the inner-product of vectors of dimension m  where each entry is the sum of values derived from individual tuples in s. these can be approximated using the sketching technique of alon et al. . the fourth term can be thought of as a three-way join between relations encoded as vectors x y z  where it happens that x = y. this can be approximated using the technique of dobra et al. . thus we can build an estimator  by building four ams sketches  one each for the vectors k1 and k1  and two for k1  two are needed to estimate the last term   plus the exact computation of kk1 in constant space. our estimator is formed as:
	d	 	   
where k1 ， k1 is the  approximate  dot product of the vectors
k1 and k1  and is the  approximate  three-way dot product of those vectors. with probability at least 1   δ  the error
|varg f1 s     vard| is at most
.
　by the above observations on the relative size of the cumulants from which the vectors are formed  we have
|κ1 b p  | ＋ κ1 b p   ＋ κ1 b p  .
　each variable xt is formed as the sum of bernoulli random variables  giving |κ1 xt | ＋ κ1 xt  ＋ κ1 xt  and hence kk1 ＋ kk1 ＋ kk1. as a result  the error from the approximation is bounded by . to write this in terms of the quantities we are estimating  we observe that kk1 ＋ kk1 for any stream  and note that e  so
. we note that a similar line of reasoning
using the cumulant representation can show that varg f1 s   ＋
eg f1 s  1  meaning that this bound is reasonable. we conclude that using our pams technique  we can build an estimator in constant space  independent of n and m  so  after rescaling of  by a constant ＋ 1 : pr 
example 1. we study computing  
  to demonstrate the reduction to computation of cumulants. first  over all ground streams we compute e  so .
for the cumulant based approach  we have the vectors
.
thus we confirm that  as claimed  
.
1 expected join size with pams
consider two streams s1 and s1  and we wish to evaluate
eg |s1 ./ s1|   i.e. the expected join size between all possible grounded streams of s1 and s1. this initially seems even more challenging  since we are now reasoning over a space of 1|s1| 〜 1|s1| possible worlds. in fact  we can again use pams sketches.
　theorem 1. in one pass we can compute an estimate j of eg |s1 ./ s1|  such that
with probability at least 1   δ in space.
proof:let k1 s1  and k1 s1  be the vectors of first cumulants as defined above. expanding out the definitions and using the independence of s1 and s1 we find eg s1 ./ s1  = k1 s1 ，k1 s1   the dot product of the two vectors. using sketches again we can compute the estimate and  by properties of
the sketches  the error is bounded by

with probability at least 1   δ using a summary for each stream of size  independent of m and n. this summary can be maintained incrementally as each new hti pii is read.
　note that in the case that each pi is 1  a deterministic stream   this is identical to the regular join size estimation using sketches  as one would hope.
example 1. consider again our example stream  
  and let s1 = s1 = s be two independent streams which happen to have the same distribution. even with this tiny example  it is somewhat laborious to evaluate all 1 possible combinations of grounded streams in order to compute eg |s1 ./ s1|  = . in comparison  it is straightforward to find
         and so compute e.
　note that this is a different answer compared to eg f1 s   on the same stream  whereas for ground stream computations  the size of the join between a stream and itself is equal to the second frequency moment of the stream. there is no contradiction here: note that s1 and s1 are two independent streams. the join of two independent streams with the same distribution is different from the self-join of a single probabilistic stream  i.e. eg f1 s     since the former must consider all possible combinations of ground streams. in fact  we can always be sure that eg |s ./ s|  ＋ eg f1 s    since we can write e  whereas eg f1 s   =
. equality occurs when all probabilities are 1
or 1  i.e. deterministic streams. this highlights an important example where our intuitions from dealing with deterministic streams do not directly carry over to the world s  of probabilistic streams.
1. higher moments estimation
　we extend our approach to higher frequency moments  fk   1  and higher central moments  eg  x   eg x  k   k   1 .
1 higher central moments
the central moments of a distribution are defined as
　　　　　　　ck x  = eg  x   eg x  k  for k （ n. it follows that c1 x  = 1  and c1 x  = e  x  
e x  1  = var x . the higher central moments further define the distribution. by analogy to our earlier definitions  we can define for probabilistic streams and aggregate f:
ck g f  =eg  f s    eg f s   k 
= x pr g  f g    eg f s   k
g（grnd s 
　they can also be written in terms of cumulants. for example  c1 x  = κ1 x   and thus we can compute c1 g f1 s   in constant space. continuing:
c1 x  =κ1 x  + 1κ1 x 1
c1 x  =κ1 x  + 1κ1 x κ1 x 
c
thus c1 g f1 s    c1 g f1 s   and c1 g f1 s   can also be computed exactly in constant space: we can compute each κk g f1 s   exactly in constant space for k ＋ 1  by computing κk b pi   of each individual bernoulli random variable defined by a hti pii tuple  and using the summability of cumulants to get the correct result. this method works for all higher cumulants and central moments κk f1 s   and ck g f1 s    thus we can completely characterize the kth central moments in o k  space.
example 1. consider computing c1 g f1 s    where s =
  as usual. we find e  and
thus eg  f1 s    eg f1 s   1  =
.
we compute  and
.
thus using the cumulants approach we find
	.	
computing c1 g f1 s  . now that we have completely characterized all central moments based on f1 exactly  we turn our attention to central moments based on the f1 aggregate  in section 1 we showed how to estimate c1 g f1 s    that is  varg f1 s    by reducing to the estimation of eg f1 s1   of a derived stream s1. the same approach can work for c1 g f1 s  .
　theorem 1. given an  estimator for eg f1 s    the quantity c1 g f1 s   can be estimated with error at most  with probability at least 1   1δ.
proof:as before  define s1 = hti 1pi p1ii and additionally define s1 = hti 1pi   1i + p1ii. observe that both new streams generate new probabilities which are well behaved  i.e.  lie in the range  1...1 . we claim that
eg f1 s     1eg  f1 s1   + 1eg f1 s1   = c1 g f1 s  .
defining pt = 1   pt  we have e and so  simplifying to the definition of κ1 xt :
e which is c1 g f1 s  . from lemma 1  we have eg f1 s1   ＋ 1eg f1 s  . similarly  we can observe that
1pt   1t + p1t ＋ 1pt   1pt1   p1t 1   pt  ＋ 1pt.
so eg f1 s1   ＋ 1eg f1 s  . consequently  the net error in the estimate of c1 g f1 s   is bounded by  1，1，1 eg f1 s    with probability at least 1   1δ by the union bound.
　we note that the same idea of keeping a single pfm sketch and deriving a sketch pfm1 of s1 and pfm1 of s1 works here  meaning that the estimation can be made using a single sketch.
　similar techniques can be adapted for higher central frequency moments  ck g f1 s    although they become more involved  as the terms become harder to approximate  and the relevant constants increase  so we omit the lengthy details. likewise  we can build estimators for ck g f1 s   by generalizing the technique we used for varg f1 s    but these also become quite involved and are of lesser value compared to eg f1 s   and varg f1 s    so we do not discuss them further here.
1 higher frequency moments
　a similar technique cam be applied to approximate the expected value of higher frequency moments. we focus on eg f1 s    the expected value of the third frequency moment  defined as
eg f1 s   = pg（grnd s  pr g  ， f1 g .
　we make use of properties of the third cumulant  κ1 x   which can also be written as
.
thus  to find eg f1 s   we compute
e
　we address each of these terms in turn. we already know that we can compute κ1 s   the third cumulant of the whole stream  exactly with a single variable. the second term is an inner product between the vectors k1 and k1 that were defined in section 1 on estimating varg f1 s  . using the ams sketching technique  this quantity can be estimated up to error in space
p κ1 xt 1 = pt=1 phti pii（st pi 1  t=1
which is a  generalized version of a  third moment computation of the stream of pi's. in order to approximate this  we define a variation of the generic ams estimator for the more general problem of estimating the fk s  for any k   and prove that it gives the necessary accuracy1. note that the original estimator is only defined for  unary  streams where each arrival of an item counts for an increment of exactly one for the frequency of that item  and so requires some careful analysis and proof.
estimator for fk on streams of fractional values. we define the generalized kth frequency moment as:
.
　note that this is quite distinct from the definition of eg fk  above: this question does not respect the semantics of s being a probabilistic stream  instead treating it as a deterministic stream of fractional non-negative updates. we can think of it as kv kkk  in terms of a vector norm on a vector v defined by the stream. we define an estimator as follows. let pj = phti pii（s i j pj  i.e. pj is eg f1 s   for the stream seen so far. from the stream  randomly sample the t-th tuple  htj pji  with probability pj/pj. this can be done using a weighted version of standard reservoir sampling: track the value of pj = pji=1 pi  and sample the next tuple j  replacing the currently sampled item  with probability pj/pj. it follows inductively that the probability of an item surviving this process is exactly pj/pj  as required. then compute pt j = phti pii i j ti=t pi  the sum of probabilities of all tuples containing t which occur after the sampled j th tuple. we output the estimator
ej =   pj + pt j k   pkt j pj/pj.
　theorem 1. taking the mean of copies of the estimator ej  and the median of o log1/δ  means gives an  approximation kv kkk for fk s  in one pass. 
　thus we can estimate the term in the expression for eg f1 s   in space sublinear in the support size of the stream m  and independent of the stream length n. putting these together  we form our estimator for eg f1 s   as f 1 = . with probability at least 1   δ 
.
note that the κ1 s  term may be negative  however one can show
e  which is non-negative. if we assume that the average value of k1 is no less than 1  i.e.  kk1 − m   and recalling that kk1 ＋ kk1  see proof of theorem 1   we can prove   giving a bound on the error as .
thus  after rescaling  by a constant at most 1  we have a relative error approximation in sublinear space  and so we conclude 
theorem 1. in one pass  we can find an  estimator for
eg f1 s   in space  if eg f1 s   − m.	
1. experimental results
　we implemented our algorithms for the main frequency moments f1  f1 and f1 in c: probabilistic fm  pfm   probabilistic ams
 pams   and the universal sampling algorithm using fm and ams

	 1	 1 1 1 1 1
total space  kb 
figure 1: time for eg f1 s   estimation
estimators  so our results are fairly compared. we exhaustively computed the exact answers to our aggregates  in order to compare them to the approximate values we obtain from our algorithms. our experiments were performed on standard desktop class machines. we found our algorithms to be very efficient  and we report timings as a general guide to relative performance  since the exact behavior will vary depending on optimizations  cache policy and so on.
data sets. we considered a variety of real and synthetic data. for synthetic data  we created each tuple hti pii independently by drawing each ti from a zipfian distribution  and each pi uniform from the range  1 . this captures many settings where data is drawn from distributions with skew  and with a variety of uncertainty values. we vary the skewness parameter of the zipfian distribution from z = 1  uniform  to z = 1  highly skewed . in each experiment  we generated 1 tuples from this distribution. we also used a real data set from the mystiq project1 which includes approximately 1 probabilistic tuples. each tuple links a film in the imdb database to a product in amazon.com's inventory  and includes a  probability of match  value. so eg f1 s   is the expected number of  distinct  matched titles matched  and eg f1 s   is the expected self-join size.
experiments on f1 estimation. our experiments on f1 estimation are shown in figure 1. we see that  given a fixed amount of space  the universal sampling algorithm consistently obtains an approximation of eg f1 s   that is within a few percentage points  while the probabilistic fm approach is typically between 1% to 1% relative error from uniform to skewed data  figure 1 a  . partly this is due to the space efficiency of the algorithms: the universal algorithm can use fm sketches which are based on bitmaps  whereas for each bit in a regular fm sketch the pfm structure uses a 1 bit integer; thus  it uses 1 times as many structures in the same space. the trend is for the accuracy to improve as more space becomes available  figure 1 b  . because the fm algorithm is itself randomized  accuracy does not improve uniformly with more space  but rather the trend is for the variability to decrease as more space is used  as seen more clearly in figure 1 c . the conclusion appears to be that for the task of estimating eg f1 s    the universal algorithm is the most accurate within a given space bound. however  there is a price to pay in terms of the time cost. since it has 1 times as many data structures  the time to update is correspondingly slower. figure 1 shows that processing time increases linearly with space for both algorithms  as predicted by our analysis. but the universal algorithm is about thirty times slower to process a million tuples in our implementation  it manipulates bit vectors instead of floating-point numbers  so is not fully 1 times slower .

	 a  eg f1 s    1kb space	 b  eg f1 s    zipfian data	 c  eg f1 s    imdb data

	 d  varg f1 s    1kb space	 e  varg f1 s    zipfian data	 f  varg f1 s   and varg f1 s    imdb data
figure 1: results on eg f1 s   and varg f1 s   estimation on real and synthetic data　when it comes to computing varg f1 s    the tables are turned  and quite dramatically so. the probabilistic fm algorithm consistently achieves a relative accuracy within 1% with a moderate amount of memory  recall  its guarantee is in terms of eg f1 s    not varg f1 s   . meanwhile  computing the variance of the samples found by the universal algorithm yields estimates that are many orders of magnitude adrift. the reason for this is given in section 1: by studying the output of the algorithms  we verified that it obtains a very accurate estimate of eg f1 s  1 and eg f1 s  1. but these terms are both very large compared to varg f1 s    so the final error becomes huge  in relative terms. meanwhile  the probabilistic fm algorithm  which gives a guarantee on its error proportional to eg f1 s   instead of eg f1 s  1  can give much better estimates. this is true across a broad range of synthetic data  figures 1 d  and 1 e    and on real data  figure 1 f    with a trend for improving accuracy as space increases.
experiments on f1 estimation. our experimental results on f1 estimation are shown in figure 1. again  the universal algorithm does well with an average of 1% error for estimating eg f1 s   on all data types  and improving accuracy as the given space increases  figures 1 b  and 1 c  . data with moderate skew seems to be the most challenging for the probabilistic ams algorithm  which obtained accuracy of better than 1% throughout. timing results are quite comparable: our implementation of pams used 1 bit floating point values where the universal algorithm uses the same structure but with 1 bit integers. so the universal algorithm did twice as many hashing operations  but then uses integer arithmetic to update counts. on our set up  the time cost was about the same for both algorithms: approximately 1 second for our algorithms to completely process a data set of 1 items for eg f1 s   estimation  independent of the total summary size. this increases to 1 seconds for varg f1 s    since our algorithm requires four sketches: one for each of the vectors k1 and k1  and two for k1  to perform the estimation of k1 ，k1 ，k1 . thus either algorithm scales well to large quantities of data and restricted resources.
　again for varg f1 s   estimation  we see the limitation of the universal algorithm. only when the data set is extremely skewed  z   1  does it have a chance of making any reasonable estimate. this is for the same reason as in the f1 case: the error in its estimate is proportional to eg f1 s  1  which is very large in comparison to varg f1 s  . our analysis shows that the error in pams estimation depends in the worst case on eg f1 s  1  which is enough of an edge to make its estimates quite tolerably accurate. the hard case for both algorithms is on more uniform data. figure 1 e  shows that increasing the space for probabilistic fm improves the accuracy for the 'hard' data  while the error on more skewed data is close to zero throughout. the same conclusion is shown on the imdb data in figure 1 f . likewise  increasing the amount of space available did tend to improve the accuracy of the universal algorithm  but not to sufficiently reliable levels even for highly skewed data. interestingly  increasing the number of ground streams used to make the estimate did not seem to reliably improve the quality  and in fact tended to reduce the accuracy.

	 a  eg f1 s    1kb space	 b  eg f1 s    zipfian data	 c  eg f1 s    imdb data

	 d  varg f1 s    1kb space	 e  varg f1 s    zipf skewness = 1	 f  varg f1 s  
figure 1: results on eg f1 s    varg f1 s   and varg f1 s   estimation　lastly  we looked at estimating varg f1 s   using the universal algorithm. note that we can compute this exactly in constant space using our analysis from section 1. however  this was a useful example to study since there is no error introduced in computing f1 of the ground stream: it is precisely the number of items observed in that stream  consequently  the skewness of the synthetic zipfian distribution is irrelevant for this aggregate . figure 1 f  shows that increasing the number of ground streams does seem to improve the accuracy  but somewhat slowly and unreliably. even with 1 ground streams  the expected accuracy is above 1% relative error. for other aggregates  drawing even a hundred ground streams is not always be feasible  since this entails storing 1 separate ams or fm summary structures  each of which is typically 1s of kbs or more. moreover  the throughput decreases by a factor of 1s compared to the probabilistic sketching methods we propose here.
experimental conclusions. from our experiments  we conclude that  given a fixed amount of space  the universal sampling algorithm obtains high accuracy for computing the first central moment  such as eg f1 s   or eg f1 s  . but  some cases such as eg f1 s   it can be many times slower than the probabilistic equivalent  which is somewhat less accurate. for the second central moment  varg f1 s   and varg f1 s    we can do dramatically better using methods tailored to the aggregate. this seems the only way to obtain efficient estimates of these quantities  since the amount of space to estimate them well using the universal algorithm is greater than the space needed to run the exact algorithm.
