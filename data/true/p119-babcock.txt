research on query optimization has focused almost exclusively on reducing query execution time  while important qualities such as consistency and predictability have largely been ignored  even though most database users consider these qualities to be at least as important as raw performance. in this paper  we explore how the query optimization process can be made more robust  focusing on the important subproblem of cardinality estimation. the robust cardinality estimation technique that we propose allows for a user- or application-specified trade-off between performance and predictability  and it captures multi-dimensional correlations while remaining space- and time-efficient.
1. introduction
¡¡from a system management point of view  the consistency and predictability of a database management system are very important. this is particularly true since the dbms is typically just one component of a larger system involving many application programs. tuning and testing of the system as a whole is greatly simplified when its components behave predictably. despite the importance of consistency and predictability  these qualities have received relatively little attention from database researchers and implementors  as compared to raw performance considerations. one goal of this paper is to argue that the oft-overlooked qualities that lead to maintainable systems should be formulated as important objectives for database systems. our results develop this principle in the context of query planning: we illustrate how broader  system-level considerations such as predictability can be incorporated into the query optimization process to produce a more robust query optimizer.
¡¡the task of the query optimizer is to select a low-cost query plan  but only incomplete and imprecise information about query plan costs is available to the optimizer at query compilation time. the standard approach to query optimization is as follows: first  generate rough guesses as to the values of the relevant cost model parameters  using rules of thumb or extrapolating from any available statistics. next  using the rough guesses as inputs  invoke a search algorithm to find the least costly plan. the search phase typically treats the estimated parameter values as though they were

 work performed while visiting microsoft research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  baltimore  maryland  usa.
copyright 1 acm 1-1/1 $1.
completely precise and accurate  rather than the coarse estimates that they actually are. not surprisingly  query optimization has acquired the dubious repuatation of being something of a black art.
¡¡in this paper  we argue for an alternative approach to query optimization. unlike the standard approach  which ignores the uncertainty about the values of important cost parameters  our approach uses probabilistic reasoning to acknowledge uncertainties in the query planning process in a principled manner. consequently  our approach is capable of producing query plans that are more robust to estimation errors and changes in the runtime environment.
