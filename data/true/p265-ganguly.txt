there is growing interest in algorithms for processing and querying continuous data streams  i.e.  data that is seen only once in a fixed order  with limited memory resources. in its most general form  a data stream is actually an update stream  i.e.  comprising data-item deletions as well as insertions. such massive update streams arise naturally in several application domains  e.g.  monitoring of large ip network installations  or processing of retail-chain transactions .
모estimating the cardinality of set expressions defined over several  perhaps  distributed  update streams is perhaps one of the most fundamental query classes of interest; as an example  such a query may ask  what is the number of distinct ip source addresses seen in passing packets from both router r1 and r1 but not router r1  . earlier work has only addressed very restricted forms of this problem  focusing solely on the special case of insert-only streams and specific operators  e.g.  union . in this paper  we propose the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general update streams. our estimation algorithms are probabilistic in nature and rely on a novel  hash-based synopsis data structure  termed  1-level hash sketch . we demonstrate how our 1level hash sketch synopses can be used to provide low-error  high-confidence estimates for the cardinality of set expressions  including operators such as set union  intersection  and difference  over continuous update streams  using only small space and small processing time per update. furthermore  our estimators never require rescanning or resampling of past stream items  regardless of the number of deletions in the stream. we also present lower bounds for the problem  demonstrating that the space usage of our estimation algorithms is within small factors of the optimal. preliminary experimental results verify the effectiveness of our approach.
1. introduction
query-processing algorithms for conventional database
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
management systems  dbms  rely on  possibly  several passes over a collection of static data sets in order to produce an accurate answer to a user query. for several emerging application domains  however  updates to the data arrive on a continuous basis  and the query processor needs to be able to produce answers to user queries based solely on the observed stream of data and without the benefit of several passes over a static data image. as a result  there has been a flurry of recent work on designing effective query-processing algorithms that work over continuous data streams to produce results online while guaranteeing  1  small memory footprints  and  1  low processing times per stream item  1  1  1  1 . such algorithms typically rely on summarizing the data stream s  involved in concise synopses that can be used to provide approximate answers to user queries along with some reasonable guarantees on the quality of the approximation.
모in their most general form  real-life data streams are actually update streams; that is  the stream is a sequence of updates to data items  comprising data-item deletions as well as insertions 1. such continuous update streams arise naturally  for example  in the network installations of large internet service providers  where detailed usage information  snmp/rmon packet-flow data  active vpn circuits  etc.  from different parts of the underlying network needs to be continuously collected and analyzed for interesting trends. other application domains giving rise to continuous and massive update streams include retail-chain transaction processing  e.g.  purchase and sale records   atm and creditcard operations  logging web-server usage records  and so on. the processing of such streams follows  in general  a distributed model where each stream  or  part of a stream  is observed and summarized by its respective party  e.g.  the element-management system of an individual ip router  and the resulting synopses are then collected  e.g.  periodically  at a central site  where queries over the entire collection of streams can be processed . this model is used  for example  in lucent's interprenet and cisco's netflow products for ip network monitoring.
모clearly  there are several forms of queries that users or applications may wish to pose  online  over such continuous update streams; examples include joins or multi-joins  1  1   norm computations  1  1   or quantile estimation . perhaps one of the most fundamental queries of interest is estimating the result cardinalities of set expressions defined

1
 item modifications are simply seen as a deletion directly followed by an insertion of the modified item.
over several update streams. as an example  an application monitoring active ip-sessions may wish to correlate the ipsession sources seen at routers r1  r1  and r1 by posing a query such as:  estimate the number of distinct ip addresses seen at both r1 and r1 but not r1 . this is simply the number of distinct elements  i.e.  set cardinality  for the  multi- set  source r1 뫌source r1   source r1   where source ri  is the multi-set of ip source addresses seen at router ri. the ability to effectively estimate the cardinality of such set expressions over the observed streams of updates for ip-session data in the underlying network can be crucial in quickly detecting possible denial-of-service attacks  network routing or load-balancing problems  potential reliability concerns  catastrophic points-of-failure   and so on. set expressions are also an integral part of query languages for relational database systems; for example  the sql standard supports set operators like union  intersect  and except  i.e.  difference  in queries over tables with compatible schemas . thus  one-pass synopses for effectively estimating set-expression cardinalities can be extremely useful  e.g.  in the optimization of such queries over terabyte relational databases.
prior work. estimating the cardinality of set union  i.e.  number of distinct elements  over  one or more  element streams is a very basic problem with several practical applications  e.g.  query optimization ; as a result  several solutions have been proposed in the literature for the setunion estimation problem. in their influential paper  flajolet and martin  propose a randomized estimator for distinct-element counting that relies on a hash-based synopsis data structure; to this date  the flajolet-martin  fm  technique remains one of the most effective approaches for this estimation problem. the analysis of flajolet and martin makes the  unrealistic  assumption of an explicit family of hash functions exhibiting ideal random properties; in a later paper  alon et al.  present a more realistic analysis of the fm estimator that relies solely on simple  linear hash functions. several estimators based on uniform random sampling have also been proposed for distinct-element counting  1  1 ; however  such sampling-based approaches are known to be inaccurate and substantial negative results have been shown by charikar et al.  stating that accurate estimation of the number of distinct values  to within a small constant factor with constant probability  requires nearly the entire data set to be sampled! more recently  gibbons et al.  1  1  have proposed specialized sampling schemes specifically designed for distinct-element counting; their sampling schemes rely on hashing ideas  similar to   to obtain a random sample of the distinct elements in the input streams that is then used for estimation. finally  baryossef et al.  propose improved distinct-count estimators that combine new techniques and ideas from  1  1  1 .
모all these earlier papers on set-union estimation either ignore the possibility of deletions in the input stream s  or fail to deal with deletions in a completely satisfactory manner. for example  sampling-based solutions like  1  1  may very well require rescanning and resampling of past stream items when deletions cause the maintained sample to be depleted; this is clearly an unrealistic requirement in a data-streaming environment. more importantly  none of the above-mentioned papers addresses the problem of dealing with general set expressions  including operators like set intersection or difference   which is obviously significantly more complex than simple set union.
모the method of minwise independent permutations  mips   1  1  1  is  to the best of our knowledge  the only known technique that can accurately estimate the result cardinalities of set operators other than union  e.g.  intersection  over an insertion stream rendering a multi-set of data items. furthermore  extending the basic technique to deal with set expressions is relatively straightforward  e.g.  see  . unfortunately  mips are also ill-equipped for dealing with general update streams. deletions can easily deplete the mip synopsis  rendering it useless for the purposes of set-expression estimation unless we are able to rescan past stream items; again  however  this is not a realistic option in a data-stream setting.
our contributions. in this paper  we present the first space-efficient algorithmic solution for the full-fledged problem of estimating set-expression cardinalities over general update streams. our proposed estimators are probabilistic in nature and rely on a novel  hash-based synopsis data structure  termed  1-level hash sketch . we present novel estimation algorithms that use our 1-level hash sketch stream synopses to provide low-error  high-confidence estimates for the cardinality of general set expressions  including set union  intersection  and difference operators  over continuous update streams  using only small space and small processing time per update. we also present lower bounds demonstrating that the space usage of our basic estimators is within small factors of the best possible for any  randomized  solution. furthermore  our estimators never require rescanning or resampling of past stream items  regardless of the number of deletions in the stream: at any point in time  our 1-level hash sketch summary is guaranteed to be identical to that obtained if the deleted items had never occurred in the stream! more concretely  the key contributions of our work are summarized as follows.
  novel 1-level hash sketch synopses and basic setoperator estimators over update streams. we formally introduce the 1-level hash sketch synopsis data structure and describe its maintenance over a continuous stream of updates  rendering a multi-set of data elements . briefly  1-level hash sketches extend the hash-based synopses of flajolet and martin  in a non-trivial manner that renders them  a  robust to item deletions in the stream  and  b  useful for estimating the cardinalities of set difference and intersection  in addition to set union . we then present novel algorithms for  probabilistically  estimating the cardinalities of the three basic set operations  union  difference  and intersection  over 1-level hash sketches. to simplify the analysis of our basic estimators  we initially assume ideal  fully-independent hash mappings for 1-level hash sketch construction; we then demonstrate how our analysis carries over to the  more realistic  limited-independence case. we also prove a lower bound for all randomized approximation algorithms  showing that the space requirements of our estimators is actually within small polynomial and log factors of the optimal.
  extension to general set-expression estimation over update streams. we generalize our basic-operator estimators  and their analysis  to derive an accurate  smallspace  probabilistic  estimation algorithm for the cardinality of general set expressions over a collection of continuous update streams. once again  ours is the first approach to solve this estimation problem for arbitrary update streams  while guaranteeing that no access to past stream items will ever be needed. furthermore  even though we present our estimators in a single-site setting  our solution also naturally extends to the more general  distributed-streams model with stored coins  of gibbons and tirthapura .
  experimental results validating our methodology. we present preliminary results from an experimental study with different synthetic data sets that verify the effectiveness of our 1-level hash sketch synopses and estimation algorithms. the results substantiate our theoretical claims  demonstrating the ability of our techniques to provide space-efficient and accurate estimates for set-expression cardinality queries over continuous streaming data.
1. preliminaries
모in this section  we discuss the basic elements of our updatestream processing architecture and introduce some key concepts and notation for our estimation algorithms. we also describe the hash-based flajolet-martin  fm  distinct-value count estimator in more detail  as it will provide the basis for our 1-level hash sketch synopses  introduced in section 1 .
1 update-stream processing model
모the key elements of our update-stream processing architecture for set-expression estimation are depicted in figure 1; similar architectures for processing data streams have been described elsewhere  see  for example   .

figure 1: update-stream processing architecture.

모without loss of generality  we assume that each input stream renders a multi-set ai of elements from the integer domain  m  = {1 ...  m   1} as a continuous stream of updates.  to simplify the exposition  we also assume that m is a power of 1.  each such update is a triple of the form   i e 찼    where i identifies the multi-set ai being updated  e 뫍  m  denotes the specific data element whose frequency changes  and 찼 is the net change in the frequency of e in ai  i.e.   +v     v   denotes v insertions  resp.  deletions  of e. we assume that all deletions in our update streams are legal; that is  an update   i e  v   can only be issued if the net frequency of e in ai is at least v. we also let n denote an upper bound on the total number of data elements  i.e.  the sum of element frequencies  in any multi-set ai. in contrast to conventional dbms processing  our stream processor is allowed to see the update tuples for each ai only once and in the fixed order of arrival as they stream in from their respective source s . backtracking over an update stream and explicit access to past update tuples are impossible.
모given a set expression e over the multi-set streams ai  we use |e| to denote the number of distinct elements in e whose net frequency is greater than zero; for example  |a1 뫋 a1| is the number of distinct elements in the union of streams a1 and a1. our stream-processing engine is allowed a certain amount of memory  typically significantly smaller than the total size of its input s . this memory is used to maintain a concise synopsis for each update stream ai. the key constraints imposed on such synopses are that:  1  they are much smaller than the number of elements in ai  e.g.  their size is logarithmic or polylogarithmic in |ai| ; and   1  they can be easily maintained  during a single pass over the update tuples for ai  in the  arbitrary  order of their arrival. at any point in time  given an arbitrary set expression e over the ai's  our set-expression cardinality estimator can combine the maintained collection of synopses to produce an estimate for |e|.
모even for the simpler case of insert-only streams  communication complexity arguments can be applied to show that the exact computation of set-expression cardinalities requires at least   m  space1  even for randomized algorithms  1  1 . instead  our focus is to approximate the quantity x = |e| to within a small relative error  with high confidence. thus  we seek to obtain a  randomized  approximation scheme  1  1   that computes an estimate x  of x such that pr.
1 the flajolet-martin distinct-count estimator
모the flajolet-martin  fm  technique  for estimating the number of distinct elements  i.e.  set-union cardinality  over a stream of insertions relies on a family of hash functions h that map incoming data values uniformly and independently over the collection of binary strings in the input data domain  m . it is then easy to see that  if h 뫍 h and lsb s  denotes the position of the least-significant 1 bit in the binary string s  then for any i 뫍  m   lsb h i   뫍
{1 ...  log m   1} and pr basic hash synopsis maintained by an instance of the fm algorithm  i.e.  a specific choice of hash function h 뫍 h  is simply a bit-vector of size 붣 logm . this bit-vector is initialized to all zeros and  for each incoming value i in the input multi-set ai  the bit located at position lsb h i   is turned on. of course  to boost accuracy and confidence  the fm algorithm employs averaging over several independent instances  i.e.  r independent choices of the mapping hashfunction h 뫍 h and corresponding synopses . the overall fm algorithm is depicted in figure 1.
모intuitively  the fm algorithm works since  by the properties of the hash functions in h  we expect a fraction of of the distinct values in ai to map to location l in each synopsis; thus  we expect |ai|/1 values to map to bit 1  |ai|/1 to map to bit 1  and so on. therefore  the location of the leftmost zero in a bit-vector synopsis is a good indicator of

1
 the asymptotic notation f n  =   g n   is equivalent to g n  = o f n  . similarly  the notation f n  = 붣 g n   means that functions f n  and g n  are asymptotically equal  to within constant factors ; in other words  f n  = o g n   and g n  =
o f n   . 1
all log's in this paper denote base-1 logarithms.

procedure estimatedistinctfm  s  {h1   ...  hr  }  
input: stream s of data items  i.e.  insertions  in the domain  family of randomizing hash functions
output: estimate r of the number of distinct values in s.
begin
1. for i := 1 to r do
1. bitsketchi :=  1 ...  1 	// bit-string of length 붣 logm 
1. for each j 뫍 s do
1. for i := 1 to r do bitsketchi hi j   := 1
1. for i := 1 to r do
1. for m := logm   1 downto 1 do
1. if bitsketchi m  = 1 then leftmostzero := m
1. sum := sum + leftmostzero
1. endfor
1. r := 1 뫄 1sum/r 1. return  r   end
figure 1: the flajolet-martin distinct-count estimation procedure.

log |ai|. in fact  flajolet and martin proved that the estimation procedure depicted in figure 1 is guaranteed to return an unbiased estimate for |ai|  i.e.  the expected value of the returned quantity r is e r  = |ai| .
모the analysis of flajolet and martin actually assumes the existence of an explicit family of hash functions h exhibiting some ideal random properties  namely  fully-independent value mappings  ; unfortunately  such hash functions are impossible to compute in small space. alon et al.  present a more realistic analysis of a very similar scheme  based again on bit-vector hash synopses  that relies solely on linear hash functions  guaranteeing only pairwise independence . such hash functions can be computed using only a seed of size o log m  and  as shown in   produce synopses that guarantee a distinct-value estimate that is within a constant multiplicative factor with constant probability.
1. processing set operators over update streams
모in this section  we describe the key ideas underlying our proposed solution for processing set expressions over continuous update streams. we begin by defining our basic synopsis data structures  termed 1-level hash sketches  and the algorithm for maintaining a 1-level hash sketch over a streams of updates  insertions/deletions  to an input multiset. we also describe some procedures for testing  with high probability  certain elementary properties over our 1-level hash sketch synopses that are used as basic primitives in our set-operator routines. we then present our estimation algorithms for processing the three basic set operations  set union  set difference  and set intersection  over 1-level hash sketch synopses. our algorithm for union can utilize a simple extension of the fm hashing data structure  so it does not actually require the full power of our 1-level hash sketch synopses.  the case of set union is not the focal point of this paper  as the union sub-problem has already been extensively treated in the literature; however  for the sake of homogeneity  we do present a novel algorithm in the context of our sketches and describe its analysis.  the role of 1-level hash sketches becomes critical in our algorithms for estimating set difference and intersection; to the best of our knowledge  ours is the first approach to provide lowerror  high-confidence probabilistic estimates for these two set operators for general update streams with arbitrary deletions  without ever requiring resampling or rescanning of the stream .
모to simplify the exposition in this section  we first present and analyze our estimation schemes assuming ideal randomizing hash functions that guarantee fully-independent value mappings. then  in section 1  we demonstrate some key statistical lemmas that enable all our results to carry over to the more realistic limited-independence case. more specifically  we show that our analysis can be carried out assuming only  -wise independence  where  denotes the relative-error guarantee provided by our techniques.  an
 -wise independent randomizing hash function over
 m  can be implemented using only space
with standard techniques  1  1 .  finally  section 1 presents a lower bound on the space usage of any randomized setoperator cardinality estimation algorithm showing that our estimators are within small factors of the best possible solution.
1 ourstreamsynopsis: 1-levelhashsketches
모our proposed synopsis data structure  termed 1-level hash sketch  is a generalization of the basic bit-vector hash synopsis proposed by flajolet and martin for distinct-value estimation . 1-level hash sketch synopses rely on two distinct  independent families  i.e.  levels  of hash functions h and g. the first-level hash functions h 뫍 h are randomizing hash functions that map  m  uniformly onto a range  mk   i.e.  h :  m  뫸  mk    where k is a small integer constant  e.g.  k = 1  used to guarantee that the h mapping over the elements of  m  is injective with high probability. on the other hand  second-level hash functions g 뫍 g randomize the domain values in  m  uniformly over the binary domain  = {1}  i.e.  g :  m  뫸  .
모a 1-level hash sketch uses one randomly-chosen first-level hash function h 뫍 h and s independently-chosen secondlevel hash functions g1 ...  gs 뫍 g  where s is a parameter of the 1-level hash sketch. as in the flajolet-martin algorithm  figure 1   the first-level hash function h is used in conjunction with the lsb operator to map the domain values in  m  onto a logarithmic range {1 ...  붣 logm } of first-level buckets with exponentially decreasing probabilities. then  each of the s second-level hash functions gi  i = 1 ...  s  is applied to the collection of elements mapping to a given first-level bucket to further map each element to one of two second-level buckets  i.e.  1 or 1  and the corresponding element counter. conceptually  a 1-level hash sketch for a streaming multi-set a can be seen as a threedimensional array xa of size 붣 log m  뫄 s 뫄 1  where each entry xa i1 i1 i1  is a data-element counter of size o log n . the structure of our 1-level hash sketch synopses is pictorially depicted in figure 1.
maintenance. the algorithm for maintaining a 1-level hash sketch synopsis xai over a stream of updates to a multi-set ai is fairly simple. the sketch structure is first initialized to all zeros and  for each incoming update   i e 찼    the element counters at the appropriate locations of the xai sketch are updated; that is  for each j = 1 ...  s  we simply set xai lsb h e   j gj e   := xai lsb h e   j gj e    v. note here that our 1-level hash sketch synopses are essentially impervious to delete operations; in other words  the sketch obtained at the end of an update stream is identical

figure 1: our 1-level hash sketch synopses.

to a sketch that never sees the deleted items in the stream.
모we now proceed to describe our algorithms for processing basic set operators over update streams using our 1level hash sketch synopses and their analysis. as mentioned earlier  to simplify the exposition  our analysis initially assumes ideal  fully-independent first-level hash functions.  for second-level hash mappings  simple pair-wise independence is sufficient for our analysis  lemma 1 . 
1 elementary property checks
모our basic set-operator estimators rely on checking certain elementary properties for the collection of elements that map to a given first-level bucket in a 1-level hash sketch synopsis. we now describe the procedures for performing these elementary property checks. briefly  the key idea here is to make use of the second-level information maintained for the first-level bucket in question; of course  given the space limitations for our 1-level hash sketches  the implications made are necessarily probabilistic  with high confidence for sufficiently large s; see lemma 1 .
모our three procedures for checking elementary 1-level hash sketch properties  termed singletonbucket  identicalsingletonbucket  and singletonunionbucket  are depicted in figure 1. procedure singletonbucket x  i  returns true iff the collection of distinct elements mapping to the ith first-level bucket is a singleton  i.e.  contains only one distinct element . given two 1-level hash sketches xa and xb  for update streams a and b  built using the same first- and second-level hash functions  procedure identicalsingletonbucket xa  xb  i  returns true iff the ith first-level buckets for both xa and xb  1  are singletons  and  1  contain the exact same distinct value from  m . finally  procedure singletonunionbucket xa  xb  i  returns true iff the set union of the elements from a and b mapping to the ith first-level bucket of xa and xb is a singleton.
모the key intuition behind our three property checkers is that they employ the randomizing properties of the secondlevel binary hash signatures in a 1-level hash sketch to derive a high-confidence answer about properties of the element collection in the corresponding first-level bucket. as the following lemma demonstrates  our procedures are guaranteed to draw a valid conclusion with a confidence of at least 1   붻 as long as the number of  independent  second-level hash functions s is 붣 log 붻1  and each such function gi is  at least  pair-wise independent.
모lemma 1. procedures singletonbucket  identicalsingletonbucket  and singletonunionbucket return the correct value with procedure singletonbucket  x  i  
input: 1-level hash sketch x  first-level bucket index i.
output: true iff ith bucket is a singleton.
begin
1. if  x i 1  + x i 1  = 1  return false // bucket is empty
1. unique := true; j := 1
1. while   unique and j 뫞 s   do
1. if   x i j 1    1 and x i j 1    1   then
1. unique := false	// at least two elements in this bucket
1. j := j + 1
1. endwhile 1. return  unique   end
procedure identicalsingletonbucket  xa  xb  i   input: 1-level hash sketches xa  xb  first-level bucket index i. output: true iff ith buckets in xa and xb contain the same singleton value.
begin
1. if  not singletonbucket xa  i   or  not singletonbucket xb  i   then
1. return  false 
1. same := true; j := 1
1. while   same and j 뫞 s   do
1. if    xa i j 1    1  1=  xb i j 1    1  or
 xa i j 1    1  1=  xb i j 1    1    then
1. same := false	// buckets differ in at least one element
1. j := j + 1
1. endwhile 1. return  same   end
procedure singletonunionbucket  xa  xb  i   input: 1-level hash sketches xa  xb  first-level bucket index i. output: true iff the union of ith buckets in xa and xb is a singleton.
begin
1. if    singletonbucket xa  i  and  xb i 1  + xb i 1  = 1   or  singletonbucket xb  i  and  xa i 1  + xa i 1  = 1     then
1. return  true  	// one singleton and one empty bucket
1. else return  identicalsingletonbucket xa  xb  i    end
figure 1: elementary property check procedures for 1-level hash sketches.

probability at least 1   붻 if the number of second-level hash functions is s = 붣 log 1붻  and each gi is m-wise independent with m 뫟 1.
proof  sketch : consider  as an example  the singletonbucket procedure. in order to check whether a first-level bucket contains just a single distinct value  the procedure essentially checks each one of the s second level bucket pairs to see if there is at least one pair with both counts positive; if such a pair cannot be found  then the procedure concludes that the bucket is a singleton. assume that the procedure returns an erroneous conclusion. clearly  if the bucket is truly a singleton  the singletonbucket procedure will always return the correct answer; so  the only possible error occurs when the bucket contains at least two distinct values but the singletonbucket procedure concludes that the bucket is a singleton. the only way this can happen is if  for each one of the s second-level bucket pairs  the corresponding hash function maps the two distinct element values to the same binary value  1 or 1 . by pair-wise independence  this happens with probabilityfor a given second-level hash function gj; thus  by the independence of the gj's the proba-
procedure setunionestimator
input: r independent 1-level hash sketch pairs {xa xb} for streams a and b  relative accuracy parameter .
output: estimate for |a 뫋 b|.
begin
1. 
1. index := 1
1. while   true   do
1. count := 1
1. for i := 1 to r do
1. if   xai   index  1  + xai   index  1    1   or
  xbi   index  1  + xbi   index  1    1   then
1. count := count +1
1. endfor
1. if   count 뫞 f   then break // first index with count 뫞 f
1. else index := index +1
1. endwhile
1. p  := count / r ; r := 1index+1
1. return end
figure 1: set-union cardinality estimator.

bility of an erroneous singleton conclusion is upper-bounded by 
1 the set-union estimator
모given two multi-sets of elements a and b in the form of continuous update streams  the set-union cardinality |a뫋b| is the number of distinct elements with positive net frequency in either a or b. we present an  -estimator for the set-union cardinality |a 뫋 b| based on maintained 1-level hash sketch synopses xa and xb for streams a and b  respectively. our set-union algorithm does not actually require the full power of 1-level hash sketches  since it does not need to make use of any second-level hash structures. thus  our union estimator can work by simply maintaining a single counter  of size o log n   for each of the 붣 log m  first-level hash buckets  figure 1 . 1 this means that  in the simpler case of set union  we can use a simple extension of the basic fm hash data structure.
모our algorithm for producing an  -estimator for the union of two update streams a and b  termed setunionestimator  is shown in figure 1. briefly  our estimator examines an input collection of r independent 1-level hash sketch synopses built over a and b in parallel  each copy using independently-chosen first- and second-level hash functions from h and g  in order to determine the smallest first-level bucket index at which only a constant fraction of the sketch buckets turns out to be non-empty for the union a 뫋 b  steps 1 .  note that the non-empty if-condition in step 1 can be checked by simply maintaining a single element counter at the corresponding first-level bucket; we present the algorithm assuming full 1-level hash sketch synopses for uniformity.  as our analysis shows  the observed fraction p  step 1  of non-empty first-level hash buckets can be used to provide an estimate for the probability of observing a non-empty bucket at this level of the sketch which  in turn  allows us to give a robust estimate for |a뫋b|  step 1 . analysis. we first demonstrate that  when the number of

1
 of course  for the general set-expression estimation problem considered in this paper  we need to build full 1-level hash sketch synopses for each update stream  since we do not know a-priori how a stream will be used in an incoming expression  figure 1 .
independent input sketches is   algorithm setunionestimator terminates with a bucket index for which the non-empty bucket count satisfies  count
with probability at least 1   붻. consider a specific level j of first-level hash buckets and let pj denote the probability that bucket j is non-empty for a 뫋 b  i.e.  bucket j is not empty in either xa or xb. by independence  this probability is exactly pj = 1    1   1/rj u  where u = |a 뫋 b| and rj = 1j+1. a simple application of the binomial expansion gives us that u/rj    1  u/rj 1 뫞 pj 뫞 u/rj.
모now  fix j to be a positive integer such that 1   u/rj 뫞 1; for this value j of the bucket index  the above bounds for the probability pj = p give 1   p 뫞 1. note that the ratio count/r at this level j is essentially an average over r independent observations of the 1 random variable corresponding to pj = p. by chernoff bounds  the estimate p j = p  = count/r at level j satisfies 
with probability at least 1   붻 as long as   or  since . consequently  with this value j of the bucket index and   our
setunionestimator procedure findswhich implies that  count  with probability at least 1 붻
 since 1   p 뫞 1 at this level j .
모thus  with probability 뫟 1   붻  setunionestimator finds a level j such that count and the ratio p j = count/r satisfies. the following lemma then demonstrates that  for pj 뫞 1  we can directly substitute the estimate p j in the equation pj = 1    1   1/rj u and solve for u without any significant change in the relative accuracy guarantees.  a similar lemma is proven in   even though their proposed estimation technique is quite different from ours. 
   lemma 1. let f x  = log 1   x /log 1   1/r . if |y    for 
proof: by taylor series  there is a value in w 뫍  x y  such that ln  1   y  = ln  1   x     y   x / 1   w   ln denotes the natural logarithm function . thus  we have:
.
now  since  which gives:   /r |f f | 뫞 뫞 x .
since  for any x  ln x = log x 몫 ln 1  the result follows.
모we summarize the results of the above analysis in the following theorem.
모theorem 1. procedure setunionestimator returns an estimate for the size of the set union |a 뫋 b| of two update streams a and b using 1-level hash sketch synopses with a total storage requirement of .
1 the set-difference estimator
모given two multi-sets a and b presented as a continuous stream of updates  the cardinality of the set difference of a and b  i.e.  |a b|  is defined as the number of distinct element values whose net frequency is positive in a and zero in b. in this section  we present an  -approximation scheme for estimating the set-difference operator over two update streams a and b based on their maintained 1-level
procedure setdifferenceestimator
input: r independent 1-level hash sketch pairs {xai  xbi } for streams a and b  set-union cardinality estimate u   relative accuracy parameter . output: estimate for |a   b|. begin
1. sum := count := 1
1. for i := 1 to r do
1. atomicestimate := atomicdiffestimator  xai   xbi   u  
1. if   atomicestimate 1= noestimate  then
1. sum := sum + atomicestimate; count := count +1
1. endif
1. endfor
1. return  sum 뫄 u  / count   end
procedure atomicdiffestimator begin
1. index := 	// 붹 is constant   1  see analysis 
1. if   not singletonunionbucket xai   xbi   index    then
1. return  noestimate 
1. estimate := 1
1. if   singletonbucket xai   index  and
 xbi   index  1  + xbi  index  1  = 1    then
1. estimate := 1	// found witness of a   b
1. return  estimate   end
figure 1: set-difference cardinality estimator.

hash sketch synopses xa and xb. our set-difference algorithm assumes the existence of an   -estimate u  that approximates the cardinality of the union u = |a 뫋 b| to within a relative error of  with probability at least 1 붣 붻 .  u  can be obtained using the xa and xb synopses  and the procedure described in section 1. 
모our algorithm for estimating set difference over two update streams a and b  termed setdifferenceestimator  is depicted in figure 1. briefly  our algorithm uses averaging over r independent copies of 1-level hash sketch synopses built over a and b; each of the r copies using independentlychosen first- and second-level hash functions from h and g. for each corresponding pair of 1-level hash sketches for a and b  which  of course  use the same hash functions   our basic difference-estimation procedure  termed atomicdiffestimator  is called to return an atomic estimate. the key idea in atomicdiffestimator is to try to discover a singleton first-level bucket in the pair xai and xbi that contains a  witness  element for a   b. this is accomplished by selecting a first-level bucket at a level located slightly higher than log |a 뫋 b|  steps 1   so that we actually find a singleton bucket for a 뫋 b with constant probability; if the bucket is not a singleton  then we cannot use this pair of sketches in our set-difference estimation and a noestimate flag is returned. otherwise  we check to see whether the bucket contains a witness element for a   b using the singletonbucket procedure for the xai bucket and a simple test to see if the xbi bucket is empty  step 1 . atomicdiffestimator returns an atomic estimate of 1 if it finds a witness singleton and 1 otherwise. setdifferenceestimator then simply averages all the valid  i.e.  1 or 1  atomic estimates and scales the result by the union estimate u  to compute the final estimate for the set difference |a   b|.
analysis. consider the first-level bucket  index  chosen in step 1 of our atomicdiffestimator procedure  and let r = index+1. note that  by our selection of u   r is at least 붹|a뫋b| with high probability. in this bucket  our procedure tries to discover a witness value for a   b by checking the following condition.
set-difference witness condition: bucket  index  is a non-empty singleton for a and empty for b  provided that bucket  index  is a singleton bucket for a 뫋 b.
let p denote the  conditional  probability that the set-difference witness condition is true. then  we can write:
pr  index  singleton for a and empty for b 
p = 
pr  index  singleton for a 뫋 b  
.
to see this  note that the probability of any given element mapping to bucket  index  is 1/r  so  by independence  the probability of a given element being the single element mapping to that bucket is exactly r1  1/r |a뫋b| 1. now  the number of elements that can give a singleton bucket for a and an empty bucket for b is exactly |a   b|  giving the numerator a similar argument applies for the denominator   and the derivation follows.
our technique relies on using the 1 atomic estimates
returned from the atomicdiffestimator procedure as independent  observations  of p and averaging them to obtain an estimate p  of p. since  as shown above  p = |a b|/|a뫋b|  our final estimate for the set-difference size |a b| is d = p 몫u   step 1 . let r denote the number of independent 1-level hash sketch synopses maintained and r1 be the number of independent observations of p used to obtain p . clearly  r1 뫞 r since for some of our sketches the first-level  index  bucket is not a singleton and a noestimate flag is returned; however  we can lower-bound the probability that a valid observation of p is obtained as follows:
pr 1 observation  = pr  index  singleton for a 뫋 b 
	|a 뫋 b|	  1	|a뫋b| 1
=	1 r	r
	a	b	a	b	붹	1
	  |	r뫋	|	1   |	r뫋	|	 	붹 1	 
where the first inequality follows from bernoulli's inequality and the second inequality comes from the fact that |a 뫋 b|/r   1/붹. then  we can simply apply chernoff bounds to show that  with probability at least 1   붣 붻   for any constant 1  the number of valid observations r1 is going
to be at least	 as long as
모in order to produce an  -estimate d  for the set difference |a   b|  we ensure that our p  average determined in procedure setdifferenceestimator is an   -estimate for p; that is:
pr.
by chernoff bounds  the above inequality holds if r1p 뫟  or  equivalently   assuming this condition is satisfied  we have:
 

since  thus  we obtain an  -estimate for |a   b| provided that the number of valid p observations r1 satisfies  or the total number of independent 1-
level hash sketches maintained is
 since  as discussed above with high probability if the optimal values for the constants 1 and 붹  i.e.  the values minimizing the required number of independent sketch copies  can be easily determined from the above expression asand 붹 = 1. based on the above analysis  we can state the following theorem.
theorem 1. procedure setdifferenceestimator returns an
-estimate for the size of the set difference |a b| of two update streams a and b using 1-level hash sketch synopses with a total storage requirement of
		.

proof: follows easily from the above analysis. the 붣 log m  term denotes the size of each 1-level hash sketch synopsis maintained by our algorithm. note that  in order to guarantee a confidence of 1   붻 for the final estimate  each of the possible basic property checks done over the chosen level for each of our r maintained 1-level hash sketch synopses has to have a probability of failure  the union bound . since
   the number of second-level hash buckets  and counters  required is
1 the set-intersection estimator
모given two continuous update streams a and b  the cardinality of the set intersection of streams a and b  i.e.  |a 뫌 b|  is defined as the number of distinct data elements whose net frequency is positive in both a and b. the structure of our set-intersection estimator  termed setintersectionestimator  for a and b based on their 1-level hash sketch synopses xa and xb is basically identical to that of the setdifferenceestimator procedure depicted in figure 1. the only difference is that  since we are now looking for  witness  elements for the intersection a 뫌 b  the if-condition in step 1 of procedure atomicdiffestimator is changed to:    singletonbucket xai   index  and singletonbucket xbi   index      to obtain the corresponding atomic set-intersection estimation algorithm atomicintersectestimator. the following theorem can then be shown using an analysis similar to that of section 1.
theorem 1. procedure setintersectionestimator returns an
-estimate for the size of the set intersection |a 뫌 b| of two update streams a and b using 1-level hash sketch synopses with a total storage requirement of
		.

1 extension to limited independence
모thus far  the analysis of our set-operation estimators has made the  unrealistic  assumption that the first-level hash functions used in our 1-level hash sketch synopses guarantee fully  i.e.  m-wise  independent value mappings.  secondlevel hash mappings only require pair-wise independence  lemma 1 .  in this section  we present a series of statistical lemmas that allow the analysis of our  set-operation estimators to be extended to the much more realistic setting of t-wise independent first-level hashing  
note that maintaining these first-level hash functions implies an additive storage  per 1-level hash sketch for storing an appropriate seed  e.g.   1  1  . this cost can be factored in the equations of theorems 1.1 by simply adding a  multiplicative factor.
모the only place in our analysis where the assumption of full independence is used is in deriving the closed-form expression for the probability of the conditions checked through our 1-level hash sketch synopses  e.g.  the  set-difference witness condition  for set difference  or the non-empty bucket condition for set union . our results below demonstrate that these  fully-independent  probabilities are actually estimated to within small relative error if only t-wise independence is assumed with  assume that we have fixed a first-level bucket j and let 1/r = 1j+1 denote the probability that an element in  m  maps to bucket j. we use i-subscripted small letters  e.g.  xi  yi  to denote the boolean random variables for the simple event  the ith distinct value in a stream maps to bucket j . throughout this section  we use pr  prt   to denote the probability function under full  resp.  t-wise  independence of these boolean random variables.  we omit the proofs of these statistical results since they are fairly long and do not offer much in terms of understanding; similar results for limited independence variables have appeared elsewhere  e.g.  . 
모lemma 1. let boolean random variables such that e xi  = 1/r  for 1 뫞 i 뫞 m. then  |prt  x 뫟 1    pr x 뫟 1 | 뫞 1 mt  1/r t.
모corollary 1. under the assumptions of lemma 1  and if then 
and
.

모note that  with xi's corresponding to the distinct elements of the union a뫋b  the condition x 뫟 1 in corollary 1 is essentially the set-union condition in step 1 of procedure setunionestimator  figure 1  that checks for a non-empty bucket for a 뫋 b. thus  corollary 1 shows that the non-empty bucket fraction p  assuming full independence  in the analysis of section 1  is approximated to within a relative error of  if only   -independent hash functions are used. the corresponding result for the set-difference and set-intersection witness conditions is slightly more complicated and relies on the following statistical lemma  we omit the detailed constants to simplify the exposition .
모lemma 1. let x = s1 xi and y = s1 yj denote the sums of disjoint sets s1  s1 of boolean random variables with e xi = e yi = 1/r for each xi 뫍 s1  yi 뫍 s1  and let e denote the event e :=  x = 1뫇y = 1|x +y = 1 . if
  then.
again  it is easy to see that  with s1 := a   b and s1
:= b  the event e :=  x = 1 뫇 y = 1|x + y = 1  in
lemma 1 is exactly the set-difference witness condition described in the analysis of our setdifferenceestimator estimator in section 1. similarly  with s1 := a 뫌 b and s1 :=  a뫋b  s1  e gives the corresponding condition for set intersection  section 1 . thus  lemma 1 that the witness condition probability estimate p  assuming full independence  see analysis in section 1  is estimated to within a relative error of  using only   -wise independence.
모an interesting question  of course  is how this additional level of approximation affects the storage requirements of our estimators. we now demonstrate that the effect is bounded by a small constant factor. consider the case of set difference and let p t denote the probability of a set-difference witness under t-wise independence with our final estimate is p tu  and we would like to guarantee that it is within . a simple application of the triangle inequality gives:

and  assuming that p u   the estimate under full independence  approximates pu to within a relative error of /1  we have:

for any thus  simply tightening our relative-error requirement   with the corresponding increase in our earlier storage-cost expressions  is sufficient to guarantee a relative error of  for the final set-difference estimate with only   -wise independence. very similar derivations can also be given for our set union and intersection estimators under limited independence.
1 lower bounds
모at this point  it is interesting to ask how good our randomized estimators for set operators really are - is it possible to design new estimation procedures that significantly improve on the space requirements stated in theorems 1- 1  in this section  we answer this question in the negative by demonstrating space lower bounds for set-operation estimators working in the streaming model.
모the space requirements of our set-union estimator actually match those of earlier algorithms for set union over insertion streams  see  for example   1  1  1 ; this is  of course  modulo the o log n  factor  since our algorithms need to maintain counters for dealing with deletions in the stream.
our setunionestimator space requirements also match  to within log and constant factors  the lower bounds shown by alon et al.  on the space needed by any randomized algorithm for estimating the number of distinct values in a data stream. as evidenced in the space bounds of theorems 1-1  estimating set difference and intersection is a significantly more difficult problem than that for union; essentially  our results show that  with limited space  our estimators can only provide robust estimates for differences/intersections that are sufficiently large compared to the corresponding set union  i.e.  |a 뫋 b| .  similar observations have been made for estimators designed for the special case of insertonly streams  1  1 .  the following theorem proves a lower bound for all randomized approximation algorithms stating that the space requirements of our setdifferenceestimator and setintersectionestimator estimators cannot be significantly improved  their space usage is within small polynomial and log factors of the optimal .
모theorem 1. any randomized algorithm that  with high probability  estimates the set cardinality |aopb|  op 뫍 {  뫌}  to within any constant relative error  must use at least  bits.
proof: let n = |a뫋b|. consider first the problem of estimating the set-intersection cardinality |a뫌b|. determining the value of |a뫌 b| exactly with high probability is at least as hard as the well-known set-disjointness problem of  probabilistic  communication complexity  which requires at least 붣 n  bits of communication  i.e.  space   1  1 . assume now that we have a procedure that estimates |a뫌 b| with high probability to within a relative error of  using less than bits; then  we will show that this implies an o n  solution for set-disjointness. more specifically  our algorithm for solving the set-disjointness problem for a and b is as follows. pick any constant  to determine a high-probability estimate t  of |a 뫌 b|. by our assumptions for p    we know that this run will use only bits  remember that 1 is a constant  and  with high probability   1  t   
. this last inequality also implies that  with high probability 
	.	 1 
now  run algorithm  again  this time with ; since   it is easy to see that this run will estimate |a 뫌 b| to within an additive error of less that 1  so it essentially allows us to estimate |a 뫌 b| exactly  with high probability . furthermore  the space used by  is only   by inequality  1    which is obviously o n . thus  we have a procedure for solving the set-disjointness problem using less than o n  bits; this is clearly a contradiction. the same argument also goes through for set difference  since a   b is simply a 뫌 b.
1. processing set expressions
모in this section  we generalize the estimation techniques for individual set operators presented in sections 1-1 to formulate an  -estimator for the cardinality of general set expressions over a collection of update streams ai  i = 1 ...  n. such set expressions are of the generic form e :=    a1op1 op1 몫몫몫an   where the connectives opj denote the standard set operators  namely  union  intersection  and set difference  as an example  e := a1    a1 뫌  a1 뫋a1   . our goal is to estimate |e|  that is  the number of distinct elements with positive net frequency in the output of e using only a collection of independent small 1-level hash sketch synopses built over the ai update streams  of course  as in the simple set-operator case  for a given sketch  we use the same first- and second-level hash functions across all ai's .
모briefly  our general set-expression estimator follows along the lines of our set-difference and set-intersection algorithms. as in the setdifferenceestimator and setunionestimator procedures  we assume a robust estimate u  for the union cardinality u = |뫋iai|  where i ranges over the streams participating in our input set expression e  and uses u  to select an appropriate first-level bucket index  where 붹 is a constant   1.  this estimate u  can be obtained from the synopses using our setunionestimator procedure.  our setexpression estimation algorithm for e starts by discarding all parallel 1-level hash sketch collections {xa1 xa1 ...} for which bucket j is not a singleton bucket for 뫋iai.  an easy generalization of our elementary check procedures in section 1 can be used to determine this fact with high confidence.  then  e is mapped to a boolean expression b e  over the level-j buckets of the 1-level hash sketch synopses for ai's; this expression is defined inductively as follows:
e = ai : define b e  :=  xai j 1  + xai j 1    1   i.e.  true iff bucket j is non-empty in xai .
e = e1 뫋 e1 : define b e  := b e1 뫈b e1   i.e.  the disjunction of the sub-expressions b e1  and b e1  .
e = e1 뫌 e1 : define b e  := b e1 뫇b e1   i.e.  the conjunction of the sub-expressions b e1  and b e1  .
e = e1 1   1   i.e.  must satisfy b e1 b e1
it is easy to see that  with the above methodology  our boolean condition b e  for set expression e essentially corresponds to an  e witness condition  at the selected bucket index j  as defined below.
e witness condition: bucket j is a non-empty singleton for the set expression e defined over a1 ...  an  provided that bucket j is a singleton bucket for.
as in our development for the setdifferenceestimator estimator  letting pe denote the  conditional  probability that the e witness condition is true and r = 1j+1  we have:
pr bucket j non-empty singleton for e 
pe = 
pr bucket j singleton for u = 뫋iai  
.
an analysis similar to that in section 1 can then be employed to demonstrate the following theorem.
모theorem 1. the set-expression estimator described above returns an -estimate for the cardinality of a set-expression |e| over a collection of update streams a1 ...  an using 1level hash sketch synopses with a total storage requirement of
		.

모we can also easily extend the limited-independence analysis of section 1 to show that our result for set-expression estimation holds under only   -wise independent first-level hash functions for our sketches.  once again  the cost for storing these functions can be factored in by simply adding a  multiplicative factor in the expression of theorem 1. 
모we should note here that the technique presented in this section for dealing with the union operator in the context of larger set expressions is  in fact  different from the setunionestimator procedure described in section 1. instead  the manner in which our set-expression estimator handles union essentially follows along the general paradigm of our set difference and intersection estimators  with an appropriatelydefined  witness  condition - sections 1-1 . it is easy to see that both techniques basically have the same asymptotic storage requirements  remember that set union does not require second-level hashing . on the other hand  a detailed analysis shows that our more specialized setunionestimator algorithm does have better  i.e.  smaller  constants for setunion estimation which is  in general  much easier than the corresponding problem for set difference/intersection. the key benefit of the  witness -based union algorithm is that  as shown in this section  it allows for a very clean  uniform algorithm for processing general set expressions.
1. experimental study
모in this section  we present the results of a preliminary empirical study of our 1-level hash sketch synopses and setexpression estimators with several synthetic data sets. the objective of this study is to test the effectiveness of our novel stream-synopsis data structures and probabilistic estimation algorithms in practical data-streaming scenarios  and study their average-case behavior over several different problem instances. our preliminary experimental results substantiate our theoretical claims  demonstrating the ability of our techniques to provide  with only limited space  accurate approximate answers to set-expression cardinality queries over continuous streaming data.
1 testbed and methodology
methodology. in our experiments  we study the accuracy of the probabilistic set-expression cardinality estimation techniques developed in this paper using 1-level hash sketch synopses constructed over different synthetic data streams. the primary metric used to gauge the accuracy of our estimators is the conventional absolute relative error metric; that is  given an expression e and an estimate e  = |e| of its cardinality  we define the error of the estimate as the ratio |e  |e|e| ||. we perform experiments to measure the errors of our cardinality estimators as a function of the space made available for building 1-level hash sketch synopses for the input data streams. this accuracy/space tradeoff is studied over various input expressions  ranging from simple binary set operations  primarily difference and intersection  to more complex set expressions  over three or more streams .  again  note that our techniques are the first to deal with set difference/intersection and set expressions over general update streams; thus  in a sense  comparing against the accurate answer is probably the best measure of effectiveness for our approach. 
모to account for the probabilistic nature of our estimation algorithms  we run each experiment between 1   1 times  with different random-seed values . the numbers used in our plots are averages of the observed relative error values after trimming away 1% of the highest relative errors for each experiment. we used this more robust   trimmed-average  error metric to avoid the effects of outlier estimates  due to the variance of our randomized schemes  on the observed average-case behavior of our estimators.
synthetic data generation. our 1-level hash sketch synopses are impervious to delete operations  in the sense that a sketch obtained at the end of an update stream is identical to one that never sees the deleted items in the stream. given this fact  our synthetic data generator produces insert-only streams for updating the 1-level hash sketch synopses for our estimation algorithms. furthermore  since the accuracy of our cardinality estimates for a set expression e crucially depends on the ratio of the underlying set union to |e|  theorems 1  1  1   we generate our data streams in a controlled manner that allows us to vary this cardinality ratio and observe the behavior of our techniques for different settings.  we fix the size of the underlying set union  i.e.  | 뫋ai뫍e ai|  to u 뫘 1 in all our experiments. 
모we now describe the data-generation process for a binary set operation  say a 뫌 b  assuming a given target size e for the cardinality |a 뫌 b|.  we vary the value of e from u/1 down to u/1 in diminishing powers of 1.  in a first step  we generate 1-bit random unsigned integers and eliminate all duplicates  thus  the actual union size u can be slightly less than 1 . then  for each generated integer x  we insert x to either  a  both a and b  with probability e/u; or   b  only a or only b  with equal probability . thus  at the end of this process  we expect to have approximately ue u = e elements in a뫌b  and about equal numbers of elements in both a and b. it is easy to devise a very similar controlled data-generation scheme for a   b.
모for set expressions e involving multiple  say n  streams  our controlled data generation is slightly more complicated. briefly  the main idea is to keep track of all 1n  1 partitions in the venn diagram of the underlying set union and give  assignment probabilities  to each partition such that the sum of probabilities for all partitions that comprise e is approximately |e|/u.  for simplicity  the probabilities are chosen so that all underlying sets have the same expected size.  generated random integers are then assigned to these partitions as discussed above.
1 experimental results
모we now present some of our preliminary experimental numbers for our probabilistic estimation algorithms. we focus our discussion here on three input set expressions: binary set intersection a뫌b  binary set difference a b  and the more complex three-stream expression  a b 뫌c.  we have observed qualitatively similar results for the estimation of other expressions.  we present plots that depict the  average  relative error behavior of our estimators as a function of the number of 1-level hash sketch synopses maintained on each data stream. a rough estimate for the number of bytes used by our synopses is given by multiplying the number of sketches with 1; since we are only considering insert-only streams  this estimate assumes simple bits  instead of counters  at each cell of our 1-level hash sketches.  the number of second-level hash functions used is kept fixed at 1. 
모figure 1 a  depicts the average  percentage  relative-error numbers for our set-intersection cardinality estimator as a function of the number of 1-level hash sketches used  and for three distinct values of the target intersection size |a 뫌 b|.
the plots demonstrate the effectiveness and accuracy of our estimation algorithm. even with as few as 1   1level hash sketches  the error of our estimates is close to or below 1%  essentially across the range of the target intersection sizes tested. and  of course  increasing the number of sketches can lead to significant further reductions in the observed estimation error which finally drops to 뫞 1% for 1 sketches.
모similar trends can also be observed for set-difference cardinality estimator in figure 1 b . in this case  errors for smaller target difference sizes  i.e.  |a   b| = 1  are higher  about 1%  for small numbers of sketches. once again  however  when our synopsis space reaches 1 sketches  all errors are in the area of 1% or lower. note that  as predicted by our theoretical results  the quality of our estimates for a given number of sketches  in general  improves with higher target expression sizes. we do  of course  observe certain crossovers in the plots but they are to be expected given the variance of our randomized estimation techniques.
모finally  figure 1 depicts the average relative-error plots for our set-expression cardinality estimator with the input expression  a   b  뫌 c  for three different target expression sizes. the numbers clearly show trends that are very similar to those observed for the simpler binary set intersection/difference experiments. once again  error numbers are fairly small even for moderate synopsis sizes  eventually tailing off to 1% or lower for 1 sketches. and  in accordance with our theoretical results  theorem 1   larger target expression sizes imply better cardinality estimates  for a given synopsis size .

1 1 1 1 1 1
space  in sketches 
figure 1: average relative error for estimating the set-expression cardinality | a   b  뫌 c|.
1. conclusions

figurecardinality1: average|a   b|.relative error for estimating:  a  set-intersection cardinality |a 뫌 b|;  b  set-difference모estimating the cardinality of set expressions defined over several  perhaps  distributed  continuous update streams is a fundamental class of queries that next-generation datastream processing systems need to effectively support. in this paper  we have proposed the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general streams of updates  including item deletions as well as insertions . our estimators rely on a novel  1-level hash sketch synopsis data structure to provide low-error  high-confidence estimates for the cardinality of set expressions  including operators such as set union  intersection  and difference  over continuous update streams  using only small space and small processing time per update. furthermore  unlike earlier approaches  our algorithms never require require rescanning or resampling of past stream items  regardless of the number of deletions in the stream. preliminary results from an empirical study of our estimators have substantiated our theoretical claims  showing that our techniques can provide space-efficient and accurate set-expression cardinality estimates over streaming data.
