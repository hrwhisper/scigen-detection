in this paper the score distributions of a number of text search engines are modeled. it is shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. experiments show that this model fits trec-1 and trec-1 data for not only probabilistic search engines like inquery but also vector space search engines like smart for english. we have also used this model to fit the output of other search engines like lsi search engines and search engines indexing other languages like chinese.
　it is then shown that given a query for which relevance information is not available  a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. these distributions can be used to map the scores of a search engine to probabilities. we also discuss how the shape of the score distributions arise given certain assumptions about word distributions in documents. we hypothesize that all 'good' text search engines operating on any language have similar characteristics.
　this model has many possible applications. for example  the outputs of different search engines can be combined by averaging the probabilities  optimal if the search engines are independent  or by using the probabilities to select the best engine for each query. results show that the technique performs as well as the best current combination techniques.
1. introduction
　in this paper we model score distributions of text search engines using a novel approach. we first show that the score distributions for a given query may be modeled using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. we further show that when relevance information is not available  these distributions can be recovered by fitting a mixture model with a gaussian and an exponential component to the output scores of search engines on a per query basis. this novel approach to score modeling is then used to map the
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  september 1  new orleans  louisiana  usa..
copyright 1 acm 1-1/1 ...$1.
scores to probabilities using bayes' rule. note that no training is required for this approach and in addition no assumption is made on the kind of search engine used. the model has been shown to work for a large number of search engines on trec-1 and trec-1 data including probabilistic search engines like inquery and vector space search engines like smart. this model has also been shown to work for other engines like the lsi search engine and the score distributions of trec-1 inquery and smart data on chinese. to our knowledge  this is the first attempt at recovering the relevant and non-relevant distributions when no relevance information is available.
　the probabilities of relevance obtained from this model have many possible applications. for example thresholds for filtering may be selected using this approach or the probabilities may be used to combine the search from many distributed databases or multi-lingual or multi-modal databases. here  we will focus on using them to combine the outputs of different search engines  the meta-search problem .
　most combination methods proposed in the literature are ad-hoc in nature and often involve the linear combination of scores . this is unsatisfactory as scores from different search engines can be very different since they are often the result of computing some metric  or non-metric  distance over sets of features. both the distance and the features may vary from engine to engine. in fact even the distributions of scores of different search engines could vary widely - for example  the scores of relevant documents may be clumped together for one engine while for those of a second engine may be distributed in a much more uniform manner. a linear combination of results in such cases could lead to misleading results. the problem is more acute when search engines operating on different media have to be combined for then the scores really mean different things.
　the approach proposed here allows us to combine the outputs of search engines using the probabilities derived from the model of score distributions. in this paper we examine two approaches to combination. the first involves averaging the probabilities which is optimal in the sense of minimizing the bayes' error if the search engines are treated as independent classifiers . the second approach involves using the probabilities to discard  bad  engines while keeping the  good  ones. we show that the combination approaches proposed using these techniques do as well as the best combination techniques proposed in the literature. in addition  our technique is less ad-hoc and easier to justify. the technique can also be extended to multi-lingual and multi-modal combination.
　the rest of the paper is divided as follows. section 1 discusses prior work in modeling score distributions as well as in the area
1
of combination. this is followed by section 1 which discusses the modeling of score distributions of relevant and non-relevant documents and how these distributions may be recovered in the absence of relevance information by using a mixture model. solving for the mixture model using expectation-maximization  em  is also discussed. finally  bayes' rule is used to map the scores to probabilities of relevance. section 1 discusses the theoretical intuition behind using such models. section 1 discusses how the model and the probabilities derived from it can be used for evidence combination. finally  section 1 concludes the paper.
1. prior work
　note that it is not obvious that the non-relevant data should be fitted with an exponential and the relevant data with a gaussian. a number of researchers in the 1's and 1's starting with swets  proposed fitting both the relevant and non-relevant scores using normal distributions and then using statistical decision theory to find a threshold for deciding what was relevant. bookstein  pointed out that swets implicitly relied on an equal variance assumption. bookstein also raised the issue of whether it might be more appropriate to model the score distributions using poissons. this modeling does not appear to have been done. van rijsbergen  commented that for search engines like smart there was no evidence that the distributions were similarly distributed let alone normally. we observe here that the empirical data for a large number of search engines clearly shows that the two distributions are not similar. independent of this work  arampatzis et al  recently discussed how the relevant scores could be modeled using a gaussian distribution and the non-relevant scores using an exponential distribution for a filtering application. based apparently on modeling one filtering engine  they argue that the gaussian only arises when massive query expansion is undertaken   terms . our results indicate that the gaussian can arise when the number of query terms is much smaller. all of the previous work here  including arampatzis et al's   assumes that relevance information for some or all of the set is available. to our knowledge  there is no literature on recovering the relevant and non-relevant distributions when no relevance information is available and ours is the first attempt to do this.
　a recent and extensive survey of evidence combination in information retrieval is provided by croft . tumer and ghosh  discuss past work in a related area - the combination of classifiers. they show that for statistically independent classifiers  the optimal combination is obtained by averaging the probabilities. they define optimality as equivalent to minimizing the bayes' error.
　fox and shaw  proposed a number of combination techniques including operators like the min and the max. other techniques included one that involved setting the score of each document in the combination to the sum of the scores obtained by the individual search engines  combsum   while in another the score of each document was obtained by multiplying this sum by the number of engines which had non-zero scores  combmnz . note that summing  combsum  is equivalent to averaging while combmnz is equivalent to weighted averaging. lee  1  1  studied this further with six different engines. his contribution was to normalize each engine on a per query basis improving results substantially. lee showed that combmnz worked best  followed by combsum while operators like min and max were the worst. lee also observed that the best combinations were obtained when systems retrieved similar sets of relevant documents and dissimilar sets of non-relevant documents. vogt and cottrell  also verified this observation by looking at pairwise combinations of systems. a probabilistic approach using ranks rather than scores was proposed last year by aslam and montague . this involved extensive training across about 1 queries to obtain the probability of a rank given a query. their results for trec-1 were close to but slightly worse than lee's combmnz technique 1
 . aslam and montague were able to demonstrate that rank information alone can be used to produce good combination results. the main difficulty with this technique seems to be the extensive training required of every engine on a substantial number of queries.
　a number of people have also looked at the problem of combining outputs of systems which search overlapping or disjoint databases. voorhees et al  experimented with combination using a set of learned weights. callan  gave a value to each database. he showed that weighting the scores by this value was substantially better than interleaving ranks. some researchers have also investigated the notion of combining search engines over multiple media. qbic  combined scores from different image techniques using linear combination. fagin  used standard logical operators like min and max to combine scores in a multimedia database. however  lee's experiments showed  at least for text  that these operators perform significantly worse than averaging .
1. modelingscoredistributionsof search engines
　in this section we describe how the outputs of different search engines were modeled using data from the text retrieval conferences  trec . trec data provides the scores and relevance information for the top 1 documents for different search engines. for the experiments here data from the ad hoc track of the trec-1 and trec-1 for a number of different search engines was used. we will show examples of the modeling on queries from inquery and smart from trec-1. inquery is a probabilistic search engine from the university of massachusetts  amherst while smart is a vector space engine from cornell university.
　our modeling begins with trec-1 data for inquery. there are 1 queries available with document scores and relevance information for each query. we examine the relevant and non-relevant data separately. the data are first normalized so that the minimum and maximum score for a query are 1 and 1 respectively.
　figure 1 shows a histogram of scores for query 1 from trec1 for set of non-relevant documents. the histogram clearly shows the rapid fall in the number of non-relevant documents with increasing score. a maximum-likelihood fit of an exponential curve to this data is also shown. for the purposes of fitting the exponential  the origin is shifted to the document with the lowest score. it can be shown that the maximum-likelihood for an exponential is obtained by setting the mean of the exponential to the sample mean of the data .
　figure 1 shows a histogram of scores for the set of non-relevant documents for the same query. the histogram approximates a normal distribution. the plot also shows a maximum-likelihood fit using a gaussian with mean 1 and variance 1. the maximumlikelihood fit involves setting the mean and variance of the gaussian to the sample mean and sample variance respectively of the data . the exponential fit obtained for the non-relevant documents is also plotted in the figure.
　the same process was repeated for all 1 queries in this track and in most cases it was found possible to fit the non-relevant data with exponentials and the relevant data using gaussians. the relevant data can be fitted with a gaussian reasonably well when there is a sufficient number of relevant documents. usually more than 1 relevant documents were needed. when the number of relevant

the graph for lee's technique in  is incorrect.
1
inq1: non-relevant distribution for query 1

figure 1: histogram and shifted exponential fit to non-relevant data for query 1 inquery  inq1 
documents was small  the fit was bad. however  we believe this is not because the gaussian was a bad fit but because we don't have enough relevant documents to compute the statistics in these cases. the exponential was also a good fit to the non-relevant data.
　we have so far been able to fit parametric forms to the score distributions given relevance information. when running a new query  however  relevance information is not available. clearly  it would be useful to fit the score distributions of such data. a natural way to do this is to fit a mixture model of a shifted exponential and a gaussian to the combined score distribution. this approach is discussed in the next section.
1 mixture model fit
　consider the situation where a query is run using a search engine. the search engine returns scores but there is no relevance information available. we show below that in this situation  a mixture model consisting of an exponential and a gaussian may be fitted to the score distributions. we can then identify the gaussian with the distribution of the relevant information in the mixture and the exponential with the distribution of the non-relevant information in the mixture. essentially this allows us to find the parameters of the relevant and non-relevant distributions without knowing relevance information apriori.
　the density of a mixture model can be written in terms of the densities of the individual components as follows:  1 
1 
 1 
where j identifies the individual component  the are known as mixing parameters and satisfy . in the present case  there are two components  an exponential density with mean
 1 
and a gaussian density with mean	and variance
		 1 
　a standard approach to finding the mixing parameters and the parameters of the component densities is to use expectation maximization  em   1  1 . this is an iterative procedure where the expectation and maximization steps are alternated. space precludes us from dicussing the details of the em algorithm and the update equations used. the reader is referred to  1  1  for a good introduction to em.
　the procedure needs an initial estimate of the component densities and mixing parameters. given that  it rapidly converges to a solution. using em to fit the data gives the mixture fit shown in figure 1. the figure plots the mixture density as well as the component densities for the exponential and gaussian fits. for comparison figure 1 shows the exponential and gaussian fits to the non-relevant and relevant data. comparing the two figures  it appears that the strategy of interpreting the gaussian component of the mixture with the relevant distribution and the exponential component of the mixture with the non-relevant distribution is a reasonable one. we should note that the correspondence between the mixture components and the fits to the relevant/non-relevant data is not always as good as that shown here but in general it is a reasonable fit.
　the same technique was used to model the result of query 1 for the cornell smart vector space engine. similar results were obtained as shown in figures 1 and 1.
　this model has been fitted to a large number of search engines on trec-1 and trec-1 data including probabilistic engines like inquery and city and a vector space engine  smart  as well as bellcore's lsi engine. the fit appears to be better for  good  search engines  engines with a higher average precision in trec1  and worse for those with a lower average precision. the model has also been able fitted to document scores for searches on inquery and smart indexing a chinese database.

figure 1: exponential fit to non-relevant data and gaussian fit to relevant data for query 1 inquery  inq1 
1 computing posterior probabilities
　using bayes' rule one can compute the probability of relevance given the score as

 1 
where is the probability of relevance of the document given its score  and are the probabilities of score given that the document is relevant and score given that the document is non-relevant respectively. p rel  and p nonrel  are the prior probabilities of relevance and non-relevance.
　in our model  is given by the gaussian component of the mixture while is given by the exponential
1

figure 1: mixture model fit showing exponential component  gaussian component and the combined mixture for query 1 inquery  inq1 . compare with figure1

figure 1: exponential fit to non-relevant data and gaussian fit to relevant data for query 1 smart  crnlea .

figure 1: mixture model fit showing exponential component  gaussian component and the combined mixture for query 1 smart  crnlea . compare with figure 1
part of the mixture. p rel  and p nonrel  may be obtained by using the mixing parameters. thus  can be computed in a simple manner.
　figure 1 shows the posterior probability obtained for smart for query 1 using the previously determined mixture fits. the figure shows the posterior probabilities obtained from the separate gaussian and exponential fits when relevance information is available and also the posterior probabilities obtained from the gaussian and exponential part of the mixture. p rel  and p nonrel  are taken to be the mixing parameters in this case. note that the differences in the two curves reflect fitting errors both for the mixture fit as well as the separate gaussian and exponential fits obtained when relevance information is available.

figure 1: posterior probability for query 1 for the smart engine for trec-1 data. the dotted line is obtained from the separate gaussian and exponential fits computed using relevance information. the solid line is obtained from the mixture fits.

figure 1: posterior probabilityfor query 1 for the inquery engine for trec-1 data. the bold dotted line is obtained from the separate gaussian and exponential fits computed using relevance information. the solid line is obtained from the mixture fits. the dotted line joins the maximum point of the mixture to the point 1 . the final posterior mapping follows the solid line up to the maximum point and then the straight line curve thus preserving monotonicity
　in general we expect the posterior probabilities to be a monotonic function of the score. in other words as the score increases so should the posterior probability. in some cases we may have the situation depicted in figure 1 where the posterior seems to decrease
1
with increasing scores. the figure depicts the posterior probabilities for inquery for query 1 using trec-1 data. this situation arises because the gaussian density falls much more rapidly than the exponential and hence the two densities intersect twice. note that in this case the posterior probabilities obtained both from the mixture fit  no relevance information available  as well as that obtained using relevance data show this problem. one solution would be not to use a gaussian density but to use another function which has the same form  like a gamma distribution  but decreases less rapidly. as we discuss below the problem with this approach is that the mixture model does not converge to a reasonable solution. instead we force the posterior probability to be monotonic by drawing a straight line from the point where the posterior is maximum to the point  1 . the final posterior probability curve is given by the portion of the posterior probability computed using bayes' rule up to the maximum portion of the curve and the straight line thereafter.
　we have assumed that the priors p rel  and p nonrel  may be estimated using the mixing parameters. when there are few relevant documents the mixing parameters provide a poorer estimate of the priors. in a normal retrieval the number of relevant documents is small and hence estimates of the mixing parameters are less accurate. extensive experiments have shown that p nonrel  is best estimated using the following procedure. let p 1  be the mixing parameter corresponding to the exponential. then
 1 
and p rel  = 1 - p nonrel . this approach to estimating the priors improves the average precision results slightly when we combine results.
1 comments on fitting distributions and mixture models
　there is a large family of densities which could possibly have fit the data. for example  the poisson and gamma distributions approximate the gaussian for appropriate parameter choices. however  using a poisson/poisson  non-relevant/relevant  or an exponential/ poisson combination did not fit the data well. on the other hand  while an exponential/gamma fit the non-relevant and relevant data when separately fitted  a mixture fit with exponential and gamma components did not converge to the right answer. in this
case the gamma component also converged to an exponential  the exponential density is a special case of the gamma function . thus our choice of distributions - exponential for the non-relevant and gaussian for the relevant - is dictated by two considerations; that the functions fit the data well and that they can be recovered using
a mixture model when relevance information is not available.
　like any non-linear equation solver  em may find solutions arising from local maxima and is sometimes sensitive to initial conditions . different approaches to picking initial conditions were tried.
1. the first involved picking arbitrary initial conditions.
1. a second approach involved fitting an initial exponential toall the document scores  relevant and non-relevant . this is reasonable since there are far more non-relevant documents than relevant documents. thus  the distribution of the scores of the combined documents essentially resembles that of the of scores of the non-relevant documents i.e. its an exponential. scores of documents which do not fit the exponential are removed and fitted with a separate gaussian. the exponential and gaussian provide initial estimates of the parameters.
some sensitivity to initial conditions was noticed but usually for the poorer search engines  search engines much lower down in a trec-1 ranking by average precision .
1. shape of distributions
　we will now attempt to give some insight into the shape of the score distributions.
　given a term  or word  assume that the distribution of this term in the set of relevant documents is given by a poisson distribution with parameter . that is 
		 1 
where x is the number of times that the term occurs in a particular document and is the probability of x occurrences of the term in the set of relevant documents. also assume that its distribution in the set of non-relevant documents is given by another poisson distribution with the parameter . that is 
		 1 
where is the probability of x occurrences of the term in the set of non-relevant documents. in general  will be much smaller than .
　numerous attempts have been made to model word distributions in the past. harter  used a mixture of 1 poissons to model the distributions of words in a document. our model in this section is closely related to his model. it has been argued by some researchers that the 1 poisson model is not a good approximation and that other distributions like the negative binomial are better models of the distributions of words in documents . a mixture of a large number of poissons has also been used to fit the data . since we would like to fit a distribution to the relevant and another to the non-relevant  it is much more convenient for us to assume the 1-poisson model here. additionally  the main purpose of this section is to provide some insight and not a rigorous derivation.
　given a query consisting of 1 term and assuming that the score given to a document is proportional to the number of matching words in the document  the distribution of the scores of relevant documents is then given by the poisson distribution:
		 1 
and the distribution of the scores of non-relevant documents is given by the poisson distribution:
		 1 
the actual scores for many search engines is weighted by some function of the term frequency and the inverse document frequency. however  empirical evidence  shows that the score may be reasonably approximated as being proportional to the number of matching words.
　for the set of relevant documents  will usually be large. for large values of   the poisson distribution tends to a normal distribution  see figure 1 . on the other hand for small values of   the poisson distribution will tend towards a distribution which is falling rapidly  see figure 1 . the shape of these curves is consistent with the experimental modeling of scores for trec-1 and trec-1 data  see the previous section . the experiments showed us that the normal distribution is a good fit for the score distributions of the relevant data. for non-relevant data  the experiments show that the exponential distribution is a good fit. for small  
1
the poisson distribution shows a decreasing distribution. although  this is not the same as an exponential distribution  it does have the same general shape as an exponential  rapid monotonic decrease .

figure 1: the poisson distribution for different values of	.
　it is much harder to derive the score distributions if the query consists of two or more terms. this is because the actual scores of search engines are complicated functions. however  there is empirical evidence that the major contribution to the scores is provided by the number of matching terms . we also note that robertson and walker  motivated a scoring function from the 1-poisson model. we assume first that the score is proportional to the number of matching words and provide an intuitive argument for queries with two or more terms. for simplicity we will consider the case where the query has just two terms but the argument applies in general. in this case we can assume that the two terms say  oil  and  spill  can be clubbed together to create a single term -  oil spill . then the  the average frequency of a term over relevant documents  for joint occurrences of this term  oil spill  is much lower than the for either  oil  or  spill . in other words the chances that the terms  oil spill  occur together is much less than that of finding  oil  or  spill . when the query contains two terms  it is reasonable to assume that the  i.e. the average frequency over non-relevant documents  does not change much as it essentially reflects the background probabilities of the word.
　the poisson model for the shape of the relevant and non-relevant distributions that we have derived applies to both probabilistic engines like inquery and vector space engines like smart. for vector space engines the number of matching terms is given by the dot product of two vectors - one representing the query and the other the document. further  this model is language independent  as long as word frequencies in any language have an approximately 1-poisson like distribution . thus  we predict that a mixture of exponential and gaussian distributions will fit a much larger class of text search engines operating on different languages.
　the model in this section although intuitive can be used to make a prediction. the model predicts that on a statistical basis as the number of query terms is increased the overall should decrease and hence the mean and variance of the gaussian fitting the relevant documents should also decrease. note that for any particular query the mean score can be arbitrary. however when a large enough sample of queries is considered  the mean query should decrease with the number of query terms.
it is a well known fact in information retrieval that with query expansion the score of the relevant documents decreases and the range also decreases which is consistent with this prediction. for the 1 queries from trec-1 for inquery  inq1  and smart  crnlea   we plotted the mean scores of the relevant data versus the number of query terms  including expanded queries . a small statistical decrease with the number of query terms was observed for inquery and smart. the figures are not produced here because of a lack of space.
1. combining search engines
　the posterior probabilities obtained by using the model discussed above has many possible applications. for example the probabilities could be used to select a threshold for filtering documents or for combining the outputs in distributed retrieval. here we discuss one possible application which involves combining the outputs of different search engines on a common database to improve results.
　it would be of considerable use to combine the output of different search engines. in this section we discuss how the score of search engines may be combined while taking into account the actual score distributions.
　in general the range of search engine scores may vary quite a bit for example  one engine may have scores ranging from 1 to 1 while another can have scores ranging from -1 to 1. other approaches to combining score distributions have focused on normalizing the range of the scores and then combining them by simple techniques like linear combination or by taking the minimum and maximum scores. however  range normalization does not take into account the actual distribution of the scores. consider  for example  the model of the scores discussed previously where the scores of the relevant documents follow a normal distribution and the scores of the non-relevant follow an exponential distribution. also consider two different search engines which have different parameter values for these distributions. a simple  linear  range normalization and combination does not clearly suffice.

figure 1: recall precision graphs for combining inq1 and crnlea using different techniques  see text . data from trec-1
　there are a number of possible ways the probabilities can be used to combine the search engines. we propose two alternative schemes for combination. the first involves averaging the probabilities. this is optimal in the sense of minimizing the bayes'
1
error if the search engines are independent . of course the outputs of search engines are not necessarily independent. in the following figures and discussions  data are taken from trec-1. inq1 and crnlea denote runs of the inquery and smart engines  meta1 denotes combination by averaging the posteriors obtained using the mixture model  while meta1 denotes the combination by averaging the posterior probabilities using the gaussian and exponential fits assuming relevance is given. thus  any difference between meta1 and meta1 is caused by the errors in performing a mixture fit to the model. lee denotes lee's combmnz technique while the manual engine selection technique involves selecting the best engine s  and discarding the worst engine s  on a per query basis using the average precision for that query. manual engine selection provides an indication of the best combination result we can achieve. note that both meta1 and manual engine selection require relevance information and are only plotted to provide a baseline for understanding the limits of combination.
　figure 1 shows recall-precision plots for combining inquery and smart on trec-1 data. precision is defined as the fraction of retrieved documents which are relevant while recall is the fraction of relevant documents which have been retrieved. the recallprecision graph is usually created by averaging over a certain number of queries - in this case 1. as the figure shows meta1 performs considerably better than either inquery and smart - in fact about 1% better than inquery and 1% better than smart. lee is slightly better  about 1%  than meta1 although the difference is not significant. meta1 has an average precision about 1% better than inquery and clearly performs better than either meta1 or lee's implying that if the mixture fit could be improved the technique would perform even better. finally  the plot for manual engine selection clearly indicates that both meta1 and lee's are close to obtaining the best performance possible from combination.
　figure 1 describes combination results for the top five engines in trec-1. the x-axis is the number of engines combined while the y-axis is the average precision. as the plot clearly shows combination clearly improves the results. there are four graphs in the figure. the first curve is the average precision of the individual search engines. the second plot meta1 shows the combination method applied to 1  1  1  1 or 1 engines. as can be clearly seen there is a considerable improvement over using even the best search engine and overall the improvement seems to increase with the number of search engines combined. with the top 1 engines  meta1 shows an improvement of 1% over the best single engine and using the top 1 engines  meta1 shows an improvement of almost 1%. lee's combmnz technique is also shown in the same graph. it's average precision is seen to be slightly worse than meta1 but the difference is not really significant. the performance obtained using meta1  i.e. combination with the posterior probabilities obtained with relevance information  is 1% better than the best single engine. again this indicates that if the mixture fit were improved we could do even better.
　figure 1 demonstrates that this approach also works for other languages. the figure shows the combination results for inquery and smart when indexing a chinese database. the data in this case is from trec-1. as can be clearly seen  combination using both meta1 and lee's combmnz show an improvement over either engine. however  in this particular case the improvement is much less than that for english. also the difference between meta1 and meta1 is small indicating that perhaps we are close to the limit of what can be achieved.
combination of  good  search engines usually improves the scores.

figure 1: recall precision graphs for combining the best five techniques from trec-1.

figure 1: recall precision graphs for combining inq1ch1 and crnlch1 with chinese queries and chinese databases. data from trec-1
1partly this reflects the fact that the score distribution models fit  good  search engines better than  poor  search engines. however  the combination of two search engines when the performance of one is substantially worse than the others leads to a result which can be worse than that of the best engine. this partly reflects the well known observation that combining a bad classifier with a good classifier can lead to a result which may not be better than the best individual classifier. two search engines inquery  inq1  and xerox  xerox1  were picked. on the basis of average precision  inq1 is ranked 1th while xerox1 is ranked 1th among 1 engines in trec-1. the average precision of inq1 is more than twice that of xerox1. figure 1 clearly shows that inquery performs much better than the xerox engine. the combination meta1 is much better than xerox but worse than inquery. lee's is slightly better than meta1 but still worse than inquery. the best option in such cases is to avoid combination.

figure 1: recall precision graphs for combining inq1 and xerox1 using different techniques. data from trec-1.
1 automatic engine selection
　the previous example shows that if we could have somehow figured out that we need to pick inquery as the best possible engine for every query then the performance would improve considerably. the ability to model and compute the relevant and non-relevant distributions allows us to develop techniques to automatically selecting engines on a per query basis. here  we examine two such approaches.
　the first approach essentially tries to ensure that the distance between the mean of the normal distribution and the point at which the densities intersect is large  all distributions are obtained using the mixture model . the idea is that if this distance is large then it will be easier to separate the relevant and non-relevant documents. if the distance is less than a threshold  the engine is discarded for that query. the posterior probability of all engines selected  i.e. not discarded  for a particular query are averaged to obtain document probabilities as before. if all engines for a particular query are below the threshold  then the one with the highest posterior probability is selected. the threshold is selected based on empirical data to be 1. the results for this technique are labelled as meta1 in figure 1
　the second approach uses the posterior probabilities obtained using bayes' rule from the mixture components. in some situations the maximum of the posterior probability is quite small. a posterior of 1 indicates that the relevant and non-relevant distributions weighted by the priors are of equal magnitude. in other words a posterior of 1 indicates the point at which the exponential and gaussian densities intersect after weighting by the prior. it is clear that a  good  engine should preferably have a higher posterior. empirically if the posterior for a particular engine and a particular query was less than 1 then that engine was regarded as poor and discarded for that particular query. if both engines had maximum posteriors greater than 1 then they were averaged. if neither engine had a maximum posterior greater than 1 both were again averaged and combined - this is plotted as meta1.
　figure 1 shows the results of combining two engines whose performance is very different. we again use inq1 and xerox1. as is clear from figure 1  meta1 and meta1 perform about equally well and and both are better than meta1  straight averaging of posterior probabilities . the average precisions of meta1 and meta1 are essentially the same as lee's. we have also carried out other experiments with other engines all of which demonstrate that engine selection can be done using the models of score distributions discussed here. we note that both meta1 and meta1 are still worse than using inquery alone indicating that there is further scope for improving the engine selection procedure. of course  this also implies that when one search engine performs much worse than another it may be best not to use the  poor  search engine.

figure 1: recall precision graphs for combining inq1 and xerox1 by automatically selecting the best engine using probabilities and distributions.
1 discussion of combination results
　the results above show that the mixture modeling performs as well as the best current techniques  lee's  available for combination. although there is scope for a slight improvement it is also clear that we are approaching the limits of the best performance we can achieve.
　lee's combmnz technique performs surprisingly well. in the case where inquery and smart are combined we note that for many queries inquery and smart have distributions which are very similar. in such a situation  their posterior distributions will also look remarkably similar and hence averaging is a good strategy. since combsum involves computing a document score by just adding the scores for all engines which find that document  it will produce the same ranking as averaging and hence it will also be good. combmnz involves multiplying combsum by the number of engines which found that document and hence it will also produce good results. in this particular situation combmnz involves essentially combining the posterior probabilities without having to do the mixture modeling. however  in the more general case  the good performance of combmnz is hard to explain.
　the model for combination proposed here is more intuitively satisfying for a number of reasons. first  it combines engines in a natural way using probabilities and is therefore easier to explain. second  it indicates where improvements can be made for better performance. third  the same technique may be used for combin-
1
ing multi-lingual engines. it will also extend to multi-modal engines provided the distributions of scores behave in a similar way for search engines indexing other media.
1. conclusion
　we have demonstrated how to model the score distributions of a
　number of text search engines. specifically  it was shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents.
　it was then shown that given a query for which relevance information is not available  a mixture model consisting of an exponential and a normal distribution may be fitted to the score distribution. these distributions were used to map the scores of a search engine to probabilities.
　the model of score distributions was used to combine the results from different search engines to produce a meta-search engine. the results were substantially better than either search engine provided no  search engine  performed really poorly. different combination techniques were proposed including averaging the posterior probabilities of the different engines as well as using the probabilities and distributions to selectively discard some engines on a per query basis.
　future work will include attempts to further improve the modeling for better performance. other possible applications of modeling score distributions like filtering will also be examined. finally we will also examine the possibility that search engines indexing other media like images can also be modeled in the same way.
1. acknowledgements
　the origins of this work can be traced back to a discussion at the university of glasgow. the first author would like to thank bruce croft for extensive discussions and also for pointing out relevant literature especially previous literature on score modeling in information retrieval.
　this material is based on work supported in part by the national science foundation  library of congress and department of commerce under cooperative agreement number eec-1  in part by the national science foundation under grant numbers iri-1 and iis-1  in part by nsf multimedia cda1 and in part by spawarsyscen-sdgrant number n1-1. any opinions  findings and conclusions or recommendations expressed in this material are the author s  and do not necessarily reflect those of the sponsor s .
