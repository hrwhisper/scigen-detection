traditional association mining algorithms use a strict definition of support that requires every item in a frequent itemset to occur in each supporting transaction. in real-life datasets  this limits the recovery of frequent itemset patterns as they are fragmented due to random noise and other errors in the data. hence  a number of methods have been proposed recently to discover approximate frequent itemsets in the presence of noise. these algorithms use a relaxed definition of support and additional parameters  such as row and column error thresholds to allow some degree of  error  in the discovered patterns. though these algorithms have been shown to be successful in finding the approximate frequent itemsets  a systematic and quantitative approach to evaluate them has been lacking. in this paper  we propose a comprehensive evaluation framework to compare different approximate frequent pattern mining algorithms. the key idea is to select the optimal parameters for each algorithm on a given dataset and use the itemsets generated with these optimal parameters in order to compare different algorithms. we also propose simple variations of some of the existing algorithms by introducing an additional post-processing step. subsequently  we have applied our proposed evaluation framework to a wide variety of synthetic datasets with varying amounts of noise and a real dataset to compare existing and our proposed variations of the approximate pattern mining algorithms. source code and the datasets used in this study are made publicly available.
categories and subject descriptors
h.1  database management : database applications-data mining; d.1  software engineering : metrics-performance measures; i.1  pattern recognition : miscellaneous
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm 1-1-1/1 ...$1.
general terms
algorithms  experimentation  performance  verification
keywords
association analysis  approximate frequent itemsets  error tolerance  quantitative evaluation
1. introduction
모traditional association mining algorithms use a strict definition of support that requires every item in a frequent itemset to occur in each supporting transaction. in real-life datasets  this limits the recovery of frequent itemset patterns as they are fragmented due to random noise and other errors in the data.
모motivated by such considerations  various methods  1  1  1  1  1  1  have been proposed recently to discover approximate frequent itemsets  often called error-tolerant itemsets  etis   by allowing itemsets in which a specified fraction of the items can be missing. please see figure 1 for a conceptual overview. the most basic approach is to require only that a specified fraction of the items in a collection of items and transactions be present. however  such a 'weak' eti  provides no guarantees on the distribution of the items within this 'block ' i.e.  some rows or columns could be completely empty. to address this issue  a 'strong' eti was defined   which required that each row must have at most a specified fraction of items missing. the support of strong etis is simply the number of transactions that support the pattern  as in the traditional case  but support does not have the anti-monotone property  i.e.  support can increase as the number of items increases. thus  a heuristic algorithm for finding strong etis was developed.
모indeed  the lack of an anti-monotone support measure is one of the factors that has made the construction of algorithms for finding approximate itemsets very challenging. one solution that leads to an anti-monotone support measure is to allow only a fixed number of missing items in each row . this approach does not enforce any constraint on the number of items missing in a column  and is unappealing in that bigger itemsets should be allowed more missing items than the smaller ones. another potential solution is an approach that uses 'dense' itemsets . the support of an approximate itemset is defined in terms of the minimum support of every subset and is anti-monotone  but one may argue whether the definition of an approximate pattern used by this approach is as appealing as some of the other definitions since different subsets of items may be supported by different transactions.
모both strong and weak eti patterns can have empty columns. to handle this situation  approximate frequent itemsets  afi   were proposed. afis enforce constraints on the number of missing items in both rows and columns. one of the advantages of afis over weak/strong etis is that there is a limited version of an anti-monotone property that helps prune the search space. however  this algorithm cannot guarantee that no afis are overlooked since a heuristic approach is used to verify the column constraint. although afis might seem like the most natural and complete definition of an approximate itemset pattern  one might argue that it is easier and perhaps more meaningful to find approximate itemsets if at least some transactions contain all the items comprising an approximate itemset. this formed the motivation for ac-close  which finds afis based on the notion of core itemsets .
모although these approaches have shown to be successful in finding the approximate frequent itemsets  a systematic and quantitative approach to evaluate the patterns they generate is still lacking. indeed  most of the papers on approximate itemsets have provided only limited comparisons with other work: the paper that introduced afis  only presented a comparison to strong etis   while ac-close  only compared against afis  1  1 .
contributions of this paper:
  we perform a comprehensive evaluation of algorithms for finding approximate itemsets. building on the initial work by others  we propose an evaluation framework to compare various frequent pattern mining algorithms. we apply our evaluation framework to compare different approximate pattern mining algorithms based on their robustness to the input parameters  minsup   r  and  c  and on the quality of the patterns  measured in terms of significance and redundancy .
  our work highlights the importance of choosing optimal parameters. in general  approximate pattern mining algorithms use various parameters and given that each parameter can be set to many different values  the performance of various algorithms can be greatly affected. thus  it is more reasonable and fair to compare each algorithm's performance using its own optimal parameters in the parameter space.
  we propose simple variations of the existing algorithms proposed in  and  by adding an additional step to check if each candidate approximate pattern also satisfies the column error threshold   c . these modified algorithms not only generate high quality patterns but also compare well with those generated by 'afi'    and 'ac-close'   . the source codes of all the algorithms as well as synthetic and real data sets used in this study are available at the following website: www.cs.umn.edu/몲kumar/eti/.
모organization: the remainder of the paper is organized as follows. in sections 1  we briefly review the existing algorithms for approximate frequent pattern mining and we also propose simple variations of some of them. section 1 gives the details of our proposed evaluation methodology  including the definition of measures and how to select optimal parameters for each algorithm on a given dataset. we discuss our experimental results in section 1 and conclude with final remarks and future work in section 1.
1. approximate frequent pattern mining algorithms
모below  we briefly define different generalizations of frequent itemsets  which can generally be viewed as a hierarchy as shown in figure 1. it is important to note that each of these definitions

figure 1:	a conceptual hierarchy of approximate pat-
terns/algorithms
will lead to a traditional frequent itemset if their error-tolerance is set to 1. however  first we define a set of common terminology. assume  we have a binary transaction database consisting of a set i={i1 ... im} of items  and a set t={t1 ... tn} of transactions  where each ti has a subset of items from i. it is useful to think of such a database as a n-by-m binary matrix d  with each column corresponding to an item  and each row corresponding to a transaction. thus dj k = 1 if ik 뫍 tj  and 1 otherwise. an itemset  or pattern  i뫣 is said to be a frequent itemset if its support  the number of transactions it appears in  |{t뫣 : i뫣   t뫣}|   is more than some user-specified threshold denoted by minsup.
1 error tolerant itemsets  etis  - geti
모the concept of weak and strong eti was defined in . an itemset i뫣 at support minsup is said to be a weak eti with tolerance   if  t뫣 뫍 t such that |t뫣| 뫟 minsup and the following condition holds:
모it is difficult to find all weak etis  since we effectively have to search the entire pattern space without any pruning. also  we can have both rows and columns included that consist entirely of zeros  since there is no constraint as to where the 1's can occur within the itemset. an itemset i뫣 at support minsup is said to be a strong eti with tolerance   if  t뫣 뫍 t such that |t뫣| 뫟 minsup and the following condition holds
모this implies that for a given set of parameters  any strong eti is also a weak eti  figure 1 . also  the definition of strong eti helps to eliminate the possibility of adding spurious transactions. a greedy approach for computing strong etis is also proposed .
1 recursive weak etis - rw
모this algorithm was developed by sepp nen and mannila and named  somewhat misleadingly  dense itemsets . the idea was to add a recursive condition to the definition of a weak eti in order to overcome weak etis inherent susceptibility to spurious items. thus an itemset i뫣 is a recursive weak eti if i뫣  as well as all subsets of i뫣  are weak etis. one key point to note here is the set of transactions for the subset of items do not necessarily need to be the same. while this may seem to be a drawback  it still guarantees that any set of items within the itemset are related to one another. this algorithm also has an advantage of apriori-like pruning meaning if an itemset is not a recursive weak eti  no superset of it can possibly be a recursive weak eti. we denote this algorithm as 'rw' in this paper  see figure 1 for its relationship to other algorithms .
1 approximate frequent itemsets  afi 
모the concept of an approximate frequent itemset  afi  was developed in   although it was earlier introduced in . the idea is to extend the concept of strong eti to include separate row and column constraints   r and  c respectively . an itemset i뫣 at support minsup is said to be an afi with tolerance  r and  c if  t뫣 뫍 t such that |t뫣| 뫟 minsup and the following two conditions hold:
	p d	p d
.
모from the above properties  it can be seen that afis are a subset of strong etis  see figure 1 . one of the advantage of afis over weak/strong etis is that a relaxed version of an anti-monotone property holds for this pattern.
1 ac-close
모ac-close  uses a core pattern constraint in addition to row   r  and column   c  error thresholds to find frequent approximate patterns. specifically  this algorithm uses a parameter 붸 to control the percentage of supporting transactions that must have all the items in an itemset. this essentially further filters out patterns generated by the algorithm 'afi'  see figure 1 . in   an efficient top-down mining algorithm was also proposed to discover approximate frequent patterns with core patterns as the initial seeds. in this paper  we apply the additional parameter 붸 as a post-processing step to 'afi' patterns to obtain 'ac-close' patterns. conceptually  the patterns should be equivalent to the original ac-close  our implementation is not as efficient as the original 'ac-close' implementation.
1 error tolerant itemsets  etis  with strong post-processing - geti-pp
모in order to overcome the possibility that 'geti' can pick spurious items  we propose a variation of 'geti' which uses an additional parameter   c  to make sure every item in each eti generated by the algorithm 'geti' also satisfies the column constraint.
1 recursive weak etis with strong post processing - rw-pp
모in order to overcome the drawback of both strong etis and recursive weak etis  we added a post-processing step in algorithm 'rw' to make sure that all recursive weak etis also meets a certain column threshold  say  c . hence  patterns generated using 'rw-pp' lie in the intersection of 'rw' and strong eti  figure 1 .
1 recursive strong etis - rs
모since the recursive weak definition seems to work well in practice   a natural step is to define a recursive strong eti  where each subset must also be a recursive strong eti  figure 1 .
1. evaluation methodology
모the evaluation approach given in  compares the result quality of different noise-tolerant models using precision and recall. another evaluation framework for frequent sequential pattern mining  considers recoverability  spuriousness  redundancy and number of extraneous items as quantitative measures and influenced the design of our evaluation framework. building upon the definition of recoverability and spuriousness given in   below we describe the evaluation measures in more detail. note  we define b = {b1 ... bm} to be the set of base itemsets   true  patterns  and f = {f1 ... fn} to be the set of found patterns.
1 recoverability
this quantifies how well an approximate pattern mining algo-
rithm recovers the base patterns. in this definition  recoverability is similar to recall. to measure the recoverability of the true patterns using the found patterns  we create a matrix of size |f|x|b|  whose ijth element  ith row and jth column  is represented as fbij  i.e  the number of common items in found pattern fi and base pattern bj. we consider the recoverability of any base pattern bj to be the largest percent of the itemset found by any pattern fi that is associated with bj. for illustration purposes  as shown in table 1  for each base pattern  each column in table 1   we put a   on an entry which is maximum in that column. if there is a tie among found patterns for the maximum value  we put   on multiple entries. for computing the recoverability of the base pattern bj  we take any entry with a   on it and divide it by the size of bj. when we have more than one true pattern in the data  we need to combine the recoverability for each pattern into one measure. this is done by taking a weighted average  bigger patterns count more than smaller patterns  over all base patterns.

1 spuriousness
모although recoverability gives a good idea of what fraction of patterns are recovered by an algorithm  it does not give a complete picture. it is possible that a pattern is recovered solely because a found pattern contained a large number of items  not all necessarily related to the base pattern. thus just as precision is complementary to recall  we need another sibling measure of recoverability  recall  that measures the quality of the found patterns. the quality of a pattern can be estimated using the spuriousness measure which computes the number of items in the pattern that are not associated with the matching base pattern  i.e.  are spurious . hence  precision of a found pattern can be defined as 1   spuriousness. for illustration purposes  as shown in table 1  for each found pattern  each row in table 1   we put a # on an entry which is maximum in that row. if there is a tie among base patterns for the maximum value  we put a # on multiple entries in a row. for computing the spuriousness of the found pattern fi  we take any entry with a # on it  subtract it from |fi| and divide it by |fi|. since there are usually numerous found patterns  the spuriousness of a set of found patterns is equivalent to the number of spurious items over total items found.

 1 
f/bb1...bj...bmf1fb1 #...fb1j ...fb1m..................fifbi1...fbij#...fbim ..................fn...fbnj...fbnm#table 1: illustration of the matrix formed by found patterns and base  or true  patterns
1 significance
모based on the two measures  recoverability and spuriousness that are defined above  we define a measure called 'significance of the found patterns' that combines the two just as f-measure combines precision and recall .
모모모모모모모모모1    recoverability    1   spuriousness   significance f  = 
 recoverability +  1   spuriousness  
                                                    1  this measure balances the trade-off between useful and spurious information in the generated patterns.
1 redundancy
모as mentioned above  many approximate pattern mining algorithms produce huge number of patterns that are often a small variation of one another. hence  it is important to quantify how many of the found patterns are actually useful. for example  an itemset of size 1 has 1 subsets of size 1. if we recovered all of these  we would have a recoverability of 1  and a spuriousness of 1  but we would quickly be overwhelmed by the number of patterns. to measure the extent of the redundancy in the found patterns  we create a matrix r of size |f|x|f|  whose ijth element  ith row and jth column  is represented as ffij  i.e  the number of common items in patterns fi and fj. we then take the sum of the upper triangular matrix r excluding the diagonal to estimate the redundancy of a set of patterns.

모note  we do not take the average in the computation of redundancy to differentiate between the algorithms that generate unequal number of patterns but have the same average pairwise overlap of items. hence  this definition of redundancy indirectly takes into account the number of patterns generated by a mining algorithm.
1 robustness
모it is also important to compare approximate pattern mining algorithms based on their sensitivity to the input parameters. some of the algorithms are more sensitive to these parameters than others and the quality of the patterns changes drastically if the optimal parameters are not used. another parameter that these algorithms are usually sensitive to is the percentage of noise in the data. ideally  an evaluation framework should evaluate the algorithm not only on the basis of the quality of patterns  based on significance which is defined as the combination of recoverability and spuriousness  but also on the basis of the size of the parameter space for which this algorithm generates patterns of acceptable quality. this parameter sensitivity analysis quantifies the robustness of an algorithm  which is a very important criterion as optimal parameters for a real life dataset will not be known. to do this  we explore a reasonable three-dimensional parameter space of support threshold  minsup   row constraints   r  and column constraint   c  for each algorithm and choose the top k% combinations of parameters for which the values of the significance measure are highest. we then take the mean and variance of these top k% significance values. while the mean denotes the performance of the algorithm in terms of quality of the patterns  variance denotes how sensitive it is to the selection of parameters. ideally  one wants the mean to be high and variance to be low. however  in real-life applications  the unavailability of ground truth makes it inconceivable to obtain optimal parameters. hence  one would like to choose an algorithm that consistently shows low variance  high stability  in top k% significance values even at some cost of pattern quality  when tested on synthetic datasets.
1 choosing optimal parameters
모as mentioned earlier  different approximate pattern mining algorithms generate best results on different optimal parameters. to do a fair comparison among them  it is very important to compare the results obtained by each on its own optimal parameters setting. depending on the application  one may use different evaluation measures to choose optimal parameters. for a given dataset  we define the optimal parameters for an algorithm to be the ones that give the best value of significance. in case there is a tie among multiple parameter combinations  any parameter combination can be used.

figure 1: images of the base  no noise  synthetic datasets used in the experiments. all datasets have 1 transactions and 1 items.
1. experimental study
모we implemented the algorithms 'afi' and 'geti' and used the publicly available version of 'rw'. we also implemented the variations of 'geti' and 'rw' as discussed earlier. for 'ac-close'  we use the patterns generated by 'afi' and identify the ones that also have a core  completely dense  block of at least 붸   minsup support. though we also implemented and tested the algorithm 'rs'  we exclude its results because performance is generally worse than 'rw'. we then apply our proposed evaluation framework on both synthetic and real datasets to compare the performance of these approximate pattern mining algorithms. synthetic datasets are used because it is easier to evaluate the merits/demerits of different algorithms when the ground truth is available. we also used zoo dataset  as an example of a real dataset since a number of previously published studies have also used zoo dataset  1  1 . moreover to avoid the problem of huge number of patterns  we only compare the maximal patterns generated by different algorithms.
1 synthetic datasets
모various synthetic datasets were generated keeping in mind different characteristics of a real-life dataset. these characteristics include: 1  noise: almost all real-life datasets are noisy and finding true patterns but not just the confounding groups of noisy attributes is a non-trivial task; 1  types of patterns: there are variety of patterns that a real-life data set may have depending on the application domain. for example  patterns may be overlapping  either in items or in transactions or both  or non-overlapping  of different sizes  in terms of number of items or in terms of transactions .
모we generated 1 synthetic datasets  shown in figure 1  based on the above data characteristics. all the generated datasets have 1 items and 1 transactions. note that the datasets shown in figure 1 are only the base datasets with 1% noise level but we also generated noisy versions of each of them by uniformly adding random noise in fixed increments. following is the brief description of each of the synthetic datasets.
모data 1 - single embedded pattern of 1 items with a support of 1. data 1 - two non-overlapping embedded patterns of 1 and 1 items with a support of 1 and 1 respectively. data 1 - two embedded patterns of 1 items each  1 overlapping items  with a support of 1 and 1 respectively. data 1 - two embedded patterns of 1 and 1 items and with a support of 1 and 1 respectively  1 overlapping transactions . data 1 - two embedded patterns of 1 items each  1 overlapping items  with a support of 1 and 1 respectively  1 overlapping transactions . data 1 - three embedded patterns of 1  1 and 1 items with a support of 1  1 and 1 respectively. while patterns 1 & 1  and patterns 1 & 1 overlap in 1 items and 1 transactions  there is no overlap of either items or transactions in patterns 1 & 1. data 1 - four embedded patterns of 1  1  1 and 1 items with a support of 1  1  1 and 1 respectively. patterns 1 & 1 overlap in 1 items and 1 transactions  patterns 1 & 1 overlap in 1 item and 1 transactions  and patterns 1 & 1 overlap in 1 transactions but no items. data 1 - similar to data 1 except that the patterns are generated in a different way. all the rows and columns in the data are randomly shuffled before a new pattern is embedded. figure 1 shows the shuffled data after 1 patterns are embedded.
모given a base  no noise  synthetic dataset  we first add random noise by flipping its elements with a probability of n%  which means that  1   n % is the probability of any element remaining constant. we vary the value of n to obtain noisy versions of the base synthetic dataset. we then run the algorithms on each of the noisy dataset using a wide range of parameters like support threshold  minsup   row   r  and column   c  tolerance. while we use 1  1  1  1  1 and 1 as 1 different values for row   r  and column   c  tolerance; the range of minsup is selected based on the size of the implanted true patterns in the synthetic data. moreover  as the noise is random  we repeat the complete process of adding noise and running the algorithms on all possible parameter combinations 1 times and report average results. to give an illustration of the computational complexity  consider applying the afi  which uses both row   r  and column   c  tolerance  on a single dataset. to cover the parameter space defined by noise  support threshold  minsup   row   r  and column   c  threshold  we need to run this algorithm 1  number of runs  x 1  assuming 1 different noise levels  x 1  assuming 1 different values of support parameter  x 1  # of values of  r  x 1  # of values of  c  = 1 times. as the true patterns for all synthetic datasets are known  we run all the algorithms on a wide range of parameters to select the best combination  refered to as the optimal parameters  for each. the performance of the algorithms is then compared with each other using optimal parameters.
1.1 results on synthetic datasets
모due to the space constraints  we only show results on synthetic data 1  figure 1  and synthetic data 1  figure 1 . results on other datasets are similar and are available on the website that contains all the source codes and the datasets  see section 1 . also  tables 1 and 1 shows the optimal parameters selected by each algorithm for these datasets at different noise levels. sometimes there are multiple parameter values for which the generated patterns show the same performance measured in terms of significance. in such cases  we show the parameter combinations corresponding to minimum and maximum support within such cases. noise level is varied from 1% to 1% in increments of 1% for synthetic data 1 and from 1% to 1% in increments of 1% for synthetic data 1. again  the maximum amount of noise level introduced in the data is governed by the size of the smallest implanted pattern.
모in both figure 1 and figure 1  we can see that the performance of the 'apriori' algorithm  measured in terms of significance  falls most rapidly as the random noise in the data increases. although as seen from table 1  'apriori' uses low support threshold  minsup = 1  for all noise levels to recover true patterns  due to the rigid definition of support  overall recoverability and hence significance is low.
generally speaking  the performance of all the algorithms falls
as expected when the random noise is increased in the data. as the noise increases  recoverability goes down  spuriousness goes up and as a result  the significance of the patterns goes down. although  every algorithm chooses optimal parameters corresponding to the best value of significance  the effect of random noise is different on each algorithm. broadly  these algorithms could be divided into two groups: one that uses single parameter   and one that uses two parameters  r and  c. usually  single parameter algorithms 'rw' and 'geti' pick more spurious items than those that uses two parameters. this is because these single parameter algorithms only require each supporting transaction to have at least  1    r  fraction of items. they do not impose any restriction on the items in the column. on the other hand  algorithms 'afi'  'ac-close' and our proposed variations 'geti-pp' and 'rw-pp' pick fewer spurious items and hence have a better significance values. afi uses two parameters  r and  c and hence additionally requires each item in a pattern to be supported by at least  1    c  fraction of total supporting transactions. 'ac-close' further requires that pattern should have a core  completely dense  block of support at least 붸   minsup  where 붸 뫍  1 . 'geti-pp' and 'rw-pp' uses another parameter  c in addition to the parameter r used by the algorithms 'geti' and 'rw' to check if all the items in each pattern have more than  1  c  fraction of supporting transactions. this helps in filtering some of the patterns that have spurious items. hence  'geti-pp' and 'rw-pp' finds the patterns with a flavor similar to the ones generated by 'afi'.
모as can be clearly seen from significance plots in figure 1 and figure 1  generally 'afi'  'geti-pp'  and 'rw-pp' have similar performance. however  the optimal parameters used by these algorithms are different as shown in tables 1  for data 1  and 1  for data 1 . for instance at a noise level of 1% in synthetic data 1  'getipp' can find the patterns at minsup = 1  but 'afi' and 'rw-pp' can only find them at minsup = 1. similarly at a 1% noise level in table 1  'rw-pp' finds same quality patterns at either parameters minsup = 1   r = 1  and  c = 1 or at minsup = 1   r = 1  and  c = 1. therefore  by relaxing  c from 1 to 1  'rw-pp' was able to find same quality patterns at higher support. all such cases  where multiple optimal parameter values are possible  are shown in the optimal parameters tables.
모our results here demonstrate that differences amongst most of these algorithms are not very large when optimal parameters are used. this finding is not consistent with some of the conclusions in previous work   1  1  . in  'afi' and 'geti' were compared on a simple synthetic dataset with one embedded pattern  similar to our synthetic data 1  and 'afi' was found to outperforms 'geti' by a huge margin both in terms of recoverability and spuriousness. following are some of the possible reasons for this inconsistency:  1  parameter space  minsup   r   c  is not explored in  to choose the optimal parameters for each algorithm  and  1  an exact matching criterion between the found pattern and the true pattern might have been used. in  'afi' and 'ac-close' were compared on synthetic datasets generated using the ibm data generator. ground truth is defined to be the traditional dense patterns obtained using the apriori algorithm in the noise-free version of the data. this truth appears to favor the algorithm 'ac-close'  which requires a pattern to have a core block of support at least 붸   minsup.


figure 1: results on synthetic data 1 - comparing different algorithms in terms of performance and robustness.
algorithmsnoise levels  % 111apriori 1  1  1  1  1  sup  1 geti 1  1  1.1  1.1  1.1  sup  r  1  1.1 geti-pp 1 1  1 1  1.1.1  1.1.1  1.1.1  sup  r  c  1 1  1.1.1  1.1.1 rw 1  1  1.1  1.1  1.1  sup  r  1.1  1.1  1.1 rw-pp 1 1  1 1  1.1.1  1.1.1  1.1.1  sup  r  c  1 1  1.1.1  1.1.1 afi 1 1  1 1  1.1.1  1.1 1  1.1 1  sup  r  c  1 1  1.1.1  1.1.1 acclose 1 1  1 1  1.1.1.1  1.1.1.1  1.1.1.1  sup  r  c 붸  1 1 table 1: optimal parameters for different algorithms on synthetic data 1.모we also compare the algorithms based on their sensitivity to the input parameters because in real datasets where the ground truth is not available  optimal parameters cannot be estimated. ideally for a real dataset  one would like to choose an algorithm which gives acceptable performance as measured in terms of significance and yet be less sensitive to the input parameters. figures 1 and 1 shows the variance of the top k% significance values obtained on different parameter combinations for dataset 1 and 1 respectively. the mean of the top k% significance values is also shown to indicate the overall performance. note  we do not show 'apriori' in the plots of mean and variance of top k% values because 'apriori' only uses one minsup parameter while parameters  r and  c are 1 by design. also remember  variance does not indicate the performance of the algorithm  it only indicates how consistently an algorithm generates the patterns with similar quality on different parameter settings. it is more meaningful to compare only those algorithms on this measure  which show acceptable significance values. we set of the total runs  in this paper. it is important to note that 'geti' and 'rw'  which require only one parameters  r apart from minsup have fewer number of runs in comparison to algorithms 'afi'  'ac-close'  'geti-pp' and 'rw-pp'  which require two parameters  r and  c apart from minsup and hence have more number of runs. figure 1 and 1 shows the mean and variance of top 1  out of total 1  and top 1  out of total 1  significance values for these two sets of algorithms respectively. we notice that although the difference in variance is not too high on these datasets  'ac-close' shows relatively high variance  hence less robustness  than others. this may be because of the requirement of specifying fourth parameter 붸  which makes it difficult to estimate the optimal parameters.
1 real dataset: zoo data


figure 1: results on synthetic data 1 - comparing different algorithms in terms of performance and robustness.
algorithmsnoise levels  % 111apriori 1  1  1  1  1  sup  1 geti 1  1  1  1.1  1.1  sup  r  1.1  1.1 geti-pp 1 1  1 1  1 1  1.1.1  1.1.1  sup  r  c  1.1.1  1.1.1 rw 1  1  1  1.1  1.1  sup  r  1.1  1.1 rw-pp 1 1  1 1  1 1  1.1.1  1.1.1  sup  r  c  1.1.1  1.1.1  1.1.1 afi  sup  r  c  1 1  1.1.1  1 1  1.1 1  1.1 1 acclose  sup  r  c 붸  1 1  1.1.1.1  1 1  1.1.1.1  1.1.1.1 table 1: optimal parameters for different algorithms on synthetic data 1.모in the zoo dataset   there are 1 instances  animals  with 1 boolean attributes  e.g. aquatic  tail  hair  eggs etc.  and a class label  mammal  bird etc. . for approximate pattern mining  we consider transactions to be animals and items to be different features that characterizes them. finding frequent itemsets in this data provides the ability to predict the class of an animal. in general  approximate itemsets are more suited for this problem because not all instances of the same class have all the common features. for example  though most mammals produce milk  are covered in hair  and are toothed  platypus lack teeth and dolphin lack hair.
1.1 results on real dataset
모we only focused on three classes  mammals  birds and sea-creatures  as they have more than 1 instances each. as we saw from the results on synthetic datasets  'geti-pp' and 'rw-pp' usually outperforms 'geti' and 'rw' respectively  we only show the results of 'afi'  'geti-pp' and 'rw-pp'  see table 1 . our results indicate that all the algorithms discussed in this paper  except of course 'apriori'  can find the itemsets that defines the two classes  mammals and birds  almost perfectly. an itemset of size-1 is supported by  instances of mammals  and interestingly no other instance  animal  from any other class supports it. similarly  an itemset that is only supported by the instances of the bird's class can be found. for example  'geti' finds an itemset of size-1 at minsup = 1 and  r = 1  which is supported by all  and only  the instances from the bird's class. the inconsistency of these results with those in  1  1  is due to the difference in selection of optimal parameters.
모the instances of the third class sea-creatures share 1 common features but the same 1 features are also shared by some instances from mammals and birds class. hence  an itemset comprising of
algorithmsmammalsbirdsgeti-pp 1  1  1  1  1  1 rw-pp 1  1  1  1  1  1 afi 1  1  1  1  1  1 table 1: parameters  sup  r  c  for different algorithms on zoo data
these 1 features alone cannot be used to predict the instances of the class sea-creatures. truly speaking  sea-creatures distinguish themselves from other classes because they lack some features that instances from other classes have. association pattern mining in general does not find patterns to capture such information. this requires generalizing the definition of patterns to not only include patterns like  a and b and c  but also like   a or b  and  notc   but this generalization is beyond the scope of this paper.
1 efficiency and scalability
1.1 run-time
모in this section  we compare the efficiency of different algorithms for varying amount of noise in the dataset. considering the fact that different parameters will result in different run-time for each algorithm  we run the algorithms on all the different parameter combinations and use the total run-time to compare them. all the algorithms are run on a linux machine with 1 intel r  xeon r  cpus  e1   1ghz   with 1 processes . because 'geti-pp' and 'rw-pp' are the variations of 'geti' and 'rw' respectively  we only report results on 'geti-pp' and 'rw-pp'. also  the run-time of 'ac-close' algorithm is not included. in table 1  we report the run-times  in seconds  of the algorithms 'geti-pp'  'rw-pp' and 'afi' on synthetic data 1 with noise levels varied from 1% to 1% in increments of 1%. note  this is the total time taken by the algorithm for 1 paramater combinations  1 different minsup values and 1 different  r and  c values . it is interesting to see the differences among the algorithms in terms of the increase in run-time as the noise increases. though  'afi' is computationally more efficient than 'geti-pp' when noise in the dataset is low  it is very expensive when noise in the dataset is high. however  it is important to note that this is also due to high value of row and column error threshold. it is also interesting that run-time of 'rw-pp' only increase marginally when the noise is increased from 1% to 1% after which it also shows rapid increase.
algorithmsnoise level  % 111111afi1111----geti-pp111111rw-pp11111-table 1: comparison of run-times of different algorithms for varying amount of random noise.
1 effect of higher random noise
모in the experiments shown so far  random noise added to the synthetic data was limited by the size  number of transaction  of the smallest base  or true  pattern. this was done  1  to make sure that the truth remains meaningful and  1  to make the parameter space search feasible for mining algorithm afi  which is computationally expensive when the random noise in the data is high. however  in some real-life applications  small but true patterns are hidden in even larger amounts of random noise. in such cases  it is still desired that algorithms could recover as many significant and useful patterns from the dataset as possible. hence  the trade off between recoverability and spuriousness becomes even more challenging. however  as the computational efficiency of the algorithms decreases as the noise increases  we designed the following two schemes to make the comparison among algorithms feasible:
  scheme 1: for algorithms that could finish in time 붻  붻 = 1 hour in our experiments   we search the whole parameter space.
  scheme 1: for algorithms that could not finish certain parameter combinations in time 붻  we search the whole parameter space in the order of complexity. also  the row and the column thresholds were set equal. 'stricter' parameter combinations  high support threshold  small error tolerance  take less time to finish and hence will be tested before the 'less strict' ones that have small support threshold and high error tolerance. any combination of parameters  support threshold and error tolerance  for which the algorithm does not finish in time 붻  is not considered while selecting the optimal parameters.
모we compare the performance of different algorithms under more noise only on synthetic data 1 since it appears to be closest to a real dataset. in addition to the original noise levels of 1%  1%  1%  1%  and 1% considered in the previous experiment  noise levels of 1%  1%  and 1% were also tried. as the algorithm 'afi' could not finish certain parameter combinations at noise level of 1% within time 붻  we use the second scheme above to search the parameter space. although other algorithms also took longer  they were able to finish all the runs in time 붻. figure 1 shows the results on synthetic data 1 including more noise levels 1%  1%  and 1%. it is clear the performance of all the algorithms suffers due to more random noise in the data. however  performance of 'rw' seems to suffer more than the others as it is picking more spurious items. also as 'afi' could not finish certain runs at noise level 1% in time 붻 and hence had to choose the optimal parameters from a smaller parameter space  its performance falls at noise level 1%. moreover  as we derive 'ac-close' patterns from the 'afi' patterns using the core block constraint  performance of 'ac-close' falls as well. a more efficient implementation of 'ac-close' may not have this problem. interestingly  in this case 'geti' and 'geti-pp' have better performance than 'rw-pp' but as we see from the variance of the top k%  where of the total runs   significance values  figure 1   'geti' and 'geti-pp' seems to be less robust as the noise in the data increases.
1. conclusions
모in this paper  we have proposed an evaluation framework and showed its applicability to compare different approximate pattern mining algorithms on both synthetic and real datasets. following are the general conclusions of our evaluation study:   our results suggest that enforcing the column error tolerance  c  as introduced in afi algorithm   over the concepts of strong eti or recursive weak eti  makes them much more effective in terms of finding the true patterns with less spurious information.
  all the existing  'afi' and 'ac-close'  as well as our proposed variations  'geti-pp' and 'rw-pp'  of the algorithms  which use the column error tolerance  c perform similarly when the optimal parameters are selected. this is because adding an additional constraint helps filter out patterns with spurious items.
  the computational efficiency of the variations 'geti-pp' and 'rw-pp' seems to be much better than 'afi' specially at higher noise levels. moreover  their performance in terms of the quality of the patterns is also comparable to those generated by 'afi'.


figure 1: results on synthetic data 1 with more random noise levels 1% 1% 1%모these conclusions are in contrast to some of the previously published studies   1  1  1  .  1  1  compared only 'geti' and 'afi' and suggested 'afi' significantly outperforms 'geti'. we showed that although this is true  the difference is not that significant. moreover 'geti-pp'  a simple variation of 'geti' we proposed performs comparable to 'afi'.  compared only 'ac-close' and 'afi' and suggested 'ac-close'performs better than 'afi'. however  we observed no significant differences in 'afi' and 'ac-close'. we believe these differences are partly due to the fact that previous studies did not select the optimal parameters for each algorithm and partly because of the choice of the datasets.
모this comparative study  though far more comprehensive than other previous studies  has several limitations. most of the patterns considered in this paper are simple embedded patterns in the synthetic datasets and hence may not reflect various aspects of complex real datasets. even the real zoo dataset is not very complex in terms of its size and availability of ground truth. though  it would be better to apply the evaluation framework on a complex real dataset  lack of ground truth knowledge makes this much harder.
모there are still many interesting problems that need to be studied. in the evaluation framework  it would be interesting to incorporate redundancy and other measures in the process of optimal parameter selection. on the algorithm side  extending approximate pattern mining algorithms to work with categorical and continuous valued data could prove to be very beneficial to many application domains.
1. acknowledgements
모this work was supported in part by nsf and by the university of minnesota rochester biomedical informatics and computational biology program traineeship award and research seed grants. access to computing facilities was provided by the minnesota supercomputing institute.
