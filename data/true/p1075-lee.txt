due to its superiority such as low access latency  low energy consumption  light weight  and shock resistance  the success of flash memory as a storage alternative for mobile computing devices has been steadily expanded into personal computer and enterprise server markets with ever increasing capacity of its storage. however  since flash memory exhibits poor performance for small-to-moderate sized writes requested in a random order  existing database systems may not be able to take full advantage of flash memory without elaborate flash-aware data structures and algorithms. the objective of this work is to understand the applicability and potential impact that flash memory ssd  solid state drive  has for certain type of storage spaces of a database server where sequential writes and random reads are prevalent. we show empirically that up to more than an order of magnitude improvement can be achieved in transaction processing by replacing magnetic disk with flash memory ssd for transaction log  rollback segments  and temporary table spaces.
categories and subject descriptors
h. information systems  h.1 database management : h.1 physical design
general terms
design  algorithms  performance  reliability  this work was partly supported by the it r&d program of mic/iita  1-s-1  and mic  korea under itrc iita-1- c1-1 . the authors assume all responsibility for the contents of the paper.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
keywords
flash-memory database server  flash-memory ssd
1. introduction
　due to its superiority such as low access latency  low energy consumption  light weight  and shock resistance  the success of flash memory as a storage alternative for mobile computing devices has been steadily expanded into personal computer and enterprise server markets with ever increasing capacity of its storage. as it has been witnessed in the past several years  two-fold annual increase in the density of nand flash memory is expected to continue until year 1 . flash-based storage devices are now considered to have tremendous potential as a new storage medium that can replace magnetic disk and achieve much higher performance for enterprise database servers .
　the trend in market is also very clear. computer hardware manufacturers have already launched new lines of mobile personal computers that did away with disk drives altogether  replacing them with flash memory ssd  solid state drive . storage system vendors have started lining up their flash-based solutions in terabyte-scale targeting large-scale database servers as one of the main applications.
　adoption of a new technology  however  is often deterred by lack of in-depth analysis on its applicability and costeffectiveness  and is even considered risky when it comes to mission critical applications. the objective of this work is to evaluate flash memory ssd as stable storage for database workloads and identify the areas where flash memory ssd can be best utilized  thereby accelerating its adoption as an alternative to magnetic disk and maximizing the benefit from this new technology.
　most of the contemporary database systems are configured to have separate storage spaces for database tables and indexes  log data and temporary data. whenever a transaction updates a data object  its log record is created and stored in stable storage for recoverability and durability of the transaction execution. temporary table space stores temporary data required for performing operations such as sorts or joins. if multiversion read consistency is supported  another separate storage area called rollback segments is created to store previous versions of data objects.
　for the purpose of performance tuning as well as recoverability  these distinct storage spaces are often created on physically separate storage devices  so that i/o throughput can increase  and i/o bottlenecks can be detected and addressed with more ease. while it is commonly known that accessing data stored in secondary storage is the main source of bottlenecks in database processing  high throughput of a database system cannot be achieved by addressing the bottlenecks only in spaces for tables and indexes but also in spaces for log  temporary and rollback data.
　recent studies on database availability and architecture report that writing log records to stable storage is almost guaranteed to be a significant performance bottleneck  1  1 . in on-line transaction processing  oltp  applications  for example  when a transaction commits  all the log records created by the transaction have to be force-written to stable storage. if a large number of concurrent transactions commit at a rapid rate  the log tail will be requested to be flushed to disk very often. this will then lengthen the average wait time of committing transactions and delay the release of locks further  and eventually increase the overall runtime overhead substantially.
　accessing data stored in temporary table spaces and rollback segments also takes up a significant portion of total i/o activities. for example  queries performing a table scan  join  sort or hash operation are very common in a data warehousing application  and processing those queries  except simple table scans  will require a potentially large amount of intermediate data to be written to and read from temporary table spaces. thus  to maximize the throughput of a database system  it is critical to speed up accessing data stored in those areas as well as in the data space for tables and indexes.
　previous work has reported that flash memory exhibits poor performance for small-to-moderate sized writes requested in a random order  and the best attainable performance may not be obtained from database servers without elaborate flash-aware data structures and algorithms . in this paper  in contrast  we demonstrate that flash memory ssd can help improve the performance of transaction processing significantly  particularly as a storage alternative for transaction log  rollback segments and temporary table spaces. to accomplish this  we trace quite distinct data access patterns observed from these three different types of data spaces  and analyze how magnetic disk and flash memory ssd devices handle such i/o requests  and show how the overall performance of transaction processing is affected by them.
　while the previous work on in-page logging is targeted at regular table spaces for database tables and indexes where small random writes are dominant   the objective of this work is to understand the applicability and potential impact that flash memory ssd has for the other data spaces where sequential writes and random reads are prevalent. the key contributions of this work are summarized as follows.
  based on a detailed analysis of data accesses that are traced from a commercial database server  this paper provides an understanding of i/o behaviors that are dominant in transaction log  rollback segments  and temporary table spaces. it also shows that this i/o pattern is a good match for the dual-channel  superblock design of flash memory ssd as well as the characteristics of flash memory itself.   this paper presents a quantitative and comparative analysis of magnetic disk and flash memory ssd with respect to performance impacts they have on transactional database workloads. we observed more than an order of magnitude improvement in transaction throughput and response time by replacing magnetic disk with flash memory ssd as storage media for transaction log or rollback segments. in addition  more than a factor of two improvement in response time was observed in processing a sort-merge or hash join query by adopting flash memory ssd instead of magnetic disk for temporary table spaces.
  the empirical study carried out in this paper demonstrates that low latency of flash memory ssd can alleviate drastically the log bottleneck at commit time and the problem of increased random reads for multiversion read consistency. with flash memory ssd  i/o processing speed may no longer be as serious a bottleneck as it used be  and the overall performance of query processing can be much less sensitive to tuning parameters such as the unit size of physical i/o. the superior performance of flash memory ssd demonstrated in this work will help accelerate adoption of flash memory ssd for database applications in the enterprise market  and help us revisit requirements of database design and tuning guidelines for database servers.
　the rest of this paper is organized as follows. section 1 presents a few key features and architecture of samsung flash memory ssd  and discusses its performance characteristics with respect to transactional database workloads. section 1 describes the experimental settings that will be used in the following sections. in section 1  we analyze the performance gain that can be obtained by adopting flash memory ssd as stable storage for transaction log. section 1 analyzes the patterns in which old versions of data objects are written to and read from rollback segments  and shows how flash memory ssd can take advantage of the access patterns to improve access speed for rollback segments and the average response time of transactions. in section 1  we analyze the i/o patterns of sort-based and hash-based algorithms  and discuss the impact of flash memory ssd on the algorithms.
lastly  section 1 summarizes the contributions of this paper.
1. design of samsung flash ssd
　the flash memory ssd  solid state drive  of samsung electronics is a non-volatile storage device based on nandtype flash memory  which is being marketed as a replacement of traditional hard disk drives for a wide range of computing platforms. in this section  we first briefly summarize the characteristics of flash memory as a storage medium for databases. we then present the architecture and a few key features of samsung flash memory ssd  and discuss its performance implications on transactional database workloads.
1 characteristics of flash memory
　flash memory is a purely electronic device with no mechanically moving parts like disk arms in a magnetic disk drive. therefore  flash memory can provide uniform random access speed. unlike magnetic disks whose seek and rotational delay often becomes the dominant cost of reading or writing a sector  the time to access data in flash memory is almost linearly proportional to the amount of data irrespective of their physical locations in flash memory. the ability of flash memory to quickly perform a sector read or a sector  clean  write located anywhere in flash memory is one of the key characteristics we can take advantage of.
　on the other hand  with flash memory  no data item  or a sector containing the data item  can be updated in place just by overwriting it. in order to update an existing data item stored in flash memory  a time-consuming erase operation must be performed before overwriting. the erase operation cannot be performed selectively on a particular data item or sector  but can only be done for an entire block of flash memory called erase unit containing the data item  which is much larger  typically 1 kbytes  than a sector. to avoid performance degradation caused by this erase-before-write limitation  some of the data structures and algorithms of existing database systems may well be reconsidered .
　the read and write speed of flash memory is asymmetric  simply because it takes longer to write  or inject charge into  a cell until reaching a stable status than to read the status from a cell. as will be shown later in this section  table 1   the sustained speed of read is almost twice faster than that of write. this property of asymmetric speed should also be considered when reviewing existing techniques for database system implementations.
1 architecture and key features
　high bandwidth is one of the critical requirements for the design of flash memory ssd. the dual-channel architecture  as shown in figure 1  supports up to 1-way interleaving to hide flash programming latency and to increase bandwidth through parallel read/write operations. an automatic interleaving hardware logic is adopted to maximize the interleaving effect with the minimal firmware intervention .

figure 1: dual-channel architecture of ssd
a firmware layer known as flash translation layer  ftl   1  1  is responsible for several essential functions of flash memory ssd such as address mapping and wear leveling. the address mapping scheme is based on super-blocks in order to limit the amount of information required for logical-tophysical address mapping  which grows larger as the capacity of flash memory ssd increases. this super-block scheme also facilitates interleaved accesses of flash memory by striping a super-block of one mbytes across four flash chips. a super-block consists of eight erase units  or large blocks  of 1 kbytes each. under this super-block scheme  two erase units of a super-block are allocated in the same flash chip.
　though flash memory ssd is a purely electronic device without any moving part  it is not entirely latency free for accessing data. when a read or write request is given from a host system  the i/o command should be interpreted and processed by the ssd controller  referenced logical addresses should be mapped to physical addresses  and if mapping information is altered by a write or merge operation  then the mapping table should be updated in flash memory. with all these overheads added up  the read and write latency observed from the recent ssd products is approximately 1 msec and 1 msec  respectively.
　in order to reduce energy consumption  the one-chip controller uses a small amount of sram for program code  data and buffer memory.1 the flash memory ssd drives can be interfaced with a host system through the ide standard ata-1.
1 flash ssd for database workload
　typical transactional database workloads like tpc-c exhibit little locality and sequentiality in data accesses  a high percentage of which are synchronous writes  e.g.  forcedwrites of log records at commit time . such latency hiding techniques as prefetching and write buffering become less effective for this type of workload  and the performance of transactional database applications tends to be more closely limited by disk latency than disk bandwidth and capacity . nonetheless  for more than a decade in the past  the latency of disk has improved at a much slower pace than the bandwidth of disk  and the latency-bandwidth imbalance is expected to be even more evident in the future .
　in this regard  extremely low latency of flash memory ssd lends itself to being a new storage medium that replaces magnetic disk and improves the throughput of transaction processing significantly. table 1 shows the performance characteristics of some contemporary hard disk and flash memory ssd products. though the bandwidth of disk is still two to three times higher than that of flash memory ssd  more importantly  the read and write latency of flash memory ssd is smaller than that of disk by more than an order of magnitude.
　as is briefly mentioned above  the low latency of flash memory ssd can reduce the average transaction commit time and improve the throughput of transaction processing significantly. if multiversion read consistency is supported  rollback data are typically written to rollback segments sequentially in append-only fashion and read from rollback segments randomly during transaction processing. this pe-

1
 the flash memory ssd drive tested in this paper contains 1 kbyte sram.
storagehard disk flash ssd average1 ms1 ms  read latency1 ms  write sustained1 mb/sec1 mb/sec  read transfer rate1 mb/sec  write  disk: seagate barracuda 1 st1as  average latency for seek and rotational delay;  ssd: samsung mcaqe1app-1xa drive with k1wag1a 1 gbits slc nand chips
table 1: magnetic disk vs. nand flash ssd
culiar i/o pattern is a good match for the characteristics of flash memory itself and the super-block scheme of the samsung flash memory ssd. external sorting is another operation that can benefit from the low latency of flash memory ssd  because the read pattern of external sorting is quite random during the merge phase in particular.
1. experimental settings
　before presenting the results from our workload analysis and performance study in the following sections  we describe the experimental settings briefly in this section.
　in most cases  we ran a commercial database server  one of the most recent editions of its product line  on two linux systems  kernel version 1.1   each with a 1 ghz intel pentium dual-core processor and 1 gb ram. these two computer systems were identical except that one was equipped with a magnetic disk drive and the other with a flash memory ssd drive instead of the disk drive. the disk drive model was seagate barracuda 1 st1as with 1 gb capacity  1 rpm and sata interface. the flash memory ssd model was samsung standard type mcaqe1app-1xa with 1 gb capacity and 1 inch pata interface  which internally deploys samsung k1wag1a 1 gbits slc nand flash chips  shown in figure 1 . these storage devices were connected to the computer systems via a sata or pata interface.

figure 1: samsung nand flash ssd
　when either magnetic disk or flash memory ssd was used as stable storage for transaction log  rollback segments  or temporary table spaces  it was bound as a raw device in order to minimize interference from data caching by the operating system. this is a common way of binding storage devices adopted by most commercial database servers with their own caching scheme. in all the experiments  database tables were cached in memory so that most of io activities were confined to transaction log  rollback segments and temporary table spaces.
1. transaction log
　when a transaction commits  it appends a commit type log record to the log and force-writes the log tail to stable storage up to and including the commit record. even if a noforce buffer management policy is being used  it is required to force-write all the log records kept in the log tail to ensure the durability of transactions .
　as the speed of processors becomes faster and the memory capacity increases  the commit time delay due to force-writes increasingly becomes a serious bottleneck to achieving high performance of transaction processing . the response time tresponse of a transaction can be modeled as a sum of cpu time tcpu  read time tread  write time twrite and commit time tcommit. tcpu is typically much smaller than io time. even tread and twrite become almost negligible with a large capacity buffer cache and can be hidden by asynchronous write operations. on the other hand  commit time tcommit still remains to be a significant overhead  because every committing transaction has to wait until all of its log records are force-written to log  which in turn cannot be done until forced-write operations requested by other transactions earlier are completed. therefore  the amount of commit-time delay tends to increase as the number of concurrent transactions increases  and is typically no less than a few milliseconds.
　group commit may be used to alleviate the log bottleneck . instead of committing each transaction as it finishes  transactions are committed in batches when enough logs are accumulated in the log tail. though this group commit approach can significantly improve the throughput of transaction processing  it does not improve the response time of individual transactions and does not remove the commit time log bottleneck altogether.
　log records are always appended to the end of log. if a separate storage device is dedicated to transaction log  which is commonly done in practice for performance and recoverability purposes  this sequential pattern of write operations favors not only hard disk but also flash memory ssd. with no seek delay due to sequential accesses  the write latency of disk is reduced to only half a revolution of disk spindle on average  which is equivalent to approximately 1 msec for disk drives with 1 rpm rotational speed.
　in the case of flash memory ssd  however  the write latency is much lower at about 1 msec  because flash memory ssd has no mechanical latency but only a little overhead from the controller as described in section 1. even the no in-place update limitation of flash memory has no negative impact on the write bandwidth in this case  because log records being written to flash memory sequentially do not cause expensive merge or erase operations as long as clean flash blocks  or erase units  are available. coupled with the low write latency of flash memory  the use of flash memory ssd as a dedicated storage device for transaction log can reduce the commit time delay considerably.
　in the rest of this section  we analyze the performance gain that can be obtained by adopting flash memory ssd as stable storage for transaction log. the empirical results from flash memory ssd drives are compared with those from magnetic disk drives.
1 simple sql transactions
　to analyze the commit time performance of hard disk and flash memory ssd drives  we first ran a simple embedded sql program on a commercial database server  which ran on two identical linux systems except that one was equipped with a magnetic disk drive and the other with a flash memory ssd drive instead of the disk drive. this embedded sql program is multi-threaded and simulates concurrent transactions. each thread updates a single record and commits  and repeats this cycle of update and commit continuously. in order to minimize the wait time for database table updates and increase the frequency of commit time forcedwrites  the entire table data were cached in memory. consequently  the runtime of a transaction excluding the commit time  i.e.  tcpu + tread + twrite  was no more than a few dozens of microseconds in the experiment. table 1 shows the throughput of the embedded sql program in terms of transactions-per-seconds  tps .
no. of concurrent transactionshard diskflash ssdtps%cputps%cpu11111111111111table 1: commit-time performance of an embedded
sql program measured in transactions-in-seconds  tps  and cpu utilization
　regarding the commit time activities  a transaction can be in one of the three distinct states. namely  a transaction  1  is still active and has not requested to commit   1  has already requested to commit but is waiting for other transactions to complete forced-writes of their log records  or  1  has requested to commit and is currently force-writing its own log records to stable storage.
　when a hard disk drive was used as stable storage  the average wait time of a transaction was elongated due to the longer latency of disk writes  which resulted in an increased number of transactions that were kept in a state of the second or third category. this is why the transaction throughput and cpu utilization were both low  as shown in the second and third columns of table 1.
　on the other hand  when a flash memory ssd drive was used instead of a hard disk drive  much higher transaction throughput and cpu utilization were observed  as shown in the fourth and fifth columns of table 1. with a much shorter write latency of flash memory ssd  the average wait time of a transaction was shortened  and a relatively large number of transactions were actively utilizing cpu  which in turn resulted in higher transaction throughput. note that the cpu utilization was saturated when the number of concurrent transactions was high in the case of flash memory ssd  and no further improvement in transaction throughput was observed when the number of concurrent transactions was increased from 1 to 1  indicating that cpu was a limiting factor rather than i/o.
1 tpc-b benchmark performance
　in order to evaluate the performance of flash memory ssd as a storage medium for transaction log in a more harsh environment  we ran a commercial database server with tpc-b workloads created by a workload generation tool. although it is obsolete  the tpc-b benchmark was chosen because it is designed to be a stress test on different subsystems of a database server and its transaction commit rate is higher than that of tpc-c benchmark . we used this benchmark to stress-test the log storage part of the commercial database server by executing a large number of small transactions causing significant forced-write activities.
　in this benchmark test  the number of concurrent simulated users was set to 1  and the size of database and the size of database buffer cache of the server were set to 1 mbytes and 1 mbytes  respectively. note that this setting allows the database server to cache the entire database in memory  such that the cost of reading and writing data pages is eliminated and the cost of forced writing log records remains dominant on the critical path in the overall performance. when either a hard disk or flash memory ssd drive was used as stable storage for transaction log  it was bound as a raw device. log records were force-written to the stable storage in a single or multiple sectors  of 1 bytes  at a time.
　table 1 summarizes the results from the benchmark test measured in terms of transactions-per-seconds  tps  and cpu utilization as well as the average size of a single log write and the average time taken to process a single log write. since multiple transactions could commit together as a group  by a group commit mechanism   the frequency of log writes was much lower than the number of transactions processed per second. again  due to the group commit mechanism  the average size of a single log write was slightly different between the two storage media.
hard diskflash ssdtransactions/sec1cpu utilization  % 1log write size  sectors 1log write time  msec 11table 1: commit-time performance from tpc-b benchmark  with 1 simulated users 
　the overall transaction throughput was improved by a factor of 1 by using a flash memory ssd drive instead of a hard disk drive as stable storage for transaction log. evidently the main factor responsible for this improvement was the considerably lower log write time  1 msec on average  of flash memory ssd  compared with about 1 times longer log write time of disk. with a much reduced commit time delay by flash memory ssd  the average response time of a transaction was also reduced considerably. this allowed transactions to release resources such as locks and memory quickly  which in turn helped transactions avoid waiting on locks held by other transactions and increased the utilization of cpu. with flash memory ssd as a logging storage device  the bottleneck of transaction processing now appears to be cpu rather than i/o subsystem.
1 i/o-bound vs. cpu-bound
　in the previous sections  we have suggested that the bottleneck of transaction processing might be shifted from i/o to cpu if flash memory ssd replaced hard disk as a logging storage device. in order to put this proposition to the test  we carried out further performance evaluation with the tpc-b benchmark workload.
　first  we repeated the same benchmark test as the one depicted in section 1 but with a varying number of simulated users. the two curves denoted by disk-dual and ssd-dual in figure 1 represent the transaction throughput observed when a hard disk drive or a flash memory ssd drive was used as a logging storage device  respectively. not surprisingly  this result matches the one shown in table 1  and shows the trend more clearly.
　in the case of flash memory ssd  as the number of concurrent transactions increased  transaction throughput increased quickly and was saturated at about 1 transactions per second without improving beyond this level. as will be discussed further in the following  we believe this was because the processing power of cpu could not keep up with a transaction arrival rate any higher than that. in the case of disk  on the other hand  transaction throughput increased slowly but steadily in proportion to the number of concurrent transactions until it reached the same saturation level. this clearly indicates that cpu was not a limiting factor in this case until the saturation level was reached.

figure 1: commit-time performance of tpc-b benchmark : i/o-bound vs. cpu-bound
　next  we repeated the same benchmark test again with a more powerful cpu - 1 ghz intel pentium quad-core processor - instead of a 1 ghz dual-core processor in the same setting. the two curves denoted by disk-quad and ssd-quad in figure 1 represent the transaction throughput observed when the quad-core processor was used.
　in the case of disk  the trend in transaction throughput remained almost identical to the one previously observed when a dual-core processor was used. in the case of flash memory ssd  the trend of ssd-quad was also similar to that of ssd-dual  except that the saturation level was considerably higher at approximately 1 transactions per second. the results from these two benchmark tests speak for themselves that the processing speed of cpu was a bottleneck in transaction throughput in case of flash memory ssd  while it was not in the case of disk.
1. mvcc rollback segment
　multiversion concurrency control  mvcc  has been adopted by some of the commercial and open source database systems  e.g.  oracle  postgresql  sql server 1  as an alternative to the traditional concurrency control mechanism based on locks. since read consistency is supported by providing multiple versions of a data object without any lock  mvcc is intrinsically non-blocking and can arguably minimize performance penalty on concurrent update activities of transactions. another advantage of multiversion concurrency control is that it naturally supports snapshot isolation  and time travel queries  1  1 . 1
　to support multiversion read consistency  however  when a data object is updated by a transaction  the original data value has to be recorded in an area known as rollback segments. the rollback segments are typically set aside in stable storage to store old images of data objects  and should not be confused with undo log  because the rollback segments are not for recovery but for concurrent execution of transactions. thus  under multiversion concurrency control  updating a data object requires writing its before image to a rollback segment in addition to writing undo and redo log records for the change.
　similarly  reading a data object can be somewhat costlier under the multiversion concurrency control. when a transaction reads a data object  it needs to check whether the data object has been updated by other transactions  and needs to fetch an old version from a rollback segment if necessary. the cost of this read operation may not be trivial  if the data object has been updated many times and fetching its particular version requires search through a long list of versions of the data object. thus  it is essential to provide fast access to data in rollback segments so that the performance of database servers supporting mvcc are not hindered by increased disk i/o activities .
　in this section  we analyze the patterns in which old versions of data objects are written to and read from rollback segments  and show how flash memory ssd can take advantage of the access patterns to improve access speed for rollback segments and the average response time of transactions.
1 understanding the mvcc write
　when a transaction updates tuples  it stores the before images of the updated tuples in a block within a rollback

1
 as opposed to the ansi sql-1 isolation levels  the snapshot isolation level exhibits none of the anomalies that the sql-1 isolation levels prohibit. time travel queries allow you to query a database as of a certain time in the past. segment or an extent of a rollback segment. when a transaction is created  it is assigned to a particular rollback segment  and the transaction writes old images of data objects sequentially into the rollback segment. in the case of a commercial database server we tested  it started with a default number of rollback segments and added more rollback segments as the number of concurrent transactions increased.
　figure 1 shows the pattern of writes we observed in the rollback segments of a commercial database server processing a tpc-c workload. the x and y axes in the figure represent the timestamps of write requests and the logical sector addresses directed by the requests. the tpc-c workload was created for a database of 1 mbytes. the rollback segments were created in a separate disk drive bound as a raw device. this disk drive stored nothing but the rollback segments. while figure 1 a  shows the macroscopic view of the write pattern represented in a time-address space  figure 1 b  shows more detailed view of the write pattern in a much smaller time-address region.
　the multiple slanted line segments in figure 1 b  clearly demonstrate that each transaction writes sequentially into its own rollback segment in the append-only fashion  and concurrent transactions generate multiple streams of such write traffic in parallel. each line segment spanned a separate logical address space that was approximately equivalent to 1 sectors or one mbytes. this is because a new extent of one mbytes was allocated  every time a rollback segment ran out of the space in the current extent. the length of a line segment projected on the horizontal  time  dimension varied slightly depending on how quickly transactions consumed the current extent of their rollback segment.
　the salient point of this observation is that consecutive write requests made to rollback segments were almost always apart by approximately one mbytes in the logical address space. if a hard disk drive were used as storage for rollback segments  each write request to a rollback segment would very likely have to move the disk arm to a different track. thus  the cost of recording rollback data for mvcc would be significant due to excessive seek delay of disk.
　flash memory ssd undoubtedly has no such problem as seek delay  because it is a purely electronic device with extremely low latency. furthermore  since old images of data objects are written to rollback segments in appendonly fashion  the no in-place update limitation of flash memory has no negative effect on the write performance of flash memory ssd as a storage device for rollback segments. of course  a potential bottleneck may come up  if no free block  or clean erase unit  is available when a new rollback segment or an extent is to be allocated. then  a flash block should be reclaimed from obsolete ones  which involves costly erase and merge operations for flash memory. if this reclamation process happens to be on the critical path of transaction execution  it may prolong the response time of a transaction. however  the reclamation process was invoked infrequently only when a new rollback segment or an extent was allocated. consequently  the cost of reclamation was amortized over many subsequent write operations  affecting the write performance of flash memory ssd only slightly.
　note that there is a separate stream of write requests that appear at the bottom of figure 1 a . these write requests followed a pattern quite different from the rest of write requests  and were directed to an entirely separate  narrow area in the logical address space. this is where metadata of rollback segments were stored. since the metadata stayed in the fixed region of the address space  the pattern of writes directed to this area was in-place updates rather than appendonly fashion. due to the no in-place update limitation of flash memory  in-place updates of metadata would be costly for flash memory ssd. however  its negative effect was insignificant in the experiment  because the volume of metadata updates was relatively small.
　overall  we did not observe any notable difference between disk and flash memory ssd in terms of write time for rollback segments. in our tpc-c experiment  the average time for writing a block to a rollback segment was 1 msec for disk and 1 msec for flash memory ssd.
1 mvcc read performance
　as is mentioned in the beginning of this section  another issue that may have to be addressed by database servers with mvcc is an increased amount of i/o activities required to support multiversion read consistency for concurrent transactions. furthermore  the pattern of read requests tends to be quite random. if a data object has been updated by other transactions  the correct version must be fetched from one of the rollback segments belonging to the transactions that updated the data object. at the presence of long-running transactions  the average cost of read by a transaction can get even higher  because a long chain of old versions may have to be traversed for each access to a frequently updated data object  causing more random reads  1  1  1 .
　the superior read performance of flash memory has been repeatedly demonstrated for both sequential and random access patterns  e.g.   . the use of flash memory ssd instead of disk can alleviate the problem of increased random read considerably  especially by taking advantage of extremely low latency of flash memory.
　to understand the performance impact of mvcc read activities  we ran a few concurrent transactions in snapshot isolation mode on a commercial database server following the scenario below.
 1  transaction t1 performs a full scan of a table with 1 data pages of 1 kbytes each.  the size of the table is approximately 1 mbytes. 
 1  each of three transactions t1  t1 and t1 updates each and every tuple in the table one after another.
 1  transaction t1 performs a full scan of the table again.
the size of database buffer cache was set to 1 mbytes in order to cache the entire table in memory  so that the effect of mvcc i/o activities could be isolated from the other database accesses.
　figure 1 shows the pattern of reads observed at the last step of the scenario above when t1 scanned the table for the second time. the x and y axes in the figure represent the timestamps of read requests and the logical addresses of sectors in the rollback segments to be read by the requests. the pattern of read was clustered but randomly scattered across quite a large logical address space of about one gbytes. when each individual data page was read from the table 

 1	 1	 1	 1	 1
time  second  1	 1	 1	 1	 1	 1	 1	 1
time  second  1 1 1 a  macroscopic view b  microscopic viewfigure 1: mvcc write pattern from tpc-c benchmark  in time〜address space figure 1: mvcc read pattern from snapshot isolation scenario  in time〜address space 
t1 had to fetch old versions from all three rollback segments  or extents  assigned to transactions t1  t1 and t1 to find a transactionally consistent version  which in this case was the original data page of the table before it was updated by the three transactions.
hard diskflash ssd# of pages read11read time1s1scpu time1s1selapsed time1s1stable 1: undo data read performance
　we measured actual performance of the last step of t1 with a hard disk or a flash memory ssd drive being used as a storage medium for rollback segments. table 1 summarizes the performance measurements obtained from this test. though the numbers of pages read were slightly different between the cases of disk and flash memory ssd  presumably due to subtle difference in the way old versions were created in the rollback segments   both the numbers were close to what amounts to three full scans of the database table  1 〜 1 = 1 pages . evidently  this was because all three old versions had to be fetched from rollback segments  whenever a transactionally consistent version of a data page was requested by t1 running in the snapshot isolation mode.
　despite a slightly larger number of page reads  flash memory ssd achieved more than an order of magnitude reduction in both read time and total elapsed time for this processing step of t1  when compared with hard disk. the average time taken to read a page from rollback segments was approximately 1 msec with disk and 1 msec with flash memory ssd. the average read performance observed in this test was consistent with the published characteristics of the disk and the flash memory ssd we used in this experiment. the amount of cpu time remained the same in both the cases.
1. temporary table spaces
　most database servers maintain separate temporary table spaces that store temporary data required for performing operations such as sorts or joins. i/o activities requested in temporary table spaces are typically bursty in volume and are performed in the foreground. thus  the processing time of these i/o operations on temporary tables will have direct impact on the response time of individual queries or transactions. in this section  we analyze the i/o patterns of sort-based and hash-based algorithms  and discuss the impact of flash memory ssd on the algorithms.
1 external sort
　external sort is one of the core database operations that have been extensively studied and implemented for most database servers  and many query processing algorithms rely on external sort. a sort-based algorithm typically partitions an input data set into smaller chunks  sorts the chunks  or runs  separately  and then merges them into a single sorted file. therefore  the dominant pattern of i/o requests from a sort-based algorithm is sequential write  for writing sorted runs  followed by random read  for merging runs  .
	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
	time  second 	time  second 
	 a  hard disk	 b  flash memory ssd
figure 1: io pattern of external sort  in time〜address space 

 1 1	 1	 1	 1	 1	 1
cluster size in merge step  kb  1 1	 1	 1	 1	 1	 1	 1	 1	 1
buffer size  mb  a  varying cluster size b  varying buffer size buffer cache size fixed at 1 mb  cluster size fixed at 1 kb for disk and at 1 kb for ssd figure 1: external sort performance : cluster size vs. buffer cache size　to better understand the i/o pattern of external sort  we ran a sort query on a commercial database server  and traced all i/o requests made to its temporary table space. this query sorts a table of two million tuples  approximately 1 mbytes  using a buffer cache of 1 mbytes assigned to this session by the server. figure 1 illustrates the i/o pattern of the sort query observed  a  from a temporary table space created on a hard disk drive and  b  from a temporary table space created on a flash memory ssd drive. a clear separation of two stages was observed in both the cases. when sorted runs were created during the first stage of sort  the runs were written sequentially to the temporary table space. in the second stage of sort  on the other hand  tuples were read from multiple runs in parallel to be merged  leading to random reads spread over the whole region of the time-address space corresponding to the runs.
　another interesting observation that can be made here is different ratios between the first and second stages of sort with respect to execution time. in the first stage of sort for run generation  a comparable amount of time was spent in each case of disk and flash memory ssd used as a storage device for temporary table spaces. in contrast  in the second stage of sort for merging runs  the amount of time spent on this stage was almost an order of magnitude shorter in the case of flash memory ssd than that in the case of disk. this is because  due to its far lower read latency  flash memory ssd can process random reads much faster than disk  while the processing speeds of these two storage media are comparable for sequential writes.
　previous studies have shown that the unit of i/o  known as cluster  has a significant impact on sort performance beyond the effect of read-ahead and double buffering . because of high latency of disk  larger clusters are generally expected to yield better sort performance despite the limited fan-out in run generation and the increased number of merge steps. in fact  it is claimed that the optimal size of cluster has steadily increased roughly from 1 or 1 kbytes to 1 kbytes or even larger over the past decade  as the gap between latency and bandwidth improvement has become wider  1  1 .
　to evaluate the effect of cluster size on sort performance  we ran the sort query mentioned above on a commercial database server with a varying size of cluster. the buffer cache size of the database server was set to 1 mbytes for this query. the input table was read from the database table space  and sorted runs were written to or read from a temporary table space created on a hard disk drive or a flash memory ssd drive. figure 1 a  shows the elapsed time taken to process the sort query excluding the time spent on reading the input table from the database table space. in other words  the amount of time shown in figure 1 a  represents the cost of processing the i/o requests previously shown in figure 1 with a different size of cluster on either disk or flash memory ssd.
　the performance trend was quite different between disk and flash memory ssd. in the case of disk  the sort performance was very sensitive to the cluster size  steadily improving as cluster became larger in the range between 1 kb and 1 kb. the sort performance then became a little worse when the cluster size grew beyond 1 kb. in the case of flash memory ssd  the sort performance was not as much sensitive to the cluster size  but it deteriorated consistently as the cluster size increased  and the best performance was observed when the smallest cluster size  1 kbytes  was used.
　though it is not shown in figure 1 a   for both disk and flash memory ssd  the amount of time spent on run generation was only a small fraction of total elapsed time and it remained almost constant irrespective of the cluster size. it was the second stage for merging runs that consumed much larger share of sort time and was responsible for the distinct trends of performance between disk and flash memory ssd. recall that the use of a larger cluster in general improves disk bandwidth but increases the amount of i/o by reducing the fan-out for merging sorted runs. in the case of disk  when the size of cluster was increased  the negative effect of reduced fan-out was overridden by considerably improved bandwidth. in the case of flash memory ssd  however  bandwidth improvement from using a larger cluster was not enough to make up the elongated merge time caused by an increased amount of i/o due to reduced fan-out.
　apparently from this experiment  the optimal cluster size of flash memory ssd is much smaller  in the range of 1 to 1 kbytes  than that of disk  in the range of 1 to 1 kbytes . therefore  if flash memory ssd is to be used as a storage medium for temporary table spaces  a small block should be chosen for cluster so that the number of steps for merging sorted runs is reduced. coupled with this  the low latency of flash memory ssd will improve the performance of external sort quite significantly  and keep the upperbound of an input file size that can be externally sorted in two passes higher with a given amount of memory.
　figure 1 b  shows the elapsed time of the same external sort executed with a varying amount of buffer cache. the same experiment was repeated with a disk drive and a flash memory ssd drive as a storage device for temporary table space. the cluster size was set to 1 kbytes for disk and 1 kbytes for flash memory ssd  because these cluster sizes yielded the best performance in figure 1 a . evidently  in both the cases  the response time of external sort improved consistently as the size of buffer cache grew larger  until its effect became saturated. in all the cases of buffer cache size  flash memory ssd outperformed disk - by at least a factor of two when the buffer cache was no larger than 1% of the input table size.
1 hash
　hashing is another core database operation frequently used for query processing. a hash-based algorithm typically partitions an input data set by building a hash table in disk and processes each hash bucket in memory. for example  a hash join algorithm processes a join query by partitioning each input table into hash buckets using a common hash function and performing the join query bucket by bucket.
　both sort-based and hash-based algorithms are similar in that they divide an input data set into smaller chunks and process each chunk separately. other than that  sort-based and hash-based algorithms are in principle quite opposite in the way an input data set is divided and accessed from secondary storage. in fact  the duality of hash and sort with respect to their i/o behaviors has been well studied in the past . while the dominant i/o pattern of sort is sequential write  for writing sorted runs  followed by random read  for merging runs   the dominant i/o pattern of hash is said to be random write  for writing hash buckets  followed by sequential read  for probing hash buckets .
　if this is the case in reality  the build phase of a hash-based algorithm might be potentially problematic for flash memory ssd  because the random write part of hash i/o pattern may degrade the overall performance of a hash operation with flash memory ssd. to assess the validity of this argument  we ran a hash join query on a commercial database server  and traced all i/o requests made to a temporary table space. this query joins two tables of two million tuples  approximately 1 mbytes  each using a buffer cache of 1 mbytes assigned to this session by the server. figures 1 a  and 1 b  show the i/o patterns and response times of the hash join query performed with a hard disk drive and a flash memory ssd drive  respectively.
　surprisingly the i/o pattern we observed from this hash join was entirely opposite to what was expected as a dominant pattern suggested by the discussion about the duality of hash and sort. the most surprising and unexpected i/o pattern can be seen in the first halves of figures 1 a  and 1 b . during the first  build  phase  both input tables were read and partitioned into multiple  logical  buckets in parallel. as shown in the figures  however  the sectors which hash blocks were written to were somehow located in a consecutive address space with only a few outliers  as if they were written in append-only fashion. what we observed from this phase of a hash join indeed was similarity rather than duality of hash and sort algorithms with respect to their i/o behaviors.

	 a  hash join with disk	 b  hash join with ssd

	 c  sort-merge join with disk	 d  sort-merge join with ssd
figure 1: io patterns of hash join and sort-merge join　since the internal implementation of this database system is opaque to us  we cannot explain exactly where this idiosyncratic i/o behavior comes from for processing a hash join. our conjecture is that when a buffer page becomes full  it is flushed into a data block in the temporary table space in append-only fashion no matter which hash bucket the page belongs to  presumably because the size of each hash partition  or bucket  cannot be predicted accurately. then  the affinity between temporary data blocks and hash buckets can be maintained via chains of links or an additional level of indirection. we believe this explains why the read requests during the second  probe  phase of hash join was randomly scattered rather than sequential.
　whatever the reason is for this implementation of hashing  the net effect is quite favorable for flash memory ssd. as figures 1 a  and 1 b  show  the average response time of the hash join was 1 seconds with disk and 1 seconds with flash memory ssd.
　now that the dominant i/o pattern of both hash and sort is sequential write followed by random read  it is not difficult to expect a similar performance trend between disk and flash memory ssd for a sort-merge join query as well. from the same experiment repeated for a sort-merge query on the same commercial database server  the average response time of the sort-merge join was 1 seconds with disk and 1 seconds with flash memory ssd  as shown in figures 1 c  and 1 d .
　when the hash join and sort-merge join queries were executed with disk  the response time of hash join was about twice faster than that of sort-merge join. this result concurs to some extent with the previous comparative study arguing that sort-merge join is mostly obsolete with a few exceptions . on the other hand  when the hash join and sort-merge join queries were executed with flash memory ssd  their response times were almost identical. this evidence may not be strong enough to claim that sort-merge join is better suited than hash join with flash memory ssd  but it certainly warrants a fresh new look at the issue all over again.
1. conclusions
　we have witnessed a chronic imbalance in performance between processors and storage devices over a few decades in the past. although the capacity of magnetic disk has improved quite rapidly  there still exists a significant and growing performance gap between magnetic disk and cpu. consequently  i/o performance has become ever more critical in achieving high performance of database applications. most contemporary database systems are configured to have separate storage spaces for database tables and indexes  transaction log  rollback segments and temporary data. the overall performance of transaction processing cannot be improved just by optimizing i/o for tables and indexes  but also by optimizing it for the other storage spaces as well. in this paper  we demonstrate the burden of processing i/o requests for transaction log  rollback and temporary data is substantial and can become a serious bottleneck for transaction processing. we then show that flash memory ssd  as a storage alternative to magnetic disk  can alleviate this bottleneck drastically  because the access patterns dominant in the storage spaces for transaction log  rollback and temporary data can best utilize the superior characteristics of flash memory such as extremely low read and write latency without being hampered by the no in-place update limitation of flash memory.
　in our experiments  we have observed more than an order of magnitude improvement in transaction throughput and response time by replacing a magnetic disk drive with a flash memory ssd drive for transaction log or rollback segments. we have also observed more than a factor of two improvement in response time for processing a sort-merge or hash join query by adopting a flash memory ssd drive instead of a magnetic disk drive for temporary table spaces. we believe that a strong case has been made out for flash memory ssd  and due attention should be paid to it in all aspects of database system design to maximize the benefit from this new technology.
