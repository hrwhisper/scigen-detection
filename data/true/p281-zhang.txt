in this paper we study the problem of finding most topical named entities among all entities in a document  which we refer to as focused named entity recognition. we show that these focused named entities are useful for many natural language processing applications  such as document summarization  search result ranking  and entity detection and tracking. we propose a statistical model for focused named entity recognition by converting it into a classification problem. we then study the impact of various linguistic features and compare a number of classification algorithms. from experiments on an annotated chinese news corpus  we demonstrate that the proposed method can achieve near human-level accuracy.
categories and subject descriptors
i.1  artificial intelligence : natural language processing-text analysis; h.1  information storage and retrieval : content analysis and indexing-linguistic processing
general terms
algorithms  experimentation
keywords
naive bayes  decision tree  robust risk minimization  text summarization  topic identification  information retrieval
1.	introduction
　with the rapid growth of online electronic documents  many technologies have been developed to deal with the enormous amount of information  such as automatic summarization  topic detection and tracking  and information retrieval. among these technologies  a key component is to
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
identify the main topics of a document  where topics can be represented by words  sentences  concepts  and named entities. a number of techniques for this purpose have been proposed in the literature  including methods based on position   cue phrases   word frequency  lexical chains and discourse segmentation . although word frequency is the easiest way to representing the topics of a document  it was reported in  that position methods produce better results than word counting based methods.
　important sentence extraction is the most popular method studied in the literature. a recent trend in topic sentence extraction is to employ machine learning methods. for example  trainable classifiers have been used in  1  1  1  1  to select sentences based on features such as cue phrase  location  sentence length  word frequency and title  etc.
　all of the above methods share the same goal of extracting important sentences from documents. however  for topic representation  sentence-level document summaries may still contain redundant information. for this reason  other representations have also been suggested. for example  in   the authors used structural features of technical papers to identify important concepts rather than sentences. the authors of  presented an efficient algorithm to choose topic terms for hierarchical summarization according to a probabilistic language model. another hybrid system  presented in   generated summarizations with the help of named entity foci of an article. these named entities include people  organizations  and places  and untyped names.
　in this paper  we study the problem of finding important named entities from news articles  which we call focused named entity recognition. a news article often reports an event that can be effectively summarized by the five w  who  what  when  where  and why  approach. many of the five w's can be associated with appropriate named entities in the article. our definition of focused named entities is mainly concerned with who and what. therefore it is almost self-evident that the concept of focused named entity is important for document understanding and automatic information extraction. in fact  a number of recent studies have already suggested that named entities are useful for text summarization  1  1  1  1 . moreover  we shall illustrate that focused named entities can be used in other text processing tasks as well. for example  we can rank search results by giving more weights to focused named entities.
　we define focused named entities as named entities that are most relevant to the main topic of a news article. our task is to automatically select these focused named entities from the set of all entities in a document. since focused named entity recognition is a newly proposed machine learning task  we need to determine whether it is well-posed. that is  whether there exists a sufficient level of agreement on focused named entities among human reviewers. a detailed study on this matter will be reported in the section 1. the conclusion of our study is that there is indeed a sufficient level of agreement. encouraged by this study  we further investigated the machine learning approach to this problem  which is the focus of the paper. we discuss various issues encountered in the process of building a machine learning based system  and show that our method can achieve near human performance.
　the remainder of this paper is organized as follows. in section 1 we introduce the problem of focused named entity recognition and illustrate its applications. section 1 describes a general machine learning approach to this problem. in section 1  we present features used in our system. section 1 presents a study of human-level agreement on focused named entities  and various experiments which illustrate the importance of different features. some final conclusions will be given in section 1.
1.	the problem
　figure 1 is an example document.1 this article reports that boeing company would work with its new research and technology center to develop a new style of electric airplane. on the upper half of the page  we list all named entities appearing in the article and mark the focused entities. among the twelve named entities   boeing company  and its  research and technology center  are most relevant to the main topic. here we call  boeing company  and  research and technology center  the focuses. clearly  focused named entities are important for representing the main topic of the content. in the following  we show that the concept of focused named entity is useful for many natural language processing applications  such as summarization  search ranking and topic detection and tracking.
1	using focused named entity for summarization
　we consider the task of automatic summarization of the sample document in figure 1. a traditional method is to select sentences with highest weights  where sentence weights are calculated by averaging term frequencies of words it contains. the resulting summarization is given in figure 1. using focused named entities  we consider two methods to refine the above summarization. the first method is to increase the weight of the focused named entity  boeing  in the sentences  leading to the summary in figure 1. the other method simply picks sentences containing the focused named entity  boeing  as in figure 1. from this example  we can see that summarization using focused named entities gives more indicative description of an article.
1 using focused named entity for ranking search results
　suppose we want to find news about world cup football match from a collection of news articles. first we search

1
the	original	article	can	be	accessed	at

figure 1: sample document with focused named entities marked
boeing to explore electric airplane
fuel cells and electric motors will not replace jet engines on commercial transports  but they could one day replace gas turbine auxiliary power units.
unlike a battery  which needs to be recharged  fuel cells keep working as long as the fuel lasts.
 fuel cells show the promise of one day providing efficient  essentially pollution-free electrical power for commercial airplane primary electrical power needs   daggett said.
figure 1: summary using term frequency weighting
boeing to explore electric airplane
boeing commercial airplanes will develop and test an electrically powered demonstrator airplane as part of a study to evaluate environmentally friendly fuel cell technology for future boeing products.
fuel cells and electric motors will not replace jet engines on commercial transports  but they could one day replace gas turbine auxiliary power units.
 by adapting this technology for aviation  boeing intends to demonstrate its leadership in the pursuit of delivering environmentally preferred products. 
http://www.boeing.com/news/releases/1/q1/nr1a.html.figure 1: summary weighted by focused named entities
boeing to explore electric airplane
boeing commercial airplanes will develop and test an electrically powered demonstrator airplane as part of a study to evaluate environmentally friendly fuel cell technology for future boeing products.
the airplane manufacturer is working with boeing's new research and technology center in madrid  spain  to modify a small  single-engine airplane by replacing its engine with fuel cells and an electric motor that will turn a conventional propeller.
boeing madrid will design and integrate the experimental airplane's control system.
 by adapting this technology for aviation  boeing intends to demonstrate its leadership in the pursuit of delivering environmentally preferred products. 
figure 1: summary using sentences containing focused named entities
for documents containing the key phrase  world cup . the ranking function  which determines which document is more relevant to the query  is very important to the search quality. since our query is a single phrase  the ranked search results  displayed in table 1  are based on the term frequency of the phrase  world cup . it is clear that without deeper text understanding  term frequency is a quite reasonable measure of relevancy. however  although some articles may contain more  world cup  than others  they may actually focus less on the world cup event which we are interested. therefore a better indication of document relevancy is whether a document focuses on the entity we are interested in. a simple method is to re-order the search results first by whether the query entity is focused or not  and then by its term-frequency. it is quite clear that this method gives higher quality ranking.
　in this example  we use chinese corpus for demonstration  so the original searching results are in chinese  which we have translated into english for reading convenience.
1	other uses of focused named entity
　we believe that focused named entities are also helpful in text clustering and categorization tasks such as topic detection and tracking. this is because if focused named entities are automatically recognized  then the event for each document can be described more precisely. since focused named entities characterize what an article talks about  it is natural to organize articles based on them. therefore by giving more weights to focused named entities  we believe that we can potentially obtain better quality clustering and more accurate topic detection and tracking.
　our study of the focused named entity recognition problem is motivated by its potential applications as illustrated above. experiments in section 1 indicate that there is a sufficient agreement on focused named entities among human reviewers. therefore our goal is to build a system that can automatically detect focused named entities among all named entities in a document. we shall mention that although this paper only studies named entities  the basic idea can be extended to tasks such as finding important words  noun-phrases in a document.
1. learning based focused named entity recognition
　focused named entity recognition can be regarded as a binary classification problem. consider the set of all named entities in a document extracted by a named entity recognition system. each entity in this set can be labeled yes if it is a focused entity  or no if it is not. we formally define a two-class categorization problem as one to determine a label y （ { 1} associated with a vector x of input variables.
　however  in order to build a successful focused named entity extractor  a number of issues have to be studied. first  named entities that refer to the same person or organization need to be grouped together; secondly what features are useful; and thirdly  how well different learning algorithms perform on this task. these issues will be carefully studied.
1	coreference resolution
　coreference is a common phenomenon in natural language. it means that an entity can be referred to in different ways and in different locations of the text. therefore for focused named entity recognition  it is useful to apply a coreference resolution algorithm to merge entities with the same referents in a given document. there are different kinds of coreference according to the basic coreference types  such as pronominal coreference  proper name coreference  apposition  predicate nominal  etc. here in our system  we only consider proper name coreference  which is to identify all variations of a named entity in the text.
　although it is possible to use machine learning methods for coreference resolution  see  as an example   we shall use a simpler scheme  which works reasonably well. our coreference resolution method can be described as follows.
1. partitioning: the set of named entities is divided intosub-sets according to named entity types  because coreference only occurs among entities with the same types.
1. pair-wise comparison: within each sub-set  pair-wisecomparison is performed to detect whether each entitypair is an instance of coreference. in this study  we use a simple algorithm which is based on string-matching only. since we work with chinese data  we split each entity into single chinese characters. we study two different schemes here: using either exact string matching or partial string matching to decide coreference. in the case of exact string matching  two entities are considered to be a coreference pair only when they are identical. in the case of partial string matching  if characters in the shorter entity form a  non-consecutive  sub-string of the longer entity  then the two entities are considered to be a coreference pair.
1. clustering: merge all coreference pairs created in thesecond step into the same coreference chains. this step can also be done differently. for example  by using a sequential clustering method.
table 1: search result of  world cup 
focus/not	tf	titlefocus	1	uncover the mystery of world cup draws focus	1	brazil and germany qualified  iran kicked out
focus	1	preparing for world cup  china football federation and milutinovic snatch the time focus	1	sun wen understands the pressure milutinovic and china team faced focus	1	korea leaves more tickets to china fans focus	1	paraguay qualified  but head coach dismissed
	no	1	lixiang: special relationships between milutinovic and i
	no	1	three stars on golden eagle festival
	focus	1	adidas fevernova  the official 1 fifa world cup ball  appears before the public in beijing
	no	1	china's world top 1 start to vote
	focus	1	qualified vs. kicked out: mccarthy stays on  blazevic demits
	focus	1	china attends group match in korea  but not in the same group with korea
	no	1	don't scare peoples with entering wto
no 1 kelon tops china's home appliance industry in cctv ads bidding no 1 lou lan: great secrets behind
	focus	1	australia beats uruguay by one goal
	no	1	chang hong's  king of precision display : good friends of football fans　although the coreference resolution algorithm described above is not perfect  it is not crucial since the results will be passed to a machine learning algorithm in a later stage  which can offset the mistakes made in the coreference stage. our experiment shows that by using coreference resolution  the overall system performance can be improved appreciably.
1	classification methods
　in this paper  we compare three methods: a decision tree based rule induction system  a naive bayes classifier  and a regularized linear classification method based on robust risk minimization.
1.1	decision tree
　in text-mining application  model interpretability is an important characteristic to be considered in addition to the accuracy achieved and the computational cost. the requirement of interpretability can be satisfied by using a rule-based system  such as rules obtained from a decision tree. rulebased systems are particularly appealing since a person can examine the rules and modify them. it is also much easier to understand what a system does by examining its rules.
　we shall thus include a decision tree based classifier in this study. in a typical decision tree training algorithm  there are usually two stages. the first stage is tree growing where a tree is built by greedily splitting each tree node based on a certain figure of merit. however after the first stage  the tree can overfit the training data  therefore a second stage involving tree pruning is invoked. in this stage  one removes overfitted branches of the tree so that the remaining portion has better predictive power. in our decision tree package  the splitting criteria during tree growth is similar to that of the standard c1 program   and the tree pruning is done using a bayesian model combination approach. see  for detailed description.
1.1	naive bayes
　another very popular binary classification method is naive bayes. in spite of its simplicity  it often achieves reasonable performance in practical applications. it can be regarded as a linear classification method  where we seek a weight vector w and a threshold θ such that wtx   θ if its label y =  1 and wtx − θ if its label y = 1. a score of value wtx   θ can be assigned to each data point as a surrogate for the likelihood of x to be in class.
　in this work  we adopt the multinomial model described in . let { x1 y1  ...  xn yn } be the set of training data. the linear weight w is given by w = w1   w 1  and θ = θ1   θ 1. denote by xi j the j-th component of the data vector xi  then the j-th component wjc of wc  c = ＼1  is given by
 
and θc  c = ＼1  is given by.
　the parameter λ   1 in the above formulation is a smoothing  regularization  parameter.  fixed λ to be 1  which corresponds to the laplacian smoothing.
1.1	robust risk minimization method
　similar to naive bayes  this method is also a linear prediction method. given a linear model p x  = wtx + b  we consider the following prediction rule: predict y = 1 if p x  − 1  and predict y =  1 otherwise. the classification error  we shall ignore the point p x  = 1  which is assumed to occur rarely  is
 
a very natural way to compute a linear classifier is by finding a weight    w  b  that minimizes the average classification error in the training set:
n
	 w    b  = argmin	.
i=1
unfortunately this problem is typically np-hard computationally. it is thus desirable to replace the classification error loss i p y  with another formulation that is computationally more desirable. large margin methods such as svm employ modified loss functions that are convex. many loss functions work well for related classification problems such as text-categorization  1  1 . the specific loss function consider here is

that is  our linear weights are computed by minimizing the following average loss on the training data:
n
	 w    b  = argmin	.
i=1
this method  which we refer to as rrm  robust risk minimization   has been applied to linguistic processing  and text categorization  with good results. detailed algorithm was introduced in .
1.	features
　we assume that named entities are extracted by a named entity recognition system. many named entity recognition techniques have been reported in the literal  most of them use machine learning approach. an overview of these methods can be found in . in our system  for the purpose of simplicity  we use human annotated named entities in the experiments. in the learning phase  each named entity is considered as an independent learning instance. features must reflect properties of an individual named entity  such as its type and frequency  and various global statistical measures either at the document scale or at the corpus scale. this section describes features we have considered in our system  our motivations  and how their values are encoded.
1	entity type
　four entity types are defined: person  organization  place  and proper nouns. the type of a named entity is a very useful feature. for example  person and organization are more likely to be the focus than a place. each entity type corresponds to a binary feature-component in the feature vector  taking a value of either one or zero. for example  a person type is encoded as  1 1   and an organization type is encoded as  1 1 .
1	in title or not
　whether a named entity appears in the title or not is an important indicator of whether it is a focused entity. this is because title is a concise summary of what an article is about. the value of this feature is binary  1 or 1 .
1	entity frequency
　this feature is the number of times that the named entity occurs in the document. generally speaking  the more frequent it occurs  the more important it is. the value of this feature is just the frequency of the named entity.
1	entity distribution
　this feature is somewhat complicated. the motivation is that if a named entity occurs in many different parts of a document  then it is more likely to be an important entity. therefore we use the entropy of the probability distribution that measures how evenly an entity is distributed in a document.
　consider a document which is divided into m sections. suppose that each named entity's probability distribution is
given by {p1 ... pi ... pm}  where pi = total occurrence in the dococcurrence in ith section .
the entropy of the named entity distribution is computed by. in our experiments  we select m = 1. this feature contributes a real valued featurecomponent to the feature vector.
1	entity neighbor
　the context in which a certain named entity appears is quite useful. in this study  we only consider a simple feature which counts its left and right neighboring entity types. if several named entities of the same type are listed side by side  then it is likely that the purpose is for enumeration  and the listed named entities are not important. each neighboring side has five possible types - four named entity types plus a normal-word  not a named entity  type. for example  consider a person mentioned three times in the document. among the three mentions  the left neighbors are two person names and one common word  and the right neighbors are one place name and two common words. then the entity neighbor feature components are  1 1 1 1 1 .
1	first sentence occurrence
　this feature is inspired by the position method  1  1  in sentence extraction. its value is the occurrences of the entity appearing in the beginning sentence of a paragraph.
1	document has entity in title or not
　this feature indicates whether any entity exists in the title of the document  and thus takes binary value of 1 or 1.
1	total entity count
　this feature is the total number of entities in the document  which takes integer value. the feature reflects the relative importance of an entity in the entity set.
1	document frequency in the corpus
　this is a corpus level feature. if a named entity has a low frequency in the document collection  but relatively high frequency in the current document  then it is likely to be a focused entity. when this feature is used  the term frequency feature in section 1 will be computed using  tf/docsize   log n/df   where df is the number of documents that a named entity occurs in.
1.	experiments
　in this section  we study the following issues: corpus annotation  human-level agreement on focused named entities  performance of machine learning methods compared with a baseline  influence of different features  and the impact of coreference module to the overall performance.
1	corpus annotation
　we select fifteen days of beijing youth daily news in november 1 as our testing corpus  which contains 1 articles. the text  downloaded from http://bjyouth.ynet.com  is in chinese. the articles belong to a variety of categories  including politics  economy  laws  education  science  entertainments  and sports.
　since different people may have different opinions on the focused named entities  a common set of rules should be agreed upon before the whole corpus is to be annotated. we use the following method to come up with a general guideline for annotating focused named entities.
　first  the named entities in each document were annotated by human. then  we selected twenty documents from the corpus and invited twelve people to mark focused named entities. nine of the twelve people are experts in natural language processing  so their opinions are very valuable to define focused named entities. based on the survey result  entities marked by more than half of the survey participants were defined as focused named entities. we obtained fifty focused named entities for the twenty articles. by studying the focused named entities in those articles  we were able to design specifications for focused named entity annotation. the whole corpus was then marked according to the specifications.
1	human agreement statistics
　in our survey  fifty entities were identified as focused entities from the total number of 1 entities in the 1 documents. table 1 shows  among the 1 focused entities  1 entities are agreed as focus by all 1 persons  and 1 entities are agreed by 1 persons  etc.
table 1: human agreement statistics
num of focused named entities1111num of person agreeing1111　let nk denotes the number of person with agreement on focused named entity k  then the human agreement level agreek on the k-th focused named entity is . the average agreement on the 1 focused named entities is	1%  with variance
1%. we also computed the precision and the recall for the survey participants with respect to the fifty focused named entities. table 1 shows that the best human annotator achieves an f1 measure of 1%. some of the participants marked either too many or too few named entities  and thus had much lower performance numbers. this problem was fixed when the whole corpus was annotated using specifications induced from this small-scale experiment.
table 1: human annotation performance
user idprecisionrecallf11111.1.1.11111.1.1.11111.1.1.11111.1.1.11111.1.1.11111.1.1.1.1	corpus named entity statistics
　we consider two data sets in our experiments: one is the whole corpus of 1 articles  and the other is a subset of 1 articles with named entities in their titles. table 1 shows there are totally 1 focused entities among 1 entities in the whole corpus  which means that 1 percent of the entities are marked as focused. on average  there are 1 focused named entities for each article  which is consistent with the small-scale survey result.
table 1: corpus statistics on named entities
setdocnumentitiesfocusesfocus percentfocus/doc1 1 1 1.1%11111%11	baseline results
　since named entities in title or with high frequency are more likely to be the focal entities  we consider three baseline methods. the first method marks entities in titles to be the foci; the second method marks most frequent entities in each article to be the focal entities; the third method is a combination of the above two  which selects those entities either in title or occurring most frequently. we use partial string matching for coreference resolution in the three baseline experiments.
　named entities occurring in the title are more likely to be the focus of the document  but they only represent a small portion of all focal entities. baseline experiment 1 shows the precision of this method is quite high but the recall is very low.
　baseline experiment 1 implies that most of the top 1 named entities are focused entities  but again the recall is very low. however  if more named entities are selected  the precision is decreased significantly  so that the f1 measure does not improve. the top-1 performance is the worst  with an f1 measure of only 1%. note that several named entities may have the same occurrence frequency in one document  which introduces uncertainty into the method.
　by combining named entities from the title and with high frequency  we obtain better results than either of the two basic baseline methods. the best performance is achieved by combining the in-title and top 1 named entities  which achieves f1 measures of 1% for data set 1  and 1% for data set 1.
1	machine learning results
　since in our implementation  decision tree and naive bayes methods only take integer features  we encode the floating features to integer values using a simple equal interval binning method. if a feature x is observed to have values bounded by xmin and xmax  then the bin width is computed by and the bin boundaries are at xmin + iδ where i = 1 ... k   1. the method is applied to each continuous feature independently and k is set to 1. although more sophisticated discretization methods exist  the equal interval binning method performs quite well in practice.
　machine learning results are obtained from five-fold crossvalidation. coreference resolution is done with partial stringmatching. the test results are reported in table 1.
　this experiment shows that good performance can be achieved by using machine learning techniques. the rrm performance on both data sets are significantly better than the base line results  and comparable to that of the best human annotator we observed from our small-scale experiment in section 1.
corpusmethodfocusesfocus/docprecisionrecallf1docstitle1.1.1.1.1 1docstop1 1.1.1.1.1top1 1.1.1.1.1top1 1.1.1.1.1docstitle+top1 1.1.1.1.1title+top1 1.1.1.1.1title+top1 1.1.1.1.1 1docstitle+top1 1.1.1.1.1title+top1 1.1.1.1.1title+top1 1.1.1.1.1table 1: baseline results table 1: machine learning results
datasetrrmdecision treenaive bayesprf1prf1prf1docs1111111111 docs1111111111	influence of features
　the goal of this section is to study the impact of different features with different algorithms. results are reported in table 1. feature id corresponds to the feature subsection number in section 1.
　experiment a uses frequency-based features only. it is quite similar to the bag-of-word document model for text categorization  with the entity-frequency and in-title information. by adding more sophisticated document-level features  the performance can be significantly improved. for the rrm method  f1 finally reaches 1%. it is interesting to observe that the corpus-level feature  experiment f versus g  has different impacts on the three algorithms. it is a good feature for naive bayes  but not for the rrm and decision tree. whether corpus-level features can appreciably enhance the classification performance requires more careful investigation.
　the experiments also indicate that the three learning algorithms do not perform equally well. rrm appears to have the best overall performance. the naive bayes method requires all features to be independent  which is a quite unrealistic assumption in practice. the main problem for decision tree is that it easily fragments the data  so that the probability estimate at the leaf-nodes become unreliable. this is also the reason why voted decision trees  using procedures like boosting or bagging  perform better.
　the decision tree can find rules readable by a human. for example  one such rule reads as: if a named entities appears at least twice  its left and right neighbors are normal words  its discrete distribution entropy is greater than 1  and the entity appears in the title  then the probability of it being a focused entity is 1.
1	coreference resolution
　in order to understand the impact of coreference resolution on the performance of focused named entity recognition  we did the same set of experiments as in section 1  but with exact string matching only for coreference resolution in the feature extraction process. table 1 reports the five-fold cross validation results. on average the performance is decreased by about 1 to 1 percent. this means coreference resolution plays an important role in the task. the reason is that it maps variations of a named entity into a single group  so that features such as occurrence frequency and entity distribution can be estimated more reliably. we believe that with more sophisticated analysis such as pronominal coreference resolution  the classification performance can be further improved.
1.	conclusions and future work
　in this paper  we studied the problem of focused named entity recognition. we gave examples to illustrate that focused named entities are useful for many natural language processing applications. the task can be converted into a binary classification problem. we focused on designing linguistic features  and compared the performance of three machine learning algorithms. our results show that the machine learning approach can achieve near human-level accuracy. because our system is trainable and features we use are language independent  it is easy for us to build a similar classification model for other languages. our method can also be generalized to related tasks such as finding important words and noun-phrases in a document.
　in the future  we will integrate focused named entity recognition into real applications  such as information retrieval  automatic summarization  and topic detection and tracking  so that we can further study and evaluate its influences to these systems.
1.	acknowledgments
　we thank honglei guo for providing the original corpus with named entity annotation. jianmin jiang helped us to set up named entity annotation server  which made it much easier to view and annotate the corpus. we thank zhaoming qiu  shixia liu  zhili guo for their valuable comments on the experiments. we are also grateful to our colleagues who spent their time to participate in the focused named entity survey.
