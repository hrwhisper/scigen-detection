clustering methods can be either data-driven or need-driven. data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements. thus  need-driven  e.g. constrained  clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks  privacy preservation  and market segmentation. however  the existing methods of constrained clustering require users to provide the number of clusters  which is often unknown in advance  but has a crucial impact on the clustering result. in this paper  we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters. for this purpose  we introduce a novel cluster model  constraint-driven clustering  cdc   which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints. two general types of constraints are considered  i.e. minimum significance constraints and minimum variance constraints  as well as combinations of these two types. we prove the np-hardness of the cdc problem with different constraints. we propose a novel dynamic data structure  the cd-tree  which organizes data points in leaf nodes such that each leaf node approximately satisfies the cdc constraints and minimizes the objective function. based on cd-trees  we develop an efficient algorithm to solve the new clustering problem. our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm.
categories and subject descriptors
h.1  database applications : data mining
general terms
algorithms
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  san jose  california  usa.
copyright 1 acm 1-1-1/1 ...$1.
keywords
constraints  clustering  np-hardness
1. introduction
　clustering aims at grouping data objects into clusters in an effective and efficient manner. the generated clusters provide useful knowledge to support decision making in many applications. depending on perspective  clustering methods can be either data-driven or need-driven . the data-driven clustering methods intend to discover the true structure of the underlying data by grouping similar objects together while the need-driven clustering methods group objects based on not only similarity but also needs imposed by particular applications. thus  the clusters generated by need-driven clustering methods are more useful and actionable to meet certain application requirements.
　need-driven or actionable data mining research has attracted significant interest recently. kleinberg et al.  propose a framework to evaluate data mining results  in particular clustering results  by their utility in decision making. in this framework  the goal of generating useful clusters is achieved by a sophisticated objective function which is defined based on business needs. alternatively  complex application needs can be captured by corresponding constraints. for example  in market segmentation  relatively balanced customer groups are more preferable so that the knowledge extracted from each group has equal significance and are thus easier to evaluate . the special requirement of identifying balanced clusters can be effectively captured by imposing balancing constraints  1  1  1  1 .
　these models enable us to find useful clusters. however  they require users to provide the number of clusters  k  which is often unknown in advance. even if the most appropriate number of clusters determined from the data distribution is known  it may not suit the application needs . furthermore  an inappropriate number of clusters may result in generating distorted and less actionable clusters. we argue that a more natural way to generate actionable clusters is to let the constraints decide the number of clusters. in this paper  we propose a cluster model  constraintdriven clustering  which aims at utilizing user-provided constraints to discover an arbitrary number of compact and balanced clusters. the compactness of a clustering is measured by the sum of the squared distances of all data objects to their corresponding cluster representatives.
　for different application needs  various balancing constraints can be designed to restrict the generated clusters in order to make them actionable. particularly  we are interested in constraint types  i.e.  minimum significance constraints and minimum variance constraints. the minimum significance constraint specifies the minimum number of objects in a cluster. the minimum variance constraint poses a lower bound on the variance of a cluster. by imposing a minimum significance constraint or/and a minimum variance constraint  our model searches for clusters which are balanced in terms of cardinality or/and variance.
　to motivate constraint-driven clustering with minimum significance and variance constraints  we use applications in energy aware sensor networks and privacy preservation as our running examples.
　energy aware sensor networks  1  1  1 : grouping sensors into clusters is an important problem in sensor networks since it can drastically affect the network's communication energy consumption . normally  a master node is chosen from sensors in each cluster or deployed to the central area of each cluster. other sensors will communicate with the outside world through the closest master node. in this context  it is desirable to require each cluster to contain at least a certain number of sensors in order to balance the work load of master nodes. to prolong the lifetime of a sensor network  evenly distributing energy consumption among clusters is desired. since the energy consumption of message transmissions increases quadratically with the distance between communicating sensors   the variance of a group of sensors corresponds to the amount of energy consumed by those sensors on average. the minimum variance constraint allows to group sensors into clusters which are balanced in terms of energy consumption. moreover  in this application  it is natural to have the constraints decide the appropriate number of clusters instead of specifying a number in advance.
　privacy preservation  1  1 : in a privacy preservation application  we may want to release personal records to the public without a privacy breach. to achieve this  we can group records into small clusters and release the summary of each cluster to the public. in this context  the usability of a clustering is evaluated by how much privacy is preserved in the clustering. to preserve individual privacy  the k-anonymity model  requires that each cluster has to contain at least a certain number of individuals. however  these individuals could have very similar  even identical attribute values  allowing an adversary to accurately estimate their sensitive attribute values with high confidence from the summary. we argue  therefore  that the clusters to be published should also have a minimum variance which translates into the width of the confidence interval of the adversary estimate. in the context of privacy preservation  again  it is typically unreasonable to specify the number of groups in advance.
　ppmicrocluster model  requires both minimum significance and minimum radius constraints to preserve privacy. compared to this model  our constraint-driven clustering model adopts a more practical constraint  i.e.  minimum variance constraint  which describes the statistical properties of a cluster better and can be used for a wide range of applications. besides  we systematically study the complexity of the constraint-driven clustering problem and propose a dynamic algorithm which is shown to be more efficient than the static algorithm for solving the ppmicrocluster problem.
　in this paper  we introduce the constraint-driven clustering problem. we prove the np-hardness of the proposed clustering problem with different constraints. inspired by the np-hardness proof  we propose a novel data structure  named cd-tree  which organizes data points in leaf nodes such that each leaf node approximately satisfies the significance and variance constraint and minimizes the sum of squared distances. based on cd-trees  we develop an efficient algorithm to generate constrained clusters with good quality. furthermore  benefiting from the hierarchical tree structure  the cd-tree algorithm can easily adapt to dynamic updates of the data.
the contributions of this paper are as follows:
 1  we propose the constraint-driven clustering problem  which incorporates a minimum significant constraint and a minimum variance constraint for discovering actionable clusters  but does not require an a priori specification of the number of clusters.
 1  we prove the np-hardness of the proposed clustering problem with different constraints.
 1  we develop the efficient cd-tree algorithm to generate clusters w.r.t the introduced constraints.
 1  we evaluate our cd-tree algorithm on synthetic and real datasets and demonstrate the quality of the generated clusters and the efficiency of the algorithm.
　the rest of the paper is organized as follows. section 1 surveys related work. section 1 introduces the new cluster model and analyzes its complexity. section 1 presents the cd-tree algorithm. we report experimental results in section 1 and conclude the paper in section 1.
1. related work
　actionable clustering. actionable clustering was first proposed by kleinberg in   in which a clustering is evaluated by its utility in decision-making. by introducing a new objective function  the goal of clustering is shifted from identifying the true structure of the underlying distribution to discovering useful clusters. ester et al. extend the catalog segmentation problem  by introducing a new utility measured by the number of customers that have at least a specified minimum interest in the catalogs. a joint optimization approach is proposed in  to address two issues in market segmentation  i.e.  segmenting customers into homogeneous groups and determining the optimal policy towards each segment.
　cluster-level constraints. the research on clustering with constraints was introduced by  and systematically studied in . both clustering models aim at partitioning data points into k clusters while each cluster satisfies a significance constraint. bradley et al.  proposed a constrained k-means algorithm and suggested to achieve a cluster assignment by solving a minimum cost network flow problem. tung et al.  propose to solve this problem by starting with any valid clustering. the solution is repeatedly refined by moving some objects between clusters to reduce the clustering cost and maintaining the constraint satisfaction at the same time.
　for the same problem  banerjee et al. present an efficient three-step scheme  1  1  which gives a very general methodology for scaling up balanced clustering algorithms. the same problem is converted to a graph partition problem in  to discover balanced clusterings. however  the complexity of the graph-based approach is higher than the one in . different from our proposed model  all these clustering models require the number of clusters as an input.
　instance-level constraints. work on instance level constraints and clustering were first presented to the machine learning and data mining communities by wagstaff and cardie . in this line of work the constraints are on instances either forcing them to be in the same cluster  must-link  or different clusters  cannot-link . though it is possible to specify cluster level constraints as instance level constraints this would require a large number of constraints for even moderately sized data sets. for example  specifying all clusters must have all their points more than δ distance apart can be achieved by must-linking all those points less than or equal to δ distance apart . specifying too many constraints is also problematic as algorithms that attempt to satisfy all constraints can quickly be overconstrained  so that efficient algorithms to find just a single solution cannot be found even though they exist.
　clustering methods in sensor networks. ghiasi et al. propose to cluster sensor nodes such that the number of sensors in each cluster  which has a master node  is in the range of  and the total distance between sensor nodes and k master nodes is minimized . the clustering problem presented in  is different from the constraint-driven clustering in that it specifies the number of clusters. more practical protocols are studied in the sensor network literature to minimize the energy consumption or message transmissions by grouping sensors to clusters. coyle et al. proposed a randomized algorithm to find the optimal number of cluster heads by minimizing the total energy spent on communicating between sensors and the information-processing center through the cluster heads. authors in  present a clustering method on selforganizing sensor networks  for the purpose of grouping sensors into the optimal number of clusters that minimize the number of message transmissions. similar approaches on clustering in sensor networks also include  1  1  1  etc. however  these clustering methods focus on dealing with engineering constraints instead of systematically studying the properties of the proposed clustering models. k-anonymity. the k-anonymity model  1  1  was proposed for the purpose of protecting data privacy. the k-anonymity framework archives the goal by generalizing entries in a table with minimum cost such that every record becomes textually indistinguishable from k 1 other records in the table.  and  prove that k-anonymity with suppression is np-hard and study approximation algorithms. the k-anonymity model is defined on categorical data  and thus has different properties from our model which assumes a geometric space with the euclidean distance.  introduces a k-nearest neighbor based algorithm to solve the k-anonymity problem for numerical data. domingo-ferrer et al.  study the optimal k-partition problem which can be considered as the k-anonymity model in the euclidean space. and it is a special case of our model where only significance constraints are allowed. but in  the complexity of the proposed problem is not analyzed.  considers privacy preservation as a problem of finding the minimum number of hyperspheres with a fixed radius to cover a dataset satisfying that each hypersphere covers at least a certain number of data objects. a similar model  named ppmicrocluster  is studied in  which requires both significance and radius constraints. compared to the ppmicrocluster model  our model adopts a more practical constraint  i.e.  minimum variance constraint which describes the statistical properties of a cluster better and can be used for a wide range of applications. besides   does not analyze the complexity and proposes a static algorithm.
1. problem definition and complexity analysis
　in this section  we introduce the constraint-driven clustering problem and analyze its complexity under different types of constraints.
1 the cdc problem
　first we define the general constraint-driven clustering problem  also referred to as cdc problem  as follows:
　definition 1. constraint-driven clustering cdc  given a set of points p in d-dimensional space  a set of constraints c  the task is to partition p into disjoint clusters
　　　　　　　　　　　　　　　　　　　　　　　　　　  s.t.:  1  each cluster pi 1 ＋ i ＋ m satisfies all constraints in c and  1  the sum of squared distances of data points to their corresponding cluster representatives is minimized.
　note that a cluster representative can be either a real data point or the mean vector of a cluster. in this paper  we study the cdc problem under the following two types of constraints.
definition 1. for each cluster pi 1 ＋ i ＋ m 
  significance constraint   variance constraint
v ar  where μ is the representative of pi.
　remark. the cdc model is general in that the constraint set c can include one or more constraint types. note that when sig = 1  the significance constraint is trivially satisfied  similarly the variance constraint when v ar = 1. for the cdc problem to be meaningful  at least one of the constraints has to be non-trivial.
　in the current definition of cdc  we require the generated clusters to be non-overlapping. yet  the cdc problem can also be extended to allow overlapping. for privacy preservation  for example  possible overlaps among the generated clusters can not only make the model more flexible  but also enhance privacy protection. when overlapping is allowed  a data point assigned to multiple clusters would contribute to the objective function  sum of squared distances  multiple times. the complexity analysis in the following section remains valid when overlapping is permitted since the optimal solution for the instance in the proof can never contain overlapping clusters.
1 complexity analysis
　in this section we show that the cdc problem is np-hard under significance or variance constraints. we shall focus on the significance constraint  and show how the same proof can be easily extended for the variance constraint.
　in order to study the complexity of the cdc problem  we consider the decision version of the cdc problem with a significance constraint only  sig-cdc  as follows.
　definition 1.  sig-cdc . given a set of points p in d dimensional space  a constant sig   1 and a cost threshold w. decide whether p can be partitioned into disjoint clusters {p1 ，，，   pm}  which satisfies the following conditions:
1.  pi  |pi| − sig  the significance constraint .
1.   where pj is the medoid of cluster pj and dist p pj  is the euclidean distance between p and pj.
　in the following we prove that the sig-cdc problem is np-complete by a reduction from a known np-complete problem  planar x1c.
definition 1.  planar x1c  
given a set q with |q| = 1q and a set t of triples from q 〜 q 〜 q such that  1  every element of q occurs in at most three triples and  1  the induced graph is planar.  the induced graph g contains a vertex for every element of q and for every triple in t. there is an edge connecting a triple to an element if and only if the element is a member of the triple. clearly  g is bipartite with vertex bipartition q and t.  decide whether there exists a subset of q triples in t which contains all the elements of q.
theorem 1. sig-cdc is np-complete for sig − 1.
　proof. first  the problem is in np since it takes polynomial time to verify whether a given clustering solution is feasible. to prove the np-hardness  we perform a reduction from planar x1c. given an instance i =  q t  of planar x1c  we create an instance  of the sig-cdc problem by the following procedure.
1. construct a planar bipartite graph g v e  of the instance i where v = q “ t and e = { q t |q （ q t （ t q is a member of t}.

figure 1: rectilinear layout l.
1. compute a rectilinear layout of g where each vertex v of g is mapped to a point pv on the integer lattice. we further enlarge the layout by a factor of 1 to ensure that every two distinct horizontal  vertical  line segments are far away enough from each other. each edge e =  q t  of g is broken into a sequence of line segments of length 1 by placing points in the rectilinear layout  figure 1 . the resulting layout is denoted as l.  proposed a linear time algorithm to compute such a layout. a similar rectilinear layout is used in  to prove the np-completeness of the feasibility problem for the must-link and  constraints.
1. replace the corresponding point pt of a triple t （ t by a point set ut = {p1t p1t p1t}. p1t p1t p1t form an isosceles triangle with two sides of length 1 and one side of length 1. ut is called triple set in the following. then  we connect p1t p1t  and p1t each with a different path of pt leading to the corresponding

figure 1: final layout l
element points in the original layout. to adapt to this change  the layout l needs to be adjusted by allowing edge segments to be inclined. see figure 1 for such a transformation. in the following  we refer to points whose corresponding vertices are in q as element points  and refer to points in the triple sets as triple points.
1. break each line segment of length 1 into three segments with length 1 1 respectively  by adding two auxiliary points to the line segment  see figure 1 .
1. let l denote the final layout. let p be the set consisting of all the points in l and set sig = 1 and w = 1|p|/1.
　let m be the number of edge segments in l. note that |p| is a multiple of 1 since |p| = 1m + |q| and |q| is a multiple of 1. for any t （ t  let qt   q be the set of the three elements covered by t. we define a path as a collection of line segments from a triple point pit （ ut to its corresponding element point pq where q （ qt without going through any other triple points. furthermore  note that the distance between two neighboring points on a path is 1 or 1  and the distance between two non-neighboring points is greater than 1.
　assume that there is a set of triples c   t which covers all elements of q exactly once. we construct a feasible clustering of instance i with a cost of 1|p|/1 as follows. for every triple set ut  we first group all the three triple points of ut into one cluster if t （ c. we then start from the first unassigned point on each path originating from the points in ut and group every three consecutive points together. figure 1 illustrates how points are grouped along a path. note that for any element q （ qt that is covered by t （ c  the corresponding point pq （ p is grouped with its two closest auxiliary points on the path from pit to pq. since q is only covered by one triple in c  pq is uniquely assigned to one cluster. furthermore  every cluster has a cost of 1  including those clusters consisting of all the points in a triple set . thus  we obtain |p|/1 clusters each with a cost of 1 so that the total cost is 1|p|/1.
　now assume that there is a feasible clustering with the total cost smaller than or equal to 1|p|/1. we demonstrate how to obtain a subset of triples c   t that covers every element in q exactly once. first we prove that any feasible clustering consists of |p|/1 disjoint clusters each consist of 1 points. for any i − 1  let hi denote the number of clusters consisting of i points. we have |p| = i ihi and the total number of clusters is i hi. note that the minimum cost of a cluster with three points is 1 by grouping three points connected through two consecutive edges and choosing the

figure 1:  a . points in a triple set are grouped into one cluster.  b . points in a triple set are grouped separately with auxiliary points.
middle point as the medoid. furthermore  every additional node in a cluster with more than three points contributes at least 1 to the total cost  check the cluster containing four points in the dashed circle in figure 1 . thus 
total clustering cost i 1 i   1 hi + 1 i hi = 1 i ihi   1 i hi.
　hence i hi − |p|/1 since |p| = i ihi. besides  since the cost of a cluster is at least 1 and the total cost is at most 1|p|/1  i hi ＋ |p|/1. thus i hi = |p|/1. thus we can conclude that there must be |p|/1 disjoint clusters  each consists of 1 points and has a cost of 1.
　in a feasible clustering  observe that all the points in a triple set either  a  form a single cluster  or  b  belong to three different clusters  otherwise at least one cluster would cost more than 1. we call a triple set ut is of type  a  if  a  happens. define g = {t （ t |ut is of type  a }. it remains to show that g covers every element in q exactly once. if ut is of type  a   each of the three points in qt must be grouped into different clusters with the two nearest auxiliary points along the path from ut  see figure 1 a  . if ut is of type  b   none of the points in qt is grouped with points along the paths from ut  see figure 1 b  . since every element in q is uniquely assigned to a cluster  t must consist of |q|/1 type  a  triples  i.e.  |g| = |q|/1 . these triples must cover every element in q exactly once. 
　next we demonstrate that the sig-cdc problem remains np-complete if using the mean vector  instead of the medoid  of a cluster as the representative. we refer to this model as μ-sig-cdc. due to limited space  we only present the sketch of np-hardness proof for the following problems.
theorem 1. the μ-sig-cdc problem is np-complete.
　proof.  sketch.  the proof is by a reduction from planar x1c that is similar to the one for theorem 1. we first construct a rectilinear layout for a planar x1c instance and replace each triple set by an isosceles trian-〔 gle with two sides of length 1 and one side of length 1. see figure 1 for the final layout. next  we set the significance constraint sig = 1 and set the total cost threshold w = 1|p|/1. similar to the proof of theorem 1  we can show that in a feasible clustering  there must be exactly |p|/1 disjoint clusters  each consists of three points

figure 1: transformed layout for theorem 1 and theorem 1.
and has a cost of 1. given this  we can show that there is a feasible clustering if and only if the corresponding instance of planar x1c has a feasible solution. hence  the μ-sig-cdc problem is np-complete. 
　finally  we apply the previous technique to show that the cdc problem is np-complete if only a variance constraint is specified and mean vectors are used as the cluster representatives. we refer to this model as μ-var-cdc.
theorem 1. the μ-var-cdc problem is np-complete.
　proof.  sketch  to prove that the μ-var-cdc problem is np-complete  we use the same construction for proving theorem 1. we set the variance constraint v ar = 1 and set the total cost w = 1|p|/1. let cl be an arbitrary cluster and denote by cost cl  the total cost of cl. note that due to the minimum variance constraint  cost cl  − 1|cl|/1  and by a case analysis we can show that the equality holds only when |cl| = 1. hence  in order to have a total cost of 1|p|/1  we must have |p|/1 disjoint clusters each of which consists of 1 points. the rest of the proof is similar to that of theorem 1. 
1. algorithm
　in the last section we have shown that the cdc problem with either a significance or a variance constraint is nphard. in order to efficiently solve the cdc problem  we design a heuristic algorithm which builds a compact tree structure to generate clusters satisfying user specified constraints. the algorithm is general in that it can handle both constraints separately or together.
　we observe that a solution to the cdc problem has the following characteristics.  1  to minimize the objective function  the sum of squared distances   the generated clusters in an optimal solution should be balanced in terms of given constraints. for example  when only a significance constraint is provided  the generated clusters in an optimal solution should contain similar number of data points.  1  the membership assignment of any data point can be decided by considering its close neighbors. thus  easily retrieving the local neighborhood of data points is critical to the design of a universal algorithm that can handle different constraints. guided by these observations  we propose an algorithm based on a novel data structure  called the
cd-tree  which is similar to the b-tree and cf-tree .
1 the cd-tree
　the cd-tree has two input parameters  i.e.  a significance parameter s  a variance parameter v . normally we set s = sig and v = v ar  i.e.  the parameters of the cdc problem. if one of the cdc constraints is trivial  methods for automatically determining appropriate parameter values are needed which are discussed in section 1.
　in a cd-tree  the maximum capacity of leaf nodes is set to 1s   1 and the variance of points in leaf nodes is upper bounded by 1v . note that the cdc problem specifies minimum constraints on the significance and variance of a cluster  while in the cd-tree we specify upper bounds for the significance and variance of leaf nodes in order to keep them compact. keeping leaf nodes compact matches our goal of minimizing the sum of squared distances of generated clusters since points in leaf nodes will be used to generate constrained clusters to solve the cdc problem.
　it is appropriate to upper-bound the variance  because a too small variance may yield too many leaf nodes  while a too large variance will make the statistical information of this leaf node less meaningful when it is used to direct a new point to its closest leaf node. we set 1v as the upper bound of the variance since it makes leaf nodes to be reasonably compact and to likely satisfy the minimum variance constraint v ar. the maximum capacity of leaf nodes is set to 1s  1 since  as we show in the following lemma  there is always an optimal solution where the number of points in every cluster is smaller than 1sig.
　lemma 1. in the μ-sig-cdc problem  there exists an optimal clustering s.t. the number of data points in any cluster is less than 1sig and greater than or equal to sig.1
　proof. by contradiction. assume that in every optimal clustering  there is a cluster c containing l data points where l − 1sig.
	we arbitrarily split c into two clusters	and c1 where
|c1| = sig and   be the mean vector  of the points in . similarly let m1 and m1 be the mean vectors of the points in c1 and c1 respectively. note that
	 	 1 
let f c  be the objective value of cluster c. we have
 
since.	similarly  
sigm  1 and.
applying equation 1 and straightforward algebra  we get

　thus  we could obtain a clustering whose objective value is smaller than or equal to the previous one by splitting c into c1 and c1  yielding a contradiction. 

1
note that a similar result is presented in .
　in the cd-tree  each entry of a leaf node represents an individual data point. the maximum capacity of a nonleaf node is set to z which is a constant and can be set arbitrarily. every entry of a non-leaf node corresponds to the subtree rooted at one of its child nodes. the entry stores a pointer to the child node  as well as the statistical information  the mean vector  linear sum and squared sum  of all the points in the corresponding subtree. similar to the cf-features   the statistical information is used to direct a new point along a path to the closest leaf node. besides  all the leaf nodes are linked together for easy access of their neighborhood.
　the construction of a cd-tree relies on two basic operations: insertion and split. the cd-tree algorithm takes one data point at a time and inserts it into an appropriate leaf node following the path from the root. a tree node is split into two nodes whenever its capacity is exceeded. the cd-tree is constructed by repeatedly invoking these two operations until all data are processed. after the cd-tree is ready  some post-processing is needed to generate clusters that satisfy the constraints of the cdc problem.
　this approach has three advantages:  1  building a cdtree requires only one scan of a dataset so that the disk access is minimized.  1  benefiting from the tree structure  our approach can easily deal with incremental updates of the dataset.  1  the cd-tree algorithm ensures that the points in the same leaf node are similar to each other. thus it is sufficient to examine only the neighboring leaf nodes whenever there is some change to the required constraints. insertion. given a new data point p  we first locate the leaf node that p shall be inserted into. starting from the root of the cd-tree  every time we pick the subtree whose mean vector  which is part of the statistical information  is the closest to p. repeat this process until we reach some leaf node. if the variance of a leaf node exceeds the threshold after inserting p  we need to create a new leaf node to accommodate it.
　split. in the construction of the cd-tree  a split is invoked whenever the capacity of a node is exceeded. the proof of lemma 1 provides an efficient way to evaluate the drop of the objective value during a split. based on equation 1  we design an efficient algorithm to split a group of 1s data points c into two clusters c1 and c1  algorithm 1 . the algorithm proceeds in a greedy fashion. first we pick the point that is farthest from c's mean vector and add it to c1. then we iteratively add points that are closest to c1's mean vector into c1 until the objective value stops decreasing. and c1 keeps all remaining points in c. splitting non-leaf nodes can be done similarly by considering the mean vector saved in the entries of the non-leaf nodes.
　finally  after each split  we need to decide how to link the newly created leaf nodes with the existing nodes. suppose we just split a leaf node c into c1 and c1. let cprev and cnext be the both neighbors of c before split. there are two ways to link c1 and c1. let μ c  be the mean vector of c. if |μ cprev    μ c1 | ＋ |μ cprev    μ c1 |  we create links cprev ★ c1 ★ c1 ★ cnext  otherwise cprev ★ c1 ★ c1 ★ cnext.
1 solving the cdc problem
　after we construct a cd-tree  the cdc problem can be solved by post-processing the leaf nodes of the cd-tree. similar to many other hierarchical tree structures  a cdtree has the property that data points located in the same algorithm 1 splitting a group of 1s points.
1: input: c contains 1s points
1: output: c1 and c1
1:	  ; size = |c1|;
1: set m1 to the mean vector of points in
1: pick the point p in c1 that is farthest from m  1;
1: remove and update	;
1: 
1: maxobjdrop =	 ;
1: for i = 1 to s do
1:	pick a point p from c1 that is nearest to;
1:	remove p from c1 and update ;
1:	add	and update
1:	objdrop =   size    m  1 + m  1   1m  1m  1 ;
1:	if objdrop − maxobjdrop then
1:	continue;
1:	else
1:	remove p from c1 and add p back to c1;
1:	end if
1: end for


figure 1:  a  data points.  b  visualization of the cd-tree.
subtree tend to be more similar than the points located in different subtrees. by linking tree nodes appropriately  we are able to retrieve the neighbors of any data point easily. for example  figure 1 shows a small dataset and its corresponding cd-tree. in this figure  dashed circles represent leaf nodes and solid circles correspond to non-leaf nodes of the cd-tree. since every new data point is inserted into the tree based on its distance to the mean vectors of subtrees  data points in l1 are more similar to data points in l1 than to data points in l1 or l1. this allows us to postprocess the neighboring leaf nodes to obtain constrained clusters.
　we propose a sliding window approach for solving the cdc problem. a sliding window consists of exactly z leaf nodes with the same parent node. starting with the first leaf node in the window  we examine one leaf node at a time. depending on the given significance constraint sig and variance constraint v ar  we distinguish between two types of a leaf node l. l is called qualified if l has at least sig points and variance at least v ar  otherwise notqualified. for every qualified leaf node l  we output its  kernel   which is a subset of points of l that just satisfies the given constraints. kernels can be easily calculated using a greedy approach  case a of algorithm 1 . the remaining points in l are treated together with those under-qualified leaf nodes. we repeatedly absorb points from other leaf nodes in the same window to form clusters that satisfy the given constraints  case b of algorithm 1 . note that all the points which have been assigned to some clusters are not considered in case b. after looping through all the leaf nodes  a set of constrained clusters is generated.
algorithm 1 cluster leaf nodes
1: input: significance constraint sig  variance constraint v ar  a leaf node l.
1: output: a set of constrained clusters
1: q =  
case a:
1: if l.size − sig and l.var − v ar then
1:	insert the point closest to l.mean to q
1:	while q.size   sig or q.var   v ar do
1:	add the point closest to the q.mean from l to q
1:	end while
1:	output q
1:	l = l   q  apply case b to l
1: end if case b:
1: while l.size   sig or l.var   v ar /*not-qualified*/ do
1: absorb similar points from other leaf nodes in the same window following the link  shift the window if all points in the current window are absorbed
1: end while 1: output l

1 runtime analysis
　we assume that both cdc constraints are non-trivial. in order to analyze the runtime of the cd-tree algorithm  we first bound the height of a cd-tree. in the worst case  a cd-tree can have o n  levels if every inserted point triggers a split and every split results in two leaf nodes containing 1sig   1 points and one point respectively.
　an insertion of a single point into the cd-tree involves two operations: locating the right leaf node to insert the point and splitting the leaf node if its capacity is exceeded. the time to locate the right leaf node is o n  since the height of the cd tree is o n . in a split  algorithm 1   we create a new node starting with the farthest point from the mean of the old node  which can be found in o sig  steps   and gradually absorb the closest point to the mean of the new node  each of the o sig  absorptions takes o sig  steps . hence  the runtime of a split is o sig1 . for building a cd-tree with n points  the total runtime is o n1 + sig1n  in the worst case.
　in the phase of postprocessing the cd-tree to solve the cdc problem  if the variance constraint v ar is not specified  generating a valid cluster consisting of o sig  points requires o z，sig1  steps since there are at most o z，sig  points in a sliding window of length z which is often considered as a constant. thus  generating a set of o n/sig  valid clusters takes o n，sig  steps. if a variance constraint v ar is also specified  the number of points in a valid cluster could be o n  in the worst case  imagine that there is only one valid cluster containing all data points . in such a case the runtime of the second phase is o n1 . therefore  the overall runtime is o n1 + sig1n .
　yet  in practice  sig is typically small compared to n. if the data distribution is not highly skewed  the height of the cd-tree is usually small. in such cases  our algorithm is very efficient as demonstrated by the experimental evaluation in section 1.
1 discussion
how to handle a trivial variance constraint  if a variance constraint is trivial  i.e.  v ar = 1  we need to set the variance parameter v used for the cd-tree construction automatically. a suitable threshold should allow sig closest points to be grouped together. ideally  if we know the average sig nearest neighbor distance of a dataset  this distance can be used to approximate the variance parameter. however  the exact computation of the average sig nearest neighbor distance is expensive. therefore  we propose to estimate the sig nearest neighbor distance based on the following lemma.
　lemma 1. given a dataset of n points that are uniformly distributed in a d dimensional space with volume v ol. fix any point p  let r be the random variable indicating the radius of the smallest enclosing ball of the sig nearest neighbors of p. let where Γ is the gamma function . then
pr 1/dr  ＋ r ＋ 1/dr   − 1   1e sig.
proof. we first show that pr r   1/dr   ＋ n 1. let
h be the hypersphere with radius 1/dr  centered at p. let vol h  be the volume of h. then  from   we have

let q be a random variable indicating the number of points in h. we have e q  = n ， vol h /v ol = 1sig. using chernoff's bound we obtain pr q   sig  ＋ pr q    1   e q ， 1 1 ＋ e sig. consequently pr r   1 e q     e
1/dr   ＋ pr q   sig    e sig. similarly we can show that pr r   1/dr     e sig so the result follows. 
　lemma 1 shows that we can estimate the sig nearest neighbor distance with high probability if we know the volume v ol of a dataset for uniformly distributed data. in practice  if the volume is not known  we can draw a small set of samples to obtain a good estimation.
how to handle a trivial significance constraint  a cd-tree requires a meaningful significance constraint to set its leaf node capacity. if the supplied significance constraint is 1 and only a variance constraint v ar is given  we can estimate the number of points located in a hypersphere of〔

radius v ar for uniformly distributed data and set the number to be sig. let vol h  be the hypersphere with

radius 〔v ar and v ol be the volume of a dataset. the expected number of points located in this hypersphere is n， vol h /v ol  n is the total number of points in the dataset.

figure 1: results for dataset ds1  only significance constraints are specified .
1. experimental evaluation
　in this section  we experimentally demonstrate the efficiency and effectiveness of the cd-tree algorithm using real and synthetic datasets.

figure 1: results for dataset ds1  both significance and variance constraints are specified .
1 methodology
　in order to evaluate the cd-tree algorithm for sensor network applications  we generated a synthetic dataset  ds1  consisting of 1 two dimensional data points which simulates a sensor network with 1 sensors uniformly deployed in a two dimensional space. a similar simulation was used in  to evaluate clustering results for sensor networks. in addition  we evaluated the cd-tree algorithm on two real datasets. the first one is the  abalone  dataset and the second one is the  letter  dataset. both datasets are from uci machine learning repository . the  abalone  dataset was also used by  for evaluating the quality of the condensation group approach for privacy preservation applications. the original abalone dataset contains 1 data points and 1 attributes. we preprocessed the dataset and kept 1 out of total 1 continuous attributes since one of the attributes is the class label. the letter dataset includes 1 instances and has 1 continuous attributes. finally  we generated three large synthetic datasets  containing 1 million  1 million  and 1 million three dimensional data points  to evaluate the scalability of the cdtree algorithm.

figure 1: results for abalone dataset  only significance constraints are specified .
　two related approaches  the ppmicrocluster algorithm  and the condensation group approach   solve a constrained clustering problem similar to the cdc problem. we chose the ppmicrocluster algorithm as our comparison partner due to the following reasons. first  its problem definition is equivalent to the cdc problem except that is specifies a radius constraint instead of a variance constraint. different from the variance  the radius of a cluster is the maximum distance between all points in a cluster to the cluster representative. we have adapted the ppmicrocluster algorithm to handle the variance constraint in figure 1: results for abalone dataset  both significance and variance constraints are specified .
order to have a meaningful comparison. second  the ppmicroclustering problem generalizes the condensation group approach  which has only the significance constraint   and the significance constraint is handled by the ppmicrocluster algorithm in the same style as by the condensation group approach. note that the following experiments were conducted on the static phase of the ppmicrocluster algorithm since we did not evaluate the incremental updates of databases and the static phase of the ppmicrocluster algorithm is more effective due to the availability of the global knowledge for all data points.
1 results
　we compared the two algorithms from two aspects  clustering quality and runtime. we set the same parameters for both algorithms and ran them on the aforementioned datasets. the clustering quality is measured by the sum of squared distances of data points to their corresponding cluster representatives. note that the cd-tree and the adapted ppmicrocluster algorithm both satisfy the same constraints  but with possibly different compactness of the discovered clusters. for the cd-tree algorithm  we set the capacity of non-leaf nodes to 1 and the runtime is the total time spent on building a tree and generating constrained clusters from the tree. for the ppmicrocluster algorithm  we assume an index structure  r-tree  existing for supporting fast k-nearest-neighbor query. the runtime only records the time spent on generating valid clusters based on the index structure  excluding the index construction time. all the experiments were conducted on a server with an intel pentium iv 1ghz cpu and 1gb memory running the window server 1.

figure 1: results for the letter dataset  only significance constraints are specified .
figure 1: results for the letter data set  both significance and variance constraints are specified .
　for dataset ds1  the results are presented in figure 1 and 1 for significance constraints only and both constraints respectively. for both comparisons  the cd-tree algorithm outperforms the ppmicrocluster algorithm in terms of clustering quality. similar behavior is observed on the abalone dataset  for which the results are depicted in figure 1 and 1. due to the smaller size of the abalone dataset  both algorithms require comparable time.
　for the larger letter dataset  see figure 1 and 1   we observe that the ppmicrocluster algorithm slightly outperforms the cd-tree algorithm in terms of clustering quality. this behavior is expected since the ppmicrocluster algorithm relies on an index structure to maintain an accurate neighborhood relations among data points  while the tree structure built by the cd-tree algorithm only keeps approximate neighborhood relations among data points. however  the cd-tree algorithm runs more than 1 times faster than the ppmicrocluster algorithm. a small sacrifice of the clustering quality is reasonable for a dynamic algorithm like the cd-tree algorithm.
# of points1 million1 million1 millionruntime  in seconds 11table 1: scalability vs. number of points.
　in order to evaluate the scalability of the cd-tree algorithm to large datasets  we generated three synthetic datasets with 1 million  1 million  and 1 million three dimensional data points. we evaluated only the cd-tree algorithm on these synthetic datasets since the ppmicrocluster algorithm can not handle such large datasets. the runtime results of the cd-tree algorithm with the significance constraint sig = 1 are presented in table 1. in order to evaluate the impact of different values of the constraints  we apply the cd-tree algorithm on the dataset with 1 million points. table 1 contains the runtime results.
constraint combinationsruntime  in seconds sig = 1 v ar = 1sig = 1 v ar = 1sig = 1 v ar = 1sig = 1 v ar = 11sig = 1 v ar = 11sig = 1 v ar = 11table 1: scalability vs. different constraints.
1. conclusion
　clustering methods can be either data-driven or needdriven. among need-driven methods  constrained clustering captures application requirements by specifying constraints. in this paper  we have introduced a novel clustering model  constraint-driven clustering  cdc   which aims at utilizing constraints to drive the cluster formation. we have focused on two constraint types  i.e.  minimum significance constraints and minimum variance constraints  for discovering actionable clusters for applications such as energy aware sensor networks and privacy preservation applications. we have proved the np-hardness of the proposed cdc problem with difference constraints. we have also proposed a novel dynamic data structure  the cd-tree  which keeps dataset summaries that approximately satisfy the given constraints by minimizing the sum of squared distances during its construction. based on cd-trees  an efficient algorithm is developed for the new clustering problem. our experimental evaluation on synthetic and real datasets showed that our algorithm yields good clusters efficiently.
　this paper suggests several interesting directions for future research. first  some issues related to our heuristic algorithm  such as how the maximum capacity of a nonleaf node influences the final partition and how effective is our heuristic algorithm on high dimensional data  can be further studied. second  we want to evaluate the cdc model in real life applications. third  the application needs may not always suggest exact values for the significance and variance constraints. therefore  it is worthwhile to explore variants of the cdc model that allow the user to specify ranges instead of a fixed minimum value. finally  we believe that the cdc framework and the cd-tree algorithm can be generalized to include other constraint types  such as minimum separation constraints   to produce actionable clusters in an even broader category of applications.
