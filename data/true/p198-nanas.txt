term dependence is a natural consequence of language use. its successful representation has been a long standing goal for information retrieval research. we present a methodology for the construction of a concept hierarchy that takes into account the three basic dimensions of term dependence. we also introduce a document evaluation function that allows the use of the concept hierarchy as a user profile for information filtering. initial experimental results indicate that this is a promising approach for incorporating term dependence in the way documents are filtered.
categories and subject descriptors
h.1.  information storage and retrieval : content analysis and indexing; h.1.  information storage and retrieval : information search and retrieval
general terms
algorithms  design  verification
keywords
term dependence  concept hierarchies  information filtering
1. introduction
　 the statistically dependent placement of words in text is a natural consequence of the way people think and communicate . in his classic paper  semantic road maps for literature searchers    doyle describes two basic phenomena that cause statistical dependencies between terms.  language redundancy  refers to the habitual use of lexical compounds as semantic units  while  reality redundancy  relates to the pattern of reference to various aspects of the topic which is being discussed. in the rest of this paper
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
a.deroeck open.ac.uk
we will refer to the term correlations caused by these two phenomena as  lexical correlations and topical correlations respectively. finally  doyle attributes a third phenomenon called  documentation redundancy   to document series like progress reports and newsletters  that repeatedly and periodically refer to the same spectrum of topics.
　doyle goes on to argue  that in building a retrieval system one has to take into account all these three dimensions of term dependence. while term weighting can tackle documentation redundancy  the other two phenomena require a different treatment. to deal with language redundancy  doyle suggests joining terms with strong adjacent  lexical correlations into a single compound term. for reality redundancy on the other hand  he proposes expressing the non-adjacent  topical correlations with an hybrid structure that combines the characteristics of both hierarchies and associative graphs. in other words  he envisions the use of appropriately constructed connectionist networks that express the hierarchical topic-subtopic relations between terms.
　connectionist networks have been the principal approach to representing term dependence in information retrieval  ir . usually  pure associative graphs which only exploit the stochastic dependence that is caused by the above three phenomena  are adopted  1  1  1 . in contrast  recent research has focused on the construction of hierarchical networks based on either subsumption relations  or lexical relations between terms  1  1 . despite their extensive use in ir research  connectionist networks have not been as popular for information filtering  if . exceptions include the if systems described by  1  1  1 . so far no single network structure has managed to represent all three dependence dimensions. in addition the application of such networks for if has been limited.
　in this paper we present a methodology for the construction of a concept hierarchy that complies with the above requirements. a hierarchy is constructed based on a set of user specified documents on a topic of interest. a document evaluation function on the constructed hierarchy is then proposed for filtering documents according to that topic. in that sense the concept hierarchy functions as a representation of the user's interests  i.e. a user profile. the profile's filtering performance has been evaluated on the reuters rcv1 collection. the results indicate that the proposed approach is promising.
1. related work
　in ir  it was recognized early  that the term independence assumption is false and that ways for taking into account term dependence should be found. connectionist approaches have played an important role in this endeavor. different connectionist networks have been proposed. here we concentrate on those that explicitly represent the associations between terms. in these networks terms are represented by nodes and associations between them as links.
　in order to incorporate term dependence in probabilistic ir  van rijsbergen proposes the use of an appropriately constructed  dependence tree  . the dependence tree is derived as the maximum spanning tree of the associative graph that represents the statistical dependence between every possible pair of terms. van rijsbergen suggests the use of the constructed dependence tree for query expansion. bhatia has adopted the latest idea to provide personalized query expansion based on a user profile represented by a dependence tree .
　query expansion can also be based on thesauri. however  the manual construction of a thesaurus is expensive and time consuming. therefore  methods for the automatic generation of thesauri are in great demand. park et. al  propose the automatic construction of a thesaurus using a term similarity measure on a  collocation map  . a collocation map is a sigmoid bayesian network that encodes the statistical associations between terms on a given document collection and has been introduced by . a similar approach has been described by  for automatic subject indexing  an application very similar to query expansion.
　in contrast to the above connectionist networks  where the associations between terms are stochastic  approaches for the automatic construction of hierarchical networks has also been proposed. these networks are usually referred to as  concept    topic  or  subject  hierarchies and can be applied for the organization  summarization and interactive access to information. one method for the automatic construction of a concept hierarchy is through the use of subsumption associations between terms   subsumption hierarchies   . a term ti is said to subsume tj if the documents that contain tj are a subset of the documents containing ti. another approach is to rely on frequently occurring words within phrases or lexical compounds. the creation of such  lexical hierarchies  has been explored by  1  1 . in addition to the above two approaches  lawrie et. al. have investigated the generation of a concept hierarchy using a combination of a graph theoretic algorithm and a language model . the approach has been shown to perform comparatively to both subsumption and lexical hierarchies. finally  an evaluation that indicates that subsumption hierarchies are advantageous comparing to lexical hierarchies has been conducted by .
　despite their extensive use in ir research  connectionist networks of the type discussed here have not been as popular in if. exceptions include the informer  and psun  filtering systems  which adopt an associative term network to represent user interests. for constructing the network  terms are considered to be associated if they appear in the same phrase. in the case of informer  document evaluation is performed using a spreading activation function on the associative term network. a similar approach has also been adopted by . in psun on the other hand an extra representational layer of  supervisors  is introduced to group strongly associated terms into lexical compounds. when these compounds are found in a document  the corresponding supervisors contribute to the document's relevance.
　so  in contrast to pure associative term networks that only represent stochastic relations between terms  concept hierarchies have the potential of representing topic-subtopic relations. however  while subsumption hierarchies do not take into account the lexical relations between terms  lexical hierarchies are only based on such relations. in addition  although recent connectionist approaches to if have made significant steps towards representations of the user interests that incorporate term dependence  these systems are only based on lexical  phrase-based relations between terms. both a methodology for constructing an hierarchical network that tackles all three dependence dimensions and a way of applying such a network to if is legitimate.
1. building a personal concept hierarchy
　in this section we present a methodology for constructing a concept hierarchy that represents this content of a set of user specified documents that distinguishes them from the rest of the documents in a complete collection. therefore  the methodology can be applied for the initialization of a user profile that represents the underlying documents' topic. the construction takes place in three different phases that progressively tackle the three term dependence dimensions. document redundancy is dealt with in the next section  where term weighting is applied to identify those out of the unique terms in the user specified documents that are most specific to those documents. the extracted terms are used as the concept hierarchy's building blocks. in section 1  term associations are derived using a sliding window approach in order to generate an associative graph  from which a concept hierarchy is extracted in section 1. these two phases have the joint effect of lexical and topical correlations being represented by the final concept hierarchy.
1 term weighting and selection
　our first goal is to identify the most appropriate terms for building the concept hierarchy  user profile . given a number r of user specified documents  we initially apply stop word removal and stemming to reduce the space of unique terms in the documents. term weighting is used to assess the specificity of terms to the documents' underlying topic  i.e. their ability to distinguish documents about that topic from the rest of the documents in the collection. the most specific terms are then selected to populate the profile  fig. 1 . we thus tackle document redundancy by filtering out terms that don't relate to the underlying topic. of course the exact number of profile terms is a parameter that is subject to optimization. if we ignore term dependence then this initial  unconnected profile version can be used to evaluate documents. for example  the inner product between the profile's and the document's vector representations  that is described in   can be applied.
　we used a novel term weighting method called relative document frequency  reldf   that measures the relative importance of terms within the user specified documents and a general collection of documents. the essence behind the approach is analogous to the relative frequency technique that has been suggested by edmundson and wyllys 

figure 1: specific terms populate the profile
 hence the adopted name . based on the assumption that special or technical words are more rare in general usage than in documents about the corresponding subjects  they presented a number of ways for assessing the relative frequency of terms within a document and a general collection. their approach has been highlighted by doyle as a potential remedy for document redundancy .
　in a similar way  we assume that terms pertaining to the topic of interest to the user will appear in a larger percentage of the user specified documents than in the complete collection. the method assigns to each term  a weight in the interval  -1   according to the difference between the term's probabilities of appearance in the user specified documents and in the general collection. we define reldf using equation 1  where n is the number of documents in the collection  r the number of user specified documents  and r and n are respectively the number of user specified documents and the number of documents in the collection that contain t. while the first part of the equation    favors those terms that exhaustively describe the user specified documents and therefore the underlying topic of interest  the second part   nn   biases the weighting towards terms that are specific within the general collection. experiments presented in  have indicated that reldf outperforms a large number of existing term weighting methods  when the user specifies a small number of documents.
		 1 
1 link generation and weighting
　having extracted the most specific terms from the user specified documents  the next step is to appropriately associate them. a span of contiguous words that slides through each document's text  i.e. a  sliding window   is used for selecting the extracted terms to be linked. the size of the window defines the kind of associations that we take into account. a small window of a few words is usually called  local context  and is appropriate for identifying adjacent  lexical correlations.  topical context  on the other hand  is defined by a larger window that incorporates several sentences. the goal of topical context is to identify semantic relations between terms that are repeatedly used in text discussing a certain topic   topical correlations .
　to identify topical correlations we adopt topical context. for the work presented here we have chosen a window of

figure 1: associative links between profile terms
size 1 which is larger that the typical size of local context. topical correlations between terms that appear within the window are measured using a formula similar to the one adopted by . in addition  the formula has been extended to measure the lexical correlations between terms by their average distance in text. more specifically  a weight wij is assigned to the link between two extracted terms ti and tj using the following formula:
		 1 
in equation 1  frij is the number of times ti and tj appear within the sliding window  fri and frj are respectively the number of occurrences of ti and tj in the user specified documents and d is the average distance between the two linked terms. two extracted terms that appear next to each other have a distance of 1  while if n words intervene between them the distance is n + 1.
　the above process connects the extracted  profile terms of figure 1 with symmetric  associative links  figure 1 . the first fraction of equation 1 measures the likelihood that the two extracted terms will appear within the sliding window. the second fraction on other hand is a measure of how close the two terms usually appear. as a result  a link's weight is a combined measure of the statistical dependencies caused by both lexical and topical correlations. according to doyle we could tackle language redundancy by joining terms into a single lexical compound if their link's weight was over a certain threshold. nevertheless  we have chosen not to do so for reasons explained later.
1 extracting a hierarchy
　in order to extract a concept hierarchy out of the associative graph of figure 1  a way of identifying topic-subtopic relations between terms is required. we adopt forsyth and radas' hypothesis  that the more documents a term appears in the more general the term is assumed to be . some of the profile terms will broadly define the underlying topic  while others co-occur with a general term and provide its attributes  specializations and related concepts .
　based on this hypothesis  profile terms are ordered according to decreasing relevant document frequency  rdf   i.e. their document frequency in the user specified documents. the higher a term's rank the more general the term

1
1 words at either side of an extracted term

figure 1: concept hierarchy profile representation
is assumed to be. the terms are then assigned to three layers according to rdf thresholds. for the current work we used the following thresholds: if rdfmax is the rdf of the most frequent profile term  then profile terms with rdf − 1   rdfmax are assigned to the top layer  those with rdf − 1   rdfmax to the middle layer and the rest of them to the third  lowest layer1  fig. 1 . this division allows the implementation of the document evaluation function that is described in the next section. we are currently experimenting with document evaluation functions that do not require this division and can instead be applied on the continuous hierarchical network that results from the initial term ordering.
　terms in the top layer define the general topic of interest  terms in the middle layer correspond to related subtopics and terms in the lowest layer comprise the subvocabulary used when the topic is discussed. links that cross layer boundaries connect general terms in the top layers with more specific terms in the layers bellow  fig. 1: links in black . in other words  cross-layer links reflect topic-subtopic relations between terms. the profile terms and the cross-layer links comprise a cyclic  hierarchical network that complies with most of the design principles set by sanderson and croft for the generation of a concept hierarchy using subsumption . however  in contrast to subsumption hierarchies  where the links are generated based on pure co-occurrence data  the adopted link generation and weighting process combines co-occurrence with distance between terms. in conclusion  while topic-subtopic relations are represented by cross-layer links  lexical correlations are reflected by the link weights.
1. document evaluation
　the above methodology generates out of a set of user specified documents on a topic of interest  a concept hierarchy that takes into account all three dependence dimensions. in this section  we address how to use this profile representation for document filtering. we have drawn ideas from the application of neural networks  1  1  and semantic networks  1  1  1  to ir. according to these connectionist approaches  documents  queries and index terms are represented by nodes. terms that are contained in a document

1
 since profile terms are only the most specific terms in the user specified documents  see section 1   very few terms are assigned to the top layer  some more to the middle and the majority to the lowest layer.
or a query are linked to the corresponding nodes. links between terms and between documents are not used. retrieval takes place on the basis of a spreading activation function. a fixed amount of energy is initially deposited to the query node. the energy is then disseminated through the network and towards the document nodes. a document's relevance is finally estimated by the amount of energy that a document received. the difference is that in our case the network consists only of terms. spreading activation functions have also been applied in the case of associative graphs that express the stochastic correlations between terms  1  1 . due to the inherent lack of direction in these networks  an initial energy is assigned to the terms that appear in a specific document and is then iteratively disseminated through the network until an equilibrium is reached. the document evaluation function that is established here is based on a directed  spreading activation function that combines the characteristics of the above approaches. although the network contains only terms the direction implied by the hierarchy and more specifically its layering  is taken into account.
　given a document d  an activation of 1  binary indexing  is passed to those profile terms that appear in d. the initial energy of an activated term ti  is ei = 1 ， wi  where wi is the weight that has been assigned to the term by the term weighting method  reldf . if and only if  an activated term ti is directly linked to another activated term tj in a higher profile layer  then an amount of energy eij is disseminated by ti to tj through the corresponding link. eij = ei ， wij  where wij is the weight of the link between ti and tj. a direction from lower to higher profile layers is thus imposed on the cross-layer links  fig. 1: arrows . activated terms in the top two layers update their initial energy by the amounts of energy that they receive from activated terms in the layers bellow. for example  the updated energy of term tj is  where al is the set of activated terms that are directly linked to tj and that appear in lower profile layers. terms in the middle layer update their energies first and terms in the top layer last  feedforward . as a result  activated terms in the middle layer disseminate part of their  updated  energy. if ah is the set of activated terms in the top two layers that have either received or disseminated energy  then the document's score is calculated by equation 1  where k is the number of words in d. only terms in ah are thus allowed to directly contribute to the document's relevance. this constrain is a drawback of the current implementation for two reasons. first  according to this constrain  term t in figure 1 does not contribute to the documents relevance  despite representing a subtopic of interest. this can negatively affect the filtering performance  especially for profiles with a small number of terms and consequently of links. in addition  the current document evaluation function favors terms in the top two layers. activated terms in the lowest layer contribute only implicitly by the energy that they disseminate to activated terms in the layers above. as already mentioned  we are currently experimenting with continuous evaluation functions which tackle both of the above issues.
		 1 
　the above establish a non-linear document evaluation function that takes into account the term dependencies that the concept hierarchy represents. as energy disseminates from lower to higher hierarchical layers  terms in the higher profile layers that appear in the document together with their associated terms in the layers bellow  have an increased contribution to the document's score. therefore  a document about both  topics and related subtopics of interest  receives a higher score than a document about the same number of un-related topics. on the other hand  terms in the lowest profile layer disseminate their energy  only if they are found together with their associated topics and sub-topics of interest  in the layers above. in other words  these terms contribute implicitly to the document's relevance if they are found in the topical context that they were extracted from. topical correlations between terms are thus taken into account.
　lexical correlations are also taken into account due to the way links are weighted. the amount of energy disseminated between two terms of different profile layers  is larger if they are part of a lexical compound. on the other hand  if the compound's terms are found in the same layer  then their contribution to the document's score can be enhanced because of the large number of links that they probably have in common. terms that are found in adjacent positions in text share a lot of links to other terms.
　in addition to taking into account both topical and lexical correlations in the way documents are evaluated  the proposed approach is also computationally cheap. in contrast to traditional spreading activation approaches where numerous computational cycles are required for the network to reach an equilibrium  document evaluation takes place in two forward steps. the combination of the above two characteristics of the proposed document evaluation process  could represent a significant improvement over existing connectionist approaches to if  1  1  1 . furthermore  as we will shortly discuss in section 1  adaptation of the proposed profile representation to changes in the user interests can be effectively achieved.
1. evaluation
　to evaluate the performance of the proposed hierarchical profile  we have performed some pilot experiments on a slight modification of the trec-1 routing subtask.
　the text retrieval conference  trec  has been held annually since 1. its purpose is to provide a standard infrastructure for the large-scale evaluation of ir systems. trec-1 adopts the reuters corpus volume 1  rcv1 . the latter is an archive of 1 english language news stories that has recently been made freely available for research purposes1. the stories have been manually categorized according to topic  region  and industry sector . the trec-1 filtering track is based on 1 out of the 1 rcv1 topic categories. furthermore  it divides rcv1 into 1 training stories and a test set comprising the rest of the stories1.
　according to trec's routing task  systems are allowed to use the complete relevance information and any nonrelevance-related information from the training set. systems are evaluated on the basis of the best 1 scoring documents  using the average uninterpolated precision  aup 

1 http://about.reuters.com/researchandstandards/
corpus/index.asp 1
 for more details on the trec 1 filtering track see: http://trec.nist.gov/data/t1filtering/t1filter guide.htm table 1: per topic average aup score
profile versiontopicunconnectedhierarchicalr1.1.1r1.1.1r1.1.1r1.1.1r1.1.1r1.1.1r1.1.1r1.1.1r1.1.1r1.1.1measure. the aup is defined as the sum of the precision value at each point in the list where a relevant document appears  divided by the total number of relevant documents.
　we have deviated from the strict trec routing guidelines in two ways. to reduce the required completion time  the experiments have been conducted on only the first 1  r1-r1  out of the 1 topic categories. furthermore  only 1 relevant documents per topic and not the hundreds provided by the training set  were allowed for constructing the corresponding profile. this amount was considered a better approximation of the real number of documents that a user might provide.
　for each one of the 1 topics a profile was constructed using the best terms on the basis of reldf weighting. we have experimented with different numbers of extracted terms. these could be 1  1  1  1 and 1. for each possible combination of topic and number of extracted terms two different kind of profiles were constructed. in the first case no links between terms were generated and documents were evaluated using the inner product measure  unconnected profile version . in the second case the same extracted words were used to generate the concept hierarchy and documents were evaluated using the introduced approach  hierarchical profile version . since the same weighted terms have been used in both cases any differences in performance is due to the incorporation of term dependence by the hierarchical profile representation.
　figure 1 presents for each number of profile terms the average aup score over all 1 topics for the unconnected and hierarchical profile version. the x axis corresponds to the number of profile terms and the y axis presents the average aup. table 1 presents for each topic  the average aup score over the different numbers of profile terms.
　the results indicate that the hierarchical profile performs better than the unconnected profile for large numbers of profile terms  for which representing term dependence is of particular importance. its inferior performance for small number of profile terms is justified to some extent by the fact that the hierarchy is not substantially complete. the fact that terms do not contribute to the document's relevance if they don't receive or disseminate energy  as mentioned in section 1  is an additional reason. it is also important to note that the performance of the hierarchical profile is
 unconnectedhierarchicalfigure 1: experimental resultsnot significantly affected by the number of profile terms. it favors only those combinations of terms that comprise a meaningful hierarchy. finally  its overall performance is better for 1 out of the 1 topics. for topics r1 and r1 in particular  the hierarchical profile performs almost twice as well as the unconnected version. for topic r1 however  the opposite appears to be happening. one reason could be  that although topic r1 is relatively specific  reuters-code c1    it has a large number of documents in the test set  1 . this particularity of topic r1 in combination to the small number of initialization documents may have resulted in profile overspecialization to the subjects being discussed during the time period that the initialization documents derive from.
　these findings encourage us to believe  that the extracted concept hierarchy is a promising approach for representing a user profile. of course  further improvements can be achieved by appropriate fine tuning of parameters like the window size and in general via improvements in the document evaluation function.
1. conclusions and future work
　we have presented a methodology that generates a concept hierarchy from a set of user specified documents about a topic of interest  through a series of processes that take into account document  language and reality redundancy. appropriately weighted and selected terms are used as the hierarchy's nodes while hierarchical links and their weights represent topical and lexical correlations between the selected terms. a computationally cheap  document evaluation function has allowed us to apply the concept hierarchy as a user profile for information filtering. initial experimental results are promising and point towards further improvements. these include experimentation with document evaluation functions that exploit a continuous ordering of terms.
　we should also note  that the hierarchical profile structure can represent more than one topic of interest. in the context of information filtering  this means that distinct or overlapping hierarchical networks can be constructed for each topic  to compose a user profile. a document's topic could then be indicated by the hierarchical network that was more strongly activated. automatic summarization can also be achieved by using the document evaluation function to score individual sentences within the document. since no syntactical rules are used the approach can also be applied to the personalized filtering of any media for which meaningful ordered features can be extracted.
　current work is focusing on using this research as part of the development of an adaptive information filter  called nootropia. adaptation of the hierarchical profile is achieved via an economic model in which profile terms are assigned  credit  analogous to their relevant document frequency and must pay part of it as  rent  on the basis of the layer they occupy. credit is awarded or penalized according to user feedback and as a result terms are promoted or demoted from one profile layer to another. this process results in new cross-layer links while terms that are demoted from the lowest layer are purged from the profile. these terms are replaced by new informative terms that are extracted from documents that received positive feedback. term and link weights are also updated according to temporary profiles that are generated using the process described here from the sets of documents that have received positive and negative feedback. in conclusion  the hierarchical representation that we have described is not static. it can be adapted to changes in the user interests.
1. additional authors
　additional author: john domingue  knowledge media institute  email: j.b.domingue open.ac.uk .
