모모cas-ict took part in the trec conference for the second time this year and we undertook two tracks of trec-1. for filtering track  we have submitted results of all three subtasks. in adaptive filtering  we paid more attention to undetermined documents processing  profile building and adaptation. in batch filtering and routing  a centroid-based classifier is used with preprocessed samples. for web track  we have submitted results of both two subtasks. different factors are considered to improve the overall performance of our web systems. this paper describes our methods in detail. keywords: trec-1  filtering  web track 
 
1. introduction 
모모cas-ict took part in the trec conference for the second time this year  and we have submitted results of filtering track and web track. 
모모for filtering track  we undertook all three subtasks. our adaptive filtering system is still based on vsm. our rocchio-like profile adaptation algorithm puts stress on the undetermined documents and some strategies are proposed for t1u or t1f optimization. four runs have been submitted for evaluation: all of them are optimized for t1u measure  but in three of them t1f measure is also considered at the same time. in batch filtering and routing  we use a centroid-based classifier with preprocessed samples. two batch filtering runs and two routing runs have been submitted for evaluation. in all of our filtering experiments  we do not use any other resources except the new reuters corpus. 
모모for web track  we undertook both the named page finding task and the topic distillation task. our system is based on smart ftp://ftp.cs.cornell.edu/pub/smart . in the former task  we try to integrate different factors to improve the overall system performance. in the latter task  a variant hits algorithm is used to the top n results returned by smart. five named page finding results and three topic distillation results have been submitted for evaluation. 
 
1. filtering 
모모for filtering track  we undertook all three subtasks  but we paid more attention to the adaptive filtering task. batch filtering and routing tasks are used to test our new classifier.  
1 adaptive filtering 
1.1 introduction 
모모the total 1 topics used in the filtering task this year can be divided into two sets: the first 1 r1-r1  topics are called assessor topics  which are hand-built by nist assessors  and the last 1 r1-r1  topics are called intersection topics  which are derived from reuters category intersections. the two sets have been evaluated separately. 
모모new reuters corpus  http://about.reuters.com/researchandstandards/corpus/  is still used this year  but the training set and testing set are different with trec-1. the first 1 documents are used for training  training set  and the remaining about 1 documents for testing  testing set . the official adaptive filtering measures are utility  t1u  scaled using ault's formula   and f-beta  t1f  beta = 1 . the former is a linear utility measure and the latter is a kind of f-measure. additionally  set precision and set recall measures are also reported in the final results. in the adaptive subtask  only three positive samples in training set are given for each topic  and the goal is to retrieve relevant documents one by one from the coming testing documents stream and get maximum t1u or t1f value at the same time. 
1.1 system description 
모모last year  we have built an adaptive filtering system  which consists of two components: the profile initialization component and the profile adaptation. this year we made some improvement based on this system  in particular  in the profile initialization and optimization modules. 
1.1 initialization 
모모our initialization process includes common operations such as term tokenization  stop words elimination  stemming  tf and idf computation. each topic is treated as a document and processed in the same way. the initial profile vector can be obtained by summing up the topic vector and the three positive documents vectors with different weight. meanwhile  we set the initial threshold by computing the similarities between the initial profile and all the documents in the training set.  
모모since we can't use the idf statistics of testing set till now  we take the idf statistics of the training set as an alternative for term weighting. ideally  we should update the idf statistics when retrieving new documents from the testing documents stream. but our previous experiments have indicated that it does not seem to improve the overall filtering performance. therefore  we use the  idf statistics of the training set without any modification all over our experiments. 
term selection 
모모last year  we applied a new method for feature selection  which can be regarded as a variation of mutual information. the final results indicated that our method is successful when the topic is a single reuters category. 
모모however  each topic of this year has been changed into a natural language statement or an intersection of some reuters categories. our experiment shows that the method does not work well this year. several experiments show that the simple term selection according to the tf and df values is a good choice. 
profile initialization 
모모for each topic  the profile vector  denoted as   p    is the weighted sum of the topic vector  denoted as t   and the feature vector  denoted as f    which is the sum of the initial three positive documents vectors. the formula is: 
	p  =붸* f  + 붹*t                  	 	                 	 	 1  
in our experiment  we set 붸=1 붹=1 to give prominence to the topic vector. 
similarity computation 
we still use the vector cosine distance to compute the similarity between a profile vector   p i   
and a document vector  d j  . tfidf value is used in our system  which is computed by 
 log tfi  +1 *log 1+ n     where n is the number of the total documents in the training set. dfi
1.1 adaptation 
모모for each topic  after initializing the profile and the threshold  we can scan documents one by one from the testing set. if the similarity between the profile and the document is higher than the threshold  the document is retrieved  else not. then we check the answer list of the testing set to find whether the document is really relevant or not. with this information  we can take some kind of adaptation to improve the system performance. the adaptation may include threshold updating and profile updating.  
threshold adaptation 
모모as we know  the goal of the trec-1 adaptive filtering system is to get maximum t1u or t1f. therefore  we adjust the threshold for t1u optimization or t1f optimization. 
모모for t1u  our direct goal is to avoid negative utility value for each topic. when the utility value becomes negative during filtering  which means the system retrieves too many non-relevant documents  we augment the threshold to reduce the number of retrieved documents. another optimization strategy we take is to improve the precision while the recall can't be greatly reduced. 
모모for t1f  our goal is to avoid retrieving zero  relevant  documents. we reduce the threshold when the system retrieves zero documents at an interval. 
profile adaptation 
모모as the filtering task indicates  each profile vector represents a user's interest. after retrieving more and more relevant or non-relevant documents  we can get more and more useful information about the user's interest  which can help us adapt the profile. our profile adaptation includes positive adaptation  negative adaptation and adaptation based on undetermined documents. for positive adaptation  we add the positive documents vectors to the old profile vector with weight 붸. for negative adaptation  we subtract the negative documents vectors from the old profile vector with weight 붹. for adaptation based on undetermined documents  we set a relative high threshold  we use t=1  to filter the retrieved undetermined documents. those retrieved documents that have similarity below t are regarded as pseudo-negative documents and treated as real negative documents. a pseudo-negative document is used in negative with smaller붹value. when retrieving the n+1 th document dn+1  we can adapt the nth profile to the n+1 th profile according to the following formula: 
 pn
	p n + 1 =     pp   nn   +붸붹붹뫣***dd  d nnn+++1                    1    otherwise                                                    and      sim if  if d dnp n+n 1+  is1d  is irrelevantn +relevant1    t	       	 1  
모모thus after we have retrieved n+1 documents  all the retrieved documents are divided into four sets: the relevant set denoted as {d+}  the irrelevant set {d-}  the undetermined but pseudo-negative set {du-} and the remaining documents set {du+}. we do not use {du+} in the adaptation. then the new profile vector is computed by밐 
                                      
	p n+1 = p 1 +붸* 뫉d i  붹* 뫉d  j  붹뫣* 뫉d k   	 	 	 	 1  
	di 뫍{d+}	di뫍{d  }	dk뫍{du }
모모formula  1  is some kind of the rocchio algorithm except one point: we do not compute the centroid of a document set and regard all documents in each set as one vector. in other words  we emphasize the retrieved documents and endow them the ability to adjust the profile vector quickly. as in last year  we investigate the values of 붸  붹 and 붹'. in our experiments  we set 붸=1  붹=1 and 붹'=1. 
undetermined documents processing 
모모in trec-1  the relevance of most documents in the testing set is unknown to the system. in order to get more feedback information  we make some experiments on the undetermined documents. 
모모experiment 1: ignoring the undetermined documents when filtering  we adjust the threshold only according to the relative proportion between the known relevant documents and irrelevant ones. but there is an important presupposition that such a distribution is the same in the undetermined documents. unfortunately  we can' t prove this presupposition. 
모모experiment 1: a simple idea is that if we could know the real relevance of all documents in the testing set  the adaptation strategy proved effective in trec-1 can still be applied. therefore  we make a positive centroid and a negative centroid with the retrieved relevant and irrelevant documents during retrieving the testing set. when retrieving an undetermined document  we judge its relevance by computing its distance from the positive centroid and the negative centroid. those undetermined documents that are nearer to the positive centroid will be treated as real relevant documents  while others will be treated as irrelevant documents. thus we can simulate a situation as in trec-1. this method allows the system retrieving plenty of  relevant  documents  which is helpful to the recall but against the precision. it seems that the initial values of the positive centroid and negative centroid greatly affect the judgment of undetermined documents. the positive centroid can be made by the known three positive samples  but we can't make a good negative centroid because we haven't any negative samples. 
모모experiment 1: suppose the answer list has provided most real relevant documents in the testing set  we treat all or most of the undetermined documents as irrelevant documents. as we've introduced above  a threshold t can be used to filter the undetermined documents  those have similarity below t will be treated as irrelevant documents. the discussion of trec-1 filtering mailing list shows that such a supposition is reasonable. with this method  we can control the retrieved  relevant  documents effectively  which is helpful to the precision. but when the number of real relevant documents in the testing set is big  such a system will suffer a heavy loss. 
모모of the three methods above  we apply the third one finally  partly suggested by the discussion of trec-1 filtering mailing list. the results are encouraging. 
1.1 evaluation results and analysis 
모모we have submitted four adaptive filtering runs: all for t1u optimization  in three of them we make balance between t1u and t1f. ictadaft1ud is optimized for recall  avoiding the heavy loss of relevant documents. as to the optimization method  we use local maximum optimization strategy at every adaptation interval to obtain the holistic maximum. we also adopt a method to avoid zero return at next interval by learning from the current adaptation interval. 
모모table 1 shows the results of the 1 assessor topics. table 1 shows the results of the 1 intersection topics. table 1 is the evaluation results of all 1 topics. of the assessor topics  the system exhibits a good performance. but of the intersection topics  the system behaves badly. 
 
run id meant1u t1u vs. median topic nums meant1ft1f vs. median topic nums   best =  worst/zero   best  =   worst/zero ictadaft1ua 1 1  1 1  1 1  1 1/1  ictadaft1ub 1 1  1 1  1 1  1 1/1  ictadaft1uc 1 1  1 1  1 1  1 1/1  ictadaft1fd 1 1  1 1  1 1  1 1/1  table 1 ict adaptive filtering runs assessor topics  in trec-1 
 
run id meant1u t1u vs. median topic nums meant1ft1f vs. median topic nums   best =  worst/zero   best  =   worst/zero ictadaft1ua 1 1  1 1  1 1  1 1/1  ictadaft1ub 1 1  1 1  1 1  1 1/1  ictadaft1uc 1 1  1 1  1 1  1 1/1  ictadaft1fd 1 1  1 1  1 1  1 1/1  table 1 ict adaptive filtering runs intersection topics  in trec-1 
 
run id meant1u t1u vs. median topic nums meant1ft1f vs. median topic nums   best =  worst/zero   best  =   worst/zero ictadaft1ua 1 1  1 1  1 1  1 1/1  ictadaft1ub 1 1  1 1  1 1  1 1/1  ictadaft1uc 1 1  1 1  1 1  1 1/1  ictadaft1fd 1 1  1 1  1 1  1 1/1  table 1 ict adaptive filtering runs all 1 topics  in trec-1 
 
모모we had partly noticed the problem of intersection topic in our experiment. it seems that the intersection topic itself makes the vsm unsuccessful. after comparing the assessor topics with the intersection topics  we guess the reason maybe that the natural language style of the assessor topics makes them appropriate to be represented and computed with vectors  while the intersection topics are not  because the different dimensions of an intersection topic vector have no internal relations as organic as those of a natural document. another reason we guess is that there are few relevant documents on each topic in the testing set that can be used to adjust the profile vector. in trec-1 our system has proved suitable for  big  topics but not so for  small  topics. the results of last year have also proved that as long as enough relevant documents can be provided  on the intersection-like topics we can still obtain good performance. although in such circumstances we may not make a good initial profile vector  enough feedback can greatly adapt it to the best position. but this year the case is different. we don't have so many relevant documents  so the weakness of vsm on the intersection topics becomes distinct. an evidence is that our system still gets better scores on most intersection topics with relative more relevant documents  such as topic r1  r1  r1  r1 and r1. 
모모in next step  our goal is to find a new way to effectively process the semi-automatically made intersection topics. we believe such topics represent the trend in future and are worthy of much more efforts. accomplishment of the efforts will to some extent lighten assessors' burden in the filtering task. 
1 batch filtering and routing subtasks 
1.1 text representation 
모모in our batch and routing filtering system  when preprocessing the documents  we give additional prominence to the words that occur in the  title  field and we only use tf weight in the vector representation. 
1.1 samples preprocessing 
모모we believe some samples in the training set are not good enough to train the classifier  so we want to eliminate them beforehand. indeed  samples have different weights since features of documents have different weights. importance of samples and importance of features are closely related: 
 an important sample contains many important features;  an important feature appears in many important samples; we calculate the weights of samples as following: 
모모let amn is the matrix of the feature frequency in each sample  m is the number of the documents and n the number of the features. aij is the frequency of the jth feature in the ith sample. 
the weight vectors of samples and features are respectively wf =  wf1 wf1    wfm  ' 
wt =  wt1 wt1    wtn  '. their initial values are wf 1  and wt 1   with each component set to 1. 
모모the formulas below are to compute the weights. it can be proved that the computing process is convergent. 
	w t j  k +1  = 뫉m aij *w fi  k    	 	 	 	 	 	  	 	 	 	 	 1  
i=1
	w f i   k +1  = 뫉n a ij * w t j   k +1  	 	 	 	        	 	 	 	 1  
j =1
 j = 1  1  ...  n밃i = 1  1  ...  m  
모모after computing the weights of all samples  for each topic  we remove the lowest 1% samples and use the remaining samples to train the classifier. 
1.1 training 
	the system uses rocchio method in the training process. for topic  	i  its representative feature 
vector pip is calculated as following:   =p +  붹p                 1  i  
모모where p+ is the centroid of the relevant documents and p  is the centroid of the irrelevant documents in the training set  붹 is an experiential parameter. 
모모since the file filter1 qrels.test cannot be used for training  we use the training set to choose proper values of붹and the threshold by loocv  leave-one-out cross-validation   which is the most extreme and most accurate version of cross-validation.  
 
모모in test process  those documents with high cosine distance to pi are retrieved to form the final results.  
1.1 evaluation results and analysis 
모모we have submitted two batch-filtering runs and two routing runs. all of them are optimized for t1u. the only difference between the two runs are thresholds and the parameter 붹 in the formula  1 .  
 
batch filtering 
모모the evaluations of batch results are shown in table 1. table 1 shows that in each run  the scores of t1u and t1f are close to medians. for the first 1 topics  we get a set precision higher than the median  but the set recall is lower than it. for the last 1 topics  we set a very strict threshold to avoid t1u becoming negative  because the baseline of t1u is 1. as a result  the scores of t1f  set precision  and set recall are all very low. since we have set the same threshold for all 1 topics  we think the results show that the threshold for every topic should be different.  
run id t1u median t1u t1u vs. median  topic nums  t1f median t1f t1f vs. median  topic nums    =    =   ictbatft1ua 1  1 1 1 11 1 1 1 1 ictbatft1ub 1  1 1 11 1 1 1 ictbatft1ua 1 1 1 1 1 1 1 1 1 1 ictbatft1ub 1 1 1 1 1 1 1 1 table 1 ict batch filtering runs  all 1 topics  in trec-1 
 
routing 
모모we set a lower threshold to get 1 documents for each topic to form the routing results. the only difference between the two runs is the parameter 붹. 
run id average precision average precision vs. medians topic nums  all results   =   min med maxictrouft1ua 1  1 1 1 1 1 1 1ictrouft1ub 1  1 1 1 1 ictrouft1ua 1 1 1 1 1 1 1 1ictrouft1ub 1 1 1 1 1 table 1 ict routing runs  all 1 topics  in trec-1 
모모we can see that all of our results are similar to the medians. we think this is because we only set one same threshold for all topics and lack an effective parameter optimization method. we will try to research on automatic parameter optimization methods. 
모모in the future  we have a lot of work to do to improve our work. for feature selection  we want to use n-gram to add more terms to represent the documents. for the last 1 topics  we have tried to use knn to improve the classification results. to our surprise  its result is much worse than the rocchio method. we will research on the phenomenon and try more complex methods.  
 
1. web track 
1 introduction 
모모last year we took part in trec for the first time and we only submitted four runs for the ad hoc task. this year we submitted runs for both two tasks. 
모모this year  web track consists of two new subtasks:  the named page finding task  which is introduced to investigate methods for finding a particular page that has been named by the user  and the topic distillation task  which is introduced to investigate methods for finding key resources in a particular topic area. in the former task  the system should return a single named page as the result. for instance  for the query  passport application form   the correct answer should be the page travel.state.gov/dsp1.pdf  which contains the electronic copy of requested form. in the topic distillation task  a single relevant document is not important any more. the concept resource is introduced as the basic element of results and judgments. the test collection of this year's web track is changed to .gov data set which substitutes wt1g used in previous years. 
모모though the web track tasks have been significantly modified  the basis of experiments is still the traditional ir systems. in trec 1 we investigated the effectiveness of the combination of classical boolean model and probabilistic model in the ad hoc task. we also investigated methods that make use of link information between pages in the same task. neither of the results was as good as we had expected. so this year we decide to adopt vector space model and to make use of only text contents and internal structure of pages. our retrieval system is based on smart. in order to deal with large data set such as wt1g and .gov test collection  we modified the basic smart system  and the lnu-ltu weighting method was added to the system. this method has been proven to be very effective and efficient in our experiments. the classical weighting methods such as lnc-ltc do not behave well in our experiments. 
1 named page finding task 
모모as introduced above  the goal of named page finding task is to find appropriate page s  named by users. it is rather close to a special kind of user requirement  i.e.  finding a few documents that precisely meet the information need of users. the query  passport application form  is an example. another one is the query  table of contents gnu make manual   by which a user would like to find the exact page that is the table of contents of gnu make manual. by analyzing these examples we have found some features that can be utilized. 
모모firstly  the content-based ranking score of traditional ir system is still the most important factor in named page finding. if we assign the content-based score a less important coefficient in result merging process that will be described below  the final results will be worse. this can be explained if we notice that single term is more important in named page finding task than in ad hoc task. this task pays more attention to precision than to recall. only those pages that contain all or most of the query terms would have high possibility of meeting information need implied by the query in  thus they would have higher content-based scores than most of the irrelevant documents. certainly some of irrelevant documents will also have high content-based scores  but we will enhance the scores of relevant documents by result merging process. 
모모secondly  the internal structure of documents will give us plenty of information. as the name of task suggests  query terms of named page finding task are the names of relevant documents. usually they are precise representations of topics. they should more possibly appear in important positions such as document title  beginning sentences of paragraphs and section headers  or display in a striking manner  for example a bold  italic  and large size font face. in such situation authors of documents have explicitly defined them as important terms. we can get a lot of relevance information by comparing the query terms with them. besides this there is another reason why the method is especially useful for the task. queries in ad hoc task are often about general topics. they must be described by natural language so that people can understand the information need under which the queries are developed. so they are prone to ambiguity. correspondingly the relevant documents cannot be named clearly and easily. on the contrary  the information need of named page finding task can be very easily understood  even without extra descriptions  so authors and searchers of the same documents will in the gross adopt the same terms as topic descriptions. the homepage finding task in last year's web track can be regarded as a kind of named page finding task. in fact  when we added the phrase  home page  to the original queries we got obvious improved results. in our contrast experiment  ad hoc runs using document structure information gave poor results whose average precisions are too low to be mentioned while homepage finding runs gave fairly satisfactory results. 
모모the last factor we have proven to be effective for the named page finding task is anchor texts of documents. they act as almost the same role as the second factor. they can be regarded as names given by referrers to target documents. when the target documents can be easily named and referrers adopt the same names widely  retrieval results using the names are fairly satisfactory. 
모모as we have stated above we believe that homepage finding task is a special kind of named page finding task. so except some special methods for homepage finding such as analysis of url depth  the methods that are effective for homepage finding should also be effective for named page finding. we ran our experiments on wt1g data set using topics and qrels developed for the homepage finding task to find the most optimized parameters. the results are shown in table 1 and table 1. we then applied the same system to the .gov data set and named page finding task. the experimental results that we observed have proven to be satisfactory. 
we use the linear result merging method to get the last result of named page finding task. 
the merging formula is 
	w p  =붸*wc p +붹*ws p +붺*wa p            	 	 	 	 	 	 1  
where wc p  is the content weight of page p  ws p  is the weight from structure information  wa p  is the weight from the anchor text of page p and 붸 붹 붺 are their coefficients. in our experiments only the titles of documents are used as structure information. the evaluation results are shown in table 1. 
 
average precision r-precision recall 1 1 1 table 1 our content-based experiment for the ad hoc task of trec-1. 
 
content 붸  structure 붹  anchor text 붺  mrr correct answers 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 table 1 our homepage finding experiments of trec-1 
 
run id mrr answers found 1 not found all ictnp1 1 1 1 ictnp1 1 1 1 ictnp1 1 1 1 ictnp1 1 1 1 ictnp1 1 1 1 table 1 ict named page finding runs in trec-1 
 
1 topic distillation task 
as described in the trec-1 web track guideline  a key resource might be: 
 the home page of a site dedicated to the topic.  
모모 the main page of a sub-site  part of a site  dedicated to the topic.  if there are several relevant pages but no main page linking them  then the individual pages must be judged on their own merit.   
모모 a highly useful html  pdf  doc  ps page dedicated to the topic  should be an outstandingly useful page . return the page's url. 
 a highly useful page of links  hub page  on the topic. return its url.  
 a relevant service e.g. perhaps http://www.nasa.gov/search/ for the nasa topic. 
모모except the last two cases key resources are some important pages inside individual sites. our first experiment was based on hits algorithm. we submitted queries to smart and retrieved ranked page lists  and then applied hits to every group of pages coming from the same site. we extracted the page that had the maximum hub+authority value from each group of pages and added them to the final result. we found that the average result of this method was disappointing  partly because many hub and authority pages computed by hits cannot meet the definition of key resource. our last experiment on this task was based on a simple idea. after the first retrieval  we scanned the page list. if we found a page's url containing the other's  we then re-weighted the latter page by adding the former's weight to the latter's. after re-weighting the weight of a certain result page x is 
	w=뫉 wp 	 	 	 	 	 	 	 	 	 	 	 	 	 	 1  
p 
모모where p is a page whose url string contains x's  wp is the content weight of page p and rp is the rank of page p. the run icttd1 is based on this approach  and ictted1 is based on icttd1 plus some additional re-weighting methods. the evaluation result is shown in table 1. 
모모the run icttd1 is a baseline run produced by our retrieval system. it is the best one among the three runs. it seems that our re-weighting methods are not so effective as we have expected. we believe that more attentions should be paid to the instances of key resources given by the trec qrels so that characters of them can be found. 
 
run id average precision r-precision rel ret icttd1 1 1 1 icttd1 1 1 1 icttd1 1 1 1 table 1 result of topic distillation task in trec-1 
1. conclusion 
모모we've participated in the trec conference for two times. by communicating with the researcher all over the world  we've learned more. we've got many experiences in english information processing  which will benefit us greatly in our chinese information processing. 
모모trec not only advances our research on ir  but also enlighten our insights. from here  we can find our advantages and disadvantages comparison to the foreign friends going the same way. we are glad to take part in trec continuously. 
 
acknowledgements 
 this research is supported by the national 1 fundamental research program under contact of g1  the institute youth fund under contact 1 and the institute youth fund under contact 1. we give our thanks to all the people who have contributed to this research and development  in particular yanbo han  li guo  qun liu  xin zhang  hao zhang  dongbo bu and huaping zhang. 
 
