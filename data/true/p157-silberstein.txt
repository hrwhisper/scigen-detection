wireless sensor networks have created new opportunities for data collection in a variety of scenarios  such as environmental and industrial  where we expect data to be temporally and spatially correlated. researchers may want to continuously collect all sensor data from the network for later analysis. suppression  both temporal and spatial  provides opportunities for reducing the energy cost of sensor data collection. we demonstrate how both types can be combined for maximal benefit. we frame the problem as one of monitoring node and edge constraints. a monitored node triggers a report if its value changes. a monitored edge triggers a report if the difference between its nodes' values changes. the set of reports collected at the base station is used to derive all node values. we fully exploit the potential of this global inference in our algorithm  conch  short for constraint chaining. constraint chaining builds a network of constraints that are maintained locally  but allow a global view of values to be maintained with minimal cost. network failure complicates the use of suppression  since either causes an absence of reports. we add enhancements to conch to build in redundant constraints and provide a method to interpret the resulting reports in case of uncertainty. using simulation we experimentally evaluate conch's effectiveness against competing schemes in a number of interesting scenarios.
1 introduction
wireless sensor networks have the potential to enable data collection on an unprecedented scale. they have a range of scientific  industrial  and military applications. for example  our collaborators in environmental science have deployed a sensor network in a forest to collect light  temperature  soil moisture  and sap flow data to understand how environmental changes influence the growth  survival  and reproduction of trees. the key challenge in this project is to collect data from the network efficiently and continuously for analysis. a straightforward approach is to instruct all nodes in the sensor network to send their readings at regular intervals to a base station. periodic reporting  however  is at odds with one of the major concerns for wireless sensor networks: energy. nodes have

 the authors are supported by nsf career award iis-1  nsf grant cns-1  and an ibm faculty award.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
limited battery life  and radio transmission is the primary consumer of energy. excessive transmission quickly depletes batteries  rendering the nodes useless.
　if we can describe the data to be collected as a declarative query  we can  push  the query down into the sensor network-an approach pioneered by systems such as cougar and tinydb  1  1 . given the query  we can perform filtering and aggregation in the network to reduce the amount of data transmitted to the base station. for example  if we only need to obtain the highest temperature reading from the network  only one reading need reach the base station; other readings can be dropped on their way to the base station as soon as they encounter another reading that is higher. in many situations  however  users may want to collect all sensor readings  without any filtering or aggregation  to produce a dataset that will support offline  ad hoc analysis later. this situation is especially common in scientific applications of sensor networks  where analysis of data is often exploratory in nature. for example  our collaborators in environmental science would like to collect all temperature readings  up to a certain precision  in an area over a time  which can then be used offline to develop and evaluate sophisticated statistical models of tree growth.
　there are several opportunities for reducing the cost of monitoring and reporting a group of sensor readings  independent of query type. first  readings often change slowly over time and do not usually deviate from their expected values. for example  in industrial applications  a change in some monitored quantity may be a warning of impending failure and occurs rarely. second  readings are often spatially correlated. in habitat monitoring  if one sound sensor detects a loud cry from an animal  its neighboring sensors likely hear the same sound. in general  many types of sensor data exhibit strong correlation in both space and time. for example  if one sensor detects high soil moisture due to precipitation  sensors in neighboring areas with similar soil composition and elevation likely observe similar moisture readings; furthermore  soil moisture usually stays at a saturated level during precipitation  and tapers off to a normal level afterward. a primary goal of this paper is to investigate how to make continuous monitoring in sensor networks more energy-efficient by exploiting these opportunities.
　we use the term suppression to refer generally to queryindependent techniques for reducing the cost of reporting changes in sensor values. we outline several basic techniques below  and motivate the need for developing better ones. temporal suppression in this scheme  a node does not transmit a value if it has not changed since last reported. the base station  in turn  assumes any unreported values remain unchanged. this scheme is effective when values seldom change. on the other hand  when large-scale change occurs in an area  all nodes must report to the base station  incurring a high cost.

figure 1: moving front of a phenomenon.
spatial suppression in the basic version of this scheme  a node suppresses its value if it is identical to those of its neighboring nodes. meng et al.  suggests a more effective scheme based on averages  in which all nodes attempt to report their values at different time slots during a logical timestep. nodes overhear reports from their neighbors. when a node's slot comes up  it first computes the average of all values overheard so far. if its value equals this average  its report is suppressed. the base station later fills in missing values with the average of the neighboring values. this approach does introduce some complications. to accurately derive a node's value  the base station must know which neighbors were averaged when suppression was triggered; the average of this subset may be different from the average of all neighbors. resolving this discrepancy requires a global reporting order of all nodes  maintained by the base station. holding to a global order in the presence of node clock skew is likely expensive for sensors. we ignore this overhead  however  for the sake of comparison with more advanced schemes.
spatio-temporal suppression the next step is to combine the advantages of both spatial and temporal suppression techniques. one idea is to suppress nodes if they qualify for either type of suppression. this idea is flawed  however  because it is impossible for the base station to correctly derive unreported values. the  or ing of the schemes creates ambiguity. the base station does not know if a missing value is temporally or spatially suppressed; if the values derived by assuming each case turn out to be different  there is no way to determine the correct value. another possibility is to  and  the schemes  suppressing only if a node qualifies for both. this approach  however  would be less effective than using only one type of suppression. fewer nodes can be suppressed both temporally and spatially than can be suppressed one way or the other. therefore   and ing the schemes would only hurt efficiency.
　nevertheless  there is potential in combined suppression. if values do not change  they should not be reported. in addition  if they do change  but the relationship between neighboring nodes remain the same  we may suppress some reports. consider the network fragment shown in figure 1. in one timestep  a physical phenomenon causes a band of nodes to all rise in value  shift from white to black . discretized environmental readings  e.g.  temperature or moisture  often exhibit this type of correlated behavior  where values in the interior of the band change  but maintain the same relative relationships with their neighbors. we want to suppress these changes. ideally  only one node on each boundary  old and new  of the band needs to report the change that is consistent within the band. in contrast  with pure temporal suppression  all nodes in the band must report. with pure spatial suppression  all nodes on the band boundary must report; for every local cluster of nodes within the band  at least one node must report.
our contributions as we have illustrated  it is not obvious how to design an effective spatio-temporal suppression policy. our primary goal  therefore  is to develop a monitoring algorithm that fully exploits the potential of spatio-temporal suppression in minimizing the energy cost. an additional challenge we address is handling of failures  which can have particularly bad effects on data quality of highly efficient suppression techniques. if the base station does not hear from a node  there may be no change to report  but it could be the case that an update message was lost. in the worst case  a single lost update message can affect the data quality in a large area; for example  with ideal spatio-temporal suppression for the scenario in figure 1  if the update message from the boundary node is lost in transmission  the base station will miss the new values in the entire band. we investigate how to augment our monitoring algorithm in the presence of failures  which are common in wireless sensor networks deployments. specifically  our contributions in this paper include the following:
  we present a spatio-temporal suppression technique called conch  short for constraint chaining. by monitoring a combination of individual values  as node constraints  and relationships between neighboring values  as edge constraints   conch exploits spatio-temporal correlations in data much more effectively than previous suppression schemes. conch  chains  together locally monitored node and edge constraints into a network of constraints spanning all nodes. this constraint network allows us to use a change detected locally at one point in the network to globally infer values at nodes around the network  without incurring the cost of communicating between these nodes. for example  with these features  conch is able to achieve ideal spatio-temporal suppression for the scenario in figure 1.
  we develop cost-based optimization techniques for constructing conch monitoring plans aimed at minimizing the total energy consumption in the sensor network.
  to cope with failures in the sensor network  we propose to augment a minimum-cost conch monitoring plan with redundancy. we present a framework for removing data inconsistency caused by failures and for recovering correct values. we provide a computationally feasible method for recovering the most likely correct values under simplified value and failure models.
  we experimentally evaluate conch against other monitoring algorithms using simulation  and demonstrate its advantages  often an order of magnitude reduction in energy costs  for a number of representative scenarios. our experiments also show the effectiveness of our failure-handling techniques.
1 related work
suppression a number of papers support monitoring queries using suppression. one approach that closely compares to ours is event contour . this method uses temporal suppression to only report values when they change by a significant amount  and applies a neighborhood approach to invoke spatial suppression as well. when a node attempts to report its value to its parent node  it must compete with neighboring nodes that also wish to send. while it waits for a chance  the node overhears the values transmitted by its neighbors. if these values average to within some threshold of the node's  it suppresses its value. therefore  if a neighborhood contains nodes all with similar values  not all will be sent. nevertheless  we are left with a number of problems. first  we can never suppress all readings in an area  even if all are the same. second  if a pair of nodes have different values  but are highly correlated such that they always move together  we will always report both  even though one is sufficient to derive the other. finally  the spatial component suffers from the flaw mentioned in section 1 where the base station does not know on which values suppression is based.
　solis and obrazcka  present a solution for continuously maintaining isoline maps. they recognize the need to combine spatial and temporal suppression. their algorithm is very similar to our precursor neighborhood scheme presented in section 1. unlike our scheme  though  they report whenever a difference between neighbors is detected; they miss out on the chance to suppress values that differ  but whose relative difference remains consistent. both suffer from redundant reporting  as we describe in section 1.
conch resolves this issue.
　chintalapudi and govindan  address the problem of edge detection and returning enough data so the root can construct an accurate depiction of the boundaries of some phenomenon. nodes can individually determine if they reside within the phenomenon  but must consult other nodes within some radius to determine if they are on the edge of it  i.e.  if there is a neighbor not registering the phenomenon . if nodes contact too large a neighborhood  the scheme risks sending redundant data to the root. if nodes contact too small a neighborhood  the scheme risks missing some data and mis-setting the boundary.
　temporal suppression fitsnaturally with continuous queries. one example    temporally suppresses nodes from reporting their values if they have not changed by more than some percent since previously reported. this bounded approximation supports continuous aggregate queries with bounded error. less is at stake in suppression for aggregates  whose semantics naturally imply compression  than monitoring all values  where as many as all values are potentially returned.
spatial suppression with representative nodes a managed approach to avoid sending correlated data to the root is to gather such data at intermediate points  1  1 . the main observation is if a set of data is highly correlated  rather than sending values to the root by the shortest route  it is beneficial to send them to a mutual gathering point. at this point the correlation is discovered  and only non-redundant values are sent on to the root. this technique is essentially one of spatial suppression. because these approaches are tailored to ad hoc queries  neither considers temporal suppression. in  nodes are organized into clusters; all nodes in a cluster send their values to a cluster head node; suppression happens at the cluster head and also en route. the other approach    declares regions of interest and arranges for nodes within a region to send to a common border node  where suppression can occur. in both cases  opportunities to suppress across clusters/regions are not exploited. furthermore  the cluster shapes are fixed based on network topology and are not tailored to actual correlation patterns. the regions of interest are set by the root and the cost for adjusting the regions is significant. the rigidity of these may lead to inefficiency. these are issues we solve with conch  whose planning exploits correlation patterns  and can adapt to changes.
　chu et al. propose the ken  framework for suppression in continuous monitoring. it uses a joint probability model to suppress as much data as possible from reaching the root  while still allowing the root to derive suppressed values from those reported with some level of certainty. they propose a disjoint-clique approach to divide nodes into groups and then build models for each of these. temporal suppression cannot be fully achieved since all values are sent to clique roots each round. there is a trade-off between temporal and spatial suppression. the larger the cliques  the more spatial can be exploited  but the further values must be sent each round.
　another category of work chooses representative nodes from neighborhoods to exploit spatially correlated data. examples are snapshot queries  and connected k-coverage problem . these choose a subset of nodes to respond to queries  where each chosen node stands in for neighboring nodes within some error. this work leverages correlation as we do  and implies spatial suppression when they allow some nodes to ignore queries. the use of coverage nodes means results are approximate  although with some error guarantee. therefore  regular checks must be done between nodes and their representatives. this step is not discussed in . in the case of   these look similar to our  update  messages  though theirs are sent with regular frequency  while ours are triggered by value change. the use of coverage nodes ensures some number of nodes respond to every query. both approaches require every node be represented by a node within communication distance. therefore  this approach can never achieve our goal of near or total suppression. finally  these schemes are not tailored to continuous queries  so do not address temporal suppression.
　a common theme among the preceding approaches is that certain nodes become responsible for reporting for their regions and must report in the event of change. conch as we will show  differs from these because it uses constraint chaining. it is possible for a region to experience change but have no node from that region report to the root. such changes can be inferred using change reports from distant nodes  or from changes at the root itself. model-based suppression we have already seen the use of models for spatial suppression in . an example of temporal suppression using models rather than values is . they use kalman filters to suppress node data as long as it fits the current model. this work is orthogonal to ours  and exactly the type of sophisticated modeling we think can be plugged into conch to direct suppression. a strategy in  is to buffer large amounts of measurements at each node. rather than transmit the buffer contents  they identify and transmit a base signal of a few parameters  which can be used to estimate all measurements. this approach is again orthogonal to ours; instead of monitoring actual values  we can apply our techniques to monitor the parameters of the base signal.
failure the more effectively suppression is exploited  the more damage can be done by misinterpreting a failed message as a suppressed message. failure handling has been approached by use of redundancy in the context of routing and ad hoc querying. for example   divides routing into graph and tree portions  where data is initially sent up multiple paths to protect against failure. the higher the risk of failure  the larger the graph portion. failure is not as well studied in association with suppression. tina  uses heartbeat message to check if nodes that have been suppressing for some time are still alive. unless these are sent with high frequency  however  there is a risk that failure will go undetected for a long time and affect many rounds of results. on the other hand  sending them with high frequency defeats the purpose of suppression.
1 preliminaries
sensor network a sensor network consists of a set n of n fixedlocation nodes {ui | 1 ＋ i ＋ n}  each measuring a value vi. one node serving as the base station is a full-scale computer with no energy limitations. the base station knows all nodes and their locations  and is responsible for planning queries and monitoring tasks and extracting and reconstructing values from the network. a communication edge eij exists between any pair of nodes ui and uj within communication distance. a communication network utilizing these edges connects all nodes to the base station  either directly or through other nodes. any existing protocol  1  1  can be used for routing.
　the primary use of energy in sensor nodes is for radio communication. the cost of transmitting data dwarfs the cost of doing computation . therefore  we optimize energy efficiency by minimizing the number and size of messages sent through the network  and evaluate all algorithms on this metric. the total amount of energy spent in sending a message with x bytes of content is given by σs + δsx  where σs and δs represent the per-message and perbyte sending costs  respectively. as an example  typical values for
mica1 motes  are σs = .1mj and δs = .1mj/byte. receiving cost is defined analogously  with typical values of σr and δr roughly 1% less than their sending counterparts.
　for a continuous query or monitoring task  the base station initially disseminates a query or monitoring plan  consisting of instructions to be carried out at runtime by individual nodes  into the network. since this dissemination occurs only once and its cost is amortized over the lifetime of the task  we ignore it in cost-based optimization. we also permit the plan to be updated at runtime  but infrequently so that it has overall negligible effect on the amortized energy consumption.
problem statement in each timestep  each node ui produces a new value for vi. our goal is to collect  at the base station  all values produced in every timestep. for simplicity of discussion  we assume that these values are discretized according to the precision requirement of the data collection task. for example  if we wish to collect temperature at precision of 1 degree celsius  each temperature reading can be discretized to an integer  providing an error bound of ＼1 degree celsius.
　note that our  values  are much more general than just the sensor readings; they can be quantities derived from readings or even a history of readings through model fitting. this generalization makes our techniques applicable in conjunction with sophisticated model-based compression schemes such as  1  1 . as a very simple example  suppose a temperature reading x can be predicted by a function f t p  with reasonable accuracy  where t is the time of the day and p is a model parameter that intuitively captures the  baseline  temperature for the day independent of t. instead of tracking x as a  value  in our approach  we can track two  values : p and the quantity e = x   f t p   predication error . model parameter p changes much less than x  and if the model is accurate  the e should remain 1 most of the time  with appropriate discretization . hence  monitoring p and e should be much cheaper than monitoring x  which can be reconstructed as f t p +e. our techniques then help with monitoring of p and e  by further exploiting the fact that they do not change often  and their values at neighboring nodes are spatially correlated -p's are naturally correlated in space  while some neighboring e's may change together due to movement of canopy shades.
1 a first cut: neighborhood
before delving into conch  we first present an algorithm called
neighborhood that serves two purposes. first  it introduces an idea for combining spatial and temporal suppression that will also be used by conch. second  by analyzing its shortcomings  we gain insights that lead us to the development of conch. we call the set of nodes within communication distance of ui the neighborhood of ui  denoted ni. note that ui （ nj implies uj （ ni  assuming communication is always bidirectional. the main idea of neighborhood is for each node ui to maintain dij  the difference in value between itself and each node uj （ ni.
　in every timestep  each ui broadcasts its value vinew to all nodes in ni only if its value has changed since the value viold from the previous timestep-as in temporal suppression. then  ui updates dij for each neighbor uj as follows. if ui receives a broadcast message from uj with updated value vjnew  ui updates dij to vinew   vjnew; if ui has not received any broadcast message from uj by the end of the timestep  ui infers that vj remains unchanged  and updates dij to dij viold +vinew. at the end of the timestep  for each dij that has been updated to a different value in this timestep  ui sends dij to the base station if i   j. as an optimization  these updates can be grouped into one message. in essence  dij is a spatial relationship
	u1 u1 u1 u1	u1 u1 u1 u1

   u u u u u u u u  a  1 1  b  1 1
figure 1: edge monitoring in neighborhood  a  vs. node/edge monitoring in conch  b .
suppressed temporally.
　in neighborhood  the base station knows the difference  dij  in value across every communication edge at all times. thus  neighborhood is an edge monitoring scheme. also  the value at the base station is always known  if the base station does not monitor any value  it simply monitors a  dummy  value of zero . from this value  it is easy to reconstruct all values in the network in one pass with a traversal of the network topology. during this traversal  suppose we have already reconstructed vi; then for each neighbor uj of ui  we can reconstruct vj as vi  dij. note this traversal is done completely at the base station with its knowledge of the network topology  without any communication to the real network.
　neighborhood improves over the suppression schemes discussed in section 1. once again  suppose two neighbors share the same value; if both values remain constant or rise in tandem  we can suppress reporting of their difference to the base station. moreover  even if two neighbors have different values that both change  as long as the difference in value remains constant across changes  we can suppress reporting.
shortcomings while neighborhoodcombines spatial and temporal suppression effectively  it still has important flaws. these are best seen with examples. say we start with a collection of 1 nodes all with the same value  vl  and in the next timestep  the 1 nodes above jump to a higher value  vh  as depicted in figure 1 a . a horizontal axis shows the split in values. the dotted lines indicate neighbor relationships between pairs of nodes. when the 1 nodes above rise in value  the value differences along all edges across the split  colored in black  change. for each such edge  neighborhood must update the base station. obviously  there are more updates than necessary. for instance  the base station should be able to infer the new value at node u1 from just one monitored edge  say e1  but the other edge  e1  is also reported. this problem is inherent in neighborhood because it monitors all communication edges  most of which are redundant; for example  d1 should always be the same as d1+d1. this redundancy causes lots of unnecessary update traffic  a problem that we shall address in conch with more judicious choice of edges to monitor. although there are lots of value changes in the network in this example  the overall effect can be described succinctly: all nodes above the axis move to vh. with conch  we will be able to use one short message to capture this overall effect.
　another shortcoming of neighborhood arises in its handling of  outlier  nodes whose values simply change haphazardly without any spatio-temporal correlation. suppose u1 turns out to be such a node in figure 1 a . neighborhood monitors all edges incident to u1  but v1 changes haphazardly  causing updates to all these edges in every timestep. conch will need to be able to identify such outliers and protect the rest of the monitoring plan from being affected by them.
1 min-cost conch
we now present conch  the main contribution of this paper. the crux of conch is that we can provide effective spatio-temporal suppression using a minimum spanning forest covering all nodes in the sensor network; the roots and edges of this forest correspond to monitored constraints. like neighborhood  conch uses edge monitoring to exploit spatio-temporal correlations in data  and relies on  chaining  of value differences along monitored edges to recover node values. the key observation here is  to be able to reconstruct all values in the network  we simply need to monitor all edges in a spanning tree  covering all nodes. for the purpose of reconstructing node values  any spanning tree suffices  but we should choose one that is the cheapest to monitor; intuitively  we prefer those edges connecting values that tend to change in tandem. a further improvement over neighborhood is that conch allows nodes to be directly monitored  using just temporal suppression . by relaxing the requirement that every value must be monitored in conjunction with a neighbor  conch makes it easier to separate uncorrelated areas of values and  in particular  to isolate outliers  as discussed at the end of section 1 . this flexibility implies a spanning forest of trees  whose roots are directly monitored.
　as an example of how conch works  consider the same scenario as in section 1. figure 1 b  illustrates how conch might choose to monitor this network. the spanning forest consists of one skinny tree  in fact just a single path  that connects the top 1 nodes with 1 left-most bottom ones  plus one singleton-tree with just u1. the first thing to notice versus neighborhood is that far fewer edges are monitored. the second is that when the top nodes change to vh  only one edge  e1  observes a change in value difference  so only one update message is sent to the base station. because none of the edges between black nodes were reported  we know all of those edges must still have the same value; if u1 now has value vh  then so do u1  u1  and u1. finally  note conch directly monitors the outlier  u1  and does not monitor any edges incident to u1  because there is no benefit  and only overhead cost  in monitoring u1 in conjunction with an uncorrelated value.
　this rest of this section starts with a formal description of a conch plan and discusses how the plan is carried out at runtime to perform lossless data collection  assuming no failure  from the sensor network  section 1 addresses failure . we then discuss how to obtain a min-cost conch plan  i.e.  one that is expected to spend the least amount of energy over time.
1 conch plan
suppose the sensor network consists of a set of nodes n and a set of communication edges e. formally  a conch plan is specified by a set of nodes nm   n  called the monitored nodes   a set of  undirected  edges em   e  called the monitored edges   and a reporter assignment function rep : em ★ n that maps each monitored edge eij （ em to one of ui and uj. we call rep e  the reporter of e  and the other node incident to e the updater of e. nothing prevents a node from serving either role on behalf of any of its incident edges  but each node serves only one role per edge.
　a valid conch plan satisfies the following requirements:  1  the base station is in nm;  1  every node u （ n is either in nm  directly monitored   or there exists a path from some node in nm to u  indirectly monitored   where every edge on this path is in em. this section focuses on minimal conch plans  valid plans for which it is impossible to remove some node from nm or some edge from em while still maintaining the validity of the plan. it is easy to see that we can construct a minimal conch plan from a spanning forest of the sensor network.
the potential savings of a minimal conch plan over neigh-
borhood can be seen in part by calculating the number of quantities monitored. a minimal conch plan  by definition  contains no more than n monitored quantities. in contrast  the number of edges monitored by neighborhood is n/1 multiplied by the average number of neighbors per node  which is likely much higher. the potential savings comes from the flexibility of requiring far fewer monitored quantities and then choosing them carefully.
　intuitively  conch monitors a network of constraints. each node in nm maintains a node constraint  reporting to the base station whenever its value changes. each edge in em is an edge constraint  jointly maintained by a reporter and an updater: the updater notifies the reporter whenever the updater's value changes  and the reporter notifies the base station whenever the value difference along the edge changes. the chaining of node and edge constraints allows the base station to recover all values in the network. we next describe the operation of conch in detail. start-up phase once a conch plan has been constructed at the base station  we disseminate it into the network. each node ui （ n builds two lists: eri  the list of edges for which ui is the reporter; and eui  the list of edges for which ui is the updater. at timestep 1  to initialize state  ui carries out the following steps. if eui is not empty  ui broadcasts its value vi to its neighbors  specifically for reporters of edges in eui . meanwhile  for each edge eij （ eri  ui listens for a broadcast message from uj  the updater of eij  containing its value vj; ui then computes the edge difference dij = vi   vj and records it. finally  ui sends a message to the base station containing all its recorded edge differences  together with its own value vi  if ui （ nm . at the end of initialization  the base station knows the values of all directly monitored nodes  as well as value differences along all monitored edges.
continual phase the continual phase is carried out by all nodes simultaneously at every timestep  without additional intervention by the base station. the frequency of timesteps is a user-controlled parameter. note the proper choice of this parameter depends on many factors beyond the scope of this paper  such as applicationspecific measures of information utility  and the amount of time required to finish all communication in each timestep; hence  we will not elaborate on this point further in this paper.
　at each timestep  each node ui obtains its new value vinew and compares it with the old value viold from the previous timestep. if the two differ  ui broadcasts vinew to the reporters of edges in eui  if eui 1=     and also sends it to the base station  if ui is directly monitored . for each edge eij （ eri  ui updates dij as follows. if ui receives a broadcast message from uj with updated value vjnew  ui updates dij to vinew   vjnew; if ui has not received a broadcast message from uj by the end of the timestep  ui infers that vj remains unchanged  and updates dij to dij   viold + vinew. at the end of the timestep  for each dij that has been updated to a different value in this timestep  ui sends dij to the base station. as an optimization  all updates from ui to the root can be grouped into one message. the base station receives update messages and uses them to keep its collection of monitored nodes values and edge differences up-to-date; if a quantity is not updated in a timestep  it is assumed to have remained the same since the previous timestep.
value reconstruction the base station can reconstruct the value of any node at any time. consider node ui. with a valid conch plan  one of the following must be true:  1  ui is directly monitored;
 1  there is a path from some directly monitored node uj1 to ui  and each edge on the path is monitored. in the first case  the base station should know the value at ui directly. in the second case  the base station knows the value at uj1 as well as the value differences dj1 dj1 ... djmi along the edges on the path to ui. clearly  the value of ui can be computed by
vj1     x djkjk+1    djmi.
1＋k m
note that if djk+1jk is tracked instead of djkjk+1  the latter is sim-
ply given by  djk+1jk.
　in fact  the base station can reconstruct all values efficiently in one pass using a topological sort of the constraint network starting from the directly monitored node values.
1 cost-based construction of conch plan
while any spanning forest can serve as a minimal and valid conch plan  it may not necessarily be the best choice in terms of minimizing energy cost. in particular  it is very important to make a clear distinction up front between the conch forest  for monitoring  and the routing tree of the sensor network rooted at the base station  for communication . in a routing tree the goal might be for messages to travel from a node to the base station in as few hops as possible. it is quite likely  however  that some edges in the routing tree connect nodes whose values happen to be uncorrelated; such edges would not be good candidates for monitoring. this observation is confirmed by experiments in section 1. it is also possible for a conch forest to be shaped in a way that would be horrible for the purpose of routing  such as the example in figure 1 b . there is no problem here: conch edges are not used for routing; updates are still sent to the base station following shortest paths  or using whatever protocol is supported by the underlying network layer.
　in this section  we discuss how to construct a conch plan that minimizes the energy cost of monitoring. intuitively  we want to monitor node values and edge differences that change less  or more precisely  that are cheaper to maintain. we solve the optimization problem in two steps. the first step constructs a min-cost spanning forest  with roots corresponding to monitored nodes and edges corresponding to monitored edges. here   cost  is defined as the expected energy cost of monitoring  which is calculated from statistics of past value changes and estimated change reporting costs  including communication reliability. the second steps takes the spanning forest as input  and further decides the roles of reporter and updater for each monitored edge. this time the optimization uses a more detailed communication model  which  for example  accounts for the possibility of combining multiple messages to save per-message overhead. we now discuss these two steps in more detail below.
step 1: constructing a min-cost conch forest we solve this optimization problem by reformulating it as a problem of finding a min-cost spanning tree  mst  of a graph  where the cost is given by the sum of edge weights in the tree. the resulting mst problem can be solved using standard techniques such as prim's or kruskal's algorithms .
　we start with a graph with all nodes n and communication edges e in the sensor network. we weigh each edge eij according to the estimated cost of reporting its changes to the base station. this estimate is given by freq dij  〜 dist eij   where freq dij  is the frequency with which dij changes  and dist eij  is the number of hops in the shortest path  in a routing tree  from eij to the base station. we can obtain dist eij  readily from the network topology; the exact distance is difficult to know outside the routing protocol  and may change over time  but an estimate based on a static topology will likely serve for the purpose of optimization. we defer the discussion of how to obtain freq dij  to later in this section. it is also possible to integrate reliability into weight calculation  which we do in section 1.
　for each node ui  we also add an  imaginary  edge connecting it to the base station  just for the purpose of solving this optimization problem   intuitively for capturing the possibility of monitoring ui directly. this imaginary edge is assigned a weight freq vi 〜 dist ui   where freq vi  is the frequency with which vi changes  and dist ui  is the distance from ui to the base station in hops.
　we then find the mst of the constructed graph using standard algorithms such as kruskal's in time o |e|log |e| . to convert the result mst into a conch forest  we simply make all nodes incident to the imaginary edges in the mst directly monitored nodes; all non-imaginary edges in the mst become monitored edges.
step 1: assigning updaters and reporters intuitively  a good assignment should consider the following optimization opportunities. consider a monitored edge eij. first  if vi changes less frequently than vj  then it might be better to make ui the updater  because it requires fewer broadcasts to track the edge difference than the other way around. second  if ui is closer to the base station than uj  then making ui the reporter might be better because of cheaper reporting. third  if ui is incident to more monitored edges than uj  then making ui the updater might be better because it can service more edges with just a single broadcast message. because of the complex interactions among these opportunities and role assignment  it is rather difficult to gauge the quality of heuristic role assignment. instead  we formulate optimal role assignment as a linear program  and we can guarantee that the assignment will be a factor of 1 within the optimum.
　to guide optimization  we again use past observations to approximate future behaviors. in particular  we use a sequence s of snapshots of the sensor network previously observed at consecutive timesteps  where each snapshot contains all node values in the same timestep. again  we defer the discussion of how to obtain s to later in this section. from s we can easily derive the following information: change i t  means that the value at ui changed in timestep t; change i j t  means that the value difference along eij changed in timestep t; freq u  is the total number of times that the value at node u changed in s; freq e  is the total number of times that the difference along edge e changed in s. as shorthand notation  let anc i j  denote the condition that ui is a node on the path from uj to the base station  including uj itself ; let inc j e  denote the condition that node uj is incident to edge e. given a conch forest with monitored nodes nm and monitored edges em  the mixed-integer program assigns the following variables: rei is set to 1 if ui is assigned as reporter for edge e  and 1 otherwise; xit is set to 1 if ui is used for transmitting a message to the base station in timestep t  and 1 otherwise; and yit is set to 1 if ui must broadcast an update message at timestep t  and 1 otherwise. the program minimizes:
 σs + σr pt pi xit +
 δs + δr pe（em pinc i e  dist i freq e rei +
subject to: σr + δr p σes（+emδsp pinct i ep ifreqyit + vi  1   rei 
 e （ em ui （ n uj （ n s.t. inc i e  inc j e  :
	rei + rej = 1	 1 
 t uj （ nm ui （ n s.t. change j t  anc i j  :
	xit − 1	 1 
 t e （ em ui （ n uj （ n s.t. change e t  inc j e  anc i j  : xit − rej	 1 
 t e （ em uj （ n : change j t  inc j e  :
	yjt − 1   rej	 1 
line  1  dictates that each edge has one and only one reporter. lines  1  and  1  enforce that for each node that is directly monitored or a reporter  all it ancestors  and itself  must communicate in timesteps when some of its monitored quantities change. line  1  enforces that each node acting as an updater must broadcast in timesteps when its value changes. the objective function captures parts of the energy cost that are affected by reporter assignment  which do not include the per-byte cost of updating directly monitored values . the idea is to find a reporter assignment that works best for the sample sequence s; if the past is a reasonable predication of the future  we would expect this assignment to work well.
　despite the use of boolean variables  we can solve the above program as a linear one  and simply round fractional solutions to the nearest integer. because of the linear objective function  the solution is guaranteed to be within a factor 1 of the optimum; in practice  the solution tends to be much better than that.
discussion collecting history and statistics for conch optimization is straightforward and in a sense comes  for free  because the very goal of the lossless data collection is to record all historical sensor values. thus  the sample sequence s used in the second optimization step discussed above can be taken directly from the archived history. statistics such as freq  used also in the first optimization step  can be easily derived from s. if no previous history is available  i.e.  conch is used on a new deployment  we can simply use any reasonable spanning forest  e.g.  the routing tree  as a  tentative  plan. we run this tentative plan for a number of timesteps  e.g.  for several days   collect the data  and then use it to find a better conch plan. for the purpose of optimization  we do not need the tentative plan to specifically monitor or sample any candidate edge-any valid conch plan would give us the change characteristics of all nodes and all edges.
　our description of conch assumes central planning at the base station  with periodic re-optimization. we can also make conch more dynamic and more responsive to evolving data characteristics  by enabling nodes to locally adjust portions of the plan. for example  an edge reporter can easily swap its role with the updater by keeping a time-weighted measure of how often the updater sends updates  together with a mirrored measure of how often its own value changes. if its own value is changing less frequently than the updater's  then it can initiate a swapping of roles.
　as another example  a reporter can locally evaluate how much an edge benefits from spatial suppression. it again maintains statistics  this time on the frequency with which both nodes change values together and the frequency with which at least one node changes. the ratio between the two is a measure of the value of monitoring the edge. if the nodes rarely change together  then the spatial suppression is not beneficial. the reporter may then sever the edge. in that case  both nodes must be directly monitored  since neither knows which is connected to a directly monitored node .
1 failure-resilient conch
reliability is a major concern for wireless sensor networks because of high failure rates for both nodes and edges. the main obstacle in conch is that it depends heavily on suppression for energy savings. it is difficult to differentiate whether the absence of a message is due to suppression or failure. because we assume the former  when failure occurs  if a message would otherwise be sent  we will mis-assign a node value or edge difference; constraint chaining may cause such errors to propagate to multiple derived values.
　there are two types of failures: permanent node failures  and transient failures causing message loss. to handle permanent node failures  we use the standard heartbeat technique. if a node has suppressed updates and reporting for some number of consecutive timesteps  it sends a heartbeat. if a node fails  neighboring nodes will detect the absence of either standard or heartbeat messages  and report the failure to the base station. in addition  if the edge between a neighbor and the failed node was a monitored edge  the neighbor severs the edge  reports its own value to the base station  and turns itself into a directly monitored node. these local adjustments to the conch plan ensure the validity of the plan is maintained  i.e.  values at all nodes besides the failed one can still be correctly monitored . the base station has the option of installing a new conch plan if the result of local adjustments is inefficient.
　in the remainder of this section  we focus on the message loss problem due to transient failures. a communication edge may temporarily fail for a number of reasons  such as when a moving obstacle temporarily blocks the line of sight between two nodes. it does not help to exclude such an edge from a conch plan  since we can often expect the edge to be functional again by the time we could make adjustments. instead  we propose a framework for coping with transient failures  which supports reconstruction of values despite missing updates and removal of data inconsistencies caused by missing updates.
overview of strategy conch relies on three types of messages for monitoring:  1  those reporting node constraint violations  from a monitored node to the root   1  those reporting edge constraint violations  from the reporter of a monitored edge to the root  and  1  those updating node values used in maintaining edge constraints  from a monitored edge's updater to its reporter. transient failures of messages of the third type are relatively cheaper to handle because these messages are single-hop. here  we use a simple approach that requires an acknowledgment from reporter back to updater; the updater retransmits its message until an acknowledgment is received.
　for failures of messages of the first two types  however  this simple approach would be too expensive because these messages may involve many hops. not only are multi-hop messages and acknowledgments more costly and prone to higher failure rates  the sequential nature of the acknowledgment protocol also implies communication within each timestep can take much longer to finish  which may lead to unacceptably low reporting frequency. therefore  for failures of messages reporting constraint violations  we propose an alternative strategy that does not require the use of acknowledgments. the remainder of this section discusses this strategy in detail.
reintroducing redundancy the key to maintaining conch's viability is to build redundancy back into conch. in section 1  we discussed how to build minimal conch plans to minimize energy consumption. the highly non-redundant nature of such plans  however  makes them extremely susceptible to failures. there are a number of ways to reintroduce redundancy into conch plans  and we describe one here.
　the idea is to build multiple  different spanning forests over the network. if an edge is used in any of the spanning forests  it is monitored; also  if a node is a root in any of the spanning forests  it is directly monitored. in building these forests  our intuition is to have less reliable edges participate in fewer forests  so the impact of their failures will be minimal. on the other hand  we want more reliable edges to participate in most or all forests  in order to avoid adding more monitored edges where we are confident we can do without. this behavior can be implemented by integrating failure probabilities into edge weights prior to forest construction using mst  cf. section 1 . assume a message for an edge eij gets lost with probability fail eij . suppose cnt eij  is the number of forests for which eij has already been chosen. we assign eij the following weight in constructing a new forest: freq dij 〜dist eij 〜 1+fail eij  cnt eij +1. this weight captures two intuitions. first  less reliable edges have higher weights. second  the weights of less reliable edges rise for each subsequent

figure 1: conflicting solutions for node c.
forest in which they are chosen. in contrast  edges with perfect reliability  fail eij  = 1  never rise in weight.
　this approach assumes independent failure probabilities  and then adds redundancy as needed. other approaches are possible. for example  with knowledge of joint failure probabilities  we might choose subsets of redundant edges that are unlikely to fail at the same time. if failure is geographically correlated  this approach implies choosing scattered edges. we plan to fully explore construction of non-minimal conch plans in future work.
reconstructing values simply incorporating redundancy does not allow correct reconstruction of values in the presence of failures. in fact  while following redundant constraint chains to a particular node  if an error has occurred in one of these chains  we will likely generate conflicting values. consider the network fragment in figure 1. it contains one redundant edge  producing two independent paths from a directly monitored node r to another node c in question. the edges are labeled with their last reported differences  one of which changes from +1 to +1 in the current timestep. in calculating c's value  we derive both r+1 and r+1. while we know the correct current value of ebc  we have no way of discerning which of the other edges has changed but failed to report and  therefore  no way of choosing the correct solution. it is crucial that we be able to step from simply having redundancy to utilizing it to generate a solution. one option is to generate all possible consistent solutions with associated probabilities. doing that is difficult  however  because the solution space can be huge.
　instead  we frame the problem as an optimization that finds the assignment of node values with the maximum likelihood  conditioned on all reports received by the root in this timestep  the previous values from the last timestep  as well as any prior knowledge of value change probabilities and message failure rates. to make the optimization problem computationally feasible  however  we have to make some simplifying assumptions. while these assumptions obviously may not hold in practice  they may still lead us to a high-quality  i.e.  reasonably likely  assignment of the node values. specifically  we assume each monitored quantity changes independently with a fixed probability  ci for node ui and cij for edge eij . we also assume messages reporting constraint violations fail independently with a fixed probability  fi for ui reports and fij for eij reports .
　with these assumptions  we can formulate the optimization problem as a mixed-integer program. intuitively  the program attempts to set node values as consistently as possible  favoring inconsistencies in constraints that change more and for which reports fail more. in the following  let nr denote the set of monitored nodes for which the root has received report  and let er denote the set of monitored edges for which the root has received report. the program uses the following constants: viold denotes the previous  default  value at node ui; doldij = viold   vjold denotes the previously known  default  difference along edge eij; vir denotes the new value for ui  if received ; drij denotes the new edge difference value for eij  if received . the program sets variables vi  for each node  and dij  for each monitored edge   representing its belief of the new values for ui and eij  respectively. boolean variables xi  for each monitored node without a report  and yij  for each monitored edge without a report  are set appropriately by the program to indicate whether the respective quantities have changed. the program maximizes:
 
subject to:
 ui （ nm   nr :  xi  max  − |vi   viold| 1  eij （ em   er :  yij  max  − |dij   dijold| 1  eij （ em : vi + dij = vj 1  ui （ nr : vi = vir 1  eij （ er : dij = drij 1 lines  1  and  1   where max is a large number chosen to be greater than all possible values on the right of the inequalities  appropriately sets the boolean variables. if the calculated node value does not equal its previous value  the corresponding x variable must be set to 1; otherwise  it will be set to 1. the same applies to edges. line  1  encodes constraint chaining itself; to build a consistent solution  the difference in calculated node values across an edge constraint must be equal to the calculated value assigned to the edge by the program. lines  1  and  1  state that the calculated value for any node or edge reported in the current timestep must be set to the reported value. therefore  any solution that violated these would certainly be incorrect. further  there must exist a feasible assignment with all of these variables fixed. the correctness of reported node values is obvious; the correctness of reported edge values is established by the acknowledgment protocol for updater-reporter communication discussed earlier. this protocol guarantees the reporter sees the correct value at the updater  so it never reports an erroneous edge difference.
　the maximization goal  which may appear cryptic at the first glance  effectively maximizes the logarithm of the probability of the solution conditional on the received reports. given the choice between a reliable  infrequently changing constraint and an unreliable  frequently changing constraint  the program prefers to believe that the latter constraint was violated but the report was lost. note that independence assumption is needed here to express the goal as a linear objective; otherwise  the objective function will be much more complicated.
　the maximum-likelihood approach does have a potential problem. although it guarantees its solution is the most likely one  the probability that it reflects reality may still be very low because there are so many possibilities. we might have more directed questions  such as what is the probability a particular node has value greater than some amount. the mixed-integer program cannot answer these. we plan to explore alternative approaches that do support these types of questions.
1 experimental evaluation
we now evaluate performance of the presented query processing algorithms: temporal suppression  spatial suppression  neighborhood  and conch. spatial is an implementation of   with the global ordering concession mentioned in section 1. for testbed  we use our own simulator of a network of crossbow mica1 motes   which uses a generic mac-layer protocol  and models communication as explained in section 1. the network resides in a rectangular grid. each grid point represents a square meter and produces some value at each timestep. nodes are randomly placed on grid points  and takes the values at these grid points as their readings. node radio range is set at 1 meters. because we are interested in continuous queries  we ignore start-up costs  which are admittedly higher for conch than simpler query plans  because such costs are amortized over a large number of subsequent timesteps. hence  in the following  we focus on the average energy  mj  expended by communication per timestep.
　we discretize node values in  tiers.  tier resolution is set to 1 for all experiments  i.e.  values in  1  fall into tier 1  values in  1  1  fall into tier 1  and so on. node density is set to 1 node per 1 unless otherwise noted. each test simulates a different combination of value change and network scenarios. to prime conch  we first run each test for a short amount of time with a default spanning forest  and feed the results to conch forest construction. we then evaluate performance using the output forest for the remainder of the test.
density of sensor nodes we use a simple density experiment to demonstrate the redundancy problem in neighborhood. in this scenario  all node values are equal throughout  all rising by a single tier every timestep. the number of nodes is fixed  but the area of the network  and therefore density  varies across runs. the results are shown in figure 1 with a log-scaled y-axis. most striking in this graph is that neighborhood performs far worse than other algorithms  and its energy consumption increases with density. the reason is that  as density increases  the number of pairs of nodes within communication distance of each other increases. therefore  average neighborhood size increases  and so does the number of monitored edge constraints. in each timestep  because all nodes change tiers  each node broadcasts its value change  and messages are sent across all edges. while the total number of broadcasts is unaffected by density  the amount of listening that must occur is equal to the number of edges. listening  therefore  accounts for the increasing energy consumption as density increases. note since all node values are equal in this experiment  none are ever sent to the root. otherwise  increasing density might also cause reporting nodes to send increasingly larger reports listing more neighbor values. due to neighborhood's poor performance  we omit it from future graphs to avoid diluting more interesting results. in contrast  conch  which maintains the same number of constraints regardless of density  is unaffected in this experiment. in their reports.
　another interesting feature in the density graph is that spatial suppression performs quite well  and far better than it does in subsequent experiments. since all nodes have the same tier value  its suppression opportunities are maximized. as we increase density by shrinking area  there are fewer  coverage  nodes  whose reports allows neighboring nodes to suppress  because each coverage node has more neighbors.
consistently rising values this experiment is a simple case where results are predictable. nodes start with values directly proportional to their distance from the center of the network and all rise at the same rate of one tier every timestep. we vary the size of the range between minimum and maximum node values  and thus the number of tiers; the smaller the range  the fewer the number of tiers  and the more nodes in the same tier. the results are shown in figure 1.
　temporal suppression performs poorly and is unaffected by the number of tiers. since nodes always change tiers  all nodes report their values every timestep. spatial outperforms temporal with only a few tiers. when the number of tiers is low  most nodes suppress themselves because of high likelihood of overhearing a neighbor in the same tier. conch outperforms both spatial and temporal. the only energy spent is on updaters broadcasting across their edges to reporters. since the differences across all monitored edges remain the same from timestep to timestep  the reporters always suppress and no messages are sent to the base station. finally  we examine neighborhood's performance  not shown in the figure . like
conch  it suppresses all reports  but still performs poorly despite this ideal setup. its excessive edge maintenance consumes as much energy  at all numbers of tiers  as spatial does at 1 tiers.
　varying the number of tiers does affect spatial's performance. increasing the number of tiers increases the number of unique tier values that any particular node overhears  which in turn decreases the probability that the overheard values average to the node's own tier and thus  the probability of suppression. since conch utilizes spatial suppression  one might expect the number of tiers to impact it as well. remember  though  that conch monitors the differences between nodes. as the number of tiers increases  more neighboring nodes do have differing tier values  but the differences between them remain the same in every timestep  so still no reports are sent.
　conch completely leverages spatial suppression in this case. all edges have frequencies of change of zero  while nodes change every timestep. therefore  only edge constraints  and no node constraints  are monitored. in this case  the spanning forest consists of a single spanning tree.
outliers the next two scenarios examine the impact of outliers. non-outlier nodes all share the same behavior. in case 1 nonoutliers remain in the same tier over all timesteps. in case 1 they change tiers every timestep. in both cases  with some probability  a node is chosen to be an outlier  whose value changes drastically and unpredictably each timestep. we vary the probability with which nodes are made outliers across runs. performance for case 1 is shown in figure 1  while case 1 is shown in figure 1.
　the key difference between the cases is that temporal suppression shifts from performing as well as conch in case 1  where both send more energy as outlier probability increases  to performing much worse in case 1  where temporal suppression is unaffected by probability. in case 1  temporal reports every node  whether outlier or not  every timestep  so is not affected by outlier probability. in case 1  non-outliers never change tiers  so temporal only reports the outliers  exhibiting ideal behavior. conch reacts to case 1 by monitoring the outliers with node constraints  matching the same fundamental behavior of temporal. in case 1  while temporal suffers  conch continues to perform well  although with the added expense of edge updater messages  since nodes change tiers every timestep . conch achieves good performance by continuing to leverage spatial suppression by monitoring the edges connecting non-outliers as edge constraints  while monitoring outliers as node constraints.
wavefront wavefront simulates waves passing as vertical lines from left to right over the network. the frequency is such that as soon as one wave reaches the right edge of the network  another starts. wave speed determines how often waves occur. a node's value is tied to the distance covered by the recurring wavefront since it last passed over the node; a node is highest when the wave is over it and lowest just before. we set value range such that there are 1 possible tiers. we vary wave speed across runs. the faster the wave  the more ground it covers between timesteps  so more nodes change tiers each timestep.
　results are shown in figure 1. as wave speed increases  as expected  temporal consumes more energy. conch  on the other hand  continues to consume a low amount of energy; no matter how quickly the wave moves  the number of reported edges is tied to the number of borders  the vertical lines dividing the network into different tiers  1 in this case . only two types of edges detect changes to their difference calculations  and must report in a particular timestep: those that now cross a border but previously did not 

and those that now do not cross a border  but previously did. spatial  because of its lack of memory of prior state  is unaffected by wave speed  and performs worse than the other algorithms simply because too many coverage nodes and nodes along borders must report.
　we also include the algorithm conch-comm  which is conch but with edge scores based solely on the dist  communication distance to base station  function  ignoring freq. conch greatly outperforms conch-comm  and by an increasing margin as wave speed increases. the key observation is  since the wave is a vertical line  the best edges to monitor are those connecting nodes with small difference in x-coordinates. monitoring these edges decreases the likelihood the edge intersects a tier border at a given time. if no intersection ever occurs  the edge's nodes always change tiers simultaneously  so the edge constraint is never violated  and no reporting is ever done. the slower the wave moves  the fewer edges chosen by conch-comm are intersected each timestep. as the wave speeds up  the more likely intersections become  up to a plateau. conch  on the other hand  expressly seeks out vertical or near-vertical edges that are unlikely to be intersected at any time. this shows the importance of constructing a minimal spanning forest with conch scoring  rather than using a naively scored one or simply the routing tree.
　the wavefront example's favoritism for vertical conch edges provides an opportunity to differentiate the routing tree from the conch forest. we depict these for a mini-setup of the wavefront scenario with 1 nodes in figure 1. the routing tree connects all nodes to the root  in the lower left  in as few hops as possible. there is no preference for edge direction  and we see edges at a variety of angles. the conch forest contains mostly vertical or near-vertical edges  with only a few horizontal edges  necessary to connect all nodes.
heat transfer this scenario uses a model of heat transfer borrowed from . an event raises the temperature at some grid point. in subsequent timesteps  the heat disperses outward to neighboring points  until the network eventually reaches equilibrium. in each epoch  t i j   the temperature at grid point  i j   is updated using the temperatures from the previous epoch according to the following calculation:
t i j  ○t i j  + a t i + 1 j  + t i   1 j +
t i j + 1  + t i j   1    1t i j  .
here  a ＋ 1 is a dispersion factor. we experiment with heat transfer by varying the length of a timestep  time between queries  as measured by the number of epochs. the more time lapses  the more heat transfers without detection  and the more change occurs from the previous sensor reading.
　we find spatial suppression  in addition to neighborhood  is dramatically more expensive than temporal and conch. to avoid diluting the most interesting result  we plot only these last two in figure 1. as expected  both algorithms spend more energy the more time lapses between queries. when the change between consecutive timesteps is small  they perform similarly. with longer time between queries  changes accumulate more. the cost of temporal increases at a much higher rate because more nodes report  but conch increases slowly. as in the wavefront scenario  conch prefers to monitor edges whose nodes are affected by heat transfer in the same way and at the same time. in this scenario  it favors edges between nodes that are equidistant from the heat source. both algorithms' slopes of increasing consumption begin to level out when query interval is very long  because the number of nodes exhibiting change each query update increases slowly by this point. experiments involving failures
	figure 1: routing vs. conch trees.	figure 1: random walk with failures.we next evaluate the approach to handling failures presented in section 1 with a 1-node network. our goal is to test the viability of the approach for recovering a reasonably accurate solution in the presence of failures. to that end  we build a simple conch tree containing just a single tree root  and incorporate redundancy by adding additional edges to monitor at random. the number of additional edges is chosen as a percentage of the number of possible remaining edges that can be monitored. therefore  1% leaves the conch tree as is  while 1% turns the conch tree into neigh-
borhood  with all possible edges monitored. as we will see from experiment results  the effectiveness of low-percentage redundancy points to the overkill in neighborhood's redundancy. we generate failures as follows. given a failure rate range  say 1%-1%  we pick a rate for each monitored quantity at random from this range  and subject reporting messages for this quantity to this failure rate.
random walk the first reliability experiment is a random walk scenario with density set to 1 node per 1. all grid points begin with equal values. at each timestep  the new value at a grid point is drawn from a normal distribution centered at the previous value at this grid point. there is no spatial correlation among values in this scenario  as in wavefront or heat transfer  and so conch does not provide great benefit. we mainly use this scenario to produce a test where many reports are needed to produce the correct solution  rather than just few ones. if only one report is needed in a round  for example  we either get 1% or 1% accuracy for the round  depending on if that report is successfully transmitted. with random walk  we see a smooth interplay between failure rates  redundancy  and solution error. we graph failure rate ranges versus average error  percentage of node values that are not correctly assigned by our approach per timestep   and plot results for a series of conch plans  each built with differing amounts of redundancy. we run the random walks for 1 timesteps  starting after an initial timestep when all constraints try to report. results are shown in figure 1  and depict two major trends: first  the higher the failure rate  the higher the error; second  the higher the redundancy rate  the lower the error.
heat transfer we next return to the heat transfer scenario to see the impact of redundancy under conditions favorable to conch. density is set higher at 1 node per 1. in the base case of neither failure nor redundancy  few reports are sent to derive all node values. we expect  then  to be  lucky  some timesteps and receive all of those reports  and  unlucky  other timesteps  where even one failed report results in many miscalculated values. to investigate this phenomenon  we make two plots. figure 1 shows performance of different redundancy levels for increasing failure rates. figure 1 shows the cumulative distribution of error rate in each timestep  query rounds  during a run  for a number of redundancy levels. that is  for a particular error percentage e on the xaxis  the y-coordinate of a point is the percentage of query rounds with error rate no more than e. failure rate for figure 1 is fixed in the range 1%-1%. the cumulative distribution shows that the lucky/unlucky phenomenon occurs at low redundancy levels  with some rounds at low and some at high error levels. this phenomenon is pronounced at low redundancy levels by constraints that change infrequently  which our approach tends to trust  but may have failed at some point. in subsequent rounds  until the constraint changes again  the reported value continues to be incorrect  with few chances to counter it. the phenomenon diminishes at higher redundancy levels  where error rate is low for all rounds.
　finally  we examine the cost of redundancy. additional redundancy means more constraints potentially report. in contrast  failure means less reports reach the root. using the heat transfer scenario  figure 1 plots average number of constraint-update reports received at the root versus average error rate. the varying number of reports is the result of running conch at different redundancy levels  labeled on the plot. failure rate is fixed at 1%-1%. as expected  as redundancy level increases  the number of reports increases  and error decreases. temporal suppression appears as a point. for temporal  we assume that nodes fail to report with 1% probability. in that case  given a 1-node network  1 nodes report and average error is 1%. conch is immediately more accurate than temporal with far fewer reports. the 1%-failure  and 1%-redundancy  case is drawn as a vertical line  which serves as a base case for comparison. at 1% redundancy conch sends fewer reports than the base case but  of course  with a sacrifice in accuracy. as the number of reports meets and exceeds the baseline  accuracy improves. the number of reports necessary to push error near 1% is quite a bit higher than the base case  demonstrating redundancy is not close to free. this finding serves as motivation to do a thorough investigation of what to monitor redundantly in order to push error suitably low  while at the same time minimizing the overhead.
　in the scenarios we have tested  failure rates are assigned in a fairly tight range. in practice  we might expect reports for some constraints to fail with high likelihood  due to location of the node  for example  and others to be very reliable. if so  it should be easier for our integer-program approach to decide which constraints to dismiss when resolving conflicts. more thorough evaluation is needed to verify this conjecture.
　in summary  our experiments involving failures show that we can effectively leverage the extra information provided by redundancy to set node values with some accuracy. nevertheless  we pay a price for redundancy by monitoring more constraints. further  as failure rate increases  our ability to recover values accurately decreases. therefore  redundancy is a viable technique for coping with failures  but more work is needed in this area.
1 conclusion and future work
we have investigated using temporal and spatial suppression for the problem of continuously collecting all readings from a sensor network. our suppression techniques can benefit many monitoring scenarios  where change is slow or predictable  and data is spatially correlated. we develop an effective monitoring policy combining both suppression types. we present neighborhood as a first attempt at using this policy  but show its redundancy hampers its usefulness. progressing from there  minimal conch uses a spanning forest that monitors the minimum number of constraints possible  solving the redundancy problem. we present techniques for building a spanning forest with minimal monitoring costs. then  to cope with failures  we add redundancy back into conch  and develop a method for correctly interpreting results affected by failure  but bolstered by redundancy. our experimental results show conch indeed performs as well as or outperforms other algorithms in most cases  degrading gracefully to temporal suppression in specific cases where edge monitoring has no advantage. the results also show that it is feasible to use redundancy to compensate for failure and recover node values with reasonable accuracy.
　in the future we plan on advancing conch in two directions. the first is to maintain constraints of increasing complexity  involving more larger spatial models that move beyond pair-wise monitoring. the second is to further improve our failure-handling strategy  by developing better techniques for selectively introducing redundancy to maximize its benefit  and for reasoning with data containing uncertainty that arises from failures.
