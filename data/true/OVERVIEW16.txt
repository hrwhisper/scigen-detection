1.1 documents
the document set of a test collection should be a sample of the kinds of texts that will be encountered in the operational setting of interest. it is important that the document set reflect the diversity of subject matter  word choice  literary styles  document formats  etc. of the operational setting for the retrieval results to be representative of the performance in the real task. frequently  this means the document set must be large. the initial trec test collections contain 1 to 1 gigabytes of text and 1 to 1 1 documents. while the document sets used in various tracks throughout the years have been smaller and larger depending on the needs of the track and the availability of data  the general trend has been toward ever-larger document sets to enhance the realism of the evaluation tasks. similarly  the initial trec document sets consisted mostly of newspaper or newswire articles  but later document sets have included a much broader spectrum of
 num  number: 1
 title  mutual funds
 desc  description: blogs about mutual funds performance and trends.  narr  narrative: ratings from other known sources  morningstar  or relative to key performance indicators  kpi  such as inflation  currency markets and domestic and international vertical market outlooks. news about mutual funds  mutual fund managers and investment companies. specific recommendations should have supporting evidence or facts linked from known news or corporate sources.  not investment spam or pure  uninformed conjecture. 
figure 1: a sample trec 1 topic from the blog track feed task.
document types  such as recordings of speech  web pages  scientific documents  blog posts  email messages  and business documents . each document is assigned an unique identifier called the docno. for most document sets  high-level structures within a document are tagged using a mark-up language such as sgml or html. in keeping with the spirit of realism  the text is kept as close to the original as possible.
1.1 topics
trec distinguishes between a statement of information need  the topic  and the data structure that is actually given to a retrieval system  the query . the trec test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. what is now considered the  standard  format of a trec topic statement-a topic id  a title  a description  and a narrative-was established in trec-1  1 . but topic formats vary in support of the task. the spam track has no topic statement at all  for example  and the topic statements used in the legal track contain much more information as might be available from a negotiated request to produce. an example topic taken from this year's blog track feed task is shown in figure 1.
   the different parts of the traditional topic statements allow researchers to investigate the effect of different query lengths on retrieval performance. the description   desc   field is generally a one sentence description of the topic area  while the narrative   narr   gives a concise description of what makes a document relevant. the  title  field has served different purposes in different years. in trecs 1 the field is simply a name given to the topic. in later ad hoc collections  ad hoc topics 1 and following   the field consists of up to three words that best describe the topic. for some of the test collections where topics were suggested by queries taken from web search engine logs  the title field contains the original query  sometimes modified to correct spelling or similar errors .
   participants are free to use any method they wish to create queries from the topic statements. trec distinguishes among two major categories of query construction techniques  automatic methods and manual methods. an automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. the definition of manual query construction methods is very broad  ranging from simple tweaks to an automatically derived query  through manual construction of an initial query  to multiple query reformulations based on the document sets retrieved. since these methods require radically different amounts of  human  effort  care must be taken when comparing manual results to ensure that the runs are truly comparable.
   trec topics are generally constructed specifically for the task they are to be used in. when outside resources such as web search engine logs are used as a source of topics the sample selected for inclusion in the test set is vetted to insure there is a reasonable match with the document set  i.e.  neither too many nor too few relevant documents . topics developed at nist are created by the nist assessors  the set of people hired to both create topics and make relevance judgments. most of the nist assessors are retired intelligence analysts. the assessors receive track-specific training by nist staff for both topic development and relevance assessment.
1.1 relevance judgments
the relevance judgments are what turns a set of documents and topics into a test collection. given a set of relevance judgments  the ad hoc retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. trec usually uses binary relevance judgments-either a document is relevant to the topic or it is not. to define relevance for the assessors  the assessors are told to assume that they are writing a report on the subject of the topic statement. if they would use any information contained in the document in the report  then the  entire  document should be marked relevant  otherwise it should be marked irrelevant. the assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.
   relevance is inherently subjective. relevance judgments are known to differ across judges and for the same judge at different times . furthermore  a set of static  binary relevance judgments makes no provision for the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. despite the idiosyncratic nature of relevance  test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments .
   the relevance judgments in early retrieval test collections were complete. that is  a relevance decision was made for every document in the collection for every topic. the size of the trec document sets makes complete judgments infeasible. for example  with one million documents and assuming one judgment every 1 seconds  which is very fast   it would take approximately 1 hours to judge a single topic. thus by necessity trec collections are created by judging only a subset of the document collection for each topic and then estimating the effectiveness of retrieval results from the judged sample.
   the technique most often used in trec for selecting the sample of documents for the human assessor to judge is pooling . in pooling  the top results from a set of runs are combined to form the pool and only those documents in the pool are judged. runs are subsequently evaluated assuming that all unpooled  and hence unjudged  documents are not relevant. in more detail  the trec pooling process proceeds as follows. when participants submit their retrieval runs to nist  they rank their runs in the order they prefer them to be judged. nist chooses a number of runs to be merged into the pools  and selects that many runs from each participant respecting the preferred ordering. for each selected run  the top x  frequently x = 1  documents per topic are added to the topics' pools. many documents are retrieved in the top x for more than one run  so the pools are generally much smaller than the theoretical maximum of x ¡Á the-number-of-selected-runs documents  usually about 1 the maximum size .
   the critical factor in pooling is that unjudged documents are assumed to be not relevant when computing traditional evaluation scores. this treatment is a direct result of the original premise of pooling: that by taking top-ranked documents from sufficiently many  diverse retrieval runs  the pool will contain the vast majority of the relevant documents in the document set. if this is true  then the resulting relevance judgment sets will be  essentially complete   and the evaluation scores computed using the judgments will be very close to the scores that would have been computed had complete judgments been available.
   various studies have examined the validity of pooling's premise in practice. harman  and zobel  independently showed that early trec collections in fact had unjudged documents that would have been judged relevant had they been in the pools. but  importantly  the distribution of those  missing  relevant documents was highly skewed by topic  a topic that had lots of known relevant documents had more missing relevant   and uniform across runs. zobel demonstrated that these  approximately complete  judgments produced by pooling were sufficient to fairly compare retrieval runs. using the leave-out-uniques  lou  test  he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. for the trec-1 ad hoc collection  he found that using the unique relevant documents increased a run's 1 point average precision score by an average of 1 %. the maximum increase for any run was 1 %. the average increase for the trec-1 ad hoc collection was somewhat higher at 1 %.
   as document sets continue to grow  the proportion of documents contained in standard-sized pools shrinks. at some point  pooling's premise must become invalid. the test collection created in the robust and hard tracks in trec 1 showed that this point is not at some absolute pool size  but rather when pools are shallow relative to the number of documents in the collection . with shallow pools  the sheer number of documents of a certain type fill up the pools to the exclusion of other types of documents. this produces judgments sets that are biased against runs that retrieve the less popular document type  resulting in an invalid evaluation.
   several recent trec tracks have investigated new ways of sampling from very large documents sets to obtain judgment sets that support fair evaluations. the primary goal of the terabyte track that was part of trecs 1 was to investigate new pooling strategies to build reusable  fair collections at a reasonable cost despite collection size. the new million query track is a successor to the terabyte track in that it has the same goal  but a different approach. the goal in the million query track is to test the hypothesis that a test collection containing very many topics  each of which has a modest number of well-chosen documents judged for it  will be an adequate tool for comparing retrieval techniques. the legal track has used a different sampling strategy still to address the challenging problem of comparing recall-oriented  see below  searches of large document sets for both ranked and unranked result sets.
1 evaluation
retrieval runs on a test collection can be evaluated in a number of ways. in trec  ad hoc tasks are evaluated using the treceval package written by chris buckley of sabir research . this package reports about 1 different numbers for a run  including recall and precision at various cut-off levels plus singlevalued summary measures that are derived from recall and precision. precision is the proportion of retrieved documents that are relevant  number-retrieved-and-relevant/number-retrieved   while recall is the proportion of relevant documents that are retrieved  number-retrieved-and-relevant/number-relevant . a cut-off level is a rank that defines the retrieved set; for example  a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. the treceval program reports the scores as averages over the set of topics where each topic is equally weighted.  an alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important. 
   precision reaches its maximal value of 1 when only relevant documents are retrieved  and recall reaches its maximal value  also 1  when all the relevant documents are retrieved. note  however  that these theoretical maximum values are not obtainable as an average over a set of topics at a single cut-off level because different topics have different numbers of relevant documents. for example  a topic that has fewer than ten relevant documents will have a precision score at ten documents retrieved less than 1 regardless of how the documents are ranked. similarly  a topic with more than ten relevant documents must have a recall score at ten documents retrieved less than 1. for a single topic  recall and precision at a common cut-off level reflect the same information  namely the number of relevant documents retrieved. at varying cut-off levels  recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.
   of all the numbers reported by treceval  the interpolated recall-precision curve and mean average precision  non-interpolated  are the most commonly used measures to describe trec retrieval results. a recall-precision curve plots precision as a function of recall. since the actual recall values obtained for a topic depend on the number of relevant documents  the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. the particular interpolation method used is given in appendix a  which also defines many of the other evaluation measures reported by treceval. recallprecision graphs show the behavior of a retrieval run over the entire recall spectrum.
   mean average precision  map  is the single-valued summary measure used when an entire graph is too cumbersome. the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . the mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. the average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents  and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later.
   the measures described above are traditional retrieval evaluation measures that assume  relatively  complete judgments. as concerns about traditional pooling arose  new measures and new techniques for estimating existing measures given a particular judgment sampling strategy have been investigated. bpref is a measure that explicitly ignores unjudged documents in the retrieved sets  and thus it can be used when judgments are known to be far from complete . it is defined as the inverse of the fraction of judged irrelevant documents that are retrieved before relevant ones. the sampling strategies used in the million query and legal tracks have corresponding methods for estimating the value of evaluation measures based on the sampled documents. the track overview paper gives the details of the evaluation methodology used in that track.
1 trec 1 tracks
trec's track structure began in trec-1  1 . the tracks serve several purposes. first  tracks act as incubators for new research areas: the first running of a track often defines what the problem really is  and a track creates the necessary infrastructure  test collections  evaluation methodology  etc.  to support research on its task. the tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. finally  the tracks make trec attractive to a broader community by providing tasks that match the research interests of more groups.
   table 1 lists the different tracks that were in each trec  the number of groups that submitted runs to that track  and the total number of groups that participated in each trec. the tasks within the tracks offered for a given trec have diverged as trec has progressed. this has helped fuel the growth in the number of participants  but has also created a smaller common base of experience among participants since each participant tends to submit runs to a smaller percentage of the tracks.
   this section describes the tasks performed in the trec 1 tracks. see the track reports later in these proceedings for a more complete description of each track.
table 1: number of participants per track and total number of distinct participants in each trec
trectrack'1'1'1'1'1'1'1'1'1'1'1'1'1'1'1'1ad hoc1111routing111interactive11111spanish11confusion1merging1filtering1111chinese1nlp1speech11xlingual111high prec1vlc1query11qa11111web111video1novelty11genomics111hard11robust11terabyte11enterprise11spam11legal1blog1million q1participants111111111 the blog track
the blog track first started in trec 1. its purpose is to explore information seeking behavior in the blogosphere  in particular to discover the similarities and differences between blog search and other types of search. the trec 1 track contained three tasks  an opinion retrieval task that was the main task in 1; a subtask of the opinion task in which systems were to classify the kind of the opinion detected  the polarity task ; and a blog distillation  also called a feed search  task.
the document set for all tasks was the blog corpus created for the 1 track and distributed by the
university of glasgow  see http://ir.dcs.gla.ac.uk/testcollections . this corpus was collected over a period of 1 weeks from december 1 through february 1. it consists of a set of uniquely-identified xml feeds and the corresponding blog posts in html. for the opinion and polarity tasks  a  document  in the collection is a single blog post plus all of its associated comments as identified by a permalink. the collection is a large sample of the blogosphere as it existed in early 1 that retains all of the gathered material including spam  potentially offensive content  and some non-blogs such as rss feeds. specifically  the collection is 1gb of which 1gb is permalink documents  1gb is feeds  and 1gb is homepages. there are approximately 1 million permalink documents.
   in the opinion task  systems were to locate blog posts that expressed an opinion about a given target. targets included people  organizations  locations  product brands  technology types  events  literary works  etc. for example  three of the test set topics asked for opinions regarding coretta scott king  jstor  and barilla brand pasta. targets were drawn from a log of queries submitted to a commercial blog search engine. the query from the log was used as the title field of the topic statement; the nist assessor who selected the query created the description and narrative parts of the topic statement to explain how he or she interpreted that query.
   the systems' job in the opinion task was to retrieve posts expressing an opinion of the target without regard to the kind  polarity  of the opinion. nonetheless  the relevance assessors did differentiate among different types of posts during the assessment phase as they had done in 1. a post could remain unjudged if it was clear from the url or header that the post contains offensive content. if the content was judged  it was marked with exactly one of: irrelevant  not on-topic   relevant but not opinionated  on-topic but no opinion expressed   relevant with negative opinion  relevant with mixed opinion  or relevant with positive opinion. these judgments supported the polarity subtask. for the polarity subtask  participants' systems labeled each document in the ranking submitted to the opinion task with the predicted judgment  positive  negative  mixed  of that document.
   the goal in the blog distillation task was for systems to find blogs  not individual posts  with a principal  recurring interest in the subject matter of the topic. such technology is needed  for example  when a user wishes to find blogs in an area of interest to follow regularly. the system response for the feed task was a ranked list of up to 1 feed ids  as opposed to permalink ids.  topic creation and relevance judging for the feed task were performed collaboratively by the participants.
   twenty-four groups total participated in the blog track including 1 in the opinion task  1 in the polarity subtask  and 1 in the feed task.
   to address the question of specific opinion-finding features that are useful for good performance in the opinion task  participants were asked to submit both a topic-relevance-only baseline and an opinionfinding run. results from this comparison were mixed  with some systems showing a marked increase in effectiveness over good baselines by using opinion-specific features  but others showing serious degradation. nonetheless  as in the 1 track the correlation between topic-relevance effectiveness and opinion-finding effectiveness remains very high  indicating that topic-relevance effectiveness is still a dominant factor in good opinion finding.
1 the enterprise track
trec 1 was the third year of the enterprise track  a track whose goal is to study enterprise search: satisfying a user who is searching the data of an organization to complete some task. enterprise data generally consists of diverse types such as published reports  intranet web sites  and email  and a goal is to have search systems deal seamlessly with the different data types.
   because of the track's focus on supporting a user of an organization's data  the data set and task abstraction are particularly important. the document set in the first two years of the track was a crawl of the world-wide web consortium web site. this year the document set was instead a crawl of www.cisro.au  the web site of the commonwealth scientific and industrial research organisation  csiro   which is australia's national science agency. csiro employs people known as science communicators who enhance csiro's public image and promote the capabilities of csiro by managing information and interacting with various constituencies. in the course of their work  science communicators can come upon an area of focus for which no good overview page exists. in such a case a communicator would like to find a set of key pages and people in that area as a first step in creating an overview page  or to stand as a substitute for such a page . this  missing page  problem was the motivation for the two tasks in the track.
   in the document search task systems were to retrieve a set of key pages related to the target topic. as in previous years  a key page was defined as an authoritative page that is principally about the target topic. in the search-for-experts task systems returned a ranked list of email addresses representing individuals who are experts in the target topic. unlike previous years  there was no a priori list of people made available to the systems. instead  systems were required to mine the document set to find people and decide whether they are experts in a given field. systems were required to return a list of up to 1 documents in support of the nomination of an expert.
   the topics for the track were developed by current csiro science communicators  with the same set of topics used for both tasks. communicators were given a csiro query log and asked to develop topics using queries taken from the log or something similar to those. in addition to the query  the communicators were also asked to supply examples of key pages for the area of the query  one or two csiro staff members who are experts in that area  and a short description of the information they would consider relevant to include in the overview page.
   systems were provided with the query and description as the official topic statement. systems could also access the communicator-provided key page examples for relevance feedback experiments. the experts supplied by the science communicators were used as the relevance judgments for the expert search task. document pools were judged by participants based on the full topic statements to produce the relevance judgments for the document task.
   twenty groups total participated in the enterprise track  with 1 groups participating in the document task and 1 in the expert search task. comparison between feedback and non-feedback runs in the document task shows that successfully exploiting the example key pages was challenging: only a few teams submitted feedback runs that were more effective than their own non-feedback runs. the results from the expertfinding task suggest that systems are finding only people associated with a given topic rather than actual expertise. for example  systems suggested the science communicators as experts for some topics.
1 the genomics track
the goal of genomics track is to provide a forum for evaluation of information access systems in the genomics domain. it was the first trec track devoted to retrieval within a specific domain  and thus a subgoal of the track is to explore how exploiting domain-specific information improves access. the task in the trec 1 track was similar to the passage retrieval task introduced in 1. in this task systems retrieve excerpts from the documents that are then evaluated at several levels of granularity to explore a variety of facets. the task is motivated by the observation that the best response for a biomedical literature search is frequently a direct answer to the question  but with the answer placed in context and linking to original sources.
   the document collection used for 1 was the same as that used for 1. this document collection is a set of full-text articles from several biomedical journals that were made available to the track by highwire press. the documents retain the full formatting information  in html  and include tables  figure captions  and the like. the test set contains about 1 documents from 1 journals and is about 1 gb of html. a passage is defined to be any contiguous span of text that does not include an html paragraph token   p  or   p  . systems returned a ranked list of passages in response to a topic where passages were specified by byte offsets from the beginning of the document.
   the format of the topic statements differed from that of 1. the 1 topics were questions asking for lists of specific entities such as drugs or mutations or symptoms. the questions were solicited from practicing biologists and represent actual information needs. the test set contained 1 questions.
   relevance judgments were made by domain experts. the judgment process involved several steps to enable system responses to be evaluated at different levels of granularity. passages from different runs were pooled  using the maximum extent of a passage as the unit for pooling.  the maximum extent of a passage is the contiguous span between paragraph tags that contains that passage  assuming a virtual paragraph tag at the beginning and end of each document.  judges decided whether a maximum span was relevant  contained an answer to the question   and  if so  marked the actual extent of the answer in the maximum span. in addition  the assessor listed the entities of the target type contained within the maximum span. a maximum span could contain multiple answer passages; the same entity could be covered by multiple answer passages and a single answer passage could contain multiple entities.
   using these relevance judgments  runs were then evaluated at the document  passage  and aspect  entity  levels. a document is considered relevant if it contains a relevant passage  and it is considered retrieved if any of its passages are retrieved. the document level evaluation was a traditional ad hoc retrieval task  when all subsequent retrievals of a document after the first were ignored . passage- and aspect-level evaluation was based on the corresponding judgments. aspect-level evaluation is a measure of the diversity of the retrieved set in that it rewards systems that are able to find more different aspects. passage-level evaluation is a measure of how well systems are able to find the particular information within a document that answers the question.
   the genomics track had 1 participants. results from the track showed that effectiveness as measured at the three different granularities was highly correlated. as in the blog track  this suggests that basic recognition of topic relevance remains a dominating factor for effective performance in each of these tasks.
1 the legal track
the legal track was started in 1 to focus specifically on the problem of e-discovery  the effective production of digital or digitized documents as evidence in litigation. since the legal community is familiar with the idea of searching using boolean expressions of keywords  boolean search is used as a baseline in the track. the goal of the track is thus to evaluate the effectiveness of boolean and other search technologies for the e-discovery problem.
   the trec 1 track contained three tasks  the main task  an interactive task  and a relevance feedback task. the document set used for all tasks was the iit complex document information processing collection  which was also the corpus used in the 1 track. this collection consists of approximately seven million documents drawn from the legacy tobacco document library hosted by the university of california  san francisco. these documents were made public during various legal cases involving us tobacco companies and contain a wide variety of document genres typical of large enterprise environments. a document in the collection consists of the optical character recognition  ocr  output of a scanned original plus metadata.
   the main task was an ad hoc search task using as topics a set of hypothetical requests for production of documents. the production requests were developed for the track by lawyers and were designed to simulate the kinds of requests used in current practice. each production request includes a broad complaint that lays out the background for several requests and one specific request for production of documents. the topic statement also includes a negotiated boolean query for each specific request. stephen tomlinson of open text  a track coordinator  ran the negotiated boolean queries to produce the task's reference run. participants could use the negotiated boolean query  the set of documents that matched the boolean query  and the size of the retrieved set of the boolean query  b  in any way  including ignoring them completely  for their submitted runs. for each topic systems returned a ranked list of up 1 documents  or up to b documents if b was larger than 1 .
   because of the size of the document collection and the legal community's interest in being able to evaluate the effectiveness of the  unranked  boolean run  special pools were built from the submitted runs to support estimated-recall-at-b as the evaluation measure. the pooling method sampled a total of approximately 1 documents from the set of submitted runs respecting the property that documents at ranks closer to one had a higher probability of being selected for inclusion in the pools.  see the track overview paper for more details.  note that it is not currently known how reusable the resulting collection is  that is  whether the judgments can be usefully exploited to evaluate runs that did not contribute to the pools . the relevance assessments were made by legal professionals  mostly law students  who followed the legal community's typical work practices.
   iterative search methods generally offer increased effectiveness as compared to the single running of a static query  even if that query is the result of prior negotiation. the feedback and interactive search tasks were introduced into the legal track to explore the level of performance obtainable by iterative search methods in e-discovery and to investigate how best to evaluate those techniques. both tasks used a subset of the topics from the trec 1 legal track.
   the goal in the interactive task was for a user to find as many relevant documents as possible for a topic while actively engaging with the retrieval system. twelve topics were available for this task  ranked in priority order. participants in the interactive task could do as many of the twelve topics as desired  but were required to perform them in priority order. submissions consisted of up to 1 documents per topic  which were scored using a utility measure  gaining one point for each relevant document retrieved and losing a half point for each nonrelevant retrieved .
   for the relevance feedback task  systems re-ran the trec 1 topics exploiting the relevance judgments produced as a result of the trec 1 track. documents that had been judged in 1 were removed from the submissions   residual collection  evaluation  and new pools were formed for 1 topics  a subset of the 1 topics used in the interactive task 1. the main evaluation measure used in the task was again
estimated-recall-at- residual -b.
   a total of 1 groups participated in the legal track: 1 in the main task  1 in the interactive task  and 1 in the relevance feedback task. results from the trec 1 tasks confirm results from the trec 1 track with respect to the boolean run. collectively the runs produced by track participants retrieve many relevant documents not retrieved by the negotiated boolean queries of the reference run  but the average effectiveness of the reference boolean run is at least as great as the average effectiveness of the other individual runs  with respect to estimated-recall-at-b . in other words  all of the runs  including the reference boolean run  have significant room for improvement with respect to consistently obtaining high recall.
1 the million query track
the million query track was a new track in trec 1. one of the main goals of the track was to investigate a specific retrieval evaluation hypothesis: that a test collections built using many topics with few  shallow judgments may be a better evaluation tool than a test collection built from fewer topics with relatively thorough judgments. the track also provided an opportunity for participants to explore ad hoc retrieval on a large document set.
   the retrieval task of the track was an ad hoc search task over the gov1 document set. gov1 is a collection of web pages from within the .gov domain spidered in early 1. the collection contains about 1 million documents and is available from the university of glasgow  see http://ir.dcs.

gla.ac.uk/testcollections . the topics for the track were taken from a web search engine log and consisted only of the equivalent of the standard trec topic statement's title field  some of these topics later had standard topic statements developed for them during the assessing phase . the test set consisted of 1 queries  including the title field from some of the topics that had been used in previous years' terabyte tracks.
   relevance judging was performed by both nist assessors and track participants. the judging procedure was as follows:
1. the assessment system presented the judge with 1 queries randomly selected from the test set.
1. the judge selected one of the queries; the others were returned to the query pool.
1. the judge wrote a description and narrative for this query  thus creating a standard trec topic statement.
1. the system presented a gov1 document to the judge and obtained a 1-way judgment  highly relevant  relevant  not relevant  for it.
1. the process continued until at least 1 documents were judged. the judge could continue past 1 documents if he or she wanted to.
   the documents to be judged were selected by one of two different sampling methods  the minimal test collection method and the statistical evaluation method  each of which supports a particular evaluation strategy. the details of the sampling and corresponding evaluation methods are given in the track overview paper in these proceedings. the target was to have half the queries that were judged have 1 documents selected by both methods  a quarter of the queries have 1 documents selected by the minimal test collection sampling method  and the remaining queries have 1 documents selected by the statistical evaluation method. approximately 1 queries were judged  with a small set receiving judgments from multiple people.
   the judgments gathered in this way allow evaluation using the appropriate measure s  associated with the selection method. the use of the terabyte topics allows runs to be evaluated over those topics using treceval and the standard nist-produced relevance judgments created in the terabyte track as a third evaluation strategy. the 1 runs submitted by 1 groups were each evaluated using the three evaluation strategies in turn. the three different strategies agreed with one another with respect to  big picture  results: all three strategies found the same three clusters of systems with similar effectiveness. more fine-grained comparisons differed across strategies  though  in that rankings of systems within clusters varied depending on the evaluation strategy used. the rankings produced by the two sampling-based evaluation methods were more similar to each other than either was to the ranking produced by evaluation over the terabyte topics.
1 the question answering  qa  track
the goal of the question answering track is to develop systems that return actual answers  as opposed to ranked lists of documents  in response to a question. the 1 track contained two tasks  the main task that was a series task similar to the task used since 1  and a complex interactive qa  ciqa  task introduced in 1.
   the questions in the main task were organized into a set of series. a series consisted of a number of  factoid   questions with fact-based  short answers  and list questions that each related to a common  given target. the final question in a series was an explicit  other  question  which systems were to answer by retrieving information pertaining to the target that had not been covered by earlier questions in the series. answers were required to be supported by a document from the corpus used in the track.
   the 1 main task differed from the task in earlier years in that the corpus consisted of both newswire documents  the aquaint-1 collection  and blog documents  the same corpus as was used in the blog track . introducing blogs into the track created two significant new challenges for qa systems. first  since language use in blogs can be much more informal than in newswire  systems were required to handle language that is not well-formed. second  blog data also contains discourse structures that are less formal and reliable than newswire  so systems had to do more vetting of candidate responses to determine if those responses were indeed answers.
   despite the introduction of the blog data  which was expected to increase the difficulty of the qa task  individual component scores for the best systems were greater in 1  after having generally declined each year since trec 1. while it is possible that the questions in the 1 test set are intrinsically easier than previous years  no procedural changes in the way questions were formed were instituted  so large changes in difficulty are not likely.
   the ciqa task was introduced in trec 1 and is a blend of the trec 1 relationship qa task and the trec 1 hard track. the goal of the task is to extend systems' abilities to answer more complex information needs than those covered in the main task and to provide a limited form of interaction with the user in a qa setting.
   as in 1  the questions used in the task contained two parts  a specific question derived from templates of relationship question types  and a narrative that provided more explanation for the specific question. the system response to a question was a ranked list of information  nuggets  supported by aquaint documents  the blog corpus was not used in the ciqa task   where each nugget provides evidence for the relationship in question.
   the interaction was accomplished using the nist assessor as the surrogate user and web forms to implement the interface. unlike 1  the forms were hosted at the individual participants' home site  so any type of web-based qa system could be used in the task. for each topic  the assessor was given a list of urls  one url per participating run. the lists of urls for different topics were sorted differently  and assessors processed each list in the order given  to control for presentation order effects. assessors clicked on a url to begin an interaction and had a maximum of five minutes to finish the task for that pair of run/topic. participants were responsible for instrumenting the application to capture the results of the interaction.
   the protocol for the ciqa task had participants submit initial runs prior to the interaction  perform the interaction  and then submit final runs that  presumably  made use of the information gathered in the interaction. retrieval results were scored using pyramid nuggets f-score. in addition  an exit questionnaire gathered data on the assessors' perceptions of the interactions.
   results from the ciqa task showed that  unlike in trec 1  most runs were more effective than a sentence-retrieval baseline run. however  many interactions degraded effectiveness; that is  the final run score was less than the corresponding initial run's score. analysis of the data collected from the exit questionnaire suggested a possible contributing factor for the decrease in effectiveness through interaction: nist assessors are unusual users in that they already know a lot about the topic  yet the typical users assumed by many participating systems were naive users searching for basic information. future instantiations of interactive tasks will need to take this mismatch into consideration.
   a total of 1 groups participated in the qa track. the main task had 1 participants and the ciqa task had 1 participants.
1 the spam track
the spam track was first run in trec 1. the goal of the track is to evaluate how well systems are able to separate spam and ham  non-spam  when given an email sequence. the trec 1 track repeated the three 1 tasks using new data. the tasks all involved classifying email messages as ham or spam  differing in the amount and frequency of the feedback the system received.
   for each task the track used a test jig that implements a simple interface between the evaluation infrastructure on the one hand and a participant's classifier on the other. the jig takes an email stream  a set of ham/spam judgments  and a classifier  and runs the classifier on the stream reporting the evaluation results of that run based on the judgments. in the main on-line filtering task  the classifier receives the correct designation for a message as soon as it classifies the message  this represents ideal user feedback . in the delayed feedback extension to the task  the classifier might eventually receive the correct designation for a message  but the designation for a given message m may come after some number of intervening messages that must be classified before the feedback for m is received  or the feedback may never come at all. in the partial feedback extension to the task  feedback is provided only for messages sent to a subset of the users of a mail server  though the filter is expected to filter messages to all users. in the active learning task  the classifier must explicitly request the correct designation for a document  and may do so for only a given number n of messages.
   the track used both a private email stream and a public email stream. participants ran their own filters on the public corpora using the jig and submitted the evaluation output to nist. for the private corpora  participants submitted their filters to nist. nist passed the filters onto the university of waterloo after stripping all identification of which filters came from which participant. the university of waterloo used the jig to run the filters on the private stream and returned the evaluation results to nist  who then forwarded the evaluation results to the appropriate participant.
   twelve groups participated in the spam track. as in previous years of the track  the general effectiveness of the track's filters has improved relative to the then-current state-of-the-art. comparison among the different types of training show that both delayed and partial feedback degrade filter effectiveness with respect to ideal feedback  but longer delay periods do not appear to cause more deterioration than shorter delay periods.
1 the future
trec1 contained a brainstorming session designed to get feedback as to what research areas individuals in the trec community were personally interested in. in the spirit of true brainstorming  we asked for any ideas without initial filtering by feasibility concerns such as data availability or privacy issues. the session was lively with approximately 1 ideas suggested before discussion was stopped due to time constraints. enough people expressed interest in three broad areas for those ideas to be further explored informally over a group lunch at the conference and discussion lists after the conference. the goal of the discussions was to formulate a proposal for a trec track in the area to begin in trec 1. the three areas included:
informal text: a track to focus on data access tasks within social media contexts such as instant messaging systems or social tagging;
scientific literature: a track to focus on providing access to the scientific literature more broadly than within a single topic domain as in the genomics track; and
user interaction: a reprise of the trec interactive track where the focus is on understanding how best to support humans in the search process.
   there are five confirmed tracks for trec 1. the blog  enterprise  legal  and million query tracks will continue. a new track to examine the effectiveness of relevance feedback across different retrieval models and under different conditions  such as amount of relevance data  will begin. the question answering track will move to a new nist evaluation conference called the text analysis conference  tac   see http: //www.nist.gov/tac. the genomics and spam tracks are ending as trec tracks  though tasks similar to those investigated in these tracks are expected to appear in other venues.
acknowledgements
the track summaries in section 1 are based on the track overview papers authored by the track coordinators. my thanks to the coordinators who make the variety of different tasks addressed in trec possible.
