we study leading-edge local search heuristics for balanced hypergraph partitioning and boolean satisfiability  intending the generalization of such heuristics beyond their original domains. we adapt the fiduccia mattheyses  fm  hypergraph partitioning heuristic to boolean satisfiability  sat   and the walksat sat solver to hypergraph partitioning. focusing on balanced hypergraph partitioning  we propose a combination of the classical fm heuristic and our  cross-over  heuristic walkpart  and empirically show that it is more powerful than each component alone. empirically  we show a 1% improvement in net cut and a 1% improvement in runtime over a leading-edge implementation of the fm heuristic. 
1 	introduction 
the focus of our work is on search heuristics for balanced min-cut hypergraph partitioning  a well-known combinatorial optimization problem. in general terms  the problem seeks to assign every vertex of a hypergraph to one of two subsets to minimize the number of connections between the two subsets  subject to having approximately the same number of vertices in each subset. hypergraph partitioning is important in circuit layout because circuits arc often represented by hypergraphs  e.g. in placement by recursive bisection. many efficient hypergraph partitioning algorithms have been developed  1 . here  we discuss the combination of two local search heuristics: the fiduccia-mattheyses  fm  hypergraph partitioning heuristic   and the biased random walk heuristic  adapted from the walksat sat solver . 
   our work is motivated by attempts to generalize well-known local search procedures  1  1  beyond their original domains. we developed a sat solver based on the fm algorithm   and a hypergraph partitioner  walkpart  based on the walksat algorithm from . the walkpart algorithm is described in section 1. a detailed description of the sat solver is beyond the scope of this paper. our results show that walkpart outperforms a highly tuned implementation of fm on several circuit benchmarks. the algorithms also produce very different solutions and appear to have different local minima  suggesting that a hybrid strategy might be effective. empirical results show that the hybrid approach outperforms both algorithms  in terms of runtime and solution quality. our main contributions are summarized as follows. 
  we describe the balanced min-cut hypergraph partitioning algorithm  walkpart  based on the walksat satisfiability solver  
  we propose the combination of the fm and walkpart heuris-tics to form a  hybrid  heuristic  and perform two experiments 
  empirically  we show that a hybrid strategy achieves signifi-cantly better performance than either heuristic alone 
1 
1 background and previous work 
1 	hypergraph partitioning problem: 
a hypergraph h v e  is defined as a set  v  of vertices  and a set  
e  of hyperedges  where each edge e e is a set of two or more vertices from v. the k-way hypergraph partitioning problem is stated as follows: let h v e  be a hypergraph with weighted vertices and edges. the problem asks to find a minimum cost partition of the vertices of // to k disjoint subsets. the cost function typically used is edge cut  the sum of the weights of the hy-
peredges cut by the partition  this is called min-cut partitioning . a hypcredgc is cut exactly when all of its vertices arc in more than one subset. constraints may be imposed on a partition e.g. limiting the total vertex weight in each subset  balance  constraints . the balance-constrained weighted hypergraph partitioning problem is np-hard. the algorithms in this work deal with the balanced min-cut hypergraph bi-partitioning problem  i.e.  we consider only the case where k = 1. in circuit layout  hypergraphs represent circuits by mapping vertices to signal nodes  and hyperedges to signal nets. in this context  we use the words  node  and  net  to mean  vertex  and  hyperedge  when referring to hypergraphs in this document. 
1 	the fiduccia-mattheyses heuristic 
the fiduccia-mattheyses  fm   heuristic for bi-partitioning circuit hypergraphs is an iterative improvement algorithm. fm starts with an arbitrary initial solution and changes the solution by a sequence of moves  organized as passes. at the beginning of a pass  all vertices are free to move  unlocked . each possible move is labeled with the immediate change in total cost it would cause  called  gain  . iteratively  a move with highest gain is selected and executed  which may result in a lower  higher or unchanged solution cost1. the moved vertex is locked  i.e.  not allowed to move again during that pass. since moving a vertex can change gains of adjacent vertices  all affected gains are updated. selection and execution of a best-gain move  followed by gain update  are repeated until every vertex is locked. the best solution seen during the pass is adopted as the starting solution of the next pass. the algorithm terminates when a pass fails to improve solution quality. 
   the fm heuristic is now used as a component in multi-level partitioning algorithms  which create a series of clustered hypergraphs  partition the most clustered with  flat  fm and then refine unclustered versions of the produced partition by applying fm in incremental mode. a highly optimized  leading edge implementation of fm in this context is available in . this work only deals with  flat  partitioning heuristics in the hope that improved flat heuristics will lead to improved multi-level heuristics. this assumption will be verified in our future work. 

table 1: results for 1 passes of a hybrid algorith m: fm is run once to com pletion  followed by 1k moves of walkpart alternated with single passes of fm . 
benchmark 
name nodes nets walkpart fmpart hybrid  cut ratio time 1 
ratio moves time cut time cut time | 	cut ibm1 + 1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm  1+io 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibml1 1 1 1m 1 1 1 1 1 1 1 1 average 1 1 
1 	the walksat satisfiability solver 
the walksat algorithm  is a stochastic local search heuristic for boolean satisfiability. walksat is an improvement on gsat  sclman et al.  1   a greedy algorithm that starts with a random assignment and flips variables one at a time. the variable chosen for flipping is always the one that would cause the greatest immediate reduction in the number of violated clauses. walksat  improves on gsat by using a biased random walk to avoid getting stuck in local minima. 
　　to select the next variable to flip  walksat picks a clause at random from the unsatisfied clauses. with probability p  it flips a variable picked randomly from this clause. with probability 1 - p it flips the variable that would cause the least number of clauses to be violated when flipped. 
1 	the walkpart algorithm 
the walkpart algorithm is a stochastic local search algorithm for balance-constrained hypergraph bisection. it performs a sequence of moves  until a preset maximum number of moves is reached. a  move  moves a vertex from one partition to another  if balance constraints arc not violated. moves are chosen using criteria similar to walksat: with probability p  a vertex is picked at random  and with probability 1 - p  a vertex is chosen to minimize some cost function. the selected vertex is then moved to the other partition  if balance constraints allow . after a move  the costs of vertices adjacent to the moved vertex will change  and these are updated. 
   the move selection procedure is explained in more detail as follows. a net from among the cut nets is chosen at random. with probability p  a vertex is chosen from that net at random  and with probability 1 - p  the vertex that minimizes the cost function under the decision heuristic in use is chosen from the net. if a chosen move is illegal  the selection process is repeated until a legal move is found. 
   due to space constraints  we describe only the decision heuristic that performed best on the benchmarks in this paper  the normalized node score heuristic. 
　　normalized score heuristic: the fm heuristic is concerned with the  gain  of a node  determined by the number of nets that can be  cut  or  uncut  by moving the node. therefore  it only considers  critical  nets that can be cut or uncut in a single move. it is mentioned in  that a more thorough measure would also estimate the effort required to uncut the net. we propose a new measure  the  score  of a node  defined as the number of moves required to uncut the net if the node in question was moved. in practice  scores are normalized to avoid bias due to net sizes. we can quickly approximate the score for a given node for a particular net as follows. consider a node  ni  presently in partition j  j - o/i . let e/c be a hyperedge containing ni  dk be the degree of  be the number of nodes from edge ek in partition 1  including ni . then the normalized score of ni  due to ek is the total node score is the sum of the scores due to all nets in which the node occurs. the idea of using a score comes from the work in   which discusses an adaptation of walksat to formulas with pseudo-boolean constraints. in walkpart  with probability i - p the algorithm picks the node from the chosen net with the minimum score. for the benchmarks in this paper  our empirical results are strongest when this heuristic is used  and when /; - 1. 
1 hybrid partitioner 
through empirical evaluation  we observe that walkpart often outperforms a leading-edge implementation of fm   on standard industry circuit benchmarks . also  walkpart frequently does well on benchmarks where fm does not  implying that both algorithms do not find similar solutions or search in the same areas of the search space. these observations suggest that a hybrid algorithm might be able to exploit the strengths of both. 
   the hybrid approach alternates runs of fmpart  the fm implementation available in   and walkpart. it is known that fmpart reduces the net cut greatly in the first few passes  and then spends time performing more passes for relatively small improvements. the hybrid algorithm starts by performing one or more passes of 
fmpart  and then performs moves of walkpart to improve fmpart's solution. control then switches back to fmpart  and so on. a combined fmpart-walkpart pass constitutes one pass of the hybrid algorithm. a preset number of hybrid passes are run before the algorithm terminates. the use of a walksat-like randomized component makes it difficult to find obvious convergence criteria. we hope to address this issue in future works. in practice  we observe that the tendency of fm to converge rapidly causes considerable reductions in net cut in a fairly small number of passes. this can be seen from our results in section 1. many stochastic local search approaches use such preset limits because of the unpredictable nature of the search. 
1 results 
in this section  we discuss the empirical performance of our algorithms. we cannot include the results of all our experiments with different tuning and parameter settings due to limited space. we present results of two relatively simple experiments that illustrate the utility of the hybrid technique. the benchmarks used are standard ibm circuit partitioning benchmarks . although 

poster papers 	1 

table 1: results for 1 passes of our hybrid algorithm. at each pass  two passes of f m are followed by 1k moves of walkpart. 
| benchmark 
| name nodes nets walkpart fmpart hybrid cut ratio time 
ratio moves time cut time cut time cut ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm1 1 1 1k 1 1 1 1 1 1 1 1 ibm  1 + 1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1+io 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibm1 1 1 1m 1 1 1 1 1 1 1 1 ibml1 1 1 1m 1 1 1 1 1 1 1 1 average 1 1 
the benchmarks arc chosen from the vlsi domain  all the algorithms discussed here are general and can be used on instances from any application. in the experiments  walkpart uses the normalized score heuristic  and randomness quotient p - 1. 1 we use the fm partitioning package by caldwell ct. al.  available at   for the fm component. walkpart is written in c++. 
   in the first experiment  we run 1 hybrid passes after first running fm once to completion. each hybrid pass performs 1 walkpart moves followed by a single fmpart pass. the solution after a pass is used as the starting solution for the next pass. results for this experiment arc shown in table 1. 
the hybrid algorithm is compared against fmpart and walk-
part  run individually. walkpart uses the normalized score heuristic and p - 1  with two different move limits: 1 and 1 1. because some moves are picked randomly  increasing the move limit docs not always produce better solutions. wc show results for the move limit where net cut was lower. for each benchmark we report average solution quality and runtime over three independent starts on a 1 ghz amd athlon processor with 1gb  running linux. 
　　in the tabic  columns 1 describe benchmark statistics. the benchmarks are named ibmno+t%  where ibm indicates an ibm benchmark  no is the serial number  / indicates the balance constraint. tolerances specify the maximum legal disparity between subset weights as a percentage of total vertex weight. the next three columns give walkparf s number of moves  runtime and resulting net cut. the next four columns show runtimes and net cuts for fm and the hybrid algorithm. the lowest net cut out of the 1 algorithms is boldfaced. the last two columns show the ratio of net cut and runtime of the hybrid algorithm to net cut and runtime of fmpart. averages are shown at the bottom. 
   we observe that the hybrid algorithm achieves smaller cuts than both fmpart and walkpart on most benchmarks. this is to be expected  since hybrid passes are run on a solution produced by fmpart. nevertheless  we can make some important observations. 
first  fmpart frequently converges when a much better solution is achievable. second  it is possible to achieve a better solution by introducing a randomized component with a different decision heuristic  normalized score . walkpart can be used to lead fmpart out of local minima by performing random moves  after which fmpart can be run again to find a better solution. however  this experiment has a runtime cost. on average  this strategy improves run time by 1% over fmpart  but is 1 times slower. 
our second experiment improves runtime by avoiding the ini-
tial run of fmpart. this experiment also uses 1 hybrid passes. each pass consists of 1 passes of fmpart followed by 1 moves of walkpart. results for this experiment are in table 1. 
   it is clear that the second hybrid configuration is competitive with fmpart and walkpart in terms of runtime  and frequently produces better solutions. also  while either fmpart or walkpart may achieve lower net cut than the hybrid algorithm  it is very rare that both of them do so. the hybrid approach is usually close to the best solution achieved by either. on average  the second hybrid configuration improves net cut by 1% and is about 1% faster than fmpart alone. 
1 	conclusions and future work 
in this work  wc introduce our  crossover  hypergraph partitioning algorithm  walkpart. wc combine walkpart with the well-known fiduccia-mattheyses  fm  heuristic for hypergraph partitioning to produce a hybrid heuristic that draws on the strengths of both techniques. wc achieve substantial improvement over fmpart with a very simple approach that requires almost no tuning. 
   our first experiment shows that net cuts achieved by fm  a widely used algorithm for this application  can be significantly improved  by 1%   albeit with a runtime cost. however  our second experiment reduces net cuts by an average of 1%  and runtime by 1% over fm. our results show that combining these two heuristics results in a search procedure that is more powerful than either heuristic used alone. strong empirical results for the proposed hybrid algorithm motivate further research. we are incorporating our flat partitioning algorithms into the multilevel framework  1  and plan to make it available for use in cad tools. we are also continuing our investigation of effective search procedures for partitioning and looking for new techniques that can be hybridized with existing ones. 
