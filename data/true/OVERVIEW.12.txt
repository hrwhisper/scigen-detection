1.1 documents
the document set of a test collection should be a sample of the kinds of texts that will be encounteredin the operational setting of interest. it is important that the document set reflect the diversity of subject matter  word choice  literary styles  document formats  etc. of the operational setting for the retrieval results to be representative of the performance in the real task. frequently  this means the document set must be large. the primary trec test collections contain about 1 gigabytes of text  between 1 and 1 1 documents . the document sets used in various tracks have been smaller and larger depending on the needs of the track and the availability of data.
   the primary trec document sets consist mostly of newspaper or newswire articles  though there are also some government documents  the federal register  patent applications  and computer science abstracts  computer selects by ziff-davis publishing  included. high-level structures within each document are tagged using sgml  and each document is assigned an unique identifier called the docno. in keeping of the spirit of realism  the text was kept as close to the original as possible. no attempt was made to correct spelling errors  sentence fragments  strange formatting around tables  or similar faults.
1.1 topics
trec distinguishes between a statement of information need  the topic  and the data structure that is actually given to a retrieval system  the query . the trec test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. the format of a topic statement has evolved since the earliest trecs  but it has been stable since trec-1  1 . a topic statement generally consists of four sections: an identifier  a title  a description  and a narrative. an example topic taken from this year's robust retrieval track is shown in figure 1.
   the different parts of the trec topics allow researchers to investigate the effect of different query lengths on retrieval performance. for topics 1 and later  the  title  field was specially designed to allow experiments with very short queries; these title fields consist of up to three words that best describe the topic. the description field is a one sentence description of the topic area. the narrative gives a concise description of what makes a document relevant.
   participants are free to use any method they wish to create queries from the topic statements. trec distinguishes among two major categories of query construction techniques  automatic methods and manual methods. an automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. the definition of manual query construction methods is very broad  ranging from simple
 num  number: 1
 title  czech  slovak sovereignty
 desc  description:
retrieve information regarding the process by which the czech and slovak republics of czechoslovakia established separate sovereign countries.
 narr  narrative:
a relevant document will provide specific dates and details regarding the separation movement. documents relating to normal activities of the separate nations  both internal and external are not relevant.
figure 1: a sample trec 1 topic from the robust retrieval track.
tweaks to an automatically derived query  through manual construction of an initial query  to multiple query reformulations based on the document sets retrieved. since these methods require radically different amounts of  human  effort  care must be taken when comparing manual results to ensure that the runs are truly comparable.
   trec topic statements are created by the same person who performs the relevance assessments for that topic  the assessor . usually  each assessor comes to nist with ideas for topics based on his or her own interests  and searches the document collection using nist's prise system to estimate the likely number of relevant documents per candidate topic. the nist trec team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors.
1.1 relevance judgments
the relevance judgments are what turns a set of documents and topics into a test collection. given a set of relevance judgments  the retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. trec almost always uses binary relevance judgments-either a document is relevant to the topic or it is not. to define relevance for the assessors  the assessors are told to assume that they are writing a report on the subject of the topic statement. if they would use any information contained in the document in the report  then the  entire  document should be marked relevant  otherwise it should be marked irrelevant. the assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.
   relevance is inherently subjective. relevance judgments are known to differ across judges and for the same judge at differenttimes . furthermore  a set of static  binary relevance judgmentsmakes no provisionfor the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. despite the idiosyncratic nature of relevance  test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments .
   the relevance judgments in early retrieval test collections were complete. that is  a relevance decision was made for every document in the collection for every topic. the size of the trec document sets makes complete judgments utterly infeasible-with 1 documents  it would take over 1 hours to judge the entire document set for one topic  assuming each document could be judged in just 1 seconds. instead  trec uses a technique called pooling  to create a subset of the documents  the  pool   to judge for a topic. each document in the pool for a topic is judged for relevance by the topic author. documents that are not in the pool are assumed to be irrelevant to that topic.
   the judgment pools are created as follows. when participants submit their retrieval runs to nist  they rank their runs in the order they prefer them to be judged. nist chooses a number of runs to be mergedinto the pools  and selects that many runs from each participant respecting the preferred ordering. for each selected run  the top documents  usually    per topic are added to the topics' pools. since the retrieval results are ranked by decreasing similarity to the query  the top documents are the documents most likely to be relevant to the topic. many documents are retrieved in the top for more than one run  so the pools are generally much smaller than the theoretical maximum of the-number-of-selected-runs documents  usually about 1 the maximum size .
   the use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents.
   zobel demonstrated that the quality of the pools  the number and diversity of runs contributing to the pools and the depth to which those runs are judged  does affect the quality of the final collection . he also found that the trec collections were not biased against unjudged runs. in this test  he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. for the trec-1 ad hoc collection  he found that using the unique relevant documents increased a run's 1 point average precision score by an average of 1 %. the maximum increase for any run was 1 %. the average increase for the trec-1 ad hoc collection was somewhat higher at 1 %.
   a similar investigation of the trec-1 ad hoc collection showed that every automatic run that had a mean average precision score of at least 1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents . that investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs  an effect noted by the organizers of the ntcir  nacsis test collection for evaluation of informationretrieval systems  workshop who performedtheir own manual runs to supplement their pools .
   while the lack of any appreciable difference in the scores of submitted runs is not a guarantee that all relevant documents have been found  it is very strong evidence that the test collection is reliable for comparative evaluations of retrieval runs. the differences in scores resulting from incomplete pools observed here are smaller than the differences that result from using different relevance assessors .
1 evaluation
retrieval runs on a test collection can be evaluated in a number of ways. in trec  all ad hoc tasks are evaluated using the treceval package written by chris buckley of sabir research . this package reports about 1 different numbers for a run  including recall and precision at various cut-off levels plus single-valued summary measures that are derived from recall and precision. precision is the proportion of retrieved documents that are relevant  while recall is the proportion of relevant documents that are retrieved. a cut-off level is a rank that defines the retrieved set; for example  a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. the treceval programreports the scores as averages over the set of topics where each topic is equally weighted.  the alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important. 
   precision reaches its maximal value of 1 when only relevant documents are retrieved  and recall reaches its maximal value  also 1  when all the relevant documentsare retrieved. note  however  that these theoretical maximum values are not obtainableas an averageovera set of topics at a single cut-offlevel because differenttopics havedifferent numbers of relevant documents. for example  a topic that has fewer than ten relevant documents will have a precision score less than one after ten documents are retrieved regardless of how the documents are ranked. similarly  a topic with more than ten relevant documents must have a recall score less than one after ten documents are retrieved. at a single cut-off level  recall and precision reflect the same information  namely the number of relevant documents retrieved. at varying cut-off levels  recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.
   of all the numbers reported by treceval  the recall-precision curve and mean  non-interpolated average precision are the most commonly used measures to describe trec retrieval results. a recall-precision curve plots precision as a function of recall. since the actual recall values obtained for a topic depend on the number of relevant documents  the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. the particular interpolation method used is given in appendix a  which also defines many of the other evaluation measures reported by treceval. recall-precision graphs show the behavior of a retrieval run over the entire recall spectrum.
   mean average precision is the single-valued summary measure used when an entire graph is too cumbersome. the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . the mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. the average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents  and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. geometrically  mean average precision is the area underneath a non-interpolated recallprecision curve.
table 1: number of participants per track and total number of distinct participants in each trec
tracktrec111111ad hoc1111----routing111------interactive--111111-spanish--11-------confusion---1-------db merging---1-------filtering---11111-chinese----1------nlp----1------speech-----11---cross-language-----1111-high precision-----1-----vlc------1----query------11---qa-------1111web-------1111video--------	-11anovelty--------	--11genome--------	---1hard--------	---1robust--------	---1total participants111111
athe video track was spun off as a separate evaluation effort in 1.
   as trec has expanded into tasks other than the traditional ad hoc retrieval task  new evaluation measures have had to be devised. indeed  developing an appropriate evaluation methodology for a new task is one of the primary goals of the trec tracks. the details of the evaluationmethodologyused in a track are described in the track overview paper.
1 trec 1 tracks
trec's track structure was begun in trec-1  1 . the tracks serve several purposes. first  tracks act as incubators for new research areas: the first running of a track often defines what the problem really is  and a track creates the necessary infrastructure  test collections  evaluation methodology  etc.  to support research on its task. the tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. finally  the tracks make trec attractive to a broader community by providing tasks that match the research interests of more groups.
   table 1 lists the different tracks that were in each trec  the number of groups that submitted runs to that track  and the total number of groups that participated in each trec. the tasks within the tracks offered for a given trec have diverged as trec has progressed. this has helped fuel the growth in the number of participants  but has also created a smaller common base of experience among participants since each participant tends to submit runs to fewer tracks.
   this section describes the tasks performedin the trec 1tracks. see the track reports later in these proceedings for a more complete description of each track. some of the descriptions given here are taken directly from the track overview papers.
1 the genomics track
the genomics track was a new track for trec 1. it is the first trec track devoted to retrieval within a specific domain  and one of the goals of the track is to see how exploiting domain-specific information improves retrieval effectiveness. the track contained two tasks  the primary task that was an ad hoc retrieval task and the secondary task that was an information extraction task.
   the scenario that motivated the primary task was that of a biological researcher or graduate student-that is  someone who already has considerable domain knowledge-confronted with the need to learn about a new gene very quickly. since nist assessors do not have the expertise to make judgments for the track  this first track made use of existing data that could serve as surrogate relevance judgments. the document collection consisted of approximately 1 medline records that were indexed between april 1  1 and april 1  1  and were donated to the track by the u.s. national library of medicine. a topic consisted of a gene name and an organism  and was to be interpreted as a request for the basic biology of the gene and its protein products in the designated organism. this is the information given by the gene reference into function  generif  data in the locuslink database  a database of biological information created by the national center for biotechnology information. the generif data were used as the relevance judgments for the track.
