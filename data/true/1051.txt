a wealth of information is available on the web. but often  such data are hidden behind form interfaces which allow only a restrictive set of queries over the underlying databases  greatly hindering data exploration. the ability to materialize these databases has endless applications  from allowing the data to be effectively mined to providing better response times in web information integration systems. however  reconstructing database images through restricted interfaces can be a daunting task  and sometimes infeasible due to network traffic and high latencies from web servers. in this paper we introduce the problem of generating efficient query covers  i.e.  given a restricted query interface  how to efficiently reconstruct a complete image of the underlying database. we propose a solution to the problem of finding covers for spatial queries over databases accessible through nearestneighbor interfaces. our algorithm guarantees complete coverage and leads to speedups of over 1 when compared against the naive solution. we use our case-study to illustrate useful guidelines to attack the general coverage problem  and we also discuss practical issues related to materializing web databases  such as automation of data retrieval and techniques which make it possible to circumvent unfriendly sites  while keeping the anonymity of the person performing the queries.
keywords
query coverage  dynamic content  restricted query interfaces  spatial queries  wrappers
1.	introduction
the hidden web has had an explosive growth as an increasing number of databases  from product catalogs and census data to celebrities' burial sites  go online. this information is often hidden  in the sense that it is placed behind form interfaces  and published on demand in response to users' requests. it is estimated that 1% of all the data in the web can only be accessed via form interfaces .
there are many reasons for providing such interfaces on the web. transferring a big file with all the information in a database can unnecessarily overload web servers  especially if users are interested in a small subset of the data . furthermore  accessing a particular record within a big file can be rather cumbersome. the alternative of giving direct access to the databases through expressive query languages such as sql  or xml-ql  is not practical  as these languages are too complex for casual web users. form interfaces are thus a good choice as they provide a very simple way to query  and filter  data.
simplicity however  comes at a price. form interfaces can be quite restrictive  disallowing interesting queries and hindering data exploration. in some cases  the restrictions are intentionally imposed by content providers to protect their data  e.g.  the book database and user comments are important ip for amazon.com  they are key to the business - and it is to amazon's benefit that others are not able to replicate their data . in other instances  the restrictions are just an annoyance and often the result of bad design. take for example the u.s. census bureau tract street locator  1 which requires a zip code and the first 1 letters of the street name  not allowing users to get information about all streets in a given zip code. as a result  there is a great wealth of information buried and apparently inaccessible in these web sites.
for answering queries that are not allowed through a given interface  or simply to get better performance  many applications can benefit from materializing a view of this hidden data. this approach is currently used by a number of services  such as for instance  comparison shopping engines and job search sites. in general  these services act in cooperation with web sites  which may give them a less restrictive back-door interface to the data. however  when web sites do not cooperate  an interesting problem is how reconstruct their databases using the restricted query interfaces readily available on the web.
the problem of querying web sites through restrictive query interfaces has been studied in the context of information mediation systems  see e.g.   . however  to the best of our knowledge  the problem of generating efficient query covers that retrieve complete images of the databases through restricted query interfaces has not been considered before.
in this paper we define and study the problem of generating efficient query covers for non-cooperative web sites: given a restricted query interface  how to efficiently reconstruct a complete image of the underlying database. we focus on an important subset of this problem  namely  finding efficient covers for spatial queries over databases that are accessible through nearestneighbor interfaces  and propose an algorithm that guarantees complete coverage and leads to speedups of over 1 when compared against the naive solution. we also discuss issues involved in building these database images and propose general guidelines.
1.	spatial covers through
nearest-neighbor interfaces
consider the following scenario. a large retail business is working on expansion plans  and needs to figure out where to place 1 new stores. a good strategy would be to discover where their competitors are located  and/or how their competitors have expanded over time. nowadays  these data are often available on web sites of the businesses themselves  as most retailers offer store locators which allow a potential customer to identify a location near them. one feature of many such locators is that they return only the closest k results to the query point  and in practice k ranges between 1 and 1. using this restrictive form interface 

http://tier1.census.gov/ctsl/ctsl.htm

figure 1: the top figure shows the location of stores from one u.s. retailer. the bottom figure shows some typical coverages queries  and the partial coverages while completing the cover for the region.
it can be tricky  and costly  to find the location of all the stores of a particular retailer in the u.s.. a natural solution would be to get a list of all the zip codes in the u.s.  submit one by one  and merge the results. however  given the high latencies of most web sites  executing up to 1 queries  1 per zip code  may take a long time; and this problem is compounded if these queries have to be executed periodically  for example  to track your competitor's expansion strategy .
given a query interface for nearest-neighbor queries of the type  find the k closest stores to a particular location   and a region   our goal is to minimize the number of queries necessary to find all the stores in . while in principle our coverage algorithm works for any number of dimensions  we focus our discussion on the two-dimensional case. a naive technique for finding a cover of a region is to simply break the region into small pieces  then perform one query for each piece  say  perform the query for the centroid of the region . this technique is quite popular on the web. for instance  often to find all the stores of a particular chain  one performs one query per zip code. while this does not guarantee coverage  since it might happen that more than k stores belong to a single zip code  in practice  this often produces satisfactory results. given that there are several thousand zip codes in the united states  this technique is likely to be very time consuming. also  this technique does not explore the data-sensitive nature of the k-nn queries being performed  because it does not take into account the radius1 returned by the query. as can be clearly seen in figure 1  the radius returned by a given query can vary substantially. our technique explores such variations to achieve an efficient solution. our algorithm is quite simple  and is composed of two parts:
 1  we use a spatial data structure to keep track of which parts of have already been covered by previous queries.
 1  at any given point in time  we use the coverage information obtained thus far to determine where to perform the next query as to minimize overlaps.
we use a simple greedy scheme for maximizing profit of queries.
we assume the best place to perform a query is the largest empty

 in practice  some sites do not return the radius directly  but given an address  location   it is possible to find the latitude and longitude  and compute the radius by performing extra queries  possibly in different web sites .
circle in the uncovered region. in practice  we use a quadtree  to mark which regions of have been covered  the unmarked regions are the regions of space which have not been seen  and for which no information is available . given a query point   the output of the query is a list of neighbors of . we simply mark on the quadtree the nodes inside a ball centered at   and of radius . see figure 1 for an example.
in effect  we find the largest uncovered quadtree node  and use its center as the next query point. note that we use the quadtree for two purposes: to determine coverages  and decide when we can stop; and to determine the next query. a nice feature of using recursive data structures such as the quadtree is that they make it easier to extend the technique to higher dimensions .
querying for the u.s. locations of one retailer that has about 1 stores using the zip code query  which returns 1 stores  and can be performed in 1 seconds for each query  requires over 1 queries  corresponding to the different zip codes  at a total cost of 1 seconds  or over three-and-a-half hours. using our simple scheme  we need only 1 queries at a total cost of 1 seconds. this is over 1 times faster  see  for more complete experimental results .
1.	acquiring web data
a critical requirement to reconstruct database images is to minimize the number of queries required to retrieve the data. in general  good strategy is to pose queries as general as possible  and some straightforward quidelines can be followed: leave optional attributes unspecified; for required attributes choose the most general options; and given a choice between required attributes  the attribute with smallest domain should be selected. however  for query interfaces that limit the number of returned answers  this strategy is not effective as one can not guarantee that all answers are retrieved. in the previous section we discussed how to address this problem by using spatial constraints to guarantee complete coverage.
there are a number practical issues involved in the web data acquisition process: in order to build a warehouse of hidden web data  several queries to one or more web sites may be required - as a result  these queries must be automated  their execution optimized  and possibly anonymized; and since the degree of difficulty of coverage queries is directly related to how data is accessed and represented  finding alternative means to access the data  or different representations  may simplify the problem. we refer the reader to  for more details on these issues.
it is worth pointing out that the problem of reconstructing database images through restricted interfaces is very broad  and in this paper we propose a solution to a small but important subset of the problem. we are currently investigating a comprehensive framework to address the general reconstruction problem.
