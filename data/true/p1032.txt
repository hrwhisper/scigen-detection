term weighting scheme  which has been used to convert the documents as vectors in the term space  is a vital step in automatic text categorization. in this paper  we conducted comprehensive experiments to compare various term weighting schemes with svm on two widely-used benchmark data sets. we also presented a new term weighting scheme tf.rf to improve the term's discriminating power. the controlled experimental results showed that this newly proposed tf.rf scheme is significantly better than other widely-used term weighting schemes. compared with schemes related with tf factor alone  the idf factor does not improve or even decrease the term's discriminating power for text categorization. categories and subject descriptors
i.1  document and text processing : document preparation
general terms
performance
keywords
term weighting schemes  text categorization  svm 1. introduction
　text categorization  the task of automatically assigning unlabelled documents into predefined categories  has been widely studied in the recent decades. many researchers have studied text categorization based on different term weighting schemes and different kernel functions of svms   . in   the authors pointed out that it is the text representation schemes which dominate the performance of text categorization rather than the kernel functions of svm. that is  choosing an appropriate term weighting scheme is more important than choosing and tuning kernel functions of svm for text categorization.
　however  even given these previous studies  we could not definitely draw a conclusion as to which term weighting scheme is better than others for svm-based text categorization  because we know that comparisons are reliable only when based on experiments performed by the same author under carefully controlled conditions.
　for this purpose  our study focused on the comparison of various term weighting schemes only. specifically  our benchmark adopted the linear svm algorithm and we used mcnemar's significance tests  to validate if there is significant difference between two term weighting schemes.
	1.	term weighting schemes
　we adopted a tabular representation similar to that one in  and compared the following ten term weighting schemes listed in table 1. most of these term weighting schemes have been widely used in information retrieval and text categorization and/or have shown good performance in practice. noted that other weighting schemes may exist  but these ten term weighting schemes were chosen due to their reported superior classification results or their typical representation when using support vector machines. for example  itf representation proposed by  is included because the experimental results show that when combined with linear kernel of svm it needs the minimum of support vectors  i.e. best generalization .
　from this table  we can find that the first four term weighting schemes are different variants of tf factor. then the next
1
table 1: summary of term weighting schemes
namedescriptionbinarybinary feature representation ence and 1 for absence  1 for pres-tftf onlylogtflog 1 + tf itf1   1/ 1 + tf idfidf alone  idf = log n/ni  tf.idfclassic tf.idflogtf.idflog 1 + tf .idftf.idf-probprobabilistic idf  actually is mate tf.term relevance the approxi-tf.chitf.χ1tf.rftf.relevance frequency is our new weighting scheme  rf=log 1 + ni/ni   four schemes are different variants of tf.idf. the tf.chi scheme is a typical representation which combines tf factor with one feature selection metric  here is χ1 . the last weighting representation is our newly presented scheme in order to improve the term's discriminating power for text categorization.
1.	comparative experiments 1	benchmark methodology
　to compare the performance between two term weighting schemes  we employed the mcnemar's significance tests  based on the micro-averaged precision/recall break-even point  which is defined as the value where recall equals to precision. the first data collection we used is from the top 1 largest categories of the reuters-1 corpus. one noticeable issue of reuters corpus is the skewed category distribution problem. for the second data collection  we randomly selected 1 samples per category among 1 categories from the 1 newsgroups corpus. compared with the skewed category distribution in the reuters corpus  the 1 categories in the 1 newsgroups corpus are uniform distribution.
1	results and discussion
　figure 1 and 1 depict the micro-averaged break-even point performance on the reuters and the 1 newsgroups data sets by using ten term weighting schemes at different number of features  respectively. to achieve high performance in terms with break-even point  different number of vocabularies is required for the two data sets; however  both of the best break-even points are achieved by using our newly presented scheme tf.rf. furthermore  these term weighting schemes have been shown consistent performance compared with the others on the two different data sets. the trends are distinctive that the tf.rf scheme has always been shown significant better performance than others. it is clearly to know that all the observations are supported by the following mcnemar's significance tests.
1.	conclusions
based on the experimental results  our conclusions are:
  our newly presented tf.rf scheme shows significant better performance than other schemes based on two
figure 1: results for the reuters-1 corpus

figure 1: results for the 1 newsgroups corpus
widely-used data sets with different category distributions
  the schemes related with term frequency alone  such as tf  logtf  itf show rather good performance but still worse than the tf.rf scheme
  the idf factor  taking the collection distribution into consideration  does not improve or even decrease the term's discriminating power for text categorization
  the binary and tf.chi representations significantly underperform the other term weighting schemes
