previous examinations of search in textual archives have assumed that users first retrieve a ranked set of documents relevant to their query  and then visually scan through these documents  to identify the information they seek. while document scanning is possible in text  it is much more laborious in speech archives  due to the inherently serial nature of speech. yet  in developing tools for speech access  little attention has so far been paid to users' problems in scanning and extracting information from within  speech documents .
we demonstrate the extent of these problems in two user studies. we show that users experience severe problems with local navigation in extracting relevant information from within  speech documents .  based on these results  we propose a new user interface  ui  design paradigm: what you see is  almost  what you hear   wysiawyh  - a multimodal method for accessing speech archives. this paradigm presents a visual analogue to the underlying speech  enabling visual scanning for effective local navigation. we empirically evaluate a ui based on this paradigm. we compare our wysiawyh ui with a visual  tape recorder   in relevance ranking  fact-finding  and summarization tasks involving broadcast news data. our findings indicate that an interface supporting local navigation multimodally helps relevance ranking and fact-finding  but not summarization. we analyze the reasons for system success and identify outstanding research issues in ui design for speech archives.
keywords
speech indexing and retrieval  field/empirical studies of the information seeking process  comparing interfaces for information access  user studies.
1. introduction
recently  there have been major increases in the amounts of data stored in digital speech archives. broadcasting companies have made radio programs available  public records such as the us congressional debates are being archived  and large private archives of audio conferences and voicemail can be cheaply stored for subsequent reference. such archives are potentially highly valuable  as speech has been shown to be both ubiquitous and critical for the execution of many workplace tasks  1 . however  these archives are currently under-utilized  in large part due to the absence of effective user-centered techniques for archival access. although a number of speech retrieval systems have been built for trec   these systems have generally paid little attention to user requirements  or to the development of uis. we consequently lack systematic information about: the processes by which people currently access information from speech archives  general principles for designing uis to speech archives  and methods for evaluating such interfaces.  this study addresses those issues.
a natural starting point for identifying how people might access information from speech archives is the large body of research on text retrieval. yet with few exceptions  such as hearst   and the interactive track of trec   text retrieval research has focused on document search  where the retrieval engine's goal is simply to identify a ranked set of documents relevant to the user's query. subsequent scanning within these documents to actually locate information  e.g. extracting specific facts  or identifying relevant paragraphs  are behaviors generally not addressed. it is usually assumed that  for more detailed information seeking  users can easily scan and browse the retrieved texts  although hearst's  tilebars is an important exception .
in the context of a speech corpus  however  it is apparent that uis supporting only document search are insufficient  because of the problems for users of scanning and browsing speech data. a story in the nist broadcast news corpus  for example  can be 1 minutes long. given the sequential nature of speech  it is extremely laborious to scan through multiple speech stories to obtain an overview of their contents   or to identify specific information of direct relevance within speech  1 . interfaces for accessing speech archives therefore need to support local navigation within  speech documents   as well as relevance based search.
the structure of the paper is as follows: we present two user studies of voicemail that:  a  examine user problems of local navigation in accessing speech; and  b  identify the strategies users employ to overcome these problems. from these studies we derive a new paradigm for the design of speech access systems: what you see is  almost  what you hear  wysiawyh . this

permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.  to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. sigir '1   1 berkley  ca  usa
copyright 1 acm 1-1/1 . . . $1
paradigm presents a visual analogue to the underlying speech  enabling visual scanning for effective local navigation. we describe a new ui designed according to that paradigm. the interface is to scan  a system that accesses a broadcast news archive. we empirically evaluate a ui based on this paradigm. we compare the scan ui with a visual  tape recorder   in relevance ranking  fact-finding  and summarization tasks involving broadcast news data. our findings indicate that an interface supporting local navigation multimodally helps relevance ranking and fact-finding  but not summarization. we analyze the reasons for system success and identify outstanding research issues in ui design for speech archives.
1. local navigation: scanning and information extraction strategies
to identify how users currently browse and search speech corpora  we conducted two studies of voicemail access. voicemail represents a real-world domain with experienced users who have evolved strategies for dealing with important speech data. it is therefore a good starting place for studying local navigation in speech.
we examined local navigation strategies under two controlled laboratory conditions. we gave users two types of graphical interfaces to a voicemail archive of 1 messages whose average length was about 1s . users were given two types of access tasks  derived from interviews and surveys conducted with voicemail users . the tasks were to summarize a message or to extract specific information  e.g. a name  from a message. subjects experienced serious problems with local navigation  even for a small archive of short messages. they learned the global structure of the archive but were unable to remember specific message contents. information extraction tasks were extremely hard  particularly when multiple facts had to be retrieved: users repeatedly replayed material they had just heard  suggesting problems with remembering local message structure.  in a posthoc memory task  users also showed poor recall for message contents.
a second study  used a combination of interview and survey methods to investigate voicemail retrieval strategies. 1 high volume users  recipients of more than 1 messages per day  experienced two main problems in accessing voicemail:  a  scanning - navigating to the correct message or relevant part of the message;  b  information extraction - accessing specific facts from within the message. note-taking was a key processing strategy  with 1% of users reporting that they 'almost always' took notes. users described two different note-taking strategies:  a  full-transcription  attempting to produce a verbatim transcript of messages to avoid later replayings;  b  message indexing  abstracting only key points  such as caller name  caller number  reason for calling  important dates/times and action items . typically  users kept originals as a backup  in case their notes were insufficient.  many users kept sequential notes  so they could use this temporal index to locate a message in their archive. finally we identified cues used in processing voicemail  such as the importance of intonation to indicate or clarify speaker intentions.
both studies illustrate the problems of local navigation with users finding it hard to scan messages and extract information from within messages. the different note-taking strategies indicate the methods that users currently employ for local navigation: indexing provides an abstract overview of each message  presenting its key points and serves as a guide for archive scanning  i.e. locating one message in relation to others. full-transcription provides a  laborintensive  textual rendering of speech to facilitate subsequent information extraction. user comments indicate  however  that they prefer not to rely wholly on a text transcription  being concerned about losing the extra intonational information provided by the original speech.
these data together suggest general principles for improved uis to speech archives  and indicate a potential solution to the problem of local navigation. by taking notes  users construct visual analogues of voicemail messages  in the form of transcripts and indices  allowing them to visually scan and index into the corresponding speech. we therefore need interfaces that:  a  address the problem of local navigation;  b  provide visual analogues to underlying speech. furthermore   c  these interfaces must be multimodal: people want access to the original speech  as well as this visual information.
1. the scan user interface
while the ir literature focuses on global search  our initial experiments show an additional critical role for local scanning and information extraction. both tasks are particularly difficult for speech data. the scan  spoken content-based audio navigation  ui for accessing broadcast news data attempts to support both local and global navigation  see figure 1 .  the underlying system provides access to a corpus of 1 hours of broadcast news from the nist/darpa test set . this is a set of recorded radio and tv news. it is made up of programs such as current affairs discussions  breaking news and headlines. the stations and programs include: npr: all things considered  abc: world news tonight  cnn: early primetime news  npr market place  figure 1 provides more instances of programs .
the user interface consists of three elements depicted in figure 1  namely search  overview and transcript. search is intended to support global access to  speech documents   while overview and transcript elements address local navigation. we describe each ui element in turn.
1 search
the scan interface's search component provides access to sets of relevant  speech documents  in response to user queries. we identify these sets by applying information retrieval methods to errorful textual transcriptions of each  document   that have been generated by automatic speech recognition  asr .  to generate the asr transcriptions  we first segment the speech into paratones   audio paragraphs    using acoustic information   classify the recording conditions for every paratone  narrowband or other  and apply asr to each.  we combine results for each paratone so that for every  speech document  we have a corresponding  errorful  asr transcript. terms in each transcript are then indexed for retrieval by the smart ir engine  1  1 . when the user types a query   what is the status of the trade deficit with japan    into the text box at the top of the browser  labeled query   the system searches the errorful transcripts for relevant documents. the search results are depicted in the results panel immediately below  as a relevance-ranked list of 1  speech documents   corresponding to the 1 most relevant news stories. we also present additional information about each news story  including program name  date  story number  to distinguish the multiple stories occurring within a program   relevance score  length  in seconds   and total hits  number of instances of query words . the user selects a story by clicking on it.

figure 1: the scan user interface1 overview
the overview component provides high level visual information about individual  speech documents . users can rapidly scan this to locate potentially relevant audio regions. it displays which query terms appear in each paratone of the story. each query word is color coded  and each paratone is represented by a vertical column in a histogram. thus the word  trade  occurs in the second paratone and hence in the second histogram column. the width of the histogram bar represents the relative length of that paratone. the height of each bar in the histogram represents the overall query word weights  the term weighted indices of the query terms for the corpus normalized for the paratone length . different query terms are combined within the same histogram column  so that column 1 in the overview in figure 1 contains instances of each of the words   trade    japan   and  deficit . the co-occurrence of these terms suggests a potentially highly relevant region within the  document . users can also locate specific query terms by examining color distributions across paratones. a similar technique is used for textual documents in . users can directly access the speech for any paratone by clicking on the corresponding column of the histogram. selecting a column initiates play from the start of the corresponding paratone. this component also supports global comparison between  speech documents . comparing overviews for multiple documents can reveal which  documents  have a greater density of query terms and hence contain potentially more relevant regions.
1 transcript
the scan asr transcript supports information extraction  providing detailed  if sometimes inaccurate  information about the contents of a story. these are the same asr transcripts that were used to support search. the transcript panel displays a transcription of the selected story. the transcript in figure 1 has been scrolled so that the first visible paragraph does not correspond to the start of the  speech document . because the transcript has been generated automatically  it usually contains errors  in paragraph 1 of the transcript in figure 1   to normalize  is transcribed as  the normal eyes  . when the speech recognizer makes errors  they are deletions  insertions and substitutions of the recognizer's vocabulary  rather than the types of non-word errors that are generated by ocr. if the target speech contains large numbers of words that are not in the recognizer's vocabulary  the out-of-vocabulary problem   this leads to multiple word substitution errors. in addition  recognition errors often cascade: the underlying language model explicitly models inter-word relationships  so that one misrecognition may lead to others. finally function words tend to be misrecognized more than content words.
query terms in the transcript are highlighted and color-coded  using the same coding scheme used in the overview panel  e.g. the word  trade  is highlighted in paragraph 1 . users can play a given paratone by clicking on the corresponding paragraph in the transcript.
the transcript has several potential functions. first  in regions where it is mostly accurate  users can find relevant information simply by reading -- without listening to the audio. like the overview  it supports rapidly visual scanning to find relevant regions in the audio. the transcript also provides local contextual information: users can decide whether to play a particular paratone by reading surrounding paragraphs to determine its likely relevance. finally  overall transcript quality can help users assess the likely accuracy of transcript  search and overview information. for example  bizarre phrases like  buster and those ties and assess state....   beginning of paragraph 1  indicate the transcript is inaccurate. they also suggest that query terms in the overview may have been misrecognized.  if errors are prevalent  then users may rely more on the speech than transcripts.
1 player
the current scan interface also provides random access to  speech documents  using a simple play bar representing a single story. the ui is analogous to a tape-recorder. users can insert the cursor at any point in the bar to indicate where to begin playing. start and stop audio buttons are available to control play and may also be used for the overview and transcription panels. the player is not visible in figure 1  but users can scroll down to it below the transcript.
1  what you see is  almost  what you
hear  principles for speech retrieval uis
together  the elements of the ui support a new paradigm for speech retrieval interfaces:  what you see is  almost  what you hear   wysiawyh . a key principle of this design paradigm is to provide a visual analogue to the underlying speech  using text formatting  such as headers and paragraphs  to exploit well understood text conventions in order to present useful local context for speech browsing. by depicting the abstract structure of  audio documents  in the overview  and by providing a formatted transcript  we hope to make visual scanning and information extraction fast and effective  addressing the problems of local navigation identified in our user studies.
while we believe this visual information will be helpful in local navigation  however  we do not think it will eliminate the need to access the original speech. there are two reasons why visual information alone is insufficient. first  asr errors mean that the transcript frequently diverges from the underlying speech. there were about 1% word errors for the scan corpus  and it seems unlikely that error-free asr will be available within the foreseeable future. this is especially true for domains like news that have spontaneous speech  unforeseen recording conditions  and large numbers of out-of-vocabulary items  because of constantly changing content . a second reason for needing the original speech is the importance of intonation. our voicemail users stressed the importance of preserving original speech messages  so that they could fully interpret their handwritten notes. voice quality and intonational characteristics are lost in transcription  and intonational variation has been widely shown to change even the semantics of simple sentences . so  transcription alone is unlikely to be an effective substitute for multimodal access.
1. evaluation study
1 method
to test our hypotheses about the usefulness of our wysiawyh paradigm in supporting local browsing  we compared the scan browser  with a control interface that supported only search. this control gave users only the search panel and the player   tape recorder   component described above.  users used the search panel to find stories  as with the scan browser  but had only the random access player   tape-recorder   for browsing within  documents .
from our previous user interviews and experiments  1   we developed a task taxonomy for retrieval. we wanted to compare different retrieval situations along several important task dimensions  including: making global judgments about sets of  speech documents   locating specific information from within a  document   and extracting the overall gist of a  document . we therefore collected data experimentally to compare the two interfaces on the following 1 tasks:
  relevance judgments - compare five news stories to determine which was most relevant to a given topic  e.g.  how good was valujet's safety record prior to the florida accident   ;
  fact-finding - extract factual information from a story to answer a specific question  e.g.  who starred in the broadway musical 'maggie flynn'   ;
  summarization - produce a 1 sentence summary of a given story  e.g. for a story on a bombing in manchester .
 because our focus was on browsing behavior rather than search  we wanted all users to access the same set of stories. so  rather than spontaneously generating their own queries  users were given the queries to type in to the search panel for each task. in the relevance task  users were asked to consider five stories  but for the fact-finding and summary tasks  they only accessed one story. we attempted as far as possible to normalize story length across the 1 tasks.
 the experimental design was randomized within subjects. twelve subjects were given a total of 1 questions each  1 of each of the 1 task types . for half the questions they used the scan browser  and the control browser for the other half. for each question we measured outcome information: time to solution and quality of solution  as assessed by two independent judges . we also collected information about the processes by which people answered each question: number  type  and duration of browsing and play operations. we also collected subjective data. after each question we had subjects judge task difficulty. because we were interested in browsing strategies and processes  we encouraged subjects to  think aloud  as they carried out the tasks  and we recorded their statements. we also administered a post-test survey probing relative task difficulty  how well the scan ui supported each task  overall browser utility  how the browser might be improved  quality of the transcript  and what factors led users to evaluate the transcript positively or negatively.
variablescan
 mean control
 mean prediction confirmed outcometime to solution  secs. 11yessolution quality  maximum score = 1 11yessubjective ratingsperceived 	task 	difficulty
 scale of 1  1 is  hard  11yesperceived browser utility  scale of 1  1 is  very useful  11yesprocess measuresnumber of operations11noamount of audio played
 secs. 11yestable 1: effects of browser type on local navigation
1 	 hypotheses
  supporting local navigation: we expected the scan browser to support local navigation better than the control for the two outcome measures  time to solution and solution quality . users should evaluate tasks as easier using the scan browser  and rate the scan browser as better overall. we expected our process measures to show the scan browser supported more efficient retrieval: users should require fewer operations to complete tasks  and play less audio with the scan browser;
  task differences: in terms of solution time  solution quality  and perceived task difficulty  we expected the fact-finding task to be easier than the summary task  which in turn would be easier than the relevance task  based on the amount of information users had to access to perform the task.  factfinding requires access to part of a single document  whereas summaries require access to an entire document  and relevance judgements require access to multiple documents;
  asr transcript quality: we predicted that asr transcript quality  as assessed by word error rates  would influence performance. high quality asr should improve solution quality  reduce solution time  reduce perceived task difficulty and reduce the amount of speech played.
1 results
1.1 supporting local navigation
users performed better with the scan browser than the control. we conducted multiple independent anovas with users  task type  and browser type as the independent variables. the dependent variables in each anova were: time to solution; solution quality; perceived task difficulty; users' rating of browser usefulness. results are summarized in table 1  and the data for solution quality  solution time and perceived task difficulty depicted in figures 1  1 and 1. our predictions were confirmed for outcome measures: solution time  f 1  = 1  p  
1   solution quality  f 1  = 1  p   1   and also for subjective ratings: perceived task difficulty  f 1  = 1  p   1   perceived browser utility  f 1  = 1  p   1 .
        figure 1: effects of browser and task on solution quality
qualitative data: how did the scan ui provide support for local navigation  with only the  tape-recorder  browser  users reported several problems. although listening to an entire story was highly tedious  users were forced to do so because they lacked clues to information structure:  it's so painful to try and find specific information that i'm going to surrender and listen to the whole thing . lack of structural information meant they could not skip over parts of the story  or they ran the risk of missing significant information.  it doesn't help to skip forward because i don't know where this section ends  so it means that i have to listen to the whole thing . users also complained that with the  tape-recorder  that on occasion audio came  too fast   so they had to listen to passages multiple times.  i actually played the answer but i didn't hear it. i realized later that i'd heard it and then had to go back . going back to relevant regions was also a major problem:  i missed an explanation and i knew that it was slightly before halfway so i moved the cursor one third of the way back and listened to all that again .
the scan ui addressed these problems. there were 1 ways that it reduced the amount of speech subjects played. the ui enabled global relevance judgments based on the overview or the transcript alone:  listening is clearly too slow - i don't want to listen to every story  so i'm just looking here  in the overview  for stories that talk about the topic in a broad sense . if the transcript was high quality  users could avoid listening entirely:  i'm just reading the transcripts to determine relevance - even without listening to it  i'm sure that story 1 is the best match . even errorful transcripts  however  like the overview  gave users greater precision in judging which parts of the  speech document  they needed to play.  for scanning  the transcript is really useful. also i can just buzz around the overview to find when i'm in the right area ... i'm going to play the places where the transcript is awful .
the scan interface was often used multimodally  with simultaneous playing and reading. people scrolled forward and backward around the paratone they were playing  reading the surrounding transcript paragraphs to obtain context for what they were hearing. on other occasions they would listen to an important part of the  speech document   e.g. the beginning  to set some context  while scrolling the transcript to visually scan the remainder of the story.  i was trying to get the story opening through audio and then look ahead for the rest of it in the transcript .
process data: the process data  number  type and duration of play operations  support these qualitative observations. our prediction that the scan ui would prove more efficient was confirmed. with the scan browser  people played much less speech  f 1 =1  p   1 . however  contrary to our expectations  we found that subjects used more operations with the scan browser than with the control  f 1  =1  p   1 . user behavior suggested the reason: in the control condition  with no effective means of scanning  users often played a  speech document  from beginning to end. in contrast  scan users might play brief parts of several regions within a  document  to quickly identify relevant portions  and then sometimes listen to these multiple times.
1.1 task differences
the results did not support our predictions. there were main effects for task for two of our dependent variables: solution time  f 1  = 1  p   1   solution quality  f 1  = 1  p  
1   but not for perceived task difficulty  f 1  = 1  p   1 . the effects of task and the impact of the browser  for each of these variables are shown in figures 1  1 and 1. planned comparisons showed that solution time was lower for fact-finding than relevance and both were faster than summaries. for solution quality  fact-finding was equivalent to relevance judgments  and both were better than summaries. furthermore  there were interactions between task and browser: for solution time  f 1  = 1  p   1   and solution quality  f 1  = 1  p   1   with the scan browser producing higher quality  quicker solutions for fact-finding and relevance tasks but not for summaries. for perceived task difficulty  there was a significant interaction  f 1  = 1  p   1   with the browser only affecting the  relevance tasks.
figure 1: effects of browser and task on solution time
why was the summary task so hard  and why did scan local browsing capabilities fail to improve performance   from user behavior and comments  we conclude that the scan overview failed to improve the summarization task for two reasons. first  the even distribution of query terms that often occurred throughout a story provided no clue to which particular termhighlighted region provided the best summary information. in consequence  users were unable to focus their activities on specific parts of the story. second  highly relevant regions for summarizing were sometimes not highlighted at all because synonyms were used instead of the actual query terms:  i'm not going to zoom in on various paragraphs  from the overview  because the whole story is about the topic . as for the transcripts  most users felt that they did not offer accurate enough information for summarizing.  first i thought the transcript would help  but it didn't get the slant of the story. the transcript only helps with information extraction. ... to get the whole slant i need to listen to it all .  the transcription errors also disrupted a smooth reading of the story.  i just couldn't parse it  everything was so disjointed i couldn't make sense of it .
1.1 effects of asr quality
asr quality varied significantly among the  documents  retrieved  from a maximum of 1% words correctly recognized to a minimum of 1%  with a mean of 1%. we correlated asr transcript quality  percentage of words correctly recognized  with process and outcome measures for summary and fact-finding tasks.. it is unclear how to allocate a single asr quality score to multiple documents  so the relevance task was not included in this analysis. we also restricted the analysis to the scan condition - the only one in which the transcript was available.
as we predicted  for fact-finding  better quality asr led to higher quality solutions  r 1  = 1  p   1   and there was a trend towards lower perceived task difficulty  r 1  = 1  p =
1 . user comments also suggested that with higher quality transcriptions  they were able to extract more information from the transcript alone  reducing the amount of speech they needed to play  and allowing them to be more precise about what they played. where transcription quality was poor  they were forced to do more listening:  i wanted to scan the transcript but i found a massive number of errors in the speech recognition  so i decided to listen . however  we could find no objective evidence for reduced playing with accurate asr  r 1  = 1  p   1   nor were users faster to solve the task  r 1  = 1  p   1 . there were also no effects of asr quality on any measure  for the summary task. this is consistent with our earlier finding - that the scan ui did not help with the summary task.
why did transcript quality not affect outcome and process measures more directly  consistent with our earlier results we found some task-specific effects  with no influence of transcript quality on summaries. it may also be that our asr accuracy measure was too crude to affect user behavior. our measure was for overall asr quality for entire  documents . it may be  however  that we need to measure asr in specific regions of the  document . for example  if the asr at the beginning of the  document  is accurate  this not only gives the user useful contextual information for understanding the remainder of the  document   but also motivates them to continue using the transcript  as opposed to switching to listening to the story directly. future work needs to devise more local measures of asr quality to examine such effects.
1. conclusions
this research has identified a new problem in uis for speech retrieval  i.e. support for local navigation. we have outlined a new paradigm for interfaces to address this  wysiawyh:  what you see is  almost  what you hear    where a multimodal interface provides a visual analogue and straightforward indexing into the underlying speech. our user evaluation showed that we made have considerable progress in addressing the problem of local navigation. comparing the wysiawyh-based interface with a simple visual tape-recorder interface showed superiority for wysiawyh for fact-finding and relevance judgment tasks. the overview and transcript elements of the scan ui offer multiple methods for users to reduce the problems of timeconsuming serial access to speech. the interface allows users to: use overview and transcript information to avoid playing entire  speech documents  they judge to be irrelevant; extract information from the transcript without playing anything at all; and  finally  if playing is necessary  focus on paratones they judge to be most relevant to their task. users can also access information multimodally by listening to relevant paratones and reading the relevant transcript simultaneously.
how does wysiawyh relate to other interface work on speech access  similar techniques  using visual handwritten notes to index into recorded speech  have been successful for accessing personal speech data  1 1 . several video retrieval systems have presented key video frames to provide visual overviews to video programs  1 . other broadcast news and meeting recording systems present high level topic or speaker switching information  1 1 . however  with the exception of  these latter uis have not been evaluated on access tasks with real users.

figure 1: effects of browser and task difficulty on perceived task difficulty
we find it significant in our studies that the multimodal scan interface is beneficial only for certain tasks  such as fact-finding and relevance ranking. for these tasks  users were able to exploit the overview and transcript to extract local facts or to make global judgments. however the summary task required access to the specific content of an entire document. all sections of the document were potentially relevant to the summary. it was therefore difficult to judge what was important information without a good transcription of the document  or actually listening to what was said. in general  the transcript was too inaccurate to allow users to identify important summary information  and they were forced to play entire documents. even when the word accuracy rate is as high as 1%  this problem persists. how might we then improve summarization   first  of course  even higher word accuracy might help  but it is unclear from our data how close to perfect a transcript must be for subjects to trust it fully for summarization. second  automatic speech summarization might provide a starting point for human summary creation  although  again  it is not clear how 'good' this must be  or how people would subjectively judge its quality for this purpose . it may be that  given the laborious nature of speech access  even poor automatic summaries are preferable to playing entire stories.  third  skimming techniques  that use acoustic information to identify areas of high relevance might provide shortcuts to summarization similar to automatic summarization  with comparable potential weaknesses. lastly  we might explore ui techniques that control playback by allowing speeded up playback  or access using structural properties  e.g. speaker or topic shifts .
finally our data have implications for basic measures and evaluations in information retrieval. the problem of local navigation arises from the fact that supporting relevance at the  speech document  level is insufficient to address retrieval problems with speech. we need to move away from a purely document-level view to successfully address speech access. our studies showed generally better performance with the scan ui  which provides within-story relevance information. this indicates that we need to generate relevance metrics that operate within stories  to support notions of local relevance. we also need to devise new evaluation tasks  such as the ones we used here  that draw on the requirements for local information. finally for speech retrieval  our data suggest that traditional document level relevance judgments may be affected by document length. users stated a preference for shorter documents  because of the tedium of accessing speech. if future experiments support this observation  then future document level metrics may need to modify relevance metrics depending on the retrieval medium  with speech retrieval relevance showing greater weighting for shorter documents.
