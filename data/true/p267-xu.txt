in this paper  we propose a novel document clustering method based on the non-negative factorization of the termdocument matrix of the given document corpus. in the latent semantic space derived by the non-negative matrix factorization  nmf   each axis captures the base topic of a particular document cluster  and each document is represented as an additive combination of the base topics. the cluster membership of each document can be easily determined by finding the base topic  the axis  with which the document has the largest projection value. our experimental evaluations show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering methods not only in the easy and reliable derivation of document clustering results  but also in document clustering accuracies.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-clustering
general terms
algorithms
keywords
document clustering  non-negative matrix factorization
1. indroduction
¡¡document clustering techniques have been receiving more and more attentions as a fundamental and enabling tool for efficient organization  navigation  retrieval  and summarization of huge volumes of text documents. with a good document clustering method  computers can automatically organize a document corpus into a meaningful cluster hierarchy  which enables an efficient browsing and navigation of
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1 july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
the corpus. an efficient document browsing and navigation is a valuable complement to the deficiencies of traditional ir technologies. as pointed out in   the variety of information retrieval needs can be expressed by a spectrum where at one end is a narrowly specified search for documents matching the user's query  and at the other end is a broad information need such as what are the major international events in the year 1  or a need without well defined goals but to learn more about general contents of the data corpus. traditional text search engines fit well for covering one end of the spectrum  which is a keyword-based search for specific documents  while browsing through a cluster hierarchy is more effective for serving the information retrieval needs from the rest part of the spectrum.
¡¡in recent years  research on topic detection and tracking  document content summarization and filtering has received enormous attention in the information retrieval community. topic detection and tracking  tdt  aim to automatically detect salient topics from either a given document corpus or an incoming document stream and to associate each document with one of the detected topics. the tdt problems can be considered as a special case of the document clustering problem and actually most of the tdt systems in the literature were realized by adapting various document clustering techniques. on the other hand  document summarization is intended to create a document abstract by extracting sentences/paragraphs that best present the main content of the original document. again  many proposed summarization systems employed clustering techniques for identifying distinct content  and finding semantically similar sentences of the document.
¡¡document clustering methods can be mainly categorized into two types: document partitioning  flat clustering  and agglomerative  bottom-up hierarchical  clustering. although both types of methods have been extensively investigated for several decades  accurately clustering documents without domain-dependent background information  nor predefined document categories or a given list of topics is still a challenging task.
¡¡in this paper  we propose a novel document partitioning method based on the non-negative factorization of the term-document matrix of the given document corpus. in the latent semantic space derived by the non-negative matrix factorization  nmf    each axis captures the base topic of a particular document cluster  and each document is represented as an additive combination of the base topics. the cluster membership of each document can be easily determined by finding the base topic  the axis  with which the document has the largest projection value. our method differs from the latent semantic indexing method based on the singular vector decomposition  svd  and the related spectral clustering methods in that the latent semantic space derived by nmf does not need to be orthogonal  and that each document is guaranteed to take only non-negative values in all the latent semantic directions. these two differences bring about an important benefit that each axis in the space derived by the nmf has a much more straightforward correspondence with each document cluster than in the space derived by the svd  and thereby document clustering results can be directly derived without additional clustering operations. our experimental evaluations show that the proposed document clustering method surpasses svd- and the eigenvector-based clustering methods not only in the easy and reliable derivation of document clustering results  but also in document clustering accuracies.
1. related works
¡¡generally  clustering methods can be categorized as agglomerative and partitional. agglomerative clustering methods group the data points into a hierarchical tree structure  or a dendrogram  by bottom-up approach. the procedure starts by placing each data point into a distinct cluster and then iteratively merges the two most similar clusters into one parent cluster. upon completion  the procedure automatically generates a hierarchical structure for the data set. the complexity of these algorithms is o n1 logn  where n is the number of data points in the data set. because of the quadratic order of complexity  bottom-up agglomerative clustering methods could become computationally prohibitive for clustering tasks that deal with millions of data points.
¡¡on the other hand  document partitioning methods decompose a document corpus into a given number of disjoint clusters which are optimal in terms of some predefined criteria functions. partitioning methods can also generate a hierarchical structure of the document corpus by iteratively partitioning a large cluster into smaller clusters. typical methods in this category include k-means clustering   probabilistic clustering using the naive bayes or gaussian mixture model  1  1   etc. k-means produces a cluster set that minimizes the sum of squared errors between the documents and the cluster centers  while both the naive bayes and the gaussian mixture models assign each document to the cluster that provides the maximum likelihood probability. the common drawback associated with these methods is that they all make harsh simplifying assumptions on the distribution of the document corpus to be clustered. k-means assumes that each cluster in the document corpus has a compact shape  the naive bayes model assumes that all the dimensions of the feature space representing the document corpus are independent of each other  and the gaussian mixture model assumes that the density of each cluster can be approximated by a gaussian distribution. obviously  these assumptions do not often hold true  and document clustering results could be terribly wrong with broken assumptions.
¡¡there have been research studies that perform document clustering using the latent semantic indexing method  lsi  . this method basically projects each document into the singular vector space through the svd  and then conducts document clustering using traditional data clustering algorithms  such as k-means  in the transformed space. although it was claimed that each dimension of the singular vector space captures a base latent semantics of the document corpus  and that each document is jointly indexed by the base latent semantics in this space  negative values in some of the dimensions generated by the svd  however  make the above explanation less meaningful.
¡¡in recent years  spectral clustering based on graph partitioning theories has emerged as one of the most effective document clustering tools. these methods model the given document set using a undirected graph in which each node represents a document  and each edge  i j  is assigned a weight wij to reflect the similarity between documents i and j. the document clustering task is accomplished by finding the best cuts of the graph that optimize certain predefined criterion functions. the optimization of the criterion functions usually leads to the computation of singular vectors or eigenvectors of certain graph affinity matrices  and the clustering result can be derived from the obtained eigenvector space. many criterion functions  such as the average cut   average association   normalized cut   min-max cut   etc  have been proposed along with the efficient algorithms for finding their optimal solutions. it can be proven that under certain conditions  the eigenvector spaces computed by these methods are equivalent to the latent semantic space derived by the lsi method. as spectral clustering methods do not make naive assumptions on data distributions  and the optimization accomplished by solving certain generalized eigenvalue systems theoretically guarantees globally optimal solutions  these methods are generally far more superior than traditional document clustering approaches. however  because of the use of singular vector or eigenvector spaces  all the methods in this category have the same problem as lsi  i.e.  the eigenvectors computed from the graph affinity matrices usually do not correspond directly to individual clusters  and consequently  traditional data clustering methods such as k-means have to be applied in the eigenvector spaces to find the final document clusters.
1. the proposed method
¡¡assume that a document corpus is comprised of k clusters each of which corresponds to a coherent topic. each document in the corpus either completely belongs to a particular topic  or is more or less related to several topics. to accurately cluster the given document corpus  it is ideal to project the document corpus into a k-dimensional semantic space in which each axis corresponds to a particular topic. in such a semantic space  each document can be represented as a linear combination of the k topics. because it is more natural to consider each document as an additive rather than subtractive mixture of the underlying topics  the linear combination coefficients should all take non-negative values. furthermore  it is also quite common that the topics comprising a document corpus are not completely independent of each other  and there are some overlaps among them. in such a case  the axes of the semantic space that capture each of the topics are not necessarily orthogonal. based on the above discussions  we propose to use non-negative matrix factorization  nmf  to find the latent semantic structure for the document corpus  and identify document clusters in the derived latent semantic space.
¡¡in fact document clustering methods based on the lsi and the spectral clustering  as described in section 1  also strive to find the latent semantic structure for the document corpus by computing singular vectors or eigenvectors of certain matrices. the derived latent semantic space is orthogonal  and each document can take negative values in some directions in the space.

figure 1: illustration of the differences between nmf and lsi.
¡¡in contrast  nmf does not require the derived latent semantic space to be orthogonal  and it guarantees that each document takes only non-negative values in all the latent semantic directions. these two characteristics make the nmf superior to the lsi and spectral clustering methods because of the following reasons  see figure 1 . first  when overlap exists among clusters  nmf can still find a latent semantic direction for each cluster  while the orthogonal requirement by the svd or the eigenvector computation makes the derived latent semantic directions less likely to correspond to each of the clusters. second  with nmf  a document is an additive combination of the base latent semantics  which makes more sense in the text domain. third  as the direct benefit of the above two nmf characteristics  the cluster membership of each document can be easily identified from nmf  while the latent semantic space derived by the lsi or the spectral clustering does not provide a direct indication of the data partitions  and consequently  traditional data clustering methods such as k-means have to be applied in this eigenvector space to find the final set of document clusters. the following subsections provide the detailed descriptions of the proposed document clustering method.
1 document representation
¡¡we use the weighted term-frequency vector to represent each document. let w = {f1 f1 ... fm} be the complete vocabulary set of the document corpus after the stopwords removal and words stemming operations. the termfrequency vector xi of document di is defined as

where tji  idfj  n denote the term frequency of word fj ¡Ê w in document di  the number of documents containing word fj  and the total number of documents in the corpus  respectively. in addition  xi is normalized to unit euclidean length. using xi as the i'th column  we construct the m¡Án term-document matrix x. this matrix will be used to conduct the non-negative factorization  and the document clustering result will be directly obtained from the factorization result.
1 document clustering based on nmf
¡¡nmf is a matrix factorization algorithm that finds the positive factorization of a given positive matrix  1  1  1 . assume that the given document corpus consists of k document clusters. here the goal is to factorize x into the non-negative m ¡Á k matrix u and the non-negative k ¡Á n matrix vt that minimize the following objective function:
		 1 
where  ¡¤  denotes the squared sum of all the elements in the matrix. the objective function j can be re-written as:
1 t	t t
	j	=	tr  x   uv   x   uv    
1
1
	=	tr xxt   1xvut + uvtvut 
1
1tr xvut  + tr uvtvut    1 
where the second step of derivation uses the matrix property tr ab  = tr ba . let u =  uij   v =  vij   u =  u1 u1 ... uk . the above minimization problem can be restated as follows: minimize j with respect to u and v under the constraints of uij ¡Ý 1  vxy ¡Ý 1  where 1 ¡Ü i ¡Ü m 
1 ¡Ü j ¡Ü k  1 ¡Ü x ¡Ü n  and 1 ¡Ü y ¡Ü k. this is a typical constrainted optimization problem  and can be solved using the lagrange multiplier method. let ¦Áij and ¦Âij be the lagrange multiplier for constraint uij ¡Ý 1 and vij ¡Ý 1  respectively  and ¦Á =  ¦Áij   ¦Â =  ¦Âij   the lagrange l is 
	l = j + tr ¦Áut  + tr ¦Âvt 	 1 
the derivatives of l with respect to u and v are:
	 l	t
	=	xv + uv v + ¦Á	 1 
	 u	 
		 1 
using the kuhn-tucker condition ¦Áijuij = 1 and ¦Âijvij = 1  we get the following equations for uij and vij:
	 xv ijuij    uvtv ijuij = 1	 1 
	 xtu ijvij    vutu ijvij = 1	 1 
these equations lead to the following updating formulas:
	ij	ij	t	 1 
 uv v ij
	vij	vij	t	 1 
 vu u ij
it is proven by lee  that the objective function j is nonincreasing under the above iterative updating rules  and that the convergence of the iteration is guaranteed. note that the solution to minimizing the criterion function j is not unique. if u and v are the solution to j  then  ud  vd 1 will also form a solution for any positive diagonal matrix d. to make the solution unique  we further require that the euclidean length of the column vector in matrix u is one.
this requirement of normalizing u can be achieved by1:
		 1 
		 1 
¡¡there is an analogy with the svd in interpreting the meaning of the two non-negative matrices u and v. each element uij of matrix u represents the degree to which term fi ¡Ê w belongs to cluster j  while each element vij of matrix v indicates to which degree document i is associated with cluster j. if document i solely belongs to cluster x  then vix will take on a large value while rest of the elements in i'th row vector of v will take on a small value close to zero.
¡¡in summary  our document clustering algorithm is composed of the following steps:
1. given a document corpus  construct the termdocument matrix x in which column i represents the weighted term-frequency vector of document di.
1. perform the nmf on x to obtain the two non-negative matrices u and v using eq. 1  and eq. 1 .
1. normalize u and v using eq. 1  and eq. 1 .
1. use matrix v to determine the cluster label of each data point. more precisely  examine each row i of matrix v. assign document di to cluster x if x = argmaxvij. j
¡¡the computation complexity for eq. 1  and eq. 1  is o kn  and the total computation time is o tkn   where is t is number of iterations performed.
1 nmf vs. svd
¡¡at the beginning of section 1  we discussed the characteristics of the nmf and its differences with the methods based on the svd and eigenvector computations. here we further illustrate these differences using experiments. we have applied both the nmf and the svd to a data set that consists of three clusters  and plotted the data set in the spaces derived from the nmf and the svd  respectively. figure 1 a  and  b  show the data distributions in the two spaces in which data points belonging to the same cluster are depicted by the same symbol. the three figures in  a  plot the data points in the space of v1-v1  v1-v1  and v1-v1  respectively  where v1 v1 v1 are the three row vectors of v from the nmf  while the three figures in  b  plot the data points in the space of e1-e1  e1-e1  and e1-e1  respectively  where e1 e1 e1 are the first three singular vectors of the svd. clearly  in the nmf space  every document takes non-negative values in all three directions  while in the svd space  each document may take negative values in some of the directions. furthermore  in the nmf space  each axis corresponds to a cluster  and all the data points belonging to the same cluster spread along the same axis. determining the cluster label for each data point is as simple as finding the axis with which the data point has the largest projection value. however  in the svd space  although the

1
 when normalizing matrix u  matrix v needs to be adjusted accordingly so that uvt does not change.
three axes do separate the data points belonging to the different clusters  there is no direct relationship between the axes  eigenvectors  and the clusters. traditional data clustering methods such as k-means have to be applied in this eigenvector space to find the final set of data clusters.

 a  data distribution in the nmf subspace ofv1 v1  v1 v1  and v1 v1  respectively.

 b  data distribution in the svd subspace ofe1 e1  e1 e1  and e1 e1  respectively.
figure 1: data distribution in the nmf and lsi spaces. documents belonging to the same cluster are depicted by the same symbol.
1. performance evaluations
¡¡in this section  we describe the document corpora used for the performance evaluations  unveil the document clustering accuracies of the proposed method and its variations  and compare the results with the representative spectral clustering methods.
1 data corpora
¡¡we conducted the performance evaluations using the tdt1 and the reuters1 document corpora. these two document corpora have been among the ideal test sets for document clustering purposes because documents in the corpora have been manually clustered based on their topics and each document has been assigned one or more labels indicating which topic/topics it belongs to. the tdt1 corpus consists of 1 document clusters  each of which reports a major news event occurred in 1. it contains a total of 1 documents from six news agencies such as abc  cnn  voa  nyt  pri and apw  among which 1 documents have a unique category label. the number of documents for different news events is very unbalanced  ranging from 1 to 1. in our experiments  we excluded those events with less than 1 documents  which left us with a total of 1 events. the final test set is still very unbalanced  with some large clusters more than 1 times larger than some small ones.

1
nist	topic	detection	and	tracking	corpus	at
http://www.nist.gov/speech/tests/tdt/tdt1/index.htm
1
reuters-1 	distribution	1	corpus	at
http://kdd.ics.uci.edu/databases/reuters1/ reuters1.html
table 1: statistics of tdt1 and reuters corpora.
tdt1reutersno. documents1no. docs. used1no. clusters1no. clusters used1max. cluster size1min. cluster size1med. cluster size1avg. cluster size1¡¡on the other hand  reuters corpus contains 1 documents which are grouped into 1 clusters. compared with tdt1 corpus  the reuters corpus is more difficult for clustering. in tdt1  each document has a unique category label  and the content of each cluster is narrowly defined  whereas in reuters  many documents have multiple category labels  and documents in each cluster have a broader variety of content. in our test  we discarded documents with multiple category labels  and removed the clusters with less than 1 documents. this has lead to a data set that consists of 1 clusters with a total of 1 documents. table 1 provides the statistics of the two document corpora.
1 evaluation metrics
¡¡the testing data used for evaluating the proposed document clustering method are formed by mixing documents from multiple clusters randomly selected from the document corpus. at each run of the test  documents from a selected number k of topics are mixed  and the mixed document set  along with the cluster number k  are provided to the clustering process. the result is evaluated by comparing the cluster label of each document with its label provided by the document corpus. two metrics  the accuracy  ac  and the northe document clustering performance. given a document malized mutual information metric mi are used to measure
di  let li and ¦Ái be the cluster label and the label provided by the document corpus  respectively. the ac is defined as follows:

ac = 1 
n
where n denotes the total number of documents in the test  ¦Ä x y  is the delta function that equals one if x = y and equals zero otherwise  and map li  is the mapping function that maps each cluster label li to the equivalent label from the document corpus. the best mapping can be found by using the kuhn-munkres algorithm .
on the other hand  given the two sets of document clusters
  their mutual information metric mi c c  is defined as:

mi  1 
where  denote the probabilities that a document arbitrarily selected from the corpus belongs to the clusters
  respectively  and   denotes the joint probability that this arbitrarily selected document belongs to the clusters ci as well as  at the same time. mi c c  takes values between zero and max h      where h c  and h c  are the entropies of  respectively. it reaches the maximum max h c  h c    when the two sets of document clusters are identical  whereas it becomes zero when the two sets are completely independent. another important character of mi   is that  for each ci ¡Ê c  it does not need to find the corresponding counterpart in c  and the value keeps the same for all kinds of permutations. to simplify comparisons between different pairs of cluster sets  instead of using mi    we use the following normalized metric mi   which takes values between zero and one:

mi  1 
c   
1 performance evaluations and comparisons
¡¡to demonstrate how our method improves the document clustering accuracy in comparison to the best contemporary methods  we implemented two representative spectral clustering methods: average association  aa in short    and normalized cut  nc in short    and conducted the performance evaluations on the two methods using the same data corpora. these methods model the given document set using a undirected graph in which each node represents a document  and each edge  i j  is assigned a weight wij to reflect the similarity between documents i and j. the document clustering task is accomplished by finding the graph's best cuts that optimize certain predefined criterion functions. let g = g v e  be a weighted graph with the node set v and edge set e  w =  wij  be the graph weight matrix  a and b be two subgraphs of g. the criterion functions adopted by the aa and nc methods are defined as:
	cut a a 	cut b b 
	aa	=	+	 1 
	|a|	|b|
		 1 
where |a| is the size of a and
	.	 1 
it has been proven that with certain relaxation  the minimization of the above two criterion functions can be approximated by solving the following eigenvalue systems:
	aa:	wy = ¦Ëy
	nc:	d 1wd 1y = ¦Ëy	 1 
where d = diag we   e =  1 ... 1 t  and y is the cluster indication vector that can be used to determine the cluster label of each document.
¡¡interestingly  zha et al has shown that the aa criterion function  eq. 1   is equivalent to that of the lsi followed by the k-means clustering method  if the inner product  is used to measure the document similarity. we can prove that when the weight 1'th diagonal element of the matrix d  is applied to column vector i of the matrix w  the aa method becomes exactly the same as the nc method. in other words  the essential difference between the aa and the nc methods is that nc applies the weights to w while aa does not.  see appendix a .
table 1: performance comparisons using tdt1 corpus
kmutual informationaccuracyaancnmfnmf-ncwaancnmfnmf-ncw1.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1average11111111table 1: performance comparisons using reuters corpus
kmutual informationaccuracyaancnmfnmf-ncwaancnmfnmf-ncw1.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1111111111.1.1.1.1.1.1.1.1average11111111¡¡inspired by the above observations  we have conducted performance evaluations on the proposed method with its standard form as well as the nc weighted variation:
standard form  nmf in short : conduct document clustering using the original data matrix x.
nc weighted form  nmf-ncw in short : calculate d = diag xtxe   conduct document clustering using the weighted data matrix x = xd 1  see appendix b .
¡¡table 1 and 1 show the evaluation results using the tdt1 and the reuters corpus  respectively. the evaluations were conducted for the cluster numbers ranging from two to ten. for each given cluster number k  1 test runs were conducted on different randomly chosen clusters  and the final performance scores were obtained by averaging the scores from the 1 tests. because nmf algorithm is not guaranteed to find the global optimum  it is beneficial to perform nmf algorithm a few times with different initial values and choose the trial with minimal square error j. in reality  if the data-set has reasonable clusters  usually a very few number of trials is enough to find a satisfactory solution. in all of our experiments  1 trials of nmf are performed in each test run.
¡¡our finding can be summarized as follows: regardless of the document corpora  the performance ranking is always in the order of aa  nmf  nc  and nmf-ncw. applying the nc weighting always brings positive effects for both the spectral clustering  nc vs. aa  and the nmf methods  nmf-ncw vs. nmf . the improvement becomes more obvious for the tdt1 corpus than the reuters corpus. as described in section 1  document clusters in tdt1 are generally more compact and focused than the clusters in reuters. the above experimental results for the two document corpora are mostly in line with the expectations because document clustering methods generally produce better results for document corpora comprised of compact and well-focused clusters.
1. summary
¡¡in this paper  we have presented a novel document partitioning method based on the non-negative factorization of the term-document matrix of the given document corpus. our method differs from the latent semantic indexing method based on the singular vector decomposition  svd  and the related spectral clustering methods in that the latent semantic space derived by nmf does not need to be orthogonal  and that each document is guaranteed to take only non-negative values in all the latent semantic directions. as evidenced by the experiment in section 1  these two differences bring about an important benefit that each axis in the space derived by the nmf has a much more straightforward correspondence with each document cluster than in the space derived by the svd  and thereby document clustering results can be directly derived without additional clustering operations. our experimental evaluations show that the proposed document clustering method surpasses svd- and the eigen decomposition clustering methods not only in the easy and reliable derivation of document clustering results  but also in document clustering accuracies.
