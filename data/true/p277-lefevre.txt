protecting data privacy is an important problem in microdata distribution. anonymization algorithms typically aim to protect individual privacy  with minimal impact on the quality of the resulting data. while the bulk of previous work has measured quality through one-size-fits-all measures  we argue that quality is best judged with respect to the workload for which the data will ultimately be used.
　this paper provides a suite of anonymization algorithms that produce an anonymous view based on a target class of workloads  consisting of one or more data mining tasks  as well as selection predicates. an extensive experimental evaluation indicates that this approach is often more effective than previous anonymization techniques.
categories and subject descriptors
h.1  database management : database applications
general terms
algorithms  experimentation  security
keywords
privacy  anonymity  data recoding  predictive modeling
1. introduction
　k-anonymity  1  1  and l-diversity  have been studied widely as mechanisms for preventing re-identification attacks in microdata release. of course  subject to the given anonymity constraints  the data should remain as useful as possible. unfortunately  there is often a tension between these two goals.
　it is our position that the best way of measuring quality is based on the task for which the data will ultimately be used. this paper provides anonymization techniques that incorporate a target workload of selections and mining tasks.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  philadelphia  pennsylvania  usa.
copyright 1 acm 1-1/1 ...$1.
1 motivating example
　suppose that a trusted agency compiles a database of disease information for several million hospital patients. however  the agency is prohibited by law from distributing this data without taking precautions to ensure individual privacy. for example  the agency should take steps to guarantee that the released data does not reveal any individual's hiv status.
　alice is an external researcher who is directing two separate studies  each of which could benefit from using the data in the central database. as part of the first study  alice wants to build a classification model that uses age  smoking history  and hiv status to predict life expectancy. in the second study  she would like to find combinations of variables that are useful for predicting elevated cholesterol and obesity in males over 1.
　in this situation  it is desirable to distribute anonymized microdata to individuals like alice  the data recipients .1 one might consider a simpler protocol  in which alice requests a specific model  constructed entirely by the agency. however  there are two downsides to this approach. first  the simple model-distribution protocol assumes that the tasks are fully-specified at the time of the initial request. however  in our example  alice's second study involves an entire class of models  each constructed using a subset of the data  attributes and records . indeed  workloads like this arise naturally in certain types of exploratory data analysis .
　also  the inference implications of releasing one or more models constructed on the agency's unmodified data are not well-understood. each such model reveals something about the distributional characteristics of the agency's data  and in certain cases  the revealed information might constitute a breach of privacy. however  in the case of a single released view  there are well-defined notions of anonymity  and the best alice can do is to approximate the distribution in the  sanitized  data she is given.
　the work presented in this paper is motivated by this type of scenario  where the goal is to create a single view of the database that respects all given anonymity constraints  but that remains useful for carrying out the tasks in a target class of workloads.
1 paper overview and contributions
　we begin by reviewing the problems of anonymity  classification  and regression in section 1. because previous definitions of anonymity with respect to a sensitive attribute  i.e.  l-diversity   have assumed that the sensitive attribute is nominally-valued  we also propose a novel diversity requirement for numeric attributes.
　our first main contribution  described in section 1  is a suite of algorithms for generating an anonymous data snapshot  while preserving the utility of the data with respect to a target class of workloads. while previous work has considered incorporating a single classifier  constructed over the entire released data set   1  1  1   we incorporate the following expressive workload characteristics:
  classification & regression we incorporate models predicting both categorical and numeric attributes.
  multiple target models often  the data recipient will want to build separate models to predict multiple different attributes.
  selection & projection frequently  one or more of the mining tasks will involve only a subset of the data  e.g.  males over 1 . in this case  it is important to guarantee that this data can be precisely and accurately selected from the released snapshot. similarly  it is important to guarantee that the data remains useful when only a subset of the released attributes is used for a particular task.
　our second main contribution is an extensive experimental evaluation  described in section 1. the results show that our anonymization algorithms are often more effective than previous algorithms in producing high-quality data  as judged by a variety of workloads.
　much of the previous work on k-anonymity has measured data quality or optimality using simple measures based on equivalence class size or the total number of generalizations/suppressions  1  1  1  1  1  1 . not surprisingly  our experiments also show that one-size-fits-all measures are not necessarily indicative of quality with respect to a particular workload.
　in order to assess the impact of anonymization on subsequent analysis techniques  we first had to address some additional problems. because standard learning algorithms use point data for training  rather than the region data produced by multidimensional recoding  section 1 proposes a pre-processing step for converting regions to points. following pre-processing  standard learning algorithms can be applied without modification.
　the paper concludes with discussions of related and future work in sections 1 and 1.
1. preliminaries
　k-anonymity  1  1  and l-diversity  were proposed to limit re-identification risk in microdata publishing. consider a single relation t. in defining anonymity  each attribute in t is characterized by at most one of the following types:
  unique identifiers a unique identifier is any attribute that identifies individuals  e.g.  ss# . known identifiers are typically removed entirely from released microdata.
  quasi-identifier  q1 ... qd  a quasi-identifier is a minimal set of attributes that can be joined with external information to re-identify individual records. we assume that a quasi-identifier is recognized based on knowledge of the domain.
  sensitive attributes  s  an attribute is considered sensitive if an adversary should not be permitted to uniquely associate its value with a unique identifier. for example  the hiv status field in released medical data would likely be considered sensitive. previous work assumed a single  nominally-valued  sensitive attribute ; we also propose an extension to a numeric sensitive attribute.
　the k-anonymity requirement is quite simple. intuitively  it stipulates that no individual record should be uniquely identifiable from a group of k on the basis of its quasiidentifier values. we will refer to each group of tuples in t with identical quasi-identifier values as an equivalence class.
k-anonymity  1  1  a table t is k-anonymous with respect to quasi-identifier set q1 ... qd if every unique tuple hq1 ... qdi in the  multiset  projection of t on q1 ... qd occurs at least k times.
　l-diversity  provides a natural extension  incorporating a nominal sensitive attribute s. the l-diversity principle requires that each equivalence class  as defined by kanonymity  also contain at least l  well-represented  distinct values for s. this principle can be instantiated in various ways. the strictest proposal formulates l-diversity in terms of entropy. because entropy is concave  entropy l-diversity requires that the full database have entropy at least log l . ds denotes the  finite  domain of attribute s.
entropy l-diversity  nominal s   a table t is entropy l-diverse with respect to quasi-identifier set q1 ... qd and sensitive attribute s if  for every equivalence class e in fraction of tuples inp s e with s = s. t  s（d  p s|e log p s|e  − log l   where p s|e  is the
　for numeric sensitive attributes  diversity is more subtle. for example  if s = salary  an equivalence class containing salaries {1k 1k 1k} is considered 1-diverse  but intuitively does not protect privacy as well as an equivalence class containing salaries {1k 1k 1k}. for this reason  we define a new diversity requirement that guarantees a certain level of dispersion within each equivalence class:
squared-error diversity  numeric s  table t is squarederror diverse with respect to quasi-identifier set q1 ... qd and sensitive attribute s if  for every equivalence class e in of spin e  and error is the diversity parameter. t  i（e si  s e  1 − error  where s e  is the mean value
1 classification & regression
　in classification/regression  attributes are typically characterized by at most one of the following types:
  target attribute  c or r  the goal of classification is to build a model that accurately predicts the value of a nominal class label  c . regression aims to predict a numeric attribute  r .
  predictor attributes some set of  discrete or continuous  predictor attributes  also commonly called features  are used to predict the target attribute.
　when a target classification or regression model is considered in conjunction with anonymity  each attribute has two

figure 1: a possible value generalization hierarchy for the nationality domain
characterizations. in the remainder of this paper  we will assume that the set of predictor attributes is a quasi-identifier. under this assumption  it is contradictory to categorize an attribute as both target and sensitive  and we disallow this categorization.
1 recoding
　numerous recoding techniques have been proposed for sanitizing microdata to satisfy an anonymity constraint. in a relational database  each attribute x has a domain of values dx. a global recoding achieves anonymity by mapping the quasi-identifier domains to ranges or coarsened values.
　global recoding can be broken down into two sub-classes  1  1 . if the quasi-identifier consists of d attributes  q1  ... qd   a single-dimensional global recoding is defined by a set of functions φ1 ... φd such that each φi : dqi ★ d1. an anonymous view v of t is obtained by applying each φi to the value of qi in each tuple of t.
　on the other hand  a multidimensional global recoding is defined by a single function φ : dq1 〜...〜dqd ★ d1  which is used to recode the domain of unique vectors associated with the quasi-identifier. in this case  v is obtained by applying φ to the vector of quasi-identifier values in each tuple of t.
　for attributes with continuous or ordinal  ordered categorical  domains  it is convenient to think of each vector of quasi-identifier values hq1 ... qdi as a point in a ddimensional space. a class of multidimensional recoding models partitions the domain space into non-overlapping ddimensional rectangular regions . recoding function φ is defined by mapping each point to the region in which it is contained. thus  each region corresponds to an equivalence class in anonymous view v .1
　when the domain of a quasi-identifier attribute is nominal  this partitioning may be further constrained by a userdefined value generalization hierarchy  or partial order  as described by samarati and sweeney  1  1 . for example  figure 1 shows a possible hierarchy for the nationality domain; the domain values are found at the leaves. the notation french 1 european indicates that french is descended from european in the hierarchy.
　the hierarchy can be used in several ways to constrain the set of possible recodings . in this paper  within a particular d-dimensional region  we require that if φ maps a leaf value v to some ancestor a  then all leaves that are descended from a must also be mapped to a.
　every single-dimensional recoding can be equivalently expressed as a multidimensional recoding  but the reverse is frequently not true . depending on the distribution of the data  this can affect data quality. for example  consider a dataset with exactly two predictors/quasi-identifiers  age and zip . suppose the distribution of class labels  +    is as shown in figure 1  and that k = 1. in this case  there is

	 a  multidimensional	 b  single-dimensional
figure 1: comparing multidimensional and singledimensional recoding in two dimensions a k-anonymous multidimensional recoding that groups together only records with like labels  but this cannot be accomplished with single-dimensional recoding  which requires that the values of each attribute be recoded uniformly.
1. workload-awareanonymization
　this section proposes several algorithms for creating a single snapshot of a given data set that respects a given anonymity constraint  but remains useful for executing a particular class of workloads. the target class of workloads is specified by the following parameters:
1. a set of predictor attributes  q1 ... qd 
1. either a set of one or more nominal target class labels
 c1 ... cm   or numeric target attributes  r1 ... rm 
1. optionally  a set of selection predicates  pr1 ...  prn 
　the anonymity constraint is k-anonymity  optionally extended by l-diversity or squared-error diversity. also  we assume that the predictor attributes are a quasi-identifier.
　in the simplest case  when the target workload consists of one classification or regression model  without selection predicates  the heuristics used by our algorithms implement entropy l-diversity and squared-error diversity in reverse.
1 single target classification model
　the mondrian algorithm was recently proposed for kanonymization using multidimensional recoding . the algorithm is based on a greedy recursive partitioning of the  multidimensional  quasi-identifier domain space  see figure 1 . in order to obtain approximately uniform partition occupancy   suggests recursively choosing the split attribute with the largest normalized range of values  and  for continuous or ordinal attributes  partitioning the data around the median value of the split attribute. this process is repeated until no allowable split remains  meaning that a particular region cannot be further divided without violating the anonymity constraint  or constraints imposed by value generalization hierarchies. we refer to this algorithm as median mondrian.
　when the  set of  target mining model s  is known  we can improve this heuristic. first consider a single target classification model  with predictor attributes q1 ... qd  also the quasi-identifier  and class label c. in this case  we propose a heuristic partitioning scheme based on information gain  which is reminiscent of decision tree construction. intuitively  the goal of this greedy criterion is to produce homogeneous partitions of class labels.
　at each recursive step  we choose the split that minimizes the weighted entropy over the set of resulting partitions  without violating the anonymity constraint . p denotes the current  recursive  tuple set  and partitions p1 denotes the set of partitions resulting from the candidate split. p c|p1  is the fraction of tuples in p1 with class label c = c. we refer to this algorithm as infogain mondrian.
x
d
 1 
　infogain mondrian handles continuous quasi-identifier values as they are typically handled by decision-trees  partitioning around the threshold value with smallest entropy  see  . the data is first sorted with respect to the split attribute. then the data is scanned  and each time there is a change in class label  this candidate threshold is checked with respect to anonymity and entropy. in the event that no candidate threshold satisfies the anonymity constraint  the median is also checked as a default.
　infogain mondrian scales to large data sets through a straightforward adaptation of an existing scalable decisiontree induction scheme  such as rainforest .
1 single target regression model
　similar greedy heuristics can be used when the target attribute is numeric. specifically  we use the mean squared error  mse  to measure the impurity of target attribute r within a candidate partition p1. a heuristic inspired by the cart algorithm for regression trees  recursively chooses the split that minimizes the weighted sum of mses over the   denotes the mean value of
x x
　because |p| is constant for all candidate splits  the algorithm chooses the split that minimizes the following expression  without violating anonymity . we call this least squared deviance  lsd  mondrian. this algorithm handles continuous attributes through discretization.
	error1 p r  = x 1 x  ri   r p1  1	 1 
partitions p i（p1
1 multiple target models
　in certain cases  we would like to allow the data recipient to build several models  to accurately predict the marginal distributions of several class labels  c1 ... cm  or regression attributes  r1 ... rm . infogain mondrian and lsd mondrian can be extended to handle multiple discrete and numeric target attributes  respectively.
　for classification  there are two ways to make this extension. in the first approach  the data recipient would build a single model to predict the vector of class labels  hc1 ... cmi  which has domain dc1 〜 ... 〜 dcm. a greedy split criterion would minimize entropy with respect to this single variable.
　however  in this simple approach  the size of the domain grows exponentially with the number of target attributes.

anonymize tuples  attrs  if  no allowable split for tuples 
return φ : t （ tuples ★ bounding region tuples 
else
best ○ choose attribute attrs  tuples  if continuous best  or ordinal best 
threshold ○ choose threshold best  lhs ○ {t （ tuples : t.best ＋ threshold} rhs ○ {t （ tuples : t.best   threshold} return anonymize rhs attrs  “ anonymize lhs attrs 
else if nominal best 
recodings ○ {} for each child vi of root best.hierarchy 
tuplesi ○ {t （ tuples : t.best 1 vi} attrs1 ○ replace root best.hierarchy  with vi in attrs recodings ○ recodings “ anonymize tuplesi  attrs1 
return recodings

figure 1: basic mondrian algorithm
to avoid potential problems due to data sparsity  we instead simplify the problem by assuming independence among target attributes. this is a reasonable assumption because we are ultimately only concerned about the marginal distribution of each target attribute. under the independence assumption  a greedy split criterion minimizes the sum of weighted entropies:
	 	 1 
　in regression  the squared error split criterion in particular   there is no analogous distinction between treating the set of target attributes as a single variable and assuming independence. for example  if we have two target attributes  r1 and r1  the joint error is the distance between an observed point  r1 r1  and the centroid  r1 p  r1 p   in 1dimensional space. the squared joint error is just the sum of individual squared errors   r1   r1 p  1 +  r1   r1 p  1. for this reason  the greedy split criterion minimizes the sum of squared error:
	 	 1 
1 incorporating selection
　sometimes one or more of the tasks in the target workload will use only a subset of the released data  and it is important that this data can be selected precisely  despite recoding. for example  a researcher may want to build a model using only males over 1  but this is difficult if the ages of some men are recoded to the range  1   1 . this problem was originally described in .
　consider a set of selection predicates  pr1 ... prm  defined by boolean functions of the quasi-identifier attributes  q1 ... qd . conceptually  each pri defines a query region
ri in the domain space such that ri = {p （ dq1〜...〜dqd : pri p  = true}. for the purposes of this work  we only consider selections for which the query region can be expressed as a hyper-rectangle.  some additional selections can be decomposed into two or more hyper-rectangles  and incorporated as separate queries. 
　a multidimensional recoding function φ divides the domain space into non-overlapping regions p1 ... pn. formally  the recoding region pi = {p （ dq1 〜 ... 〜 dqd : φ p  = p1i}  where p1i is a particular generalization of the
figure 1: selection example
quasi-identifier vector. when evaluating pri over the recoded view v   it may be that no subset of the recoding regions can be combined to produce query region ri. instead  it is intuitive to return the tuples from v that are contained in any recoding region overlapping ri. more formally 
	overlap ri 	=	“{pi : pi ” ri 1= φ}
	pri v  	=	{φ p  : φ p  （ v … p （ overlap ri }
　notice that this will often produce a larger result set than evaluating pri over the original table t; the imprecision is the difference in size between these two result sets. imprecision pri {p1 ... pn}  = |pri v  |   |pri t |  1 
　for example  figure 1 shows a 1-dimensional domain space. the shaded area represents a query region  and the tuples of t are represented by points. the recoding regions are bounded by dotted lines and numbered. recoding regions 1  1  and 1 overlap the query region. if we evaluated this query using the original data  the result set would include 1 tuples. however  evaluating the query using the recoded data yields 1 tuples  an imprecision of 1.
　ideally  the goal of selection-oriented anonymization is to divide the domain space into a set of  anonymous  recoding regions that minimize imprecision for the set of target predicates. we incorporate this goal into the mondrian algorithm through a new greedy splitting heuristic. specifically  at each recursive step  when partitioning a recursive region p  we choose the split that minimizes the total imprecision for the set of resulting regions:
	 	 1 
　the algorithm proceeds until there is no allowable split that reduces the imprecision of the current partition p  and continuous attributes are handled through discretization. we will call this algorithm selection mondrian.
　in practice  we expect this technique to be used most often for simple selections  such as breaking down health data by state. after incorporating selections  we continue to anonymize each resulting partition independently  using the appropriate classification- or regression-oriented algorithm.
1. experimental evaluation
　our experimental evaluation has several goals  the first of which is to provide some insight about quality evaluation methodology. we describe an experimental protocol for evaluating an anonymization algorithm with respect to a target data mining workload  and we compare the results to those obtained using some simpler quality measures.

figure 1: mapping a d-dimensional rectangular region to 1   d attributes
　the second goal is to evaluate the algorithms described in section 1. in particular  we assess the impact of incorporating a set of target classification or regression models into the anonymization  and multidimensional recoding. also  we evaluate the effectiveness of our algorithms with respect to selections  projections  and multiple target models.
1 methodology
　given a target classification or regression workload  the most direct way to evaluate the quality of an anonymization is by training each target model using the anonymized data  and evaluating the resulting models using predictive accuracy  classification   mean absolute error  regression   or similar measures. we will call this methodology model evaluation. all of our model evaluation experiments follow a common protocol:
1. the data is first divided into training and testing sets  or 1-fold cross-validation sets   ttrain and ttest.
1. the anonymization algorithm determines recoding function φ using only the training set ttrain. anonymous view vtrain is obtained by applying φ to ttrain.
1. the same recoding function φ is then applied to the testing set  ttest   yielding vtest.
1. the classification or regression model is trained using vtrain  and tested using vtest.
　this experimental design is different from the setup used by fung et al.  for an important reason. in   the combined training and testing sets were anonymized using a single-dimensional recoding algorithm based on information gain. following this step  the data was separated into training and testing sets. in our opinion  this setup is inappropriate for evaluating the anonymization algorithm because incorporating the test set when choosing a recoding is tantamount to looking at the test set while doing feature selection. instead  all of our experiments hold out the test set during both the anonymization and training phases.
　we used k-anonymity as the anonymity constraint  and we used the implementations of the following learning algorithms provided by the weka software package :
  decision tree  j1  default settings were used.
  naive bayes supervised discretization was used for continuous attributes; otherwise all default settings were used.
  random forests each classifier was comprised of 1 random trees  and all other default settings were used.
  support vector machine  smo  default settings were used  including a linear kernel function.
  linear regression default settings were used.
  regression tree  m1  default settings were used.
attributedistributiongeneralizesalaryuniform in  1  1 continuouscommissionif salary − 1  then 1
else uniform in  1  1 continuousageuniform integer in  1 continuouseleveluniform integer in  1  1 hierarchycaruniform integer in  1  1 hierarchyzipcodeuniform integer in  1  1 continuoushvaluezipcode * h * 1 where h uniform in  1  1 continuoushyearsuniform integer in  1  1 continuousloanuniform in  1  1 continuousfunctiongroup a1  age   1  …  1k ＋ salary ＋ 1k  ‥
  1 ＋ age   1  …  1k ＋ salary ＋ 1k  ‥
  age − 1  …  1k ＋ salary ＋ 1k  1  age   1 …
   elevel （ {1}   1k ＋ salary ＋ 1k  
:  1k ＋ salary ＋ 1k    ‥
  1 ＋ age   1 …
   elevel （ {1 1}   1k ＋ salary ＋ 1k  
:  1k ＋ salary ＋ 1k    ‥
  age − 1 …
   elevel （ {1 1}   1k ＋ salary ＋ 1k  
:  1k ＋ salary ＋ 1k    1  age   1 …
 1k ＋  salary + commission  ＋ 1k  ‥
  1 ＋ age   1 …
 1k ＋  salary + commission  ＋ 1k  ‥
  age − 1 …
 1k ＋  salary + commission  ＋ 1k  1disposable = .1 〜  salary + commission 
 .1 〜 loan   1k disposable   1figure 1: synthetic predictor/quasi-identifier attributes and class label functions
　in addition to model evaluation  we also measured certain characteristics of the anonymized training data to see if there was any correlation between these simpler measures and the results of the model evaluation. specifically  we measured the average equivalence class size  and for classification tasks  we measured the conditional entropy of the class label given the partitioning:
	h c|p  = x p p  x  p c|p  log p c|p 	 1 
	partitions p	classes c
1 learning from regions
　when single-dimensional recoding is used  standard learning algorithms can be applied directly to the resulting point data  notwithstanding the  coarseness  of some points . although multidimensional recoding techniques are more flexible  using the resulting hyper-rectangular data to train standard data mining models poses an additional challenge. to address this problem  we make a simple observation. because we restrict the recoding regions to include only ddimensional hyper-rectangles  each region can be uniquely represented as a point in  1   d -dimensional space. for example  figure 1 shows a 1-dimensional rectangle  and its unique representation as a 1-tuple. this assumes a total order on the values of each attribute  similar to the assumption made by support vector machines.
　following this observation  we adopt a simple pre-processing technique for learning from regions. specifically  we extend the recoding function φ to map data points to d-dimensional census database
attributedist. valsgeneralizationregion1hierarchyage1continuouscitizenship1hierarchymarital status1hierarchyeducation  years 1continuoussex1hierarchyhours per week1continuousdisability1hierarchyrace1hierarchysalary1/continuoustargetcontraceptives database
attributedist. valsgeneralizationwife's age1continuouswife's education1hierarchyhusband's education1hierarchychildren1continuouswife's religion1hierarchywife working1hierarchyhusband's occupation1hierarchystd. of living1continuousmedia exposure1hierarchycontraceptive1targetfigure 1: summary of real-world data sets
regions  and in turn  to map these regions to their unique representations as points in  1   d -dimensional space.
　our primary goal in developing this technique is to establish the utility of our anonymization algorithms. there are many possible approaches to the general problem of learning from regions. for example  zhang and honavar proposed an algorithm for learning decision trees from attribute values at various levels of a taxonomy tree . however  a full comparison is beyond the scope of this paper.
1 experimental data
　our first set of experiments used synthetic data based on the classification generator introduced by agrawal et al. . predictor/quasi-identifier attributes were generated according to the distributions described in figure 1  and class labels were generated as a function of the predictor values. we present results for four representative label functions  chosen from the original ten  functions 1 1 . to simplify the evaluation  we applied the labeling functions deterministically  without injecting noise.
　notice that the basic labeling functions in figure 1 include a number of constants  e.g.  1k . in order to get a more robust understanding of the behavior of the various anonymization algorithms  for functions 1  1  and 1  we instead generated many independent data sets  varying the function constants independently at random over the range of the attribute.
　figure 1 notes  for each predictor/quasi-identifier attribute  whether it was treated as continuous or nominal  with an associated generalization hierarchy  during anonymization.

	 c  random forests	 d  svm
figure 1: average predictive accuracy for models trained using anonymized synthetic data  k=1 　in addition to the synthetic data  we also used several real-world data sets. the first was derived from a sample of the 1 public use microdata  distributed by the united states census american community survey1  with target attribute salary. this data was used for both classification and regression  and contained 1 records. for classification  we replaced the numeric salary with a salary class    1k or − 1k ; approximately 1% of the data records had salary   1k. for classification  this is similar to the adult database from the uci machine learning repository   which has been used in numerous k-anonymity evaluations. however  we chose to compile a new data set that can be used for both classification and regression.
　the second real data set is the smaller contraceptives database from the uci repository  which contained 1 records after removing those with missing values. this data includes nine socio-economic indicators  which are used to predict the choice of contraceptive method  long-term  shortterm  or none  among sampled indonesian women. summaries of both real data sets are provided in figure 1.
1 comparison with previous algorithms
　infogain mondrian and lsd mondrian combine multidimensional recoding with classification- and regression-oriented
splitting heuristics. in this section  we evaluate the effects of these two components through a comparison with two previous anonymization algorithms. all of the experiments in this section consider a single target model  constructed over the entire anonymized training set.
　several previous algorithms have incorporated a single target classification model while choosing a single-dimensional recoding  1  1  1 . to gage the impact of multidimensional recoding  we compared infogain mondrian and the greedy top-down specialization  tds  algorithm . also  multidimensional recoding was used in median mondrian   without regard to workload. we compare this to infogain mondrian and lsd mondrian to gage the effects of incorporating a target model.
　using the synthetic data  figure 1 compares the predictive accuracy of classifiers trained on data produced by the different anonymization algorithms. in these experiments  we generated 1 independent training and testing sets  each containing 1 records  and we fixed k = 1. the results are averaged across these 1 trials. for comparison  we also include the accuracies of classifiers trained on the  not anonymized  original data.
　infogain mondrian consistently outperforms both tds and median mondrian  a result that is overwhelmingly significant based on a series of paired t-tests. it is important to note that the pre-processing step used to convert regions to points  section 1  is only used for the multidimensional recodings; the classification algorithms run unmodified on the single-dimensional recodings produced by tds . thus  should a better technique be developed for learning from regions  this would improve the results for infogain mondrian  but it would not affect tds.1
　we performed a similar set of experiments using the realworld data. figures 1 a b c  show results for the census classification data  for increasing k. the graphs show test set accuracy  averaged across 1 folds  for three learning algorithms. the variance across the folds was quite low  and the differences between infogain mondrian and tds  and between infogain mondrian and median mondrian  were highly significant based on paired t-tests.

 g  j1 with selection  census 	 h  j1 with projection  census 	 i  m1 regression tree  census 
figure 1: comparing anonymization techniques using real-world data　it is important to point out that in certain cases  notably random forests  the learning algorithm overfits the model when trained using the original data. for example  the model for the original data in figure 1 c  gets 1% accuracy on the training set  but only 1% accuracy on the test set. when overfitting occurs  it is not surprising that the models trained on anonymized data obtain higher accuracy because anonymization acts as a form of feature selection/construction. interestingly  we also tried applying a traditional form of feature selection  ranked feature selection based on information gain  to the original data  and this did not improve the accuracy of random forests for any number of chosen attributes. we suspect that this discrepancy is due to the flexibility of the recoding techniques. single-dimensional recoding  tds  is more flexible than traditional feature selection because it can incorporate attributes at varying levels of granularity. multidimensional recoding is more flexible still because it  conditionally  incorporates different attributes for different data subsets.
　next  figures 1 d e  show conditional entropy and average equivalence class size measurements  averaged across the ten anonymized training folds of the census classification data. average equivalence class size  which does not take into account any characteristics of the workload  is not a very good indicator of model accuracy. conditional entropy  which incorporates the target class label  is a lot better; low conditional entropy generally indicates higher accuracy.
　we performed the same set of experiments using the contraceptives database  and observed similar behavior. infogain mondrian yielded higher accuracy than tds or median mondrian. results for j1 are shown in figure 1 f . the remaining results are omitted due to space constraints.
　for regression  we found that lsd mondrian generally led to better models than median mondrian. figure 1 i  shows the mean absolute test set error for the m1 regression tree  using the census regression data. a similar relative comparison was observed for linear regression  but the overall error was higher because salary is non-linear.
1 multiple target models
　in section 1 we described a simple adaptation to the basic infogain mondrian algorithm that allowed us to incorporate more than one target attribute  expanding the set of models for which a particular anonymization is  optimized.  to evaluate this technique  we performed a set of experiments using the synthetic classification data  increasing the number of class labels.
　figure 1 shows average test set accuracies for j1. we first generated 1 independent training and testing sets  containing 1 records each. we used synthetic labeling functions 1 1  and 1 from the agrawal generator   randomly varying the constants in functions 1 as described in section 1.
　each column in the figure  models a-g  represents the average of 1 random permutations of the synthetic functions. the anonymizations  rows in the figure  are  optimized  for an increasing number of target models.  for example  the anonymization in the bottom row is optimized exclusively for model a.  there are two important things to note from the chart  and similar behavior was observed for the other classification algorithms.
  looking at each model  column  individually  when the model is included in the anonymization  above the bold line   test set accuracy is higher than when the model is not included  below the line .

figure 1: average test set accuracy for multiple incorporated target models  j1  k=1 

figure 1: imprecision for synthetic function 1
  as we increase the number of included models  moving upward above the line within each column   the test set accuracy tends to decrease. this is because the quality of the anonymization with respect to each individual model is  diluted  by incorporating additional models.
1 selection
　in section 1  we discussed the importance of preserving selections  and described an algorithm for incorporating rectangular selection predicates into an anonymization. we conducted an experiment using the synthetic data  1 generated records   but treating synthetic function 1 as a selection predicate. figure 1 shows the imprecision of this selection when evaluated using the recoded data. the figure shows results for data recoded using three different anonymization algorithms. the first algorithm is median mondrian  with greedy recursive splits chosen from amongst all of the quasi-identifier attributes. it also shows a restricted variation of median mondrian  where splits are made with respect to only age and salary. finally  it shows the results of selection mondrian  incorporating function 1 as three separate rectangular query regions. it is intuitive that imprecision increases with k  and that imprecision is reduced by incorporating the selection into the anonymization.
　incorporating selections can also affect model quality. in the absence of selections  infogain and lsd mondrian choose recursive splits using a greedy criterion driven by the target model s . when selections are included  the resulting partitions may not be the same as those that would be chosen based on the target model s . in the worst case  there may be a selection on an attribute that is uncorrelated with the target attribute.
　to test this intuition  we performed an experiment using the census classification data. to simulate the effect of selections that are uncorrelated with the target model  we first assigned each training tuple to one of n groups  chosen uniformly at random.  we assume .  this mimics the behavior of selection mondrian for a set of equality selections on a new attribute  group number  which takes values 1 ... n. we then anonymized each group independently  using either infogain mondrian or median mondrian. once recodings were determined for each training group  we randomly assigned each test tuple to one of the n groups  and recoded the tuple using the recoding function for that group. finally  we trained a single classification model using the full recoded training set  union of all training groups   and tested using the full recoded test set. this process was repeated for each of ten folds.
　the results of this experiment for j1 are shown in figure 1 g   for increasing n and k = 1. as expected  accuracy decreases slightly as the number of selections  n  increases. however  several selections can be incorporated without large negative effects. similar results were observed for the other classification algorithms.
1 projection
　sometimes not every model constructed by the data recipient will use the full set of predictor attributes; rather  they will use a projected attribute subset. we conducted an experiment to compare anonymization algorithms when only a subset of the released predictor attributes is actually used. first  we ranked the attributes using the original data and a greedy information gain criterion. then we removed the attributes in order  from most to least predictive  and constructed classification models using the remaining attributes. we fixed k = 1.
　as expected  test set accuracy decreases as the most predictive attributes are dropped. however  the rate of this decline varies depending on the anonymization algorithm used. figure 1 h  shows the observed accuracies for j1 using the census database. because of the single-dimensional recoding pattern  which is known to preserve fewer attributes over non-uniform quasi-identifier distributions   this rate of decay is the most precipitous for tds.
　the results were similar for the other classification algorithms and the contraceptives data.
1. related work
　the most closely-related work includes several algorithms that have incorporated a single classification model  constructed over the full data set  while choosing a k-anonymous single-dimensional recoding. the proposed algorithms include top-down  and bottom-up  greedy heuristic searches  and genetic algorithms . each of these papers used the target classification model to evaluate the recoding. additionally  other recent work suggested using a workload of aggregate queries as a tool for evaluating the quality of anonymizations .
　numerous other k-anonymization algorithms have been proposed  1  1  1  1  1  1 . however  much of the previous work has sought to optimize simple general-purpose measures of quality  such as the size of equivalence classes  or the total number of generalizations/suppressions.
　aside from k-anonymity  a variety of other methods have been proposed for protecting individual privacy while allowing certain data mining tasks. one widely-studied approach is based on the randomized response paradigm  1  1  1 . the main advantage of generalization is that the released data is  truthful   though at a coarsened level of granularity. this allows additional workloads to be carried out using the data  including selection. generalization also has similar advantages as compared to data swapping .
　several cluster-based techniques have also been proposed that are similar in spirit to k-anonymity. the condensation approach first divides the data into  condensation groups  with required minimal occupancy  and then generates point data based on the aggregate statistical properties of each group . microaggregation first clusters the data into  ideally homogeneous  groups of required minimal occupancy  and then publishes the centroid of each group . however  neither of these approaches requires that the resulting groups be hyper-rectangular  nor do they handle categorical attributes with hierarchical generalization constraints.
　finally  privacy-preserving histogram sanitization was proposed with the similar goal of guaranteeing that individuals blend into a crowd  based on some suitable distance measure . however  the probabilistic privacy definition does not capture situations where the identification of even a single individual would be considered a breach  and the proof of privacy is highly dependent on the original data distribution.
1. conclusion and future work
k-anonymity and l-diversity are widely-studied techniques
for protecting individual privacy in microdata release. subject to the anonymity requirement  the data should remain as useful as possible with respect to the workload for which it will ultimately be used.
　this paper provided algorithms for incorporating a class of target workloads  consisting of classification or regression models  as well as selection predicates  when generating an anonymous data recoding. an extensive experimental study validated the effectiveness of these algorithms with respect to a variety of workloads. additionally  our results show that simple quality measures are not always indicative of data quality with respect to a particular workload.
　this work also brought to light several interesting opportunities for future work. as described in section 1  anonymization sometimes behaves as a form of feature selection or construction. this has some interesting implications because multidimensional recoding naturally leads to a form of feature selection where different attributes are conditionally retained  at varying levels of granularity  for different data subsets. in the future  it will be valuable to characterize the situations under which this approach leads to better predictive accuracy than traditional feature selection.
　additionally  our selection-oriented anonymization algorithm  section 1  currently only supports selections that can be expressed as rectangular regions. although we expect simple queries to be the most common  we are working to extend this algorithm to a more expressive class of queries.
　finally  a full study of the learning from regions problem is the topic of future research.
acknowledgments
our thanks to bee-chung chen  hector corrada bravo  ted wild  and jude shavlik for insightful conversations  to jesse davis for comments on an earlier draft of this paper  and to benjamin fung for providing an implementation of the tds algorithm.
　this work was supported by an ibm ph.d. fellowship and national science foundation grant iis-1.
