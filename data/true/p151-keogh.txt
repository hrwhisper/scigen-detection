similarity search in large time series databases has attracted much research interest recently. it is a difficult problem because of the typically high dimensionality of the data.. the most promising solutions involve performing dimensionality reduction on the data  then indexing the reduced data with a multidimensional index structure. many dimensionality reduction techniques have been proposed  including singular value decomposition  svd   the discrete fourier transform  dft   and the discrete wavelet transform  dwt .  in this work we introduce a new dimensionality reduction technique which we call adaptive piecewise constant approximation  apca . while previous techniques  e.g.  svd  dft and dwt  choose a common representation for all the items in the database that minimizes the global reconstruction error  apca approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. we show how apca can be indexed using a multidimensional index structure. we propose two distance measures in the indexed space that exploit the high fidelity of apca for fast searching: a lower bounding euclidean distance approximation  and a non-lower bounding  but very tight euclidean distance approximation and show how they can support fast exact searching  and even faster approximate searching on the same index structure. we theoretically and empirically compare apca to all the other techniques and demonstrate its superiority.
categories and subject descriptors
h.1  information search and retrieval : search process.
h.1   systems  multimedia databases.
keywords
indexing  dimensionality reduction  content-based retrieval.
1. introduction
time series account for a large proportion of the data stored in financial  medical and scientific databases. recently there has been much interest in the problem of similarity search  query-bycontent  in time series databases. similarity search is useful in its own right as a tool for exploratory data analysis  and it is also an important element of many data mining applications such as clustering   classification  1  1  and mining of association rules .
the similarity between two time series is typically measured with euclidean distance  which can be calculated very efficiently. however the volume of data typically encountered exasperates the problem. multi-gigabyte datasets are very common. as typical example  consider the machco project. this database contains more than a terabyte of data and is updated at the rate of several gigabytes a day .
the most promising similarity search methods are techniques that perform dimensionality reduction on the data  then use a multidimensional index structure to index the data in the transformed space. the technique was introduced in  and extended in  1  1 . the original work by agrawal et. al. utilizes the discrete fourier transform  dft  to perform the dimensionality reduction  but other techniques have been suggested  including singular value decomposition  svd   1  1  1   the discrete wavelet transform  dwt   1  1  1  and piecewise aggregate approximation  paa   1  1 .
for a given index structure  the efficiency of indexing depends only on the fidelity of the approximation in the reduced dimensionality space. however  in choosing a dimensionality reduction technique  we cannot simply choose an arbitrary compression algorithm. what is required is a technique that produces an indexable representation. for example  many time series can be efficiently compressed by delta encoding  but this representation does not lend itself to indexing. in contrast svd  dft  dwt and paa all lend themselves naturally to indexing  with each eigenwave  fourier coefficient  wavelet coefficient or aggregate segment mapping onto one dimension of an index tree.
the main contribution of this paper is to propose a simple  but highly effective compression technique  adaptive piecewise constant approximation  apca   and show that it can be indexed

permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are permission to make digital or hard copies of all or part of this work for not made or distributed for profit or commercial advantage and that copies personal or classroom use is granted without fee provided that copies are bear this notice and the full citation on the first page.  to copy otherwise  to not made or distributed for profit or commercial advantage and that republish  to post on servers  or to redistribute to lists  requires prior copies bear this notice and the full citation on the first page. to copy specific permission and/or a fee. otherwise  or republish  to post on servers or to redistribute to lists  acm sigmod 1 may 1  santa barbara  california usa requires prior specific permission and/or a fee. copyright 1 acm 1-1/1...$1acm sigmod may  1  santa barbara  california  usa. 
 copyright 1-1-1/1
using a multidimensional index structure. this representation was considered by other researchers  but they suggested it  does not allow for indexing due to its irregularity  .  we will show that indexing apca is possible  and  using apca is up to one to two orders of magnitude more efficient than alternative techniques on real world datasets. we will define the apca representation in detail in section 1  however an intuitive understanding can be gleaned from figure 1.

figure 1: a visual comparison of the time series representation proposed in this work  apca   and the 1 other representations advocated in the literature. for fair comparison  all representations have the same compression ratio.there are many situations in which a user would be willing to sacrifice some accuracy for significant speedup . with this in mind we introduce two distance measures defined on the apca representation. the first tightly lower bounds the euclidean distance metric and is used to produce exact nearest neighbors. the second is not lower bounding  but produces a very close approximation of euclidean distance and can be used to quickly find approximate nearest neighbors. both methods can be supported by the same index structure so that a user can switch between fast exact search and even faster approximate search.
the rest of the paper is organized as follows. in section 1 we provide background on and review related work in time series similarity search. in section 1 we introduce the apca representation and the two distance measures defined on it. in section 1 we demonstrate how to index the apca representation. section 1 contains a comprehensive experimental comparison of apca with all the competing techniques. in section 1 we discuss several advantages apca has over the competing techniques  in addition to being faster. section 1 offers some conclusions.
1. background and related work
given two time series q = {q1 ... qn} and c = {c1 ... cn} their euclidean distance is defined as:

figure 1: the intuition behind the euclidean distance. the euclidean distance can be visualized as the square root of the sum of the squared lengths of the gray lines.
there are essentially two ways the data might be organized :
1. whole matching. here it assumed that all sequences to be compared are the same length n.
1. subsequence matching. here we have a query sequence q  of length n   and a longer sequence c  of length m . the task is to find the subsequence in c of length n  beginning  at ci  which best matches q  and report its offset within c.
whole matching requires comparing the query sequence to each candidate sequence by evaluating the distance function and keeping track of the sequence with the lowest distance. subsequence matching requires that the query q be placed at every possible offset within the longer sequence c. note it is possible to convert subsequence matching to whole matching by sliding a  window  of length n across c  and making copies of the  m-n  windows. figure 1 illustrates the idea. although this causes storage redundancy it simplifies the notation and algorithms so we will adopt this policy for the rest of this paper.

           figure 1: the subsequence matching problem can be converted into the whole matching problem by sliding a  window  of length n across the long sequence and making copies of the data falling within the windows. any indexing scheme that does not examine the entire dataset could potentially suffer from two problems  false alarms and false dismissals. false alarms occur when objects that appear to be close in the index are actually distant. because false alarms can be removed in a post-processing stage  by confirming distance estimates on the original data   they can be tolerated so long as they are relatively infrequent. a false dismissal is when qualifying objects are missed because they appear distant in index space.
we will refer to similarity-searching techniques that guarantee no false dismissals as exact  and techniques that do not have this guarantee as approximate. we will review approximate techniques in section 1 and exact techniques in section 1.
1 approximate techniques for similarity searching
several researchers have suggested abandoning the insistence on exact search in favor of a much faster search that returns approximately the same results. typically this involves transforming the data with a lossy compression scheme  then doing a sequential search on the compressed data. typical examples include  1  1  1  1   who all utilize a piecewise linear approximation. others have suggested transforming the data into a discrete alphabet and using string-matching algorithms  1  1  1  1  1  1 . all these approaches suffer from some limitations. they are all evaluated on small datasets residing in main memory  and it is unclear if they can be made to scale to large databases.
the work of  1  1  1  1  1  differs from the above in that they focus in providing a more flexible query language and not on performance issues.
1 exact techniques for similarity searching.
a time series c = {c1 ... cn} with n datapoints can be considered as a point in n-dimensional space. this immediately suggests that time series could be indexed by a multidimensional index structure such as the r-tree and its many variants . since realistic queries typically contain 1 to 1 datapoints  i.e. n varies from 1 to 1  and most multidimensional index structures have poor performance at dimensionalities greater than 1   we need to first perform dimensionality reduction in order to exploit multidimensional index structures to index time series data. in  the authors introduced generic multimedia
indexing method  gemini  which can exploit any dimensionality reduction method to allow efficient indexing. the technique was originally introduced for time series  but has been successfully extend to many other types of data .
an important result in  is that the authors proved that in order to guarantee no false dismissals  the distance measure in the index space must satisfy the following condition:
	dindex space a b   ¡Ü  dtrue a b  	 	 1 
this theorem is known as the lower bounding lemma or the contractive property. given the lower bounding lemma  and the ready availability of off-the-shelf multidimensional index structures  gemini requires just the following three steps.
 	establish a distance metric from a domain expert  in this case euclidean distance .
 produce a dimensionality reduction technique that reduces the dimensionality of the data from n to n  where n can be efficiently handled by your favorite index structure.
 produce a distance measure defined on the n dimensional representation of the data  and prove that it obeys dindex space a b   ¡Ü  dtrue a b .
the efficiency of the gemini query algorithms depends only on the quality of the transformation used to build the index. the tighter the bound on dindex space a b   ¡Ü  dtrue a b  the better  as tighter bounds imply fewer false alarms hence lower query cost . time series are usually good candidates for dimensionality reduction because they tend to contain highly correlated features. for brevity  we will not describe the three main dimensionality reduction techniques  svd  dft and dwt  in detail. instead we refer the interested reader to the relevant papers or to  which contains a survey of all the techniques. we will briefly revisit related work in section 1 when the reader has developed more intuition about our approach.
1. adaptive representation
in recent work keogh et. al.  and yi & faloutsos  independently suggested approximating a time series by dividing it into equal-length segments and recording the mean value of the datapoints that fall within the segment. the authors use different names for this representation  for clarity we will refer to it as piecewise aggregate approximation  paa . this simple technique is surprisingly competitive with the more sophisticated transforms.
the fact that each segment in paa is the same length facilitates indexing of this representation. suppose however we relaxed this requirement and allowed the segments to have arbitrary lengths  does this improve the quality of the approximation  before we consider this question  we must remember that the approach that allows arbitrary length segments  which we call adaptive piecewise constant approximation  apca   requires two numbers per segment. the first number records the mean value of all the datapoints in segment  the second number records the length. so a fair comparison is n paa segments to m apca segments  were m =  n/1 .
it is difficult to make any intuitive guess about the relative performance of the two techniques. on one hand paa has the advantage of having twice as many approximating segments. on the other hand apca has the advantage of being able to place a single segment in an area of low activity and many segments in areas of high activity. in addition one has to consider the structure of the data in question. it is possible to construct artificial datasets where one approach has an arbitrarily large reconstruction error  while the other approach has reconstruction error of zero.
figure 1 illustrates a fair comparison between the two techniques on several real datasets. note that for the task of indexing  subjective feelings about which technique  looks better  are irrelevant. all that matters is the quality of the approximation  which is given by the reconstruction error  because lower reconstruction errors result in tighter bounds dindex space a b   ¡Ü dtrue a b . .
1 the apca representation
given a time series c = {c1 ... cn}  we need to be able to produce an apca representation  which we will represent as
	c ={ cv1 cr1  ...  cvm crm }  	cr1 = 1	 1 
where cvi is the mean value of datapoints in the ith segment  i.e. cvi = mean  ccri 1 ... ccri    and cri the right endpoint of the ith segment. we do not represent the length of the segments but record the locations of their right endpoints instead for indexing reasons as will be discussed in section 1. the length of the ith segment can be calculated as cri - cri-1. figure 1 illustrates this notation.

figure 1: a time series c and its apca representation c  with m = 1
in general  finding the optimal piecewise polynomial representation of a time series requires a o nn1  dynamic programming algorithm  1  1 . for most purposes  however  an optimal representation is not required. most researchers  therefore  use a greedy suboptimal approach instead  1  1  1 . in this work we utilize an original algorithm which produces high quality approximations in o nlog n  . the algorithm works by first converting the problem into a wavelet compression problem  for which there are well known optimal solutions  then converting the solution back to the acpa representation and  possibly  making minor modifications.
1 distance measures defined for apca
suppose we have a time series c  which we convert to the apca representation c  and a query time series q. clearly  no distance measure defined between q and c can be exactly equivalent to the euclidean distance  d q c   defined in equation 1.  because c generally contains less information than c. however  we will define two distance measures between q and c that approximate d q c . the first  dae q c  is designed to be a very tight approximation of the euclidean distance  but may not always lower bound the euclidean distance d q c . the second  dlb q c  is generally a less tight approximation of the euclidean distance  but is guaranteed to lower-bound  a property necessary to utilize the gemini framework. these distance measures are defined below  figure 1 illustrates the intuition behind the formulas.
1.1 an approximate euclidean measure dae
given a query q  in raw data format  and a time series c in the apca representation  dae q c  is defined as:
	dae q c  ¡Ô ¡Æim=1¡Æcrk=i1 cri 1 cvi  qk+cri 1  1	 1 
this measure can be efficiently calculated in o n   and it tightly approximates the euclidean distance  unfortunately it has a drawback which prevents its use for exact search.
proposition 1 dae q c  does not satisfy the triangular inequality proof: by counter example.
consider the time series a = {-1  -1  -1  1  1}  b = {1  1  1  -1  -1} and
c = {1  1  1  1  -1} their apca representations a = { -1    1
 1 }  b = { 1  1   -1 }  c = { 1  1    -1  1 } the triangular inequality states that for any objects ¦Á  ¦Â and ¦Ö
d ¦Á ¦Â  ¡Ü d ¦Á ¦Ö  + d ¦Â ¦Ö 
assume that dae q c  does satisfy the triangular inequality  then we can write
dae a b  ¡Ü  dae a c  + dae b c 
apply equation 1.1 ¡Ü 1 + 1
but this implies	1 ¡Ü 1
so the assumption was wrong and dae q c  does not satisfy the triangular inequality. 
the failure of dae to obey the triangular inequality means that it may not lower bound the euclidean distance and thus cannot be used for exact indexing . however  we will demonstrate later that it is very useful for approximate search.
1.1 an lower-bounding measure dlb
to define dlb q c  we must first introduce a special version of the apca. normally the algorithm mentioned in section 1 is used to obtain this representation.  however we can also obtain this representation by  projecting  the endpoints of c onto q  and finding the mean value of the sections of q that fall within the projected intervals. a time series q converted into the apca representation this way is denoted as q'. the idea can be visualized in figure 1 iii.
q' is defined as:
q' ={ qv1 qr1  ...  qvm qrm } 
where qri = cri and qvi = mean  qcri 1 ... qcri   
dlb q' c  is defined as:  1 	dlb q' c  ¡Ô ¡Æim=1 cri  cri 1  qvi  cvi  1	 1 
this distance measure does lower bound the euclidean distance. for brevity we omit the proof which is a generalization of the proof for the special case of equal length segments in .

figure 1: a visualization of the two distance measures define on the apca representation. i  a query time series q and a apca object c. ii  the dae measure can be visualized as the euclidean distance between q and the reconstruction of c. iii  q' is obtained by projecting the endpoints of c onto q and calculating the mean values of the sections falling within the projected lines. iiii  the dlb measure can be visualized as the square root of the sum of the product of squared length of the gray lines with the length of the segments they join.
1. indexing apca
the apca representation proposed in section 1 defines a ndimensional feature space  n = 1m . in other words  the proposed representation maps each time series c = {c1 ... cn} to a point  c = {cv1  cr1  ...  cvm  crm}  in a n-dimensional space. we refer to the
n-dimensional space as the apca space and the points in the apca space as apca points.  in this section  we discuss how we can index the apca points using a multidimensional index structure  e.g.  r-tree  and use the index to answer range and k nearest neighbors  k-nn  queries efficiently.  we will concentrate on k-nn queries in this section; range queries will be discussed briefly at the end of the section.
algorithm exactknnsearch q k 
variable queue: minpriorityqueue;
variable list: temp;
1. queue.push root node of index  1 ;
1. while not queue.isempty   do
1. top = queue.top  ;
1. for each time series c in temp such that d q c  ¡Ü top.dist
1. remove c from temp;
1. add c to result;
1. if |result| = k return result; 
1. queue.pop  ;
1. if top is an apca point c
1. retrieve full time series c from database;
1. temp.insert c  d q c  ;
1. else if top is a leaf node
1. for each data item c in top
1. queue.push c  dlb q' c  ;
1. else 	  // top is a non-leaf node
1. for each child node u in top
1. queue.push u  mindist q r   // r
is mbr associated with u
table 1: k-nn algorithm to compute the exact k nearest neighbors of a query time series q using a multidimensional index structure.
a k-nn query  q  k  with query time series q and desired number of neighbors k retrieves a set c of k time series such that for any two time series c ¡Ê  c  e    c  d q  c  ¡Ü d q  e . the algorithm for answering k-nn queries using a multidimensional index structure is shown in table 1. the above algorithm is an optimization on the gemini k-nn algorithm described in table 1 and was proposed in . like the basic k-nn algorithm  1   the algorithm uses a priority queue queue to navigate nodes/objects in the index in the increasing order of their distances from q in the indexed  i.e. apca  space. the distance of an object  i.e. apca point  c from q is defined by dlb q' c   cf. section 1.1  while the distance of a node u from q is defined by  the minimum distance mindist q r  of the minimum bounding rectangle  mbr  r associated with u from q  definition of mindist will be discussed later . initially  we push the root node of the index into the queue  line 1 . subsequently  the algorithm navigates the index by popping out the item from the top of queue at each step  line 1 . if the popped item is an apca point c  we retrieve the original time series c from the database  compute its exact distance d q c  from the query and insert it into a temporary list temp  lines 1 . if the popped item is a node of the index structure  we compute the distance of each of its children from q and push them into queue  lines 1 . we move a time series c from temp to result only when we are sure that it is among the k nearest neighbors of q i.e. there exists no object e   result such that d q e    d q c  and |result|   k. the second condition is ensured by the exit condition in line 1. the first condition can be guaranteed as follows. let i be the set of apca points retrieved so far using the index  i.e. i = temp ¡È result .  if we can guarantee that   c ¡Ê  i    e    i 
dlb q' c  ¡Ü d q e   then the condition  d q c  ¡Ü top.dist  in line 1 would ensure that there exists no unexplored time series e such that d q  e    d q c . by inserting the time series in temp  i.e. already explored objects  into result in increasing order of their distances d q c   by keeping temp sorted by d q c    we can ensure that there exists no explored object e such that d q 
e    d q c . in other words  if   c ¡Ê  i    e    i  dlb q' c  ¡Ü d q e   the above algorithm would return the correct answer.
before we can use the above algorithm  we need to describe how to compute mindist q r  such that the correctness requirement is satisfied i.e.   c ¡Ê  i    e    i  dlb q' c  ¡Ü d q e .  we now discuss how the mbrs are computed and how to compute mindist q r  based on the mbrs. we start by revisiting the traditional definition of an mbr . let us assume we have built an index of the apca points by simply inserting the apca points c = {cv1  cr1  ...  cvm  crm} into a mbr-based multidimensional index structure  using the insert function of the index structure .  let u be a leaf node of the above index. let r =  l  h  be the mbr associated with u where l = {l1  l1  ...  ln} and h = {h1  h1  ...  hn} are the lower and higher endpoints of the major diagonal of r. by definition  r is the smallest rectangle that spatially contains each apca point c = {cv1  cr1  ...  cvm  crm} stored in u. formally  r =  l  h  is defined as:
definition 1  old definition of mbr 
	li = minc inu cv i+1  / 1    if i is odd	  1 
= mincl inu cri/1       if i is even
hi =maxcl inu cv i+1 /1  if i is odd 
= maxcl inu cri/1      if i is even
the mbr associated with a non-leaf node would be the smallest rectangle that spatially contains the mbrs associated with its immediate children .

figure 1: definition of cmaxi and cmini for computing mbrs
however  if we build the index as above  i.e. the mbrs are computed as in definition 1   it is not possible to define a mindist q r  that satisfies the correctness criteria. to overcome the problem  we define the mbrs are follows. let us consider the mbr r of a leaf node u. for any apca point c = {cv1  cr1 ...  cvm crm } stored in node u  let cmaxi and cmini denote the maximum and minimum values of the corresponding time series c among the datapoints in the ith segment  i.e.
	    cmaxi = maxcrt=icri 1 ct             and  	 1 
cmini = mintcr=icri 1 ct   
the cmaxi and cmini of a simple time series with 1 segments is shown in figure 1.
we define the mbr r =  l  h  associated with u as follows: definition 1  new definition of mbr 
	li = minc inu cmin i+1 / 1  if i is odd   	 1 
= minc inu cri / 1         if i is even 
        hi = maxc inu cmax i+1 / 1 if i is odd
	 	   = maxc inu cri / 1         if i is even 
as before  the mbr associated with a non-leaf node is defined as the smallest rectangle that spatially contains the mbrs associated with its immediate children.
how do we build the index such that the mbrs satisfy definition 1. we insert rectangles instead of the apca points. in order to insert an apca point c = {cv1 cr1 .. cvm crm}  we insert a

rectangle c =  {cmin1  cr1  ...  cminm  crm} { cmax1  cr1  ...  cmaxm  crm}   i.e. {cmin1  cr1  ...  cminm  crm} and { cmax1  cr1  ...  cmaxm  crm}  are the lower and higher endpoints of the major

diagonal of c   into the multidimensional index structure  using the insert function of the index structure . since the insertion algorithm ensures that the mbr r of a leaf node u spatially

contains all the c 's stored in u  r satisfies definition 1. the same is true for mbrs associated with non-leaf nodes. since we use one of the existing multidimensional index structures for this purpose  the storage organization of the nodes follows that of the index structure  e.g.   mbr  child ptr  array if r-tree is used  kdtree if hybrid tree is used . for the leaf nodes  we need to store the cvi's of each data point  in addition to the cmaxi's  cmini's and cri's  since they are needed to compute dlb  line 1 of the k-nn algorithm in table 1 . the index can be optimized  in terms of leaf node fanout  by not storing the cmaxi's and cmini's of the data points at the leaf nodes i.e. just storing the cvi's and cri's  a total of 1m numbers  per data point in addition to the tuple identifier. the reason is that the cmaxi's and cmini's are not required for computing dlb  and hence are not used by the k-nn algorithm. they are needed just to compute the mbrs properly  according to definition 1  at the time of insertion. the only time they are needed later  after the time of insertion  is during the recomputation of the mbr of the leaf node containing the data point after a node split. the insert function of the index structure can be easily modified to fetch the cmaxi's and cmini's of the necessary data points from the database  using the tuple identifiers  on such occasions. the small extra cost of such fetches during node splits is worth the improvement in search performance due to higher leaf node fanout. we have applied this optimization in the index structure for our experiments but we believe the apca index would work well even without this optimization.
once we have built the index as above  i.e. the mbrs satisfy definition 1   we define the minimum distance mindist q r  of the mbr r associated with a node u of the index structure from the query time series q. for correctness    c ¡Ê  i    e    i 
dlb q' c  ¡Ü d q e   where i denotes the set of apca points retrieved using the index at any stage of the algorithm . we show that the above correctness criteria is satisfied if mindist q r  lower bounds the euclidean distance d q c  of q from any time series c placed under u in the index.
lemma 1:
   if mindist q r  ¡Ü d q c  for any time series c placed under u  the algorithm in table 1 is correct i.e.   c ¡Ê  i    e    i  dlb q' c  ¡Ü d q e  where i denotes the set of apca points retrieved using the index at any stage of the algorithm.
proof:
   according to the k-nn algorithm  any item e    i must satisfy one of the following conditions:
1  e has been inserted into the queue but has not been popped yet  i.e.   c ¡Ê  i  dlb q'  c  ¡Ü dlb q' e 
1  e has not yet been inserted into the queue i.e. there exists a parent node u of e whose mbr r satisfies the following condition:   c ¡Ê  i  dlb q' c  ¡Ü mindist q r .
since dlb q' e  ¡Ü d q e   see section 1.1    1 
implies    c ¡Ê  i  dlb q' c  ¡Ü d q e . if
mindist q r  ¡Ü d q e  for any time series e under u   1  implies that   c ¡Ê  i  dlb q'  c  ¡Ü d q e . since
either  1  or  1  must be true for any item e    i    c ¡Ê i    e    i   dlb q' c  ¡Ü d q e .
a trivial definition mindist q r  that lower bounds d q c  for any time series c under u is mindist q r  = 1 for all q and r. however  this definition is too conservative and would cause the k-nn algorithm to visit all nodes of the index structure before returning any answer  thus defeating the purpose of indexing . the larger the mindist  the more the number of nodes the knn algorithm can prune  the better the performance. we provide such a definition of mindist below.
region 1
	r	h = {h1  h1  h1  h1  h1  h1}
g1 = {l1  l1  h1  h1}

	time  end points  axis	l={l1  l1  l1  l1  l1  l1}
figure 1: the m regions associated with a 1m-dimensional mbr. the boundary of a region g is denoted by g = {g  g  g  g}
let us consider a node u with mbr r =  l h . we can view the mbr as two apca representations l={l1  l1  ...  ln} and h = {h1  h1  ...  hn}. the view of a 1-dimensional mbr  {l1 l1 .. l1}  {h1 h1 ... h1}  as two apca representations {l1  l1  ...  l1} and {h1  h1  ...  h1} is shown in figure 1. any time series c = {c1  c1 ...  cn} under the node u is  contained   within the two bounding time series l and h  as shown in figure 1 . in order to formalize this notion of containment  we define a set of m regions associated with r. the ith region gri  i = 1 ...  m  associated with r is defined as the 1-dimensional rectangular region in the valuetime space that fully contains the ith segment of all time series stored under u. the boundary of a region g  being a 1-d rectangle  is defined by 1 numbers: the low bounds g and g and the high bounds g and g  along the value and time axes respectively.
by definition 
	gir= minc underu  cmini   	            1 
gir=minc underu  cri 1 +1 
gir= maxc underu  cmaxi  
gir=maxc underu  cri  
r
based the definition of mbr in definition 1  gi can be defined in terms of the mbr r as follows:
definition 1  definition of regions associated with mbr 
	gir  = l{1i-1}   	               1 
gir  = l{1i-1}+1
gir  = h{1i-1}
gir  = h{1i}
figure 1 shows the 1 regions associated with the 1-dimensional mbr  {l1 l1 .. l1}  {h1 h1 ... h1} . at time instance t  t = 1 ... n   we say a region gir is active iff gir  ¡Ü t ¡Ü gir . for example  in figure 1  only regions 1 and 1 are active at time instant t1 while regions 1  1 and 1 are active at time instant t1. the value ct of a time series c under u at time instant t must lie within one of the regions active at t i.e. ¡Å r gir  ¡Ü ct ¡Ü
                                                                 gi is active gir .
1 lemma 1:the value ct of c under u at time instant t must lie within one of the regions active at t.
proof:
let us consider a region gir that is not active at time instant t i.e. either gir    t or gir    t.  first  let us consider the case gir    t. by definition  gir  ¡Ücri-1
+ 1 for any c under u . since gir    t  t    cri-1 + 1 i.e. ct is not in segment i.
now let us consider the case gir    t. by definition 
gir  ¡Ý cri for any c under u. since gir    t  t   cri i.e. ct is not in segment i.
hence  if region gir is not active at t  ct cannot lie in segment i i.e. ct can lie in segment i only if gir is active. by definition of regions  ct must lie within one of the regions active at t i.e.
¡Ågir is active gir  ¡Ü ct ¡Ü gir .

= min  mindist q  g1r  t1   mindist q  g1r  t1  	= min  mindist q  g1r  t1   mindist q  g1r  t1   mindist q  g1r  t1  
= min   qt1-h1   qt1-h1 	= min   qt1-h1  1   qt1-h1 
=  qt1-h1	= 1
figure 1: computation of mindist
given a query time series q = {q1  q1  ...  qn}  the minimum distance mindist q r t  of q from r at time instant t  cf. figure
1  is given by 	mindist q g t  where
minregiong is active at t
	mindist q g t  	=  g-qt  1 if qt   g  1 
  
	                         	=  qt -g 1 if g   qt
                                = 1 otherwise. mindist q r  is defined as follows:
¡Æ
n
	mindist q r  = mindist q r t 	 1 
t=1
lemma1: mindist q r  lower bounds d q c  for any time series c under u.
proof:
we will first show mindist q r t   lower bounds d q c t   =  qt-ct 1 for any time series c under u. we know that ct must lie in one of the active regions  lemma 1 . without loss of generality  let us assume that ct lies in an active region g i.e. g ¡Ü ct ¡Ü g. hence mindist q g t  ¡Ü d q c t . also  mindist q r t   = mindist q g t   by definition of mindist q r t  . hence mindist q r t   lower bounds d q c t . since
mindist q r  = ¡Æn mindist q r t  and d q c  =
t=1
¡Æn mindist q c t    mindist q r t  ¡Ü d q c t 
       t=1 implies mindist q r  ¡Ü d q c .
note that  in general  lower the number of active regions at any instant of time  higher the mindist  better the performance of the k-nn algorithm. also  narrower the regions along the value dimension  higher the mindist. the above two principles justify our choice of the dimensions of the apca space. the odd dimensions help clustering apca points with similar cvi's  thus keeping the regions narrow along the value dimension. the even dimensions help clustering apca points that are approximately aligned at the segment end points  thus ensuring only one region  minimum possible  is active for most instants of time.
algorithm exactrangesearch q  ¦Å t 
1. if t is a non-leaf node
1. for each child u of t
1. if mindist q r ¡Ü ¦Å   exactrangesearch q  ¦Å u ;
// r is mbr of u
1. else 	      // t is a leaf node
1. for each apca point c in t
1. if dlb q' c ¡Ü ¦Å
1. retrieve full time series c from database;
1. if d q c  ¡Ü ¦Å  add c to result;
table 1: range search algorithm to retrieve all the time series within a range of ¦Å from query time series q. the function is invoked as
exactrangesearch q  ¦Å root node of index .
although we have focussed on k-nn search in this section  the definitions of dlb and mindist proposed in this paper are also needed for answering range queries using a multidimensional index structure. the range search algorithm is shown in table 1. it is a straightforward r-tree-style recursive search algorithm combined with the gemini range query algorithm shown in table 1. since both mindist q r  and dlb q' c  lower bound d q c    the above algorithm is correct .
in this section  we described how to find the exact nearest neighbors of a query time series using a multidimensional index structure. in section 1.1  we proposed an approximate euclidean distance measure dae q c  for fast approximate search. if we want to use the same index structure to answer both exact queries and approximate queries  we can simply replace the distance function dlb q c  in line 1 of the k-nn algorithm  table 1  by dae q c  to switch from exact to approximate queries and vice-versa. since dae q c  is a tighter approximation of d q c  than dlb q' c   the k-nn algorithm would need to retrieve fewer apca points from the index before the algorithm stops. this would result in fewer disk accesses to retrieve the full time series corresponding to the retrieved apca points  line 1 of table 1   leading to lower query cost. since the approximate distance dae q c  between a time series query q = {q1  q1 ...qn} and an apca point  c = {cv1  cr1  ...  cvm  crm} almost always lower bounds the euclidean distance d q c  between q and the original time series c = {c1  c1 ...  cn}  see figure 1   the approximate function can be used to get reasonably accurate results more efficiently using the same index structure.
if an index is used exclusively for approximate search based on dae  further optimizations are possible. for such an index  we can construct the mbrs as defined in definition 1 i.e. by inserting the apca point  c = {cv1  cr1  ...  cvm  crm} itself instead of the corresponding rectangle  {cmin1  cr1 ...  cminm  crm}  { cmax1  cr1  ...  cmaxm  crm} . the mindist computation is the same as in the exact case. it can be shown that mindist q r  of the query from the above mbr  definition 1  lower bounds dae q c   therefore ensuring retrieval of apca points in the order of their distances dae q c . since these mbrs are always smaller than the mbrs in definition 1  the mindists will be larger resulting in fewer node accesses of the index structure compared to approximate search using the same index as the exact search and hence even better performance. to exploit this optimization  one can maintain two separate indices  one with mbrs as defined in definition 1 and one with that defined in definition 1  for exact and approximate searches respectively.
1. experimental evaluation
in this section we will experimentally demonstrate the superiority of apca in terms of query response time.
for completeness we experimentally compare all the state of the art indexing techniques with our proposed method. we have taken great care to create high quality implementations of all competing techniques. for example we utilized the symmetric properties of the dft as suggested in . additionally when taking the dft of a real signal  the first imaginary coefficient is zero  and because all objects in our database have had their mean value subtracted  the first real coefficient is also zero. we do not include these constants in the index  making room instead for two additional coefficients that carry information. all other approaches are similarly optimized.
1 experiment methodology
we performed all tests over a range of reduced dimensionalities  n  and query lengths  i.e original dimensionalities  n . because we wanted to include the dwt in our experiments  we are limited to query lengths that are an integer power of two. we consider a length of 1 to be the longest query likely to be encountered  by analogy  one might query a text database with a word  a phrase or a complete sentence  but the would be little utility in a paragraphlength text query. a time series query of length 1 corresponds approximately with sentence length text query .
we tested on two datasets  one chosen because it is very heterogeneous and one chosen because it is very homogenous.
 homogenous data: electrocardiogram. this dataset is taken from the mit research resource for complex physiologic signals . it is a  relatively clean and uncomplicated  electrocardiogram. the total size of the data is 1 objects.
 heterogeneous data: mixed bag. this dataset we created by combining 1 datasets with widely varying properties of shape  structure  noise etc. the only preprocessing performed was to insure that each time series had a mean of zero and a standard deviation of one. the 1 datasets are  space shuttle sts-1  1  1   arrhythmia   random walk  1  1 
1  1   interball plasma processes  figure 1    astrophysical data  figure 1    pseudo periodic synthetic time series . exchange rate  figure 1  . the total size of the data is 1 objects.
to perform realistic testing we need queries that do not have exact matches in the database but have similar properties of shape  structure  spectral signature  variance etc. to achieve this we used cross validation. we removed 1% of the dataset  and build the index with the remaining 1%. the queries are then randomly taken from the withheld subsection. for each result reported for a particular dimensionality and query length  we averaged the results of 1 experiments.
for simplicity we only show results for nearest neighbor queries  however we obtained similar results for range queries.
1 experimental results: pruning power
in comparing the four competing techniques there exists a danger of implementation bias. that is  consciously or unconsciously implementing the code such that some approach is favored. as an example of the potential for implementation bias in this work consider the following. at query time dft must do a fourier transform of the query. we could use the na ve algorithm which is o n1  or the faster radix-1 algorithm  padding the query with zeros for n ¡Ù 1integer  which is o nlogn . if we implemented the simple algorithm it would make the other indexing methods appear to perform better relative to dft. while we do present detailed experimental evaluation of an implemented system in the next section  we also present experiments in this section which are free of the possibility of implementation basis. we achieve this by comparing the pruning power of the various approaches.
to compare the pruning power of the four techniques under

figure 1: the fraction p  of the mixed bag database that must be examined by the four dimensionality reduction techniques being compared  over a range of query lengths  1  and dimensionalities  1 .consideration we measure p  the fraction of the database that must be examined before we can guarantee that we have found the nearest match to a 1-nn query.
	p= numbernumberof objectsof objectsthat mustin databasebe examined	 1 
to calculate p we do the following. random queries are generated  as described above . objects in the database are examined in order of increasing  feature space  distance from the query until the distance in feature space of the next unexamined object is greater than minimum actual distance of the best match so far. the number of objects examined at this point is the absolute minimum in order to guarantee no false dismissals.
note the value of p for any transformation depends only on the data and is completely independent of any implementation choices  including spatial access method  page size  computer language or hardware. a similar idea for evaluating indexing schemes appears in .
figure 1 shows the value of p over a range of query lengths and dimensionalities for the experiments that were conducted the mixed bag dataset.
note that the results for paa and dwt are identical. this because the pruning power of dwt and paa are identical when n = 1integer . having empirically shown this fact which was proved in  1  1  we have excluded paa from future experiments for clarity. we repeated the experiment for the electrocardiogram data  the results are shown in figure 1.
in both figure 1 and 1 we can see that apca outperforms dft and dwt significantly  generally by an order of magnitude. these experiments indicate that the apca technique has fewer false alarms  hence lower query cost as confirmed by the experiments below.
1 experimental results: implemented system
although the pruning power experiments are powerful predictors of the  relative  performance of indexing systems using the various dimensionality reduction schemes  we include a comparison of implemented systems for completeness. we implemented four indexing techniques: linear scan  dft-index  dwt-index and apca-index. we compare the four techniques in terms of the i/o and cpu costs incurred to retrieve the exact nearest neighbor of a query time series. all the experiments reported in this subsection were conducted on a sun ultra enterprise 1 machine with 1 gb of physical memory and several gb of secondary storage  running solaris 1.
cost measurements:
we measured the i/o and cpu costs of the four techniques as follows:
 1  linear scan  ls : in this technique  we perform a simple linear scan on the original n-dimensional dataset and determine the exact nearest neighbor of the query. the i/o cost in terms of sequential disk accesses is
 s* n*sizeof float  + sizeof id   / pagesize . since sizeof id      n*sizeof float    we will ignore the sizeof id  henceforth. assuming sequential i/o is about 1 times faster than random
	i/o  	the 	cost 	in 	terms 	of 	random 	accesses 	is
 s*sizeof float *n / pagesize*1 . the cpu cost is the cost of computing the distance d q c  of the query q from each time series c = {c1  ...  cn} in the database.
 1  dft-index  dft : in this technique  we reduce the dimensionality of the data from n to n using dft and build an index on the reduced space using a multidimensional index structure. we use the hybrid tree as the index structure. the i/o cost of a query has two components:  1  the cost of accessing the nodes of the index structure and  1  the cost of accessing the pages to retrieve the full time series from the database for each indexed item retrieved  cf. table 1 . for the second component  we assume that a full time series access costs one random disk access. the total i/o cost  in terms of random disk accesses  is the number of index nodes accessed plus the number of indexed items retrieved by the k-nn algorithm before the algorithm stopped  i.e. before the distance of the next unexamined object in the indexed space is greater than the minimum of the actual distances of items retrieved so far . the cpu cost also has two components:  1  the cpu time  excluding the i/o wait  taken by the k-nn algorithm to navigate the index and retrieve the indexed items


figure 1: the fraction p  of the electrocardiogram database that must be examined by the three dimensionality reduction techniques being compared over a range of query lengths  1  and dimensionalities  1 .
and  1  the cpu time to compute the exact distance d q c  of the query q from the original time series c of each indexed item c retrieved  line 1 in table 1 . the total cpu cost is the sum of the two costs.
 1  dwt-index  dwt : in this technique  we reduce the dimensionality of the data from n to n using dwt and build the index on the reduced space using the hybrid tree index structure. the i/o and cpu costs are computed in the same way as in dft.
 1  apca-index  apca : in this technique  we reduce the dimensionality of the data from n to n using apca and build the index on the reduced space using the hybrid tree index structure. the i/o and cpu costs are computed in the same way as in dft and dwt.
we chose the hybrid tree as the index structure for our experiments since it is a space partitioning index structure   dimensionality-independent  fanout  and has been shown to scale to high dimensionalities  1  1  1 . since we had access to the source code of the index structure      http://www-db.ics.uci.edu  we implemented the optimization discussed in section 1  i.e. to increase leaf node fanout  for our experiments. we used a page size of 1kb for all our experiments.
dataset: we used the electrocardiogram  ecg  database for these experiments. we created 1 datasets from the ecg database by choosing 1 different values of query length n  1  1 and 1 . for each dataset  we reduced the dimensionality to n = 1  n = 1 and n = 1 using each of the 1 dimensionality reduction techniques  dft  dwt and apca  and built the hybrid tree indices on the reduced spaces  resulting a total of 1 indices for each technique . as mentioned before  the queries were chosen randomly from the withheld section of the dataset. all our measurements are averaged over 1 queries.
figure 1 compares the ls  dft  dwt and apca techniques in terms of i/o cost  measured by the number of random disk accesses  for the 1 datasets  n = 1  1 and 1  and 1different dimensionalities of the index  n = 1  1 and 1 . the apca technique significantly outperforms the other 1 techniques in terms of i/o cost. the ls technique suffers due to the large database size  e.g.  1 sequential disk accesses for n = 1 which is equivalent to 1 random disk accesses . although ls is not considerably worse than apca in terms of i/o cost  it is significantly worse in terms of the overall cost due to its high cpu cost component  see figure 1 . the dft and dwt suffer mainly due to low pruning power  cf. figure 1 . since dft and dwt retrieve a large number of indexed items before it can guaranteed that the exact nearest neighbor is among the retrieved items  the second component of the i/o cost  that of retrieving full time series from the database  tends to be high. the dft and dwt costs are the highest for large n and small n  e.g.  n = 1  n=1  as the pruning power is the lowest for those values  cf. figure 1 .  the dwt technique shows a u-shaped curve for n = 1: when the reduced dimensionality is low  n = 1   the second component of the i/o cost is high due to low pruning power  while when n is high  n = 1   the first component of the i/o cost  index node accesses  becomes large due to dimensionality curse. we did not observe such u-shaped behavior in the other techniques as their costs were either dominated entirely by the first component  e.g.  n = 1 and n = 1 cases of apca  or by the second component  all of dft and n = 1 case of apca .
figure 1 compares the ls  dft  dwt and apca techniques in terms of cpu cost  measured in seconds  for the 1 datasets  n = 1  1 and 1  and 1 different dimensionalities of the index  n = 1  1 and 1 . once again  the apca technique significantly outperforms the other 1 techniques in terms of cpu cost. the ls technique is the worst in terms of cpu cost as it computes the exact  n-dimensional  distance d q c  of the query q from every time series c in the database. the dft and dwt techniques suffer again due to their low pruning power  cf. figure 1   causing the second component of the cpu cost  i.e. the time to compute the exact distances d q c  of the original time series of the retrieved apca points from the query  to become high.	ls	dft	dwt	apcafigure 1: comparison of ls  dft  dwt and apca techniques in terms of i/o cost   number of random disk accesses . for ls  the cost is computed as number sequential disk accesses/1.
figure 1: comparison of ls  dft  dwt and apca techniques in terms of cpu cost   seconds .

1. discussion
now that the reader is more familiar with the contribution of this paper we will briefly revisit related work. we believe that this paper is the first to suggest locally adaptive indexing time series indexing. a locally adaptive representation for 1-dimensional shapes was suggested in  but no indexing technique was proposed. also in the context of images  it was noted by  that the use of the first n fourier coefficients does not guarantee the optimal pruning power. they introduced a technique where they adaptively choose which coefficients to keep after looking at the data. however  the choice of coefficients was based upon a global view of the data. later work  in the context of time series noted that the policy of using the first n wavelet coefficients  1  1  1  is not generally optimal  but  keeping the largest coefficients needs additional indexing space and  more complex  indexing structures . singular value decomposition is also a data adaptive technique used for time series  1  1  1   but it is globally  not locally  adaptive. recent work  has suggested first clustering a multi-dimensional space and then doing svd on local clusters  making it a semi-local approach. it is not clear however that this approach can be made work for time series. finally a representation similar to apca was introduced in   under the name  piecewise flat approximation   but no indexing technique was suggested.
1. conclusions
the main contribution of this paper is to show that a simple  novel dimensionality reduction technique  namely apca  can outperform more sophisticated transforms by one to two orders of magnitude. in contrast to popular belief  1  1   we have shown that the apca representation can be indexed using a multidimensional index structure. in addition to fast exact queries  the approach also allows even faster approximate querying on the same index structure. we have also shown that our approach can support arbitrary lp norms  again using a single index structure.
future directions for research include further increasing the speedup of our method by exploiting the similarity of adjacent sequences  in a similar spirit to the  trail indexing  technique introduced in  . additionally we intend to explore the possibility of local adaptability for other representations and problems.
acknowledgments
this work was supported in part by the national science foundation  information technology research under grant no. iis-1 and in part by the army research laboratory under cooperative agreement no. daal-1-1 and cooperative agreement no. daad-1-1.
