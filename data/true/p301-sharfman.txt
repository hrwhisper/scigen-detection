monitoring data streams in a distributed system is the focus of much research in recent years. most of the proposed schemes  however  deal with monitoring simple aggregated values  such as the frequency of appearance of items in the streams. more involved challenges  such as the important task of feature selection  e.g.  by monitoring the information gain of various features   still require very high communication overhead using naive  centralized algorithms.
　we present a novel geometric approach by which an arbitrary global monitoring task can be split into a set of constraints applied locally on each of the streams. the constraints are used to locally filter out data increments that do not affect the monitoring outcome  thus avoiding unnecessary communication. as a result  our approach enables monitoring of arbitrary threshold functions over distributed data streams in an efficient manner.
　we present experimental results on real-world data which demonstrate that our algorithms are highly scalable  and considerably reduce communication load in comparison to centralized algorithms.
1.	introduction
　a common requirement in many emerging applications is the ability to process  in real time  a continuous high volume stream of data. examples of such applications are sensor networks   real-time analysis of financial data  1  1   and intrusion detection. these applications are commonly referred to as data stream systems . the real-time nature of data stream systems and the vast amounts of data they are required to process introduce new fundamental problems that are not addressed by traditional database management systems  dbms . traditional dbms are based on a pull paradigm  where users issue queries regarding data stored by the system  and the system processes these queries as

 partially supported by the israeli ministry of science.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
dkeren cs.haifa.ac.il
they are issued and returns results. data stream systems  1  1  1  1  1  are based on a push paradigm  where the users issue continuous queries  1  1  that specify the required processing of the data  which the system processes as it arrives  continuously providing the user with updated results.
　various types of continuous queries have been studied in the past  including continuous versions of selection and join queries   various types of aggregation queries  1  1   and monitoring queries . while most previous work regarding data stream systems considers sequential setups  the data is processed by a single processor   many data stream applications are inherently distributed: examples include sensor networks   network monitoring   and distributed intrusion detection.
　a useful class of queries in the context of distributed data streams are monitoring queries. previous work in the context of monitoring distributed data streams considered monitoring simple aggregates  such as detecting when the sum of a distributed set of variables exceeds a predetermined threshold   or finding frequently occurring items in a set of distributed streams . some work has been done on monitoring more complex constructs derived from distributed streams  but the proposed solutions are customized for the problem at hand. examples include  which presents an algorithm for detecting similar sets of streams among a large set of distributed streams  and   which presents an algorithm for approximating quantiles over distributed streams.
　a useful  more general type of monitoring query can be defined as follows: let x1 x1 ... xd be frequency counts for d items over a set of streams. let f x1 x1 ... xd  be an arbitrary function over the frequency counts. we are interested in detecting when the value of f x1 x1 ... xd  rises above  or falls below  a predetermined threshold value. we refer to this query as a threshold function query.
　there is a fundamental difference between the cases of linear and non-linear f  which can be demonstrated even for the case of one-dimensional data. let x1 and x1 be values stored in two distinct nodes  and let f x  = 1x   x1. suppose one needs to determine whether  1. if f was linear  the solution would have been simple  since in that case. suppose that initially the value at each node is   1; then a simple distributed algorithm for monitoring whether 1 is for each node i to remain silent as long as f xi    1. however  even for the simple non-linear function above  it is impossible to determine from the values of f at the nodes whether its value at the average is above 1 or not. for example  if x1 = 1 x1 = 1  then f's value in each node is below 1  but its value in the average of x1 and x1 is 1. but if x1 = 1 x1 = 1  the value at both xi and their average is below 1. so  nothing can be deduced about the location of  vis-a-vis the threshold given the locations of f xi  vis-a-vis it.
　in this trivial example  the cost of sending the data stored in the nodes is the same as sending the value  but in data mining applications the data can be of very high dimensionality. this necessitates a distributed algorithm for locally determining whether f's value at the average data vector is above the threshold.
　following is a more practical example of a threshold function query: consider a classifier built over data extracted from a set of streams  for example a distributed spam mail filtering system. such a system is comprised of agents installed on several dispersed mail servers. users mark spam
mail they have received as such  providing each server with a continuous stream of positive and negative samples. these samples serve as a basis for building a classifier. since the
vocabulary comprising these samples may be very large  an important task in such a setup is determining which words  or features  should be used for performing the classification. this task is known as feature selection. feature selection is typically performed by calculating  for every feature  a non-linear scoring function  such as information gain or χ1  over statistics collected from all the streams. all the features scoring above a certain threshold are chosen as parameters for the classification task. since the characteristics of spam mail may vary over time  one may wish to monitor the features in order to determine if selected features remain prominent  or if any of the features not selected have become prominent. determining whether a certain feature should be selected at a given time can be viewed as a threshold function query.
　threshold function queries can be implemented by collecting all the mail items to a central location  but such a solution is very costly in terms of communications load. we are interested in algorithms that implement threshold function queries in a more efficient manner. we achieve this by defining numerical constraints on the data collected at each node. as data arrives on the streams  every node verifies that the constraint on its stream has not been violated. we will show that as long as none of these constraints have been violated  the query result is guaranteed to remain unchanged  and thus no communication is required.
　in this paper we present two algorithms for efficiently performing threshold function queries. the algorithms are based on a geometric analysis of the problem. upon initialization  the algorithms collect frequency counts from all the streams  and calculate the initial result of the query. in addition  a numerical constraint on the data received on each individual stream is defined. as data arrives on the streams  each node verifies that the constraint on its stream has not been violated. the geometric analysis of the problem guarantees that as long as the constraints on all the streams are upheld  the result of the query remains unchanged  and thus no communication is required. if a constraint on one of the streams is violated  new data is gathered from the streams  the query is reevaluated  and new constraints are set on the streams.
the first algorithm is a decentralized algorithm  designed for a closely coupled environment  where nodes can efficiently broadcast messages. the second algorithm is designed for loosely coupled environments  where the cost of broadcasting a message is high. these algorithms are  to the best of our knowledge  the first to enable efficient monitoring of arbitrary threshold functions over distributed data streams.
1	detailed example
　following is a detailed description of the spam filtering example given above. we will use this example to demonstrate the concepts we present. let p1 p1 ... pn be n agents installed on n different mail servers. let mi = {mi 1 mi 1 ...  mi k} be the last k mail items received at the mail server installed on pi  and let m denote the union of the last k mail
n
items received at each one of the n mail servers  m =	mi.
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　i=1 let x denote a set of mail items  let spam x  be the set of

mail items in x labeled as spam  and let spam x  be the set of mail items in x not labeled as spam. let cont x f  be the set of mail items in x that contain the feature f 

and let cont x f  be the set of mail items in x that do not contain the feature f. let the contingency table cf x for the feature f over the set of mail items x be a 1 〜 1 matrix  cf x = {ci j}  such that 
  and
 is called the local contin-
gency table for the node pi  and cf m is called the global
n
contingency table. note that . we are interested in determining  for each feature f  whether the information gain over its global contingency table  denoted by g cf m   is above or below a predetermined threshold r. the formula for information gain is given below1

　note that the answer to the threshold function query cannot be derived from the value of the monitored function on data from each individual stream. consider  for example  a spam filtering system consisting of two streams  with a threshold value of 1. the first node may hold a contin-
gency table   resulting in g cf m1  = 1  and the second node may hold a contingency table cf m1 =
	1
  resulting in g cf m1  = 1. as we can see  the
	1
gain calculated on each individual stream is 1  and thus below the threshold value  but the gain on the global contin-
gency table for	 
is g cf m1“m1  = 1  and thus above the threshold value. note that this behaviour does not occur when monitoring frequencies of occurrence of items over distributed streams  i.e.  if the frequency of occurrence of a certain item in all the streams is below a predetermined threshold  then the frequency of occurrence of that item over the union of the streams is below the threshold as well.
1.	related work
　a well studied problem is the monitoring of frequency counts over a single data stream  1  1  1  1   however these works do not address distributed environments.
　algorithms proposed in  enable detecting when the sum of a distributed set of variables exceeds a predetermined threshold. however  the algorithms proposed in  concentrate on monitoring the sum of a set of variables  whereas our algorithm enable monitoring arbitrary threshold functions over such variables. more recently   presented algorithms that adapt local thresholds when monitoring the sum of a set of variables  so that the communication cost is minimized. an interesting avenue for future work is making use of the techniques presented in  when monitor arbitrary threshold functions.
　the algorithm proposed in  enables a central coordinator to answer continuous queries designed to track the sum  average  or minimum of a distributed set of variables within a certain predetermined error margin. that work focuses on minimizing the communications required for performing several concurrent monitoring tasks  whereas our work proposes algorithms for monitoring arbitrary threshold functions.
　babcock and olston propose an algorithm for finding the k largest aggregated values  for example the k largest frequency counters  over a set of distributed streams . their algorithm employs a coordinator  which determines the initial set of the k largest aggregates  and sends each node a set of numerical constraints. each node checks that the data received on its stream does not violate this constraint. as long as all the constraints are upheld  the list of top k values is guaranteed to remain unchanged. in case a constraint is violated at one of the nodes  it notifies the coordinator  which performs a resolution process. the goal of the resolution process is to check if the list of the k largest values has changed  and to update the constraints at the nodes. the coordinator-based algorithm proposed in our work is similar to that algorithm in its use of numerical constraints  but its purpose is to monitor an arbitrary threshold function.
　algorithms proposed in  1  1  enable estimating certain functions over a set of distributed streams  for example the number of distinct elements in the streams  but this work does not address the monitoring of threshold functions.
1.	computational model
　let s={s1 s1 ...sn}  be a set of n data streams  collected at nodes p={p1 p1 ... pn}. let   be ddimensional real vectors derived from the streams  the value of these vectors varies over time . these vectors are called local statistics vectors. let w1 w1 ... wn be positive weights assigned to the streams.
　the weight wi assigned to the node pi usually corresponds to the number of data items its local statistics vector is derived from. assume  for example  that we would like to determine whether the frequency of occurrence of a certain data item in a set of streams is above a certain threshold value. in this case  the weight we assign to each node at time t is the number of data items received on the stream at time t   is a scalar holding the frequency of occurrence of the item in the stream si . in this setup weights change over time. a variant of the problem stated above is for each node to maintain the frequency of occurrence of the item in the recent ni data items received on the stream  this is known as working with a sliding window of size ni . in that case  the weight assigned to each node is the size of its sliding window. in this setup weights do not change over time. for the sake of clarity  we assume at first that the weights are fixed  and that they are known to all nodes. later  we modify our algorithms to handle weights that vary with time.
let  is called the global statistics
vector. let f : rd ★ r be an arbitrary function from the space of d-dimensional vectors to the reals. f is called the monitored function. we are interested in determining at any given time  t  whether or not   where r is a predetermined threshold value.
　we present algorithms for two settings  a decentralized setting and a coordinator-based setting. algorithms in both settings construct a vector called the estimate vector  denoted by . the estimate vector is constructed from the local statistics vectors collected from the nodes at certain times  as dictated by the algorithms. the last statistics vector collected from the node pi is denoted by. each node remembers the last statistics vector collected from it. the estimate vector is the weighted average of the latest statis-
tics vectors collected from the nodes  i.e. .
from time to time  as dictated by the algorithm  an updated statistics vector is collected from one or more nodes  and the estimate vector is updated. at any given time the estimate vector is known to all nodes.
　in the decentralized setting  when the algorithm dictates that a statistics vector should be collected from a node  the node broadcasts the statistics vector to the rest of the nodes. each node keeps track of the last statistics vector broadcast by every node  and locally calculates the estimate vector. in the coordinator-based setting  we designate a coordinator node and denote it by p1. the coordinator is responsible for collecting local statistics vectors from the nodes  calculating the estimate vector  and distributing it to the nodes.
　in both settings  each node pi maintains a parameter called the statistics delta vector. this vector is denoted by Δ . the statistics delta vector held by the node pi is the difference between the current local statistics vector and the last statistics vector collected from the node  i.e. 
.
　in both settings  each node pi also maintains a parameter called the drift vector. this vector is denoted by . the drift vector is calculated differently in each setting. in the decentralized setting the drift vector is a displacement of   in relation to the estimate vector 
	 	 1 
　the coordinator-based algorithm employs a mechanism for balancing the local statistics vectors of a subset of the nodes. consider the case where at a certain time t the statistics delta vector in two equally weighted nodes  pi and pj  cancel each other out: that is Δ . we will see that balancing the local statistics vectors held by pi and pj can improve the efficiency of the algorithm. the coordinator facilitates this balancing by sending each node a slack vector  denoted by. the sum of the slack vectors sent to the nodes is 1. the drift vector held by each node is calculated as follows:
		 1 
　in the decentralized algorithm  nodes communicate by broadcasting messages. the cost of performing a broadcast varies according to the networking infrastructure at hand. in the worst case broadcasting a message to n nodes requires sending n point to point messages. while the decentralized algorithm remains highly efficient even in those settings  in practice  the cost of broadcasting a message is significantly lower. some networking infrastructures  such as wireless networks and ethernet based networks  support broadcasting at the cost of sending a single message. in other cases efficient broadcasting schemes have been developed that significantly reduce the cost of broadcasting.
　we assume that communication links are reliable  i.e.  no messages are lost  otherwise standard methods for implementing reliability can be employed .
1.	geometric interpretation
　at the heart of our approach is the ability to decompose the monitoring task into local constraints on streams. as data arrives on the streams  each node verifies that the local constraint on its stream has not been violated. we will show that as long as none of these constraints have been violated  the query result is guaranteed to remain unchanged  and thus no communication is required. as demonstrated in section 1  this cannot be done solely by observing the value of the monitored function on each stream. therefore  an estimated global statistics vector  called the estimate vector  is known to all nodes. the estimate vector is said to be correct at a given time if the value of the monitored function on the estimate vector and the value of the monitored function on the global statistics vector at that time  this value is unknown to any singe node  are on the same side of the threshold. given an initially correct estimate vector  our goal is to set local constraints on each stream such that as long as no constraints have been violated  the estimate vector remains correct  and thus no communication is required. the method for decomposing the monitoring task is based on the following  easily verifiable observation-at any given time the weighted average of the drift vectors held by the nodes is equal to the global statistics vector 
	 	 1 
　we refer to property  1  as the convexity property of the drift vectors. the geometric interpretation of property  1  is that the global statistics vector is in the convex hull of the drift vectors held by the nodes 
	 conv 	  	 1 
　this observation enables us to take advantage of theorem 1 in order to decompose the monitoring task.
   theorem 1. let  be a set of vectors in rd. let conv be the convex hull of . let  be a ball centered at  and

figure 1: illustration of theorem 1. the drift vectors held by 1 nodes and the balls constructed by them are depicted. the convex hull of the drift vectors is highlighted in gray. as stated by the theorem  the union of the balls bounds the convex hull.
with a radius of	 i.e. 
   then conv	.
theorem 1 is used to bound the convex hull of n+1 vectors in rd by the union of n d-dimensional balls. in our case it is used to bound the convex hull of the estimate vector and the drift vectors i.e.  conv      by a set of n balls  where each ball is constructed independently by one of the nodes. each node   constructs a ball
    which is centered at  and has a radius of  . note that at any given time each node has all the information required to independently construct its ball. theorem 1 states that conv 

　the application of theorem 1 is illustrated in figure 1  which depicts a setup comprised of 1 nodes  each holding a statistics vector . the drift vectors held by the nodes       the global statistics vector    and the estimate vector  are depicted  as are the balls constructed by the nodes. the convex hull of the drift vectors is highlighted in gray  and one can see that  as the theorem states  the area defined by the convex hull is bounded by the set of balls.
1	local constraints
　the local constraint on each stream is set as follows: the monitored function f and threshold r can be seen as inducing a coloring over rd. the vectorsare said to be green  while the vectorsare said to be red. the local constraint each node maintains is to check whether the ball     the ball centered at
  and having a radius of   is monochromatic  i.e.  whether all the vectors contained in the ball have the same color. testing for monochromicity is done by finding the maximal and minimal values of f in the ball. this is done locally at each node hence has no effect on the communication load.
if all the local constraints are upheld  the estimate vector
is correct: because all the balls contain the estimate vector  and all the balls are monochromatic  the set of vectors defined by the union of all the balls is monochromatic as well. since the union of all the balls contains the convex hull of the drift vectors and the estimate vector  conv  
     and according to equation  1  the global statistics vector is contained in the convex hull of the drift vectors  the estimate vector and the global statistics vector have the same color. therefore  they are on the same side of the threshold  i.e.  the estimate vector is correct.
1	proof of theorem 1
　following is the proof of theorem 1. the proof uses the following variant of carath＞eodory's theorem :
　theorem 1. let  be a set of vectors in rd. let conv be the convex hull of
.	any vector  can be expressed as a convex combination of x  and at most d members of .
proof of theorem 1: the theorem is proven by induction over d. the base of the induction is d = 1. proving the base of the induction is trivial   are real numbers and both conv   and   are the segment  x yi  .
　the induction step is proven as follows: according to theorem 1  since any vector  conv   can be expressed as a convex combination of x and a set of at most d members of  it is sufficient to show that for an arbitrary set of vectors  
	conv  	 1 
i=1
furthermore  we observe that any vector z （ conv  
  is a convex combination of some vector 
  and x. therefore  it is sufficient to show that:
d
	conv  	 1 
i=1
equation  1  entails equation  1  since z is a convex combination of  conv    and x  and since  accord-
                                                         d ing to equation  1   conv belongs to at least one of the balls   say . since by definition x also belongs to also belongs to . this is because z is a convex combination of and x  which both belong to    which is a convex set.
　since for any rotation matrix r and vectors {q1...qn}  conv{r q1 ...r qn } = r conv{q1...qn}   we assume without loss of generality that the vectors lie on a d 
1 dimensional hyperplane  p  which consists of vectors whose last coordinate is constant. it is easy to show that the intersection of p yields the set of d   1 dimensional balls    where xp is the projection of x on the plane p. note that
for every i  the d 1 dimensional ball in the d dimensional ball p     i  . since the vectors lie on the d   1 dimensional plane p  then  according to the induction hypothe-
d
sis  conv 	 .	since for every i    is contained in	   then conv 
 . we have proved equation  1   and thus concluded the proof. 
1.	distributed monitoring
　in this section we present algorithms that are based on the geometric method for decomposing the monitoring task into local constraints on the streams. after presenting the algorithms  we describe how they can be tuned to relax performance requirements in favour of reducing communication load  and how they can be modified to support time-varying weights.
1	the decentralized algorithm
　following is a simple  broadcast-based algorithm for monitoring threshold functions: each node maintains a copy of the last statistics vector sent by each of the nodes. the initialization stage consists of every node broadcasting its initial statistics vector. upon receipt of all the initialization messages  each node calculates the estimate vector  then  as more data arrives on the stream  each node can check its local constraint according to the estimate vector and its drift vector. if a local constraint is violated at a node  pi  the node broadcasts a message of the form  containing its identifier and its local statistics vector at the time. the broadcasting node updates itsparameter and recalculates the estimate vector. upon receiving a broadcast message from a node  pi  each node updates itsparameter and recalculates the estimate vector.
　if all the local constraints are upheld  theorem 1 guarantees the correctness of the estimate vector  enabling every node to locally calculate the value of the threshold function . after a node broadcasts a message  its local constraint is upheld  because the ball it constructs has a radius of 1  and therefore is monochromatic . if a local constraint has been violated  at worst all n nodes  but possibly fewer  will broadcast a message before all the local constraints are upheld again.
a formal description can be found in algorithm 1.
algorithm 1 the decentralized algorithm initialization: at a node pi
  broadcast a message containing the initial statistics vector and updateto hold the initial statistics vector. upon receipt of messages from all the nodes  calculate the estimate vector  
processing stage at a node pi:
  upon arrival of new data on the local stream  recalculate    and   and check if    remains monochromatic. if not  broadcast the message  and updateto hold
  upon receipt of a new message   update  to hold    recalculate    and check if
   is monochromatic. if	   is not monochromatic  broadcast the message and update to hold

1	the coordinator-based algorithm
　local constraints are also used in the coordinator-based algorithm  but the coordinator is responsible for calculating the estimate vector  maintaining its correctness  and distributing it to the other nodes. in the decentralized algorithm the violation of a constraint on one node requires communicating with all the rest of the nodes  a broadcast message is sent . while this may be a good solution in setups where the nodes are closely coupled  in other cases we can further reduce the communication load by introducing a coordinator. the presence of a coordinator enables us to resolve a violation at a node by communicating with only a subset of the nodes  as opposed to communicating with all the nodes as required in the decentralized algorithm. consider  for example  a set of equally weighted nodes monitoring the function f x  =  x   1  a function over a single dimensional statistics vector   and a threshold value of r = 1. say that at time t the estimate vector is   = 1. note that since   any drift vector in the range  1  satisfies the local constraint at the node. let us assume that the drift vector at the coordinator    = 1  and the constraints at all n nodes are satisfied except for p1  which holds the drift vector   = 1. in the decentralized algorithm  since the constraint at p1 has been violated  it would have broadcast its statistics vector to all n nodes. however  the constraint violation at p1 can be resolved by setting the drift vector at both p1 and p1 to the average of the drift vectors on both nodes  i.e.  by setting
1. after this averaging op-
eration  drift vectors on both p1 and p1 are within the range  1   and thus all the local constraints are upheld. note that this action preserves the convexity property of the drift vector  property 1 . the act of averaging out a subset of drift vectors in order to resolve a violated constraint is called a balancing process.
　in order to facilitate the balancing of vectors  every node pi holds a slack vector denoted by  as defined in section 1. the slack vector is first normalized by dividing it by the weight assigned to the node. then it is added to the drift vector as specified in equation  1 . the coordinator is responsible for ensuring that the sum of all slack vectors is 1  thus maintaining the convexity property of the drift vectors  property 1 .
　to initialize the algorithm  each node sends its initial statistics vector to the coordinator. initially the slack vector held by each node is set to 1. the coordinator calculates the estimate vector and sends it to the rest of the nodes. as more data arrives on a node's stream  the node checks its local constraint. if a local constraint is violated at one of the nodes  it notifies the coordinator by sending it a message containing its current drift vector and its current statistics vector. the coordinator first tries to resolve the constraint violation by executing a balancing process.
　during the balancing process the coordinator tries to establish a group of nodes  called the balancing group and denoted by p   such that the average of the drift vectors held by the nodes in the balancing group  called the balanced vector and denoted by b   creates a monochromatic ball with the estimate vector i.e.  such that  is monochromatic. the balanced vector is calculated as follows:
		 1 
　the balancing process proceeds as follows: when a node pi notifies the coordinator that its local constraint has been violated  it appends its drift vector and its current statistics vector to the message. the coordinator constructs a balancing group consisting of pi and itself. it then checks if the ball defined by the balanced vector    is monochromatic. if   is not monochromatic  the coordinator randomly selects a node that is not in the balancing group  and requests it to send its drift vector and local statistics vector. then it adds that new node to the balancing group and rechecks . the process is performed iteratively until either  is monochromatic  or the balancing group contains all the nodes. if the coordinator established a balancing group such that  is monochromatic  the balancing process is said to have succeeded. in this case the coordinator sends each node in the balancing group an adjustment to its slack vector. this causes the drift vectors held by all nodes in the balancing group to be equal to the adjustment to the slack vector sent to each node is denoted by Δ  and is calculated as follows:

　after receiving the slack vector adjustment  each node simply adds the adjustment to the current value  i.e. 
. one can easily verify that after a successful balancing process the sum of all slack vectors remains1  and the drift vector held by each node in the balancing group is b  thus resolving the original constraint violation.
　if the balancing process has failed  i.e.  the balancing group contains all the nodes  and   is not monochromatic   the coordinator calculates a new estimate vector  according to the updated statistics vectors sent by the nodes  and sends it to all the nodes. upon receipt of the new estimate vector  the nodes set their slack vectors to 1 and modify theirparameter to hold the value of the statistics vector they sent to the coordinator during the balancing process  thus resolving the original constraint violation.
　in order to implement the algorithm  the following messages must be defined:
 init  used by nodes to report their initial statistics vector to the coordinator in the initialization stage.
 req  used by the coordinator during the balancing process to request that a node send its statistics vector and drift vector.
 rep  used by nodes to report information to the coordinator when a local constraint has been violated  or when the coordinator requests information from the node.
 adj-slk Δ used by the coordinator to report slack vector adjustments to nodes after a successful balancing process.
 new-est e  used by the coordinator to report to the nodes a new estimate vector.
a formal description is given in algorithm 1.
1	relaxing the precision requirements
　a desired trade-off when monitoring threshold functions is between accuracy and communication load. in some cases an approximate value of the threshold function is sufficient  that is  the correct value of the threshold function is required only if the value of the monitored function is significantly far from the threshold. in other words  if ε is a predetermined error margin  and if ε  we require that the estimate vector    be correct  but we do not require it if.
　consider the feature monitoring example given in section 1. say we would like to select all the features whose information gain is above 1. obviously  it is important to select a feature whose information gain is significantly high  and not to select a feature whose information gain is significantly low. for example  it is important to select a feature whose information gain score is 1  and not to select a feature whose information gain score is 1. including or excluding features whose information gain score is very close to the threshold value  for example a feature whose information gain score is 1  will probably not have a significant effect on the quality of the selected feature set  while the cost of monitoring such features is expected to be high  since their information gain is expected to fluctuate around the threshold value. therefore we can significantly improve the efficiency of our monitoring algorithms if we set some error margin  say 1. in other words  features that are currently selected will be removed from the set of selected features only when their information gain falls below 1  and features that are currently not selected will be added to the set of selected feature only if their information gain rises above 1.
　our algorithm can be easily tuned to relax the precision requirements by an error margin of ε as follows: instead of working with a single coloring  induced by the monitored function f and the threshold value r  two sets of coloring are defined  one induced by the monitored function f and the threshold value r + ε  and a second induced by the monitored function f and the threshold value r   ε. whenever the original algorithm checks whether a ball is monochromatic  then  if  the modified algorithm will check whether the ball is monochromatic according to the first coloring  the one induced by f and  the modified algorithm will check whether the ball is monochromatic according to the second coloring  the one induced by f and r   ε . this ensures that if all the balls are in the range defined by  no messages are transmitted.
1	handling time-varying weights
　up to this point we have assumed that the weights assigned to nodes are fixed  such as when the weights are the size of sliding windows used for collecting data from the streams. we now address cases where weights assigned to nodes may vary with time  as when a node's weight at a given time is the number of data items received on its stream so far.
　we next describe the required modifications to the algorithms in order to ensure their correctness in a setup where algorithm 1 the coordinator-based algorithm
initialization:
  send an init message to the coordinator  set v to hold the initial statistics vector  and set the slack vector to 1. upon receipt of messages from all nodes  the coordinator calculates the estimate vector and informs the nodes via a new-est message. processing stage at an ordinary node pi:
  upon arrival of new data on a node's local stream  recalculate   and    and check if
   remains monochromatic. if not  send a  rep  message to the coordinator  and wait for either a new-est or an adj-slk message.
  upon receipt of a req message  send a  rep  message to the coordinator and wait for either a new-est or adj-slk message.   upon receipt of a new-est message  update the estimate vector      to the value specified in the message  set the value of v to the statistics vector sent to the coordinator  and set the slack vector to 1.   upon receipt of an adj-slk message  add the value specified in the message to the value of the slack vector

processing stage at the coordinator:   upon arrival of new data on the local stream  recalculate  and    and check if
remains monochromatic. if not  initiate a balancing process  setting the balancing group to 
.
  upon receipt of a rep message from the node pi  initiate a balancing process  setting the balancing group to.
balancing process at the coordinator:
1. calculate balanced vector 	  according to equation
 1 . if the ball  is monochromatic goto  1   otherwise goto  1 .
1. for each item in the balancing group    calculate the slack vector adjustment  Δδi = wib  
   send pi a  adj-slk Δmessage  and then exit the balancing process.
1. if there are nodes not contained in the balancinggroup  select one of these nodes at random  and send it a req message. upon receipt of the rep message  add the node to the balancing group and goto  1 . otherwise calculate a new estimate vector  based on the vi values received from all the nodes   send a new-est message to all nodes  and exit the balancing process.

weights vary with time. in such a setup we denote the weight assigned to the node pi at time t by wi t . each message in the original algorithms is modified by appending wi t  to it. along with the last vector broadcast by each of the other nodes     the nodes in the decentralized algorithm keep track of the last broadcast weight  denoted by . nodes calculate    and  as follows:


　in the decentralized algorithm the drift vector is calculated by

and in the coordinator-based algorithm  by

　in the coordinator-based algorithm  the balanced vector and the slack vector adjustments are calculated according to the weights appended to the messages:


note that if the weights are fixed    and
hold the same values they hold in the original algorithms. furthermore  one can easily verify that the new definitions of these parameters maintain equation  1  i.e. 
  and thus maintain the correctness of
the algorithm.
1.	performance analysis
　we would like to determine how the various parameters of the monitoring problem affect the communication load generated by the proposed algorithms. in order to do so we present a simplified model of our algorithm and analyze the probability that a constraint violation will occur at a node. since a constraint violation is the trigger for communications in both algorithms  this analysis should provide indications regarding the generated communication load.
　generally speaking  the dominant factor affecting the performance of the algorithms is the average distance of the estimate vector from the set of vectors for which the value of the monitoring function equals the threshold value. more formally  let the threshold set defined by the monitoring function f and threshold value r be the set of vectors for

	 a 	 b 
figure 1: the colorings induced by two sets of monitored functions and threshold values.  a  depicts the
coloring induced by f1 = sin 1 x1+y1  − 1  and  b  depicts the coloring induced by
1.
which the value of the threshold function equals the threshold value. let the threshold set be denoted by t f r   i.e. 

　let the distance of a vector x from the threshold set t f r   denoted as   be the minimum distance of x from any point in t f r  i.e. 

　the farther the estimate vector is  at a given time  from the threshold set  the more the local statistics vectors can change without violating local constraints. therefore  a greater average distance of the estimate vector from the threshold set will result in a greater reduction in communications.
　the average distance of the estimate vector from the threshold set is affected by many parameters. to begin with  it is affected by the coloring induced by the monitored function and the threshold value. figure 1 illustrates the coloring induced by two sets of a monitored function and a threshold value. figure 1 a  illustrates the coloring induced by the function f1 x y  = sin 1  x1 + y1   and the threshold value 1  and figure 1 b  illustrates the coloring induced by the function f1 x y  = 1+e1 x + 1+e1 y and the threshold value 1  f1 x y  is a simple two-layer neural net .
　it is clear that the distance of any point in r1 from the threshold set defined by f1 and 1 cannot be greater than
1. therefore  the average distance of the estimate vector from the threshold set in this case is bounded from above by 1  thus yielding a relatively low reduction in communications when monitored by our algorithms. however  the maximum distance of a point in r1 from the threshold set defined by f1 and 1 is unbounded  and the dominating factor affecting the performance of our algorithms in this case is the nature of the data received on the streams.
　in order to analyze how our algorithms are affected by the nature of the data on the streams  we consider periods during which this data is stationary  this fact  however  is not known to any of the nodes . more formally  we assume that each stream item is a d-dimensional vector  where the jth component is independently drawn from a random variable denoted by xj with a defined expectancy and variance 

figure 1: depicts the coloring induced by the function and the threshold value 1  together with the expected global statistics vector  its distance from the threshold set  and a local sta-
tistics vector  that is contained within the distance sphere. one can see that the ball is fully contained in the distance sphere.
denoted by e xj  and v  xj  respectively. we assume the system consists of n nodes  and that each node holds a sliding window of n items. we denote the last n items received on the stream monitored by  and the components of a vector as follows:
the local statistics vector held by a node is the average of the items contained in its sliding window  and the global statistics vector is the average of the items contained in the sliding windows held by all the nodes  i.e. 

　it is easy to see that the expected value for the global statistics vector and each local statistics vector is

　figure 1 depicts the coloring induced by f1 and the threshold value 1  the expected global statistics vector  and the distance of the expected global statistics vector from the threshold set. the expected global statistics vector and its distance from the threshold set define a sphere called the distance sphere. we denote the distance of the expected global statistics vector from the threshold set by dglobal i.e. 

　we present the following simplified model of our algorithms: we assume that the estimate vector holds the value of the expected global statistics vector  i.e.   e x1  e x1  ... e xd  . furthermore  we assume that data on each stream arrives in blocks of n items. we would like to bound prviolation  the probability that the arrival of a new block of data items on a stream will cause a constraint violation at the node monitoring the stream.
　we assume that the estimate vector is the expected global statistics vector. consequently  as long as the local statistics vector held by a node is contained within the distance sphere  the constraint checked by the node is guaranteed not to be violated. that is  if  then
   is fully contained in the distance sphere  see figure 1  .
　therefore  the probability that a constraint violation will occur at a node is less than the probability that the local statistics vector held by the node will not be contained in the distance sphere. using the markov inequality we obtain:

pr
　the proof is omitted due to lack of space. if the components of the data vectors are bounded between 1 and 1 - as happens in the important case in which they represent probabilities of terms to appear in a document - the hoeffding bound can be used  to show that:
	d	1
v  xi 
	prviolation ＋ exp	 1	dglobal1	  i=1	/d
n
　both bounds decrease quickly when dglobal increases. this suggests that for data mining applications  features with a small information gain will not cause many constraint violations at any node  since their dglobal is large. this is practically important  since usually most of the candidate features have a rather small information gain  and thus the proposed algorithm will considerably reduce communication. this is supported by the experimental results presented in the next section.
1.	experimental results
　we performed several experiments with the decentralized algorithm. we tested the algorithm in a distributed feature selection setup. we used the reuters corpus  rcv1-v1   in order to generate a set of data streams. rcv1-v1 consists of 1 news stories  produced by reuters between august 1  1  and august 1  1. each news story  which we refer to as a document  has been categorized according to its content  and identified by a unique document id.
　rcv1-v1 has been processed by lewis  yank  rose  and li . features were extracted from the documents  and indexed. a total of 1 features were extracted. each document is represented as a vector of the features it contains. we refer to these vectors as feature vectors. we simulate n streams by arranging the feature vectors in ascending order  according to their document id   and selecting feature vectors for the streams in a round robin fashion.
　in the original corpus each document may be labeled as belonging to several categories. the most frequent category documents are labeled with is  ccat   the  corporate/industrial  category . in the experiments our goal is to select features that are most relevant to the  ccat  category  therefore each vector is labeled as positive if it is categorized as belonging to  ccat   and negative otherwise.
　unless specified otherwise  each experiment was performed with 1 nodes  where each node holds a sliding window con-
information gain vs. document index

figure 1: information gain for the features  bosnia    ipo   and  febru  as it evolves over the streams. the information gain for the feature  bosnia  displays a declining trend as the stream evolves. the information gain for the feature  ipo  remains relatively steady  while the information gain for the feature  febru  peaks about halfway through the stream.
taining the last 1 documents it received. in each experiment we used the decentralized algorithm in order to detect for each feature  at any given time  whether its information gain in above or below a given threshold value. at any given time the information gain of a feature is based on the documents contained at the time in the sliding windows of all the nodes.
　the experiments were designed to explore several properties of the algorithm. we were interested in determining how various parameters of the monitoring task affect the performance of the algorithm. the parameters of the monitoring task can be divided into characteristics of the monitoring task  and tunable parameters. the characteristics of the monitoring task include the number of streams to be monitored  and the desired threshold value. tunable parameters include the size of the sliding window used by each node  and the permitted error margin. in addition we were interested in examining the behaviour of the algorithm when used for simultaneously monitoring several features.
　in order to examine the effect of the various parameters on the performance of the algorithm  we chose three features that display different characteristic behaviour. the chosen features are  bosnia    ipo   and  febru . figure 1 depicts how the information gain for each feature evolves over the streams. the information gain for the feature  bosnia  displays a declining trend as the stream evolve. the information gain for the feature  ipo  remains relatively steady  while the information gain for the feature  febru  peaks about halfway through the stream.
　we start by examining the influence of the characteristics of the monitoring task on the performance of the algorithm. figure 1 show the number of broadcasts produced when monitoring each one of the features for threshold values ranging from 1 to 1. in addition the cost incurred by the naive algorithm is plotted  i.e.  the number of messages required for collecting all the data to a central location. one can notice that even for adverse threshold values  the algorithm incurs a significantly lower communication cost than the cost incurred by the naive algorithm.
messages vs. threshold

figure 1: number of broadcasts produced in order to monitor each feature as a function of the threshold value. in addition  the cost incurred by monitoring a feature by a naive algorithm is plotted. even for adverse threshold values our algorithm performs significantly better than the naive algorithm.
　in order to check the effect the number of nodes has on the performance of the algorithm  we performed the following experiment: the stream of documents was divided in advance into 1 sub-streams in a round robin fashion. simulations were run with the number of nodes ranging from 1 to 1. in a simulation consisting of n nodes  the first n sub-streams were used. this methodology ensures that the characteristics of the streams remain similar when simulating different numbers of nodes. each node held a sliding window of 1 items.
　obviously  increasing the number of nodes will increase the number of broadcasts required in order to perform the monitoring task. since the nodes in our experiment receive streams with similar characteristics  we expect that the number of broadcasts will increase linearly.
　two sets of simulation were run  the first with a threshold value of 1  and the second with a threshold value of 1. the results are plotted in figure 1. both graphs show that the number of broadcasts increases linearly as more nodes are added. comparing the two graphs reveals that the number of broadcasts increases more moderately when using a threshold value of 1. this is due to the fact that as indicated in figure 1  the average information gain on the monitored features is closer to 1.
	messages vs. number of nodes  threshold - 1 	messages vs. number of nodes  threshold - 1 

figure 1: number of messages produced in relation to the number of nodes. the number of messages increases linearly as the number of nodes increases  indicating that the algorithm scales well. since the average information gain of all the features is closer to 1 than to 1  the number of messages increases more moderately when using a threshold value of 1.
	messages vs. error margin	messages vs. window size

figure 1: the influence of tunable parameters on performance. increasing the error margin is more effective in reducing the communication load than increasing the window size. using an error margin as small as 1　next we performed two experiments in order to evaluate the effect of tunable parameters on the performance of the algorithm. we performed the following experiments on the three features: for each feature we chose the threshold value that incurred the highest communication cost  1 for  bosnia   1 for  ipo   and 1 for  febru  . we ran a set of simulations on each feature  using error margin values ranging from 1 to 1 percent of the threshold value. then we ran an additional set of simulations for each feature  setting the size of the sliding window used by each node to values ranging from 1 items to 1 items. the results of these experiments are plotted in figure 1. the results indicate that increasing the error margin is very effective in reducing the communication load. using an error margin as small as 1 percent significantly reduces the communication load. increasing the window size also reduces the communications load. the effect of increasing the window size is percent significantly reduces the communication load.
most evident for the feature  ipo   which incurs the highest communication cost among the three features. in general  increasing the window size has a greater effect the closer the information gain of feature is to the threshold value.
　finally  we checked the performance of the algorithm when simultaneously monitoring multiple features. as the number of features that are monitored simultaneously increases  the probability that a constraint on one of the features will be violated when a new data item is received increases as well. furthermore  a constraint violation can cause a cascading effect. a constraint violation for a feature at one of the nodes causes all the nodes to calculate a new estimate vector for the feature. since the value of the estimate vector for the feature has changed  the constraint for the feature may be violated at additional nodes  causing these nodes to broadcast. the purpose of this experiment is to determine the number of simultaneous features the algorithm can monitor while remaining efficient  i.e.  incurring a cost that is lower than the cost incurred by the naive algorithm. the experiment consisted of a series of simulations  using a threshold value of 1. in each simulation a number of features were selected randomly. simulations were run with the number of features ranging from 1 to 1. the results of this experiment are plotted in figure 1. in addition the cost incurred by the naive algorithm is plotted.
messages vs. simultaneous features

figure 1: number of messages in relation to the number of simultaneously monitored features. our algorithm remains efficient when simultaneously monitoring up to about 1 features.
　the results indicate the algorithm remains efficient when simultaneously monitoring several thousands of features  but is inefficient when simultaneously monitoring more than about 1 features.
1.	conclusion
　monitoring streams over distributed systems is an important challenge which has a wide range of applications. scalability and efficiency of proposed solutions strongly depend on the volume and frequency of communication operations. however  despite the amount of work that was invested in this direction  most of the efficient solutions found in the literature can only be applied to simple aggregations or to linear functions. most probably the reason is that when the function is non linear  effects seen in one - or only a few - of the streams may often turn out to be misleading with regards to the global picture.
　in this work we proposed a solution through a general framework for monitoring arbitrary threshold functions over a system of distributed streams. the evaluation of this approach using real-life data  applied to the information gain function  reveals that it is highly effective in reducing communication frequency.
　immediate future work will concentrate on developing methods to fine-tune various parameters  window size  error margin  threshold  in order to find an optimal trade-off between communication load and accuracy. we also plan to try and characterize families of functions for which the algorithm is more efficient.
