we combine the speed and scalability of information retrieval with the generally superior classification accuracy offered by machine learning  yielding a two-phase text classifier that can scale to very large document corpora.  we investigate the effect of different methods of formulating the query from the training set  as well as varying the query size.  in empirical tests on the reuters rcv1 corpus of 1 documents  we find runtime was easily reduced by a factor of 1x  with a somewhat surprising gain in f-measure compared with traditional text classification. 
categories and subject descriptors 
i.1  pattern recognition : design methodology- classifier design and evaluation  feature evaluation and selection;  h.1  information storage and retrieval : information search and retrieval-query formulation  selection process. 
general terms 
algorithms  performance  experimentation. 
keywords 
machine learning  text classification  document categorization  information retrieval  enterprise scalability  forensic search. 
1. introduction 
consider using a trained document classifier to search for 'relevant' files from one's personal file system-typically containing hundreds of thousands of files-or from within an enterprise containing billions of files spread across distributed servers worldwide.  for such scale  it would be nearly infeasible to pump every single file through the document classifier.  yet such scalability could become essential for future information lifecycle management  ilm  applications seeking to verify  for example  that corporate retention and file protection policies are being followed for certain classes of confidential documents.  likewise  this scalability could be demanded by future ediscovery or forensic searches to find all files related to a legal matter.  
note that in such applications  the objective is both precision and recall. this is in contrast to most information retrieval settings where only high precision in the top few search results is needed. 
 
information retrieval methods excel in scalability  as evidenced by their success in web search engines.  however  text classification via machine learning is generally called for if one needs to balance precision and recall  assuming a training set is available.  note that for large scale corpora  the time to train the classifier is dwarfed by the cumulative classification time. the computational workload of classification is linear in the number of documents to be classified: each document is fetched from disk  its text features extracted  and then the classifier makes its class prediction.  even were thousands of cpu cores cheaply available to classify documents in parallel  it would place tremendous bandwidth demands on the disks and the i/o paths. 
in this paper we improve the scalability of text classification by leveraging a full-text index over the corpus of documents.  the availability of such indices is becoming more common in personal and corporate file systems.   the basic concept is simple: we first use the index to quickly extract a small subset of documents that are potentially relevant  and then pass only these to the traditional text classifier. the workload of such a classifier is proportional to the size of the query hit list  yielding excellent speedup in the common case where only a small fraction of the documents are sought.  this enables the system to scale up to very large document corpora.  our overall purpose is to optimize the design choices appropriate for querying one or more file systems  each with its own static full-text index. 
our research objective is to minimize runtime while maximizing f-measure-the harmonic average of precision and recall.  the research questions include how to generate an effective query from the training set  how large a query is ideal  and how great is the savings in time vs. the tradeoff in accuracy   although we expected a tradeoff  it turns out that the two-phase process can be both much faster and more accurate than a single text classification pass over all the documents. 
for reproducibility  our experiments use publically available data and software. we use 1 classes of the large reuters rcv1 corpus  indexed by apache lucene software v1.1   and classified via the weka v1 linear support vector machine  svm  model . we take care that the operating system begins each timing experiment with a cold file cache  so that we accurately measure the performance of the whole system. a typical machine learning experiment conducts many runs in succession  but for this sort of information retrieval experiment  if 

permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm  1-1-1/1...$1. 
one does not clear the cache  the pertinent data becomes cached in ram  hiding the substantial cost of slow disk seeks. 
section 1 describes the problem scope  and section 1 describes a variety of design choices around our solution. sections 1 and 1 summarize a suite of experiments we performed. section 1 discusses related work  and section 1 concludes and offers perspective on future work. 
1. the problem setting 
for the applications we are interested in  the volume of classification workload dwarfs the initial training time. this is especially so in view of recent breakthroughs in training state-ofthe-art linear support vector machine  svm  models for text classification in near linear time  e.g. .  the rise of multi-core parallelism can aid with training time  but has little impact on the classification process  which is fundamentally i/o bound in fetching files for classification.   
we expect that the full-text indices have been previously constructed for other purposes  with no special attention to the classification labels or tasks for which we leverage them. 
our research scope is limited to binary text classification tasks where the positive class of interest is usually rare  e.g.  1% of the population. such high class imbalance is often a difficult operating region for machine learning.  in particular  if the positive training examples are too rare  the learning process may select a decision threshold that classifies all items as negative  optimizing its training accuracy under uncertainty. common techniques used in such situations are to over-sample the positives in training or to under-sample the negatives .   
at some point after indexing  a training set is provided with labeled positive and negative training examples from which to learn a classifier to select all relevant documents from the entire collection.  in practical settings  it is often not feasible to get a training set that represents a true random sample of the population of all files.  for one thing  since positives are rare  asking a domain expert to provide binary labels to a stream of random samples is not an effective way to obtain a sufficiently large set of positive examples.  if 1% of a random sample is positive  a domain expert would have to consider 1 randomly selected training cases to build up a set of 1 positive examples-many fewer would likely under-represent the diversity of positives.  in practice  positive training cases are sometimes gathered by various ad hoc keyword searches  or have already been gathered in a directory by someone with an unknown  organic method. considering this  the percentage of positives may be overrepresented in the training set  but this is tantamount to undersampling training negatives anyway . 
1. two-phase classification 
our approach consists of two phases:  the first phase executes a query against a full-text index to determine a list of filenames that are likely positive.  the second phase retrieves the file contents for each specified file  extracts its feature vector from the text content  and then classifies it.  we list a number of design choices for each phase. 
phase 1: first  what is the space of query terms that may be used   most text classification research focuses on the universal bag-ofwords representation  although it has been shown repeatedly that including phrases can help substantially. in phase 1  we may only query for terms that have been previously indexed. even so  most indexing packages provide the ability-at some additional overhead-to query for phrases. this gives us the flexibility to form our query from words only  or also to include phrases- adjacent pairs of words.  we experimented with both options. 

figure 1. block diagram of training and test phase where the design choices involved in each task are listed. 
next  given the large number of terms in the training set  how shall we select the best terms   and how many terms q should we include in the query   we vary q widely  1k   and evaluate term goodness via bi-normal separation  bns  our default  or via information gain  ig  only where stated -two feature selection methods that have been shown to perform well .  we ignore terms that occur fewer than three times in the training set. 
the computation for bns is simply |f-1 tpr  - f-1 fpr |  where f-1 represents the inverse cumulative normal function from statistical tables  tpr represents the  true positive rate  of the feature  i.e. what percentage of positives the feature occurs in  and fpr represents the  false positive rate  of the feature-what percentage of negatives the feature occurs in.  the computation for ig is better known and more involved: 
        ig = h pos neg  -  p term  h tp fp  +  1-p term   h fn tn   where  
pos = number of positive training cases  minority   neg = number of negative training cases  tp = number of positive training cases containing term  fp = number of negative training cases containing term   
fn = pos - tp  tn = neg - fp  
p term  = the prevalence of the term  tp+fp / pos+neg   entropy h x y  = -nln x/ x+y   - nln y/ x+y    and nln x  = x log1 x. 
next  we have a design choice for the form of the query.  we experiment with the two logical choices:  1  a straightforward boolean query  namely lucene's default disjunction1 of the chosen terms  or  1  a weighted query  where each term is associated with a real number.  the latter amounts to a linear classifier  especially if we allow the weights to be negative and only select documents that end up having a positive score.   this opens the issue of how to set the weights.  to limit our scope  we assume-not unreasonably-that a linear svm provides state-ofthe-art classification accuracy .  thus  to obtain the weights for q terms  whether words only  or words and phrases   we filter the training set so that it contains only the q best terms according to bns or ig  train a linear svm classifier weka v1 smo using default parameters  and then extract its learned weight for each term.  this will naturally include negative weights  e.g. for features whose presence is correlated with the negative class.  
our last option in phase 1 is whether to just focus on providing the best f-measure  or to try to bias toward higher recall and expect the phase 1 classifier to restore the precision to optimize our objective  f-measure. 
phase 1:  we have the same choices with respect to the feature set to provide for training the final classifier.  that is  we can optionally include phrases  select via bns or ig  and can control the number of classifier features c used to train the final classifier. except where explicitly stated otherwise  our typical phase 1 classifier used c=1 features selected via bns from among the set of words and two-word phrases; these choices were selected after some preliminary experimentation. because in phase 1 we have the complete file contents in memory  we can cheaply afford to use many features as far as it improves fmeasure. more generally  phase 1 is not restricted to indexed features  so it could easily include other feature generators for improved accuracy  such as n-grams or domain-specific features. we leave this option for future work. 
a final consideration is what training set to use for the phase 1 classifier. it will only encounter the files that were classified positive by phase 1  correctly or incorrectly.   it seems proper then to train the phase 1 classifier only on labeled training cases that the phase 1 classifier finds positive. but it turns out this is impractical. there is a limited supply of training data  and the phase 1 classifier mostly excludes the negative training examples.  hence  the phase 1 training set would have most of the original positives  but would only sometimes contain a handful of negatives.  it would be impractical in our setting to perform phase 1  and then ask the user to label a new large set of negatives from the query hits.  hence  we are left with the simple option to train phase 1 on the full training set. 
1. experiment methodology 
for a publically available corpus that includes ground truth classification labels  we use reuters rcv1   which has 1 news articles in xml formatted text files.  we removed from all these files the metadata tags that reveal their true class labels  and saved this information in an isolated file.  the average file size is 1 kb  which closely matches typical file systems historically .  we indexed all the reuters files using the default lucene text analyzer  which does not give any special consideration to the xml structure of the text.  the indexing took just over two hours  including index optimization. because indexing is slow  we did not consider rebuilding the index for each run to exclude the files used in the training set of that run.  hence  the training positives are among the query hits found in each run.  in a real deployment  it may well be the case that the training examples are already included in the index of the local pc; but this is less likely for a federated search of many distributed file systems throughout an enterprise.   despite training examples being included among the query hits  we explicitly remove them before phase 1 and whenever we compute f-measure performance  in accordance with accepted practice for measuring performance in machine learning research.  recall that f-measure is the harmonic average of precision and recall: 1*p*r/ p+r . it drops rapidly if either precision or recall is poor. 
except where stated otherwise  each data point we show represents results averaged over a large set of separate classification tasks  i.e. macro-averaged. we selected all reuters classes that have a prevalence  = 1% positive overall and have over 1 examples  1 for training and over 1 others to find .  this leads to 1 classes in all  ranging from 1 to 1 examples  1 on average  1% to 1% positive  averaging 1% positive .  the classes include reuters geography/country codes  industry codes  and topic codes. 
in each training set  we provide 1 positive examples and 1 negative examples  selected at random just once from the ground truth labels.  we want to be sure to provide enough training data to learn a decent classifier  so that we might avoid potentially useless 'garbage-in  garbage-out' results. that said  for many classes  decent discrimination could have been learned with fewer examples. exploring these tradeoffs is outside the scope of this paper.  the query-time benefits of our methods are largely independent of the size of the training set. 
note that each training set has 1% positives  whereas the actual prevalence is typically ~1%.   we briefly tried training with 1 negatives to match 1% positive  but the weka software crashed when it exhausted the 1 gb of available heap memory.   instead of having a huge set of negative examples  we set the weka instance weights such that the total weight of the positives amounts to 1%  alternately  some svm implementations let one adjust the relative misclassification costs of positives vs. negatives .  we do this for all phase 1 classifiers we train. assuming the phase 1 classifier achieves decent precision  the phase 1 classifier should expect a much higher rate of positives. thus  there is no need for weighting the training data for the phase 1 classifier. we confirmed this experimentally. 
as mentioned in the introduction  it is important that we clear the file cache between tests  otherwise realistic disk delays are completely hidden. the ability for a user with root privileges to drop the file cache has recently been added to the linux 1.1 kernel via  echo 1  /proc/sys/vm/drop caches . 
 even so  this novel capability is still buggy as of 1.1.el1 and causes cpu soft lockups occasionally  requiring power-cycle reboots.   specifically  we drop the cache before each query. we verified that without dropping the cache  we get wildly erroneous timings.  
hardware:  hp proliant dl1 g1 server  with dual 1ghz xeon cpus and 1gb ram. it has a locally attached disk: a 1 gb  1k rpm ultra1 scsi disk with an hp smartarray 1i controller.  we actually used 1 such servers independently to complete the many experiments involved; there was no communication or interference between them. 
1. empirical results 
our first set of results present the main take-home message of this paper: that two-phase classification greatly improves the speed as well as the final accuracy  compared to the baseline of simply testing every file with the  phase 1  classifier.  refer to figure 1  which shows the overall f-measure on the y-axis  and the total elapsed time on the x-axis. the elapsed time includes the time to run the query  fetch the file contents for each query hit  excluding training cases   and extract its text features. each file is effectively classified with no additional time at the completion of its text feature extraction. 

	total time  minutes 	 
figure 1.  f-measure vs. elapsed time for various methods. 
on the far right  we see the baseline method took ~1 minutes on average  and achieved 1 f-measure averaged over all 1 classes. the baseline method consists of the phase 1 classifier  1 words and phrases selected via bns  applied to every file except the 1 training files.  since no phase 1 is involved  the baseline classifier includes reweighting the training positives  which brings up its precision and f-measure substantially. 
to the left in the graph  we see that boolean queries of words alone  or words and phrases together  can greatly cut down on the number of documents to process in phase 1.  the different points climbing up each of these curves correspond to q=1  1  1  1  1  1  or 1 query terms.  with enough query terms  1% recall is achieved on the positive class  and the phase 1 classifier obtains the same baseline f-measure  but 1 times faster.  as q increases  we see a rapid increase in the elapsed time:  once we have achieved 1% recall  additional query terms only serve to increase the number of false positives that need to be discarded by the phase 1 classifier. lastly we note that because phrases are more specific  recalling fewer documents each  we see that more terms are required to achieve a given level of recall compared to the words only curve. 
the pair of curves furthest left indicates the greatly improved overall performance of using a weighted query for phase 1.  the different points represent q=1  1  1  1  or 1 terms  as selected by bns  and the weights are determined from a trained svm.  despite the extremely large number of query terms to process  in most cases we see much improved speed vs. the boolean query-the weights give the phase 1 classifier much better control to exclude negatives while selecting positives. this increased precision cuts down on the irrelevant files that must be retrieved for phase 1. furthermore  this more accurate  weighted phase 1 classifier excludes some negatives that otherwise get past the phase 1 classifier. because it eliminates some complementary negatives  the effect is that the two-phase classifier obtains higher precision overall  improving the final f-measure average for all 1 classes.  by using q=1 word & phrase terms in the weighted query  the process completes in just 1 minutes on average-1x faster than the baseline-with an f-measure 1 averaged over all 1 classes. 
comparing these two curves  we see that including phrases consistently improves performance at any given number of query terms. this effect is known  although most text classification research is done with a simple bag-of-words only.  
 
figure 1. time taken by different methods for varying query sizes. the time columns have been segmented into querying  fetching and analyzing time. overall f-measure for each setting is shown atop each bar. 
we made sure to extend q far enough to verify that having more terms is not always better. this is consistent with feature selection literature  which usually shows a benefit to limiting the number of terms  e.g. .  but note that the weighted classifier can benefit from many more terms than the boolean classifier  q=1 vs. ~1 .  having such a large number of query terms slows down the query  but yields a speedup overall because phase 1 is more discriminating in which files to fetch for phase 1.  as a result  large q values lead to great savings in overall retrieval time. the best performing weighted query setting is 1x faster than the best performing boolean query setting while achieving 1% better fmeasure  1 points . 
1 timing breakdown 
next  we break down the elapsed time of the x-axis of figure 1 into its constituent parts: the time taken  1  to query the index   1  to fetch the files that satisfy the query  and  1  to analyze the file contents for specific text features and thereby obtain its final classification. these times correspond to the three segments in each column of figure 1. boolean queries achieve nearly 1% recall with few query terms  and hence the query time is too small to see with respect to the total time. boolean queries produce a large number of false positives  which leads to very high fetching and analyzing time. the fetching time averaged 1 ms per file  and the analysis time averaged 1 ms per file. given that we end up fetching and analyzing thousands of files  it is relatively cheap to increase q: an additional 1 ms per word on average  or 1 ms if we allow phrases.  this relatively low incremental cost of adding terms opens an opportunity for weighted queries. they can be much more accurate  but they require significantly more query terms for good performance. the right half of figure 1 shows weighted queries up to q=1 terms  where we begin to see the query time take a visually perceptible amount of the overall time.  and because of their superior accuracy to boolean queries  they waste much less time fetching and analyzing false positives. figure 1 further illustrates this effect: for weighted queries there is a sharp increase in phase 1 recall rate with very large q  and yet with very little increase in false positives. by contrast  boolean queries obtain high recall at a terrible cost in false positives  i.e. bad precision. 

	false positives	 
figure 1.  recall vs. false positives for phase 1. 
a natural question that arises at this point is whether the complete classification task can be done more efficiently in a single phase 1 pass. that is  given the trained linear svm text classifier to be applied to the entire corpus  extract its weights and execute it only on the search engine  with no follow-up phase to further test the files. this basic idea has been tried   but not compared to the baseline  nor to two-phase classification.  note that the phase 1 classifier does not pay a time penalty for having a large number of features: the file fetch time and the feature extraction time depend on the disk performance and the file size  not on the number of terms to be extracted for classification.  in our experiments  we found that 1k word and phrase terms was superior for the phase 1 classifier.  but executing a query with such a large number of terms would pay a significant time penalty.  in fact  we conducted this experiment and found it took 1 seconds  averaged over the 1 classes.  this is 1x slower than using our two-phase system with q=1 in the first phase and 1k terms in the second.  with such a large q  the query time greatly exceeds the time it would take to fetch the few likely positive files and classify them. the two-phase classifier performs a balancing act in terms of the querying time vs. fetching and analyzing time. 
but besides time  there is a further disadvantage to running a single  high-dimensional classification on the search engine: recall the baseline classifier did not achieve as good f-measure as the two-phase system.  we discuss this effect next. 
1 cascaded classifiers 
cascaded classifiers have been used extensively in face detection from images where there is a huge computational cost involved in determining for every window in an image whether it contains a face or not . the computational load is overcome by cascading several classifiers  where the complexity of classifiers increases as we go further down the cascade. the first few classifiers of the cascade  which are very cheap  help in removing most of the  easy  negatives  and the more accurate  complex classifiers at the end of the cascade polish up with excellent discrimination  yielding good overall performance. our approach in this work is similar in spirit  where we additionally use the index to quicken the early classification phase. figure 1 illustrates the impact of our two-phase classification scheme in terms of fmeasure  precision and recall. the x-axis varies the number of word+phase terms used  while the number of features in the final classifier is 1k  which yielded excellent performance on average. two-phase classification has the consistent effect of 
 1
 1
 1
 1
 1
 1
q query terms
	phase 1 alone 	both phases 
 
figure 1. effect of two-phase classification on each performance measure.  
improving precision and lowering recall. this is natural  since a case will be classified negative if either classifier rejects it. the improvement in precision is generally more than the decrease in recall  which is reflected in the overall increase in f-measure. this can be attributed to the low correlation of classification errors of the two classifiers in the cascade which has been well studied in . the average f-measure is highest at q=1 query terms.  note that at the far right point  where q matches the number of terms in the final classifier  that the two-phase classifier has no effect.  in this case  the two phases are computing the same function in different ways  and the final decisions match that of a traditional  one phase classifier.  the benefit of the two phases only happens when the two classifiers have a somewhat different perspective on the training data.  we have run additional experiments  not shown  but we could if the reviewers request  that vary the number of features in phase 1 and phase 1 independently  and they find a consistent plummet in fmeasure whenever the number of features matches.   
figure 1 illustrates the two-phase classifier effect in terms of average f-measure over different groups of reuters categories. the plots indicate a consistent f-measure improvement across all categories by using the two-phase classifier. figure 1 also illustrates the f-measure variation based on the query term count q. for the country based categories  good f-measure performance is obtained with just 1 query terms. on the other hand  the industry and the economy categories require a lot more query terms  around 1 to achieve good f-measure. the impact of the two-phase classifier is also more pronounced in these difficult cases.  
1 policies for choosing parameters 
all the experimental results shown so far present the f-measure averaged over the 1 classes  or some subset  for different parameter settings  q and c . in a real-world setting  we are interested in picking parameters that maximize the f-measure for the single class at hand. we adopt a dynamic scheme using the cross-validation performance measures obtained during the learning phase. the cross-validation results of performance measures such as f-measure  precision and recall can be used to devise policies for identifying good parameter settings for each phase separately. we limit q to be a power of 1  = 1 terms  in order to avoid inordinate query time. table 1 shows the fmeasure obtained for some of the top performing policies. the 
 
figure 1. average f-measure for different reuters categories 
results indicate that the best performing cross-validation based method is very similar to an oracle having access to the fmeasure test set averaged over all 1 classes.  oracle i has access to the f-measures on test data for each class and hence  picks a parameter setting that maximizes the f-measure for each class. clearly  this represents the highest achievable f-measure in this setting. oracle ii has access to the f-measure averaged over all classes for all possible settings. 
1 varying design choices 
in section 1 we described a palette of design choices for the phase 1 and phase 1 classifiers.  here we briefly present their effect.   
table 1: comparison of f-measure obtained through crossvalidation based policies to choose parameters compared with two different oracle methods.  
policy for q policy for c f-measure oracle i oracle i 1 oracle ii oracle ii 1 maximize f-measure  maximize 
precision 1 maximize 
recall maximize 
precision 1 maximize f-measure maximize f-measure 1  
weighting instances to approximate the test distribution:  
a practical problem that arises in machine learning is one of changing class distributions from training to test phase  although 
 
figure 1. weighting instances improves phase 1 f-measure. 
it is typically avoided in most machine learning research . as mentioned earlier  this problem arises in our problem setting as well. the class distribution of the test data of the first phase classifier is different from its training set. we adopt the approach of weighting instances appropriately to overcome the difference. while training the first phase classifiers  we weight the instances in such a way that the total weight of the positive training set is 1%-a rough estimate of the prevalence we expect for typical searches.  it would give an unfair advantage to determine the exact prevalence of positives in the test set and then set the weight exactly. yet  this might be estimated via a quantifier .  the positive effect of such a weighting on the first phase f-measure is shown in figure 1.  on deeper inspection  the reweighting is consistently improving the precision of the classifier  naturally. 
phase 1 performance: 
figure 1 shows the f-measure of the first phase alone  as we vary the cross-product of the design choices.  for clarity of presentation   we hold q=1 fixed-a reasonably good choice for all. in this visualization  it is clear that  a  bns consistently outperformed ig  consistent with past studies    b  weighting the training positives to 1% significantly outperformed using the more balanced training set provided  right vs. left   and  c  the addition of phrases to the available terms improves performance.  a paired t-test indicates very strong statistical significance  even between the two closest points  bns vs. ig with phrases and without 1% weighting . 
in addition to these differences  we also analyzed the effect on query time  not depicted .  we found that ig queries consistently took longer than bns queries  e.g. 1x slower.  this is because ig tends to select terms that are more common and therefore have longer posting lists to process: 1x longer on average.  the preference of bns for rarer features is known   but here we have exposed a side benefit: it selects terms with shorter posting lists that are speedier information retrieval queries  as the research of  sought to do with ig-hybrid methods that somewhat preferred terms with shorter posting lists . 
phase 1 performance: 
these results so far only indicate the f-measure of phase 1. one might reasonably wonder whether these differences are mirrored by the final f-measure after both phases.  figure 1 presents the final f-measure as we vary the cross-product of the design choices  holding fixed q=1 query terms and c=1k classifier terms .  each design choice is kept consistent between phase 1 

weighted to 1%
	phase 1 choices	 
figure 1.  phase 1 f-measure vs. design choices. 

weighted to 1%
	phase 1 choice = phase 1 choice	 
       figure 1.  final f-measure vs. design choices. and phase 1.   one could conceive of the cross product of phase 1 and phase 1 design choices separately  but such high dimensional results are expensive to compute and difficult to present.    the leading design choice  bns +phrases and 1% weighted  continues to dominate  but the picture has changed somewhat. all the fmeasures have improved vs. the f-measures of phase 1 in figure 1.  the superiority of each design choice for phase 1 is now less pronounced  but each is still present.  
one design choice we discussed early on was to focus the phase 1 query on high recall  and expect the phase 1 classifier to increase the precision to the point that we obtain optimal f-measure. we experimented with such a strategy by leaving off the 1% weighting of the positive training examples; the 1% prevalence of positives in the training set therefore make the phase 1 classifier less conservative  and it yields higher recall.  but then we found the phase 1 classifier was not able to bring the precision as high.  
we found that the highest average f-measure obtained was 1%  which is inferior compared to the f-measure obtained in the weighted case. 
1. discussion 
we have shown in this work that usage of phrases improves the fmeasure of the classifier over a simple bag of words representation. clearly  there are other features-such as n-grams  file metadata  file-format-sensitive features  e.g. stripping xml before indexing   and domain-specific features-which could potentially aid in improving classification performance and can be added to the index. on the other hand  there are cases where the file needs to be explicitly fetched. consider a case  where someone is searching for articles about xml on their disk  or info about a merger with xml inc. company. the index  being made with a generic bag of words parser that does not first strip xml  would have xml tags in every single reuters article. clearly  a query working on the index would do badly in terms of precision and this scenario furthers the case for having a second phase classifier which operates on file-format specific features  e.g. parse the xml and make features from the non-tag text.  
typical search engines today limit the number of query terms allowed to 1 or 1 maximum.  viewed from their perspective  they want to limit the resource consumption of customer queries. but as businesses begin to get greater value from their  central or someday  federated  search infrastructure via text classification queries  there will be business justification to raise term limits into the hundreds or thousands.  this leads to a qualitatively different operating region for search engines  and we may see a substantial increase in computer resources used for text classification searches. 
1. related work 
there is a single prior work in the literature that is highly relevant. anagnostopoulos  broder  and punera  considered a classifier executed via a weighted search engine query  which they refer to as weak and  wand .  this is equivalent to our weighted queries of phase 1 alone.  their concern was with using a search engine for classification  with no phase 1 follow-up to improve accuracy. our experiments in section 1 found that operating entirely on the index took 1x longer than if we included a separate phase 1.  even with respect to phase 1 alone  there are a number of dimensions in which our work adds further contributions: they only consider a bag-of-words model  where we also measure the benefit of phrases  which search engines easily facilitate. they only consider up to q=1 terms  where we extend to q=1. they did not consider bns  which we found superior to ig.  their results are reported in terms of area under the roc curve  which is mainly insensitive to improvements in fmeasure when positives are very rare  e.g. 1%.  and significantly  they did not run their experiments with a cold cache  so their reported results do not take disk performance into account. additionally  we address the class distribution difference between the training set and test set using weighting of training data instances and we show significant improvement in f-measure performance. they consider the novel angle of biasing the feature selection according to the length of the term posting lists  in order to reduce processing time. 
there is an older paper by broder et al  that provides techniques for speeding up a traditional information retrieval query. their experiments showed substantial speedup for queries averaging 1 to 1 terms  suffering little loss in precision. coincidentally  they also use two phases  but the similarity is superficial.  their first phase retrieves the posting lists of the more important terms  as determined by their method   and the second phase retrieves all the remaining terms  but tracking scores only for documents that scored well in the first phase.  note that the work does not address classification  but their technique could be leveraged to speed up our phase 1 query on the index with q¡Ö1.  our current software simply scores every document that is mentioned by the posting lists.  
1. conclusion and future work 
our goal was to make it text classification scalable enough to scan distributed enterprise file systems for relevant documents.  ideally such an application would also include infrastructure to efficiently federate the search over many full-text indices throughout a corporation.  in a real-world deployment  the size of the datasets would be much larger than the publicly available reuters rcv1 collection  which occupies only 1 gb of disk space  and whose lucene index occupies only ~1 mb of disk space.  the index is small enough to fit entirely in ram  although by dropping the file cache  we ensured that we were measuring realistic disk delays.  for real-world usage  the index would be much larger and the reverse posting lists would be longer.  while this suggests longer query times and perhaps a desire to reduce the query size q to save time  keep in mind that with a larger  distributed infrastructure also comes much longer latency and lower bandwidth to fetch the actual files for phase 1.  hence  if anything  we anticipate the time and workload savings of our twophase technique to be proportionately more important for larger environments.  the world wide web is an extreme thereof-a query to a fast  central index server such as google can efficiently provide a list of relevant documents  but actually fetching them for a second phase test will be very slow. moreover  on a large scale it is likely to suffer from web servers being unavailable and intermittently poor network performance.  in view of this  text classification on the web may best be served by extensive computation with the reverse indices and perhaps google's large file cache.  in the end  it may still be important to fetch and check the final list of hits  since urls often become stale when web pages are deleted.  also  a web page may have changed substantially since its indexing  and no longer match the matching criterion. 
an interesting direction of future research would be to devise schemes for using an index in an active learning setting. in the real-world where training data is so scarce  active learning plays a crucial part in acquiring labeled data through an oracle who has time constraints. in such a time constrained setting  it is crucial that the processing to identify useful training examples for labeling has to be done quickly. the index can be exploited to quicken the processing step  which normally involves identifying the most informative instance s  to be labeled towards improving the current classifier.  
in this paper  we excluded the training time from interest  for we are targeting settings where it is dwarfed by the volume to classify. but with active learning  this training time goes into the user's interactive loop. the rising multi-core revolution can decimate this training time  as well as the time to analyze file content for text features.  however  it does not address the primary bottleneck for this application: disk seek time involved in executing queries and fetching files. in fact  the relative speed between cpu and disk is becoming larger  so the techniques described in this paper should continue to be relevant. 
considering the enormous interest in ranking problems  a related research direction would be to analyze the impact of two-phase processing through an index in the context of ranking. in our experiments  we noticed a consistent improvement in precision in the top 1 from first phase to second phase. it would be interesting to analyze the performance of two-phase ranking schemes on rigorous ranking accuracy measures.  
1. acknowledgments 
our thanks to hernan laffitte and eric anderson for their invaluable support in setting up racks of machines with the updated linux kernel. 
