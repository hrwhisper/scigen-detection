in this paper  we consider the problem of identifying and segmenting topically cohesive regions in the url tree of a large website. each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier. we develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels. we propose a general framework to use these measures for describing the quality of a segmentation; we also provide an efficient algorithm to find the best segmentation in this framework. extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval
general terms
algorithms  experimentation  measurements
keywords
website hierarchy  website segmentation  tree partitioning  classification  facility location  gain ratio  kl-distance
1. introduction
　as the major established search engines vie for supremacy  and new entrants explore a range of technologies to attract users  we see researchers and practitioners alike seeking novel analytical approaches to improve the search experience. one promising family of approaches that is generating significant interest is analysis at the level of websites  rather

 
 this work was done while the author was at yahoo! research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  philadelphia  pennsylvania  usa.
copyright 1 acm 1-1/1 ...$1.
than individual webpages. there are a variety of techniques for exploiting site-level information. these include detecting multiple possibly-duplicated pages from the same site  1  1   determining entry points   identifying spam and porn sites   detecting site-level mirrors   extracting site-wide templates  and structures   and visualizing content at the site level .
　in this paper we consider site segmentation  a particular form of site-level analysis in which a website must be segmented into one or more largely uniform regions. the segmentation may be performed based on the topics discussed in each region  or based on the look and feel  or based on the authorship  or other factors. we focus specifically on topical segmentation  i.e.  segmenting a site into pieces that are largely uniform in the topics they discuss. such a topical segmentation offers many potential advantages:
　 i  various algorithms that are currently applied to websites could more naturally be applied to topically-focused segments.
　 ii  websearch already incorporates special treatment forpages that are known to possess a given topic-for instance  many engines provide a link to the topic in a large directory such as the yahoo! directory  wikipedia  or the open directory project. these approaches can naturally be extended when several pages from a search result list lie within a topically-focused segment.
　 iii  the resultant segments provide a simple and concisesite-level summary to help users who wish to understand the overall content and focus of a particular website.
　 iv  a host such as an isp may contain many individualwebsites  and a topical segmentation is a useful input to help tease out the appropriate granularity of a site.
　 v  website classification is a problem that has been addressed using primarily manual methods since the early days of the web  in part because sites typically do not contain a single uniform class. segmentation is an important starting point for this larger problem.
　site segmentation may be viewed from two distinct perspectives. first  it may be viewed as a constrained clustering problem in which the allowable segments represent constraints on the possible clusters that the algorithm may return. at the same time  site segmentation may be viewed as an extended form of site-level classification in which the algorithm may choose to classify either the entire site  or various sub-sites. the measure we propose for the quality of a segmentation is much simpler than standard measures from machine learning. as a result  while the problem may be viewed as a constrained version of the np-hard clustering problem  or an extended version of classification that incorporates a search for the appropriate objects to classify  the simple measure of segmentation quality  combined with the class of allowable segmentations  will allow us to provide an algorithm to return the optimal segmentation in polynomial time. to achieve this bound  we employ a dynamic programming algorithm that is quite different from traditional algorithms for either clustering or classification.
　one could consider many different classes of allowable segmentations of a website  for example based on the hierarchical structure of the site  or based on clusters in the intra-site link graph  or based on regions of the site that display some commonality of presentation template  and so forth. we will focus specifically on segmentations that respect the hierarchical structure of a website  for two reasons. first  we believe that of the many possible approaches to segmenting websites  hierarchy is the most natural starting point. site authors often think in terms of a site being made up of several sub-sites  each of which may contain sub-structure of its own; and the layout of pages on a website often follows a  folder  structure inducing a natural hierarchy. and second  in many applications an individual segment must be returned to the user in some succinct manner. rather than simply returning a long list of urls located at various positions within the site  it is desirable to return instead a pointer to a particular sub-site.
　in general  the hierarchical structure of a website may be derived from the tree induced by the url structure of the site  or mined from the intra-site links or the page content of the site. our algorithm makes use of whatever hierarchical information is available about a site to constrain the possible segmentations. we show that 1% of sites exhibit a nontrivial form of hierarchy based on the url tree that can exploited by our algorithm for segmentation. the remaining fraction of sites might have a latent hierarchical structure that could be mined by further analysis of intra-site links or content  but that is beyond the scope of this paper.
　thus  our paper is on hierarchical topic segmentation  hts : the segmentation of websites into topically-cohesive regions that respect the hierarchical structure of the site. formulation. consider a tree whose leaves have been assigned a class label or a distribution on class labels  perhaps by a standard page-level classifier. a distribution is induced on an internal node of the tree by averaging the distributions of all leaves subtended by that internal node. these distributions  along with a hierarchical arrangement of all the pages in the site  are provided to the hts algorithm. the algorithm must return a set of segmentation points that optimally partition the site. the objective function for the segmentation is a combination of two competing costs: the cost of choosing the segmentation points  the nodes  themselves and the cost of assigning the leaves to the closest chosen nodes. intuitively  the node selection cost models the requirements for a node to serve as a segmentation point  while the cohesiveness cost models how the selection of a node as a segmentation point improves the representation of the content within the subtree rooted at it. for example  in a particular instance of the problem  the node selection cost can capture the requirement that the segments be distinct from one another and the cohesiveness cost can capture the requirement that the segments be pure. the underlying tree structure enables us to obtain an efficient polynomial-time algorithm to solve the hts problem.
　to complete the overview of hts  we provide a brief discussion of the difference between segmentation and classification. the general website classification problem tries to assign topics to websites by employing features that are broad and varied. a few example features for this broader problem include the topic of each page  the internal hyperlinks on the site  the commonly link-to entry points to the site  with their anchor-text  the general external link structure  the directory structure of the site  the link and content templates present on the site  the description  title  and h1 tags on key pages on the site  and so forth. the final classes in a website classification problem may be distinct from the classes employed at the page level. hts  on the other hand  specifically focuses on aggregating the topic labels on webpages into subtrees according to the hierarchy of a site  in order to convey information such as   this entire sub-site is about sports.  thus  hts attacks the problem of determining whether and how to split the site  but is only the beginning of a broader research problem of classifying websites using rich features. the broader problem is of great interest in both binary cases  is the site spam  is it porn   and multi-class cases  to what topics should i assign this site  . we believe that a clean and elegant solution to the hts problem is essential to fully address the more general site classification problem.
summary of contributions. we provide a rigorous formulation of hts for websites that is general enough to capture many different hierarchical topic segmentation schemes. we show how to encode two natural requirements within our formulation: the segments themselves should be sufficiently 'distinct' from each other and the webpages in a segment should be reasonably 'pure' in topic. we also present a polynomial-time algorithm to solve the hts problem optimally.
　we conduct an extensive set of experiments to evaluate the performance of our algorithm with various natural cost measures on hand-labeled as well as semi-synthetic websites. we show that a judicious choice of the node selection cost and cohesiveness cost can vastly improve the performance of the algorithm.
organization. section 1 contains relevant work on hierarchical classification and segmentation. section 1 presents the framework for the hts problem. section 1 contains algorithm for the hts problem as well as definitions for the cohesiveness and node selection costs. finally  the experimental results are presented and discussed in section 1.
1. related work
