there has been significant recent progress in reasoning and constraint processing methods. in areas such as planning and finite model-checking  current solution techniques can handle combinatorial problems with up to a million variables and five million constraints. the good scaling behavior of these methods appears to defy what one would expect based on a worst-case complexity analysis. in order to bridge this gap between theory and practice  we propose a new framework for studying the complexity of these techniques on practical problem instances. in particular  our approach incorporates general structural properties observed in practical problem instances into the formal complexity analysis. we introduce a notion of  backdoors   which are small sets of variables that capture the overall combinatorics of the problem instance. we provide empirical results showing the existence of such backdoors in real-world problems. we then present a series of complexity results that explain the good scaling behavior of current reasoning and constraint methods observed on practical problem instances. 
1 introduction 
most interesting ai formalisms for reasoning  planning  and learning have been shown to be worst-case intractable. in the eighties and early nineties  such negative complexity results led to an extensive search for tractable subclasses of the general formalisms. unfortunately  these tractable subclasses were often too restrictive for real-world applications. in the mid-nineties  we saw the emergence of a more practical approach to computationally hard problems in ai  with the introduction of fast satisfiability solvers and fast constraint based reasoning methods . for example  in planning we saw the success of constraint-based planners  such as graphplan  and satplan 1   and most recently  heuristic search 
　*supported in part by an nsf graduate fellowship and the nsf aladdin center. 
research supported by afosr  darpa  and the nsf. 
satisfiability 
based planners  e.g.   1; 1; 1 . somewhat surprisingly  on practical problem instances these methods scale well beyond what one might expect based on a formal complexity analysis. in fact  current state-of-the-art sat solvers can handle problem instances  as they arise in finite model-checking and planning  with up to a million variables and five million clauses . the success of these methods appears to hinge on a combination of two factors:  1  practical combinatorial problem instances generally have a substantial amount of  hidden  tractable sub-structure  and  1  new algorithmic techniques exploit such tractable structure  through  e.g.  randomization and constraint learning. 
　these developments suggest that a standard worst-case complexity analysis does not capture well the true complexity of typical problem instances encountered in practical applications. theoretical computer scientists have been wellaware of the limitations of worst-case complexity results and have explored alternatives  such as average-case complexity and smoothed analysis . in average-case analysis  one studies the computational cost of solving problem instances drawn from a predefined problem distribution. such an analysis can provide valuable insights  as demonstrated by the work on uniform random instance distributions  e.g. random k-sat . however  the relatively basic distributions for which one can obtain average-complexity results appear to be quite far removed from the instance distributions one encounters in practice. in fact  formally defining the distribution of real-world problem instances is generally an open problem in itself. smoothed analysis attempts to unify worst-case and average-case  but suffers from limited applicability: it works well on algorithms for problems defined over dense fields such as the simplex algorithm  but the applicability of smoothed analysis on discrete problem domains is unclear. 
　an alternative approach  which we will pursue in this paper  is to identify special structural properties common to known problem instances and rigorously show how clever algorithms can exploit such properties. informal insights about what such special structure might be are currently already used in the design of  for example  branching and variable choice heuristics in combinatorial search methods. a common feature of these techniques is an understanding that different groups of variables in a problem encoding often play quite distinct roles. for example  at the highest level  
1 

one can distinguish between dependent and independent variables. the dependent or auxiliary variables are needed to obtain compact problem encodings but the true combinatorics arises from the independent variables; e.g.  the independent variables in an encoding of a planning domain represent the various operators applicable in a given state of the world  whereas the dependent variables encode the consequences of selecting a particular operator. a plan search technique that branches purely on the independent variables can obtain substantial speedups over search methods that do not exploit variable dependencies . 
   another powerful intuition in the design of search methods is that one wants to select variables that simplify the problem instance as much as possible when these variables are assigned values. this intuition leads to the common heuristic of branching on the most constrained variable first. in terms of boolean satisfiability  this amounts to  in effect  focusing in on the tractable substructure of the problem  namely the unit clauses  1-sat structure  and the binary clauses  1-sat structure . the true effectiveness of this approach arises from the fact that setting most constraint variables also simplifies higher arity clauses  which either become satisfied or in turn shrink themselves eventually to binary or unary clauses. 
　these general insights have been incorporated in state-ofthe-art sat and constraint solvers  and their effectiveness has been demonstrated empirically on a significant number of benchmark problems . however  a more formal underpinning explaining the practical success of these strategies has been lacking. in this paper  we introduce a formal framework directly inspired by these techniques and present rigorous complexity results that support their effectiveness. 
   preview of results. we first introduce the notion of  backdoor  variables. this is a set of variables for which there is a value assignment such that the simplified problem can be solved by a poly-time algorithm  called the  sub-solver . the sub-solver captures any form of poly-time simplification procedure as used in current sat/csp solvers. we also consider the notion of a  strong backdoor  where any setting of the backdoor variables leads to a poly-time solvable subproblem. the set of all problem variables forms a trivial backdoor set  but many interesting practical problem instances possess much smaller backdoors and strong backdoors. we will study backdoors in several practical problem instances  and identify backdoors that contain only a fraction of the total number of variables. for example  the sat encoding of a logistics planning problem   l o g i s t i c s . d. c n f   contains a backdoor with only 1 variables out of a total of nearly 1 variables. when given a set of backdoor variables of a problem instance  one can restrict the combinatorial search by branching only on the backdoor variables and thus search a drastically reduced space. 
　in general  finding a small set of backdoor variables for a 
　problem instance is  however  itself a computationally hard problem. one contribution of this paper is that we formally show how the presence of a small backdoor in a problem provides a concrete computational advantage in solving it. we analyze three scenarios. first  we consider a deterministic 
1 
table 1: time bounds for solving csps in the various scenarios considered in this work. is an upper bound on the size of the smallest backdoor  where is the number of variables in the problem  is a fixed constant. empirical results  section 1  suggest that for practical instances the backdoor is often a relatively small fraction of or even of size log  
scenario with an exhaustive search of backdoor sets. we show that one obtains provably better search complexity when the backdoor contains up to a certain fraction of all variables. we then show that a randomized search technique  which in effect repeatedly guesses backdoor sets  provably outperforms a deterministic search. finally  in our third scenario we consider the availability of a variable selection heuristic  which provides guidance towards the backdoor set. this strategy can yet further reduce the search space. table 1 gives a highlevel summary of the results. by exploiting restart strategies  we can identify a polynomially solvable case when the backdoor contains at most log n  variables. we believe that this final scenario is closest to the behavior of current effective sat and constraint solvers. our formal analysis also suggests several novel algorithmic strategies that warrant further empirical exploration. 
1 hidden structure: backbones and backdoors 
our approach and analysis applies both to sat and csp problems . sat is the abbreviation for the well-studied boolean satisfiability problem. csp is the abbreviation for the more general problem of constraint satisfaction. 
a csp problem  c  is characterized by a set v = of variables  with respective domains  
　　　　　 which list the possible values for each variable  and a set of constraints. a constraint is defined on a subset of variables denoting the variables' simultaneous legal assignments. that is  if then the constraint defines a subset of the cartesian product 
 to simplify notation  we will assume that all variables have the same domain d. we use d to denote the size of d. an assignment  is a function from variables to d. a solution to a csp is a complete variable assignment that satisfies all constraints. a partial assignment defines the values of a subset of the variables in v. sat is a special case of csp with only boolean variables and constraints given in the form of clauses. a clause is a 
disjunction of literals and a literal is a boolean variable or its negation. 
　we use the notation  to denote the simplified csp obtained from a csp  c  by setting the value of variable  to value   a constraint involving  is simplified by keeping only the allowed tuples that have  assigned to  
satisfiability 

be a partial assignment. we use to denote the simplified csp obtained by setting the variables defined in in a sat problem  this corresponds to simplifying the formula by fixing the truth values of some of the variables. 
　our goal is to capture structural properties of real world problem instances. we start by reviewing the concept of a backbone in a sat/csp problem  as introduced in  1. a variable is called a backbone variable if in all solutions to the csp the variable is assigned the same value. such variables are also called frozen variables  1. backbone variables are useful in studying the properties of the solution space of a 
　constraint satisfaction problem. 
definition 1  backbone  s is a backbone if there is a unique partial assignment  such that  is satisfiable. 
　we contrast this variable type with the kind we introduce  backdoors. backdoors are variable subsets defined with respect to a particular algorithm; once the backdoor variables are assigned a value  the problem becomes easy under that algorithm.  note that contrarily to the backbone there can be different sets of backdoor variables.  
　to begin our exposition of backdoors  we define the sort of algorithms we have in mind. we will call them sub-solvers  as they solve tractable subcases of the general constraint satisfaction problem. 
definition 1 a sub-solver a given as input a csp  c  satisfies the following: 
   trichotomy  a either rejects the input c  or  determines  c correctly  as unsatisfiable or satisfiable  returning a solution if satisfiable   
 efficiency  a runs in polynomial time  
　  trivial solvability  a can determine if c is trivially true  has no constraints  or trivially false  has a contradictory constraint   
　  selfreducibility  if a determines c  then for any variable x and value v  then a determines  
　for instance  a could be an algorithm that solves 1-sat instances but rejects all other instances. it is important to note that the results we will show in this paper are independent of a particular sub-solver; our results will hold for any a satisfying the above four properties. 
in what follows  let a be a sub-solver  and c be a csp. 
　we first consider a notion of  backdoor  that is suitable for satisfiable csps. 
definition 1  backdoor  a nonempty subset s of the variables is a backdoor in c for a if for some a returns a satisfying assignment of  
　intuitively  the backdoor corresponds to a set of variables  such that when set correctly  the sub-solver can solve the remaining problem. in a sense  the backdoor is a  witness  
satisfiability 
to the satisfiability of the instance  given a sub-solver algorithm.1 we also introduce a stronger notion of the backdoor to deal with both satisfiable and unsatisfiable  inconsistent  problem instances. 
definition 1  strong backdoor  a nonempty subset s of the variables is a strong backdoor in c for a if for all 
　　　　　a returns a satisfying assignment or concludes unsatisfiability of  
　in contrast to backbones which are necessarily set to a certain value  a  strong  backdoor s is sufficient for solving a problem. for example  when given the backdoor for a sat problem  the search cost is of order  simply check all possible assignments of this means if s is relatively small  one obtains a large improvement over searching the full space of variable/value assignments. 
　we observe that independent variables are a particular kind of backdoor. as stated in  they are a set s of variables for which all other variables may be thought of as defined in terms of s. for example  a maximal subset of independent variables in a sat encoding of a hardware verification problem is a backdoor for unit propagation  as the other variables' values may be directly determined after setting the independent ones . 
there are two key questions concerning backdoors: 
 what is the size of the backdoor in practical problem instances  
 when taking into account the cost of searching for a backdoor set  can one still obtain an overall computational advantage in solving the csp  
　we address these two key questions below. we will first show that practical problem instances can have surprisingly small backdoors. in the subsequent section  we show how even by taking into account the cost of searching for a backdoor  one can provably obtain an overall computational advantage by using the backdoor. as we will see  the magnitude of this improvement is  of course  a function of the size of the backdoor. 
1 size of backdoors 
we did an empirical study of the size of backdoors in several practical sat instances  using the sat solver satz-rand  a randomized version of satz . satz incorporates powerful variable selection heuristics and an efficient simplification strategy  i.e.  a good sub-solver . we modified satz-rand to trace the variables selected for branching  and to keep track of the minimum number of variables that need to be set before satz-rand's simplification found a satisfying assignment efficiently.  we are currently modifying this procedure to also handle unsatisfiable instances and find strong backdoors.  
   'observe that any satisfiable csp has a backdoor of size at most however  wc will see that significantly smaller backdoors arise in practice and give a computational advantage in search. 
1 

instance backdoor fract. logistics.d 1 1 1 1 1bitadd 1 1 1 1 1 pipe-1 1 1 1 1 qg 1 1 1 1 1 qg 1 1 1 1 1 table 1: size of backdoors for several practical sat instances. 
　table 1 summarizes our results. our instances are from a variety of domains . these instances are now well within the range of the fastest current solvers  such as chaff . however  they are non-trivial and cannot be solved with the previous generation of sat solvers  e.g. tableau . clearly  the new solvers are better able to discover and exploit hidden structure  such as small backdoors. in fact  as we can see from the table  these instances have fairly tiny backdoors. that is  only a very small fraction of all variables can be used to  unlock  a satisfying assignment. we conjecture that such small backdoors occur in many other real-world problem instances. 
1 exploiting backdoors formally 
we will analyze three  increasingly powerful strategies: deterministic  randomized  and heuristic branching variable selection. the first two are meant to work for any csp where the instance has a small fraction of backdoor variables  with respect to the sub-solver. the randomized strategy generally outperforms the deterministic one with high probability  1 -  where  is the number of variables . this reflects the performance gain found in practice when backtracking sat solvers are augmented with randomization  1; 1 . the third strategy yields tighter runtime bounds than the first two  but requires us to assume the existence of a good heuristic for choosing backdoor variables  which we find to be the case in practice . 
1 	deterministic strategy 
the deterministic procedure may be construed as a generalization of iterative deepening that runs over all possible search trees of each depth. we assume the algorithm has access to a particular sub-solver a running in   polynomial  time  which defines the backdoor variables  and c is an arbitrary csp instance. 
algorithm 1 given a csp c with n variables  
for  
for all subsets 1 of the variables with  
　perform a standard backtrack search  just on the variables in s  for an assignment that results in c being solved by sub-solver a. 
　an analogous algorithm works for finding and exploiting strong backdoors in a csp to prove unsatisfiability: simply keep track of whether all assignments to the variables in s result in c being a contradiction  as determined by a . all 
1 
of the following we will say holds for strong backdoors and unsatisfiable csps under this modified algorithm. 
　note the procedure uses only polynomial time for csps with a constant sized backdoor. we are interested in the case where a backdoor of size exists  for some  almost everywhere. the following gives a simple runtime bound in terms of 
　the theorem implies that when small backdoors  or strong backdoors  are present  a substantial speedup almost always results. for example: 

　in our exposition of heuristic branching variable selection  we will see an improvement on this  a poly-time bound . for a visual representation of the deterministic strategy's runtime  when d - 1 and backdoors of size  are considered  see figure 1. this graph also indicates the following corollary in the case of sat  proof omitted : 
corollary 1 for boolean formulas with a backdoor of size at most n/1  algorithm 1 solves the formula in time  where c  1. 
　as we have seen in the previous section  in practice  backdoors can be quite tiny  of the variables  for l o g i s t i c s . d. cnf . therefore  these results have real bearing on the improved solvability of real-world csps. 
1 	randomized strategy 
better performance results from adding randomization. this speed-up formally verifies a well-known fact about real-world solvers: augmenting a solver with randomization can dramatically improve performance  1; 1 . 
again  we assume a sub-solver a is on tap  with runtime 
let be a poly-time computable function on n that bounds the backdoor size  and b be a parameter to be later determined. the idea is to repeatedly choose random subsets of variables that are larger than  searching these subsets for a backdoor. 

algorithm 1 given a csp c with variables  
repeat times  and at least once : 
　randomly choose a subset s of the  variables  of size  perform a standard backtrack search on variables in s. if c is ever solvable by a  return the satisfying assignment. 
　as before  an analogous algorithm works for general  satisfiable or unsatisfiable  csps with strong backdoors: if every leaf in the search tree ends with a reporting unsatisfiability  then the c is unsatisfiable. 
the algorithm as stated requires a priori knowledge of 
 this may be corrected by choosing a constant then running the algorithm assuming a backdoor of size 1. if that fails  run it again assuming a backdoor of size a  then a1  etc.  until a solution to c is found. 
theorem 1 if c has a backdoor of size  algorithm 1 finds a satisfying assignment with probability approaching 1. 
proof. given there is a -sized backdoor in c  the probability that a randomly chosen s of size contains the entire backdoor is at least 

	one 	can 	show 	that 	the 	algorithm 	runs 	in 
	time. 	it 
remains to choose b to minimize this expression. as b depends directly on we evaluate two natural cases for 
   when b n  =. klogu for some constant k  the runtime is 	for some constant 	for large 	the runtime is optimized when 	is constant; it is an improvement over the deterministic bound. 
　　when 	for some constant 	we can show the runtime is minimized when 	resulting in a 
time bound. for 
example  when d = 1  the case of sat   and the following holds. 
corollary 1 for boolean formulas with at most backdoor variables  algorithm 1 solves the formula in  time  where c 1. 
in the corollary  c is a function of a:. see figure 1. 
satisfiability 

figure 1: improved exponential time. when d = 1  sat  and the size of the backbone is a constant fraction of the number of variables  the runtime of alg. 1  deterministic  and 1  randomized  is of the form   vertical axis  is a function of k. the top curve gives c as a function of k for the deterministic procedure. the bottom curve gives c for the randomized procedure. note that for the randomized algorithm performs exponentially better than whereas such an exponential improvement for the deterministic algorithm does not occur until  
1 	heuristic strategy 
so far  we have considered general systematic and randomized search strategies for finding and exploiting backdoors. however  practical combinatorial solvers generally use heuristics to guide the variable selection process. as noted in the introduction  a common principle is to first branch on variables that simplify an instance the most. in effect  this means such heuristics steer the variable choice towards variables in a backdoor set. we will now formally analyze such heuristic guidance. 
　restart strategies for heuristic search. by incorporating the notion of a variable choice heuristic into our framework  our results are further sharpened. we consider the case where a randomized depth-first search  dfs  solver with a sub-solver a is running on an instance c having a backdoor of size b. the solver chooses variables to branch on according to a heuristic h  which has a success probability of at least  of choosing a backdoor variable at any point in the search. we will use the notation  dfs h .1  to denote a solver with the above properties. 
　informally  a restart strategy is simply a policy that restarts a solver after running it for a specified amount of time  until a solution is found. our main result here gives a condition under which a polynomial time restart strategy exists for dfs solving csps with small backdoors. 
theorem 1 if the size of a backdoor of a csp c is b  
1 
for some constant c  then  dfs h a  has a restart 
strategy that solves c in polynomial time. 
proof. since the probability of choosing a backdoor variable is at l e a s t t h e probability that we consecutively choose them is the probability of choosing the correct solution with only a polynomial amount of backtracking in the dfs is at l e a s t f o r some constant  suppose for some constant c. then 
by restarting the solver after every steps  where is the runtime of a   there is probability in each run that the backdoor will be found within a amount of backtracking  and set correctly. from this one can show that the above inequality holds precisely w some constant c.   
   an analogous result holds for strong backdoors. it turns out that the given bound on d is asymptotically tight; we will not prove that here. when the variable domain size is constant  e.g. sat  1-coloring  etc.   we have the following. let / be any poly-time computable function on the natural numbers. 
corollary 1 given csps w i t h b a c k d o o r for which h has success probability has a polynomial time restart strategy. 
when the success probability is constant  then csps with 
o logn  backdoors can be solved using a polynomial time restart strategy on  dfs h a . this result is the best possible in terms of backdoor size  as it would take super-polynomial time to search for a solution among backdoor variables. the heuristic search runtime when is still exponential  but this exponential drops dramatically as  decreases  even when compared to the previous two al-
gorithms. that is  the runtime is on the order of where  is the domain size and is success 
probability . 
formal discovery of heavy-tails in heuristic search. 
we briefly outline our theoretical results connecting the heuristic search model described earlier with heavy-tailed runtime phenomena found empirically 1 . it was conjectured that  critically constrained  variables were a cause of the heavy-tailed behavior. we can prove that small sets of backdoor variables lead to runtime profiles that are bounded from below by heavy-tails. 
   the analysis that achieves this result introduces a selfsimilar binary tree structure  which we call a variable choice tree. such trees recursively model a heuristic's selection of backdoor variables; as more backdoor variables are chosen  the resulting search cost is much lower. it turns out that backtracking solvers with variable choice heuristics can be modeled precisely by these variable choice trees  when the size of a backdoor in the instance is small. analysis of these trees leads to the following: 
theorem 1  heavy-tail lower bound  if the backdoor size of an csp c is  then the runtime distribution of  dfs a h  on c is lower-bounded by a pareto-levy distribution  when the success probability of h is constant. 
1 
1 conclusions 
we have formalized the idea of backdoor variables in csp/sat instances. backdoor variables can be used to significantly reduce the search needed in solving csp/sat problems. we showed that practical instances can have surprisingly small backdoors. we also provided a detailed formal analysis demonstrating that one can obtain a concrete computational advantage by exploiting such backdoors. 
