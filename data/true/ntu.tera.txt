there are three tasks in the terabyte track of trec 1  i.e. efficiency  ad hoc and named page finding. we participated in all the tasks and use different retrieval methods to deal with each task  aiming to vary the retrieval method according to the different characteristics of different tasks. in ah hoc task  we adopt the technique of query-specific clustering. in named page finding task  we cared more about the information of document title and anchor text of out-links. 
1 	introduction 
this is the first year we participated in terabyte track. the primary goal of this track is to develop an evaluation methodology for terabyte-scale document collections. besides  efficiency and scalability issues are also concerned. some different criteria are used in evaluation because the information needs of the web users often vary a lot. there are three tasks in this track  i.e.  efficiency  ad hoc  and named page finding. our retrieval methods for each task are mainly based on okapi   with some variants according to the characteristics of different tasks. in this paper  we focus on ad hoc task and named page finding task since we simply used okapi to retrieve documents based on content words in efficiency task. 
 in ad hoc task  we employed clustering technique to improve retrieval performance. a scoring function was adopted to rank clusters that were generated from the top n documents retrieved by okapi  i.e.  query-specific clustering . 
¡¡¡¡in named page finding task  the information from the document title and anchor text is very useful and important to identify the named pages  so that we increased the weight of document title and anchor text when computing the relevance score of a document to a query. 
1 	preprocessing and indexing 
all the documents in the corpus were stemmed using porter's algorithm   and all words except stop words were indexed. titles  i.e.  the words within  title  and  /title  in html format  of documents are extracted and indexed additionally  so as anchor text of out-links of documents. the two additional indices will be used in named page finding task. 
1 	ad hoc task 
1 motivation and description of our method 
in a typical ad hoc retrieval task  the ir system is requested to retrieve as many relevant documents to some topics  queries  as possible. for most topics in ad hoc tasks of trec  there are usually tens of relevant documents. for example  topics of trec1  1  have 1 relevant documents  about 1 relevant documents per topic. intuitively  as the number of documents increases  the number of relevant documents increases  too. under this situation  we tried to cluster relevant documents together in the ranked list retrieved by okapi  so called query-specific clustering   to improve retrieval performance. clustering hypothesis  has been verified to be held in the manner of query-specific clustering . 
¡¡¡¡our method is a two-stage approach. in the first stage  we used okapi to retrieve 1 documents. in the second stage  the top n  n¨q1  documents retrieved in the first stage were clustered. after that  those clusters were scored and ranked by a heuristic function. in the final ranked list  all documents in a higher-scored cluster would be ranked higher than those in a lower-scored cluster  and the intra-cluster ranking of documents would follow their ranks in the first stage. 
¡¡¡¡for the reason of efficiency  we used bi-section k-means  which has shown to be an efficient and high-quality clustering algorithm  for query-specific clustering . 
1 ranking clusters 
in the ranked list returned by an ir system based on probability model  a document with a higher rank is more possible to be relevant to the query. according to this property  we proposed a simple cluster scoring function. this function  which is in terms of the rank of a document and the general performance of the ir system  estimates the probability of the document to be relevant  and further the quality of certain cluster. the general performance of okapi here is the p r of testing the topics used in last year's terabyte track  where r is one of {1  1  1  1  1  1  1  1  1}. the results are listed in table 1. different values of r represent the upper bounds of rank levels of document rank  so we partitioned documents into nine rank levels. assume the function l r  maps a document with rank r into certain rank level.  that is  l 1 =1  l 1 =1  l 1 =1  and so on. according to table 1  we can estimate the relevant probabilities of documents in different rank levels. for example  the relevant probability of a document with rank 1  i.e.  the 1th rank level  can be estimated by  
 1 * p 1 £­ 1 * p 1  £¯  1£­1  £½ 1 
in this way  we can derive table 1 from table 1. it shows the relevant probability rp l  related to rank level l. 
¡¡¡¡let ci denotes the ith cluster  rij denotes the rank of the jth document in ci and |ci| denotes total number of documents in ci. the cluster scoring function s ci  determines the average relevant probability of documents in ci: 
	1	|ci|
	s ci  = | ci | ¡Æj=1 rp l rij   	 	 	  1  
¡¡¡¡after scoring clusters  all the clusters were ranked according to their scores  and the final ranked list is generated. 
table 1. the performance of okapi on trec1 topics 
r 1 1 1 1 1 1 1 1 1 p r 1 1 111111 1table 1. relevant probability related to rank level l 1 1 1 1 1 1 1 1 1 rp l  1 1 111111 1 
¡¡¡¡besides table 1 and table 1  we also used the topics of last year to determine the number of clusters  k  which is a parameter for query-specific clustering using bi-section k-means algorithm. in the first stage of our method  1 documents were retrieved and regarded as the baseline  without re-ranking   but only the top n will be clustered and re-ranked. by testing our method with the topics of last years  k=1 and n=1 showed a better result. however  no matter what values of k and n were used  our simple cluster scoring function didn't outperform the baseline in average precision  ap . on the other hand  interestingly  our method exhibited potential for the improvement in p 1. as most users browse only a few of the top-ranked documents when they search the web  the improvement in p 1 may be useful and meaningful. 
1 experimental results and discussion 
table 1 shows the results of three runs we submitted in this task. there are three evaluation criteria  i.e.  average precision  p 1  and binary preference  bpref . ntuah1 is the run using bm1 on full documents. the run ntuah1 used bm1 to retrieve passages  which are dynamically determined  so it is time-consuming. ntuah1 is the result of our method. it is obvious that our simple two-stage approach is inferior to the baseline  ntuah1  and passage retrieval a little outperforms full document retrieval  whatever the evaluation criterion is. there are several reasons for the out-of-expected performance of our two-stage method. the first is that our simple cluster scoring function deeply depends on the performance of the first stage  i.e.  the original ranked list retrieved by okapi. when the performance of the first stage is not good enough  our cluster scoring function usually performs worse. the second reason is that the number of truly relevant documents has effects on ranking clusters. in other words  if there are fewer relevant documents in the corpus  it is more difficult to rank clusters. for the 1 topics in this task  the median of the numbers of relevant documents is 1. comparing with ntuah1  our method performs better in 1 topics. in the remaining 1 topics  1 of them have relevant documents fewer than 1. this result reflects the second reason mentioned above. the third reason is the relevant probability  refers to table 1  estimated by last year's topics is much different from the result of this year's. table 1 shows the performance of okapi on this year's topics. in table 1  it's presented that p r decreases whenever r increases. however  it's not the same condition in table 1. in table 1  p r does not decrease obviously between r=1 and r=1. the difference between table 1 and table 1 directly influences the result of our cluster scoring function. 
table 1. our results of ad hoc task 
run-id ap p 1 bpref ntuah1  okapi-doc 1 1 1 ntuah1  okapi-psg  1 1 1 ntuah1 okapi-d+clst 1 1 1 table 1. the performance of okapi on trec1 topics 
r 1 1 1 1 1 1 1 1 1 p r 1 1 111111 1 
1 	named page finding task 
1 motivation and description of our method 
named page finding task is much different from traditional ad hoc task in the aspect of the number of  correct answers . the goal of named page finding is to find a specific page or its  near duplicates  with near rank one  so using only content of the document to identify the relevance between the document and the query is obviously not sufficient for this task. in the environment with terabyte-scale corpus and without support of extremely expensive hardware  we tried to utilize titles and anchor texts of out-links of documents to improve the search result which is retrieved based on only document contents  since document title and anchor text are commonly considered as informative . besides the original index of document contents  two additional indices are produced for the titles and the anchor text of out-links  respectively. our method is described as follows:  
¡¡¡¡for each query  we perform bm1 retrieval on all the three indices  i.e.  content  title  and anchor text  and merge the three ranked lists. the results are merged by a linear combination of scores  which have been normalized  of the documents  hence the relevant score value  rsv  of document di is 
	rsv di £½cc£®sc di £«ct£®st di £«cat£®sat di  	 	 1  
where sc di   st di  and sat di  represent the score of di in the list retrieved on index of content  title  and anchor text  respectively. cc  ct  and cat are their weights and cc£«ct£«cat£½1. 
1 experimental results and discussion 
table 1 shows our results of three runs. ntunf1 is the baseline  i.e.  document retrieval using bm1. ntunf1 is the result of passage retrieval using bm1. ntunf1 is our method that utilizes the information of document title and anchor text of out-links. passage retrieval did not clearly outperform document retrieval. that indicates the information of document content was insufficient to deal with named page finding. our method got slightly improvement in the percentage of named pages retrieved at top 1  but results of the three runs exhibited no significant difference in mrr. however  it didn't mean that document titles and anchor text had no influences on the performance. by comparing the results of individual topics in ntunf1 and in ntunf1  we found that many topics got much different performance in the two runs. for example  the named page of topic 1 was ranked at 1 in ntunf1 but was ranked at 1 in ntunf1. this example indicated that document title and anchor text were intuitively informative but also very noisy  especially for this task. how to utilize the information reliably and robustly is necessary for future work. 
table 1. our results of named page finding task 
run-id arr %top1 %fail ntunf1  doc  1 1 1 ntunf1  d£«t£«at 1 1 1 ntunf1  psg  1 1 1      figure 1 shows the performance difference of rr  reciprocal of rank  between ntunf1 and ntunf1 for each topic. the x-axis stands for topic id and the y-axis stands for the performance difference  i.e. for the performance of each topic  its rr value in ntunf1 subtracting that in ntunf1. for the points in figure 1  if the y-coordinate is smaller than 1  it means our method performs better than baseline for that topic  otherwise it means that our method performs worse for that topic. it's showed that our method improves the baseline for many topics but there are also some topics got worse result at the same time. 

figure 1. the difference of rr values between ntunf1 and ntunf1 
