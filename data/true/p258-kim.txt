information retrieval using word senses is emerging as a good research challenge on semantic information retrieval. in this paper  we propose a new method using word senses in information retrieval: root sense tagging method. this method assigns coarse-grained word senses defined in wordnet to query terms and document terms by unsupervised way using co-occurrence information constructed automatically. our sense tagger is crude  but performs consistent disambiguation by considering only the single most informative word as evidence to disambiguate the target word. we also allow multiple-sense assignment to alleviate the problem caused by incorrect disambiguation.
　experimental results on a large-scale trec collection show that our approach to improve retrieval effectiveness is successful  while most of the previous work failed to improve performances even on small text collection. our method also shows promising results when is combined with pseudo relevance feedback and state-of-the-art retrieval function such as bm1.
categories and subject descriptors
h.1  information storage and retrieval : content analysis and indexing-linguistics processing; h.1  information storage and retrieval : information search and retrieval-retrieval models  search process
general terms
algorithms  performance  experimentation
keywords
wordnet  word sense disambiguation  information retrieval  performance evaluation
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
1. introduction
　many researchers have tried to improve retrieval performance by considering word sense 1  1  1  1  1  1  1 . since natural language has its lexical ambiguity  it is predictable that text retrieval systems benefits from resolving ambiguities from all words in a given collection.
　nevertheless  it is not generally accepted that word sense disambiguation makes a meaningful contribution to the information retrieval task since most of the previous ir experiments using word senses have shown disappointing results. sanderson compares and summarizes many of the previous works in   and points out some reasons for their failures  such as too skewed sense frequencies  collocation problem  inaccurate sense disambiguation  etc. word sense disambiguation for ir tasks should be performed on all ambiguous words in a collection since we cannot know user query in advance. however  computational linguists recently report that performance of word sense disambiguation reach at most about 1% precision and recall on all word task in senseval1 competition  although about 1% in the lexical sample task  which deals with only a few words. thus  it is not strange that a retrieval system adopting word sense disambiguation often drops the performance by inaccurate sense disambiguation1.
　this study is motivated by some observations that sense disambiguation for crude tasks such as ir is different from traditional word sense disambiguation. first  it is arguable that fine-grained word sense disambiguation  which considers all the senses defined in a dictionary or a thesaurus  is necessary to improve retrieval performance. for example  the word  stock  has 1 different senses in the wordnet. given that even a human can not determine the correct sense of the word in different contexts  we think that automatic disambiguation for all the fine-grained word senses certainly cause too many disambiguation errors resulting in failure of appropriate query-document matching. for this reason  coarse-grained disambiguation with broader senses may be preferable to fine-grained disambiguation for ir tasks.
　it is also doubtful that highly accurate sense disambiguation is the only solution to improve ir performance. for example  disambiguation errors in a given query term would not deteriorate document-query matching if exactly the same

1 http://www.itri.brighton.ac.uk/events/senseval/
1
 sanderson empirically shows that 1%-1% error rate could cause effectiveness to be as bad or even worse than when ambiguity was left unresolved although the rate varied across the collections
errors also occurred in the text collection. from this point of view  we believe that consistent disambiguation in documents and queries according to its neighboring words is more important than traditional accurate disambiguation. in addition  if we want to utilize the disambiguation results as safely as possible  flexible disambiguation  which assigns several possible senses to a given word  is better than strict disambiguation assigning only the most probable sense.
　in this paper  we propose a root sense tagging approach for information retrieval. root sense means one of the 1 unique beginner senses defined in wordnet hierarchies for the noun synsets such as person  act  or artifact. in our approach  each noun word in a document and a query is classified into one of the candidate root senses by considering its neighboring content words. thus  the proposed root sense tagging approach is based on coarse-grained disambiguation unlike the most of the previous work. when classifying a given word into one of the root senses  we select only one context word having the highest mutual information with the given word  and find the most probable candidate root sense. this enables us to perform consistent disambiguation  especially for many collocations or multi-word expressions. then  all the root senses assigned to each occurrence of a term in a document are merged into a final sense unit for the index term. using this multiple-sense assignment  i.e.  flexible disambiguation  our approach can alleviate the problem caused by disambiguation errors. we implement it using bitwise sense field for setting one or more sense bits  and and/or operations for merging or matching the sense field  which requires only a small amount of additional system overhead.
1. related works
　the most efforts on information retrieval using word senses are well discussed in . in this section  we review two successful previous works closely related to our work.
 is the most successful work reporting 1% improvement in retrieval effectiveness using the trec-1 category b collection. they build a word co-occurrence matrix and transformed it using svd in order to make context clusters. then  some of the context clusters are selected simply by finding close sense clusters using vector similarity  and assigned them to each word occurrence in query and document. this approach is a good example of coarse-grained disambiguation and flexible disambiguation. they did not use a set of fine-grained word senses that are pre-defined in an existing dictionary or a thesaurus like wordnet. they also assign a number of clusters to the word occurrences for flexible query-document matching.
　despite the good experimental results  some problems exist in their approach as pointed out in . their sense-based representation using automatically constructed global matrix is very similar to the latent semantic indexing  lsi . lsi 1  1  is known as an alternative approach to overcome the problems caused by bag-of-word representation. however  its heavy computational cost usually makes lsi to be an unrealistic solution  especially in the case of applications dealing with large-scale text collection. its computational cost in the retrieval on inverted index is another serious problem. their small-scale evaluation  surely due to its computational cost  and very long queries that considerably help the disambiguation are also problematic.
 is a recent successful work with short queries and table 1: list of 1 wordnet unique beginners
act	animalartifactattribute	bodycognitioncommunication eventfeelingfoodgrouplocationmotiveobjectpersonphenomenanplantpossessionprocessquantityrelationshape timestatesubstancelarge-scale trec wt1g data collection.  empirically showed that their wsd system can significantly improve the retrieval performance. in our view  this work is categorized into fine-grained disambiguation and strict disambiguation since they use all the fine-grained senses in wordnet as well as assign a single sense to each term. their disambiguator applies collocations  co-occurrence statistics  and prior sense frequency in a stepwise fashion. they represent documents and queries with sense vectors  and retrieve relevant documents using the traditional tf ，idf term weighting method. however  their supervised learning method using sense-tagged semcor corpus appears to be a problem from a practical point of view. it is also problematic that absolute performances of the baseline and the proposed system were too low to investigate the effect of sense-based text retrieval. however  we think that the most troublesome is their strict and fine-grained disambiguation  which necessarily results in many disambiguation errors.
1. root sense tagging approach
　our proposed approach aims to improve the performance of large-scale text retrieval by conducting coarse-grained  consistent  and flexible word sense disambiguation. we use 1 root senses for the nouns in wordnet 1 shown in table 1. in wordnet  all the noun synsets are organized into hierarchies  and each synset is part of at least one hierarchy  headed by one of the 1 root senses. for example  there are six different senses for  story  defined in wordnet  and five of them -  message    fiction    history    report    fib  are from the same root sense of relation  and the sense of  floor  is from the root sense of artifact. our root sense tagger classifies each noun in the documents and queries into one of the 1 root senses  which we call it coarse-grained disambiguation.
　when classifying a given ambiguous word into one of the root senses  we first select the most informative neighboring clue word having the highest mutual information with the given word. then  the single most probable sense among the candidate root senses for the given word is chosen according to the mutual information between the selected neighboring clue word and each candidate root sense for the given word. as a result  the tagger always assigns the same root sense to the word when the word occurs with its frequently co-occurring word  i.e.  consistent disambiguation. the required word-word mi and word-sense mi are calculated based on the automatically constructed co-occurrence data.
　for indexing and retrieval  if several different root senses are assigned to each occurrence of a specific word  they are


figure 1: root sense tagging approach for ir : system overview1 ftp://ftp.cogsci.princeton.edu/pub/wordnet/ merged in the document in order to do more flexible sensebased matching between query and document. to perform the flexible matching  bitwise sense field and operations are used  which require only small amount of system overhead. overall system architecture is shown in figure 1.
1 co-occurrence data construction
　we regard all nouns and compound nouns having a unique root sense as non-ambiguous units. the list of these nonambiguous units is the fundamental source to construct cooccurrence data between root senses and words. for example  the word  actor  has two senses including  role player  and  doer   but both senses are from the common root sense person. in this case   actor  is regarded as the nonambiguous unit for the root sense person. likewise  the compound noun  computer system  has only one sense from the root sense artifact  and this compound noun becomes a non-ambiguous unit.
　there are 1 non-ambiguous units in wordnet 1  and it is enough to build co-occurrence data between all 1 root senses and their neighboring words. our root sense tagger is free from the requirement for a huge amount of data by considering only 1 root senses  while traditional fine-grained sense disambiguators usually face the difficulty of data sparseness to deal with hundreds or thousands of pre-defined word senses.
　given raw text collection and a list of non-ambiguous units  co-occurrence information has been extracted from each document by the following steps:
1. assign a root sense to each non-ambiguous noun in thedocument.
1. assign a root sense to each second noun of non-ambiguouscompound nouns in the document.
1. even if any noun tagged in step 1 occurs alone in otherposition  assign the same root sense in step 1.
1. for each sense-assigned noun in the document  extractall  context word  sense  pairs within a predefined window.
1. extract all  word  word  pairs.
　for instance  if  computer system  - a non-ambiguous compound noun having the unique root sense  artifact  occurs in a given document  the second noun  system  is labeled as  artifact  in step 1  and all the occurrences of  system   even not followed by  computer   in the document table 1:	top 1 mi-valued root senses for word
 build    famous   and  last 
 build mi famous mi last miartifact1person1time1body1object1person1group1state1act1::::::are also labeled as artifact. it is based on one-sense-percollocation and one-sense-per-discourse assumption in . the context word in step 1 indicates k nearest content words on the left side and those on the right side of the senseassigned noun. in our experiments  only nouns  verbs  and adjectives are considered to be content words  and a window parameter k is arbitrarily set to 1.  word word  pairs in step 1 are also extracted within the same windows.
　all extracted  context word sense  and  word word  pairs from the collection are compiled into global co-occurrence data. using this data  we can assign all the remaining ambiguous units in the indexing phase. table 1 shows the example of mutual information between three example words and their most frequently co-occurring top three root senses.
1 mi-based root sense tagging
　when we preprocess the documents for indexing  all nonambiguous unit nouns are sense tagged with their root senses again in the same way as described in the section 1. in this section  we describe the mi-based root sense tagging method  which automatically assigns root senses to ambiguous words using global co-occurrence data constructed by the steps described in the previous section.
　our method is a very simplified version of existing word sense disambiguation methods:
  first  select the most related context word c w  to ambiguous word w among the context words by mutual information as follows:
	c w  = argmaxci（cw w mi cw ci 	 1 
  second  find the highest mi-valued candidate root sense s w  with the selected c w  in the first step as follows: s w  = argmaxsi（cs w mi c w  si   1 
table 1: examples of disambiguation for  system  and  interest 
 interest ... and bay has been designated site of special scientific interest ...  cognition 

... nations with very different interests could never reach consensus ...  cognition 
... many hours of discussion between various interests represented on it ...  cognition 

... government  which has not paid interest on its bank debt since ...  possession 

... interest margin is 1 basis points over libor ...  possession 

... related to changes in poll tax and interest rates ...  possession  system ... makes other automation control components and systems  such as vision recognition ...  artifact 
... tcs stands for traction-control system ...  artifact 
... if study's plan for opening up auction system is adopted ...  body 

... enable poland to maintain the open import system that we now enjoy ...  body 

... system is changing slowly but be suspicious ...  cognition 

... it made sense to change system slightly so that ...  cognition 　where cw w  indicates the set of the context words for w defined in the previous section  and cs w  indicates the set of the candidate root senses of w defined in wordnet.
　sometimes  there is no co-occurrence data for calculating mutual information to find c w  or s w . in these cases  we set s w  to null. in addition  we set s w  to unk for unknown words not defined in wordnet. we consider only the single most related word as the contextual evidence in order to perform consistent disambiguation especially for collocations and multi-word expressions.
　for example   interest rate  is a compound noun having the unique root sense possession  so the second word  rate  is classified into possession and the first noun  interest  should be automatically classified using co-occurrence information. in this case   rate  is selected as c interest  in most cases since  interest  and  rate  occur with each other very frequently. once  rate  is selected for c interest   the root sense possession is undoubtedly selected as s interest  because the word  rate  has the highest mi value with the root sense possession among the candidate root senses of  interest . the same result is obtained in the case of  ..rate of interest..  or even in contexts such as  ..interest in low rate accounts.. .
　table 1 shows examples of disambiguation for  interest  and  system . the examples for  interest  shows that adjective  special  or  different  usually co-occur with the nouns belonging to cognition among the possible candidate senses of  interest   while  pay  or  margin  co-occur with the possession nouns. the examples for  system  in this table show another characteristic of our disambiguation approach. the word  system  is one of the vague words to clearly disambiguate the meaning in the context. in wordnet   system  has 1 different fine-grained senses  and 1 different root senses: artifact  cognition  body  substance and attribute. one may think that disambiguation of  system  would be useless even to human in understanding the meaning of text. our root sense tagger also gives somewhat strange results as shown in this table. however  we can expect the phrasal indexing effect or proximity-based ranking by specializing the common word  system  according to whether it is co-occurred with  control    open   or  change . if  system  occurs with  control  in a query  the documents containing  system  near  control  have higher relevance scores because  system  in both the query and the documents are probably classified into artifact.
　if we consider more neighboring words in the disambiguation phase  more accurate disambiguation may be possible. however  we may also encounter a bad situation that different root senses for a word are assigned in the query and document even if they occur with the same word. for example  the word  system  co-occurring with  control  in a document is tagged as artifact  but body may be tagged to  system  immediately followed by  control   resulting in a sense-mismatch between the query and the document. this case is less desirable than the situation where the same incorrect root sense is assigned to both terms in a query and a document  which at least results in a sense-match. this is why we decide to perform consistent disambiguation in a primitive way using only the single context word  even though we sacrifice the possibility of performing more accurate disambiguation.
1 indexing and retrieval
　our indexing and retrieval strategy is basically based on bitwise sense field and operations. we add additional 1bit sense field to each term posting element in index. assigned root senses for each unique term are encoded by setting proper sense bit among the 1 bits in the sense field. the remaining 1 bit is used for unk assigned to unknown words. if s w  is set to null or w is not a noun  all the bits in the sense field are set to 1.
　unlike to the traditional bag-of-word inverted indexing approach  we must consider the following two situations caused by employing sense field. first  several different root senses may be assigned to the same word within a document according to their different contexts in the document. second  our sense tagger assigns root senses to only nouns  but a verb with the same indexing keyword form may exist in the document. for example  our root sense tagger may assign two different root senses to the noun  certification  occurring twice in a document  but does not assign any sense to the verb  certify . in this case  we simply merge all the sense fields by the bitwise-or operation  and take the merged sense field for its final sense field as shown in figure 1. by allowing multiple-sense assignment  we can avoid the problem caused by inaccurate strict disambiguation and exponential increase of the index size1.

1
 if a traditional bag-of-words indexing system defines two data fields  i.e. term identifier and within-frequency  for

figure 1: example of sense merging
　many previous approaches 1  1  1  require both a sensebased index and a term-based index  retrieve documents based on each index  and finally combine the results. this approach has several disadvantages including serious computational complexity and ad hoc heuristics of combining the results. moreover  if we employ their sense-based matching with our coarse-grained sense tagging  our system will return a huge number of the documents. for example  our system will retrieve most of the documents in a collection for the user query  car  because artifact is tagged to the  car   and the most of the documents has something tagged by artifact.
　for this reason  we propose a sense-oriented term weighting method to rank documents considering word senses. our sense-oriented term weighting just maintains the traditional term-based index  and artificially transforms term weight using sense weight sw calculated by refering to the sense field of each term posting in the retrieval phase. sense weight swij for term ti in document dj is defined as follows:
	swij = 1 + α ， q dsfij qsfi 	 1 
where dsfij and qsfi indicate the sense field of term ti in document dj and query respectively. here  α is a parameter controlling the impact of sense-matching result by qfunction. sense-matching function q is defined as follows:
	1	if  dsfij=1  or  qsfi=1 
		 1 
　sense-matching function q returns 1 if a root sense is not assigned to a term either in a query or document. otherwise  the function returns a positive sign or a negative sign. it returns either a positive sign if there is any common bit setting as 1 or a negative sign if there is no common bit setting as 1.
　for the ranking  this sense weight is multiplied by the original term weight computed in the traditional way such as tf ，idf. in other words  we boost the original term weight if the sense of a word in a query is contained in the sense field of the word in a document. otherwise  we take the original weight itself  or cut it down when the sense of the word in a query is not contained in the sense field of the word in the document.

each posting element  our strategy requires only about 1% increase in the original index size.
1. experimental results
1 data and evaluation methodologies
　in this section  we have carried out a large-scale evaluation of the proposed root sense tagging approach for information retrieval. all the experiments were conducted using two different document collections and two different query sets. for the document collections  1 documents of financial times collection in trec cd vol.1 and 1 documents of la times collection in vol.1 were used. for the query set  we used trec 1-1  and trec 1-
1  queries.
　one way to examine the usefulness of our root sense tagging approach is to check the boolean search performances  that is  to retrieve only the documents containing the query term whose sense is also same as the one assigned in the query. however  since traditional weighting methods using tf and idf are much more popular than boolean retrieval  the performance evaluation using simple boolean retrieval may be unnatural. moreover  the ranking performance can not be measured by the boolean search evaluation. for this reason  we rank documents using tf and idf  and evaluate our root sense tagging approach by multiplying the sense weight sw defined in eq. 1  to the baseline term weight.
　we use the following three baseline term weighting methods:
  w1 : simple idf weighting
  w1 : tf ， idf weighting
  w1 :  1 + log tf   ， idf weighting
　although there are several excellent term weighting heuristics such as length normalization or smoothing term frequencies  we did not use those methods to clearly investigate the behavior of sense-oriented term weighting. exceptionally  we have tested logarithm heuristics for term frequency in w1  which is well-known heuristics to improve ir performance  to check whether our proposed sense-oriented term weight is still valid with the heuristics. for the evaluation measures  we used two well-known measures including 1 points non-interpolated average precision and precision at 1 documents  p 1 .
1 text retrieval with root sense tagging
　table 1 shows the 1 points non-interpolated average precisions of the three baseline retrieval methods and their sense-oriented term weighting versions +sense . in this table   ft  and  la  represent the collection names   1  and  1  mean the query set  and  t  and  d  indicate the title  short  and description  long  queries in those query sets. this table shows that our root sense tagging approach consistently contributes to the retrieval performances on both the different collections and the different query sets. in the idf-only weighting  w1  experiment  we achieve 1% point and 1% point improvements in average for the short and long query experiments. sense weight parameter α in eq. 1  is set to arbitrary value 1. it is obvious that the performances might worsen in the long query experiment if there are many inaccurate or inconsistent root sense assignments. however  we can obtain more improvements in the long queries experiments. therefore  we can claim that
the mi-based root sense tagging successfully specialize the terms using root senses.
table 1: performances measured by non-interpolated 1 points average precisions
expw1w1+sense w1w1w1+sense w1w1w1+sense w1ft1t111%111%111%ft1t111%111%111%la1t111%111%111%la1t111%111%11-1%avg111%111%111%ft1d111%111%111%ft1d111%111%111%la1d111%111%111%la1d111%111%111%avg111%111%111%table 1: performances measured by precisions at 1 documents
expw1w1+sense w1w1w1+sense w1w1w1+sense w1ft1t111%111%11-1%ft1t111%111%11-1%la1t111%111%111%la1t111%11-1%11-1%avg111%111%11-1%ft1d111%111%11-1%ft1d111%111%111%la1d111%111%111%la1d111%111%111%avg111%111%111%　our approach is also successful in w1 and w1 experiments  but the degree of improvements were less compared to w1  especially in the w1 experiment. in the w1 experiment  the root sense tagging even deteriorates the retrieval performance on la1t. since the result of w1 experiment on the la1t shows a good performance  it seems that the tf factor used in w1 and w1 caused some problems with our weight transforming method. we have found that w1+sense and w1+sense sometimes considerably raises or drops the term weights for the highly frequent terms in a document. when the sense fields for a highly frequent term in a query and in a document matches  i.e. sense-matching function q returns 1   sense weight 1 is multiplied to the original term weight  and there are more increases in the original term weights compared to the low frequency terms. moreover the highly frequent terms have more possibilities to match with the senses of the query terms because of multiple-sense assignment within a document. this explains why the improvements in w1+sense and w1+sense are rather smaller than those for w1+sense. we think that the proper length normalization technique as well as term frequency modeling considering word senses should be developed for a more successful sense-based ir system.
　table 1 shows the precisions at 1 documents p 1 . in these results  we observe that there is a 1% improvement for short queries and 1% improvement for the long queries in w1 experiments  which are more improvements than those obtained by 1-point average precision measures. w1+sense certainly contributes to pulling up more relevant documents to the top ranks. in addition  w1+sense is more effective in long queries than in short queries  similar to the result in table 1. it is easily imagined that more consistent sense tagging in a query is possible in long queries containing more context words for disambiguating each word. however  there is less improvement in w1 and w1 expertable 1: the number of improved and deteriorated queries by  w1+sense 
impr.detr.samesumft1t11ft1t11la1t11la1t11ft1d11ft1d11la1d11la1d11iments  and w1+sense on title queries drops the baseline performances without the sense-based ranking method from 1 to 1. we think that this is due to the same overgrown weight problem.
　table 1 shows the number of improved and deteriorated queries by w1+sense in average precisions. in all the experiments  more than half of the queries benefits from our root sense tagging method  although a number of queries become deteriorated. since this is mainly due to inaccurate sense tagging results  more elaborate root sense tagging method must be developed for those queries.
1 pseudo relevance feedback
　a pseudo relevance feedback itself is a kind of method for semantic information retrieval since it aims at retrieving documents not directly containing query terms  but surely relevant to a given user query. we have conducted the
table 1: performances with pseudo relevance feedback  adding 1 terms from the top 1 docs 
expw1w1+sense w1w1w1+sense w1w1w1+sense w1ft1t111%111%111%ft1t111%111%11-1%la1t11-1%111%111%la1t111%11-1%111%avg111%111%111%ft1d111%111%111%ft1d111%11-1%111%la1d111%111%111%la1d111%111%111%avg111%111%111%pseudo relevance feedback experiments to investigate whether our root sense tagging approach can be used with relevance feedback. in this experiment  we have selected five terms from the top ten documents by the probabilistic term selection method suggested in   and added them to the original query. for the sense fields of the new query terms in +sense experiments  we used a voting method that is the most frequent root sense in the top 1 documents is assigned to the terms to add.
　table 1 shows the 1-point average precision performances. as expected  further improvements are achieved by pseudo relevance feedback. since w1+sense showed better precisions at top 1 documents in the previous experiments  it is obvious that the top 1 documents retrieved by a w1+sense initial search are more favorable sources to extract feedback queries than the documents retrieved by a w1 initial search. one interesting result is that w1+sense also achieves good performance improvements even though their initial search performances measured by p 1 are poor. we surmise that good feedback terms are selected due to the high initial search performance of w1+sense  1   and the high quality of the set of added new query terms with their senses affects the performance.
　from the performance improvements by relevance feedback with our root sense tagging approach  we can claim that even our crude sense assignment to the feedback query terms as well as sense-oriented term weighting are effective with pseudo relevance feedback since the initial search can locate more relevant documents near the top of the ranked list. however further experiments and analysis are needed.
1 bm1 using root senses
　figure 1 shows the experimental results with okapi bm1 ranking function considering word senses. we conducted this experiment to verify whether our proposed approach is appropriate for the existing state-of-the-art retrieval model. our sense weight swij is multiplied to the weight calculated by the bm1 formula. in this experiment  we evaluate the initial and feedback performances according to sense weight parameter α  unlike to the previous experiments where α is 1. the two collections are merged into one huge collection containing 1 documents  and we experimented on the collection. needless to say  the baseline performance without sense information is the one where α is 1.
　with this figure  we can observe that the initial search result is stable  while feedback performances with long query experiments fluctuate. we can achieve the best performance in all initial searches when α is approximately 1. there must be an appropriate α in the initial search  but not in the case of relevance feedback.
　we think that the feedback performance is mainly affected by the quality of the set of new keywords used in feedback  not the quality of sense tagging. among the selected feedback keywords  a few good keywords and their senses certainly contribute to retrieving more relevant documents. if there are  however  a number of bad keywords containing their senses  the performance significantly deteriorates by the feedback terms with the growth of the sense weight parameter. this is because the feedback performances fluctuate according to α.
　although some inconsistencies result from bm1 with the relevance feedback experiments  it is somewhat surprising that even our crude term weight transformation heuristics improve the bm1 performances in initial searches. we are confident that we can obtain better results by adopting our root sense tagging approach to any state-of-the-art retrieval model including bm1 if we develop more novel term weighting function considering word senses.
1. conclusions and future works
　in this paper  we have proposed a coarse-grained  consistent  and flexible sense tagging method to improve largescale text retrieval performance. for coarse-grained disambiguation  we have used only 1 unique beginner senses in wordnet instead of utilizing a large number of fine-grained senses. thus  our approach can be applied to retrieval systems in other languages in cases where there are lexical resources much more roughly constructed than expensive resources like wordnet. our sense tagger can be built without a sense-tagged corpus  and performs consistent disambiguation by considering only the single most informative neighboring word as evidence of determining the sense of target word. multiple-sense assignment has been allowed so that the system can make the risks from disambiguation errors as small as possible. although we added additional sense information to the retrieval system  the proposed sense-field based indexing and sense-weight oriented ranking do not seriously increase system overhead.
　while many previous works on information retrieval using word senses often failed to improve retrieval effectiveness even in small text collections  our large-scale experiments on the trec collection gave us promising results. more specifically  the idf-only term weighting experiment excluding the effect of term frequencies showed that our root sense tagger can find relevant documents more accurately. other experiments with term frequencies also show good performance even with the relevance feedback method or state-of-theart bm1 retrieval model  but we feel keenly the necessity for the elaborate term weighting method considering word senses.

figure 1: initial and feedback performances of bm1+sense according to sense weight parameter αfor future work  we will focus on the following two problems that we have encountered through the experiments. first  we realize that verbs also should be assigned with senses for further improvement because the words used in noun form within a query are often used in verb form in the relevant documents. since the verbs are not sense-tagged in our work  our system often fails to match their senses. second  it is essential to develop an elaborate retrieval model  i.e.  a term weighting model considering word senses. such a model may be an entirely new one or developed based on existing term-based retrieval models.
