we are witnessing an explosive increase in the complexity of the information systems we rely upon. autonomic systems address this challenge by continuously configuring and tuning themselves. recently  a number of autonomic features have been incorporated into commercial rdbms; tools for recommending database configurations  i.e.  indexes  materialized views  partitions  for a given workload are prominent examples of this promising trend.
　in this paper  we introduce a flexible characterization of the performance goals of configuration recommenders and develop an experimental evaluation approach to benchmark the effectiveness of these autonomic tools. we focus on exploratory queries and present extensive experimental results using both real and synthetic data that demonstrate the validity of the approach introduced. our results identify a specific index configuration based on single-column indexes as a very useful baseline for comparisons in the exploratory setting. furthermore  the experimental results demonstrate the unfulfilled potential for achieving improvements of several orders of magnitude.
1. introduction
　the area of autonomic computing has received considerable attention in the recent years  particularly in industry  and aims at providing systems that can adjust themselves to a changing environment. the vision of autonomic computing is to eliminate the need for human intervention in tuning the systems  and is motivated by:  1  increasing system usability  by allowing non-expert users to achieve acceptable performance ;  1  decreasing operational costs  by reducing the demands on system administrators ;  1  deploying systems in scenarios where human intervention is impossible

 work done while the author was a phd student at the university of toronto.
 work done while the author was a postdoctoral fellow at the university of toronto.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  baltimore  maryland  usa copyright 1 acm 1-1/1 ...$1.
or undesirable  e.g.  in pervasive and embedded computing environments .
　recently  a number of autonomic features have been incorporated into commercial rdbmss as well. in particular  tools that recommend indexes for a given workload  such as  1  1   are crucial first steps toward autonomic data management. agrawal et al.  discuss the recommendation of materialized views as well as indexes  possibly defined over the recommended views  given a query workload. most of the work on autonomic databases has centered around using the dbms's own query optimizer for comparing hypothetical scenarios . the input to the process is typically a query workload and a budget  often in the form of a bound on the disk space that can be used for additional indexes . currently  most major dbms vendors support tools that behave in this way  1  1  1  1  1 .
　the potential performance impact of an effective index configuration can dwarf any other system parameter that a database administrator or an autonomic tool could possibly tune. this observation is not new: two decades ago boral and dewitt concluded that parallelism is no substitute for effective and efficient indexing. the experimental results in this paper demonstrate performance improvements of several orders of magnitude due to indexing only. therefore  the practical value of configuration recommender tools can be very significant.
　in this paper  we introduce a flexible characterization of the performance goals of configuration recommenders and develop an experimental evaluation approach to benchmark the effectiveness of these autonomic tools. the proposed benchmark1 provides a valuable assessment of the capabilities of the current crop of configuration recommenders. the benchmark also provides a foundation to evaluate future autonomic proposals. finally  the experiments demonstrate the need for additional research in the area: there is the potential for achieving improvements of several orders of magnitude compared to current tools.
1 exploratory queries on nref
　to motivate this work  we present a realistic scenario for autonomic data management tools in the context of supporting exploratory queries on the non-redundant reference protein database  nref  for short   published on the web by the protein information resource . nref provides a comprehensive collection of protein sequence data from several genome sequencing projects  pir-psd  swissprot 

figure 1: query execution times on system a using a configuration with primary keys only.
trembl  refseq  genpept  and pdb  and has identical sequences from the same source organism reported as a single nref entry. the database is updated biweekly; release 1 contains 1 1 entries and its xml representation has 1gb. once the xml data is converted to  raw  relational format  i.e.  csv text files  it occupies 1gb.
　the relational schema for the nref database is shown below  primary keys are underlined .
protein nref id  p name  last updated  sequence  length 
source nref id  p id  taxon id  accession  p name  source 

taxonomy nref id  taxon id  lineage  species name 

common name 
organism nref id  ordinal  taxon id  name 
neighboring seq nref id 1  ordinal  nref id 1  taxon id 1  length 1  score  overlap length  start 1  start 1  end 1  end 1 
identical seq nref id 1  ordinal  nref id 1  taxon id 
　the protein relation  1 million rows  contains a unique identifier for each of the amino acid sequences in the database. the source relation  1 million rows  contains the name of the database  e.g.  swissprot  where a given sequence is reported  and the corresponding access key for the protein on that database. all known taxonomic information about a given amino acid sequence is stored in the taxonomy and organism relations  1 and 1 million rows  respectively . finally  the neighboring seq relation  1 million rows  associates pairs of closely related sequences within the same organism  while the identical seq relation  1 million rows  contains pairs of identical sequences that occur in different organisms. neighbor nref sequences are identified based on scores obtained by performing all-against-all fasta searches.
　consider now that the biologist in our scenario is interested in executing hundreds of exploratory queries such as the one below:
　example 1. this sql query finds the number of protein sequences  nref ids  for each taxon associated with a virus that infects apes  and has been recently linked with cancer in humans  see http://www.cancer.gov/newscenter/sv1 .
select t.lineage  count distinct t1.nref id 
from source s  taxonomy t  taxonomy t1 where t.nref id = s.nref id
and t.lineage = t1.lineage
and s.p name = 'simian virus 1'
group by t.lineage

figure 1: query execution times on system a using a recommended configuration.
　further assume that  during her exploration of nref  the biologist has to execute 1 queries sampled from a much larger family of relevant queries  which we will call nref1j in the sequel. each of those queries executes in a certain amount of time. we visualize the response times experienced by the biologist while using a given configuration of nref on a given dbms by plotting the histogram of the query execution times.
　for instance  figure 1 shows the response times of the 1 queries executed on a commercial rdbms  which we call system a . the rdbms has a configuration of the nref database where the only indexes present are those automatically created for the primary keys of each relation. note that we define the bins using a logarithmic scale  resulting in execution times that are three orders of magnitude slower on the right of the graph than those on the left. also  we report all  timeout  queries on a single bin  labeled t out in the figure  using a reasonable timeout limit of 1 minutes for each individual query.
　contrast the histogram in figure 1 with the one in figure 1 showing response times observed on system a using a different configuration with several additional recommended indexes. not only there is a significantly smaller number of timeout queries in the recommended configuration  but also the proportion of queries that complete in about 1 minutes is much larger.
　the lines superimposed in both of the preceding figures are the cumulative frequencies of the query response times. throughout this paper we argue that these curves provide a concise and valuable way of comparing the behavior of a workload on different configurations of the same system. for instance  we read in figure 1 that 1% of the queries in the workload execute in less than 1 seconds on the recommended configuration. a similar reading in figure 1 shows that only 1% finish in 1 seconds or less in the original configuration.
　the bend on the cumulative frequency curve provides visual contrast between a satisfied  bend towards the top left corner  versus a frustrated  bend towards the bottom right corner  biologist-turned-database-user.
1	our contributions.
　the literature on configuration recommenders provides very limited experimental evaluation of the quality of the recommendations produced by the tools. a few results are reported that characterize how efficiently these tools arrive at some recommendation  such as results showing how quickly they can produce a heuristic solution to the combinatorial problem of index selection. however  to the best of our knowledge  there has been no evaluation of the effectiveness of configuration recommenders.
　in this paper  we describe an approach to evaluate the quality of the configurations suggested by configuration recommenders. we focus on the two aspects of the system configuration that can have the biggest impact: indexes and  possibly indexed  materialized views. we present extensive experimental results characterizing the effectiveness of commercial rdbms configuration recommenders when presented with a workload consisting of exploratory queries. our contributions are as follows:
  we provide a novel framework for assessing the effectiveness of configuration recommenders. in particular  our framework supports describing very large workloads and also provides a flexible characterization of performance goals using cumulative frequency curves.   we present extensive experimental results on stateof-the-art commercial rdbms configuration recommenders and show that there is substantial room for improvement.
  we identify a configuration that covers all single column indexes as a very useful baseline for comparing against recommendations. in fact  the consistently good performance of the single column configuration suggests a practical improvement for rdbms configuration recommenders: avoid missing the potential gains brought by single column indexes.
　the use of cumulative frequency curves to describe the response time behavior of database systems is common among practitioners and in the experimental systems literature. we do not claim originality in the measure itself; on the contrary  we endorse it because of its widespread acceptance. however  we are not aware of the use of cumulative frequency curves either as a goal for a recommender or as a measure of its effectiveness  despite the natural applicability of the concept. in fact  existing recommenders use a single numeric measure to describe workload behavior  something that we consider insufficiently rich to capture enough relevant aspects of the workload behavior .
　the single column indexing approach that we discuss here can be viewed as an extreme case of schema design by vertical partitioning. we note that there has been work on autonomic schema design tools that use vertical partitioning   but without considering the recommendation of indexes.
　the outline of the paper is as follows. we introduce a framework for evaluating autonomic indexing tools in section 1  followed by a description of the challenges and the approach we choose in designing a benchmark in section 1. we present our initial experimental results in section 1  followed by a more detailed analysis of the recommenders performance in section 1. finally  we conclude in section 1.
1. evaluation framework
　in this section we describe the framework used to evaluate the performance of a configuration recommender. we start by describing the task that a recommender performs as well as the factors in the rdbms environment that affect the recommendations. we then present some basic definitions and notation for characterizing costs and performance goals.
1 the recommendation task
　in broad terms  the basic task of a configuration recommender is to select a new configuration for the rdbms system that improves the performance that the system exhibits when executing a workload. alternatively  the recommender can be given a performance target and it should find a configuration where the target is reached. the selected recommendation can be applied by the recommender itself  or the user may be given the option to accept or reject the recommended change in configuration. to produce a recommendation  the recommender has to:  1  assess the cost of executing a workload in a given configuration;  1  assess the cost of changing the system configuration;  1  search among possible system configurations to find a better performing configuration  given some constraints  such as a budget for changing configurations .
　the most relevant aspect of the system configuration for an index recommender is  not surprisingly  the set of indexes available. however  a number of additional aspects can be considered part of the configurations being recommended such as data placement or the selection of materialized views  1  1  .
　there needs to be some definition of the performance goal that a recommender is trying to reach or improve upon. this goal can be a simple number  the execution time of a workload  or a more comprehensive  perhaps multidimensional  measure of overall system performance. the workload can also be defined in a variety of ways. the recommender may assume a known workload: the queries  and updates  together with their frequencies are given in advance. there may be a component in charge of automatically providing such a workload to the recommender based on observing the rdbms operation . alternatively  there may be a describable set of potential queries that are candidates for workloads.
　the rdbms environment that influences the recommender task includes the instance of the data  or a summary description of the database instance  such as selected statistics   the parameters selected for the rdbms engine as well as the engine itself  the supported query plans and the operators implemented   and all aspects of the physical data storage  including not just indexes  but also the layout of the data in the storage medium  replicas  materialized views  and so on .
1 costs and performance goals
　let us denote by ci the configuration  i.e.  set of indexes  materialized views  etc.  of a system in state i. there is a set of possible configurations cj1 cj1 ... cjn that the recommender can possibly select for the next system state j  and one actual selected recommendation cj = cjm for some m.
　consider qk （f  where f is the family of queries  or updates  that the system may execute. we denote by a qk ci  the actual cost  a measure  of the system executing query qk in configuration ci. similarly e qk ci  denotes the estimated cost of executing query qk in configuration ci. finally  at ci cj  is the cost of changing the system from configuration ci to configuration cj  while et ci cj  is the corresponding estimated transition cost from configura-

figure 1: behavior of system a on nref1j.
tion ci to cj.
　a workload is defined as a subset w  f of the potential family of queries that the system may execute. alternatively  it can also be defined as a bag  in which case the repetitions can model queries with a higher frequency or weight.
　given a workload w  we can measure the actual performance of the system on a configuration ci by a single quantity a w ci  = pqk（w a qk ci   total cost . similarly  the estimated performance is defined as e w ci  = pqk（w e qk ci   total estimated cost .
　finally  we denote by cfcj the cumulative  relative  frequency of the elapsed times a qk cj  for qk （w on configuration cj  defined as:
cfcj x  = count {qk : a qk cj    x} /size w 
　figures 1 and 1 show the cumulative frequency of the elapsed times of a 1-query workload for two database configurations  as discussed in section 1. contrasting cumulative frequencies of elapsed times on a given workload is an informative approach to compare different configurations; for instance  figure 1 compares three configurations  called p  1c and r  which will be explained later  on system a. the cumulative frequency polygons convey that configuration 1c is superior to both r and p. in particular  the dotted lines in the figure show that 1% of the queries in the workload executed on configuration 1c complete in less than 1 seconds  the value 1 in the x axis   instead of 1% on the r configuration and only 1% on the p configuration.
　while the use of cumulative frequency polygons has limitations1 they have advantages over histograms  requires no binning and quantiles can be read directly  and they are widely employed in decision making requiring comparing distributions  our use corresponds to deciding first order stochastic dominance .
a model for configuration recommenders. we can describe the behavior of the configuration recommenders incorporated in commercial rdbms  1  1  using the framework discussed above. the rdbms configuration recommender takes as input a given workload w  including the relative frequencies of the queries in the workload. the recommender's goal is to select a configuration cj that improves the total estimated cost of queries e w cj   where the total cost may use different weights for the queries in the workload. this optimization goal is subject to an estimated storage budget  hence et ci cj  uses storage as the measure . the configuration recommender uses the rdbms optimizer's estimation capabilities to asses e w cj . the optimizer has to hypothesize statistics for cj from the statistics in the current configuration ci. since there is a combinatorial space of possible index configurations  the rdbms configuration recommender relies on a heuristic search to compute estimates for a subset of the configurations. performancegoals. a performance goal for the configuration recommender can be stated as a simple target measure for the sum of the individual query execution measures over the queries in the workload. more specifically  if the measure is elapsed time  then we would have total elapsed time of executing the workload as the performance goal  for example  complete the workload in less than two hours . finergrained goals than total execution cost are usually more informative:  naive folks will use the average response time; more sophisticated specifiers will opt for the 1th or 1th percentile  .
　a performance goal can also be stated as an improvement ratio ir = a w ci /a w cj  where ci cj are the existing and selected configurations  respectively. continuing with the elapsed time example  a goal could be to obtain a 1 times improvement  by decreasing elapsed times in the recommended configuration by an order of magnitude .
　we note that performance goals can be a more elaborate than a single quantity. in fact  a performance goal can be viewed as a quality of service requirement that specifies minimum levels of performance that must be met by the system. again  making use of elapsed time as an example measure  consider the performance goal below.
　example 1. a performance goal for the execution of a set of queries can be to expect 1% of the queries to complete in less than 1 seconds  1% to complete in less than one minute  and 1% to complete before a 1 minute timeout. this goal can be described by a step function:
g x  = 1 x   1g x  = 1 1 ＋ x   1g x  = 1 1 ＋ x   1g x  = 1 x − 1where seconds are used as units and we use values in the  1  interval instead of percentages.
　a performance goal such as g above can be viewed as a constraint in the shape of the cumulative  relative  frequency  cfcj  of the elapsed times on configuration cj. a configuration cj satisfies the performance goal if cfcj   g. for instance  in figure 1  configuration 1c satisfies the goal g above  while the other two do not. note that any monotonic function g can be used as a performance goal in this setting.
1. benchmark design
　we start this section by discussing the challenges of evaluating the performance of configuration recommenders  and describe how we address those challenges in the benchmarks that we propose.
1 design criteria
　generic database benchmarks aim at providing an objective means of comparing competing systems executing the same task . usually  they define a database and a workload  a set of rules specifying the environment under which all tests must be run  e.g.  which kinds of indexes are allowed   and rules for performing the measurements and reporting the results. the results of the tests are typically summarized into single quantities such as the system throughput or its price/performance ratio. the benchmark is then run on several systems  and the results are compared  determining the system with best performance or cost/performance trade-off  etc.
　one of the fundamental challenges in designing a database benchmark that is accepted by the community is defining a workload that is representative of specific scenarios in real applications. another important goal in benchmark design is to succeed in evaluating the specific aspects of the database system that are of interest in  relative  isolation. later in the paper we spend considerable attention designing workloads. we are also careful to isolate the configuration recommenders behavior by reducing the number of factors that have an effect on the experimental evaluation and also by further limiting ourselves to just two aspects of the system configuration: indexes and  potentially indexed  materialized views.
　the goal of our work is not to compare configuration recommenders across competing database systems. instead  we focus on comparing the quality of the recommendations produced by one or more configuration recommenders running on the same database system. system configurations and the results achieved by a recommender suggesting one configuration over another are inherently system-specific. this makes a recommendation not directly comparable to another recommendation in a different system. the valid question of comparing a database system equipped with a configuration recommender to a competing database system should be addressed separately  and it is not an objective of this work .
　we focus now on criteria tailored to a benchmark for configuration recommenders. there are a number of challenges in designing experiments that can adequately evaluate the performance of these recommenders. first  we have to select a suitable database and define an initial configuration  or  more generally  a method for generating such instances .
　second  we need to provide a number of sufficiently large and varied workloads w1 ... wk that are representative of an application domain. both the heterogeneity of the queries in the workload and the number of different queries included has an impact on the level of difficulty of the recommender task. it is not unusual for large database systems to routinely deal with workloads of thousands or even tens of thousands of different queries. as we will present later on  workloads with one hundred queries are challenging enough for the current state of the art.
　third  we must define the input parameters for obtaining the recommended configurations cj1 cj1 ... cjk  such as the resources available . finally  we must provide a way of evaluating the quality of the recommendations. in this work  we propose comparing the cumulative frequency of the elapsed times for each workload evaluated in the recommended configuration against the elapsed times on one or more reference configurations ch1 ch1 ... chl.
　one of the few recommender evaluations in the literature is described by valentin et. al . the database and initial configuration are those defined in the tpc-d benchmark; the 1 queries in the benchmark are used as the single workload w  and an expert-tuned configuration ch is used as reference configuration. the results presented in that work show that the recommender suggested a configuration cj that performed as well as ch in 1 out of the 1 queries. this is a very encouraging result: the comparison configuration used is expected to perform extremely well and hence matching its performance is quite an accomplishment. however  the level of difficulty of the task is limited by the small number of queries in the workload.
1 our approach
　we address the criteria described above as follows. first  we use both real and synthetic databases  which are adequately scaled to the computing resources available in the desktop computing environment we utilize. the initial configurations are instances in which all primary key and foreign key constraints in the relational schema are defined  and where only primary key indexes are created; we refer to such initial configurations as p configurations. second  we define the workloads as query families  which are sets of queries that contain a large number of structurally related yet suitably diverse queries. while we focus on retrieval queries and do not consider updates in our workloads  we present experimental results quantifying the effect of inserts on the recommended and the reference configurations. third  the only restriction we impose on the recommended configurations is that they do not use more space for indexes than the reference configurations. finally  we identify a single reference configuration that has a single column index for each possible  indexable  column in the schema  and we refer to it as 1c  for 1-column index .
1.1 the databases
　three databases are used in our experiments: the nref database discussed in section 1  the tpc-h  database  and also a skewed version of the tpc-h   generated with a zipfian factor of 1. the sizes of these databases in raw format are 1gb for nref  and 1gb for each tpc-h database. while all the sizes are relatively modest they cannot be dismissed as ridiculously small. furthermore  the raw data size is an order of magnitude larger than the main memory of the computers utilized.
　as mentioned above  the initial configurations are instances in which all primary and foreign key constraints are defined. for the nref database  such constraints are as defined in the schema in section 1  while for the tpc-h databases we follow the schemas specified in that benchmark .
1.1 the query families
　due to the exploratory nature of our motivating scenario  we focus on queries that represent fragments of typical  iceberg  queries ; that is  queries that compute aggregate functions over a set of attributes to find aggregate values satisfying certain conditions  grouped in different ways.
　we follow an approach similar to the one described in   used for generating part of the query workload in the tpcds benchmark    in which query workloads are given as large families of queries described by templates.
　in each template  variables are used instead of relation names  column names  and constants; the actual queries are obtained by binding such variables to relations and columns in the schema  as well as constants selected from the database.
　the following criteria were used for designing the families we use in this work. first  the queries should have a meaningful interpretation. this is achieved by grouping columns in the schema by domains  and allowing joins on attributes in the same domain only. for example  referring to the nref schema in section 1  all attributes used for the scientific or common names of proteins  species and organisms are in the same broad domain and could be joined meaningfully. second  the queries should be simple enough for query optimizers to have a good chance of handling them well. thus  we use only simple select-project-join sql queries defining simple aggregate functions and with at most one level of nesting  and defining only equality predicates. third  the queries should not require the materialization of large intermediate results  as this could make irrelevant the presence of indexes in the database. to achieve this  we use additional selective predicates for each query. fourth  the queries in the family should cover a reasonable spectrum of query execution times  from fast  e.g.  sub-second response  to slow  e.g.  a timeout after a reasonable long execution time . finally  and perhaps most importantly  the queries should be amenable to improvement by the addition of both simple and complex indexes  defined either over base tables or materialized views. we achieve this goal in our work by defining join predicates only on indexable columns  as well as  wide  group by clauses in our queries.
　next  we give a brief description of each of the query families used in our experiments  followed by its sql template and an explanation of how we obtain the constants used for generating the actual queries. although we have experimented with several other families with a wide range of characteristics  the results presented for two families on the nref database and three families on two different tpc-h database provide a useful assessment of the performance of current recommenders.
family nref1j. the first family  on the nref database  is a generalization of the self-join pattern in the query described in example 1. we pick a table r  and a column c1 to define a self-join on r; then pick a another table s  and column c1  in the same domain as c1  and join r.c1 with s.c1. next  we choose up to three other columns ci1 ... ci1 in r and define a group by that includes c1 as well. finally  we add a selection condition of the form s.c1 = k  where k is a constant selected as follows. for each column in each table  we pick three values k1 k1 and k1 that can be used as the constant k such that k1 has the highest selectivity for the column and the frequencies of k1 and k1 are one and two orders of magnitude  resp.  greater than the frequency of k1. this is a template for the family:
select r1.ci1 ... r1.ci1 r1.c1 count distinct r1.c1 
from r r1  r r1  s s
where r1.c1 = r1.c1 and r1.c1 = s.c1
and s.c1 = k
group by r1.ci1 ... r1.ci1 r1.c1
familynref1j. queries in the second nref family count co-occurrences of values  from the same domain  in different tables. we pick tables r  s and a column from each table  c1 and c1  such that these columns are in the same domain; we then count the number of co-occurrences of values by joining r.c1 and s.c1. next  we pick up to three other columns ci1 ... ci1 in r to define a group by clause. finally  we further restrict the values of both r.c1 and s.c1 to be relatively infrequent  i.e.  occur less than 1 times  in order to limit the size of the intermediate join r  s. the template for this family is as follows:
select r.ci1 ... r.ci1 r.c1  count * 
from r r  s s
where r.c1 = s.c1
and r.c1 in
 select c1 from r group by c1
having count *    1 
and s.c1 in
 select c1 from s group by c1 having count *    1 
group by r.ci1 ... r.ci1 r.c1
family skth1j. queries in this family define three-way joins on the skewed tpc-h database  using a zipfian factor of 1 . each query is obtained as follows. we pick tables r  s and t; define a join r  s via primary key and foreign key correspondences; define a join s  t via a pair of nonkey columns s.c1 t.c1 from the same domain; and define a selection condition θ s.c1  on a column c1 of s to limit the number of tuples in r  s.
　in this family  θ s.c1  is one of s.c = p or s.c in  select c from s group by c having count * =p   and the parameter p is used to control the sizes of the intermediate result r  s. three θ s.c1  are used for each assignment of r s and t  each defining a constant for the variable p. the criteria for selecting these three constants are that the resulting query is not empty and that the sizes of the intermediate result r  s are in different orders of magnitude. that is  if k1 k1 and k1 are the sizes of r  s for each of the constants chosen  we have that k1 and k1 are one and two orders of magnitude  resp.  greater than k1. finally  each query returns a count *  where the group is defined by choosing up to 1 columns from relation t.
this is a template for the family:
select t.ci1 ... t.ci1 count * 
from r r  s s  t t
where r.cp1 = s.cf1 and ... and r.cpj = s.cfj
and s.c1 = t.c1 and θ s.c1 
group by t.ci1 ... t.ci1
family skth1js. this family  also defined for the tpc-h database generated with skewed data  is a simpler version of family skth1j in which r s and t are always one of lineitem  orders and partsupp. an additional simplification is that all the θ s.c  constraints are of the form s.c = p  where the constants are chosen as before. familyunth1j. the last family uses the standard version of a tpc-h database  where all values are sampled with uniform distributions  and its queries are the same as those in the family skth1j above  except that different selection constants are used .
1.1 reference and recommended configurations
　we conclude the description of the benchmarks with a discussion of the relevant parameters used for obtaining the reference and the recommended configurations from the various systems. as mentioned earlier  the p configuration we use as initial configuration contains only those indexes automatically created for the primary keys of each table. the 1c reference configuration is created by adding to p all possible single column indexes  i.e.  one index for each indexable column in the schema .
　we obtain one recommended configuration for each query family in the benchmark. we direct the systems to collect statistics before obtaining the recommendations and before running the queries. all the recommended configurations are obtained using the p configuration as the starting point  the difference in size between 1c and p as the space budget  and no limit on the time the recommender is allowed to run. the decision to define the space budget as above is motivated by the desire to make 1c as comparable as possible to the recommended configuration  the space used by 1c is the same space available for the recommendation . the space usage of 1c would be considered a high budget in many application scenarios and provides a generous amount of additional storage for the recommenders to work with. however  in our results  see table 1  described in the next section   no recommended configuration in our experiments used as much space as 1c. we also obtained recommendations with an unlimited storage budget. the resulting unlimited space recommendations increased storage usage  in all cases by a reasonable amount  and did exhibit better performance than the space constrained recommendations in some  but not in all  cases.
　we observe that the query families used in our tests resulted in fairly complex recommendations  involving single and multiple column indexes  as well as materialized views over joins of base tables  tables 1 and 1  described in the next section .
1. experimental results
　in this section we describe the setup used for the experiments and we present our results.
1 experimental setup
　we used two commercial rdbmss running on four pentium 1 desktop pcs ranging from a 1ghz machine with 1 mb of ram running windows 1 server; to a 1ghz machine with 1gb of ram running windows xp.
　two sets of experiments were run. the first experiment was run on the nref benchmark and was aimed at understanding the behavior of the systems tested  which we call systems a and b for this experiment  on a realistic scenario  using real data. the second experiment was run on both tpc-h benchmarks and was aimed at verifying our observations on a standard benchmark database  and to observe the impact of uniform versus skewed data on the behavior of the index recommenders. we selected one of the two systems for the second experiment  which we will refer to as system c.
　the results we discuss next are based on actual executions of the query families in each benchmark. in all cases  the queries are run sequentially  and the machine is fully dedicated to running the experiments. for obvious practical reasons  a timeout limit of 1 minutes is set for running
benchmarksystemsize  gb time  min nrefa nref p11a nref1j r11a nref 1c11b nref p11b nref1j r11b nref1j r11b nref 1c11skthc skth p11c skth1j r11c skth1js r11c skth 1c11unthc unth p11c unth1j r11c unth 1c11table 1: sizes and build times of all configurations used in the experiments.
each query; queries that do not finish in that amount of time are reported as  timeout . we perform two additional runs on the queries that do not timeout on a first run  and report the average time of the three measures  the small variances observed do not justify removing outliers .
1.1 query workloads
　the families presented in section 1 contain large numbers of queries. for instance  nref1j has 1 queries while nref1j has 1 queries. we adopt a number of practical restrictions to further reduce the space of possible queries to consider. for instance  only subsets of each relational schemas are used in the queries: all non-indexable columns were ignored and we did not use more than 1 columns per table. another restriction was to consider fewer selection criteria  thus  fewer queries  on the larger tables on each database; similarly  we used fewer columns in group by clauses on these tables. with all the additional restrictions  the resulting sizes of families nref1j and nref1j are 1 and 1 queries  respectively.
　despite the reduction in size  running just both nref families on all configurations and systems remains a daunting task: a quick math shows it may require  1 + 1  〜 1 runs〜1 configurations〜1min = 1 hours or 1 days of machine use! the final reduction was motivated by the desire to work with the same  round  number of queries for all families: we sampled 1 queries from each family  in a way that the distribution of elapsed times of the larger family was preserved. while the query families for the tpc-h based benchmarks are substantially smaller  as fewer meaningful joins can be defined in that schema   we also work with samples of 1 queries for those families.
1.1 configurations tested
　as a naming convention  the system name is used as a prefix for identifying configurations  and a  xxx r  suffix  where xxx is the name of a query family  is used for identifying the recommended configurations; for instance  a nref p refers to the p configuration on system a for the nref database  while b nref1j r is the configuration recommended by system b for query family nref1j. table 1 shows the building times and storage required for all configurations used in our tests  obtained as discussed in section 1.1.
tablea nref1j rb nref1j rb nref1j r1c1c1c1c1c1c1c1c1c1c1c1cidentical seq111neighboring seq111organism1protein111source11111taxonomy1111totals111111table 1: number of 1 1  and 1-column indexes in each recommended configuration for the nref benchmark. no index with more than 1 columns was recommended.
tablec skth1js rc skth1j rc unth1j r1c1c1c1c1c1c1c1c1c1c1c1clineitem11orders111partsupp11supplier11 views on lineitemn/an/an/an/a1n/an/an/an/a1 views on lineitem  partsuppn/an/an/an/an/an/an/an/a11totals111111table 1: number of 1 1  and 1-column indexes in each recommended configuration for the tpc-h benchmarks. no index with more than 1 columns was recommended. also  no indexes on customer or part werewe note that we were not able to obtain recommendarecommended.
tions for family nref1j using system a  that is  the recommender did not output any recommended configuration at all . we tried with a few other samples of 1 queries from family nref1j  as well as with smaller workloads consisting of 1  1  1  and 1 queries. while we verified that we could obtain recommendations for several of the smaller workloads  it did not make sense to pick any such configuration to represent the missing recommendation for the 1 query workload. therefore  we do not report any recommendation for family nref1j using system a's recommender.
　tables 1 and 1 show the number of indexes in each recommended configuration for the nref and tpc-h experiments  respectively. note that the recommendations for skth1j and unth1j contain indexes on both base tables and materialized views. for skth1j  1 recommended indexes were defined on materialized views of lineitem  while for unth1j  1 of the 1 indexes recommended were defined on 1 materialized views over the join of lineitem and partsupp.
1 results on the nref benchmark
　recall the discussion in section 1 about cumulative frequency distributions  and how to use them for comparing different configurations on the same system and workload.
　figure 1 in section 1 describes the behavior of system a for family nref1j. the curves readily show substantial improvements in the query executions times for the workload in both the a nref 1c and a nref1j r configurations  which we will refer to as 1c and r within the context of the family and the system  relative to a nref p configuration  similarly  p in context . the graphs also shows that the reference configuration 1c behaves better than the recommended configuration r  although the performance gap is very small for queries that require more than 1 seconds to complete.
　figure 1 shows the result of system a on family nref1j. the graph shows a more pronounced difference in perfor-

figure 1: behavior of system a on nref1j.
mance for the p and 1c configurations than before: on 1c  1% of the queries finish in less than 1 seconds  each   while on p  1% of the queries may take as long as 1 seconds to complete. another way of looking at these numbers is: it takes 1 seconds to complete 1% of the queries on 1c  while it takes 1 hours and 1 minutes to complete 1% of the queries on p: an improvement of 1 times! there are only two curves in figure 1 since  as discussed in the preceding subsection  the recommender in system a was unable to produce a recommended configuration r for this family. this is particularly surprising giving the vast benefit provided by the 1c configuration using only single column indexes.
　figures 1 and 1 show the behavior of system b on families nref1j and nref1j  respectively. as one can see  the performance of the recommended configuration for query family nref1j is almost indistinguishable from that of the p configuration. in family nref1j  the recommended configuration performs relatively better  but the gap it exhibits to the 1c configuration is still significant.
in summary  the 1c configuration was always far superior
　

figure 1: behavior of system b on nref1j.

figure 1: behavior of system b on nref1j.
to the initial configuration p. a careful look at figures 1 and 1 shows wide gaps between the p and 1c configurations  indicating large potential performance improvements by the use of indexes. in many cases  1c was also far better than the configurations recommended by both systems. therefore  we arrive at the surprising observation that both recommenders may fail to improve on the p configuration even when the potential for improvement is considerable.
1 results on the tpc-h benchmarks
　the behavior observed on both synthetic datasets  shown in figures 1  1 and 1  is consistent with the behavior of the nref benchmarks described in the preceding subsection. as mentioned earlier  we limit the results presented to one of the dbmss that we call system c.
　notably  the 1c configuration continues to be very competitive with the r configurations  again  on occasions  1c is far superior to r . this happens despite some of the recommendations using materialized views defined on joins of base tables  making the relatively better performance of 1c even more remarkable.
　the only recommendation r in all our experiments to outperform 1c even on a small portion of the workload was obtained on family skth1js  see figure 1 . the r configuration succeeds in speeding up the most expensive queries compared to 1c. since the goal used by system c's recommender is total cost  instead of the quality of service curve represented in the graph   it is not surprising that the recommender favors improving long-running queries  the ones

figure 1: behavior of system c on skth1js.

figure 1: behavior of system c on skth1j.
that dominate total cost .
　a comparison of figures 1 and 1 shows a sharp contrast between the behavior of system c for the simpler and the  generalized  1-way join families in the tpc-h benchmark. this emphasizes the dependence of the configuration recommender on the input workload  for even relatively small variations in the structure of the workload .
　another interesting observation can be made by comparing the behavior of the recommender on skewed versus uniform data. contrast the recommendations for skth1j and unth1j  table 1  and the relative performance of these configurations in figures 1 and 1. clearly  the recommender did perform better for the uniformly distributed data. nevertheless  the 1c configuration still proved the best overall. finally  given that the recommender considers the overall workload performance  and not the distribution of the individual query execution times  it is informative to present overall numbers for one workload. consider the results of running skth1j on the configuration p. we observe that the total execution time for the queries that do not timeout is 1 seconds  while there are 1 queries that timeout  taking at least 1 seconds each . while we do not know how long timeout queries could take  we can use the timeout value to obtain a lower bound for the execution of workload skth1j on p of 1 seconds. a similar calculation gives lower bounds for the execution of workload skth1j on the configurations 1c and r of 1 and 1 seconds  respectively. keep in mind  though  that 1c has only one timeout query while p and r have 1 and 1 timeout queries respec-

figure 1: behavior of system c on unth1j.
tively  hence the lower bound is much tighter on 1c than on r and p . thus  a very conservative overall workload assessment results in 1c producing almost 1 times better results than r!
1 the impact of insertions
　the workloads considered in this paper do not include updates. designing update workloads  in particular  containing complex updates  is a valuable extension to the current benchmark. however  it is important to observe that using 1c as a reference configuration remains valid in the presence of a reasonable number of insertions.
　to illustrate this  we run an experiment inserting tuples into the neighboring seq relation  both the widest and the largest relation in the nref database  with almost 1 million rows. the insertions take roughly linear time in the number of tuples inserted in all configurations. as expected  it takes longer to insert tuples in the configuration 1c than in the recommended configuration. insertions in the initial p configuration are faster than in the recommended configuration. considering the workload nref1j  we can determine the number of tuples that have to be inserted to make the elapsed time of executing the insertions plus the workload on 1c the same as the time of executing the insertions plus the workload on r. this is the break point at which the slower insertions  faster queries in 1c overtake the slower queries  faster inserts in nref1j r: in both systems a and b this number is close to 1 tuples. keep in mind that this calculation is based on enforcing timeouts for query executions  so this is just a lower bound for execution times. furthermore  the threshold of 1 tuple insertions is based on a single execution of each one of the 1 queries in the workload. if the queries are executed 1 times each  then the insertion threshold corresponds to increasing the database size by almost 1%.
1. recommender limitations
　this section presents additional empirical evidence that highlights an important limitation in the existing crop of commercial recommenders.
1 reliance on estimation
　as discussed in section 1  current rdbms configuration recommenders look at statistics from an existing system configuration ci and are given query workload w together with a space budget b that limits the target size of the recommended configuration.
　recommendations are produced by performing a heuristic search on the space of possible configurations cj1 ... cjn that satisfy the budget b  while minimizing the estimated cost of execution of  possibly a sample of  the workload w. that is  none of the configurations considered by the configuration recommender are actually built during the recommendation phase. the estimated performance of a given hypothetical configuration cjk considered by the configuration recommender is obtained by feeding the rdbms query optimizer with parameters that describe cjk  e.g.  statistical information about indexes in cjk . the parameters describing cjk are also estimated by the query optimizer  1 
1 .
　the use of the rdbms query optimizer by the recommender tool has several practical advantages. most notably  it avoids potential mismatches in estimated query costs for a given configuration  as the same estimator  i.e.  the query optimizer  is used for both recommending configurations and executing queries. in practice  this eliminates the risk of having indexes that are recommended but never used in the execution plans selected by the optimizer. however  the recommender tool ends up relying extensively on the rdbms query optimizer producing accurate estimates for the costs of executing queries  it is well known that the quality of such estimates degrades severely as query complexity increases  . the recommender is also vulnerable to estimation errors in the optimizer when assessing the parameters of the hypothetical configurations considered.
　we note that there has been work on improving the accuracy of estimates based on observing actual query execution costs. the learning optimizer leo described in  is an example of a technique that has been shown to continuously improve cardinality estimates on a commercial optimizer. however  this approach has limitations when applied to a recommender: since leo depends on observed executions in the current configuration  the technique may not improve estimator accuracy for hypothetical configurations  as there is no way to observe these executions without changing the current system configuration .
　recall that figure 1 shows the actual execution times of the three configurations p  r and 1c for family nref1j on system b. figure 1 shows similar cumulative frequency curves for family nref1j on system b but looking at the values of the optimizer estimates  represented here by arbitrary units and not as seconds . five different curves are presented in the figure. the curves ep  er and e1c are the estimates for the queries provided by the optimizer when the system is in the configuration p  r and 1c respectively. the optimizer correctly estimates that the behavior of r improves over p and that 1c improves even further. however  the magnitude of the improvements is quite conservative compared with the improvements seen in the actual executions  again  refer to figure 1 .
　the recommender does not have access to the er and e1c curves while the system is in the configuration p. instead it can obtain hypothetical estimates hr and h1c for the configurations r and 1c  respectively  while on the current configuration p. the curves hr and h1c also appear in figure 1. note that while the curve for hr almost coincides with the curve for er  the curve for h1c is much more conservative about the advantages of 1c than e1c. while

figure 1: cumulative curves for the estimates for family nref1j on system b.
in this example the relative goodness of hr and h1c are correct  the data shows that the accuracy of the hypothetical estimations is not always closely related to the estimates taken at the target configuration.
1 measuring improvements
　we define the estimated improvement ratio of query qk when estimated on configuration ci compared to configuration cj as the ratio eir qk  = e qk ci /e qk cj ; similarly  the actual improvement ratio of changing between configurations ci and cj is air qk  = a qk ci /a qk cj . for simplicity  actual improvements involving timeout queries are not considered.
　we also define h qk ch ca   the hypothetical cost of executing query qk in the hypothetical configuration ch  which is an estimate obtained while the system is in the actual configuration ca as discussed in the previous section. since in our experiments we are interested in evaluating hypothetical configurations in p  we define the hypothetical improvement ratio of query qk when hypothetically estimated on p for configuration ci compared to the hypothetically estimation on p for configuration cj as the ratio
hir qk  = h qk ci p /h qk cj p .
　we are interested in the improvement ratios that compare the recommended configuration r to the reference configuration 1c. larger than 1 ratios indicate that r will do worse  while smaller ratios indicate how much faster r is than 1c for a given query.
　figure 1 shows the histograms  instead of cumulative polygons  of the three improvement ratios defined above comparing r to 1c for the queries in family nref1j on system b. looking at the actual executions  the air histogram curve   we see that 1 queries are 1 times faster in 1c than in r and 1 queries are 1 times faster in 1c than in r  while 1 queries show no improvement at all  ratio 1 . when we look at the hypothetical estimates obtained in the configuration p  the hir curve  we see that 1 queries are 1 times faster in 1c than in r and 1 queries are 1 times faster in 1c than in r  while 1 queries show no improvement at all. the hir curve states that the r and 1c configurations would  hypothetically  perform much closer than they actually do. in contrast  looking at the estimates obtained in the configurations 1c and r  the eir curve   we see improvement ratios that much more clearly favour 1c

figure 1: histograms for the improvement ratios for family nref1j on system b.
over r  in fact  even more so than the actual ratios air .
　the previous example further illustrates the point made in the preceding section regarding the discrepancies between estimates and hypothetical estimates. the configurations are evaluated by the recommender on the basis of hypothetical estimates in the current configuration  since it has no access to estimates in the target configurations  although these estimates could be much more accurate than the hypothetical ones .
1. conclusion
　this paper introduces a benchmarking framework for assessing the quality of autonomic configuration recommenders. we propose a flexible notion of workload performance  we employ large and diverse query workloads described by families of similar queries  and we identify comprehensive single column indexing as a very useful configuration for baseline comparisons. our use of curves depicting the cumulative frequencies of query execution times to characterize workload performance bring forward the advantages of designing recommenders that can accept quality of service goals specified by constraints on these curves.
　using the proposed framework  we describe three benchmarks using real and synthetic data and several classes of families which are used to provide the first assessment in the literature of current commercial configuration recommendation tools. we believe that the extensive experimental results we report have substantial value as they not only confirm the practical applicability of the approach proposed  but also demonstrate that improvements of several orders of magnitude can still be achieved.
　we can regard the experimental data collected from our experiments as the missing observation step in the observe  predict and react loop applied to autonomic indexing. current recommenders predict based on estimates and hypothetical configurations and react by recommending a new configuration but there is no attempt to observe the actual cost of query execution.
　we note that the proposed benchmark methodology can be extended to support the evaluation of non-relational systems  such as recently proposed xml-based recommender tools .
　conducting extensive experimental evaluations are a first step towards assessing and improving the effectiveness of
　
existing relational recommenders. more significantly  meaningful benchmarks form the basis for the scientific evaluation of future research in autonomic recommenders.
acknowledgments. we thank yannis ioannidis  sam lightstone  arun marathe  vivek narasayya  danny zilio  and the referees for their feedback. this work was supported in part by grants from the natural science and engineering research council of canada. d. barbosa was supported in part by an ibm phd. fellowship. a. teisanu was supported in part by an ibm cas fellowship.
