this paper introduces the tuple graph  tug  synopses  a new class of data summaries that enable accurate selectivity estimates for complex relational queries. the proposed summarization framework adopts a  semi-structured  view of the relational database  modeling a relational data set as a graph of tuples and join queries as graph traversals respectively. the key idea is to approximate the structure of the induced data graph in a concise synopsis  and to estimate the selectivity of a query by performing the corresponding traversal over the summarized graph. we detail the tug synopsis model that is based on this novel approach  and we describe an efficient and scalable construction algorithm for building accurate tugs within a specific storage budget. we validate the performance of tugs with an extensive experimental study on real-life and synthetic data sets. our results verify the effectiveness of tugs in generating accurate selectivity estimates for complex join queries  and demonstrate their benefits over existing summarization techniques.
1. introduction
﹛consider a relational query optimizer  faced with the challenging task of optimizing a complex join query. in order to approximate effectively the cost factors of a candidate plan  the optimizer needs to obtain accurate estimates on the sizes of results that are generated at different operators  or equivalently  accurate selectivity estimates for the corresponding query expressions. these estimates are typically provided by data synopses  commonly referred to as  data statistics   that approximate the underlying data distribution and can thus estimate the number of results generated by a query. the accuracy of these data synopses is therefore crucial for the effectiveness of the optimization process.
﹛the accuracy of a data synopsis depends heavily on its capacity to capture in limited space the main correlations

 this work was supported in part by nsf grant iis-1 and by an ibm faculty development award.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro㏒t or commercial advantage and that copies bear this notice and the full citation on the ㏒rst page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci㏒c permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
that exist among the distributions of joins and values in the data. consider  for instance  a simple database with information on movies that consists of three tables  namely  movies  actors  and cast. cast has foreign keys to the other two tables  and essentially records in which movie each actor appeared along with their wages. in this simple database  an example correlation across joins might be that movies in the last decade have typically higher wages for their actors. in other words  there is a dependency between the distribution of different values through the chain of joins. another example  may be that movies released in the 1s tend to have more actors than movies made in the 1s. here  the number of join results is affected by a selection on the value of one of the participating tables.
﹛table-level synopses  such as  histograms  or wavelets   are typically ineffective in capturing these complex join-based correlations as they focus on the summarization of a single table at a time. this has led to the introduction of schema-level synopses  e.g.  join synopses  and probabilistic relational models   that enable accurate estimates by summarizing the combined join and value distribution across several tables. the proposed techniques  however  are not applicable to the class of relational schemata that contain many-to-many relationships. such a schema is our toy movie database  where a single movie is associated with multiple actors and vice versa.  observe that this many-to-many relationship is encoded with several one-to-many key/foreign-key joins.  this type of join relationship is very common in real-world applications  and hence providing effective summarization techniques for such schemata is an important and practical problem; at the same time  however  the presence of many-to-many joins greatly complicates the potential statistical correlations  introducing significant new challenges to the summarization problem.
﹛motivated by these observations  we initiate the study of schema-level data synopses for data sets with arbitrary join relationships. more concretely  we introduce the class of tuple graph synopses  tugs for short  that rely on graphbased models in order to summarize the combined join and value information of a relational database with such complex joins. the inspiration for our proposed solution comes from an unlikely source: the field of xml summarization. conceptually  we adopt a  semi-structured  view of a relational database  where tuples and joins become nodes and edges respectively in a data graph  so that join queries now correspond to graph traversals. with this view in mind  we propose to summarize the structure of this data graph in order to derive accurate selectivity estimates for query selectivities. to address the challenges of relational summarization  however  our work relies on novel techniques that form a clear departure from previously proposed xml models.
﹛as we show in this paper  our tug synopses enable accurate selectivity estimates for a large class of practical join queries: queries with arbitrary join graphs  e.g.  containing cycles and many-to-many relationships  and several selection predicates on different tables. to the best of our knowledge  the tug model is the first schema-level technique to enable the combined summarization of joins and values for schemata of arbitrary join relationships. more concretely  the contributions of our work can be summarized as follows:
  tug synopsis model. we introduce the tug synopses for summarizing the combined join- and value-distribution of a relational database. our novel tug model promotes joins and values to an equal status and thus allows a uniform treatment of their potential correlations in the underlying data distributions. we demonstrate several key properties of the tug model and develop a systematic estimation framework for approximating the selectivity of queries with arbitrary join graphs.
  tug construction. we introduce an efficient construction algorithm  termed tugbuild  for building accurate tugs given a specific storage budget. our algorithm takes advantage of the unique properties of our tug model  thus achieving the effective compression of statistical information while preserving the accuracy of estimation. a key feature is the use of disk-based processing techniques that enable tugbuild to scale to large data sets under limited memory constraints. these ideas are of general interest  as they can be transferred to the xml domain enabling the scalable summarization of large xml data sets.
  experimental evaluation of tugs. we conduct an extensive empirical study to validate the effectiveness of our proposed tug model. our results on synthetic and reallife data sets verify the effectiveness of tug summaries as schema-level data synopses. moreover  our study demonstrates the scalability of our approach and its many advantages over previously proposed summarization techniques.
﹛the remainder of the paper is organized as follows. section 1 provides a formal definition of the summarization problem and introduces some necessary notation. section 1 presents in detail the proposed tug model and its properties  while sections 1 and 1 cover the tug construction process. section 1 presents the results of our experimental study for evaluating the effectiveness of tug summaries. we cover related work in section 1 and conclude with section 1.
1. preliminaries
data model. our work focuses on the summarization of a relational database d with a set of relation names r = {r1 ... rn} and a global set of attributes a = {a1 ... am}.  the attributes of different relations form subsets of a.  without loss of generality  we assume that each attribute has a unique value domain and overload aj to denote the underlying domain as well.
﹛to capture the join relationships between tables  we define the schema graph gs r﹍a es  of the database as follows:  a  each node in gs represents a relation ri or an attribute
aj   b  an edge    denotes the join between the two relations on a subset of their attributes  and  c  an edge  ri aj  denotes that the schema of ri contains attribute aj. we focus on equi-joins since they are prevalent in real-world applications  and we do not place other restrictions on the number of joins per relation or the type of each join  e.g.  one-to-one  one-to-many  or many-to-many. our assumption is that gs is part of the meta-data of d and can be either specified by the dba or derived automatically  e.g.  by interpreting the constraints of the schema and analyzing a sample workload.
﹛similar to the schema graph  we define the data graph gd td ﹍ vd ed  as the representation of the database instance. a node 而 ﹋ td represents a tuple of some relation ri  while a node 糸 ﹋ vd represents a value of some attribute aj. given a tuple 而 of another relation  the data graph includes the edge    if the edge     is in the schema graph and the two tuples have the same values for the corresponding join attributes. similarly  the data graph includes the edge  而 糸  if  ri aj  is present in the schema graph and tuple 而 has the specified value for attribute aj.
﹛example 1.: figure 1 a  shows the schema graph for a data set that records information on movies. in this example  the movie relation joins with two other relations  directedby and cast   and has two value attributes  year and genre .  the arrows in the schema graph point towards the relation containing the primary key.  figure 1 b  shows a sample data graph for the same schema  where each tuple node is named by a matching lower case letter and a numerical identifier.  the importance of the shapes of nodes will become apparent later. 
﹛overall  the data graph gd is a semi-structured interpretation of the flat relational data set based on the properties of the schema graph. as we discuss next  this formulation enables the interpretation of join queries as traversals over the data graph  and essentially forms the cornerstone of our proposed summarization framework.
query model. we focus on a general class of queries with arbitrary join graphs and table-level selection predicates. similar to the schema and the data graph  we represent a query q as a graph gq tq eq . each node qi ﹋ tq  1 ≒ i ≒ |tq|  represents a query variable that is bound to a specific relation table qi   while an edge  qi qj  ﹋ eq denotes a join predicate between the corresponding relations. we assume that join predicates are placed according to the schema graph gs and hence each query edge  qi qj  must correspond to a schema edge  table qi  table qj  . moreover  each query variable qi may be annotated with a conjunction of selections on the attributes of table qi . figure 1  a  shows an example query graph over the sample movie database  corresponding to the relational algebra expression . we note that our query model does not place any restrictions on the shape of the join graph and can thus capture a large class of queries that are common in practice  including queries with cyclic join predicates .

	 a 	 b 	 c 
figure 1:  a  schema graph  b  instance  c  data graph
﹛based on the semi-structured interpretation of a relational data set  we can define the evaluation of a query q as a complex traversal over the data graph gd. more precisely  we define an embedding of q as a mapping from query variables to tuple nodes of gd  where each variable qi is mapped to a tuple of table qi  that satisfies the selection predicates  and each edge  qi qj  is mapped to a join edge between the corresponding tuples. it is straightforward to show that each embedding corresponds to a result-tuple of the query  and hence the set of unique embeddings determines the answer of q.
problem definition. we define the selectivity sel q  of a query q as the cardinality of its result set. the problem that we tackle in this paper can thus be summarized as follows: given a database d and a space budget b  construct a data synopsis of size at most b that can estimate the selectivity sel q  of any complex query q. as we have mentioned earlier  this problem is key in the optimization of relational queries and is therefore of high practical value.
﹛clearly  the needed selectivity sel q  can be expressed as the number of the embeddings of q  which in turn depends on the structural characteristics of the data graph gd. this observation forms the basis of our proposed tug solution. in short  the key idea is to approximate the structure of gd in a concise synopsis  and to estimate the number of embeddings by matching q over the summarized graph. we discuss the specifics of this approach in the following section  where we describe in detail the tug summarization model.
1. tug synopses
﹛in this section  we introduce the proposed tug synopses. we first present a formal definition of the tug model  and then develop a systematic framework for approximating the selectivity of complex join queries over concise tug synopses.
1 model de㏒nition
﹛at a high level  our proposed tug framework employs an abstract partitioning model that groups the nodes of the data graph gd td ﹍vd ed  in disjoint partitions. the key idea is to store aggregate statistical information on a per partition basis and thus generate a concise summary that approximates the structure of gd  equivalently  the join and value distribution of the underlying database . more precisely  let p be a partitioning of td ﹍ vd. we say that p is a valid partitioning if the following property holds for every partition r ﹋ p: either r contains tuples from the same relation ri  denoted as table r    or it contains a single value 糸 ﹋ vd of an attribute aj  in this case  we assume that table r  = aj . we refer to r as a tuple partition or a value partition respectively. to simplify our notation  we will use r to refer both to the partition and the set of data graph nodes that it contains.  hence  |r| is the size of the partition.  consider two tuple partitions r and s. we say that r and s are linked if there are tuples 而r ﹋ r and 而s ﹋ s such that the edge  而r 而s  appears in the data graph  i.e.  而r joins with 而s. similarly  a tuple partition r is linked to a value partition s if there is a tuple 而r ﹋ r such that the edge  而r 糸  exists in the data graph  i.e.  而r carries the value 糸 in attribute aj √ table s . in general  we will use data-edges r s  to denote the set of edges in the data graph that exist between two linked partitions.
﹛a tug synopsis t g is a graph-based representation of a specific partitioning p  augmented with aggregate statistical information on linked partitions. more formally  a tug is defined as follows:
﹛definition 1. a tug synopsis t g of the data graph gd td ﹍ vd ed  is a graph gt g p es  such that  1  p is a valid partitioning of td ﹍ vd   1   r s  appears in es if r and s are linked   1  each node r ﹋ p records the common relation name table r  and a counter tcount r  = |r|   1  each edge  r s  ﹋ es records a counter jcount r s  =
|data-edges r s |.
essentially  a tug encodes a partitioning of the data graph nodes along with aggregate information on their join- and value-based characteristics. for each partition  the synopsis records the common relation name and the size of the extent  the contents of the extent are not stored   while for each synopsis edge it records the number of data edges between the endpoints. given the unified treatment of tuples and values in our model  an edge  r s  in t g may record the join between two tuple-sets r and s  or the appearance of a value s in tuple-set r. accordingly  jcount r s  may be the result size of  or the number of tuples in r with value s. this uniform handling of joins and values is an important characteristic of our framework  as it facilitates the identification of correlations between and across the join and value distribution of the underlying data.
﹛example 1.: figure 1 shows an example tug for the data set of figure 1  where the correspondence between partitions and tuples is denoted by the enclosing shapes. in this example  node m汐 represents two movie tuples  while node c汐 represents three cast tuples. their join is denoted by the linking edge and it includes jcount m汐 c汐  = 1 results. moreover  m汐 is linked with the value partitions of the year and genre attributes  and the corresponding join counts denote the frequency of individual values in the tuples of m汐.


	 a 	 b 
figure 1:  a  tug summary s   b  s after merging a汐 and a汕
﹛we now discuss in more detail the properties of the proposed tug model. consider a specific tug synopsis and a tuple partition r. let s be a tuple partition linked to r and define jratio r s  = jcount r s /tcount r  as the join ratio from r to s. in this case  each tuple in r is assumed to join with jratio r s  tuples in s  independent of links to other partitions. similarly  if s is a value partition  then jratio r s  is interpreted as the empirical probability of a tuple in r having value s in the corresponding attribute  again  independently of links to other partitions . the basic idea therefore is to equate all the tuples in r with a centroid  defined by the ratios jratio r s1  jratio r s1  ...  jratio r sk  to the linked partitions s1 s1 ... sk. this approach essentially assumes statistical independence across the join and value distributions of tuples in r  coupled with a uniformity assumption on the number of joining neighbors. typically  such assumptions are unfitting for the purpose of accurate selectivity estimation. the key point  however  is that they become relatively valid if r represents tuples with similar join- and value-based characteristics  or equivalently  when the centroid of r is a good representative for the tuples in r. as we discuss later  this observation forms the main idea behind our algorithm for constructing accurate tugs. up to this point  we have assumed that a tug summary stores exact information on the value distribution of attributes. to see this  observe that each tuple partition r is linked to individual values for a specific attribute aj  and the corresponding edges carry the exact frequency of each value inside r. since this approach can quickly become impractical  we replace value partitions and their corresponding edges with single-dimensional value summaries. more concretely  for each tuple partition r and attribute aj that is related to table r   the synopsis records a single-dimensional value summary vsum aj r  that approximates the value distribution of aj in the tuples of r. returning to the example of figure 1  node m汐 will record histograms vsum year m汐   and vsum genre m汐  that summarize the distribution of year and genre values respectively in the corresponding movie tuples. our framework does not make any assumptions on the summarization techniques that implement these valuesummaries  except that they are appropriate for the type of the corresponding value domain. in this paper  we focus on the summarization of numerical and categorical values and rely on conventional histogramming techniques for their approximation. on a final note  we observe that the use of single-dimensional histograms corresponds again the same independence assumption that we have mentioned earlier. the key idea  however  remains the same: by grouping together tuples of similar statistical characteristics  our tug construction algorithm promotes the validity of this assumption  thus enabling the accurate summarization of the underlying data.
1 estimation framework
﹛our estimation framework approximates the selectivity of a complex query q by matching the query graph on the graph of the tug synopsis. we formalize the matching process in terms of a query embedding h  defined as a mapping from query variables to summary nodes that respects the join and selection predicates in q. figure 1 b  shows an example with a simple query graph and its embedding on the tug of figure 1 b .
﹛overall  a query embedding represents a sub-graph of the summary on which q has a potentially non-empty result. we can thus derive the selectivity of q by aggregating the selectivities of its individual embeddings. to compute the selectivity of a single embedding  our framework traverses the selected sub-graph and uses the recorded join ratios to estimate the selectivity of different predicates. the following example illustrates this idea.
﹛example 1.: consider the embedding of figure 1 b  and in particular node a汐. given that tcount a汐  = 1 and that jratio a汐 s.female  = 1  we can estimate that an average of tcount a汐  ﹞ jratio a汐 s.female  = 1 tuples satisfy the first selection predicate. each tuple in a汐 is assumed to join with an average of jratio a汐 c汐  = 1 = 1 tuples in c汐  and hence the part of the embedding that corresponds to  will generate tcount a汐  ﹞ jratio a汐 s.female  ﹞ jratio a汐 c汐  = 1 tuples. similarly  the final estimate for the embedding is computed as sel h  = tcount a汐 ﹞ jratio a汐 s.female ﹞jratio a汐 c汐 ﹞
figure 1:  a  query graph q  b  embedding of q on the tug of figure 1 a 

﹛a natural question is whether the computed selectivity estimate depends on the traversal order of the query embedding. a key feature of the tug model is its guarantee that any traversal of the query embedding results in the same estimate  with the same accuracy of course . more formally  let h be an embedding and deg r  be the number of incidental edges  in the embedding  for each summary node r ﹋ h. the following proposition establishes the closed-form expression for the selectivity sel h  that results from any traversal of h:
﹛proposition 1. for any traversal of an embedding h  the computed selectivity estimate can be expressed as follows:
jcount r s 
	sel	 1 
tcount r deg    1
﹛while we have presented an example of equation 1 for tree-join graphs  an interesting property is that it extends unmodified to the case of embeddings for arbitrary join graphs  i.e. queries with cyclic join predicates . in the interest of space  we defer the complete details of this extension to the full version of this paper .
﹛overall  our proposed estimation framework relies on the independence assumptions of the tug model in order to combine statistical information across different summary edges. the accuracy of a tug summary is therefore linked to the validity of independence  which is in turn tied to the statistical similarity of tuples within each partition.
1. compressing tug synopses
﹛having described our tug model  we shift our attention to the equally important problem of constructing accurate synopses. in this section  we introduce a basic operation that compresses the information stored in a tug summary  and thus forms the basis of the tugbuild construction algorithm  section 1 . we first describe the operation and its semantics  and then discuss a key result related to the lossless compression of tug synopses.
1 node-merge operation
﹛our basic compression operation  termed merge  reduces the storage of a tug t g by merging partitions in the underlying partitioning p. more formally  let s be a set of summary nodes in t g that correspond to tuple partitions of the same relation name ri. the operation merge s  substitutes the nodes in s with a single new node s that represents the union of the individual tuple partitions. hence  the statistics of the new node are defined as follows: tcount s  = tcount r   and jcount jcount r t  for every neighbor t of the nodes in s. figure 1 b  illustrates the result of a merge operation on the synopsis of figure 1 a   where the merged set s involves the two actor partitions.
﹛an important issue is the effect of this localized compression on the accuracy of the computed selectivity estimates. conceptually  merge s  results in a coarser tuple partitioning  where the tuples of each original node r ﹋ s are now represented with the new centroid of s. hence  the difference between the centroids of r and s provides a good indication of the inaccuracy introduced by the merge for the tuples in r. more formally  let ri be the relation name of nodes in s and let neighbors ri  be its neighbors  relations and attributes  in the schema graph. we will use ni to denote the total number of summary nodes that correspond to the neighbors in neighbors ri . we map every node r of s ﹍{s} to a ni-dimensional point  where each dimension corresponds to a unique summary node t of some neighbor in neighbors ri   and the coordinate is equal to jratio in other words  the multi-dimensional point of r indicates to which nodes r is linked and the corresponding average ratios. we quantify the inaccuracy of a merge in terms of the similarity in this multi-dimensional space between the nodes in s and the new node s. in this paper  we employ the well known radius metric  defined as follows:
radius merge
 here   r   s 1 denotes the euclidean distance between the points that represent r and s in the multi-dimensional space of join ratios. essentially  the radius metric measures the  tightness  of a cluster that comprises the points in s and has s as its representative. a low radius  therefore  indicates that the merged nodes in s have similar join relationships and value distributions  which in turn implies that the aggregate new node s is a good approximation for every node r ﹋ s. in effect  merge s  results in a summary that generates similar estimates to the original synopsis.
1 lossless merge operations
﹛in this section  we examine lossless merge operations  i.e.  operations that do not affect the accuracy of the resulting synopsis. as we discuss in section 1  our construction algorithm relies on such operations in order to efficiently compress the data graph to a much smaller accurate summary.
﹛intuitively  we expect a merge operation to be lossless if radius merge s   = 1  i.e.  nodes in s have the exact same join ratios to every other node in the synopsis. in our work  we prove a somewhat surprising result: merge s  may be lossless even if the nodes in s do not match on every dimension of the underlying point space! this type of restricted similarity is captured by our novel concept of all-but-one similarity that we define next.
﹛let t ﹋ neighbors ri  be a schema neighbor of ri and let be nodes of ri. we say that r and r are similar with respect to t if they have the same join ratios to every summary node that corresponds to t. intuitively  r and r cannot be distinguished based solely on their neighbor nodes of t and are thus characterized by some redundancy. we define all-but-one similarity by extending this property to specific subsets of neighbors ri . more precisely  r and r are called all-but-one similar if they are similar with respect to |neighbors ri | 1 neighbors of ri. hence  r and r are all-but-one similar either if they have different join relationships to a single relation  or different frequency distributions for a single attribute only. this definition of similarity is therefore quite liberal.
﹛as our earlier discussion hinted  all-but-one similarity captures the redundancy of statistical information in a tug synopsis. we formalize this property as follows:
﹛theorem 1. consider a tug t g and two all-but-one similar nodes r and r' be the summary that results from merge. then  synopses t g and t g' yield the same selectivity estimate for every complex join query q.
to prove the theorem  we show that the embeddings of a query q over t g can be mapped to a set of equivalent embeddings over t g'.
﹛the intuition behind theorem 1 can be described as follows. consider a relation ri that is linked to another relation rk and an attribute aj in the schema graph. let r and r be two tuple partitions of ri that have the same joins to rk but different value distributions for aj.  hence  r and r are all-but-one similar.  essentially  the different

 c 
figure 1:  a  starting summary t g   b  t g after merging a with respect to c  c  t g after merging a with respect to b  and then with respect to c.

value distributions for r and r are not correlated to their joins to rk - these are identical. hence  a merge  will not result in the loss of a join-to-value correlation when the new node is linked to the union of neighboring nodes for r and r. in that sense  the new synopsis remains equivalent to the original summary.
﹛example 1.: figure 1 shows the application of theorem 1 on a sample synopsis t g. looking at figure 1 a   we observe that nodes a1 and a1 are all-but-one similar with respect to neighbor c  as they join to the same c nodes with the same ratios. a similar observation holds for nodes a1 and a1. we can thus perform two merge operations  as shown in figure 1 b   without compromising the accuracy of the summary. after these operations  no nodes are all-butone similar.
﹛similarly  we observe that nodes a1 and a1 of the original summary t g are all-but-one similar with respect to neighbor
b  and the same holds for a1 and a1  and for a1 and a1. by performing the three lossless merges  we obtain the first summary of figure 1 c . at this point  the merged nodes a1 and a1 have become all-but-one similar with respect to c. it is possible  therefore  to perform another lossless merge and derive the second synopsis of figure 1 c .
﹛the previous example demonstrates clearly that it is possible to derive a variable number of lossless merges depending on the order in which neighbors are considered for allbut-one similarity. the reason is that merge operations affect the implied multi-dimensional space of join ratios and thus impact the existence of all-but-one similarity. this is an interesting property of our framework and at the same time a significant challenge  as it becomes necessary to select carefully the order in which all-but-one neighbors are considered. we revisit this issue in section 1  where we introduce a heuristic for determining an effective sequence of merge operations.
﹛overall  all-but-one similarity is a novel feature of our model that provides a unified  in terms of joins and values  characterization of statistical redundancy in a tug synopsis. it is interesting to note that xml summarization models have also employed similar definitions of similarity  albeit with stricter conditions. in treesketches   for instance  similarity is defined in terms of all the neighbors of a node in the data graph  which is considerably more restrictive compared to the except-one subset of schema neighbors used here. as we show in our experimental study  the greater flexibility of all-but-one similarity achieves a more aggressive compression of statistical information  and is key in the scalable construction of accurate tug summaries. transferring our results to the xml domain is an interesting direction for future work.
1. tug construction
﹛in this section  we introduce the tugbuild algorithm for constructing an accurate tug synopsis within a specific storage budget. the proposed algorithm operates in three stages: in the first stage  it merges all-but-one similar nodes  section 1  in order to compress the data graph to a much smaller lossless synopsis; in the second stage  it uses merge operations of low radius in order to further compress the summary while not sacrificing accuracy; and in the third stage  it substitutes the detailed value information with compressed single-dimensional histograms. throughout this process  tugbuild relies on efficient disk-based structures and scalable algorithms in order to handle large data sets under limited memory resources. our experimental results  section 1  verify that our techniques scale well in practice and are hence of interest for other graph-based summarization methods  e.g.  in the xml domain  that operate entirely in main memory.
﹛the following sections describe the construction algorithm in more detail. we first examine the key problem of of identifying effective merge operations in a scalable fashion  and then discuss the specifics of the tugbuild algorithm.
1 scalable tug compression
﹛as noted earlier  our tugbuild algorithm derives an effective summary by applying merge operations of low radius to an initial accurate synopsis. an important problem therefore is identifying such operations in a scalable fashion  in order to maintain the efficiency of the construction process. we employ a solution that is based on the clustering of tuple partitions according to their join ratios to neighboring nodes. more concretely  our algorithm considers one relation ri at a time  and computes a clustering of the multidimensional points that correspond to the join ratios of its nodes. each identified cluster corresponds to a set of nodes with similar join fractions  and thus represents a candidate merge operation of low radius. after performing the implied merge operations for ri  the algorithm considers another relation and repeats the same process. an important observation is that the clustering can be performed with well known scalable techniques  e.g.  birch  or subspace clustering  thus enabling our solution to scale gracefully to large data sets.
﹛a natural question concerns the order in which relations are considered in this iterative process. as hinted by our example of figure 1  section 1   the order of merge operations can affect significantly the ability to find tight clusters.  our original discussion concerned merges of all-butone similar nodes  but it can be extended to the case of general merges.  in our work  we adopt an intuitive heuristic that examines relations in decreasing order of their clustering potential  that is  the ability to form few and tight clusters for the corresponding summary nodes. the goal is to enable a high compression of the tug summary  few clusters   while keeping the error of approximation low  tight clusters .
﹛we now describe a metric cv ri  that quantifies the clustering potential for a specific relation ri. as mentioned previously  the goal is to rank relations based on the cv metric and thus determine the order in which they are considered for clustering. the proposed cv ri  metric is based on the following intuitive observation: the nodes of ri will cluster well if there is a tight clustering with respect to every schema neighbor in neighbors ri .  this idea is similar to subspace clustering   where clusters in low dimensional spaces are used to compute clusters in higher dimensions.  our approach is to estimate the quality of clustering ri with respect to each schema neighbor  and to compute cv ri  by combining the individual estimates. to quantify the effectiveness of clustering ri on a specific schema neighbor t ﹋ neighbors ri   we consider the variability of join ratios from nodes of ri to nodes of t. more concretely  let t ﹋ t g be a node corresponding to t and define nt to be its neighbors that correspond to ri. we consider a conceptual merge operation merge nt  resulting in a single node rt  and use the aggregate node rt in order to measure the variance of join ratios from nt to t. more formally  we introduce a localized cv ri t  metric as follows:
	nt	1
	cv ri t  =	 jratio |r t |    jratio rt t  1
 the numerator of this expression is indicative of the  savings  in storage space  since |nt| nodes are substituted with the single node rt. similarly  the denominator is indicative of the error of approximation  as each centroid in nt is now substituted with the centroid of rt. the ratio captures the space saved per unit of introduced error  and is thus sugges-
tive of the effectiveness of clustering ri with respect to the single node t.
﹛we define the overall metric cv ri  as the average of localized metrics cv ri t  across all nodes t of t and across all neighbors t ﹋ neighbors ri . intuitively  we expect a high cv ri  value if the nodes of ri have similar join ratios to every schema neighbor t ﹋ neighbors ri   implying
that an actual clustering of ri will yield few clusters that are tight. clearly  cv ri  is only a heuristic metric that may overestimate the potential of ri. note  however  that the construction algorithm uses cv ri  only to guide the selection of relations for clustering and not to compute the actual clusters. we revisit this point in section 1  where we verify experimentally the effectiveness of the cv ri  metric in guiding the construction process.
1 construction algorithm
﹛we now proceed to describe the specifics of the tugbuild construction algorithm  shown in figure 1. tugbuild receives as input the relational database d  a storage budget bs  and a value budget bv   and returns a tug summary t g that uses bs space units for its graph structure and bv value units for the compressed value summaries vsum aj r . as we have discussed earlier  the algorithm operates in three stages: the first stage  lines 1  computes a reference summary that maintains the key statistical correlations of d while being much smaller in size than the data graph gd; the second stage  lines 1  compresses this summary by applying a sequence of carefully chosen merge operations; finally  the third stage  lines 1  substitutes value-related information with compressed histograms. the following sections discuss each stage in detail.
procedure tugbuild d  bj  bv  
input: database d; join budget bj; value budget bv
output: tug t g begin
/** construct reference synopsis **/
1. merge value nodes in d in tight ranges
1. initialize t g with one node per tuple
1. while change do
1. compute	neighbors ri ＿
1.  rm nm  :=	r	 cv  ri n  
1. cluster	1  /** clusters of radius 1 **/
1. perform a merge for each identified cluster
1. end
/** compress join information **/
1. ctj := ct1j
1. while  join space of t g    bs do
1. while change do
1. compute cv  ri   ri ﹋ d
1. rm := argmaxri cv  rm  
1. cluster rm neighbors rm  ctj 
1. perform a merge for each identified cluster
1. done
1. ctj := 汐j ﹞ ctj /** increase clustering threshold **/
1. done
/** generate compressed value summaries **/
1. ctv := ct1v
1. preverr := inf; curerr := inf
1. loop
1. preverr := curerr
1. for each relation ri and related attribute ajdo
1. cluster ri aj ctv 
1. create a distinct histogram for each cluster
1. done
1. allocate bv units of space among created histograms
1. curerr := compute value err  
1. ctv := 汐v ﹞ ctv
1. while curerr   preverr
1. substitute value partitions and edges with best histograms1. return t g end
figure 1: algorithm tugbuild

building a reference tug. the outcome of the first stage  lines 1  is a tug that is much smaller than the original data d  and at the same time preserves the key statistical correlations between and across the distribution of joins and values. this is a key step in making the construction algorithm scalable  as it reduces the size of the input for the subsequent stages of the build process.
﹛initially  tugbuild transforms each numerical value domain to a small collection of tight numerical ranges  line 1 . the intuition is that the values in each range are close in the underlying domain and can thus be treated as equivalent in the summarization process.  in our experiments  we identify 1 ranges for each domain using the max-diff heuristic .  this compression is essential in identifying all-but-one similar nodes  as it increases the similarity of tuples in terms of value information. the trade-off  of course  is the introduction of a small error in the approximation of values. after this step  tugbuild compresses the resulting data graph by identifying and merging all-but-one similar nodes  using an iterative process  lines 1  similar to the one described in section 1. in each iteration  tugbuild considers each relation ri and each except-one subset n of neighbors ri   and ranks the  ri n  pairs according to a cv ri n  metric.  cv ri n  is computed exactly as cv ri  except that it only considers the schema neighbors in n.  the top pair  rm nm  is selected and a perfect clustering is computed for the nodes of rm and for the dimensions that correspond to nm. clearly  each identified cluster comprises nodes that are all-but-one similar with respect to neighbors nm. the algorithm performs one merge operation for each such cluster  and repeats the process until every relation has been examined at least once and it is not possible to identify new clusters. note that tugbuild may select the same pair  ri n  multiple times  as merges on the neighbors of ri may increase the overlap of ri nodes in terms of their joining nodes and thus render them all-but-one similar.
﹛to make the process scalable to large data sets  tugbuild employs a disk-based organization of the tug summary using b+-trees. the clustering step  line 1  scans the nodes of rm in a single sequential pass of the b+-tree  maintaining the zero-error clusters in a main-memory hash table. the hash-table stores only node-ids and hence its memory footprint is manageable for most cases; if it gets too large  then tugbuild spills it to disk and consolidates the spilled partitions at the end of the scan  a la partitioned hash join . the complete details can be found in the full version of this paper .
compressing join information. the goal of the second stage  lines 1  is to further compress the tug summary with additional node-merge operations. since the algorithm has exhausted lossless merges in the first stage  tugbuild resorts to lossy merge operations  using the radius of the merge in order to control the loss of accuracy  section 1 .
﹛as shown in figure 1  the selection of merge operations proceeds in two nested loops. the outer loop  lines 1  controls a clustering threshold ctj that determines the maximum radius metric of the selected merge operations  section 1 . the idea is to start with a low threshold for the initial merges  and to gradually increase it if it is necessary to compress further. the inner loop  lines 1  generates and applies the node-merge operations using the approach outlined in section 1. more precisely  the algorithm selects the relation rm with the highest cv rm  value  and computes a clustering of its nodes such that each cluster has radius less than ctj. the algorithm performs one merge operation for each identified cluster  and continues the loop until no merge operations can be identified for the current threshold. at that point  the outer loop resumes and the threshold is increased. in our experiments  we have used 汐j = 1  i.e.  the clustering threshold increases by 1% on every iteration.
﹛to perform the clustering that identifies node-merge operations  line 1   tugbuild employs a modified version of the well known birch algorithm . we have chosen birch as it can generate an effective clustering in a single pass over the data  thus enabling our algorithm to scale to large data sets. to handle the high dimensionality of our point space  we have modified birch to use fixedsize randomized sketches in order to approximate the large coordinate vectors of the internal cf-tree nodes . in particular  our modified birch algorithm uses count-min sketches  to approximate the linear sum component of a cluster's statistics and to essentially estimate the distance metric for the insertion of new points. leaf clusters  on the other hand  have typically a lower number of non-zero coordinates and are stored exactly. as always  the complete details can be found in the full version of this paper .
compressing value information. the goal of the third and final stage of the tugbuild algorithm  lines 1  is to substitute the detailed value distributions in the synopsis with compressed value summaries in each node. to make effective use of the allotted space budget bv   tugbuild takes advantage of the redundancy that may exist in the data and creates a distinct value summary per group of nodes that have similar value characteristics. more concretely  consider a relation ri and a related attribute aj. tugbuild computes a clustering of ri based solely on the coordinates that correspond to aj  thus identifying clusters of nodes that have similar value distributions for aj. for each such cluster g  the algorithm creates a single summary vsum aj g  that summarizes the union of the corresponding value distributions  and shares it among all the nodes in the cluster.
﹛the selection of these clusters is performed adaptively  using a threshold ctv that controls the value-based similarity of nodes within each cluster. the selection of ctv presents an interesting trade-off: a low threshold implies higher similarity and hence more clusters  but also more summaries that share the fixed space budget bv . tugbuild explores this trade-off with an iterative strategy  lines 1   that initializes ctv to a low value ct1v and increases it gradually in order to discover more effective solutions. for a specific threshold  tugbuild computes the clustering for each relation ri and related attribute aj using ctv to control the radius of the generated clusters  line 1 . after distributing the fixed space budget bv to the initialized summaries  line 1   tugbuild computes an error metric  function compute value error  that characterizes the accuracy of the overall value approximation. if the current error constitutes an improvement over the previous iteration  then the threshold is increased and another iteration is performed. overall  the goal is to achieve an effective partitioning of summary nodes so that the individual value summaries vsum aj g  are allocated enough storage while summarizing value-distributions of adequate similarity.
﹛several details of this stage depend on the approximation methods that are used to implement the value-summaries. two key issues are the distribution of the alloted space budget bv to the value summaries that correspond to a specific threshold ctv   and the computation of the approximation error. in this paper  we rely on histograms for value summarization and use well-known relational techniques  for determining the number of buckets for each histogram. conversely  we adopt the squared error of point frequencies as the error of approximation. as always  the complete details can be found in the full version of this paper .
1. experimental study
﹛in this section  we present the results of an extensive empirical study that we have conducted in order to evaluate the performance of the proposed tug framework. overall  our results on real-life and synthetic data sets verify the effectiveness of tug summaries and demonstrate their benefits over previously proposed  table- and schema-level  summarization techniques.
1 experimental methodology
﹛this section describes the techniques that we have used in our study  the data sets and related workloads  and the metrics for evaluating the performance of summarization techniques.
techniques. we base our experimental study on the following summarization techniques:
- tugs. we have completed a prototype implementation of the tug framework that we introduce in this paper. for tug construction  our prototype uses a custom implementation of the birch algorithm with the modifications described in section 1. unless otherwise noted  we set the initial clustering thresholds to ct1j = ct1v = 1 and increase them by 汐j = 汐v = 1% on every iteration.
- join synopses: we compare the performance of tugs against

the join synopses technique of acharya et al. . a join synopsis  also known as a join sample  is essentially a uniform random sample of the join of a specific table along key/foreign-key dependencies. as we discuss in section 1  however  this technique can only be used on schemata that do not contain many-to-many relationships and hence we use it only on a subset of our experiments.
- multi-dimensional wavelets. for schemata where join synopses cannot be applied  we compare the performance of tugs against the wavelet-based techniques of chakraborti et al. 1. the key idea is to create one multi-dimensional wavelet summary per table  and to evaluate the relational operators of a query  e.g.  select  project  join  entirely in the wavelet domain. wavelet summaries are table-level synopses and can thus be applied to any type of database schema.
- histograms. we use single-dimensional histograms as the

baseline summarization technique of our study. instead of using a custom histogram implementation  we have opted to use the selectivity estimates that are generated by the histogram-based statistics of a commercial system  referred to as system x . we instruct the system to create detailed statistics on the join and value attributes of all tables and then use the explain command to obtain the optimizer's estimates on query selectivity. to the best of our knowledge  system x employs the max-diff  heuristic for the creation of each histogram.
﹛overall  our study uses two schema-level  tugs  join synopses  and two table-level  wavelets  histograms  techniques. histograms serve as our baseline technique and are not expected to perform well  as they do not capture any correlations across value distributions. wavelets and join synopses  on the other hands  are essentially  multidimensional  techniques that capture the combined distribution of joins and values. they are  therefore  the main competitors to our proposed tug synopses.
data sets. we use two data sets in our study:  a  imdb  a real-life data set that contains information on movies  and  b  a skewed version of the well-known tpc-h data set1. the characteristics of the two data sets are shown in table 1. imdb contains many-to-many relationships between its central movies table and the remaining tables  actors  movie genres  and producers   while tpc-h contains only many-to-one relationships that originate in the central lineitem table. it should be noted that both data sets use key/foreign-key joins to encode these relationships. based on these characteristics  we apply tugs and histograms on both data sets  join synopses on tpc-h only  and wavelets on imdb only.
query loads. we evaluate the accuracy of each summarization technique using synthetic query loads. we generate positive workloads of tree-join queries by selecting connected subgraphs of the schema graph and attaching random selection predicates on the selected attributes. we ensure that each query has at least 1 join predicates and at least one selection predicate and generate 1 queries in each case. the characteristics of the positive workload for the two data sets are listed in table 1. we use the same methodology to generate negative workloads with similar characteristics. while we focus on tree-join queries in our presentation  we have also experimented with workloads of cyclic join queries. our results have been qualitatively the same and we omit them in the interest of space.
evaluation metric. given a true selectivity sel and an estimate sel  we use the absolute error for negative queries   = 1   and the absolute relative error
are sel sel  = |sel sel|/max sel sn  for positive queries. parameter sn is a sanity bound that avoids the artificial high errors of low-count queries. following previous studies in summarization  we set sn to the 1-th percentile of true query counts.
﹛to compare a specific error metric on two summaries  we employ the cumulative frequency distribution  cfd  of the metric over the queries of the workload. a point  x y  in the cfd indicates that y% of the queries in the workload have an error that is less than or equal to x.  the interpretation of x depends of course on the choice of ae or are.  using this comparator  a technique a is more effective than a technique b if the cdf of a dominates the cdf of b  i.e.  a larger percentage of queries has a lower error. we have found this approach to yield more interesting comparisons instead of using a single statistic  such as  an average .
1 experimental results
﹛we present now some of the experiments that we have conducted in our study. the specific subset captures the main traits of the wide range of results we obtained by varying the data  the query workloads  and the model parameters.
1.1 effectiveness of construction algorithm
﹛in the first set of experiments  we evaluate the effectiveness of the tugbuild algorithm. we focus on three important aspects of the proposed algorithm: the use of the clusterability metric cv  all-but-one similarity  and total construction time.
cv metric. as described in section 1  tugbuild processes relations in decreasing order of their cv metric in order to maximize the compression of the summary while maintaining its accuracy. to evaluate the effectiveness of

tpchimdbnumber of relations1tuples in largest relation1 1 1tuples in smallest relation1 1size of text files1gb1 mb
tpchimdbavg. result size of positive queries11number of join predicates11number of selection predicates11	table 1: data set characteristics	table 1: workload characteristicsthis heuristic  we compare it against a strategy that processes relations in a random order. we focus on the first stage of the algorithm and measure the effectiveness of the two strategies in terms of the size  in number of nodes  of the generated reference summary. for the random strategy  we execute several experiments using different seeds for the random number generator. this results in a large number of distinct reference summaries  each one corresponding to a different sequence of choices for the clustered relation.  the sequence that is derived by cv is among the random sequences that have been generated. 
random orderings percentilesmin1%1%1%maxcv1 1 1 1 1table 1: size distribution for random reference summaries.
﹛table 1 shows percentile information for the sizes of 1 random reference tugs for a scaled down version of our imdb data set. the size of the cv-based reference tug is shown in the last column. as our results indicate  the sequence of clustered relations affects significantly the size of the reference tug. the largest random tug  for instance  is more than twice as big as the smallest random tug  1 nodes vs. 1 nodes . we observe that the cv metric is very effective in guiding the clustering process  and results in a small reference synopsis that is very close to the minimum size achieved by random orderings  1 nodes vs. 1 nodes for the minimum .
all but one similarity. as we have discussed earlier  tugbuild relies on the novel concept of all-but-one similarity in order to perform lossless merges on the data graph gd. here  we evaluate the impact of this choice by comparing it against the use of complete similarity for the generation of the reference synopsis.
﹛table 1 shows the size of the reference summary for allbut-one and complete similarity on our two data sets. to put the measurements in context  we also list the size of the starting data graph. our results demonstrate that all-butone similarity enables a substantially higher compression rate  outperforming full similarity by two orders of magnitude in both cases. in the imdb data set  for instance  all-but-one similarity generates a reference summary of 1 thousand nodes  compared to 1 million nodes for full similarity. this level of compression is a key factor for the efficiency of the tugbuild algorithm  as it reduces significantly the size of the input for the second stage of the build process.
tug construction time. in this experiment  we evalu-
data graphfull sim.all-but-one sim.tpc-h1m1m1kimdb1m1m1ktable 1: size of reference summary  in number of nodes  for full and all-but-one similarity.
ate the efficiency of the complete tugbuild algorithm. we consider all three stages of the build process and measure the execution time of tugbuild in order to compress the original data graph down to a small synopsis of 1kb in storage. all measurements are made on an otherwise unloaded 1ghz pentium 1 machine. in this setting  tugbuild requires 1 minutes for tpc-h and 1 minutes for the more complex imdb data set. given the relatively large size of our data sets  these results validate the efficiency of tugbuild and the applicability of our techniques in practice.
1.1 tugs vs. existing techniques
﹛in this set of experiments  we compare our proposed tug synopses against previously proposed techniques for relational selectivity estimation. based on the applicability of different techniques  we split our experiments in two groups: for tpc-h  we compare tugs against histograms and join synopses  while for imdb  where join synopses cannot be applied  we compare tugs against histograms and multidimensional wavelets. in all cases  the available storage budget is set equal to the size of the histograms created by system x  approximately 1kb for tpc-h and 1kb for imdb .
tpch. figure 1 a  shows the relative error cfd for histograms  join synopses  and tugs for a workload of positive queries over the tpc-h data set. our results clearly demonstrate the effectiveness of tugs as accurate data synopses. given the small space budget of 1kb  tugs enable an estimation error of less than 1% for half of the queries in the workload. this level of accuracy is very effective if one considers the limited space budget  less than 1% of the original data size  and the complexity of queries in our workloads: 1-way to 1-way joins and up to 1 selection predicates. compared to join synopses  tugs enable accurate estimates for a larger part of the workload. as an example  tugs yield an estimation error of less than 1% for 1 queries in the workload  1% of the total number of queries   compared to 1 queries  1% of the total  for join synopses and the same level of accuracy.  histograms perform consistently worse than both tugs and join synopses.  essentially  join synopses are susceptible to high estimation errors when the sampling rate is low compared to the complexity of the data distribution. tugs  on the other hand  summarize the structure of the complete data graph and can thus capture effectively in limited space the key statistical traits of the data. it is interesting to note that join synopses perform better than tugs for errors greater than 1%. this is an artifact of the estimation algorithm of join synopses  which returns an estimate of 1 tuples  1% of error  when the sample does not yield any results for the query.

	 a 	 b 	 c 
figure 1: performance of tugs vs. existing techniques:  a  tpch  b  imdb  full   c  imdb  scaled-down .

percentilesmethod1%1%1%1%1%tug1.1.1.1 1histograms11.1 1 1 1join synop.111
percentiles - full imdbmethod1%1%1%1%1%tug1.1.1.1.1histograms1.1.1.1.1
percentiles - scaled down imdbtug1.1.1.1.1wavelets1.1.1.1.1	 a 	 b 
table 1: absolute estimation error for negative queries:  a  tpc-h   b  imdb.﹛table 1 a  shows the absolute error percentiles for the three methods  for a workload of negative queries over tpch. the clear winner in this case is join synopses  which by definition compute a perfect estimate of 1 tuples for any negative query. our tug synopses continue to perform well  yielding an absolute error of less than 1 tuples for 1% of the tested queries. we observe that tugs may generate estimates of high absolute error  up to 1k tuples   but these are restricted to a few outlier queries in our workload.
imdb. for our imdb data set  we compare the performance of tugs against histograms and multi-dimensional wavelets. recall that we do not apply join synopses on imdb  as they cannot model the many-to-many relationships that are found in the schema. we note that wavelet construction timed out on the full imdb data set and hence we present a separate set of results on a scaled-down version of imdb. for the smaller data set  we retained 1% of the wavelet coefficients for each table resulting in a wavelet summary of 1mb in size. we have experimented with wavelet summaries of smaller size  but we found that the accuracy of estimation deteriorated quickly below this large space budget.
﹛figure 1 b  shows the cfd of relative estimation error for histograms and tugs  for positive queries over the imdb data set and a space budget of 1kb. figure 1 c  shows the same metric for wavelets and tugs  for the scaleddown version of imdb. in both cases  tugs outperform the competitor techniques by a large margin. for the full imdb data set  tugs enable an estimation error of less than 1% for half of the tested queries  whereas the error of the histogram-based estimates is more than 1%. a similar picture appears in the scaled-down imdb data set  where tugs consistently yield more accurate estimates than multi-dimensional wavelets. as an example  tug estimates have less than 1% of estimation error for 1% of the workload  while wavelets can provide this level of accuracy for only 1% of the tested queries. as we discuss in section 1  the reason for the bad performance of wavelet summaries  and to some extent  of histograms  is the presence of key/foreign-key joins in the underlying schema. this type of joins essentially results in frequency matrices that include the keys of relations  and which are difficult to summarize effectively due to their sparseness.
﹛table 1 b  shows the absolute estimation error for the three techniques  for a workload of negative queries. tugs continue to outperform the other two techniques  yielding low-count estimates that are close to the true selectivity. we observe that wavelets enable competitive estimates to tugs  while histogram-based estimates are again very inaccurate for negative workloads.
﹛overall  our results demonstrate the benefits of our proposed tug model over existing techniques. as shown  tugs enable more accurate estimates in almost all cases  while being applicable to a large class of relational data sets.
1. related work
﹛relational data synopses have been investigated from the early years of database development and previous studies have introduced a host of relevant techniques. our review of prior work on this topic uses an abstract classification of the developed techniques in table-level synopses and schemalevel synopses.
﹛table-level synopses  such as  histograms  1  1  1   wavelets  1  1   sketches  1  1   and table samples  1  1   approximate the joint frequency distribution of values that appear in a single table.  hence  the optimizer estimates the selectivity of a join query by combining information from several synopses.  histograms and wavelets summarize the frequency matrix of the relation  and are most effective when the matrix contains relatively few contiguous regions comprising similar frequencies. these techniques therefore do not perform well for key/foreign-key joins  where the inclusion of a key attribute results in a frequency matrix with widely dispersed non-zero frequencies. similar arguments can be made for independent table samples  which have been shown to be ineffective for key/foreign-key join queries . sketch-based techniques  1  1  have been originally developed for approximate query answering over streaming data  and can be conceptually applied as single-pass summaries over finite data sets. these works  however  assume continuous queries with fixed selection predicates  and it is not clear if they can support effectively the ad-hoc queries that are common over finite data sets.
﹛schema-level synopses  such as  join synopses   also known as join samples  and probabilistic relational models  prms    approximate the joint distribution of both joins and values on a subset of tables. as a result  selectivity estimation is based on information from a single schemalevel synopsis that covers the query. join synopses  store uniform random samples of table joins  and compute estimates by scaling up the selectivity of the query over the sample. prms   on the other hand  rely on a bayesian network that approximates the joint frequency distribution across multiple tables and enables the inference of selectivity estimates. the two techniques can only be applied to join queries that do not cover many-to-many relationships  i.e.  queries that contain a well defined  central  relation with emanating foreign-key dependencies. in the opposite case  the use of these techniques would require the application of generalized independence assumptions that are typically unrealistic and thus limit the accuracy of the generated estimates.
﹛our proposed tug synopses fall in the second category of schema-level summaries. contrary to existing schema-level techniques  tugs can model data sets with complex join relationships  including many-to-many relationships and cycles  and thus cover a larger class of real-world schemata and query loads.
1. conclusions
﹛in this paper  we initiate the study of schema-level synopses for relational data sets with complex join relationships. we introduce a new class of summaries  termed tug synopses  that rely on a graph-based model in order to capture accurately the key statistical traits of the underlying data distribution. we detail the tug model and its properties  and describe an efficient algorithm for constructing accurate tugs for a limited space budget. our experimental results verify the effectiveness of our approach and demonstrate its benefits over existing summarization techniques.
