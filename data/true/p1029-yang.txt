map-reduce is a programming model that enables easy development of scalable parallel applications to process vast amounts of data on large clusters of commodity machines. through a simple interface with two functions  map and reduce  this model facilitates parallel implementation of many real-world tasks such as data processing for search engines and machine learning.
　however  this model does not directly support processing multiple related heterogeneous datasets. while processing relational data is a common need  this limitation causes difficulties and/or inefficiency when map-reduce is applied on relational operations like joins.
　we improve map-reduce into a new model called mapreduce-merge. it adds to map-reduce a merge phase that can efficiently merge data already partitioned and sorted  or hashed  by map and reduce modules. we also demonstrate that this new model can express relational algebra operators as well as implement several join algorithms.
categories and subject descriptors
d.1  programming techniques : concurrent programming-parallel programming; d.1  programming languages : language constructs and features-frameworks; h.1  database management : systems-parallel databases; relational databases
general terms
design  languages  management  performance  reliability
keywords
cluster  data processing  distributed  join  map-reduce  map-reduce-merge  parallel  relational  search engine
1. introduction
　search engines process and manage a vast amount of data collected from the entire world wide web. to do this task
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  beijing  china.
copyright 1 acm 1-1-1/1 ...$1.
efficiently at reasonable cost  instead of relying on generic dbms  they are usually built as customized parallel data processing systems and deployed on large clusters of sharednothing commodity nodes. in   based on his experience as inktomi  now part of yahoo!  co-founder  eric brewer advocated that building novel data-intensive systems  e.g.  search engines  should  apply the principles of databases  rather than the artifacts.  it was because dbms are usually overly generalized with many features that some can be unnecessary overhead for specific applications like search engine. hence  search engine companies have developed and operated on  simplified  distributed storage and parallel programming infrastructures. these include google's file system  gfs    map-reduce   bigtable ; ask.com's neptune  using the data aggregation call  dac  framework  ; and microsoft's dryad . yahoo! also has similar infrastructures. these infrastructures adopt only a selected subset of database principles  hence are  simplified   but they are sufficiently generic and effective that they can be easily adapted to data processing in search engines  machine learning  and bioinformatics. following these useful but proprietary  non-publicly released  infrastructures  hadoop is an open-source implementation  which is reminiscent of gfs and map-reduce  and is released under the umbrella of the apache software foundation.
　common to these infrastructures is the refactoring of data processing into two primitives:  a  a map function to process input key/value pairs and generate intermediate key/values  and  b  a reduce function to merge all intermediate pairs associated with the same key and then generate outputs. the dac framework has similar primitives  called local and reduce. these primitives allow users to develop and run parallel data processing tasks without worrying about the nuisance details of coordinating parallel sub-tasks and managing distributed file storage. this abstraction can greatly increase user productivity .
　though sufficiently generic to perform many real world tasks  the map-reduce framework is best at handling homogeneous datasets. as indicated in   joining multiple heterogeneous datasets does not quite fit into the map-reduce framework  although it still can be done with extra mapreduce steps. for example  users can map and reduce one dataset and read data from other datasets on the fly. in short  processing data relationships  which is what rdbms excel at  is perhaps not map-reduce's strong suit.
　for a search engine  many data processing problems can be easily solved using the map-reduce framework  but there are some tasks that are best modeled as joins. for example  a search engine usually stores crawled urls with their contents in a crawler database  inverted indexes in an index database  click or execution logs in a variety of log databases  and url linkages along with miscellaneous url properties in a webgraph database. these databases are gigantic and distributed over a large cluster of nodes. moreover  their creation takes data from multiple sources: index database needs both crawler and webgraph databases; a webgraph database needs both a crawler and a previous version of the webgraph database.
　to handle these tasks in the map-reduce framework  developers might end up writing awkward map/reduce code that processes one database while accessing others on the fly. alternatively they might treat these databases as homogeneous inputs to a map-reduce process but encode heterogeneity with an additional data-source attribute in the data and extra conditions in the code.
　processing data relationships is ubiquitous  especially in enterprise information systems. one major focus of the extremely popular relational algebra and rdbms is to model and manage data relationships efficiently. besides search engine tasks  another scenario of applying a join-enabled mapreduce framework is to join large databases across application  company  or even industry boundaries. for example  both airliners and hotel chains have huge databases. joining these databases can permit data miners to extract more comprehensive rules than they could individually. while many traditional  shared- or shared-nothing  cluster-based or mass parallel  rdbms have been deployed in enterprise olap systems  a join-enabled map-reduce system can provide a highly parallel yet cost effective alternative.
　based on these observations  we believe that one important improvement for the map-reduce framework is to include relational algebra in the subset of the database principles it upholds. that is  it should be further extended to support relational algebra primitives without sacrificing its existing generality and simplicity. the chief focus and contribution of this paper is this extension. we extend the mapreduce framework  shown in fig. 1  to the map-reducemerge framework  shown in fig. 1 . this new framework introduces a naming and configuring scheme that extends map-reduce to processing heterogeneous datasets simultaneously. it also adds a new merge phase that can join reduced outputs.
to recap  the contributions of this paper are as follows:
  abiding by map-reduce's  simplified  design philosophy  we augment the map-reduce framework by adding a merge phase  so that it is more efficient and easier to process data relationships among heterogeneous datasets.
note that  while map-reduce tasks are usually stacked to form a linear user-managed workflow  adding a new merge primitive can introduce a variety of hierarchical workflows for one data processing task. a mapreduce-merge workflow is comparable to a rdbms execution plan  but developers can embed programming logic in it and it is designed specifically for parallel data processing.
  in a parallel setting  relational operators can be modeled using various combinations of the three functionalprogramming-based primitives: map  reduce  and merge. with proper configurations  these three primitives can

figure 1: data and control flow for google's mapreduce framework. a driver program initiates a coordinator process. it remotely forks many mappers  then reducers. each mapper reads file splits from gfs  applies user-defined logic  and creates several output partitions  one for each reducer. a reducer reads remotely from every mapper  sorts  groups the data  applies user-defined logic  and sends outputs to gfs.
be used to implement the parallel versions of several join algorithms: sort-merge  hash  and block nestedloop.
　in   jim gray et al. emphasized that there must be a  synthesis of database systems and file systems   as  file systems grow to petabyte-scale archives with billions of files.  this vision not only applies to scientific data management  the focus of   but also applies to any data-intensive system such as a search engine. as stated in   google's map-reduce framework not only abstracts parallel programming from data processing tasks  but it also abstracts files as just  containers for data  through its set-oriented model. this  synthesis  vision echoes brewer's  principle  idea as map-reduce/gfs provides both views a great example of database-oriented data processing. jim gray et al. also envisioned that simplified data/programming models like google's map-reduce could evolve into more general ones in the coming decade. our map-reduce-merge proposal is a step towards that goal.
1. map-reduce
　google's map-reduce programming model and its underlying google file system  gfs  focus mainly to support search-engine-related data processing. it has a simple programming interface  and  though seemingly restricted  it is actually quite versatile and generic. it can extend to data processing tasks beyond the search-engine domain. according to   it has also been heavily applied within google for data-intensive applications such as machine learning.
1 features and principles
　contrary to traditional data processing and management systems  map-reduce and gfs are based on several unorthodox assumptions and counter-intuitive design principles:
  low-cost unreliable commodity hardware: instead of using expensive  high-performance  and reliable symmetric multiprocessing  smp  or massively

figure 1: data and control flow for the mapreduce-merge framework. the coordinator manages two sets of mappers and reducers. after these tasks are done  it launches a set of mergers that read outputs from selected reducers and merge them with user-defined logic.
parallel processing  mpp  machines equipped with highend network and storage subsystems  most search engines run on large clusters of commodity hardware. this hardware is managed and powered by open-source operating systems and utilities  so that the cost is low.
  extremely scalable rain cluster: instead of using centralized raid-based san or nas storage systems  every map-reduce node has its own local offthe-shelf hard drives. these nodes are loosely coupled in rackable systems connected with generic lan switches. loose coupling and shared-nothing architecture make map-reduce/gfs clusters highly scalable. these nodes can be taken out of service with almost no impact to still-running map-reduce jobs. these clusters are called redundant array of independent  and inexpensive  nodes  rain  . gfs is essentially a rain management system.
  fault-tolerant yet easy to administer: due to its high scalability  map-reduce jobs can run on clusters with thousands of nodes or even more. these nodes are not very reliable. at any point in time  a certain percentage of these commodity nodes or hard drives will be out of order. gfs and map-reduce are designed not to view this certain rate of failure as an anomaly; instead they use straightforward mechanisms to replicate data and launch backup tasks so as to keep still-running processes going. to handle crashed nodes  system administrators simply take crashed hardware off-line. new nodes can be plugged in at any time without much administrative hassle. there is no complicated backup  restore and recovery configurations and/or procedures like the ones that can be seen in many dbms.
  simplified and restricted yet powerful: mapreduce is a restricted programming model  it only provides straightforward map and reduce interfaces. however  most search-engine  and generic  data processing tasks can be effectively implemented in this model. these tasks can immediately enjoy high parallelism with only a few lines of administration and configuration code. this  simplified  philosophy can also be seen in many gfs designs. developers can focus on formulating their tasks to the map-reduce interface  without worrying about such issues as implementing memory management  file allocation  parallel  multithreaded  or network programming.
  highly parallel yet abstracted: the most important contribution of map-reduce is perhaps its automatic parallelization and execution. even though it might not be optimized for a specific task  the productivity gain from developing an application with mapreduce is far higher than doing it from scratch on the same requirements. map-reduce allows developers to focus mainly on the problem at hand rather than worrying about the administrative details.
  high throughput: deployed on low-cost hardware and modeled in simplified  generic frameworks  mapreduce systems are hardly optimized to perform like a massively parallel processing systems deployed with the same number of nodes. however  these disadvantages  or advantages  allow map-reduce jobs to run on thousands of nodes at relatively low cost. a scheduling system places each map and reduce task at a nearoptimal node  considering the vicinity to data and load balancing   so that many map-reduce tasks can share the same cluster.
  high performance by the large: even though
map-reduce systems are generic  and not usually tuned to be high performance for specific tasks  they still can achieve high performance simply by being deployed on a large number of nodes. in   the authors mentioned a then world-record terabyte  sorting benchmark by using map-reduce on thousands of machines. in short  sheer parallelism can generate high performance  and map-reduce programs can take advantage of it.
  shared-disk storage yet shared-nothing computing: in a map-reduce environment  every node has its own local hard drives. mappers and reducers use these local disks to store intermediate files and these files are read remotely by reducers  i.e.  mapreduce is a shared-nothing architecture. however 
map-reduce jobs read input from and write output to gfs  which is shared by every node. gfs replicates disk chunks and uses pooled disks to support ultra large files. map-reduce's shared-nothing architecture makes it much more scalable than one that shares disk or memory. in the mean time  map and reduce tasks share an integrated gfs that makes thousands of disks behave like one.
  set-oriented keys and values; file abstracted: with gfs's help  map-reduce can process thousands of file chunks in parallel. the volume can be far beyond the size limit set for an individual file by the underlying os file system. developers see data as keys and values  no longer raw bits and bytes  nor file descriptors.
  functional programming primitives: the mapreduce interface is based on two functional-programming primitives . their signatures are re-produced here:
map:  k1 v1  ★   k1 v1  
reduce:  k1  v1   ★  v1 
the map function applies user-defined logic on every input key/value pair and transforms it into a list of intermediate key/value pairs. the reduce function applies user-defined logic to all intermediate values associated with the same intermediate key and produces a list of output values. this simplified interface enables developers to model their specific data processing into two-phase parallel tasks.
these signatures were informally defined for readability  they were not meant to be rigorous enough to pass a strongly-typed functional type checking mechanism. however   pointed out that the reduce function output  v1  can be in different type from its input  v1 .
  distributed partitioning/sorting framework: map-reduce system also includes phases that work on the intermediate data  and users usually do not need to deal with them directly. these phases include a partitioner function that partitions mapper outputs to reducer inputs  a sort-by-key function that sorts reducer inputs based on keys  and a group-by-key function that groups sorted key/value pairs with the same key into a single key/value pair of the same key and all the values. in its pure form  the system is essentially a 1-phase parallel sorter similar to the one in now .
  designed for search engine operations yet applicable to generic data processing tasks: mapreduce is a generic framework  not limited to search engine operations. it can be applied to any data processing task that fits the simple map-reduce interface.
1 homogenization
　despite all these advantages and design principles  mapreduce focuses mainly on processing homogeneous datasets. through a process we called homogenization  map-reduce can be used to do equi-joins on multiple heterogeneous datasets. this homogenization process applies one map/reduce task on each dataset that it inserts a data-source tag into every value. it also extracts a key attribute common for all heterogeneous datasets. transformed datasets now have two common attributes: key and data-source - they are homogenized. a final map/reduce task can then apply to all the homogenized datasets combined. data entries from different datasets with the same key value will be grouped in the same reduce partition. user-defined logic can extract data-sources from values to identify their origins  then the entries from different sources can be merged.
　this procedure takes lots of extra disk space  incurs excessive map-reduce communications  and is limited only to queries that can be rendered as equi-joins. in the next section  we will discuss a general approach of extending map-reduce to efficiently process multiple heterogeneous datasets.
1. map-reduce-merge
　the map-reduce-merge model enables processing multiple heterogeneous datasets. the signatures of the mapreduce-merge primitives are listed below  where α  β  γ represent dataset lineages  k means keys  and v stands for value entities.
map:  k1 v1 α ★   k1 v1  α
reduce:  k1  v1  α ★  k1  v1  α
merge:   k1  v1  α  k1  v1  β  ★   k1 v1  γ
　in this new model  the map function transforms an input key/value pair  k1 v1  into a list of intermediate key/value pairs   k1 v1  . the reduce function aggregates the list of values  v1  associated with k1 and produces a list of values  v1   which is also associated with k1. note that inputs and outputs of both functions belong to the same lineage  say α. another pair of map and reduce functions produce the intermediate output  k1  v1   from another lineage  say β. based on keys k1 and k1  the merge function combines the two reduced outputs from different lineages into a list of key/value outputs   k1 v1  . this final output becomes a new lineage  say γ. if α = β  then this merge function does a self-merge  similar to self-join in relational algebra.
　notice that the map and reduce signatures in the new model are almost the same as those in the original mapreduce. the only differences are the lineages of the datasets and the production of a key/value list from reduce instead of just values. these changes are introduced because the merge function needs input datasets organized  partitioned  then either sorted or hashed  by keys and these keys have to be passed into the function to be merged. in google's mapreduce  the reduced output is final  so users pack whatever needed in  v1   while passing k1 for next stage is not required.
　to build a merge function that reads data from both lineages in an organized manner  the design of these signatures emphasizes having the key k1 passed from map to reduce  then to merge functions. this is to make sure that data is partitioned  then sorted  or hashed  on the same keys before they can be merged properly. this condition  however  is too strong. keys still can be transformed between phases and they do not even need to be of the same type  as implied by the same type descriptor k1 used in every phase  as long as records pointed by transformed keys are still organized in the same way as the one by the mapped keys represented by k1. for example  1-digit integers can be transformed into 1byte numerical strings padded with 1s. the order of integers and the one for transformed strings are the same  so they are compatible and replaceable between phases if compatible range partitioners are used in map functions. however  since users already can transform keys in the map function  from k1 to k1   there is hardly a need to transform them again in reduce and merge functions. thus  to keep these signatures simple  we chose to have the same k1 passed between phases.
　as mentioned in   the map and reduce functions originate from functional programming. the merge function can be related to two-dimensional list comprehension  which is also popular in functional programming.
1 example
function  processor function  partition selector  and configurable iterator. we will use the employee-bonus example to explain the data and control flow of this framework and how these components collaborate.
the merge function  merger  is like map or reduce  in　in this section  we start with a simple example that will be continued to next sections. it shows how map  reduce  and merge modules work together. there are two datasets in this example: employee and department. employee's  key  attribute is emp id and the others are packed into an emp info  value.  department's  key  is dept id and the figure 1: example to join employee and department tables and compute employee bonuses  see
′ 1 .
algorithm 1 map function for the employee dataset.
1: map const key& key  /* emp id */
1:	const value& value /* emp info */  {
1:	emp id = key;
1:	dept id = value.dept id;
1:	/* compute bonus using emp info */
1:	output key =  dept id  emp id ;
1:	output value =  bonus ; 1:	emit output key  output value ;
1: }

others are packed into a dept info  value.  one example query is to join these two datasets and compute employee bonuses.
　before these two datasets are joined in a merger  they are first processed by a pair of mappers and reducers. a complete data flow is shown in fig. 1. on the left hand side  a mapper reads employee entries and computes a bonus for each entry. a reducer then sums up these bonuses for every employee and sorts them by dept id  then emp id. on the right hand side  a mapper reads department entries and computes bonus adjustments. a reducer then sorts these department entries. at the end  a merger matches the output records from the two reducers on dept id using the sortmerge algorithm  applies a department-based bonus adjustment on employee bonuses. pseudocode for these mappers and reducers are shown in alg. 1  1  1  and 1.
　after these two pairs of map-reduce tasks are finished  a merger task takes their intermediate outputs  and joins them on dept id. we will describe the details of major merge components in following sections.
1 implementation
　we have implemented a map-reduce-merge framework  in which map and reduce components are inherited from google map-reduce except minor signature changes. the new merge module includes several new components: merge ing logic. while a call to a map function  mapper  processes a key/value pair  and a call to a reduce function  reducer  processes a key-grouped value collection  a merger processes two pairs of key/values  that each comes from a distinguishable source.
　at the merge phase  users might want to apply different data-processing logic on data based on their sources. an example is the build and probe phases of a hash join  where build programming logic is applied on one table then probe the other. to accommodate this pattern  a processor is a user-defined function that processes data from one source only. users can define two processors in merge.
after map and reduce tasks are about done  a map-reduce-
merge coordinator launches mergers on a cluster of nodes
 see fig. 1 . when a merger starts up  it is assigned with a merger number. using this number  a user-definable module called partition selector can determine from which reducers this merger retrieves its input data. mappers and reducers are also assigned with a number. for mappers  this number represents the input file split. for reducers  this number represents an input bucket  in which mappers partition and store their output data to. for map-reduce users  these numbers are simply system implementation detail  but in map-reduce-merge  users utilize these numbers to associate input/output between mergers and reducers in partition selectors.

figure 1: a 1-way map-reduce-merge data flow. data is processed by a mapper  partitioner  and combiner in the map phase. then  it is read remotely and processed by a sorter and reducer in the reduce phase. in the merge phase  selected reducer outputs are processed by a matcher and merger guided by a pair of　like mappers and reducers  a merger can be considered as having logical iterators that read data from inputs. each mapper and reducer have one logical iterator and it moves from the begin to the end of a data stream  which is an input file split for a mapper  or a merge-sorted stream for a reducer. a merger reads data from two sources  so it can be viewed as having two logical iterators. these iterators usually move forward as their mapper/reducer counterparts  but their relative movement against each others can be instrumented to implement a user-defined merge algorithm. our map-reduce-merge framework provides a user-configurable module  iterator-manager  that it is called for the information that controls the movement of these configurable iterators. later  we will describe several iteration patterns from relational join algorithms. a merge phase driver  as shown in alg. 1  is needed to coordinate these configurable iterators.
algorithm 1 reduce function for the department dataset.
1: reduce const key& key  /*  dept id  */
1:	const valueiterator& value
1:	/* an iterator on a bonus adjustments collection */  {
1:
1:
1:
merge components and have them collaborate with each others.
1.1 partition selector
　in a merger  a user-defined partition selector function determines which data partitions produced by up-stream reducers should be retrieved then merged. this function is given the current merger's number and two collections of reducer numbers  one for each data source. users define logic in the selector to remove unrelated reducers from the collections. only the data from the reducers left in the collections will be read and merged in the merger.
　for the employee-bonus example  a simplified scenario stipulates that both sources have the same collection of reducer numbers and the same range partitioner function is applied to the dept id key only in both mappers  so that both reducer outputs are completely sorted and partitioned into equal number of buckets. notice that the employee mapper produces keys in pairs of  dept id  emp id   thus its reducer sorts data on this composite key  but partitioning is done on dept id only. based on these assumptions  a partition selector function can be defined to map reducers and mergers in an one-to-one relationship as in alg. 1.
1.1 processors
　a processor is the place where users can define logic of processing data from an individual source. processors can be defined if the hash join algorithm is implemented in merge  where the first processor builds a hash table on the first source  and the second probes it while iterating through the second data source. in this case  the merger function is empty. since we will apply the sort-merge algorithm on the bonus-computation join example  these processors stay empty.
1.1 merger
　in the merge function  users can implement data processing logic on data merged from two sources where this data satisfies a merge condition. alg. 1 shows the last step of computing employee bonuses by adjusting an employee's raw bonus with a department-based adjustment.
1.1 configurable iterators
　as indicated  by manipulating relative iteration of a merger's two logical iterators  users can implement different merge algorithms.
　for algorithms like nested-loop joins  iterators are configured to move as looping variables in a nested loop. for algorithms like sort-merge joins  iterators take turns when iterating over two sorted collections of records. for hashjoin-like algorithms  these two iterators scan over their data in separate passes. the first scans its data and builds a hash table  then the second scans its data and probes the already built hash table.
　allowing users to control iterator movement increases the risk of running into a never-ending loop. this risk always exalgorithm 1 merge phase driver.
1: partitionselector partitionselector; // user-defined logic
1: leftprocessor leftprocessor; // user-defined logic
1: rightprocessor rightprocessor; // user-defined logic
1: merger merger; // user-defined logic
1: iteratormanager iteratormanager; // user-defined logic
1: int mergernumber; // assigned by system
1: vector int  leftreducernumbers; // assigned by system 1: vector int  rightreducernumbers; // assigned by system
1: // select and filter left and right reducer outputs for this merger
1: partitionselector.select mergernumber 
1:	leftreducernumbers 
1:	rightreducernumbers ;
1: configurableiterator left = /*initiated to point to entries
1:	in reduce outputs by leftreducernumbers*/
1: configurableiterator right =/*initiated to point to entries
1:	in reduce outputs by rightreducernumbers*/
1: while true  {
1:	pair bool bool  hasmoretuples =
1:	make pair hasnext left   hasnext right  ;
1:	if  !hasmoretuples.first && !hasmoretuples.second  {break;}
1:	if  hasmoretuples.first  {
1:	leftprocessor.process left★key  left★value ; }
1:	if  hasmoretuples.second  {
1:	rightprocessor.process right★key  right★value ; }
1:	if  hasmoretuples.first && hasmoretuples.second  {
1:	merger.merge left★key  left★value 
1:	right★key  right★value ; }
1:	pair bool bool  iteratornextmove =
1:	iteratormanager.move left★key  right★key  hasmoretuples ;
1:	if  !iteratornextmove.first && !iteratornextmove.second  {
1:	break; }
1:	if  iteratornextmove.first  { left++; } 1:	if  iteratornextmove.second  { right++; }
1: }
algorithm 1 one-to-one partition selector.
1: bool select int mergernumber 
1:	vector int & leftreducernumbers 
1:	vector int & rigthreducernumbers  {
1:	if  find leftreducernumbers.begin   
1:	leftreducernumbers.end   
1:	mergernumber  == leftreducernumbers.end    {
1:	return false; }
1:	if  find rightreducernumbers.begin   
1:	rightreducernumbers.end   
1:	mergernumber  == rightreducernumbers.end    {
1:	return false; }
1:	leftreducernumbers.clear  ;
1:	leftreducernumbers. push back mergernumber ;
1:	rightreducernumbers.clear  ;
1:	rightreducernumbers. push back mergernumber ;
1: return true; 1: }

ists in user-defined logic and is a great concern  especially in strictly-regulated dbms systems. for programming models like the map-reduce and map-reduce-merge  this issue is lesser because they are  after all  programming models and data processing frameworks.
　still  it is a nuisance if a task never ends  so a framework should provide a mechanism to reduce the chance of it happening. in our implementation  we use a boolean pair returned by a user-defined function to indicate whether to move an iterator to point to the next entity. this function is called after each merge operation; true indicates forward and false indicates stay. if both booleans are false  then the whole merge process is terminated.
　suppose reducers produce sorted outputs in an ascendant order  alg. 1 shows the programming logic of coordinating iterator movement for sort-merge-alike algorithms. if both sources still have inputs  then move the iterator that points to a smaller key. if both keys are equivalent  then move the algorithm 1 merge function for the employee-department join.
1: merge const leftkey& leftkey 
1:/*  dept id  emp id  */1:const leftvalue& leftvalue  /* sum of bonuses */1:const rightkey& rightkey  /* dept id */1:const rightvalue& rightvalue /* bonus-adjustment */ {1:if  leftkey.dept id == rightkey  {1:bonus = leftvalue * rightvalue;1:emit leftkey.emp id  bonus ; }1: }

1:const rightkey& rightkey 1:const pair bool  bool & hasmoretuples  {1:if  hasmoretuples.first && hasmoretuples.second  {1:if  leftkey   rightkey  {1:return make pair true  false ; }1:return make pair false  true ; }1:return hasmoretuples;1: }
right iterator by default. if one source is exhausted  this information is stored in the input bool pair  hasmoretuples   move the iterator for the source that still has data.
　alg. 1 is an implementation of nested-loop iteration pattern. in a nested loop  keys are ignored in determining how to move iterators. if the left and right sources are exhausted  then the merge process is terminated. it is a logic error if the right source still have data when the left is exhausted. if the left source is not exhausted  then move the right iterator only. when the right source is exhausted  move the left iterator and reset the right iterator to the beginning of its data source.
　to implement algorithms that follow the hash join's twoscan iteration pattern  a merger first scans one data source from the beginning to the end  then repeats the scan on the other one  e.g.  see alg. 1.
　notice that  for the employee-bonus example  implementing configurable iterators is tied to the choosing of partitioners. using the sort-merge-based configurable iterators requires a range partitioner in both mappers.
1. applications to relational data processing
　one fundamental idea of map-reduce-merge is to bring relational operations into parallel data processing at the search-engine scale. on the other hand  map  reduce  and merge can be used as standardized components in implementing parallel olap dbms. novel data-processing applications such as search engines and map-reduce's unorthodox principles and assumptions make it worthwhile to revisit parallel databases  1  1 .
1 map-reduce-merge implementations of relational operators
　in our implementation  the map-reduce-merge model assumes that a dataset is mapped into a relation r with an attribute set  schema  a. in map  reduce  and merge functions  users choose attributes from a to form two subsets: k and v . k represents the schema of the  key  part of a mapreduce-merge record and v the  value  part. for each tuple t of r  this implies that t is concatenated by two field sets: k algorithm 1 iteration logic for nested-loop joins.
1: move const leftkey& leftkey 
1:	const rightkey& rightkey 
1:	const pair bool  bool & hasmoretuples  {
1:	if  !hasmoretuples.first && !hasmoretuples.second  {
1:	return make pair false  false ; }
1: if  !hasmoretuples.first && hasmoretuples.second  1: /* throw a logical-error exception */ 1: if  hasmoretuples.first && !hasmoretuples.second  {
1:	/* reset the right iterator to the beginning */
1: return make pair true  false ; } 1: return make pair false  true ;
1: }
algorithm 1 iteration logic for hash joins.
1: move const leftkey& leftkey 
1:	const rightkey& rightkey 
1:	const pair bool  bool & hasmoretuples  {
1:	if  !hasmoretuples.first && !hasmoretuples.second {
1:	return make pair false  false ; }
1:	if  hasmoretuples.first  {
1: return make pair true  false ; } 1: return make pair false  true ;
1: }

and v  where k is the schema of k and v is the schema of v. it so happens that map-reduce-merge calls k as  key  and v as  value . this naming is arbitrary in the sense that their attribute sets are decided solely by the user. this  key  is used in map-reduce-merge functions for partitioning  sorting  grouping  matching  and merging tuples. by no means it has the same uniqueness meaning in relational languages. below we describe how map-reduce-merge can be used to implement primitive and some derived relational operators  so that map-reduce-merge is relationally complete  while being load-balanced  scalable  and parallel.
  projection: for each tuple t =  k v  of the input relation  users can define a mapper to transform it into a projected output tuple t1 =  k1 v1   where k1 and v1 are typed by schema k1 and v 1  respectively. k1 and v 1 are subsets of a. namely  using mappers only can implement relational algebra's projection operator.
  aggregation: at the reduce phase  map-reduce  as well as map-reduce-merge  performs the sort-by-key and group-by-key functions to ensure that the input to a reducer is a set of tuples t =  k  v   in which  v  is the collection of all the values associated with the key k. a reducer can call aggregate functions on this grouped value list. namely  reducers can easily implement the  group by  clause and  aggregate  operators in sql.
  generalized selection: mappers  reducers  and mergers can all act as filters and implement the selection operator. if a selection condition is on attributes of one data source  then it can be implemented in mappers. if a selection condition is on aggregates or a group of values from one data source  then it can be implemented in reducers. if a selection condition involves attributes or aggregates from more than one sources  then it can be implemented in mergers.
straightforward filtering conditions that involve only one relation in a sql query's  where  and  having  clauses can be implemented using mappers and reducers  respectively. mergers can implement complicated filtering conditions involving more than one relations  however  this filtering can only be accomplished after join  or cartesian product  operations are properly configured and executed.
  joins: ′ 1 describes in detail how joins can be implemented using mergers with the help from mappers and reducers.
  set union: assume the union operation  as well as other set operations described below  is performed over two relations. in map-reduce-merge  each relation will be processed by map-reduce  and the sorted and grouped outputs of the reducers will be given to a merger. in each reducer  duplicated tuples from the same source can be skipped easily. the mappers for the two sources should share the same range partitioner  so that a merger can receive records within the same key range from the two reducers. the merger can then iterate on each input simultaneously and produce only one tuple if two input tuples from different sources
are duplicates. non-duplicated tuples are produced by this merger as well.
  set intersection: first  partitioned and sorted mapreduce outputs are sent to mergers as described in the last item. a merger can then iterate on each input simultaneously and produce tuples that are shared by the two reducer outputs.
  set difference: first  partitioned and sorted mapreduce outputs are sent to mergers as described in the last item. a merger can then iterate on each input simultaneously and produce tuples that are the difference of the two reducer outputs.
  cartesian product: in a map-reduce-merge task  the two reducer sets will produce two sets of reduced partitions. a merger is configured to receive one partition from the first reducer  f  and the complete set of partitions from the second one  s . this merger can then form a nested loop to merge records in the sole f partition with the ones in every s partition.
  rename: it is trivial to emulate rename in mapreduce-merge  since map  reduce  and merge functions can select  rearrange  compare  and process attributes based on their indexes in the  key  and  value  subsets.
map-reduce-merge is certainly more expressive than the relational algebra  since map  reduce  and merge can all contain user-defined programming logic.
1 map-reduce-merge implementations of relational join algorithms
　join is perhaps the most important relational operator. in this section  we will describe how map-reduce-merge can implement three most common join algorithms.
1.1 sort-merge join
　from   map-reduce is shown to be an effective parallel sorter. the key of sorting is to partition input records based on their actual values instead of  by map-reduce default  hashed values. that is  instead of using a hash partitioner  users can configure the framework to use a range partitioner in mappers. using this map-reduce-based sorter  the mapreduce-merge framework can be implemented as a parallel  sort-merge join operator. the programming logic for each phase is:
  map: use a range partitioner in mappers  so that records are partitioned into ordered buckets  each is over a mutually exclusive key range and is designated to one reducer.
  reduce: for each map-reduce lineage  a reducer reads the designated buckets from all the mappers. data in these buckets are then merged into a sorted set. this sorting procedure can be done completely at the reducer side  if necessary  through an external sort. or  mappers can sort data in each buckets before sending them to reducers. reducers can then just do the merge part of the merge sort using a priority queue.
  merge: a merger reads from two sets of reducer outputs that cover the same key range. since these reducer outputs are sorted already  this merger simply does the merge part of the sort-merge join.
1.1 hash join
　one important issue in distributed computing and parallel databases is to keep workload and storage balanced among nodes. one strategy is to disseminate records to nodes based on their hash values. this strategy is very popular in search engines as well as in parallel databases. it is the the default partitioning mechanism in map-reduce  and the only partitioning strategy in teradata   a parallel rdbms. another approach is to run a preprocessing map-reduce task to scan the whole dataset and build a data density . this density can be used by partitioners in later map-reduce tasks to ensure balanced workload among nodes. here we show how to implement hash join  using the map-reduce-merge framework:
  map: use a common hash partitioner in both mappers  so that records are partitioned into hashed buckets  each is designated to one reducer.
  reduce: for each map-reduce lineage  a reducer reads from every mapper for one designated partition. using the same hash function from the partitioner  records from these partitions can be grouped and aggregated using a hash table. this hash-based grouping is an alternative to the default sorting-based approach. it does not need a sorter  but requires maintaining a hashtable either in memory or disk.
  merge: a merger reads from two sets of reducer outputs that share the same hashing buckets. one is used as a build set and the other probe. after the partitioning and grouping are done by mappers and reducers  the build set can be quite small  so these sets can be hash-joined in memory. notice that  the number of reduce/merge sets must be set to an optimally large number in order to support an in-memory hash join  otherwise  an external hash join is required.
1.1 block nested-loop join
　the map-reduce-merge implementation of the block nestedloop join algorithm is very similar to the one for the hash join. instead of doing an in-memory hash  a nested loop is implemented. the partitioning and grouping done by mappers and reducers concentrate the join sets  so this parallel nested-loop join can enjoy a high selectivity in each merger.
  map: same as the one for the hash join.
  reduce: same as the one for the hash join.
  merge: same as the one for the hash join  but a nested-loop join is implemented  instead of a hash join.
1. optimizations
　map-reduce provides several optimization mechanisms  including locality and backup tasks . in this section  we describe some strategies that can reduce resources  e.g  the number of network connections and disk bandwidth  used in the merge phase.
1 optimal reduce-merge connections
　for a natural join over two datasets  a and b  suppose for a  there are ma number of mappers and ra number of reducers; and for b  mb and rb. each a mapper produces ra partitions  and each b mapper rb. conversely  each a reducer reads from every a mappers for the partitions designated for it. same applies to b reducers from b mappers. to simplify the scenario  let ra = rb = r  then in total there would be at least r 〜  ma + mb  remote reads  not counting redundant connections incurred by backup jobs  among nodes where mappers and reducers reside. this is a lot of remote reads among nodes  but it is the price to pay to group and aggregate same-key records as these records were originally scattered around in the whole cluster.
　for mergers  because data is already partitioned and even sorted after map and reduce phases  they do not need to connect to every reducer in order to get their data. the selector function in mergers can choose pertinent reduced partitions for merging. for example  in a simplified scenario  if there is also r number of mergers  then these mergers can have an one-to-one association with a reducers and also with b reducers. a user-defined selector can be like the one shown in alg. 1. this selector receives two collections of reducer numbers for a and b reducers. it then picks the reducers who share the same number with the merger and removes other reducers' numbers from the collections. the merger then uses the selected reducer numbers to set up connections with and requests data from these reducers. in the one-to-one case  the number of connections between reducers and mergers is 1r.
　if one input dataset is much larger than the other  then it would be inefficient to partition both datasets into the same number of reducers. one can choose different numbers for ra and rb  but the selection logic is more complicated.
　selector logic can also be quite complicated in the case of θ-join. however  selector is a optimization mechanism that can help avoid excessive remote reads. a naive selection can always put only the merger number in one reducer number set and leave the other set intact  see the selection logic in 1  and still get the correct result. this is basically a cartesian product between two reduced sets. the number of remote reads now becomes r1 + r.
　before feeding data from selected reducer partitions to a user-defined merger function  these tuples can be compared and see if they should be merged or not. in short  this simply a fine-grained selector.
1 combining phases
　to accomplish a data processing task  it usually takes several map-reduce-merge  or map-reduce  processes weaved in a workflow  in which the output of a process become the input of a subsequent one. the entire workflow may constitute many disk-read-write passes. for example  fig. 1 shows a tpc-h q1 join tree implemented with 1 map-reduce-
merge passes. these passes can be optimized and combined:
  reducemap  mergemap: reducer and merger outputs are usually fed into a down-stream mapper for a subsequent join operation. these outputs can simply be sent directly to a co-located mapper in the same process without storing them in secondary storage first.
  reducemerge: a merger usually takes two sets of reducer partitions. this merger can be combined with one of the reducers and gets its output directly while remotely reads data from the other set of reducers.
  reducemergemap: an straightforward combination of reducemerge and mergemap becomes reducemergemap.
　another way of reducing disk accesses is to replace disk read-writes with network read-writes. this method requires connecting up- and down-stream map-reduce-merge processes while they are running. this approach is arguably more complicated than saving intermediate data in local disks  thus it may not comply with the  simplified  philosophy of the map-reduce framework. when a process fails  this network-based i/o strategy can cause difficulties for upstream processes to recollect the data already computed and resend them to a new down-stream process.
1. enhancements
　besides optimizations  some map-reduce-merge enhancements can make coding easier.
1 map-reduce-merge library
　there are many variations and patterns for the merge module  such as the ones that implement relational operators or join algorithms. the selectors and configurable iterators for these common merge implementations can be put into a library and users can use them in their map-reducemerge tasks without reinventing the wheel.
a typical 1-pass map-reduce workflow. the middle one is a typical 1-pass map-reduce-merge workflow. the right is a multi-pass hierarchical workflow built with several map  reduce  and merge modules.
1 map-reduce-merge workflow
　map-reduce programs follow a strict two-phase workflow  i.e.  mapping then reducing. users have options to change default configurations  but some basic operations such as partitioning and sorting  are built-ins and cannot be skipped. this  sometimes  is a nuisance if users would like to do mapping only or to do reducing on already mapped datasets. these scenarios are quite common in real-world tasks as well as in debugging only one of the map-reduce modules. this constraint makes map-reduce simplified and it enables unified usage and implementation  but advanced users may want to see it relaxed  i.e.  they may want to create a customized workflow. since there are only two phases in map-reduce  it is not a serious issue. however  adding a new phase  merge  as proposed in this paper; future mapreduce improvements might include other phases  creates many workflow combinations that can fit the specific needs of a data processing task. this is especially true for processing relational queries where an execution plan constitutes a workflow of several map  reduce  and merge modules  see an example in ′ 1 .
　a map-reduce-merge enhancement is to provide a configuration api for building a customized workflow. in fig. 1  the left is a traditional map-reduce workflow. the middle one is a basic map-reduce-merge workflow. the left one is a more complicated example.
　when building a map-reduce-merge workflow  an important issue is to avoid using a distributed file system  dfs  for storing intermediate data. in google's map-reduce implementation  mapper outputs are stored in local hard drives  instead of in gfs. gfs is only used to store permanent datasets like the inputs and outputs of a map-reduce task. if a map-reduce implementation stores intermediate datasets in dfs  then it basically becomes a shared-disk architecture. this might make it not as scalable as a shared-nothing implementation .
　although we have only discussed hierarchical workflows so far  in fact  outputs can be used as inputs in a map-reducemerge workflow  making it recursive. these recursive workflows can be used to implement sql recursive queries  for example.
1. case studies
in this section  we will present two case studies applying the map-reduce-merge programming model to real-world data processing tasks. the first is a search-engine task  while the second is a rather complicated tpc-h query.
1 join webgraphs
　in simple terms  a webgraph database for a search engine stores a table in which each row has one url  regarded as the key  along with attributes such as its inlinks and outlinks. the number of attributes can be large  and for many operations  only a few of them are needed. as such  a webgraph database may store each column of the table in a separate file  distributed over many machines. this choice of storage creates a need for joins. as an example  consider the following three columns: urls  inlinks  and outlinks. suppose for each url  we need to compute the intersection of its inlinks and outlinks. one way to compute the intersection is  1  to create a table of all three columns  url  inlinks  outlinks   and  1  compute the intersection over each row and output  url  inlinks intersect outlinks . records of these columns are related to each other through row-ids. these row-ids are used in place of urls as keys to these column files. the creation of the joined table can be implemented with two 1-way joins:  1  join urls and inlinks using the row-ids as the common attribute   1  join the first join's result dataset and outlinks using row-ids as the common attribute. then  a simple map-reduce can scan the result dataset and find the inlink-outlink intersection.
　these collections of inlinks and outlinks can be considered as nested tables. as the number of inlinks and outlinks for popular websites  e.g.  www.yahoo.com  can be very large that reading them directly into a map  reduce  or merge process can overflow buffer. a safer approach is to flatten these nested tables and replicate row-id to every inlink  or outlink  record that belongs to the same url. one sortmerge-based intersect can produce records  row-id  inoutlink  that are shared by both  row-id  inlink  and  row-id  outlink  datasets. an ensuing map-reduce-merge natural join with the  row-id  url  dataset can than replace row-ids with urls and create the result dataset:  url  inoutlink .
1 map-reduce-merge workflow for tpc-h query 1
　to demonstrate how the map-reduce-merge programming model can be used to process complicated data relationships  we use the tpc-h  schema and its no. 1 query  see fig. 1  as an example.
　this query is rather complicated. it involves five tables  one nested query  one aggregate and group by clause  and at the end  the result dataset is ordered by several attributes. the conditions for the 1-way join are all equal conditions  while the nested query is only meant to select the tuples with the minimum supply cost. though this nested query is also a 1-way join  1 tables in the from clause and one outer table   because it is essentially the same as the outer join  its logic can be processed during executing the outer one. based on these observations  we use an execution plan that first does four 1-way joins for the overall 1-way join. then  this plan does group-by and selection operations for the nested query and a sorting operation for the order-by clause. the join tree of this execution plan is shown in fig. 1. this plan might not be the most efficient one. we just use it as an example for implementing a sql query under the map-reduce-merge framework. notice that the region and
-- tpc-h/tpc-r minimum cost supplier query  q1  select s acctbal  s name  n name  p partkey  p mfgr  s address  s phone  s comment
from
part  supplier  partsupp  nation  region
where
p partkey = ps partkey and s suppkey = ps suppkey and p size = :1 and p type like '%:1' and s nationkey = n nationkey and n regionkey = r regionkey and r name = ':1' and ps supplycost =   select min ps supplycost 
from
partsupp  supplier  nation  region
where
p partkey = ps partkey and s suppkey = ps suppkey and s nationkey = n nationkey and n regionkey = r regionkey and r name = ':1'
　  order by s acctbal desc  n name  s name  p partkey;
figure 1: tpc-h query 1.
nation tables are very small. they do not need a parallel join implementation with a complete suite of map  reduce  and merge tasks. in fact  they can be read into memory as look-up tables by mappers for other tables  such as supplier.
　in the join tree  part and partsupp are joined into a temporary table called p ps. in parallel  region and nation are joined into n r. table n r are then joined with supplier into s n r. later  p ps and s n r are joined into p ps s n r. once these four 1-way joins are done for the overall 1-way join  p ps s n r is processed by two map-reduce tasks. the first one does the nested query's group by clause and its reducer selects the tuples with the minimum supply-cost. the final map-reduce task is simply a sorter for the order by clause.
　in fig. 1  we mechanically replace each join with a suite of map  reduce  and merge tasks. thirteen disk-read-write passes are needed to process the execution plan. in total  there are 1 mappers  1 reducers  and 1 mergers.
　these numbers can be reduced by a simple optimization that integrates merger and reducer modules with a followup mapper. this optimization reduces the number of passes to 1 with 1 mappers  1 reducers  1 merge-mappers  and 1 reduce-mapper.
　if reducers and their follow-up mergers are further combined as suggested in ′ 1  then the number of passes is reduced to 1 with 1 mappers  1 reducer  1 reduce-mergemappers  and 1 reduce-mapper  see fig. 1 .

figure 1: a join tree for tpc-h query 1. it is implemented with 1 passes of map-reduce-merge modules  1 mappers  1 reducers  and 1 mergers .

figure 1: the join tree of fig. 1 is re-implemented with 1 passes of combined map-reduce-merge modules  1 mappers  1 reduce-merge-mappers  1 reducemapper  and 1 reducer .
1. conclusions
　map-reduce and gfs represent a rethinking of data processing that uses only the most critical database principles for their target applications  instead of relying on overly generalized dbms. this  simplified  philosophy drives down hardware and software cost for data-intensive systems such as search engines  while map-reduce still provides great features like high-throughput  high-performance  fault-tolerant  and easy administration  etc. the most important feature of map-reduce is that it abstracts parallel programming into two simple primitives  map and reduce  so that developers can easily convert many real-world data processing jobs into parallel programs.
　however  map-reduce does not directly support joins of heterogeneous datasets  so we propose adding a merge phase. this new map-reduce-merge programming model retains map-reduce's many great features  while adding relational algebra to the list of database principles it upholds. it also contains several configurable components that enable many data-processing patterns.
　map-reduce-merge can also be used as an infrastructure that supports parallel database functionality. we have demonstrated that the map-reduce-merge framework can be used to implement many relational operators  particularly joins. a natural next step is to develop an sql-like interface and an optimizer to simplify the process of developing a mapreduce-merge workflow. this work can readily reuse wellstudied rdbms techniques.
　acknowledgments. we would like to thank reviewers and yahoo! search colleagues for suggestions and discussions.
