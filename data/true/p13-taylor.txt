in many data sharing settings  such as within the biological and biomedical communities  global data consistency is not always attainable: different sites' data may be dirty  uncertain  or even controversial. collaborators are willing to share their data  and in many cases they also want to selectively import data from others - but must occasionally diverge when they disagree about uncertain or controversial facts or values. for this reason  traditional data sharing and data integration approaches are not applicable  since they require a globally consistent data instance. additionally  many of these approaches do not allow participants to make updates; if they do  concurrency control algorithms or inconsistency repair techniques must be used to ensure a consistent view of the data for all users.
　in this paper  we develop and present a fully decentralized model of collaborative data sharing  in which participants publish their data on an ad hoc basis and simultaneously reconcile updates with those published by others. individual updates are associated with provenance information  and each participant accepts only updates with a sufficient authority ranking  meaning that each participant may have a different  though conceptually overlapping  data instance. we define a consistency semantics for database instances under this model of disagreement  present algorithms that perform reconciliation for distributed clusters of participants  and demonstrate their ability to handle typical update and conflict loads in settings involving the sharing of curated data.
1. introduction
　when multiple autonomous  collaborating parties agree to share data  they often encounter situations where that data is mutually inconsistent: each party has a data instance that is internally consistent  but the different parties

 
 this research has been funded by nsf career award #iis-1 and grant #iis-1  and a seed grant from penn istar.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
may each have  facts  that conflict with the others. such inconsistency arises from many reasons: sites may have different levels of data freshness  some sites' data may be dirty  or  frequently  the sites may have different viewpoints about what data is factual.
　for instance  in bioinformatics  the same results from microarray experiments may be analyzed by different tools  or curated by different people  yielding different values. this is inevitable because most data curation is somewhat uncertain in nature  and thus correctness becomes a matter of interpretation. therefore  bioinformatics data warehouse curators often reach different conclusions  and even occasionally revise these conclusions. biologists have a sense of the authority of the different databases: for instance  swiss-prot  is generally more reliable than ncbi genbank  because it is human-curated  see  e.g.   for a query language that incorporates such policies . to help control the quality of the data they use  and also to ensure their queries are not seen by competitors   most biologists create  query  and curate their own local database instances  which they populate with downloaded data. a biologist will typically not publish the contents of this database until he or she submits a paper for publication.
　these situations result in a mismatch with existing methods of sharing data. if sharing is based on a common dbms  whether distributed or client-server   the dbms concurrency control and integrity checking routines will mandate that a single version be put into the database. even in situations where local caching and optimistic concurrency control are used  ultimately one version of the state will exist. if  instead  the collaborating parties share information through a data integration  1  1  or peer data management  1  1  system  the results will be either inconsistent  or filtered through a  repair  scheme  1  1 . other strategies have similar drawbacks; e.g.  file synchronization  1  1  typically results in conflicts that must be manually resolved in order to restore consistency.
　the solution to the needs of collaborating scientists is not to provide one globally consistent data instance  but rather to give each participant a custom  internally consistent instance with the data that this particular source accepts as authoritative. this instance should contain data  overlapping  with that at other sites  as well as certain data items that diverge from  many of  the others. in keeping with the needs of scientists as described above  data instances should be loosely coupled: each participant queries and manipulates a local database instance and may occasionally publish its database  including a log of recent changes; it may selec-

figure 1: collaborative data sharing system with three bioinformatics data warehouse participants sharing data on protein functions.
tively import others' modifications according to some policy. today such capabilities can only be provided through custom import tools  which are expensive to develop and generally do not provide well-defined conflict resolution semantics.
　the challenge in supporting this data sharing model lies in the fact that every database instance is subject to updates  in the form of transactions. not all of these transactions may originate from sources trusted by all parties; hence  each site must decide whether to accept an update  in effect  synchronizing a portion of this site's data instance with the origin of the update   to reject it as being untrusted  causing a divergence   or in some cases to defer processing an update  if there are multiple conflicting updates with no clear preference . an update-centric model is important  as opposed to simply querying the resulting data values  because it allows sites to reject removals or replacements. however  it adds significant complexity  because one update may depend on a chain of other updates  some of which are trusted and others of which are not. a clean semantics for update propagation must consider these interactions  as well as different levels of trust.
　at the university of pennsylvania  we are developing a generic platform to support this type of data sharing; we refer to this platform as a collaborative data sharing system or cdss. our overall cdss architecture was first presented in an overview paper   which sketched out our vision of confederations of autonomous data sharers who publish and import updates. the process of selectively importing updates  in particular  those that do not conflict with existing data  is termed reconciliation. the orchestra system represents our efforts to realize such a cdss  and although our system is broadly applicable  we are particularly focusing on the needs of bioinformatics and biomedical researchers.
　an orchestra system  figure 1  consists of a number of collaborating participants  p1 ... p1 in the figure   each of whom controls and edits its own data instance  denoted i1 ... i1   and each of whom has a policy about what external data it is willing to trust and accept  labels on the arcs . as data is modified at different sites  denoted  1 ...  1   orchestra publishes and propagates the updates to all sites that are willing to accept them. our end goal is to facilitate update propagation in situations with both disparate schemas  i.e.  each participant may have a different schema  with some attributes that may not have corresponding aspects in the other schemas  as well as disparate instances  i.e.  each participant may have tuples with values that may not exist in other participants' instances .
　this paper represents our first step towards addressing these goals: it defines a model and methodology for reconciling instances of a single database schema  in the presence of transactions  disagreement  and trust relationships. the complementary problem of translating updates across schemas in order to incrementally maintain data instances is ongoing work.
we make the following contributions:
  a formalization for reconciliation that emphasizes local autonomy  making forward progress  and providing intuitive behavior. we support coexistence of multiple instances  with consistency defined by trust policies.
  a model for reconciliation where individual transactions are assigned trust levels  and the goal is to propagate the highest-trusted  longest chains of transactions.
  new algorithms to reconcile database instances while maintaining consistency .
  a performance analysis of these algorithms on bioinformatics-based workloads.
　this paper is structured as follows. we discuss related work in distributed data sharing in section 1. we describe our cdss model in section 1. in section 1  we define a semantics for reconciliation. section 1 presents algorithms for implementing reconciliation. we validate our initial prototype implementation in section 1  and finally we conclude and discuss future work in section 1.
1. related work
　the problem of sharing structured data among multiple sites has a long history  and it has been revisited in many different settings. however  as described in the previous section  there has generally been an assumption that the end goal is to create a single consistent data instance  perhaps with multiple views  across all sources. we briefly outline previous work.
　file synchronizers  1  1  1  take two files  and they attempt to reconcile the changes into a maximally consistent instance. when a conflict exists  both versions are typically preserved and the user must resolve the conflict before continuing to work. the bayou  disconnected file system provides mechanisms for supplying application-specific conflict detectors and conflict resolvers. the coda  and ivy  file systems provide a mechanism for detecting conflicts  and disable updates to them until a repair is made.
　distributed databases provide a single administrative domain  database schema  and instance to all users. each participant may independently make edits to the database instance. through either lock-based or optimistic concurrency control schemes  1  1  1   the system interleaves updates while maintaining consistency. version vectors   vector clocks  and other mechanisms are often used to determine causality. an alternative approach is that of lotus notes   which  upon a conflicting update  splits the data into two versions that are no longer synchronized.
　data integration  including federated databases and peer data management  is a middleware layer above autonomously maintained data sources. here  each participant updates its own data sources in a self-contained fashion. when a participant wishes to access data from  the outside world   it poses a query over the virtual mediated schema that the data integration system builds over all data sources. in traditional data integration  there is one mediated schema; peer data management systems  1  1  1  allow for multiple mediated schemas  interrelated by a network of schema mappings. data consistency is generally managed in terms of certain answers: inconsistent data values are removed through a  repair  procedure . no facilities exist here for importing data into a given data source.
　the collaborative data sharing system was first introduced in   which proposed  but did not implement  an architecture for supporting the exchange of atomic updates in a distributed setting. our work in this paper builds upon these ideas  but focuses on the complexities introduced by transactions and different trust levels.
1. collaborative data sharing
　as shown in figure 1  a collaborative data sharing system consists of a confederation of loosely coupled participants  or peers   each of which is autonomous  but each of which wishes to share its data and updates with the other participants. in general  update translation mappings  not shown  relate instances of one published schema to instances of  semantically neighboring  published schemas  and provide a means of translating updates from one schema to another; however  in this paper  we do not consider update mappings  and instead focus on reconciliation in a single-schema setting. therefore  in the figure  all participants share the same schema  though they may of course trust different data. participants  p1 p1 p1 in the figure  make updates to their local database instances  and they publish  a subset of  these updates   1  1  1  as well as a published database instance  shown in the figure as i1 i1 i1  with a matching schema. finally  a series of acceptance rules  labels along the arcs between participants  define  for each participant  a trust priority level for updates from other participants.
　the central problem in a cdss is the propagation of updates among sites. we term this problem reconciliation: given the acceptance rules and updates published by participants  the reconciliation operation determines which updates should be applied to   accepted by   the reconciling participant p. all updates that satisfy the acceptance rules and do not mutually conflict  or conflict with existing state  should be accepted; for conflicting updates  priorities are used to determine which  if any  updates are to be applied.
conflicts. in our initial implementation  we adopt a fairly simple conflict model. a conflicting update is any update that:  1  when applied to the current database instance  results in an instance that is inconsistent with its integrity constraints; or  1  is mutually incompatible with some other published update that also satisfies an acceptance rule. instances of the latter case are updates that change a single antecedent data value into two different values  updates that simultaneously remove and replace a data value  and updates that result in a data instance that violates a constraint  though the last can also be caused by a single update to a database instance .
　we assume that reconciliation is an operation that is done frequently but not in real-time  by each specific participant: the participant will accept and apply a subset of all  recently published  updates to its data instance. note that reconciliation is a matter of importing data  and therefore it can be done more or less frequently than publishing  though we assume that the two are performed together.
1 preliminaries
　before providing an example of reconciliation in a cdss  we begin with a description of our high-level goals and basic approach. we first provide an overview of the goals  and then explain how these guide our strategy.
  maximal progress and monotonicity. each reconciliation should make maximal use of all published updates available at the time. once an update has been accepted by a participant  a future reconciliation may result in changing the results of the update  but the update itself will not be rolled back from the data instance.
  least interaction. update sequences made at participant p should not interact with update sequences made at site q in unexpected ways: in particular  if q makes a modification that conflicts with p  but revises its modification so it no longer conflicts  before p imports its changes  then p should consider q's update sequence to be compatible.
  trust policies. in many bioinformatics and other settings  some sources are known to be more credible than others.  as mentioned previously  swiss-prot is human-curated  making it more authoritative than genbank  which is not.  we allow for each site to provide a partial ranking of authority for such cases - allowing the system to automatically resolve certain conflicts.
we now describe how each of these principles guides the functionality of the cdss  before we present an example.
maximal progress and monotonicity. we define reconciliation in a participant-centric way  and assume a global ordering on when participants reconcile  and optionally publish . we term each such step an epoch. at each reconciliation  a single peer imports updates from outside. while it may also publish its own updates  no other participant will receive these until it reconciles. thus  information flow is inherently  1  relative  as the updates participant p sees from participant q are those published since p last reconciled; and  1  asymmetric  as q will not immediately receive p's updates. moreover  a reconciling participant has no way of knowing whether the updates it  sees  now will be revised in the future  or whether some other participant will publish a conflicting update in the future. we believe that a causality model in which every participant makes maximal progress based on what it has seen  and never  changes its mind   has many desirable properties.
　a second mechanism for ensuring progress is the notion of update deferral. if several updates conflict and the participant has no way of ranking them  it will mark them as being deferred until a user resolves the conflict. any future updates that might conflict with an unresolved conflict are themselves deferred - ensuring that the user does not inadvertently render them inapplicable.
least interaction. the mode of information exchange  and hence the cause of dependencies occurring among updates  will solely be via acceptance of updates from other participants. since two participants may reconcile at different frequencies  we believe that any intermediate states of tuples should not interact  i.e.  if different participants make successive modifications to a tuple while not in contact with one another  any intermediate states should be disregarded  and only the final updates should be considered.
trust policies. acceptance rules assign a numeric priority level to a set of updates  based on predicates over the content as well as the origin of these updates. we assume that higher priority levels result in larger numbers  and that in situations where conflicts arise  a participant will accept an update with a higher update priority over a conflicting one with a lower priority. when multiple updates have equivalent  and highest  priority  our semantics is to adopt a  certain answers   open-world model in which none of the conflicting updates will be applied until a user intervenes. such updates are termed deferred  and any future updates that conflict with a deferred update are themselves deferred.
　in order to support acceptance predicates over update origins  we assume that every update is annotated with the identity of its origin. while this model is not as expressive as some notions of lineage  1  1  1   it is adequate for our acceptance rules. this model resembles the information source tracking method of  and the multi-viewpoint formalism of .
1 cdss example
　we refer to the example cdss in figure 1  where participant p1 has a policy to accept update sequences from either p1 or p1  assigning them equal priority. in contrast  p1 prefers updates from p1 versus p1  and p1 only accepts updates from p1. an exception to this rule  which we describe later  is that p1 may make revisions to updates that originated from p1 - in this case  p1 must transitively accept this portion of p1's data.
notation. in this paper we assume that all updates are described in terms of changes to values  and they are annotated with the identifier of a single originating participant. we consider the following operations: insert tuple  denoted +r ．a;i  for an insertion of the tuple ．a by participant i into some relation r with a schema compliant with ．a ; delete tuple   r ．a;i  ; modify tuple  r ．a ★ a．1;i   where ．a1 is a new set of attribute values conforming to schema of r . we also assume that updates may be grouped into transactions  denoted xi:j  where i represents the identity of the originator of the transaction  and j represents its unique local transaction identifier. we assume that transaction identifiers are assigned in increasing order.
　figure 1 illustrates reconciliation over four epochs within this cdss  for a single relation f organism protein function   where  organism protein  is a key. at time 1  each participant pi's instance of this relation  denoted ii f |1  is empty. in epoch 1  participant p1 applies two transactions  one of which revises the other   and then it publishes and reconciles its data. since no other updates have been published  pe ends epoch 1 with state i1 f |1  obtained by applying its own update sequence.
　in the next epoch  participant p1 introduces two new tuples and then reconciles. its resulting state  i1 f |1  is the result of applying its own updates. although p1 published two updates that p1 trusts  these updates conflict with p1's own updates - hence  it rejects them. in epoch 1  p1 reconciles a second time. now it applies the mouse update from p1; it rejects the rat tuple that is incompatible with its own local state. finally  in the last epoch  p1 reconciles. it trusts p1 and p1 equally. hence  it accepts the nonconflicting mouse updates  but it must defer the remaining rat update transactions because they all conflict.
　given our intuitions from the basic principles and the preceding example  we now proceed to define a formal semantics for reconciliation.
1. reconciliation
　we begin by specifying the collaborative data sharing system formally. in the larger scope of orchestra  we intend to support reconciliation across multiple schemas. however  for purposes of this paper  we will define the cdss for a setting in which all participants share a single schema.
	definition 1	 collaborative data sharing system .
a collaborative data sharing system  cdss  includes the following components:
  Σ  a schema representing the relations in the system.
  p  a set of participants  {p1 ... pn}.
  a  a mapping from each pi （ p to a set of acceptance rules  each of which is a pair  θ v  where θ is a predicate on updates in   over some relation r and v is an integer priority that pi assigns to tuples satisfying θ.
     a sequence of transactions of updates of the form +r ．x;i    r ．x;i   r ．x ★ x．1;i   over each relation r and published by each participant pi.
  i Σ  = {i1 Σ  ... ii Σ  ... in Σ }  the public database instances controlled by each pi.
  e  an integer clock or reconciliation epoch counter. it is incremented each time a different participant publishes data. we assume that the first publication or reconciliation step defines the beginning of epoch 1. we denote the subset of   published in epoch e as  |e.
　suppose we are given a cdss as in definition 1. let us denote an update made to relation r as δr. we define the priority relative to participant pi of a transaction x  prii x   as follows:
  1  if any δ （ x is untrusted  i.e.  there is no  θ v  （ a pi  such that θ δ  is satisfied and v   1.
  max {v |  θ v  （ a pi  … θ δ  … δ （ x}   otherwise.
we say that two updates δr δr1 conflict iff
  δr δr1 are both insertion operations with the same values for their key attributes  but different values for at least one other attribute  or
  one of δr δr1 is a deletion and the other is a replacement or insertion operation  and they have the same values for their key attributes  or
epochparticipant p1participant p1participant p1i	f	{}
i	f	{}i	f	{}x1{+f mouse prot1 immune;1 }
x1{+f rat prot1 cell-resp;1 }1 publish and reconcile 1 reconcile 
i1 f |1: { mouse prot1 immune  
 rat prot1 immune }1 reconcile 
i1 f |1: {f mouse prot1 immune }
defer: {x1 x1 x1}figure 1: reconciliation of f organism protein function   with key  organism protein   among the participants of figure 1  over four epochs. each participant pi may apply transactions  xi:j   which it publishes and reconciles according to the policies in figure 1. the resulting instance for each epoch e is denoted with ii f |e. when transactions conflict  the participant always picks its own version first  or else the highestpriority one and its antecedents  if this is unique . it defers any transactions that have no unique  winner.   δr δr1 are both replacement operations with the same source tuple value  where the replacement tuples have different values.
an update δr may also be incompatible with an instance i if applying δr to i would violate an integrity constraint. we generalize this to say that two transactions x x1 conflict iff an update δ （ x conflicts with an update δ1 （ x1  and that a transaction x is incompatible with an instance i iff an update δ （ x is incompatible with i. finally  we assume a function apply δr ．x;i  r  that applies the updates in δr ．x;i  to relation r  and returns the resulting relation. within this setting  we define two versions of the reconciliation problem: first  the problem in an append-only setting  and then the problem in its full generality.
1 append-only reconciliation
　in the append-only case  every transaction in a given epoch can be considered independently. an insertion may be applied so long as it does not conflict with a previously applied insertion  nor does it conflict with a transaction of equal or higher priority.
　for any epoch e  let  acc i |e be the set of transactions from  |e acceptable to pi. we define  acc i |1 to be the empty set  and for all other epochs let
 acc i |e = {x （  |e :  1  x1 （  |e : x x1 conflict and
prii x1  − prii x   and  1  x1 （  e1 : e1   e and x x1 conflict }
　from this  it is straightforward to define the reconciliation problem for a given participant in the append-only model.
	definition 1	 append-only reconciliation .
let ii r |e be the instance of relation r at participant pi in epoch e. the append-only reconciliation problem for participant pi is to compute ii r |e for every r （ Σ  given some initial ii r |e1 and   from e1 to e. for each relation r  let res = ii r|e1 . a tuple t must appear in instance ii r |e iff it appears in the instance res resulting from recursively applying  for each τ from e1 to e   1  in increasing order   res = apply δr res  for every δr in  acc i | τ+1 .
append-only reconciliation is very simple to compute algorithmically: during epoch τ  for each pi  we simply consider each published transaction x in isolation and determine whether it is in  acc i |τ  meaning that it uniquely has the highest priority of any transaction with which it conflicts. if so  we apply the transaction.
1 replacement and deletion
　if we allow for replacement and removal  the semantics of reconciliation must change significantly: now an update may be dependent on other  antecedent updates  where the result of the antecedent update is used by the dependent update . these antecedent updates may have originated from participants other than the participant who published the most recent update  and the original source of that update might not itself be trusted by the peer who is reconciling. we therefore adopt the semantics that a participant who trusts an update u must transitively trust any antecedent updates made by other participants  at  at least  the priority level of u  provided the transaction has not been explicitly rejected by the reconciling participant. a transaction that depends upon or conflicts with any deferred update  or whose antecedents do so  is deferred  until the potential conflict is resolved by explicit user interaction.
　in a reconciliation model with deletions  one transaction may introduce a conflict  but a succeeding transaction may remove that conflict. for instance  to continue the example of figure 1  suppose that in epoch 1  participant p1 first introduced a sequence of transactions:
x1 : {+f mouse prot1 cell-resp }
x1 : {f  mouse prot1 cell-resp  ★
　　　　　　　　　 mouse prot1 cell-resp } where initially the wrong protein was given the function cellresp. in this case  while transaction x1 clearly conflicts with x1  intuitively p1 should accept x1  since this does not conflict with its state after applying the full transaction sequence above. in general  given a transaction sequence  one can take the constituent update sequence and  flatten  it into a set of direct updates by removing intermediate steps  as described in  1  1 . the sequence  x1 x1  above can be minimized to {+f mouse prot1 cell-resp }.
　let applied pi e  be the set of updates that have been applied by participant pi from epoch 1 through epoch e. also  for a transaction x published in epoch e  we define the antecedent set  ante x   to contain any transaction x1 （  τ  1 ＋ τ ＋ e  where x1 either inserts a new tuple  or makes a modification to a tuple  which x directly deletes or modifies.
　definition 1  transaction extension . we define pi's transaction extension of transaction x  reconciled in epoch e  to be the transitive closure of x's antecedents  so long as those transactions have not yet been accepted by pi in epoch e:
tei|e x  = {x “ ante x }
tei|e x  = tei|e x  “ {x1| x1 （ tei x  : x1 （ ante x1 
…x1（ applied pi e }
henceforth we will assume that the transaction extension is sorted by the order of each transaction in  .
　we say x subsumes some other x1 if its transaction extension is a superset of x1's transaction extension. given a list of transactions l  sorted by the order of their application  we can define their update footprint to be:
uf l  =  δ （ x for each x （ l 
now  assume we are given a function flatten s   which  given an input sequence s of updates  produces a set of mutually independent updates with all dependency chains removed  as in . for a transaction x  a subset of its antecedents l  and a reconciling participant pi  we define an update extension ui x l  to include the following components:
  an operation  the set of updates in flatten l .
  a root  which is the original transaction x.
  a source  which is the contents of l.
  a priority level equal to prii x .
the update extension represents the set of changes made by transaction list l as  seen  by peer pi  with all intermediate steps removed. our goal is to consider conflicts between update extensions  and to choose to apply the highest-priority update extensions.
　to do this  we must consider when transactions' extensions conflict; here we should only consider interactions between updates that are not shared between the transactions.
	definition 1	 direct conflict . two transactions x 
x1 directly conflict iff   δ （ ui tei|e x  s  δ1 （ ui tei|e x1  
s  s.t. δ and δ1 conflict  where s = {x1 : x1 （ tei|e x  … x1 （ tei|e x1 }.
　we can now define the general reconciliation problem for updates that include deletions and replacements. a solution to the general reconciliation problem must also maintain information about whether prior reconciliation operations marked certain transactions as rejected or deferred: as discussed previously  we similarly reject or defer  resp.  any future transactions that depend upon these.
　definition 1  general reconciliation . we define the general reconciliation problem for participant pi as follows. during epoch e  given an initial ii r |e1  a set of previously deferred transactions deferred pi e1   and previously rejected transactions rejected pi e1   and all  i from e1 to e  compute:
  a new instance ii r |e for every r （ Σ  defined as follows. let app be the set defined as follows. for each epoch τ from e1 to e  for each x in  |τ  app must contain those transactions in tei|e x  that:
1. have a priority prii   1 
1. can be completely applied to ii r|e1  without violating its integrity constraints 
1. do not directly conflict with some other x1 of equal or higher priority  which is not subsumed by x 
1. do not have a transaction in tei|e x  that is in rejected pi e1   and
1. do not delete  modify  or insert a tuple whose keymatches any update in deferred pi e1 .
for each r  ii r |e must contain tuple t iff t appears in res as defined next. initialize res = ii r |e1 and create an empty set used for transactions that have been applied. for each transaction x in app that is not antecedent to any other transaction x1  i.e.  it is not in tei|e x1  for any x1   apply all updates res = apply δr res  for all δr in flatten uf tei|e x   used  . add all transactions in tei|e x  to used.
  a new deferred set deferred1 pi e   which adds to deferred pi e  every x that directly conflicts with some update in deferred pi e1  or any other  non-subsumed  x1 of equal priority.
  a new rejected set rejected1 pi e   which adds to rejected pi e1  every x that directly conflicts with some other x1 of equal or higher priority  or whose extension tei|e contains a transaction in rejected pi e .
　proposition 1. a solution to the general reconciliation problem will always accept transactions and their antecedents for which there exist no other directly conflicting  non-subsumed transactions of equal or higher priority. proof sketch. assume for the purpose of contradiction that a transaction has not been accepted  despite being of higher priority than any transaction with which it directly conflicts  not depending an a rejected or deferred transaction  and being conformant with integrity constraints. then the update extension of the transaction must directly conflict with a transaction accepted in the same reconciliation operation at a higher priority. but the definition only rejects or defers transactions that conflict with transactions of equal or higher priority. hence the update extension must conflict with that of a higher-priority transaction  which is a contradiction.
　a greedy algorithm that closely matches the above definition  processing items in decreasing order of priority  is provided in the next section.
　once a number of items have been deferred  the process of conflict resolution makes use of the solution to the reconciliation problem stated above. to resolve a conflict  the
distributed
store
central
store
pros: no central store  medium communication cons: needs stable base of connected peers 
reconciliation work all at one peerpros: no central store  distributed reconciliation work cons: highest communication  needs stable base of connected peerspros: low communication 
high reliability
cons: needs reliable central server  reconciliation work all at one peerpros: distributes reconciliation
work across many peers  high
reliability
cons: high communication  needs reliable central serverclient-centric reconciliationnetwork-centric reconciliationfigure 1: comparison of different combinations of reconciliation algorithms and update stores.
user specifies some number of transactions to remove from the deferred set and reject. the remaining transactions are removed from the deferred set and treated as recently published transactions  and the reconciliation solution is re-run to apply those that no longer conflict.
1. reconciliation algorithms
　a general reconciliation algorithm  executed by participant pi  first determines which transactions have been published since pi's previous reconciliation  the relevant transactions   and pi's priority assignment to each of these newly published transactions. it then computes the update extensions for these transactions  and determines which updates can be applied without violating the requirements given in definition 1. finally it applies the transactions it has selected  records the set of transactions that it must defer  and rejects those that remain.
　this process can either be centralized or distributed. if the work is centralized on the reconciling participant  peer   we call it client-centric reconciliation  since it is typically the reconciling participant that retrieves all of the relevant transactions and decides which to apply. an alternative is network-centric reconciliation  in which computation is distributed across the entire network of peers. while the network-centric approach puts less load on the reconciling participant by distributing almost all of the work across the network  the client-centric approach generates less network traffic  and it allows for a considerably simpler reconciliation algorithm. it also may allow potentially sensitive information  like the trust conditions  to be kept private from other participants.
　the reconciliation algorithm needs to access several different kinds of data to perform the operations outlined above. it must access the series of published transactions operations  and the instance of the reconciling participant. it also needs to read and modify the sets of applied  rejected  and deferred transactions for the reconciling peer. we define an update store module to provide a general interface to much of the aforementioned state. we have explored using both a central server and a distributed store in which the peers themselves store the state.
　each combination of reconciliation algorithm and update store implementation has its own unique benefits  as shown in figure 1. our initial implementation uses client-centric reconciliation  which is considerably simpler both to understand and to implement; we couple that with either central or distributed storage. as future work we intend to implement network-centric reconciliation.
　in order to implement an algorithm for the general reconciliation problem given in definition 1  we introduce several new concepts:
  dirty values are key values that are modified  i.e. read or written  by a deferred transaction. any transaction that reads or writes a value whose key is in the dirty value set must be deferred  in order to ensure that a previously-deferred transaction can always be accepted later.
  conflict groups are groups of conflicts with the same type that involve the same key value; the reconciliation algorithm groups conflicts for each reconciliation into such groups.
  options are groups of transactions within a conflict group that make the same modification to the key value. at most one option can be accepted for each conflict group when conflicts are resolved; the transactions from the other groups are rejected.
1 client-centric reconciliation
　the core of the client-centric reconciliation algorithm is the reconcileupdates procedure. it determines which updates the participant can apply or reject during a particular reconciliation  and assigns the deferred transactions into conflict groups. when a participant reconciles  it first queries the update store to fetch the newly relevant transactions  their trust priorities  and their update extensions. the algorithm then determines which transactions to apply  reject  or defer; for deferrals it records conflict groups. when the user resolves one or more conflicts  this rejects the transactions in the options he or she did not select; then reconcileupdates is re-run for the earliest point of conflict resolution.1 reconcileupdates reconsiders all previously deferred transactions  and it accepts or rejects those for which conflicts have been resolved.
　the core of reconcileupdates is given in figure 1  and the various helper functions appear in figure 1. reconcileupdates begins by computing the flattened update extension of each trusted transaction. the call to checkstate at line 1 determines which transactions much be rejected or deferred because of the reconciling participant's dirty value set or materialized state. the call to findconflicts at line 1 discovers conflicts between the flattened update extensions of trusted transactions. the algorithm then calls dogroup at line 1 to consider each group of transactions with the same priority  in decreasing order of priority; the decreasing order allows the algorithm to proceed greedily and consider each group only once. within each group  transactions that conflict with higher-priority accepted transactions are rejected  and those that conflict with higher-priority deferred transactions are themselves deferred; if conflicts are found between two non-rejected transactions within a group  both are deferred. once all priority groups have been considered  reconcileupdates has made decisions for all trusted transactions. line 1 records which transactions the client
reconcileupdates recno 
1 txns ○ the ids of the undecided fully trusted transactions
1 prio ○ mapping from index in txns to priority
1 prios ○ set of all transaction priorities
1 sort prios in decreasing order
1 for t （ txns do
1 upex t  ○ the flattened update extension of t
1 decision t  ○ checkstate recno upex t  
1 endfor
1 conflicts ○ findconflicts txns upex 
1 for txnprio （ prios do
1 decision ○ dogroup txnprio conflicts prio decision 
1 endfor
1 record decision at recno
1 for t （ txns do
1 if decision t  = accept then
1 upex t  ○ the flattened update extension for t
1 apply upex t 
1 endif
1 endfor
1 deferred ○ {txn | decision txn  = defer}
1 updatesoftstate recno deferred 
figure 1: the main client-centric reconciliation algorithm. helper methods are in figure 1.
has decided to accept or reject. lines 1 update the state of the local database; it is necessary to recompute the update extension since the antecedents of the trusted transactions may overlap. line 1 updates the client's dirty value set and list of conflicts for the current reconciliation.
　suppose that during a particular reconciliation there are t relevant transactions  each of which has at most a undecided antecedents. further suppose that each transaction contains at most u component updates. in this case  computing the flattened update extensions will take time o tua   since that much time is needed even to read through the updates for the relevant transactions. checking for pairwise conflicts between the update extensions will take time at most o `t1 + tua＞  if a hash table-based conflict detection algorithm is used. this conflict detection step asymptotically dominates all other work done afterwards by the reconcileupdates procedure  giving a combined running time o `t1 + tua＞.
　by considering the trusted transactions in decreasing order by priority  reconcileupdates greedily ensures that the conditions given in definition 1 are satisfied; since lower priority transactions can never affect whether higher priority transactions are accepted  the lower priority ones can be considered independently in subsequent iterations.
1 update store
　the update store's fundamental role is to publish and retrieve updates  and to associate each published transaction with a client reconciliation time. the latter ensures that no transaction ends up in multiple reconciliations for the same client  and that no new updates for a specific reconciliation appear or disappear after it is recorded. as mentioned above  all other state  such as deferred transactions  conflicts  client state  trust conditions  and which transactions each participant has accepted or rejected  can remain private data to each participant.
　such a system  however  would require a great deal of communication across the network  as each update needed during reconciliation would have to be requested individually. our implementations  therefore  move the sets of applied and rejected transactions from the participant into the
checkstate recno upex  1	if upex contains a value dirty at recno then
1 return defer
1 else if upex contains an already decided transaction then
1 return reject
1 else if upex is incompatible with the instance at recno then
1 return reject
1 else if upex conflicts with the delta for recno then
1 return reject
1 else
1 return accept 1 endif
findconflicts txns upex 
1 conflicts ○  
1 for t t1 （ txns do
1 if upex t  conflicts with upex t1  then
1 if neither t nor t1 subsumes the other then
1 conflicts t  ○ conflicts t  “ {t1}
1 conflicts t1  ○ conflicts t1  “ {t}
1 endif
1 endif
1 endfor
1 return conflicts
dogroup txnprio conflicts prio decision 
1 priogrp ○ values in prio that map to txnprio
1 higher ○ values in prio that map to a priority   txnprio
1 remove rejected transactions from priogrp
1 for t （ priogrp do
1 for c （  conflicts t  ” higher  do
1 if decision c  = accept then
1 decision t  ○ reject	priogrp ○ priogrp   {t}
1 else if decision c  = defer then
1 decision t  ○ defer
1 endif
1 endfor
1 endfor
1 for t t1 （ priogrp do
1 if t conflicts with t1 then
1 decision t  ○ defer	decision t1  ○ defer
1 endif
1 endfor
1 return decision
updatesoftstate recno deferred 
1 clear all soft state from reconciliation recno
1 for t （ deferred do
1 upex t  ○ the flattened update extension of t
1 remove from upex t  clean updates inapplicable at recno
1 mark upex t  dirty at recno
1 endfor
1 conflicts ○ findconflicts deferred upex 
1 conflictgroups ○  
1 for t （ deferred  t1 （ conflicts t  do
1 for conflict htype valuei between t and t' do
1 add {t t1} to conflictgroups htype valuei 
1 endfor
1 endfor
1 for htype valuei （ conflictgroups.keys do
1 combine compatible txns for htype valuei into same option
1 endfor
1 record conflictgroups as conflict set for recno
figure 1: helper methods for the reconcileupdates method given in figure 1.
update store; this allows the update store to be determined remotely  and thereby reduces that amount of network traffic. an additional result of this approach is that each client contains only soft state; it is possible to reconstruct the entire state of the participant  up to his or her last reconciliation  from the update store.
　for these reasons  we implement an update store with the following basic operations: publish transactions from a peer  and record those it has already accepted 1 record that a peer has accepted and rejected certain transactions  record that a peer has decided to reconcile and associate with that reconciliation a particular set of published transactions  retrieve the current reconciliation number of a peer  and retrieve all of the transactions that a peer may need to see in order to perform its more recent reconciliation  along with the priorities associated with the fully trusted transactions in that set. in order to perform these operations efficiently  the update store must log all of the updates published and their epoch  what transactions each peer has accepted or rejected  the current epoch  the epoch corresponding to each peer's previous reconciliation  and the trust conditions for each peer.
　early prototypes of our system showed it was vital to reduce the number of messages sent between the update store and each participant. in the current interface  a constant number of procedures are invoked during each reconciliation. in the centralized server implementation  each of those sends a small number of messages across the network; in the distributed implementation  each trusted transaction requires a request message  though antecedent transactions will be sent automatically. the system is still limited by network bandwidth  since many transactions may need to be sent to the reconciling peer  but the reduced number of 'roundtrips' between the update store and the client gives a great performance improvement in many situations.
1.1 relational database update store
　relational database technology provides an efficient way to implement a centralized update store. commercial rdbmss offer high performance and durability  both important characteristics in a system such as ours. we highlight some of the more interesting and innovative aspects of our design.
　in our implementation  an epoch count  implemented using an sql sequence  is used to timestamp each batch of transactions that it is published. since publishing is not instantaneous  each peer records when it has started publishing  and also when it has finished. we decouple publishing from reconciliation to support greater concurrency: when a peer requests to reconcile after publishing  it determines the latest epoch not preceded by an  unfinished  epoch  and it uses this as its reconciliation epoch. no additional transactions will be published by any participant prior to this point. the inputs to reconciliation  then  are any transactions whose epoch number lies between the participant's prior reconciliation epoch and this new epoch.
　implementing this approach requires care to avoid sacrificing performance. the series of epoch numbers can contain gaps if reconciliations are rolled back or aborted; therefore each publishing peer must record when it has finished writing all transactions to the database  as mentioned above.
however  we also want to allow as many peers as possible

the messages sent are request epoch  1   begin epoch e  1   confirm epoch begun  1   begin publishing at epoch e  1   publish transaction ids for epoch e  1   and confirm epoch finished  1 . after this the publishing peer can send the transactions for epoch e to their transaction controllers.
figure 1: the procedure by which a peer publishes an epoch in the dht-based store.
to publish updates simultaneously. repeatable read isolation at the dbms level prevents race conditions: when the reconciling peer determines the epoch to associate with its reconciliation  it immediately stores that value in the reconciliations table and commits the transaction  releasing all locks. thus it holds an exclusive lock on the epochs table just long enough to determine the largest stable epoch number; thereafter reconciliation operations are decoupled from the epochs table. by minimizing the time that lock is held  we enable maximum concurrency in publishing updates  as well as in the operation of reconciling. as long as no peer is recording its decision to reconcile  there is no limit on the number of peers that can simultaneously start reconciliation  publish updates  or record that they have finished publishing. additionally  application of trust predicates and determination of update extensions take place inside the dbms  meaning that only relevant transactions and transactions that contribute to their transaction extensions are sent over the network.
1.1 dht-based store
　our distributed update store is based on freepastry  an open-source implementation of a distributed hash table  1  1  1 . in this version  work  both storage and computation  is spread over the entire network of peers  using transaction identifiers and epochs as keys. the peers store three kinds of data  and each responds to several kinds of messages  which are described below in detail. in this implementation  we assume successful message delivery and postpone a study of fault-tolerance to future work.
　one peer  the owner of a predesignated key  keeps track of the epoch count. when a participant wants to publish updates  it requests the next epoch count from this epoch allocator  see figure 1 . the epoch allocator informs the epoch controller for this epoch  the dht peer who  owns  the hash value of the epoch  that this participant wants to publish updates  and then returns the epoch count to the requesting peer. after sending the epoch number to the requesting participant  the epoch allocator increments its epoch counter. we observe that  if this peer were to fail  its data could be reconstructed by polling for the largest epoch present in the system.
　after a participant publishes its set of transactions during an epoch  which it sends to the peer who owns the hash of its transaction id  the transaction controller   it transmits their ids to the epoch controller. the epoch controller concludes by marking the epoch as complete.
now the participant needs to determine an epoch for the
transaction 1 transaction ta controller tc controller
	1
transaction reconciling tb controller 1 peer p
in this example p requests reconciliation information for transaction ta. p has already applied tb  but has not decided ta or tc. tb and tc are antecedents of ta; tb has other antecedents  but tc has none. the messages sent are request ta  1   send ta  1   request tc for p  1   request tb for p  1   send tc  1   and tb not relevant  1 .
figure 1: an example of retrieving a transaction for reconciliation in the dht-based store.
second step  reconciliation. it requests the most recent epoch from the epoch allocator  and uses that information to request the contents of all epochs since its last reconciliation from their respective epoch controllers. it uses this information to determine the most recent  stable  epoch  and records this as its reconciliation epoch at its peer coordinator. then  for each epoch since the participant's prior reconciliation  it requests the set of transactions published in that epoch from the epoch controller  and then requests that set from the transaction controllers. each transaction controller either sends back the requested transaction  its priority  and a set of antecedents; or a notification that the transaction is untrusted or irrelevant. the reconciling peer maintains a pending transactions set  to which it adds antecedents and from which it removes received  or irrelevant  transactions. this procedure is visualized in figure 1. when the pending transactions set becomes empty  the peer begins running the reconciliation algorithm. that algorithm notifies the appropriate transaction controllers when it accepts  rejects  or defers transactions.
1. implementation & experiments
　we have implemented the reconciliation algorithm of section 1 above in java  and also constructed a centralized update store  built in java over a major commercial rdbms  and a distributed store  based on freepastry . since we expect orchestrasystems to consist of tens of participants  we explore configurations of up to fifty peers.
