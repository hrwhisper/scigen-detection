this paper reports the experiments of using indri for the main and routing  relevance feedback  tasks in the trec 1 legal track. for the main task  we analyze ranking algorithms using different fields  boolean constraints and structured operators. evaluation results show that structured queries outperform bag-of-words ones. boolean constraints improve both precision and recall. for the routing task  we train a linear svm classifier for each topic. terms with the largest weights are selected to form new queries. both keywords and simple structured features  term.field  have been investigated. named-entity tags  lingpipe sentence breaker and metadata fields of the original documents are used to generate the field information. results show that structured features and weighted queries improves retrieval  but only marginally. we also show which structures are more useful. it turns out metadata fields are not as important as we thought.
1. introduction
the goal of legal search is to retrieve all relevant documents for production requests. a production request describes a set of documents that the plaintiff forces the defendant to produce. the plaintiff usually forms comprehensive requests to cover large amount of documents that are potentially useful in the trial. due to the high risk of missing important documents  legal search systems are usually recall-oriented.
legal track 1 and 1 both use the iit cdip collection 1. it consists of 1 1 business records from us tobacco companies and research institutes. each document contains ocr text and multiple fields of metadata  including title  authors  organizations  etc.
for the main task  1 topics from legal 1 are used as training data. for evaluation  1 new topics are generated from four hypothetical complaints. each topic contains detailed information about the background  instruction and negotiation history. four fields are especially important for retrieval. first  the plaintiff describes the desired documents in requesttext  rt . based on rt  the defendant proposes a boolean query called proposalbydefendant  pd . then the plaintiff modifies pd to form a new query rejoinderbyplaintiff  rp . finally  the two parties agree on a finalquery  fq   which is actually used to retrieve documents.
for the routing task  1 topics from 1 are adopted for evaluation. systems take advantage of existing relevance judgements to retrieve more relevant documents. during evaluation  all previously judged documents are filtered out from the ranked lists  and performance metrics are calculated based on newly judged documents.
the rest of this paper is structured as follows. section 1 and 1 describe our methods and experiments for the main and routing tasks. we conclude with section 1.
1. main task
 this section introduces our experiments for the main task. first we describe the formation of indri queries based on request topics. then we compare the results of different runs using various boolean constraints and ranking functions.
1 query formulation
a typical indri query in our experiments has two components: the boolean constraint and the ranking algorithm. for each topic  we convert finalquery to a boolean constraint  and combine terms from different fields for ranking. when the original query contains 'but not'  the indri query is of the general form
       #filrej z #filreq #band x  #combine y    otherwise  it's of the simpler form
#filreq #band x  #combine y  
here  x and z are boolean constraints  and y is the ranking component.
1.1 wildcards
most queries in the data set contain lots of wildcards. for example   boost!  matches all words with prefix  boost . since the ocr texts contain errors  there are typically thousands of matches to one wildcard expression. if we directly translate these wildcards to indri wildcards  the query execution will be very time consuming.
to speed up the retrieval  we expand the wildcards as query pre-processing. from all the terms matching a wildcard  top k words with the highest document frequencies are selected. for instance  given wildcard  multipl!    multiple    multiply and multiplicity are selected. this method dramatically reduces the number of inverted lists to be merged at query time. since top k frequent words cover the majority of matches  the potential loss in recall is low. however  queries expanded by this method does not perform well. one reason is that wildcards bring in noisy words. for example  in topic 1   high!  is expanded to  highlight    highway  and  highland . in topic 1   bee!  is expanded to  beer    beef    beach   etc.
by examining the original queries  we find that most of the wildcards are unnecessary if we use stemmer during indexing. in this paper  wildcards are manually expanded to one or multiple lexically-related terms according to the topic description. a special case is for years:  1!  is expanded to  #syn 1  1  ...  1  .
1.1 boolean constraints
each topic in the legal track provides three boolean queries:
proposalbydefendant  rejoinderbyplaintiff and finalquery. they contain boolean and proximity operators. we build a parser to parse original queries into trees  and then convert the trees to indri queries. table 1 shows the basic mapping rules  similar to those used in . for example  an original query    a or  b c   and d but not e   is converted to indri query #filrej e #filreq #band #syn a #1 b c   d  #combine ...   .
table 1: mapping from original operators to indri operators
original expressionindri expression x y #1 x y x w/k y#uw k+1  x y x or y#syn x y x and y  inner #uw x y x and y  outermost #band x y x but not z#filrej z x 1.1 ranking
suppose the original query is    a or  b c   and d  . the corresponding bag-of-words ranking component would be  #combine a b c d  .
if the phrase operators are respected  the ranking component becomes  #combine a #1 b c  d  . since a phrase usually has much higher idf than any of its composing terms  the phrase has high impact on ranking.
if we view terms connected by 'or' as synonyms  the ranking component becomes  #combine #syn a #1 b c   d  . for example  in topic 1   tv    television  and  cable  are synonyms. they may have different document frequencies  but they are treated as the same word after applying the #syn operator. the tf of a synonym set is the sum of tf of all its members  and the df of a synonym set is the number of documents containing any of its members.
1 experiments
we use the program kindly provided by howard turtle to transform original xml files to trec web format. during preprocessing  word segments separated by hyphens are connected 1. we use porter stemmer during indexing. since we don't use wildcards in indri queries  stemming compensate loss in recall to some extent.
scanned documents contains ocr errors. if the error rate is high  it is worthwhile to correct errors before indexing. observation shows that ocr errors in the cdip collection are almost character-wise  context-free   which means the  mis recognition of a character does not depend on the characters around it. experiments show that per character error rate is around 1%  which is acceptable. we don't correct ocr errors in this paper.
1.1 submitted runs
nine runs are produced for 1 queries  eight of them are submitted for official pooling and evaluation. 1 results are produced for each query.
cmul1std is the standard condition run required by legal track. it takes the keywords in requesttext to form a bag-of-words query using the  #combine  operator in indri. common query headers  e.g.  please produce any and all documents that discuss   that are not meaningful to the topic are removed.
cmul1 is an okapi ranked list using terms in finalquery. the parameters of bm1 function are the same with those used in : k1 = 1  k1 = 1  k1 = 1 and b = 1. cmul1 is the same with cmul1 except that it combines terms from three fields: proposalbydefendant  rejoinderbyplaintiff and finalquery.
cmul1irt is the bag-of-words query using keywords from finalquery. it ignores boolean constraints. cmul1ibt is the same with cmul1irt except that it uses boolean constraints to filter ranked lists. if the filtered list has less than 1 results  top ranked results from cmul1irt are appended to the end of the list. duplicate documents are removed.
cmul1irp is the same with cmul1irt except that it respects phrase operators in ranking. this run is not submitted because each group can submit up to eight runs. cmul1ibp is the same with cmul1irp except that it uses boolean constraints. it appends list with results from cmul1irp.
cmul1irs is the same with cmul1irp except that it treats all terms connected by or as synonyms. cmul1ibs is the same with cmul1irs except that it uses boolean constraints. it appends list with results from cmul1irs.
1.1 evaluation results
the legal community is more interested in recall than precision. legal 1 takes a novel sampling method  the l1 method  to support deep pooling. systems are required to return 1 documents for each query. the sampling probability of a document is inversely proportional to its highest rank in all submitted runs.
table 1 shows the evaluation results on 1 topics from legal 1. refl1b is a reference boolean run provided by the organizers. median is the median value over all 1 submitted runs  and max is the maximum value. the reference run strictly follows the finalquery and provides a b value for each query. the b value is the number of documents matching finalquery. according to the sampling method  the organizers recommend estrb as the primary measure  which is the estimated recall b. we use estpb  estimated precision at b  as an auxiliary metric.
judgedb is the judged documents at b. okapi based runs have much more documents judged than indri based ones. we suspect it is because most participating groups are using okapi ranking  as the case in last year. if there are large amount of similar runs  the l1 method tends to sample more documents from those runs.
as last year  the reference run still outperforms all submitted runs. among our nine runs  ibs is the best performing run in terms of both estrb and estpb. comparing o1 and o1  using three fields does help. since the finalquery usually covers all terms mentioned in proposalbydefendant and rejoinderbyplaintiff  the improvement primarily comes from better term weighting. comparing ibt and irt  boolean filters significantly improve performance. comparing ibt and ibp  using phrase operators alone actually hurts performance a little bit. comparing ibt and ibs  synonym operators improve both precision and recall.
table 1: performance on 1 topics of legal 1  with estrb as the primary measure.  *irp is not
submitted 
runjudgedbestrbestpbestr1krefl1b1.1.1-max1.1.1.1median1.1.1.1std1.1.1.1o1111o1111irt1.1.1.1irp*1.1.1.1irs1.1.1.1ibt1.1.1.1ibp1.1.1.1ibs1.1.1.1table 1 compares different methods on 1 and 1 topics. since legal 1 evaluation method does not support estimated metrics  we use traditional metrics. okapi ranking using three fields achieves the highest r b and map. since the sampling methods adopted by legal 1 and 1 are vastly different   the comparison should be taken with a grain of salt.
table 1: performance in legal 1 and 1  using traditional metrics.  *irp is not submitted 
1methodr bmapr bmaprefl1b1-1-max--11median--11std1111o1.1.1.1.1o1.1.1.1.1irt1111irp*1111irs1111ibt1111ibp1111ibs1111figure 1 compares per-topic estrb between cmul1ibs and median performance of 1 manual runs from all the groups. 1 out of 1 queries performs better than median  and four of them  1  1  1 and 1  achieve the highest estrb among all runs. figure 1 compares per-topic estpb. 1 queries are above the median  and two of them  1 and 1  achieve the highest estpb.

figure 1: main task: difference from median estrb of 1 manual runs
1.1 error analysis
table 1 lists six topics on which refl1b and cmul1ibs behave most differently. there are three major reasons for the performance gap: chained proximity  wildcards and estimation error.
we manually expand wildcards with relevant words  and use porter stemmer to aggregate words with the same root. this

figure 1: main task: difference from median estpb of 1 manual runs
strategy works for most of the queries. however  when wildcards match unexpected words  it may either improve or hurt performance. here we show an example where wildcards luckily improve estrb. topic 1 requests  all documents referring to the scientific or chemical process es  which result in onions have the effect of making persons cry . the finalquery is:
  scien! or research! or chemical  w/1 onion!  and  cries or cry! or tear! 
intuitively  we expand  cry!  to  cry . comparing the results of refl1b and cmul1ibs  the latter misses two relevant documents:  gar1  and  brq1   neither of which contains any form of  cry  or  tear . however  they contain non-relevant terms such as  crystalline  and  cryptococcus   and hit the relevant documents.
some final queries contain the  chained proximity  operators:  x w/k1 y w/k1 z   which requires the same occurrence of word y to satisfy x w/k1 y and y w/k1 z . one possible approximation in indri is
#band #uw k1 + 1  x y 	#uw k1 + 1  y z 
#uw k1 + k1 + 1  x y z  
however  the  y  in the indri expressions could be different occurrences of the same word. therefore  the indri expression relaxes the original constraint. for topics 1 and 1  the relaxation hurts estrb  but for topic 1 and 1  it improves estrb.
in the l1 evaluation method   the number of relevant documents is estimated as
estr =
where n is number of judged relevant documents  and pi is the sampling probability of document i. although the estimator is unbiased  it has high variance when n is small. it is often dominated by relevant documents with low sampling probability. for topic 1  six documents are judged as relevant for both refl1b and cmul1ibs at b  1 . between the two sets of relevant documents  only one is different. refl1b gets chg1  p = 1   while cmul1ibs gets  ake1   p = 1 . the latter gets much higher estrb because of a single document with low sampling probability. for topic 1  the number of judged relevant documents for refl1b and cmul1ibs at b  1  are three and eleven  respectively. one of the three documents by refl1b   twh1   has p = 1  while all eleven documents by cmul1ibs has p = 1. consequently  refl1b achieves much higher estrb.
table 1: topics on which refl1b and cmul1ibs perform most differently  measured in estrb 
topics111refl1b111111ibs111111diff-111-11-11. routing task
 in the routing task  1 topics from last year's  1  main task with their judgments were used to simulate the routing task scenario where the system is given the information need and judgments of some of the documents returned from an initial retrieval. the routing task could be seen as a relevance feedback task  as true relevance information is known for some documents. in the experiments  at training stage  two thirds of the judged documents from 1 were used as training while one third were used as validation data. for all 1 topics with judgments  1 of them have only 1 judged relevant documents which were backed off to the original queries  instead of the feedback queries.
1 svm based feedback
given enough training documents  the relevance feedback task could be formulated as a supervised document classification problem  disregarding the original query completely. in fact  each query has on average over 1 positive training examples within over 1 total training samples. thus  treating the retrieval for each query as a binary classification task allows us to apply state of the art classification algorithms to solve the problem. as long as the document collection does not change  discarding the original query and using only the training documents would still give reliable results.
1.1 feature selection for classification
in text classification  it is well known that feature selection improves classification accuracy . we try to investigate whether feature selection helps in this noisy ocr corpus for improving precision and recall. in our experiments  top 1 terms that have the highest correlations with relevance are the selected features for classification. the correlation is measured by information gain as in .
1.1 term selection & expansion
next  a linear svm classifier 1 with all default parameters is trained to obtain a linear classification model  which is just a weight vector for linear combination of feature values. the final list of words/features that corresponds to the highest svm weights are selected and used to construct a new query.
1.1 svm classification using indri queries
since the svm term selection and feedback retrieval is just trying to use a query to approximate an svm  we could approximate even better than just un-weighted keyword queries.
when plugging in the weights learnt with svm into the #weight operator of indri query language 1  when the feature values of the training samples are just tfidf scores similar to that of the retrieval model  the resulting weighted term query will be effectively classifying and ranking each document according to the linear svm classifier.
effectively  the retrieval system is using the inverted index to help classify and rank documents. how is this possible  linear svm classifier based on tfidf features is:
scoresvmtfidf ti d 
where the w's are svm weights learnt from training data  v is the set of all terms in the collection. the evaluation of a weighted sum operator of the indri query language on the document d is just
	scorelm q d  = xi	wi   logp ti|d 
                       t ¡Êq where q = #weight w1 .. wntn 
as long as the term feature values are similar during training and at retrieval time  the weights learnt from a svm model should be helpful to other similar retrieval models as well. in the experiments  for the term feature values at training time  we used tft d    1 + k  
avglen 
which is a variant of the okapi bm1 formula with a modified df component so that there are never negative weights. at retrieval time  the weighted retrieval in indri uses dirichlet smoothing to generate term tfidf scores.
1 simple structured features
besides keywords  words that appeared in a particular metadata field or named-entity  ne  annotation could also be used to enlarge the feature space  so that the classifier could pick up features that predict relevance better than simple keywords. this new term consisting of the term + its field information can also be used in document retrieval  in indri query language  simply  term.field   for example   bush.person  where person is a named-entity tag which helps disambiguate bush  . in the experiments bbn's namedentity tagger   identifinder and the lingpipe 1 sentence breaker are used for generating additional annotations. the metadata fields are also included in generating term.field features.
one interesting thing to know is whether annotations would help retrieval. we investigate whether and how many of these structured features have been weighted highly by the svm learner  linear svm .
1 experiments
for different feedback retrieval algorithms  we report results comparing svm term selection & expansion method  denoted as ex   svm approximation using weighted query with only positive weight features  wq  and weighted query of positive and negative weight features  wqn . for different feature sets  we compare unstructured keyword features  kw  with structured term.field features  f . for unweighted term query  ex  we used top 1 high weight terms from svm model. for wq and wqn runs  we used top 1 positive weight terms and top 1 terms with highest absolute weights respectively. these parameters are trained on the judged documents from legal 1. as the vocabulary of the ocr collection is noisy and huge  words with df smaller than 1 have been discarded. also  because some of the training documents are huge  only the first 1  term.field  features have been included for each document and further reduced to 1 such structured features that are highly correlated with relevance for each query  shared among the training documents.
to compare the effects of different evaluation metrics  map  recallb and est rb are used in evaluation. before evaluating these routing runs  all documents used in creating feedback queries  are excluded from the final result set. these documents are the judged relevant and non-relevant documents from trec legal track 1. the newly assessed ones have been used to evaluate the effectiveness of the routing task methods.
table 1 shows the evaluation scores for the different methods as evaluated on the trec 1 legal track assessments. in trec legal track 1  1 topics have been evaluated  and on average each topic has over 1 judged relevant documents within about 1 total judged documents. we randomly splitted the data and used 1 as training  1 for evaluation. results show that 1  same as in the main task  using boolean filter helps improve retrieval effectiveness  1  using weight is better than unweighted term expansion and 1  although lots of the expansion terms are structured  see table 1   the increase in retrieval effectiveness is only marginal - maybe these term.field structures are not complex/precise enough to be more accurate than keywords as indications of relevance.
table 1: performance on 1 topics of legal 1 routing task  with 1 documents for training  1 for validation. kw: keyword feature only  f: ne+sentence+metadata field features  f w/o meta: exclude metadata fields.
measuresexwqwqnmap111kwrecall b111r-prec111map111frecall b111r-prec111fmap111w/orecall b111metar-prec111table 1 evaluates the routing task methods on the newly judged 1 topics out of all 1 topics of legal 1.
table 1: performance on 1 topics of legal 1 routing task. topics and feedback documents are from legal track 1.  bf: short for boolean filter. rbase is run cmu1rbase which is the boolean query run  svme is the term expansion run cmu1rfbsvme  svmnp is the weighted query run cmu1rsvmnp which includes negative weight keywords also 
measuresexwqwqnmap111kwrecall b111est rb111map111frecall b111est rb111fmap111w/orecall b111metaest rb111submitted runsrbasesvmesvmnpmap111recall b111est rb111as seen from table 1  the est rb measure is more correlated with map than with recall b. consistent with the results on the development set  structured features and weighted queries help retrieval a bit  but not significant.
table 1: percentage of structured features in the top 1 features selected out by svm  averaged over 1 topics where there are enough training documents. the rest are keywords which constitute less than half of the feedback terms.
nesenmetapercentage1%1%1%in table 1  we summarize on average the percentage of all types of features being selected within top 1 as given by the weights from svm. although term.field structured features constitute slightly more than half of the high weight terms  the increase in effectiveness of the structured feedback runs are only marginally better than keywords only. more accurate structures are yet to be found. contradictory to our intuition  according to both table 1 and 1  the metadata fields only increased precision  map  a bit  but had no effect or even decreased recall  recall b or est rb . maybe for human lawyers  it is simpler and more maintainable to use the metadata field in helping retrieval  but for the routing task  machine can do better with much more field information from named-entity fields etc..
1. conclusions
 this paper reports our experiments using indri structured queries to retrieve legal documents in trec legal 1. legal search is special in that it is more concerned with recall at deep cut-off point. this is because lawyers usually go through thousands of documents. finding or missing an important document may have high impact on the result of the trial.
in the main task  we compare runs with or without boolean constraints  and runs using different fields of legal requests. we study the impact of phrase and proximity operators. we treat or connected words as synonyms. experimental results show that imposing boolean constraints improves both precision and recall. combining multiple fields gets better term weights. structured queries significantly outperform bag-of-words ones.
in the routing task  as compared to the baseline of simple queries of combined keywords  weighted term queries and simple structured queries help retrieval only marginally. also  more than half of the svm selected terms are structured. named-entity and sentence fields appear far more often in the svm high weight features than do metadata fields. given the performance  more accurate structured features need to be designed in order to show a more significant improvement over simple keyword feedback queries.
1. acknowledgements
this research was supported in part by national science foundation grant iis-1  iis-1 and ccr-1. any opinions  findings  conclusions  or recommendations are the authors' and do not necessarily reflect those of the sponsor.
