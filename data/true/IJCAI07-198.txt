coalition formation is a problem of great interest in ai  allowing groups of autonomous  individually rational agents to form stable teams. automating the negotiations underlying coalition formation is  naturally  of special concern. however  research to date in both ai and economics has largely ignored the potential presence of uncertainty in coalitional bargaining. we present a model of discounted coalitional bargaining where agents are uncertain about the types  or capabilities  of potential partners  and hence the value of a coalition. we cast the problem as a bayesian game in extensive form  and describe its perfect bayesian equilibria as the solutions to a polynomial program. we then present a heuristic algorithm using iterative coalition formation to approximate the optimal solution  and evaluate its performance.
1 introduction
coalition formation  widely studied in game theory and economics   has attracted much attention in ai as means of dynamically forming partnerships or teams of cooperating agents. while most models of coalition formation  e.g.  coalitional bargaining processes  assume that agents have full knowledge of types of their potential partners  in most natural settings this will not be the case. generally  agents will be uncertain about various characteristics of others  e.g.  their capabilities   which in turn imposes uncertainty on the value of any coalition. this presents the opportunity to learn about the types of others based on their behavior during negotiation and by observing their performance in settings where coalitions form repeatedly. agents must be able to form coalitions and divide the generated value even in such settings.
모here we present a model of discounted coalitional bargaining under agent type uncertainty. we formulate this as a bayesian extensive game with observable actions   where the actions correspond to proposing choices of potential partners and a payoff allocation  or accepting or rejecting such proposals. our model generalizes related bargaining models by explicitly dealing with uncertainty about agent types  or capabilities  and coalitional values. we formulate the perfect bayesian equilibrium  pbe  solution of this game as a decidable polynomial program. the complexity of the program makes it intractable for all but trivial problems  so we propose an alternative heuristic algorithm to find good agent strategies in the coalitional bargaining game. preliminary experiments illustrate the performance of this heuristic approach.
모although there is a considerable body of work on coalitional bargaining  no existing models deal with explicit type uncertainty. okada  suggests a form of coalitional bargaining where agreement can be reached in one bargaining round if the proposer is chosen randomly. chatterjee et al.  present a bargaining model with a fixed proposer order  which results in a delay of agreement. neither model deals with type uncertainty-instead  they focus on calculating subgame-perfect equilibria  spe . suijs et al.  introduce stochastic cooperative games  scgs   comprising a set of agents  a set of coalitional actions  and a function assigning to each action a random variable with finite expectation  representing action-dependent coalition payoff. though they provide strong theoretical foundations for games with this restricted form of action uncertainty  they do not model explicitly a coalition formation process. kraus et al.  model coalition formation under a restricted form of uncertainty regarding coalitional values in a request for proposal domain. however  type uncertainty is not captured; rather  the mean value of coalitions is common knowledge  and a  manager  handles proposals  they also focus on social welfare maximization rather than individual rationality .
모chalkiadakis and boutilier  proposean explicit model of type uncertainty and show how this translates into coalitional value uncertainty. we adopt their model in our paper. however  their results focus on stability concepts and how coalitions evolve during repeated interaction  as agents gradually learn more about each other's capabilities  in reinforcement learning style . the actual coalition formation processes used are fairly simple and are not influenced by strategic considerations  nor do agents update their beliefs about other agents' types during bargaining. our work analyzes the actual bargaining process in more depth.
1 bayesian coalitional bargaining
we begin by describing the bayesian coalition formation model and then define our coalitional bargaining game.
모we assume a set of agents n = {1 ... n}  and for each agent i a finite set of possible types ti. each agent i has a specific type t 뫍 ti. we let t = 뫄i뫍nti denote the set of type profiles. each i knows its own type ti  but not those of other agents. agent i's beliefs 뷃i comprise a joint distribution over t i  where 뷃i t i  is the probability i assigns to other agents having type profile t i. intuitively  i's type reflects its  abilities;  and its beliefs about the types of others capture its uncertainty about their abilities. for instance  if a carpenter wants to find a plumber and electrician with whom to build a house  her decision to propose  or join  such a partnership  to engage in a specific type of project  and to accept a specific share of the surplus generated should all depend on her probabilistic assessment of their abilities.
모a coalition c   n of members with actual types tc has a value v  tc   representing the value this group can achieve by acting optimally. however  this simple characteristic function representation of the model  is insufficient  since this value is not common knowledge. an agent i can only assess
               of such a coalition based on its beliefs: i    = tc뫍tc 뷃i tc   tc .
모a coalition structure cs partitions n into coalitions of agents. a payoff allocation   given the stochastic nature of payoffs in this setting  assigns to each agent in coalition c its share of the value attained by c  and must be such that for each c 뫍 cs . chalkiadakis and boutilier  define the bayesian core as a generalization of the standard core concept  capturing an intuitive notion of stability in the bayesian coalition formation game.
모while coalition structures and allocations can sometimes be computed centrally  in many situations they emerge as the result of some bargaining process among the agents  who propose  accept and reject partnership agreements . we now define a  bayesian  coalitional bargaining game for the model above as a bayesian extensive game with observable actions. the game proceeds in stages  with a randomly chosen agent proposing a coalition and allocation of payments to partners  who then accept or reject the proposal.
a finite set of bargaining actions is available to the agents.
a bargaining action corresponds to either some proposal  to form a coalition c with a specific payoff allocation pc specifying payoff shares xi to each i 뫍 c  or to the acceptance or rejection of such a proposal. the finitehorizon game proceeds in s stages  and initially all agents are active. at the beginning of stage s 뫞 s  one of the  say
active agents i is chosen randomly with probability 붺 = n to make a proposal  with i 뫍 c . each other j 뫍 c simultaneously  without knowledge of other responses  either accepts or rejects this proposal. if all j 뫍 c accept  the agents in c are made inactive and removed from the game. value vs tc  = 붻s 1v  tc  is realized by c at s  and split according to pc  where 붻 뫍  1  is the discount factor.1 if any j 뫍 c rejects the proposal  the agents remain active  no coalition is formed . at the end of a stage  the responses are observed by all participants. at the end of stage s  any i not in any coalition receives its discounted reservation value 붻s 1v  ti   discounted singleton coalition value .
1 perfect bayesian equilibrium
the coalitional bargaining game described above is clearly an extensive form bayesian game. we assume each agent will

1
모모agents could have different 붻's. as long as these are common knowledge  our analysis holds with only trivial modifications.
adopt a suitable behavioral strategy  associating with each node in the game tree at which it must make a decision a distribution over action choices for each of its possible types. furthermore  since it is uncertain about the types of other agents  its observed history of other agents' proposals and responses give it information about their types  assuming they are rational . thus  the preferred solution concept is that of a perfect bayesian equilibrium  pbe  . a pbe comprises a profile of behavioral strategies for each agent as well a system of beliefs dictating what each agent believes about the types of its counterparts at each node in the game tree. the standard rationality requirements must also hold: the strategy for each agent maximizes its expected utility given its beliefs; and each agent's beliefs are updated from stage to stage using bayes rule  given the specific strategies being played. in this section  we formulate the constraints that must hold on both strategies and beliefs in order to form a pbe.
모let 횰 denote a behavioral strategy for i  mapping information sets  or observable histories h  in the game tree at which i must act into distributions over admissible actions a h . if i is a proposer at h  at stage s   let a h  = p  the finite set of proposals available at h. then 횰h ti 뷇  denotes the  behavioral strategy  probability that i makes proposal 뷇 뫍 p at h given its type is ti. if i is a responder at h  then 횰h ti y  is the probability with which i accepts the proposal on the table  says yes  at h  and 횰h ti n  = 1 횰h ti y  is the probability i says no . let 뷃i denote i's beliefs with 뷃h ti i t i  being i's beliefs about the types of others at h given its own type is ti.
모we define the pbe constraints for the game by first defining the values to  generic  agent i at each node and information set in the game tree  given a fixed strategy for other agents  and the rationality constraints on his strategies and beliefs. we proceed in stages.
모 1  let 뷅 be a proposal node for i at history h at stage s. since the only uncertainty in information set h involves the types of other agents  each 뷅 뫍 h corresponds to one such type vector t i 뫍 t i; let h t i  denote this node in h. the value to i of a proposal is:
qih t i  ti 뷇  = phacc t i  뷇 xivs tc +xph t i  뷇 r qi뷅/뷇/r ti
r
 where:  is the probability that all j 뫍 c  other than i  accept 뷇  this is easily defined in terms of their fixed strategies ; xi is i's payoff share in pc; r ranges over response vectors in which at least one j 뫍 c refuses the proposal; ph t i  뷇 r  denotes the probability of such a response; and qi뷅/뷇/r ti denotes the continuation payoff for i at stage s + 1 at the node 뷅/뷇/r  following n after proposal 뷇 and responses r . this continuation payoff is defined  recursively  below. the value of 뷇 at history h  as opposed to a node  is determined by taking the expectation w.r.t. possible types:.
모 1  suppose i is a responder at node 뷅 = h t i  in history h at stage s. as above  뷅 corresponds to specific t i in h. w.l.o.g. we can assume i is the first responder  since all
responses are simultaneous . let denote the probability that all other responders accept 뷇. we then define the value to i of accepting 뷇 at 뷅 as:

where again r ranges over response vectors in which at least one  refuses 뷇; ph t i  뷇 r  is the probability of such a response; and qi뷅/y/r ti is the continuation payoff for i at stage s + 1 after responses r by its counterparts. the value of accepting at h is given by the expectation over type vectors tc w.r.t. i's beliefs 뷃h ti i as above.
모the value of rejecting 뷇 at 뷅 = h t i  is the expected continuation payoff at stage s + 1:

 where r ranges over all responses  including pure positive responses  of the others .
모 1  we have defined the value for i taking a specific action at any of its information sets. it is now straightforward to define the value to i of reaching any other stage s node controlled by or by nature  i.e.  chance nodes where a random proposer is chosen .
모first we note that  by assuming i responds  first  to any proposal  our definition above means that we need not compute the value to i at any response node  or information set  controlled by j. for an information set hj where j makes a proposal  consider a node 뷅 = hj tj  where j is assumed to be of type tj. then  j's strategy 횱hj tj specifies a distribution over proposals 뷇  determined given the values qjhj tj 뷇  which can be calculated as above  and j's type tj . agent i's value at this node is given by the expectation  w.r.t. this strategy distribution  of its accept or reject values  or if it is not involved in a proposal  its expected continuation value at stage s+1 given the responses of others . its value at hj is then. we define
 where i is the proposer  as in case 1 above.
모finally  i's value at information set h that defines the stage s continuation game  i.e.  where nature chooses proposer  is

where m is the number of active agents  and hj is the information set following h in which j is the proposer.
모 1  we are now able to define the rationality constraints. we require that the payoff from the equilibrium behavioral strategy  exceeds the payoffs of using pure strategies. specifically  in pbe  for all i  ti 뫍 ti  all h that correspond to one of i's information sets  and all actions b 뫍 a h   we have:
x뷃hi  t i  x 횰h ti a qih t i  ti a  뫟 x뷃ih t i qih t i  ti b 
t i	a뫍a h 	t i
모we also add constraints for the bayesian update of belief variables for any agent i regarding type t뷁j of agent j performing aj at any h  for all i  ti 뫍 ti  all h and all aj :

finally  we add the obvious constraints specifying the domain of the various variables denoting strategies or beliefs  they take values in  1  and sum up to 1 as appropriate .
모this ends the formulation of the program describing the pbe. this is a polynomial constraint satisfaction problem: finding a solution to this system of constraints is equivalent to deciding whether a system of polynomial equations and inequalities has a solution . the problem is decidable  but is intractable. for example  an algorithm for deciding this problem has been proposed with exponential complexity . specifically  the complexity of deciding whether a system of s polynomials  each of degree at most d in k variables has a solution is sk+1do k . in our case  assuming a random choice of proposer at each of s rounds  we can show that if 붸 is the number of pure strategies  n the number of agents  t the number of types  then s = o ns   d = ns and k = o 붸nt . this is due to a variety of combinatorial interactions evident in the constraints above  creating as they do interdependencies between belief and strategy variables.
모in summary  the formulation above characterizes the pbe solution of our coalitional bargaining game as a solution of a polynomial program. however  it does not seem possible that this solution can be efficiently computed in general. nevertheless  this pbe formulation may prove useful for the computation of a pbe in a bargaining setting with a limited number of agents  types  proposals and bargaining stages.
1 approximations
the calculation of the pbe solution is extremely complex due to both the size of the strategy space  as a function of the size of the game tree  which grows exponentiallywith the problem horizon   and the dependence between variables representing strategies and beliefs  as explained above. we present an approximation strategy that circumvents these issues to some degree by:  a  performing only a small lookahead in the game tree in order to decide on a action at any stage of the game; and  b  fixing the beliefs of each agent during this process. this latter approach  in particular  allows us to solve the game tree by backward induction  essentially computing an equilibrium for this fixed-beliefs game. note that while beliefs are held fixed during the lookahead  while computing an immediate action   they do get updated once the action is selected and executed  and thus do evolve based on the actions of others  this is in the spirit of receding horizon control . furthermore  we allow sampling of type vectors in the computation to further reduce the tree size.
모more precisely  at any stage of the game  with a particular collection of active agents  each with their own beliefs   we implement the following steps:
1. an agent  e.g.  proposer  constructs a game tree consisting ofthe next d rounds of bargaining  for some small lookahead d .1 all active agents are assumed to have fixed beliefs at each node in this tree corresponding to their beliefs at the current stage. the agent computes its optimal action for the current round using backward induction to approximate an equilibrium  similar in nature to an spe  of this limited depth game.  we elaborate below.  furthermore  they sample partners' types when calculating the values of coalitions and proposals.

1
if less than d rounds remain  the tree is suitably truncated.
1. each player executes its action computed for the current roundof bargaining. if a coalition is formed  it breaks away  leaving the remaining players as active.
1. all active agents update their beliefs  given the observed actions of others in the current round  using bayesian updating. further  each agent keeps track of the belief updates that any other agent of a specific type would perform at this point.
1. the next bargaining round is implemented by repeating thesesteps until a complete coalition structure is determined or the maximum number of bargaining rounds is reached.
모we stress that the algorithm above does not approximate the pbe solution; getting good bounds for a true pbe approximation would only be likely by assuming belief updating at every node of the game tree mentioned in step 1. however  if our algorithmic assumptions are shared by all agents  each can determine their best responses to others'  approximately  optimal play  and thus their play approximates an equilibrium of the fixed-beliefs game. indeed  we can define a sequential equilibrium under fixed beliefs  sefb  as an extension of the spe and a restriction of the pbe for a fixed-beliefsbargaining game  and can show the following  stated informally here :
theorem 1 if the bayesian core  bc  of a bayesian coalitional game g  is non-empty  and so is the bc of each one of g's subgames  then-regardless of nature's choice of proposers -there is an sefb strategy profile of the corresponding fixed-beliefs discounted bayesian coalitional bargaining game that produces a bc element; and conversely  if there is an order independent1 sefb profile for a bayesian coalitional bargaining game  then it leads to a configuration that is in the bc of the underlying g.
this result describes some notion of equivalence between cooperative and non-cooperative bayesian coalition formation solution concepts  and is similar to results  e.g.  moldovanu et al.   for non-stochastic environments. it also motivates further step 1 of our heuristic algorithm  equating fixed belief equilibrium computation with determination of  i's part of  the bayesian core. we now elaborate on this process.
모we assume that the agents proceed to negotiations that will last d rounds  corresponding to the algorithm's lookahead value d  underthe assumption that all beliefs will remain fixed to their present values throughout the  step 1  process. we will present the deliberations of agent i during negotiations. for fixed types t i of possible partners  drawn according to 뷃i  i will reason about the game tree and assume fixed beliefs of other agents.  agents will track of the updates of other agents' beliefs after this stage of bargaining; see step 1 above . then  i can calculate the optimal action of any tj agent  including himself  at any information set by taking expectations over the corresponding tree nodes.
모we begin our analysis at the last stage d of negotiations. in any node 뷅 after history h where i of type ti is a responder to proposal 뷇 뫍 p and assumes a specific type vector for partners  he expects a value for accepting that is different to his  discounted  reservation value only if all other responders accept the proposal as well:
	qih t i  ti y = j xvidv tdi  tc  if allotherwiset i 뫍 tc accept	 1 

1
모모a strategy profile is order independent iff when played it leads to a specific  independently of the choice of proposers.
however  to evaluate this acceptance condition  i would need to know the other responders' strategies  which in turn depend on i's strategy . therefore  i will make the simplifying assumption that all other responders j evaluate their response to 뷇 by assuming that the rest of the agents  including i  will accept the proposal. thus  anyj with tj 뫍 t i is assumedby i to accept if he evaluates his expected payoff from acceptance as being greater than his  discounted  reservation payoff:
	xj xj뫍 뷃j t j vd {tj t j}  뫟 vd tj 	 1 
t  tc
 with this assumption  i is able to evaluate the acceptance condition in eq. 1 above  and so calculate a specific qih t i  ti y  value. note that the use of this assumption can sometimes lead to an overestimate of the value of a node.
모at node 뷅 = h t i   i can also evaluate his refusal value as qih t i  ti n  = vd ti  in this last round. then  responder i's actual strategy at h can be evaluated as the strategy maximizing i's expected value given 뷃h ti i:
횰h ti = arg max
t i뫍tc
모if i is a proposer of type ti deliberating at 뷅 = h t i   the value of making proposal 뷇 is:
		 1 
  i.e.  i will get his reservation value unless all the responders of the specific type configuration agree to this proposal . furthermore  i's expected value qih ti 뷇  from making proposal 뷇 to coalition c at h can be determined given 뷃h ti i. thus  the best proposal that i of type ti can make to coalition c is the one with maximum expected payoff: 횰c;h ti =
argmax뷇 qih ti 뷇  with expected payoff qic;h ti.
모however  i can also propose to other coalitions at h as well. therefore  the coalition c  to which i should propose is the one that guarantees him the maximum expected payoff: c  = argmaxc{qic;h ti}. if p  is the payoff allocation associated with that proposal  then the optimal coalitionallocation pair that ti can propose in this subgame  that starts with i proposing at h  is: 횰 ;h ti = {c  p } with maximum expected payoff qic ;h ti. finally  if there exist more than one optimal proposal for i  i randomly selects any of them  this is taken into account in agents' deliberations accordingly .
모of course  when the subgame starts an agent i does not know who the proposer in this subgame will be; and i has only probabilistic beliefs about the types of his potential partners. thus  i has to calculate his continuation payoff qid:뷅 ti at stage d  that starts at node 뷅  with m participants  in the way explained in the previous section. this is straightforward  as i can calculate his expected payoffs from participating in any subgame where some j proposes  given that any i can calculate the optimal strategies  and associated payoffs  for any j in this round d subgame.
모now consider play in a subgame starting in period d   1  again with the participation of m agents. the analysis for this round can be performed in a way completely similar to the one performed for the last round of negotiations. however  there is one main difference: the payoffs in the case of a rejection are now the continuation payoffs  for agents of specific type  from the last round subgame. we have to incorporate this difference in our calculations. other than that  we can employ a similar line of argument to the one used for identifying the equilibrium strategies in the last period. proceeding in this way  we define the continuation payoffs and players' strategies for each prior round  and finally determine the first round actions for any proposer i of type ti or any responder j of type tj responding to any proposal.
1 experimental evaluation
to evaluate our approach  we first conducted experiments in two settings  each with 1 agents having 1 possible types. agents repeatedly engage in episodes of coalition formation  each episode consisting of a number of negotiation rounds. we compareourbayesian equilibriumapproximationmethod  be  with kst  an algorithm inspired by a method presented by kraus et al. . though their method is better tailored to other settings  focusing on social welfare maximization  it is a rare example of a successfully tested discounted coalitional bargaining method under some restricted form of uncertainty  which combines heuristics with principled game theoretic techniques. it essentially calculates an approximation of a kernel-stable allocation for coalitions that form in each negotiation round with agents intentionally compromising part of their payoff in order to form coalitions. like   our kst uses a compromise factor of 1  but we assume no central authority  only one agent proposing per round  and coalition values estimated given type uncertainty.
모during an episode  agents progressively build a coalition structure and agree on a payment allocation. the action executed by a coalition at the end of an episode  the coalitional action  results in one of three possible stochastic outcomes o 뫍 o = {1 1} each of differing value. each agent's type determines its  quality  and the  quality  of a coalition is dictated by the sum of the quality of its members less a penalty for coalition size.1 coalition quality then determines the odds of realizing a specific outcome  higher quality coalitions have greater potential . finally  the value of a coalition given member types is the expected value w.r.t. the distribution over outcomes.
모in our first setting  singleton coalitions receive a penalty of -1 quality points. we compare be and kst under various learning models by measuring average total reward garnered by all coalitions in 1 runs of 1 formation episodes each  with a limit of 1 bargaining rounds per episode and a bargaining discount factor of 붻 = 1. we also compare average reward to the reward that can be attained using the optimal  fixed  kernel-stable  coalition structure.
모we compared be and kst using agents that update their prior over partner types after observing coalitional actions- thus learning by reinforcement  rl  after each episode-and those that do not  no rl . in all cases  be agents update their beliefs after observing the bargaining actions of others

1
모모we omit the details here. we only note that agent 1  of type 1  is detrimental to any coalition  in our 1 first settings .
during each negotiation round. there are 1 proposals a be agent considers when negotiating in a stage with all five agents present  fewer in other cases .
모table 1 a  shows performance when each agent has a uniform prior regarding the types of others. the be algorithm consistently outperforms kst  even though kst promotes social welfare  i.e.  is well-aligned with total reward criterion  rather than individual rationality. kst agents without rl always converge to the coalition structure; this is due to the fact that they are discouraged from cooperating due to the lack of information about their counterparts. when kst agents learn from observed actions after each episode  kst-uni-rl  they form the coalitions in the last episode in 1 of 1 runs. be agents  in contrast  form coalitions based on evolving beliefs about others  and do not form the optimal structure  rather they tend to form coalitions of 1 or 1 members which exclude agent 1 from being their partner. in addition  payoff division for be agents is more aligned with individual rationality than it is with kst. the shares of  averaged  total payoff of kstuni-rl agents 1 are 1% 1% 1% 1% 1%  respectively  while for be-uni-rl  ss:1  la:1  they are 1% 1% 1% 1% 1%; this more accurately reflects the power  of the agents. be results are reasonably robust with changing sample size and lookahead value  at least in this environment with 1 possible type vectors in a 1-agent coalition .
모we attribute the poor performance of kst agents to the fact that they make their proposals without in any way taking into consideration the changing beliefs of others. with the beliefs of the agents varying  negotiations drag  up to the maximum of 1 rounds  due to refusals  resulting in reduced payoffs. be agents do not suffer fromthis problem since they keep track of all possible partners' updated beliefs  and use them during negotiation. thus  they typically form a coalition structure within the first four rounds of an episode.
모we also experimented with a second setting in which singleton coalitions receive a penalty of -1 quality points  rather than -1 above   and where   as coalitions get bigger they get penalized to reflect coordination difficulties . this setting makes the quality of coalitions more difficult to distinguish. here  a near-optimal configuration contains the structure. we use three different priors: uniform  misinformed  agents have an initial belief of 1 that an agent with type t has type t + 1    and informed  belief 1 in the true type of each other agent .
모the results  table 1 b   indicate that kst agents again do not do very well  engaging in long negotiations due to unaccounted-for differences in beliefs among the various agents. kst-uni-rl agents  for example  typically use all ten bargaining rounds; in contrast  be-uni-rl usually form structures within 1 rounds. even when kst uses informed priors  the fact that the expected value of coalitions is not common knowledge takes its toll. be agents  on the other hand  derive the true types of their partners with

1
methodreward optimal  cs1  expected kst-uni-norl1 1 %kst-uni-rl1 1 % be-uni-norl1 1% be-uni-rl1 1% kst-mis-norl1 1 %kst-mis-rl1 1 %be-mis-norl1 1% be-mis-rl1 1% kst-inf-norl1 1% kst-inf-rl1 1% be-inf-norl1 1% be-inf-rl1.1% methodreward optimal  cs1  expected kst-uni-norl1 1% kst-uni-rl1 1% be-uni-norl ss=1  la=1.1.1% be-uni-rl ss=1  la=1.1.1% be-uni-norl ss=1  la=1.1.1% be-uni-rl ss=1  la=1.1.1% be-uni-norl ss=1  la=1 1% be-uni-rl ss=1  la=1.1.1% methodqa/bkst-norl-111be-norl-111kst-norl-111be-norl-111kst-rl-111be-rl-111kst-rl-111be-rl-111 c  setting c; uniform priors; be
 a  setting a
uses ss=1  la=1; a/b denotes observed relative power of a over b
 b  setting b;  be uses ss=1  la=1 
table 1: settings' results  average . ss :samplesize; la :lookahead; uni :uniform  mis :misinformed  inf :informedprior.모모nor should they  given bargaining horizon and 붻-the kernel and other stability concepts do not consider bargaining dynamics. certainty in all experiments  and typically form profitable configurations with structures such asor
. we can also see that rl enhances the performance of be agents slightly  helping them further differentiate the quality of various partners.
모we also report briefly on the results in a setting with 1 agents  of 1 possible types per agent  1 agents of type a  1 of type b . the relative power of type a over b is 1.1 in this setting  forming coalitions by mixing agent types is detrimental  with the exception of the   optimal     and coalitions. there are 1 proposals an agent considers when negotiating in a stage with all 1 agents present. the setting makes discovery of opponent types difficult  and thus rational agents should settle for suboptimal coalitions  hopefully using them as stepping stones to form better ones later . we also varied the bargaining 붻  1 and 1 . agents do not accumulate much reward in this setting  bargaining for many rounds. instead of reporting reward  we report expected value q of formation decisions 
  with fc being the observed average frequency with which coalition c forms and v  c  its expected value. results  table 1 c   show that be agents outperform kst agents both in terms of social welfare and individual rationality  the observed relative power of types-the fraction of respective observed payoffs-is close to the true power   and that rl updatesare quite beneficial. further lowering the discount rate to 1 forces the agents to form coalitions early  but also contributes to better decisions  because it enables the agents to discover the types of opponents with more accuracy  effectively reducing the numberof possible opponentresponses during bargaining  intuitively  given more time  both a  strong  and a  weak  type might refuse a proposal  while if time is pressing the  weak  might be the only one to accept .
1 concluding remarks and future work
we proposed an algorithm for coalitional bargaining under uncertainty about the capabilities of potential partners. it uses

1
모모relative power a/b is the expected payoff of a in coalitions excluding b  over the expected payoff of b in coalitions without a.
iterative coalition formation with belief updating based on the observed actions of others during bargaining  and is motivated by our formulation of the pbe solution of a coalitional bargaining game. the algorithm performs well empirically  and can be combined with belief updates after observing the results of coalitional actions  in reinforcementlearning style .
모future and current work includes implementing a continuous bargaining action space version of our algorithm  and also incorporating it within a broader rl framework facilitating coalition formation and sequential coalitional decision making under uncertainty. we are also investigating approximation bounds for our heuristic algorithm.
acknowledgments
thanks to vangelis markakis for extremely useful discussions and helpful comments.
