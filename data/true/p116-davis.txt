localized search engines are small-scale systems that index a particular community on the web. they offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build  and can provide more precise and complete search capability over their relevant domains. one disadvantage such systems have over large-scale search engines is the lack of global pagerank values. such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole. in this paper  we present well-motivated algorithms to estimate the global pagerank values of a local domain. the algorithms are all highly scalable in that  given a local domain of size n  they use o n  resources that include computation time  bandwidth  and storage. we test our methods across a variety of localized domains  including site-specific domains and topic-specific domains. we demonstrate that by crawling as few as n or 1n additional pages  our methods can give excellent global pagerank estimates.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval; g.1  numerical analysis : numerical linear algebra; g.1  probability and statistics : markov processes
general terms
pagerank  markov chain  stochastic complementation
keywords
algorithms  experimentation
1. introduction
¡¡localized search engines are small-scale search engines that index only a single community of the web. such communities can be site-specific domains  such as pages within
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro¡êt or commercial advantage and that copies bear this notice and the full citation on the ¡êrst page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci¡êc permission and/or a fee.
kdd'1  august 1  1  philadelphia  pennsylvania  usa.
copyright 1 acm 1-1/1 ...$1.
the cs.utexas.edu domain  or topic-related communities- for example  political websites. compared to the web graph crawled and indexed by large-scale search engines  the size of such local communities is typically orders of magnitude smaller. consequently  the computational resources needed to build such a search engine are also similarly lighter. by restricting themselves to smaller  more manageable sections of the web  localized search engines can also provide more precise and complete search capabilities over their respective domains.
¡¡one drawback of localized indexes is the lack of global information needed to compute link-based rankings. the pagerank algorithm   has proven to be an effective such measure. in general  the pagerank of a given page is dependent on pages throughout the entire web graph. in the context of a localized search engine  if the pageranks are computed using only the local subgraph  then we would expect the resulting pageranks to reflect the perceived popularity within the local community and not of the web as a whole. for example  consider a localized search engine that indexes political pages with conservative views. a person wishing to research the opinions on global warming within the conservative political community may encounter numerous such opinions across various websites. if only local pagerank values are available  then the search results will reflect only strongly held beliefs within the community. however  if global pageranks are also available  then the results can additionally reflect outsiders' views of the conservative community  those documents that liberals most often access within the conservative community .
¡¡thus  for many localized search engines  incorporating global pageranks can improve the quality of search results. however  the number of pages a local search engine indexes is typically orders of magnitude smaller than the number of pages indexed by their large-scale counterparts. localized search engines do not have the bandwidth  storage capacity  or computational power to crawl  download  and compute the global pageranks of the entire web. in this work  we present a method of approximating the global pageranks of a local domain while only using resources of the same order as those needed to compute the pageranks of the local subgraph.
¡¡our proposed method looks for a supergraph of our local subgraph such that the local pageranks within this supergraph are close to the true global pageranks. we construct this supergraph by iteratively crawling global pages on the current web frontier-i.e.  global pages with inlinks from pages that have already been crawled. in order to provide a good approximation to the global pageranks  care must be taken when choosing which pages to crawl next; in this paper  we present a well-motivated page selection algorithm that also performs well empirically. this algorithm is derived from a well-defined problem objective and has a running time linear in the number of local nodes.
¡¡we experiment across several types of local subgraphs  including four topic related communities and several sitespecific domains. to evaluate performance  we measure the difference between the current global pagerank estimate and the global pagerank  as a function of the number of pages crawled. we compare our algorithm against several heuristics and also against a baseline algorithm that chooses pages at random  and we show that our method outperforms these other methods. finally  we empirically demonstrate that  given a local domain of size n  we can provide good approximations to the global pagerank values by crawling at most n or 1n additional pages.
¡¡the paper is organized as follows. section 1 gives an overview of localized search engines and outlines their advantages over global search. section 1 provides background on the pagerank algorithm. section 1 formally defines our problem  and section 1 presents our page selection criteria and derives our algorithms. section 1 provides experimental results  section 1 gives an overview of related work  and  finally  conclusions are given in section 1.
1. localized search engines
¡¡localized search engines index a single community of the web  typically either a site-specific community  or a topicspecific community. localized search engines enjoy three major advantages over their large-scale counterparts: they are relatively inexpensive to build  they can offer more precise search capability over their local domain  and they can provide a more complete index.
¡¡the resources needed to build a global search engine are enormous. a 1 study by lyman et al.  found that the 'surface web'  publicly available static sites  consists of 1 billion pages  and that the average size of these pages is approximately 1 kilobytes. to download a crawl of this size  approximately 1 terabytes of space is needed. for a researcher who wishes to build a search engine with access to a couple of workstations or a small server  storage of this magnitude is simply not available. however  building a localized search engine over a web community of a hundred thousand pages would only require a few gigabytes of storage. the computational burden required to support search queries over a database this size is more manageable as well. we note that  for topic-specific search engines  the relevant community can be efficiently identified and downloaded by using a focused crawler  1  1 .
¡¡for site-specific domains  the local domain is readily available on their own web server. this obviates the need for crawling or spidering  and a complete and up-to-date index of the domain can thus be guaranteed. this is in contrast to their large-scale counterparts  which suffer from several shortcomings. first  crawling dynamically generated pages-pages in the 'hidden web'-has been the subject of research  and is a non-trivial task for an external crawler. second  site-specific domains can enable the robots exclusion policy. this prohibits external search engines' crawlers from downloading content from the domain  and an external search engine must instead rely on outside links and anchor text to index these restricted pages.
¡¡by restricting itself to only a specific domain of the internet  a localized search engine can provide more precise search results. consider the canonical ambiguous search query  'jaguar'  which can refer to either the car manufacturer or the animal. a scientist trying to research the habitat and evolutionary history of a jaguar may have better success using a finely tuned zoology-specific search engine than querying google with multiple keyword searches and wading through irrelevant results. a method to learn better ranking functions for retrieval was recently proposed by radlinski and joachims  and has been applied to various local domains  including cornell university's website .
1. pagerank overview
¡¡the pagerank algorithm defines the importance of web pages by analyzing the underlying hyperlink structure of a web graph. the algorithm works by building a markov chain from the link structure of the web graph and computing its stationary distribution. one way to compute the stationary distribution of a markov chain is to find the limiting distribution of a random walk over the chain. thus  the pagerank algorithm uses what is sometimes referred to as the 'random surfer' model. in each step of the random walk  the 'surfer' either follows an outlink from the current page  i.e. the current node in the chain   or jumps to a random page on the web.
¡¡we now precisely define the pagerank problem. let u be an m ¡Á m adjacency matrix for a given web graph such that uji = 1 if page i links to page j and uji = 1 otherwise. we define the pagerank matrix pu to be:
	pu = ¦Áudu 1 +  1   ¦Á vet 	 1 
where du is the  unique  diagonal matrix such that udu 1 is column stochastic  ¦Á is a given scalar such that 1 ¡Ü ¦Á ¡Ü 1  e is the vector of all ones  and v is a non-negative  l1normalized vector  sometimes called the 'random surfer' vector. note that the matrix du 1 is well-defined only if each column of u has at least one non-zero entry-i.e.  each page in the webgraph has at least one outlink. in the presence of such 'dangling nodes' that have no outlinks  one commonly used solution  proposed by brin et al.   is to replace each zero column of u by a non-negative  l1-normalized vector. the pagerank vector r is the dominant eigenvector of the pagerank matrix  r = pur. we will assume  without loss of generality  that r has an l1-norm of one. computationally  r can be computed using the power method. this method first chooses a random starting vector r 1   and iteratively multiplies the current vector by the pagerank matrix pu; see algorithm 1. in general  each iteration of the power method can take o m1  operations when pu is a dense matrix. however  in practice  the number of links in a web graph will be of the order of the number of pages. by exploiting the sparsity of the pagerank matrix  the work per iteration can be reduced to o km   where k is the average number of links per web page. it has also been shown that the total number of iterations needed for convergence is proportional to ¦Á and does not depend on the size of the web graph  1  1 . finally  the total space needed is also o km   mainly to store the matrix u.
algorithm 1: a linear time  per iteration  algorithm for computing pagerank.
computepr u 
input: u: adjacency matrix.
output: r: pagerank vector.
choose  randomly  an initial non-negative vector r 1  such that i ¡û 1
repeat
is the random surfing probabil-
ity}
r i  ¡û ¦Í +  1   ¦Á v {v is the random surfer vector.}
untilis the convergence threshold.}
r	r¡û
1. problem definition
¡¡given a local domain l  let g be an n ¡Á n adjacency matrix for the entire connected component of the web that contains l  such that gji = 1 if page i links to page j and gji = 1 otherwise. without loss of generality  we will partition g as:
	g =l	gout	 	 1 
lout gwithin
where l is the n ¡Á n local subgraph corresponding to links inside the local domain  lout is the subgraph that corresponds to links from the local domain pointing out to the global domain  gout is the subgraph containing links from the global domain into the local domain  and gwithin contains links within the global domain. we assume that when building a localized search engine  only pages inside the local domain are crawled  and the links between these pages are represented by the subgraph l. the links in lout are also known  as these point from crawled pages in the local domain to uncrawled pages in the global domain.
¡¡as defined in equation  1   pg is the pagerank matrix formed from the global graph g  and we define the global pagerank vector of this graph to be g. let the n-length vector p  be the l1-normalized vector corresponding to the global pagerank of the pages in the local domain l:
e g
 
where el =   i | 1   is the restriction matrix that selects the components from g corresponding to nodes in l. let p denote the pagerank vector constructed from the local domain subgraph l. in practice  the observed local pagerank p and the global pagerank p  will be quite different. one would expect that as the size of local matrix l approaches the size of global matrix g  the global pagerank and the observed local pagerank will become more similar. thus  one approach to estimating the global pagerank is to crawl the entire global domain  compute its pagerank  and extract the pageranks of the local domain.
¡¡typically  however    i.e.  the number of global pages is much larger than the number of local pages. therefore  crawling all global pages will quickly exhaust all local resources  computational  storage  and bandwidth  available to create the local search engine. we instead seek a supergraph f  of our local subgraph l with size o n . our goal algorithm 1: the findglobalpr algorithm.
findglobalpr l  lout  t  k 
input: l: zero-one adjacency matrix for the local domain  lout: zero-one outlink matrix from l to global subgraph as in  1   t: number of iterations  k: number of pages to crawl per iteration.
output: p : an improved estimate of the global pagerank of l.
f ¡û l
fout ¡ûcomputeprlout
f ¡û	 f 
for  i = 1 to t 
{determine which pages to crawl nextselectnodes } pages ¡û  f  fout  f k 
crawl pages  augment f and modify fout
{update pageranks for new local domaincomputepr	} f ¡û	 f 
end
{extract pageranks of original local domain & normalize}
p 	e fis to find such a supergraph f  with pagerank f   so that f  when restricted to l is close to p . formally  we seek to minimize
	 .	 1 
we choose the l1 norm for measuring the error as it does not place excessive weight on outliers  as the l1 norm does  for example   and also because it is the most commonly used distance measure in the literature for comparing pagerank vectors  as well as for detecting convergence of the algorithm .
¡¡we propose a greedy framework  given in algorithm 1  for constructing f . initially  f is set to the local subgraph l  and the pagerank f of this graph is computed. the algorithm then proceeds as follows. first  the selectnodes algorithm  which we discuss in the next section  is called and it returns a set of k nodes to crawl next from the set of nodes in the current crawl frontier  fout. these selected nodes are then crawled to expand the local subgraph  f  and the pageranks of this expanded graph are then recomputed. these steps are repeated for each of t iterations. finally  the pagerank vector p   which is restricted to pages within the original local domain  is returned. given our computation  bandwidth  and memory restrictions  we will assume that the algorithm will crawl at most o n  pages. since the pageranks are computed in each iteration of the algorithm  which is an o n  operation  we will also assume that the number of iterations t is a constant. of course  the main challenge here is in selecting which set of k nodes to crawl next. in the next section  we formally define the problem and give efficient algorithms.
1. node selection
¡¡in this section  we present node selection algorithms that operate within the greedy framework presented in the previous section. we first give a well-defined criteria for the page selection problem and provide experimental evidence that this criteria can effectively identify pages that optimize our problem objective  1 . we then present our main algorithmic contribution of the paper  a method with linear running time that is derived from this page selection criteria. finally  we give an intuitive analysis of our algorithm in terms of 'leaks' and 'flows'. we show that if only the 'flow' is considered  then the resulting method is very similar to a widely used page selection heuristic .
1 formulation
¡¡for a given page j in the global domain  we define the expanded local graph fj:
	f  	 1 
where uj is the zero-one vector containing the outlinks from f into page j  and s contains the inlinks from page j into the local domain. note that we do not allow self-links in this framework. in practice  self-links are often removed  as they only serve to inflate a given page's pagerank.
¡¡observe that the inlinks into f from node j are not known until after node j is crawled. therefore  we estimate this inlink vector as the expectation over inlink counts among the set of already crawled pages 
f te
	s.	 1 
in practice  for any given page  this estimate may not reflect the true inlinks from that page. furthermore  this expectation is sampled from the set of links within the crawled domain  whereas a better estimate would also use links from the global domain. however  the latter distribution is not known to a localized search engine  and we contend that the above estimate will  on average  be a better estimate than the uniform distribution  for example.
¡¡let the pagerank of f be f. we express the pagerank fj+ of the expanded local graph fj as
	f  	 1 
where xj is the pagerank of the candidate global node j  and fj is the l1-normalized pagerank vector restricted to the pages in f.
¡¡since directly optimizing our problem goal requires knowing the global pagerank p   we instead propose to crawl those nodes that will have the greatest influence on the pageranks of pages in the original local domain l:
	influence	 1 
experimentally  the influence score is a very good predictor of our problem objective  1 . for each candidate global node j  figure 1 a  shows the objective function value global diff fj  as a function of the influence of page j. the local domain used here is a crawl of conservative political pages  we will provide more details about this dataset in section 1 ; we observed similar results in other domains. the correlation is quite strong  implying that the influence criteria can effectively identify pages that improve the global pagerank estimate. as a baseline  figure 1 b  compares our objective with an alternative criteria  outlink count. the outlink count is defined as the number of outlinks from the local domain to page j. the correlation here is much weaker.

	 a 	 b 
figure 1:  a  the correlation between our influence page selection criteria  1  and the actual objective function  1  value is quite strong.  b  this is in contrast to other criteria  such as outlink count  which exhibit a much weaker correlation.
1 computation
¡¡as described  for each candidate global page j  the influence score  1  must be computed. if fj is computed exactly for each global page j  then the pagerank algorithm would need to be run for each of the o n  such global pages j we consider  resulting in an o n1  computational cost for the node selection method. thus  computing the exact value of fj will lead to a quadratic algorithm  and we must instead turn to methods of approximating this vector. the algorithm we present works by performing one power method iteration used by the pagerank algorithm  algorithm 1 . the convergence rate for the pagerank algorithm has been shown to equal the random surfer probability ¦Á  1  1 . given a starting vector x 1   if k pagerank iterations are performed  the current pagerank solution x k  satisfies:
	 	 1 
where x  is the desired pagerank vector. therefore  if only one iteration is performed  choosing a good starting vector is necessary to achieve an accurate approximation.
¡¡we partition the pagerank matrix pfj  corresponding to thesubgraph fj as:
	pfj	  	 1 
where f 	 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡e s 	 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡e u  
 
and diag uj  is the diagonal matrix with the  i i th entry equal to one if the ith element of uj equals one  and is zero otherwise. we have assumed here that the random surfer vector is the uniform vector  and that l has no 'dangling links'. these assumptions are not necessary and serve only to simplify discussion and analysis.
¡¡a simple approach for estimating fj is the following. first  estimate the pagerank fj+ of fj by computing one pagerank iteration over the matrix pfj  using the starting vec-
tor  . then  estimate fj by removing the last component from our estimate of fj+  i.e.  the component corresponding to the added node j   and renormalizing.
¡¡the problem with this approach is in the starting vector. recall from  1  that xj is the pagerank of the added node j. the difference between the actual pagerank fj+ of pfj and the starting vector ¦Í is

thus  by  1   after one pagerank iteration  we expect our estimate of fj+ to still have an error of about 1¦Áxj. in particular  for candidate nodes j with relatively high pagerank xj  this method will yield more inaccurate results. we will next present a method that eliminates this bias and runs in o n  time.
1.1 stochastic complementation
¡¡since fj+  as given in  1  is the pagerank of the matrix pfj  we have:

solving the above system for fj can be shown to yield fj =  f  +  1   w  1s u tj  fj.  1 
the matrix s = f + 1 w  1s u tj is known as the stochastic complement of the column stochastic matrix pfj with respect to the sub matrix f . the theory of stochastic complementation is well studied  and it can be shown the stochastic complement of an irreducible matrix  such as the pagerank matrix  is unique. furthermore  the stochastic complement is also irreducible and therefore has a unique stationary distribution as well. for an extensive study  see .
¡¡it can be easily shown that the sub-dominant eigenvalue of s is at most  where  is the size of f. for sufficiently large   this value will be very close to ¦Á. this is important  as other properties of the pagerank algorithm  notably the algorithm's sensitivity  are dependent on this value .
¡¡in this method  we estimate the length  vector fj by computing one pagerank iteration over thestochastic complement s  starting at the vector f:
	fj ¡Ö sf.	 1 
this is in contrast to the simple method outlined in the previous section  which first iterates over the   matrix pfj to estimate fj+  and then removes the last component from the estimate and renormalizes to approximate fj. the problem with the latter method is in the choice of the   + 1  length starting vector  ¦Í. consequently  the pagerank estimate given by the simple method differs from the true pagerank by at least 1¦Áxj  where xj is the pagerank of page j. by using the stochastic complement  we can establish a tight lower bound of zero for this difference. to see this  consider the case in which a node k is added to f to form the augmented local subgraph fk  and that the pagerank of this new graph is  . specifically  the addition of page k does not change the pageranks of the pages in f  and thus fk = f. by construction of the stochastic complement  fk = sfk  so the approximation given in equation  1  will yield the exact solution.
¡¡next  we present the computational details needed to efficiently compute the quantityover all known global pages j. we begin by expanding the difference fj  f  where the vector fj is estimated as in  1  
fj   f ¡Ö sf   f
e t
e f
	+ 1   w  1 u tj f s   f.	 1 
note that the matrix  df +diag uj   1 is diagonal. letting o k  be the outlink count for page k in f  we can express the kth diagonal element as:
	1	o k1	if uj k  = 1
 df if uj k  = 1
noting that  o k  + 1  1 = o k  1    o k  o k  + 1   1 and rewriting this in matrix form yields
.
we use the same identity to express e	e	e
	.	 1 
recall that  by definition  we have pf.
substituting  1  and  1  in  1  yields
	fj   f ¡Ö	 pff   f 
 ¦Áfdf 1 df + diag uj   1diag uj f
s 
	= x + y +  u tj f z 	 1 
noting that by definition  f = pff  and defining the vectors x y  and z to be
x =	 ¦Áfdf 1 df + diag uj   1diag uj f  1  e
y 
	z =	 1   w  1s .	 1 
the first term x is a sparse vector  and takes non-zero values only for local pages k that are siblings of the global page j. we define  i j  ¡Ê f if and only if f j i  = 1  equivalently  page i links to page j  and express the value of the component   as:
	 	 1 
where o k   as before  is the number of outlinks from page k in the local domain. note that the last two terms  y and z are not dependent on the current global node j. given the
function  the quantity can be expressed as
if we can compute the function can compute each value ofusing an additional amount of time that is proportional to the number of nonzero components in x. these optimizations are carried out in algorithm 1. note that  1  computes the difference between all components of f and fj  whereas our node selection criteria  given in  1   is restricted to the components corresponding to nodes in the original local domain l.
¡¡let us examine algorithm 1 in more detail. first  the algorithm computes the outlink counts for each page in the local domain. the algorithm then computes the quantity u tj f for each known global page j. this inner product can be written as
 
	 	+ 1	    + 1
k: k j ¡Êfout
where the second term sums over the set of local pages that link to page j. since the total number of edges in fout was assumed to have size    recall that  is the number of pages in f   the running time of this step is also
¡¡the algorithm then computes the vectors y and z  as given in  1  and  1   respectively. the l1normdiff method is called on the components of these vectors which correspond to the pages in l  and it estimates the value of  for each page j. the estimation works
as follows. first  the values of u tj f are discretized uniformly into c values {a1 ... ac}. the quantity  is then computed for each discretized value of ai and stored in a table. to evaluate  for some a ¡Ê  a1 ac   the closest discretized value ai is determined  and the corresponding entry in the table is used. the total running time for this method is linear in  and the discretization parameter c  which we take to be a constant . we note that if exact values are desired  we have also developed an algorithm that runs in  time that is not described here.
¡¡in the main loop  we compute the vector x  as defined in equation  1 . the nested loops iterate over the set of pages in f that are siblings of page j. typically  the size of this set is bounded by a constant. finally  for each page j  the scores vector is updated over the set of non-zero components k of the vector x with k ¡Ê l. this set has size equal to the number of local siblings of page j  and is a subset of the total number of siblings of page j. thus  each iteration of the main loop takes constant time  and the total running time of the main loop is  . since we have assumed that the size of f will not grow larger than o n   the total running time for the algorithm is o n .
algorithm 1: node selection via stochastic complementation.
sc-select f  fout  f  k 
input: f: zero-one adjacency matrix of size  corresponding to the current local subgraph  fout: zero-one outlink matrix from f to global subgraph  f: pagerank of f  k: number of pages to return output: pages: set of k pages to crawl next {compute outlink sums for local subgraph} foreach  page j ¡Ê f 
o 
end
{compute scalar u tj f for each global node j } foreach  page j ¡Ê fout 
g 
foreach  page k :  k j  ¡Ê fout 
g 
end
end
{compute vectors y and z as in  1  and  1  }
+1 
 for all values g j }
norm diffs ¡ûl1normdiffs g ely elz  foreach  page j ¡Ê fout 
{compute sparse vector x as in  1 } x ¡û 1
foreach  page k :  k j  ¡Ê fout  foreach  page x 
end
end
x ¡û ¦Áx
scores j  ¡û norm diffs j 
foreach  k : x k    1 and page k ¡Ê l 
scores j  ¡û scores j    |y k  + g j    z k |
+|x k +y k +g j  z k  |
end
end
return k pages with highest scores1.1 pagerank flows
¡¡we now present an intuitive analysis of the stochastic complementation method by decomposing the change in pagerank in terms of 'leaks' and 'flows'. this analysis is motivated by the decomposition given in  1 . pagerank 'flow' is the increase in the local pageranks originating from global page j. the flows are represented by the non-negative vector  u tj f z  equations  1  and  1  . the scalar u tj f can be thought of as the total amount of pagerank flow that page j has available to distribute. the vector z dictates how the flow is allocated to the local domain; the flow that local page k receives is proportional to  within a constant factor due to the random surfer vector  the expected number of its inlinks.
¡¡the pagerank 'leaks' represent the decrease in pagerank resulting from the addition of page j. the leakage can be quantified in terms of the non-positive vectors x and y  equations  1  and  1  . for vector x  we can see from equation  1  that the amount of pagerank leaked by a local page is proportional to the weighted sum of the pageranks of its siblings. thus  pages that have siblings with higher pageranks  and low outlink counts  will experience more leakage. the leakage caused by y is an artifact of the random surfer vector.
¡¡we will next show that if only the 'flow' term   u tj f z  is considered  then the resulting method is very similar to a heuristic proposed by cho et al.  that has been widely used for the  crawling through url ordering  problem. this heuristic is computationally cheaper  but as we will see later  not as effective as the stochastic complementation method.
¡¡our node selection strategy chooses global nodes that have the largest influence  equation  1  . if this influence is approximated using only 'flows'  the optimal node j  is:
j 	=	argmax
	=	argmaxelz1
	=	argmaxju tj f
e
	=	argmax df
	=	argmaxjft df + diag uj   1uj.
the resulting page selection score can be expressed as a sum of the pageranks of each local page k that links to j  where each pagerank value is normalized by o k +1. interestingly  the normalization that arises in our method differs from the heuristic given in   which normalizes by o k . the algorithm pf-select  which is omitted due to lack of space  first computes the quantity ft df +diag uj   1uj for each global page j  and then returns the pages with the k largest scores. to see that the running time for this algorithm is o n   note that the computation involved in this method is a subset of that needed for the sc-select method  algorithm 1   which was shown to have a running time of o n .
1. experiments
¡¡in this section  we provide experimental evidence to verify the effectiveness of our algorithms. we first outline our experimental methodology and then provide results across a variety of local domains.
1 methodology
¡¡given the limited resources available at an academic institution  crawling a section of the web that is of the same magnitude as that indexed by google or yahoo! is clearly infeasible. thus  for a given local domain  we approximate the global graph by crawling a local neighborhood around the domain that is several orders of magnitude larger than the local subgraph. even though such a graph is still orders of magnitude smaller than the 'true' global graph  we contend that  even if there exist some highly influential pages that are very far away from our local domain  it is unrealistic for any local node selection algorithm to find them. such pages also tend to be highly unrelated to pages within the local domain.
¡¡when explaining our node selection strategies in section 1  we made the simplifying assumption that our local graph contained no dangling nodes. this assumption was only made to ease our analysis. our implementation efficiently handles dangling links by replacing each zero column of our adjacency matrix with the uniform vector. we evaluate the algorithm using the two node selection strategies given in section 1  and also against the following baseline methods:
  random: nodes are chosen uniformly at random among the known global nodes.
  outlinkcount: global nodes with the highest number of outlinks from the local domain are chosen.
at each iteration of the findglobalpr algorithm  we evaluate performance by computing the difference between the current pagerank estimate of the local domain  elf   and the global pagerank of the local domain elg . all page-
rank calculations were performed using the uniform random surfer vector. across all experiments  we set the random surfer parameter ¦Á  to be .1  and used a convergence threshold of 1. we evaluate the difference between the local and global pagerank vectors using three different metrics: the l1 and l¡Þ norms  and kendall's tau. the l1 norm measures the sum of the absolute value of the differences between the two vectors  and the l¡Þ norm measures the absolute value of the largest difference. kendall's tau metric is a popular rank correlation measure used to compare pageranks  1  1 . this metric can be computed by counting the number of pairs of pairs that agree in ranking  and subtracting from that the number of pairs of pairs that disagree in ranking. the final value is then normalized by the total number of such pairs  resulting in a   1  range  where a negative score signifies anti-correlation among rankings  and values near one correspond to strong rank correlation.
1 results
¡¡our experiments are based on two large web crawls and were downloaded using the web crawler that is part of the nutch open source search engine project . all crawls were restricted to only 'http' pages  and to limit the number of dynamically generated pages that we crawl  we ignored all pages with urls containing any of the characters ' '  '*'  ' '  or '='. the first crawl  which we will refer to as the 'edu' dataset  was seeded by homepages of the top 1 graduate computer science departments in the usa  as rated by the us news and world report   and also by the home pages of their respective institutions. a crawl of depth 1 was performed  restricted to pages within the '.edu' domain  resulting in a graph with approximately 1 million pages and 1 million links. the second crawl was seeded by the set of pages under the 'politics' hierarchy in the dmoz open directory project. we crawled all pages up to four links away  which yielded a graph with 1 million pages and 1 million links.
¡¡within the 'edu' crawl  we identified the five site-specific domains corresponding to the websites of the top five graduate computer science departments  as ranked by the us news and world report. this yielded local domains of various sizes  from 1  uiuc  to 1  berkeley . for each of these site-specific domains with size n  we performed 1 iterations of the findglobalpr algorithm to crawl a total of 1n additional nodes. figure 1 a  gives the  l1  difference from the pagerank estimate at each iteration to the global pagerank  for the berkeley local domain.

figure 1: l1 difference between the estimated and true global pageranks for  a  berkeley's computer science website   b  the site-specific domain  www.enterstageright.com  and  c  the 'politics' topic-specific domain. the stochastic complement method outperforms all other methods across various domains.¡¡the performance of this dataset was representative of the typical performance across the five computer science sitespecific local domains. initially  the l1 difference between the global and local pageranks ranged from .1  stanford  to .1  mit . for the first several iterations  the three link-based methods all outperform the random selection heuristic. after these initial iterations  the random heuristic tended to be more competitive with  or even outperform  as in the berkeley local domain  the outlink count and pagerank flow heuristics. in all tests  the stochastic complementation method either outperformed  or was competitive with  the other methods. table 1 gives the average difference between the final estimated global pageranks and the true global pageranks for various distance measures.
algorithml1lkendall¡Þ
stoch. comp..1.1.1.1.1.1.1.1.1table 1: average final performance of various node selection strategies for the five site-specific computer science local domains. note that kendall's tau measures similarity  while the other metrics are dissimilarity measures. stochastic complementation clearly outperforms the other methods in all metrics.
¡¡within the 'politics' dataset  we also performed two sitespecific tests for the largest websites in the crawl: www.adamsmith.org  the website for the london based adam smith institute  and www.enterstageright.com  an online conservative journal. as with the 'edu' local domains  we ran our algorithm for 1 iterations  crawling a total of 1n nodes. figure 1  b  plots the results for the www.enterstageright.com domain. in contrast to the 'edu' local domains  the random and outlinkcount methods were not competitive with either the sc-select or the pf-select methods. among all datasets and all node selection methods  the stochastic complementation method was most impressive in this dataset  realizing a final estimate that differed only .1 from the global pagerank  a ten-fold improvement over the initial local pagerank difference of .1. for the adam smith local domain  the initial difference between the local and global pageranks was .1  and the final estimates given by the
sc-select  pf-select  outlinkcount  and random methods were .1  .1  .1  and .1  respectively.
¡¡within the 'politics' dataset  we constructed four topicspecific local domains. the first domain consisted of all pages in the dmoz politics category  and also all pages within each of these sites up to two links away. this yielded a local domain of 1 pages  and the results are given in figure 1  c . because of the larger size of the topic-specific domains  we ran our algorithm for only 1 iterations to crawl a total of n nodes.
¡¡we also created topic-specific domains from three political sub-topics: liberalism  conservatism  and socialism. the pages in these domains were identified by their corresponding dmoz categories. for each sub-topic  we set the local domain to be all pages within three links from the corresponding dmoz category pages. table 1 summarizes the performance of these three topic-specific domains  and also the larger political domain.
¡¡to quantify a global page j's effect on the global pagerank values of pages in the local domain  we define page j's impact to be its pagerank value  g j   normalized by the fraction of its outlinks pointing to the local domain:
 
where  ol j  is the number of outlinks from page j to pages in the local domain l  and o j  is the total number of j's outlinks. in terms of the random surfer model  the impact of page j is the probability that the random surfer  1  is currently at global page j in her random walk and  1  takes an outlink to a local page  given that she has already decided not to jump to a random page.
¡¡for the politics local domain  we found that many of the pages with high impact were in fact political pages that should have been included in the dmoz politics topic  but were not. for example  the two most influential global pages were the political search engine www.askhenry.com  and the home page of the online political magazine  www.policyreview.com. among non-political pages  the home page of the journal  education next  was most influential. the journal is freely available online and contains articles regarding various aspect of k-1 education in america. to provide some anecdotal evidence for the effectiveness of our page selection methods  we note that the sc-select method chose 1 pages within the www.educationnext.org domain  the pf-select method discovered 1 such pages  while the outlinkcount and random methods found only 1 pages each. for the conservative political local domain  the socialist website www.ornery.org had a very high impact score. this all politics:
algorithml1l1kendallstoch. comp..1.1.1pr flow.1.1.1outlink.1.1.1random.1.1.1conservativism:
algorithml1l1kendallstoch. comp..1.1.1pr flow.1.1.1outlink.1.1.1random.1.1.1liberalism:
algorithml1l1kendallstoch. comp..1.1.1.1.1.1.1.1.1socialism:
algorithml1lkendall¡Þ
stoch. comp..1.1.1pr flow.1.1.1outlink.1.1.1random.1.1.1table 1: final performance among node selection strategies for the four political topic-specific crawls. note that kendall's tau measures similarity  while the other metrics are dissimilarity measures.
was largely due to a link from the front page of this site to an article regarding global warming published by the national center for public policy research  a conservative research group in washington  dc. not surprisingly  the global pagerank of this article  which happens to be on the home page of the nccpr  www.nationalresearch.com   was approximately .1  whereas the local pagerank of this page was only .1. the sc-select method yielded a global pagerank estimate of approximately .1  the pfselect method estimated a value of .1  and the random and outlinkcount methods yielded values of .1 and .1  respectively.
1. related work
¡¡the node selection framework we have proposed is similar to the url ordering for crawling problem proposed by cho et al. in . whereas our framework seeks to minimize the difference between the global and local pagerank  the objective used in  is to crawl the most highly  globally  ranked pages first. they propose several node selection algorithms  including the outlink count heuristic  as well as a variant of our pf-select algorithm which they refer to as the 'pagerank ordering metric'. they found this method to be most effective in optimizing their objective  as did a recent survey of these methods by baeza-yates et al. . boldi et al. also experiment within a similar crawling framework in   but quantify their results by comparing kendall's rank correlation between the pageranks of the current set of crawled pages and those of the entire global graph. they found that node selection strategies that crawled pages with the highest global pagerank first actually performed worse  with respect to kendall's tau correlation between the local and global pageranks  than basic depth first or breadth first strategies. however  their experiments differ from our work in that our node selection algorithms do not use  or have access to  global pagerank values.
¡¡many algorithmic improvements for computing exact pagerank values have been proposed  1  1  1 . if such algorithms are used to compute the global pageranks of our local domain  they would all require o n  computation  storage  and bandwidth  where n is the size of the global domain. this is in contrast to our method  which approximates the global pagerank and scales linearly with the size of the local domain.
¡¡wang and dewitt  propose a system where the set of web servers that comprise the global domain communicate with each other to compute their respective global pageranks. for a given web server hosting n pages  the computational  bandwidth  and storage requirements are also linear in n. one drawback of this system is that the number of distinct web servers that comprise the global domain can be very large. for example  our 'edu' dataset contains websites from over 1 different universities; coordinating such a system among a large number of sites can be very difficult.
¡¡gan  chen  and suel propose a method for estimating the pagerank of a single page  which uses only constant bandwidth  computation  and space. their approach relies on the availability of a remote connectivity server that can supply the set of inlinks to a given page  an assumption not used in our framework. they experimentally show that a reasonable estimate of the node's pagerank can be obtained by visiting at most a few hundred nodes. using their algorithm for our problem would require that either the entire global domain first be downloaded or a connectivity server be used  both of which would lead to very large web graphs.
1. conclusions and future work
¡¡the internet is growing exponentially  and in order to navigate such a large repository as the web  global search engines have established themselves as a necessity. along with the ubiquity of these large-scale search engines comes an increase in search users' expectations. by providing complete and isolated coverage of a particular web domain  localized search engines are an effective outlet to quickly locate content that could otherwise be difficult to find. in this work  we contend that the use of global pagerank in a localized search engine can improve performance.
¡¡to estimate the global pagerank  we have proposed an iterative node selection framework where we select which pages from the global frontier to crawl next. our primary contribution is our stochastic complementation page selection algorithm. this method crawls nodes that will most significantly impact the local domain and has running time linear in the number of nodes in the local domain. experimentally  we validate these methods across a diverse set of local domains  including seven site-specific domains and four topic-specific domains. we conclude that by crawling an additional n or 1n pages  our methods find an estimate of the global pageranks that is up to ten times better than just using the local pageranks. furthermore  we demonstrate that our algorithm consistently outperforms other existing heuristics.
¡¡often times  topic-specific domains are discovered using a focused web crawler which considers a page's content in conjunction with link anchor text to decide which pages to crawl next . although such crawlers have proven to be quite effective in discovering topic-related content  many irrelevant pages are also crawled in the process. typically  these pages are deleted and not indexed by the localized search engine. these pages can of course provide valuable information regarding the global pagerank of the local domain. one way to integrate these pages into our framework is to start the findglobalpr algorithm with the current subgraph f equal to the set of pages that were crawled by the focused crawler.
¡¡the global pagerank estimation framework  along with the node selection algorithms presented  all require o n  computation per iteration and bandwidth proportional to the number of pages crawled  tk. if the number of iterations t is relatively small compared to the number of pages crawled per iteration  k  then the bottleneck of the algorithm will be the crawling phase. however  as the number of iterations increases  relative to k   the bottleneck will reside in the node selection computation. in this case  our algorithms would benefit from constant factor optimizations. recall that the findglobalpr algorithm  algorithm 1  requires that the pageranks of the current expanded local domain be recomputed in each iteration. recent work by langville and meyer  gives an algorithm to quickly recompute pageranks of a given webgraph if a small number of nodes are added. this algorithm was shown to give speedup of five to ten times on some datasets. we plan to investigate this and other such optimizations as future work.
¡¡in this paper  we have objectively evaluated our methods by measuring how close our global pagerank estimates are to the actual global pageranks. to determine the benefit of using global pageranks in a localized search engine  we suggest a user study in which users are asked to rate the quality of search results for various search queries. for some queries  only the local pageranks are used in ranking  and for the remaining queries  local pageranks and the approximate global pageranks  as computed by our algorithms  are used. the results of such a study can then be analyzed to determine the added benefit of using the global pageranks computed by our methods  over just using the local pageranks.
acknowledgements. this research was supported by nsf grant ccf-1  nsf career award aci-1  and a grant from sabre  inc.
