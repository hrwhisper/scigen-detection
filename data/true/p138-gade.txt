various constrained frequent pattern mining problem formulations and associated algorithms have been developed that enable the user to specify various itemset-based constraints that better capture the underlying application requirements and characteristics. in this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern's items and its associated set of transactions. block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets. however  developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier  these block constraints are tough as they are neither anti-monotone  monotone  nor convertible. to overcome this problem  we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called cbminer to find the closed itemsets that satisfy the block constraints.
categories and subject descriptors
h.1  database management : database applications- data mining

 this	work	was	supported	in	part	by	nsf	ccr-
1  eia-1  aci-1  aci-1  and aci-1; the digital technology center at umn; and by the army hpc research center under the auspices of the department of the army  army research laboratory  arl  under cooperative agreement number daad1-1. the content of which does not necessarily reflect the position or the policy of the government  and no official endorsement should be inferred. access to research and computing facilities was provided by the minnesota supercomputing institute.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
general terms
algorithms  theory
keywords
closed pattern  tough constraint  block constraint
1. introduction
﹛finding frequent patterns in large databases is a fundamental data mining task with extensive applications to many areas including association  correlation  and causality rule discovery  rule-based classification  and feature-based clustering. as a result  a vast amount of research has focused on this problem resulting in the development of numerous efficient algorithms. this research has been primarily focused on finding frequent patterns corresponding to itemsets and sequences  but the ubiquitous nature of the problem has also resulted in the development of various algorithms that find frequent spatial  geometric  and topological patterns  as well. in recent years  researchers have recognized that in many application areas and problem settings frequency is not the best measure to use in determining the significance of a pattern as it depends on a number of other parameters such as the type of items that it contains  the length of the pattern  or various numerical attributes associated with the individual items. in such cases  even though frequent pattern discovery algorithms can still be used as a pre-processing step to identify a set of candidate patterns that are subsequently pruned by taking into account the additional parameters  they tend to lead to inefficient algorithms as a large number of the discovered patterns will eventually get eliminated. to address this problem  various constrained frequent pattern mining problem formulations have been developed that enable the user to focus on mining patterns with a rich class of constraints that capture the application semantics  1  1  1 . the key property of these itemset constraints is that they are usually  or can be converted to  anti-monotone or monotone  making it possible to develop computationally efficient algorithms to find the corresponding patterns.
﹛in this paper we introduce a new class of constraints referred to as block constraints  which determine the significance of an itemset pattern by considering the dense block that is formed by the pattern's items and its associated set of transactions. specifically  we focus on three different block constraints called block size  block sum  and block similarity. the block size constraint applies to binary datasets  the block sum constraint applies to datasets in which each instance of an item has a non-negative value associated with it that can vary across transactions  and the block similarity constraint applies to datasets in which each transaction corresponds to a vector-space representation of an object and the similarity between these objects is measured by the cosine of their vectors. according to the block size constraint  a pattern is interesting if the size of its dense block  obtained by multiplying the length of the itemset and the number of its supporting transactions  is greater than a user-specified threshold. analogously  according to the block sum constraint  a pattern is interesting if the sum of the values of its dense block is greater than a user-specified threshold. finally  according to the block similarity constraint a pattern is interesting if its dense block accounts for a certain userspecified fraction of the overall similarity between the objects in the entire dataset.
﹛finding patterns satisfying the above constraints has applications in a number of different areas. for example  in the context of market-basket analysis  the block-size and blocksum constraints can be used to find the itemsets that account for a certain fraction of the overall quantities sold or revenue/profit generated  respectively  whereas in the context of document clustering  the block similarity constraint can be used to identify the set of terms that bring a set of documents together and thus correspond to thematically related words  commonly referred to as micro-concepts  .
﹛developing computationally efficient algorithms to find these block constraints is particularly challenging because unlike the different itemset-based constraints studied earlier  these block constraints are tough as they are neither anti-monotone  monotone  nor convertible . to overcome this problem  we introduce a new class of pruning methods that can be used to significantly reduce the overall search space and make it possible to develop computationally efficient block pattern mining algorithms. specifically  we focus on the problem of finding the closed itemsets satisfying the proposed block constraints and present a projection based mining framework  called cbminer that takes advantage of a matrix-based representation of the dataset. cbminer pushes deeply the various block constraints into closed pattern mining by using three novel classes of pruning methods called column pruning  row pruning  and matrix pruning that when combined lead to dramatic performance improvements. we present an extensive experimental evaluation using various datasets that shows that cbminer not only generates more concise result set  but also is much faster than the traditional frequent closed itemset mining algorithms. moreover  we present an interesting application in the context of document clustering that illustrates the usefulness of the block similarity constraint in micro-concept discovery.
﹛the rest of the paper is organized as follows. section 1 introduces some basic definitions and notations. section 1 formulates the problem and motivates each one of the three block constraints. section 1 describes some related work. section 1 derives the framework for mining closed blocks  while section 1 discusses in detail how to efficiently mine closed patterns with tough block constraints. the thorough performance study is presented in section 1. finally  section 1 provides some concluding remarks.
1. definitions and notation
﹛a transaction database is a set of transactions  where each transaction is a 1-tuple containing a transaction id and a set of items. let i be the complete set of distinct items and t be the complete set of transactions. any non-empty set of items is also called an itemset and any set of transactions is called a transaction set. the frequency of an itemset x  denoted as freq x   is the number of transactions that contain all the items in x  while the support of x is defined as 考 x =freq  x /|t |. for a given minimum support threshold 牟  1   牟 ≒ 1   x is said to be frequent if 考 x  ≡ 牟. a frequent pattern x is called closed if there exists no proper super-pattern of x with the same support as x. an itemset constraint c is a predicate on the power set 1i  i.e.  c : 1i ↙ {true false}. an itemset constraint c is anti-monotone if for any itemset x that satisfies c  all the subsets of x also satisfy c  and c is monotone if all the supersets of x satisfy c. for example  the constraint 考 x  ≡ 牟 is anti-monotone  while 考 x  ≒ 牟 is monotone. an itemset constraint is tough if it is neither anti-monotone nor monotone  and cannot be converted to either anti-monotone or monotone constraint.
﹛a block is defined as a 1-tuple b =  i t   consisting of an itemset i and a transaction set t  such that t is the supporting set of i. the size of a block b is defined as bsize b  = |i| ℅ |t|. a weighted block is a block b =  i t  with a weight function w defined on the cross-product of the itemset and transaction set  i.e.  w : i ℅ t ↙ r+  where r+ is the set of positive real numbers. the sum of a weighted block b is defined as bsum b  = pt﹋t i﹋i w t i . a  weighted  block b =  i t  is said to be a  weighted  closed block if and only if there exists no other  weighted  block  such that . given a
 weighted  block b =  i t   a  weighted  block
is a proper superblock of. in such a case b is called a  weighted  proper subblock of b. we will use  to denote that b is a proper superblock of b and to denote that b is a proper subblock of b.
﹛a block constraint c is a predicate on 1i ℅ 1t   i.e.  c : 1i ℅ 1t ↙ {true false}. a block b is called a valid block for constraint c if it satisfies constraint c  i.e.  c b  is true . a block constraint c is a tough constraint if there is no dependency between the satisfaction/violation of a constraint by a block and the satisfaction/violation of the constraint by any of its superblocks or subblocks.
﹛a transaction-item matrix m is a matrix where each row r represents a transaction and each column c represents an item in t such that the value of the  r c  entry of the matrix  denoted by m r c  is one iff transaction r supports c  otherwise m r c  is zero. similarly a weighted transaction-item matrix m is a transaction-item matrix where for each row r and for each column c  m r c  is equal to w r c   where w is a positive weight function defined on all transactionitem pairs in t  . a  weighted  block b =  i t  can be redefined as a  weighted  dense submatrix of the  weighted  transaction-item matrix m formed with the rows of t and columns of i such that  r ﹋ t and  c ﹋ i we have m r c  = 1  m r c    1 .
﹛given a pre-defined ordering of the columns of m and a set p of columns in m  a p-projected matrix w.r.t. m  m|p  is defined as the submatrix of m containing only the rows that support itemset p and the columns that appear after p in matrix m. for any transaction t in m|p  its size is defined as the number of non-zero elements in its corresponding row of m|p and will be denoted by |t|. for any column x of m|p  the matrix obtained by keeping only the rows of m|p that contain x is denoted as m|px. for each matrix m|p and m|xp we will denote their set of corresponding transactions and items as t |p  t |xp  i|p  and i|xp  respectively.
﹛given a set of m-dimensional vectors  the composite vector of a is denoted by d and is defined to be . given a weighted block b =  i t   the composite vector of the block is denoted by  and is the |i|-dimensional vector obtained as follows. for each item i ﹋ i  the ith dimension of   denoted by    is equal to p t﹋t w t i   otherwise if i /﹋ i  bi i  = 1. also  given a p-projected matrix m|p  the composite vector of an item x within m|p is denoted by and is the |i|-dimensional vector obtained from the transactions included in t |xp such that for every   otherwise if

﹛given a matrix m  the column-sum of column i in m is denoted by csum  i  and is defined to be equal to the sum of the values of the columnm m i of m  i.e.  csumm i  = pt  t i . similarly  the row-sum of row t in m is denoted by rsum  t  and is defined to be equal to the sum of the values of the rowm t of m  i.e.  rsumm t  = pi m t i .
1. problem definition
﹛in this paper we develop efficient algorithms for finding valid closed blocks that satisfy certain tough block constraints. specifically  we focus on three types of block constraints that are motivated and described in this section.
block size constraint in the context of market-basket analysis we are often interested in finding the set of itemsets each of which accounts for a certain fraction of the overall number of transactions that was performed during a certain period of time. given an itemset i and its supporting set t  the extent to which i will satisfy this constraint will depend on whether or not |i|℅|t| is no less than the specified fraction. finding this type of itemsets is the motivation behind the first block-constraint that we study  which focuses on finding all blocks b =  i t  whose size is no less than a certain threshold. specifically  given a binary transaction database t   the block-size constraint is defined as
	bsize b  ≡ 牟n 	 1 
where 1   牟 ≒ 1 and n is the total number of non-zeros in
the transaction-item matrix ofnote that depending on the size of the itemsets associatedt   i.e.  n = pt﹋t |t|.
with each valid block  the minimum required size of the corresponding transaction set will be different. small itemsets will require larger transaction sets  whereas large itemsets will lead to valid blocks with smaller transaction sets. as a result  even if an itemset i is not part of a valid block  an extension of  may become valid  e.g.  cases in which the support of i does not significantly decrease compared to the support of i . similarly  an itemset i which is not part of any valid block may contain subsets that are part of some valid blocks  e.g.  cases in which the support of the subset is significantly greater than the support of i . consequently  the block-size constraint is a tough constraint as it is neither anti-monotone nor monotone  and cannot be converted to either anti-monotone or monotone constraints.
block sum constraint in cases in which there is a nonnegative weight associated with each individual transactionitem pair  e.g.  sales or profit achieved by selling an item to a customer   in addition to finding all itemsets that satisfy a certain block-size constraint we may also be interested in finding the itemsets whose corresponding weighted blocks have a block-sum that is greater than a certain threshold. for example  in the context of market-basket analysis  these itemsets can be used to identify the product groups that account for a certain fraction of the overall sales  profits  etc. motivated by this  the second block-constraint that we study extends the notion of the block-size constraint to weighted blocks. formally  given a transaction database t   and a weight function w the block-sum constraint is defined as
	bsum b  ≡ 牟w 	 1 
where 1   牟 ≒ 1 and w is the sum of the weights of all the transaction-item pairs in the database  i.e.  w = is a generalization of the block-size constraint it also repre-pt﹋t  i﹋i w t i . note that since the block-sum constraint
sents a tough constraint.
block similarity constraint the last block constraint that we will study is motivated by the problem of finding groups of thematically related words in large document datasets  each potentially describing a different micro-concept present in the collection. one way of finding such groups is to analyze the document-term matrix associated with the dataset and find sets of words that satisfy either a user specified minimum support constraint or a block-size constraint  as defined earlier . however  the limitation of these approaches is that they do not account for the weights that are often associated with the various words as a result of the widely used tf-idf  term-frequency-inverse documentfrequency  vector-space model. in general  groups of words that have higher weights will more likely represent a thematically coherent concept than words that have very low weights  even if the latter groups have higher support. this often happens with words that are common in almost all the documents and will be assigned very low weight due to their high document frequency.
﹛one way of addressing this problem is to first apply the tf-idf model on each document vector  scale the resulting document vectors to be of the same length  e.g.  unit length   and then find the groups of related words by using the previously defined block-sum constraint. however  within the context of the vector-space model  a more natural way of measuring the importance of a group of words is to look at how much they contribute to the overall similarity between the documents in the collection. in other words  the microconcept discovery problem can be formulated as that of finding all groups of words such that the removal of each group from their supporting documents will decrease the aggregate similarity between the documents by a certain fraction. in general  groups of words that are large  supported by many documents  and have high weights will tend to contribute a higher fraction to the aggregate similarity and hence form better micro-concepts.
﹛discovering groups of words that satisfy the above property led us to develop the block-similarity constraint that is defined as follows. let a = {d1 d1 ... dn} be a set of n documents modeled by their unit-length tf-idf representation of the set of documents  let m be the distinct number of terms in a  let b =  i t  be a weighted block with i being a set of words and t being its supporting set of documents  let s be the sum of the pairwise similarities between the documents in a  and let s be the sum of the pairwise similarities between the documents in a obtained after zeroing-out the entries corresponding to block b. the similarity of the block b is defined to be the loss in the aggregate pairwise similarity resulting from removing b  i.e.  bsim  and the block-similarity constraint is defined as
﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛bsim b  ≡ 牟s 	 1  where 1   牟 ≒ 1.
﹛in this paper  we will measure the similarity between two documents di and dj in a by computing the dot-product of their corresponding vectors and   i.e.  sim di dj  =
      . since the documents in a have already been scaled to be of unit length  this similarity measure is nothing more than the cosine of their respective vectors  which is used widely in information retrieval. the advantage of the dotproduct-based similarity measure is that it allows us to easily and efficiently compute both s and s. specifically  if the composite vector of a  it can be shown that s = d ﹞ d. similarly  if b =  i t  is a weighted block of a  and is its corresponding composite vector it can be shown that
 . as a result  the similarity of a
block b =  i t  is given by
	bsim.	 1 
﹛to simplify the presentation of the three block constraints and the associated algorithms  in the rest of this paper we will consider the set of documents a as forming a weighted transaction-item matrix m whose rows and columns correspond to the documents and terms of a  respectively. as a result  each matrix entry m i j  will be equal to    i.e.  the value in the di's vector along the jth dimension .
1. related research
﹛efficient algorithms for finding frequent itemsets in large databases have been one of the key success stories in data mining research  1  1  1  1  1 . one of the early computationally efficient algorithms was apriori   which finds frequent itemsets of length l based on the previously mined frequent itemsets of length  l-1 . more recently  a set of database-projection-based methods  1  1  1  have been developed that significantly reduce the complexity of finding frequent long patterns. this study extends the projectionbased method to mine valid sub-matrices with tough block constraints.
﹛the frequent itemset mining algorithms usually generate a large number of frequent itemsets when the support is low. to solve this problem  two general classes of techniques were proposed. the first is mining closed/maximal patterns. typical examples include max-miner   a-close   mafia   charm   cfp-tree   and closet+ . the redundant pattern pruning and column fusing methods adopted by cbminer have been popularly used in different forms by several previous studies  1  1  1  1  1  1  1 . the second class focuses on mining constrained patterns by integrating various anti-monotone  monotone  or convertible constraints. the constrained association rule mining problem was first considered in  but only for item specific constraints. since then a number of different constrained frequent pattern mining algorithms have been proposed  1  1  1  1  1  1  1 . all these algorithms concentrate on constrained itemset mining with various anti-monotone  monotone  succinct or convertible constraints.
﹛very recently some work  has been done to push aggregate constraints in the context of iceberg-cube computing. this algorithm mines aggregate constraints in the group by partitions of an sql query by using a divide-and-approximate strategy. the algorithm makes use of the strategy to derive a sequence of weaker anti-monotone constraints for a given non-anti-monotone constraint to prune the nodes in the search tree. recently the lpminer algorithm  was proposed to mine itemsets with length-decreasing support constraints. it uses a novel sve property to prune the unpromising transactions of the projected databases based on the length of the transactions. later the sve property has been used to mine closed itemsets with length decreasing support constraints . we also explore the sve property in the context of mining closed patterns with block constraints in section 1 to prune the unpromising rows of a prefixprojected matrix.
1. matrix-projection based pattern mining
﹛in this section we describe the clsdptrnminer algorithm  which forms the basis of cbminer algorithm. clsdptrnminer follows the widely used projection-based pattern mining paradigm  1  1  1   which can be used to efficiently mine the complete set of frequent patterns in a depth-first search order and as we will see later  it can be easily adapted to mine valid closed block patterns. a key characteristic of clsdptrnminer  as well as cbminer  is that it represents the transaction database t using the transaction-item matrix m and employs a number of efficient sparse matrix storage and access schemes  allowing it to achieve high computational efficiency. for the remainder of this section we describe the basic structure of clsdptrnminer for the problem of enumerating all patterns satisfying a constant minimum support constraint and then introduce several pruning methods to accelerate the frequent closed pattern mining. the extension of this algorithm for finding the closed blocks that satisfy the three tough block constraints described in section 1 will be described later in section 1.
1 frequent pattern enumeration
﹛given a database  the complete set of itemsets can be organized into a lattice if the items are in a predefined order  and the problem of frequent pattern mining then becomes how to traverse the lattice to find the frequent ones. the clsdptrnminer algorithm adopts the depth-first search traversal and uses the downward closure property to prune the infrequent columns from further mining. figure 1 a  shows a database example with a minimum support 1. if we remove the set of infrequent columns  {b f h i k m}  and sort the set of frequent columns in frequency-increasing order  then part of the lattice  i.e.  pattern tree  formed from column set {g a c e d} can be organized into the one shown in figure 1 b . each node in the lattice is labeled in the form p:q  where p is a prefix itemset and q is the set of local columns appeared in the p-projected matrix  m|p. at a certain node during the depth-first traversal of the lattice  if the corresponding prefix p is infrequent  we stop mining the sub-tree under this node. otherwise  we report p as a frequent pattern  build its projected matrix  m|p  find its locally frequent columns in m|p and use them to grow p to get longer itemsets.
﹛to store the various projected matrices efficiently  we adopt the csr sparse storage scheme . the csr format utilizes two one-dimensional arrays: the first stores the actual nonzero elements of the matrix in a row  or column  major order 

figure 1:  a  a transaction database with 牟 ≡ 1;  b  the pattern tree.
and the second stores the indices corresponding to the beginning of each row  or column . to ensure that both the matrix projection as well as the column frequency counting are performed efficiently  we maintain both the row- and the column-based representation of the matrix. the overall complexity of the algorithm depends on the two key steps of sorting and projecting. we used the radix sort algorithm to sort the column frequencies which has a time complexity that is linear in the number of columns being sorted  and because of our matrix-storage scheme  projecting the matrix on the column is linear on the number of non-zeros in the projected matrix. our matrix-projection based pattern enumeration method shares some of the ideas with the recently developed array-projection based method   which was shown to achieve good performance  especially for sparse datasets.
1 frequent closed pattern mining
﹛the above frequent pattern enumeration method can find the complete set of frequent itemsets. to get the set of frequent closed itemsets  we need to check whether a newly found itemset is closed or not and sift out the redundant  i.e.  non-closed  ones. the pattern closure checking in clsdptrnminer works as follows. we maintain the set of frequent closed itemsets mined so far in a hash-table h using the sum of the transaction-ids of the supporting transactions as the hash-key  1  1 . upon getting a new itemset p  we check against the set of already mined closed itemsets which have the same hash-key value as the one derived from p's sum of transaction-ids  to see if there is any itemset that is a proper superset of p with the same support. if that is the case  p is non-closed  otherwise the union of p and the set of its local columns with the same support as p forms a closed itemset. in the pattern enumeration process  some prefix itemsets or columns are unpromising to generate closed itemsets and thus can be pruned. clsdptrnminer adopts two pruning methods  redundant pattern pruning and column fusing  1  1  1  1  1  1 .
1. redundant pattern pruning  rpp  once we find that a prefix itemset is non-closed  that is  it is a proper subset of another already mined closed itemset with the same support  it can be safely pruned  and the sub-tree under the node corresponding to this prefix will not be traversed.
1. column fusing  cf  this optimization performs two different tasks. first  it fuses the dense columns  i.e.  those columns with the same support as the current prefix p  of the projected matrix m|p to the prefix itemset p and removes them from m|p  and thus avoiding projections on them. second  it fuses columns in m|p that have identical supporting transaction sets into a single column  and removes the original columns from m|p. by fusing them  the algorithm reduces the number of projections that need to be performed  as it essentially allows for the pattern to grow by adding multiple columns in a single step.
﹛by integrating the above optimization methods with the frequent pattern enumeration process  we get the clsdptrnminer algorithm as shown in algorithm 1. it takes as input the current pattern p  the p-projected matrix m|p  the given minimum support 牟  and the current hash-table h. the algorithm initially sorts the columns of m|p and eliminates any infrequent columns and then proceeds to perform column fusing. after that it enters its main computational loop which extends p by adding each column a ﹋ m|p  checks to see if p ﹍ {a} can be pruned by comparing it against h  redundant pattern pruning   projects m|p on a  checks to see if p ﹍ {a} is closed  and finally calls itself recursively for pattern p ﹍ {a}.
algorithm 1: clsdptrnminer p m|p 牟 h sortpruneif no column is frequentthe columns ofthe columns inm|m|p in frequency increasing orderp whose support is less than 牟
then return
do column fusing for the columns in m|p
for each1if pcolumn﹍ {a} is aa ﹋ m|redundant patternp
then continue
 ifprojectthere is no dense column inm|p on a to get m|p﹍{m|a}p﹍{a}
do
       : then  output the closed patterna}	p ﹍ {a}h insert p ﹍ {	into the hash-table
﹛﹛﹛﹛clsdptrnminer p ﹍ {a} m|p﹍{a} 牟 h  return1. closed block mining with tough constraints
﹛like the traditional frequent closed pattern mining algorithms  clsdptrnminer works under the constant support threshold framework and uses the downward closure property to prune infrequent columns. however  with tough block constraints  the nice properties derived from the antimonotone  or monotone  constraints no longer hold to be used to prune search space. designing effective pruning methods for tough block constraints is especially challenging. to address this challenge we developed three classes of pruning methods  called column-pruning  row-pruning and matrix pruning  which eliminate the unpromising columns  rows and projected matrices from mining. the specific details of these pruning methods are different for each of the three block constraints and will be described later in this section.
﹛by incorporating these three pruning methods with the overall structure of clsdptrnminer  we can easily derive the cbminer algorithm that mines the set of all valid closed block patterns. the pseudo code for cbminer is shown in algorithm 1. it takes as input the current pattern p  the p-projected matrix m|p  the hash-table h that stores the valid closed blocks that were discovered so far  and the blockconstraint c corresponding to either the block-size  blocksum  or block-similarity constraint. since it is derived from clsdptrnminer algorithm  it has many steps in common and for this reason we will only describe its key differences.
algorithm 1: cbminer p m|p h c 
sortifthen returnmatrixthe columns ofm|p can be prunedm|p in frequency increasing order
ifpruneno column is validthe columns in m|p
then return
do column fusing for the columns in m|p for each column a ﹋ m|redundant patternp
1      ifthen continuep ﹍ {a} is a
	b	p ﹍ {a}
	do   dense column in m|p﹍{a}	c b  = true
output the closed block b
insert b into the hash-table h
	          : then  	p a
prune the rows ofa} m|m|p﹍{﹍{a}} h c 
﹛﹛﹛﹛cbminer p ﹍ { return﹛the first difference has to do with the pruning methods. specifically  instead of using the constant support-based column pruning  cbminer uses the newly proposed columnpruning  row-pruning and matrix pruning methods  which are derived from the tough block constraints. the second difference has to do with the implementation of the column fusion optimization for the block-sum and block-similarity constraints. in the case of the block-sum constraint  the values of the fused columns correspond to the sum of the values of their constituent columns. this ensures that the resulting fused matrix contains all necessary information to correctly evaluate the constraints. in the case of the block-similarity constraint  since the correct evaluation of the constraints requires access to the individual column-values  we do not perform any column fusion.
﹛following we will introduce in detail the three pruning methods  column-pruning  row-pruning and matrix pruning  in terms of the three different block constraints. note that the details of the proofs of the lemmas appeared in this section can be found in .
1 column pruning
﹛given a prefix itemset p and its projected matrix m|p  the idea behind column pruning is to identify for each column x ﹋ m|p a necessary condition that must be satisfied such that there is a valid block b =  p ﹍ 污 t |p﹍污  for which 污 is a subset of the columns in m|p and x ﹋ 污. using this condition  we can then eliminate from m|p all the columns that do not satisfy it  as these columns cannot be part of a valid block that contain p. note that for each column x that we eliminate  we prevent the exploration of the subtree associated with the pattern p ﹍ {x}  thus  significantly reducing the overall search space.
1.1 block size
﹛the necessary condition for the block-size constraint is encapsulated in the following lemma  refer to section 1 for a description of the notation used .
﹛lemma 1.  block-size column pruning  let p be a pattern and x a column in m|p. then in order for x to be part of a valid block that satisfies the block-size constraint of equation 1 and is obtained from extending p by adding columns from m|p  the following must hold:
	bsize p 	 1 
﹛for each column in m|p  equation 1 can be evaluated by adding up the lengths of the rows that it supports. these sums can be computed for all the columns by performing a single scan of the p-projected matrix.
1.1 block sum
﹛the necessary condition for the block-sum constraint is similar in nature to that of the block-size constraint and is encapsulated in the following lemma.
﹛lemma 1.  block-sum column pruning  let p be a pattern and x a column of m|p. then in order for x to be part of a valid block that satisfies the block-sum constraint of equation 1 and is obtained by extending p with columns in m|p  the following must hold:
bsum p t |xp  + x m|p t j  ≡ 牟w.  1  t﹋t |xp j﹋i|p
﹛note that the summation on the left-hand-side of equation 1 is nothing more than the sum of the non-zero elements of each row in t |xp.
﹛the various quantities required to evaluate equation 1 can be computed efficiently by performing a single scan of the block  p t |xp  to compute the sum of each row  and two scans of the matrix m|p. the first scan will compute the sum of the non-zero elements of each row  and the second scan will compute the summation term in equation 1 for each column.
1.1 block similarity
﹛let d be the composite vector of t and consider a pprojected weighted matrix m|p. the necessary condition for the block-similarity constraint is encapsulated in the following lemma.
﹛lemma 1.  block-similarity column pruning  let p be a pattern   p t |p  its corresponding block  and x a column of m|p. then in order for x to be part of a block that satisfies the block-similarity constraint of equation 1 and is obtained by extending p with columns in m|p  the following must hold:
		 1 
﹛for each column of m|p  evaluating the above equation incurs a computational cost equivalent to one scan of the pprojected matrix  which is very costly. so  we make use of the following lemma  which approximates equation 1.
﹛lemma 1.  approximate block-similarity column pruning  let be the maximum value across the m dimensions of vector d and 而 be the maximum row-sum over all the rows of the p-projected matrix m|p. then in order for x to be part of a block that satisfies the block-similarity constraint of equation 1 and is obtained by extending p with columns in m|p  the following must hold:
	freq	 1 
﹛in a single scan of the projected matrix  we can compute the frequency of all its columns along with the value of 而. hence the complexity is of the order of the size of the projected matrix.
1 row pruning
﹛given a pattern p and its projected matrix m|p  the idea behind row pruning is to identify for each row t ﹋ m|p a necessary condition that must be satisfied such that there is a valid block b =  p ﹍ 污 t |p﹍污  for which 污   t. using this condition  we can then eliminate from m|p all the rows that do not satisfy it  as these rows cannot be part of a valid block that contain p. by eliminating such rows we reduce the size of mp and thus reduce the amount of time required to perform subsequent projections and enhance future column pruning operations.
﹛to derive such conditions we make use of the smallest valid extension  sve  principle  originally introduced in  for finding itemsets with length-decreasing support constraint. in the context of block constraints considered in this paper  the smallest valid extension of a prefix p is defined as the length of the smallest possible extension 污 to p  where 污 is a set of columns in m|p   such that the resulting block
b =  p﹍污 t |p﹍污  is valid for a given constraint c. that is 
	sve	  = true}.
knowing the sve of a pattern  we can then eliminate all the rows whose length is smaller than the sve value. note that the sve of a pattern that already corresponds to a valid block will be by definition zero. for this reason  the row-pruning is only applied when the pattern p does not correspond to a valid block.
﹛in the rest of this section we describe how to obtain such sve-based necessary conditions for the block-size  blocksum  and block-similarity constraints.
1.1 block size
﹛the sve of a pattern p for the block-size constraint is given by the following lemma.
﹛lemma 1.  block-size row pruning  let p be a pattern such that b =  p t |p  does not satisfy the block-size constraint. then the smallest valid extension of p for the block-size constraint of equation 1 is
	sve.	 1 
the complexity of computing the sve p  is 成 1 .
1.1 block sum
﹛the sve of a pattern p for the block-sum constraint is given by the following lemma.
﹛lemma 1.  block-sum row pruning  let p be a pattern such that b =  p t |p  does not satisfy the block-sum constraint  and z be the maximum column-sum over all columns of m|p. then the smallest valid extension of p for the blocksum constraint of equation 1 is
	sve.	 1 
﹛the complexity of computing the sve p  is of the order of the size of the projected matrix as we need one scan of the projected matrix to compute the maximum of the columnsums.
1.1 block similarity
﹛let d be the composite vector of t and consider a pprojected weighted matrix m|p. the column-similarity of column x in m|p is denoted by csimm p x  and is defined
|
to be equal to
csum1	 x . m|p
given this definition  the sve of a pattern p for the blocksimilarity constraint is given by the following lemma.
﹛lemma 1.  block-similarity row pruning  let p be a pattern such that b =  p t |p  does not satisfy the blocksimilarity constraint  and z is the maximum column-similarity over all columns of m|p. then the smallest valid extension of p for the block-similarity constraint of equation 1 is
	sve.	 1 
z
﹛the complexity of computing the sve p  is identical to that for the block-sum constraint.
1 matrix pruning
﹛given a prefix itemset p and its projected matrix m|p  the column pruning and row pruning methods are very effective in pruning some unpromising columns and rows from m|p. however  in many cases the whole projected matrix m|p cannot be used to generate any valid block patterns and thus can be pruned. hence we developed another class of pruning method called matrix pruning in order to further prune the search space in terms of the block size  block sum  and block similarity constraints.
1.1 block size
﹛the necessary condition for the block-size constraint is encapsulated in the following lemma.
﹛lemma 1.  block-size matrix pruning  let p be a pattern and t a transaction in m|p. then in order for m|p to be used to generate any valid block that satisfies the blocksize constraint of equation 1 and is obtained by extending p with some columns in m|p  the following must hold:
	bsize	 1 
﹛the sums in equation 1 can be computed by a single scan of the p-projected matrix m|p.
1.1 block sum
﹛the necessary condition for the block-sum constraint is stated in the following lemma.
﹛lemma 1.  block-sum matrix pruning  let p be a pattern  x a column in m|p  and t a transaction in m|p. then in order for m|p to be used to generate any valid block that satisfies the block-sum constraint of equation 1 and is obtained by extending p with some columns in m|p  the following must hold:
	bsum p t |p  + x m|p t x  ≡ 牟w.	 1 
t﹋t |p x﹋i|p
﹛note that the summation on the left-hand-side of equation 1 is nothing more than the sum of the non-zero elements of each row in t |p and can be computed in one scan of the p-projected matrix.
1.1 block similarity
﹛using the definition of the column-similarity introduced in section 1.1  the necessary condition for the block-similarity constraint can be stated as follows:
﹛lemma 1.  block-similarity matrix pruning  let p be a pattern  x a column of m|p  and csimm|p x  the column-similarity of x in m|p. then in order for m|p to be used to generate any valid blocks that satisfy the blocksimilarity constraint of equation 1 and is obtained from extending p with some columns in m|p  the following must hold:
	bsim p t |p  + x csimm|p x  ≡ 牟s	 1 
x﹋i|p
﹛the column-similarities of all the columns can be computed in a single scan of the p-projected matrix.
table 1: dataset characteristics.
data# trans# itemsa. m. tran.len.gazelle11 1 pumsb*11 1 big-market11 1 sports11 1 t1dx1k-1k1 1 1. experimental evaluation
﹛we evaluated the performance of cbminer for finding blocks that satisfy the three block constraints using four real datasets  gazelle  pumsb*  big-market  and sports  and a sequence of synthetic datasets  t1dx . the characteristics  number of transactions  number of items and the average maximum  transaction lengths  of these datasets are shown in the table 1. the gazelle dataset contains the clickstream data from gazelle.com. the pumsb* dataset contains census data and big-market dataset contains the transaction information of a retail store. the sports dataset is a document dataset obtained from san jose mercury  trec . the synthetic dataset series t1dx were generated from ibm dataset generator  with average transaction length of 1  number of distinct items of 1  and average frequent itemset length of 1. this dataset was used for scalability tests by varying the number of transactions from 1k to 1k. all the experiments were performed on a 1ghz intel p1 processor with 1gb of memory running linux. cbminer was implemented in c.
1 results
﹛the experimental evaluation was performed along three different dimensions. first  we compared the performance of the various pruning methods used by cbminer for different datasets and block constraints. second  we evaluated the scalability characteristics of cbminer as the number of transactions increases. third  we compared cbminer's performance against that achieved by traditional closed frequent itemset mining algorithms. the motivation behind this comparison is twofold:  i  it allows us to verify the extent to which the closed block constraints lead to a more concise set of patterns than that produced by existing closed frequent itemset approaches  and  ii  it provides a reference point by which to judge the underlying efficiency of cbminer's implementation.


1.1 effectiveness of the pruning methods
﹛we evaluated the effectiveness of the three newly proposed pruning methods  column pruning  cp   row pruning  rp   and matrix pruning  mp   and their combination  cp+rp+mp . fig. 1 shows these results for the bsize constraint and dataset gazelle  fig. 1 shows the results for the bsum constraint and dataset pumsb*  while fig. 1 shows the results for the bsim constraint and dataset big-market. these results show that the combination of all the three pruning techniques is always faster than each individual pruning method  and for the bsize and bsum constraints the overall ranking of the pruning effectiveness among the three methods is column pruning   matrix pruning   row pruning  while for the bsim constraint matrix pruning is more effective than row pruning and column pruning.
﹛note that if we do not apply any of the three pruning methods  cbminer degenerates to clsdptrnminer  denoted as no-pruning  and it performs poorly. for example  without any pruning for the gazelle dataset with bsize constraint of 1% its runtime is 1 seconds  while the corresponding runtime for cp+rp+mp is only 1 seconds  as shown in fig. 1 . for this reason we do not show the curves corresponding to no-pruning in figs. 1.
1.1 scalability study
﹛we used the synthetic dataset series t1dx for the scalability test of cbminer  where 'x' indicates the base size and varies from 1k to 1k tuples. in the experiments we fixed the bsize  bsum  and bsim threshold all at 1%. from fig. 1  we can see that cbminer has linear scalability on all the three constraints in terms of the base size.
1.1 constrained vs. all closed block pattern mining
﹛in comparing with closed pattern mining algorithms  we chose one of the recently developed closed itemset mining algorithms  closet+   for our comparisons. we compared cbminer with closet+ by providing the minimum frequency of the valid closed block patterns generated by cbminer as the absolute minimum support to closet+. this ensures that closet+ will discover all the patterns found by cbminer. however  closet+ will find additional patterns that do not satisfy the block constraints.
﹛we performed numerous experiments to compare cbminer with closet+ for all the three block constraints and us-


ing the datasets shown in table 1. due to limited space  we only show part of the results. figs. 1 show the comparison results for the bsize constraint and dataset gazelle  while figs. 1 show the results for the bsum constraint and the sports dataset. the results show that in general  cbminer is substantially faster than closet+. this is primarily due to the fact that  as it was expected  closet+ produces significantly more patterns than those produced by cbminer. for datasets with short transactions like gazelle and bigmarket  cbminer can be order s  of magnitude faster than closet+  and finds order s  of magnitude fewer patterns. while for the datasets with long transactions like pumsb*  and sports  closet+ is a little faster at high block threshold of bsize and bsum  but once the threshold is lowered  there is an explosive increase in the number of frequent closed itemsets  e.g.  with bsize/bsum 1% closet+ generates several orders of magnitude more patterns than cbminer . these results illustrate that the pruning methods used by cbminer are indeed effective in reducing the overall search space  leading to substantial performance improvements.
table 1: summary of document datasets used for the application.
datano. of documentsno. of termsno. of classesclassic11sports11la111 application - micro concept discovery
﹛finally  we demonstrate an application for the three block constraints in the context of document clustering by showing that the blocks discovered by these constraints represent sets of documents that have a great chance of belonging to the same cluster and hence can be used to identify potential cores of natural clusters in data as well as thematically related words. for this application we chose two additional document datasets viz.  la1 and classic in addition to sports. the la1 dataset contains articles that appeared in la times news  whereas the classic dataset contains abstracts of technical papers. some of the characteristics of these datasets are shown in table 1. we scaled the document vectors using the well known tf-idf scaling and normalized using l1-norm and used our closed block mining algorithm with block size  block-sum and block-similarity constraints. from the patterns that were found we chose the 1 highest ranked patterns on the basis of the constraint value. for example  for the block sum constraint  we selected the top-1 blocks ranked on block sum and in the same way for block-size and block-similarity constraints. for each of the top-1 blocks we computed the entropies of the documents that formed the supporting set of the block and took the average of the 1 entropies. similarly  we computed the average block pattern frequency and average block pattern length. for comparison purposes  we also used the closet+ algorithm to find a set of frequent closed itemsets and also selected the 1 most frequent itemsets discovered by closet+. fig. 1 shows the average entropy  frequency  and length of the various patterns discovered by the four algorithms for the three datasets. note that the closet+ results are labeled as  freq .
﹛from these results we can see that the average entropy of the patterns discovered by the four schemes are quite small  indicating that all of them do reasonably well in identifying itemsets whose supporting documents are primarily from a single class. despite that  we can see that the blocksimilarity constraint outperforms the rest  as it leads to the lowest entropies  i.e.  purest clusters  for all datasets. this verifies our initial motivation for defining the block-similarity constraint  as it is able to better capture the characteristics of the underlying datasets and problem  and discover sets of words that are thematically very related. the block-size and the itemset support constraints show some inconsistency in finding good concepts as they do not account for the weights associated with the terms in the document-term matrices. on the other hand the block-sum constraint does reasonably well as it was able to take into account the differences in the terms weights provided by the l1-norm and tf-idf scaling for the document vectors. also note that the highest ranked patterns discovered by the frequent closed mining algorithm  closet+  are in general quite short compared to the length of the patterns discovered by the block constraints.
1. conclusion and future work
﹛in this paper we studied how to mine valid closed patterns with tough block constraints and proposed a matrixprojection based framework called cbminer for mining closed block patterns in transaction-item or document-term matricies effectively. under this framework we mainly discussed three typical block constraints viz.  block size  block sum and block similarity. some widely adopted properties derived from the anti-monotone or monotone constraints no longer hold to be used to prune search space for these tough block constraints. as a result  we specifically proposed three novel pruning methods  column pruning  row pruning and matrix pruning  which can push deeply the block constraints into pattern discovery and prune the unpromising columns  rows  and projected matrices effectively.

fig. 1 evaluation of the quality of the top-1 patterns discovered by various algorithms.﹛the research in this paper can be extended along two different directions. first  the cbminer algorithm and its pruning methods assume that the entire dataset can fit into the main memory  which is not true for very large datasets. extending the matrix-based projection approach along with the row-  column-  and matrix-pruning methods to a diskbased implementation is a required step for mining these datasets. second  we believe that the underlying principles utilized by the three pruning methods are quite general and can be used  i  by other frequent pattern mining approaches  and  ii  to prune the search space of other tough constraints. identifying the conditions under which such extensions are possible can greatly help in extending existing algorithms and expanding the type of tough constraints that can efficiently be solved.
