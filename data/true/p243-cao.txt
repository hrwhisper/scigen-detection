pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. in this study  we re-examine this assumption and show that it does not hold in reality - many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. we also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. we then propose to integrate a term classification process to predict the usefulness of expansion terms. multiple additional features can be integrated in this process. our experiments on three trec collections show that retrieval effectiveness can be much improved when term classification is used. in addition  we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness  i.e. using supervised learning  instead of unsupervised learning.  
categories and subject descriptors 
h.1  information storage and retrieval : retrieval models 
general terms 
design  algorithm  theory  experimentation 
keywords 
pseudo-relevance feedback  expansion term classification  svm  language models  
1. introduction 
user queries are usually too short to describe the information need accurately. many important terms can be absent from the query  leading to a poor coverage of the relevant documents. to solve this problem  query expansion has been widely used       . among all the approaches  pseudo-relevance feedback  prf  exploiting the retrieval result has been the most effective . the basic assumption of prf is that the top-ranked documents in the first retrieval result contain many useful terms that can help discriminate relevant documents from irrelevant ones. in general  the expansion terms are extracted either according to the term distributions in the feedback documents  i.e. one tries to extract the most frequent terms ; or according to the comparison between the term distributions in the feedback documents and in the whole document collection  i.e. to extract the most specific terms in the feedback documents . several additional criteria have been proposed. for example  idf is widely used in vector space model .  query length has been considered in  for the weighting of expansion terms. some linguistic features have been tested in . 
however  few studies have directly examined whether the expansion terms extracted from pseudo-feedback documents by the existing methods can indeed help retrieval. in general  one was concerned only with the global impact of a set of expansion terms on the retrieval effectiveness. 
a fundamental question often overlooked at is whether the expansion terms extracted are truly related to the query and are useful for ir. in fact  as we will show in this paper  the assumption that most expansion terms extracted from the feedback documents are useful does not hold  even when the global retrieval effectiveness can be improved. among the extracted terms  a nonnegligible part is either unrelated to the query or is harmful  instead of helpful  to retrieval effectiveness. so a crucial question is: how can we better select useful expansion terms from pseudo-feedback documents   
in this study  we propose to use a supervised learning method for term selection. the term selection problem can be considered as a term classification problem - we try to separate good expansion terms from the others directly according to their potential impact on the retrieval effectiveness. this method is different from the existing ones  which can typically be considered as an unsupervised learning. svm    will be used for term classification  which uses not only the term distribution criteria as in previous studies  but also several additional criteria such as term proximity. 
this approach proposed has at least the following advantages: 1  expansion terms are no longer selected merely based on term distributions and other criteria indirectly related to the retrieval effectiveness. it is done directly according to their possible impact on the retrieval effectiveness. we can expect the selected terms to have a higher impact on the effectiveness. 1  the term classification process can naturally integrate various criteria  and thus provides a framework for incorporating different sources of evidence. 
we evaluate our method on three trec collections and compare it to the traditional approaches. the experimental results show that the retrieval effectiveness can be improved significantly when term classification is integrated. to our knowledge  this is the first attempt trying to investigate the direct impact on retrieval effectiveness of individual expansion terms in pseudo-relevance feedback.  

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  july 1  1  singapore. 
copyright 1 acm  1-1-1/1...$1. 
 
the remaining of the paper is organized as follows: section 1 reviews some related work and the state-of-the-art approaches to query expansion. in section 1  we examine the prf assumption used in the previous studies and show that it does not hold in reality. section 1 presents some experiments to investigate the potential usefulness of selecting good terms for expansion. section 1 describes our term classification method and reports an evaluation of the classification process. the integration of the classification results into the prf methods is described in section 1. in section 1  we evaluate the resulting retrieval method with three trec collections. section 1 concludes this paper and suggests some avenues for future work.  
1. related work 
pseudo-relevance feedback has been widely used in ir. it has been implemented in different retrieval models: vector space model   probabilistic model   and so on. recently  the prf principle has also been implemented within the language modeling framework. since our work is also carried out using language modeling  we will review the related studies in this framework in more detail.  
the basic ranking function in language modeling uses kldivergence as follows:      
score d q  =¡Æw¡Êv p w |¦Èq  log p w |¦Èd                                      1  
where v is the vocabulary of the whole collection   and  are respectively the query model and the document model. the document model has to be smoothed to solve the zero-probability problem. a commonly used smoothing method is dirichlet smoothing : 
tf  w d  +up w | c                                                     1  
p w |¦Èd   =
| d | +u
where |d| is the length of the document  tf w d  the term frequency of w within d  p w|c  the probability of w in the whole collection c estimated with mle  maximum likelihood estimation   and u is the dirichlet prior  set at 1 in our experiments .  
the query model describes the user's information need. in most traditional approaches using language modeling  this model is estimated with mle without smoothing. we denote this model by p w|¦Èo  . in general  this query model has a poor coverage of the relevant and useful terms  especially for short queries. many terms related to the query's topic are absent from  or has a zero probability in  the model. pseudo-relevance feedback is often used to improve the query model. we mention two representative approaches here: relevance model and mixture model. 
the relevance model  assumes that a query term is generated by a relevance model p w|¦Èr  . however  it is impossible to define the relevance model without any relevance information.  thus exploits the top-ranked feedback documents by assuming them to be samples from the relevance model. the relevance model is then estimated as follows: 
p w |¦Èr   ¡Ö¡Æd¡Êf p w | d p d |¦Èr   
where f denotes the feedback documents. on the right side  the relevance model ¦Èr is approximated by the original query q. 
applying bayesian rule and making some simplifications  we obtain: 
	p w | d p q | d p d 	          1  
p w |¦Èr   ¡Ö¡Æd¡Êf	p q 	=¡Æd¡Êf p w | d p q | d 
that is  the probability of a term w in the relevance model is determined by its probability in the feedback documents  i.e. p w|d   as well as the correspondence of the latter to the query  i.e. p q|d  . the above relevance model is used to enhance the original query model by the following interpolation: 
p w|¦Èq  =  1 ¦Ë p w|¦È1  +¦Ëp w|¦Èr                                             1  where  is the interpolation weight  set at 1 in our experiments . notice that the above interpolation can also be implemented as document re-ranking in practice  in which only the top-ranked documents are re-ranked according to the relevance model. 
the mixture model  also tries to build a language model for the query topic from the feedback documents  but in a way different from the relevance model. it assumes that the query topic model to be extracted corresponds to the part that is the most distinctive from the whole document collection. this distinctive part is extracted as follows: each feedback document is assumed to be generated by the topic model to be extracted and the collection model  and the em algorithm  is used to extract the topic model so as to maximize the likelihood of the feedback documents. then the topic model is combined with the original query model by an interpolation similarly to the relevance model. 
although the specific techniques used in the above two approaches are different  both assume that the strong terms contained in the feedback documents are related to the query and are useful to improve the retrieval effectiveness. in both cases  the strong terms are determined according to their distributions. the only difference is that the relevant model tries to extract the most frequent terms from the feedback documents  i.e. with a strong p w|d    while the mixture model tries to extract those that are the most distinctive between the feedback documents and the general collection. these criteria have been generally used in other prf approaches  e.g.  . 
several additional criteria have been used to select terms related to the query. for example   proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. for document filtering  term selection is more widely used in order to update the topic profile. for example   extracted terms from true relevant and irrelevant documents to update the user profile  i.e. query  using the rocchio method. kwok et al.  also made use of the query length as well as the size of the vocabulary. smeaton and van rijsbergen  examined the impact of determining expansion terms using minimal spanning tree and some simple linguistic analysis. 
despite the large number of studies  a crucial question that has not been directly examined is whether the expansion terms selected in a way or another are truly useful for the retrieval. one was usually concerned with the global impact of a set of expansion terms. indeed  in many experiments  improvements in the retrieval effectiveness have been observed with prf       . this might suggest that most expansion terms are useful. is it really so in reality  we will examine this question in the next section. 
notice that some studies  e.g.   have tried to understand the effect of query expansion. however  these studies have examined the terms extracted from the whole collection instead of from the feedback documents. in addition  they also focused on the term distribution aspects. 
1. a re-examination of the prf assumption 
the general assumption behind prf can be formulated as follows: 
most frequent or distinctive terms in pseudo-relevance feedback documents are useful and they can improve the retrieval effectiveness when added into the query. 
to test this assumption  we will consider all the terms extracted from the feedback documents using the mixture model. we will test each of these terms in turn to see its impact on the retrieval effectiveness. the following score function is used to integrate an expansion term e: 
score d  q  =¡Æw¡Êv p t |¦Èo  log p t |¦Èd   + wlog p e |¦Èd                   1  
where t is a query term   p t |¦Èo  is the original query model as described in section 1  e is the expansion term under consideration  and w is its weight. the above expression is a simplified form of query expansion with a single term. in order to make the test simpler  the following simplifications are made: 1  an expansion term is assumed to act on the query independently from other expansion terms; 1  each expansion term is added into the query with equal weight - the weight w is set at 1 or -1. in practice  an expansion term may act on the query in dependence with other terms  and their weights may be different. despite these simplifications  our test can still reflect the main characteristics of the expansion terms.  
good expansion terms are those that improve the effectiveness when w is 1 and hurt the effectiveness when w is -1; bad expansion terms produce the opposite effect. neutral expansion terms are those that produce similar effect when w is 1 or -1. therefore we can generate three groups of expansion terms: good  bad and neutral. ideally  we would like to use only good expansion terms to expand queries.  
let us describe the identification of the three groups of terms in more detail. suppose map q  and map qu e  are respectively the map of the original query and expanded query  expanded with e . 
we measure the performance change due to e by the ratio chg e  =  map q¡Èe  map q  map q .  we set a threshold at 
1 i.e.  good and bad expansion terms should produce a performance change such that |chg e | 1.  
in addition to the above performance change  we also assume that a term appearing less than 1 times in the feedback documents is not an important expansion term. this allows us to filter out some noise. 
the above identification produces a desired result for term classification. now  we will examine whether the candidate expansion terms proposed by the mixture model are good terms. our verification is made on three trec collections: ap  wsj and disk1. the characteristics of these collections are described in section 1. we consider 1 queries for each collection and 1 expansions with the largest probabilities for each query. the following table shows the proportion of good  bad and neutral terms for all the queries in each collection. 
collection good terms neutral terms bad terms ap 1% 1% 1% wsj 1% 1% 1% disk1 1% 1% 1% table 1. proportions of each group of expansion terms selected by the mixture model as we can see  only less than 1% of the expansion terms used in the mixture model are good terms in all the three collections. the proportion of bad terms is higher. this shows that the expansion process indeed added more bad terms than good ones. 
we also notice from table 1 that a large proportion of the expansion terms are neutral terms  which have little impact on the retrieval effectiveness. although this part of the terms does necessarily not hurt retrieval  adding them into the query would produce a long query and thus a heavier query traffic  longer evaluation time . it is then desirable to remove these terms  too. 

figure 1. distribution of the expansion terms for  airbus subsidies  in the feedback documents and in the collection  
the above analysis clearly shows that the term selection process used in the mixture model is insufficient. similar phenomenon is observed on the relevance model and can be generalized to all the methods exploiting the same criteria. this suggests that the term selection criteria used - term distributions in the feedback documents and in the whole document collection  is insufficient. this also indicates that good and bad expansion terms may have similar distributions because the mixture model  which exploits the difference of term distribution between the feedback documents and the collection  has failed to distinguish them. 
to illustrate the last point  let us look at the distribution of the expansion terms selected with the mixture model for trec query #1  airbus subsidies . in figure 1  we place the  top 1 expansion terms with the largest probabilities in a two-dimensional space - one dimension represents the logarithm of its probability in the pseudo-relevant documents and another dimension represents that in the whole collection. to make the illustration easier  a simple normalization is made so that the final value will be in the range  1  1 .  figure 1 shows the distribution of the three groups of expansion terms. we can observe that the neutral terms are somehow isolated from the good and the bad terms to some extent  on the lower-right corner   but the good expansion terms are intertwined with the bad expansion terms.  
this figure illustrates the difficulty to separate good and bad expansion terms according to term distributions solely. it is then desirable to use additional criteria to better select useful expansion terms. 
1. usefulness of selecting good terms 
before proposing an approach to select good terms  let us first examine the possible impact with a good term selection process. let us assume an oracle classifier that separate correctly good  bad and neutral expansion terms as determined in section 1.  
in this experiment  we will only keep the good expansion terms for each query. all the good terms are integrated into the new query model in the same way as either relevance model or mixture model.  table 1 shows the map  mean average precision  for the top 1 results with the original query model  lm   the expanded query models by the relevance model  rel  and by the mixture model  mix   as well as by the oracle expansion terms   rel+oracle and mix+oracle . the superscript   l    r  and  m  indicates that the improvement over lm  rel and mix is statistically significant at p 1.  
 
 
 
models ap wsj disk1 lm 1 1 1 rel 1l 1l 1l rel+oracle 1r l 1r l 1r l mix 1l 1l 1l mix+oracle 1m l 1m l 1m l table 1.the impact of oracle expansion classifier we can see that the retrieval effectiveness can be much improved if term classification is done perfectly. the oracle expansion terms can generally improve the map of the relevance model and the mixture model by 1%. this shows the usefulness of correctly classifying the expansion terms and the high potential of improving the retrieval effectiveness by a good term classification.  the map obtained with the oracle expansion terms represents the upper bound retrieval effectiveness we can expect to obtain using pseudorelevance feedback. our problem now is to develop an effective method to correctly classify the expansion terms. 
1. classification of expansion terms  
1 classifier 
any classifier can be used for term classification. here  we use svm. more specifically  we use the svm variant c-svm  because of its effectiveness and simplicity . several kernel functions can be used in svm. we use the radial-based kernel function  rbf  because it has relatively fewer hyper parameters and has shown to be effective in previous studies  . this function is defined as follows: 
k xi   x j   = exp   || xi   x j ||1¦Ò1                                                  1  
where ¦Òis a parameter controlling the shape of the rbf function. the function gets flatter when  is larger. another parameter c 1 in c-svm should be set to control the trade-off between the slack variable penalty and the margin . both parameters are estimated with a 1-fold cross-validation to maximize the classification accuracy of the training data  see table 1 .  
in our term classification  we are interested to know not only if a term is good  but also the extent to which it is good. this latter value is useful for us to measure the importance of an expansion term and to weight it in the new query. therefore  once we obtain a classification score  we use the method described in  to transform it into a posterior probability: suppose the classification score calculated with the svm is s x . then the probability of x belonging to the class of good terms  denoted by +1  is defined by: 
p +1| x  =1exp as x  +b                                                          1  
where a and b are the parameters  which are estimated by minimizing the cross-entropy of a portion of training data  namely the development data. this process has been automated in libsvm . we will have p +1|x  1 if and only if the term x is classified as a good term. more details about this model can be found in . note that the above probabilistic svm may have different classification results from the simple svm  which classifies instances according to sign s x  . in our experiments  we have tested both probabilistic and simple svms  and found that the former performs better. we use the svm implementation libsvm  in our experiments.  
1 features used for term classification 
each expansion term is represented by a feature vector 
f e  = f1 e   f1 e  ...  fn  e  t ¡Ê n   where t means a transpose of a vector. useful features include those already used in traditional approaches such as term distribution in the feedback documents and term distribution in the whole collection. as we mentioned  these features are insufficient. therefore  we consider the following additional features: 
- co-occurrences of the expansion term with the original query terms; 
- proximity of the expansion terms to the query terms. 
we will explain several groups of features below. our assumption is that the most useful feature for term selection is the one that makes the largest difference between the feedback documents and the whole collection  similar to the principle used in the mixture model . so  we will define two sets of features  one for the feedback documents and another for the whole collection. however  technically  both sets of features can be obtained in a similar way. therefore  we will only describe the features for the feedback documents. the others can be defined similarly.    
  term distributions 
the first features are the term distributions in the pseudo-relevant documents and in the collection. the feature for the feedback documents is defined as follows: 
f1 e  = log ¡Æ d¡Êf tf  e  d 	 
       ¡Æ¡Æt d f¡Ê tf  t  d  where f is the set of feedback documents. f1 e  is defined similarly on the whole collection. these features are the traditional ones used in the relevance model and mixture model. 
  co-occurrence with single query term 
many studies have found that the terms that co-occur with the query terms frequently are often related to the query . therefore  we define the following feature to capture this fact: 
	1	n ¡Æ c t  e | d     
f
where c ti e|d  is the frequency of co-occurrences of query term ti and the expansion term e within text windows in document d. the window size is empirically set to be 1 words. 
  co-occurrence with pairs query terms  
a stronger co-occurrence relation for an expansion term is with two query terms together.  has shown that this type of co-occurrence relation is much better than the previous one because it can take into account some query contexts. the text window size used here is 1 words. given the set  of possible term pairs  we define the following feature  which is slightly extended from the previous one: 
	1	¡Æd¡Êf c ti  t j  e | d  
f1  e  = log ¡Æ ti  t j  ¡Ê¦¸	tf  t  d 
	| ¦¸ |	¡Æ¡Æt	d f¡Ê
weighted term proximity 
the idea of using term proximity has been used in several studies . here we also assume that two terms that co-occur at a smaller distance is more closely related. there are several ways to define the distance between two terms in a set of documents . here  we define it as the minimum number of words between the two terms among all co-occurrences in the documents. let us denote this distance between ti and tj among the set b of documents by dist ti tj|b . for a query of multiple words  we have to aggregate the distances between the expansion term and all query terms. the simplest method is to consider the average distance  which is similar to the average distance defined in . however  it does not produce good results in our experiments. instead  the weighted average distance works better. in the latter  a distance is weighted by the frequency of their co-occurrences. we then have the following feature: 
 ti  e | f  
f	
i=1c ti  e 
 where c ti  e  is the frequency of co-occurrences of ti and e within text windows in the collection. the window size is set to 1 words as before.  
  document frequency for query terms and the expansion term together 
the features in this group model the count of documents in which the expansion term co-occurs with all query terms. we then have: f1 = log ¡Æd¡Êf i  ¡Ät¡Êq t¡Êd ¡Äe¡Êd +1  
where i x  is the indicator function whose value is 1 when the boolean expression x is true  and 1 otherwise. the constant 1 here acts as a smoothing factor to avoid zero value.  
to avoid that a feature whose values varies in a larger numeric range dominates those varying in smaller numeric ranges  scaling on feature values is necessary . the scaling is done in a queryby-query manner. let e¡Êgen q  be an expansion term of the query q  and fi e  is one feature value of e. we scale fi e  as follows:
  
fi'  e  = fi  e  mini    maxi  mini   where mini=min e¡Êgen q fi e  and 
 
maxi=maxe ¡Ê gen q fi e . with this transformation  each feature becomes a real number in  1  1 .  
in our experiments  only the above features are used. however  the general method is not limited to them. other features can be added. the possibility to integrate arbitrary features for the selection of expansion terms indeed represents an advantage of our method. 
1 classification experiments 
 let us now examine the quality of our classification. we use three test collections  see table 1 in section 1   with 1 queries for each collection. we divide these queries into three groups of 1 queries. we then do leave-one-out cross validation to evaluate the classification accuracy. to generate training and test data  we use the method described in section 1 to label possible expansion terms of each query as good terms or non-good terms  including bad and neutral terms   and then represent each expansion with the features described in section 1. the candidate expansion terms are those that occur in the feedback documents  top 1 documents in the initial retrieval  no less than three times.  
table 1 shows the classification results. in this table  we show the percentage of good expansion terms for all the queries in each collection - around 1. using the svm classifier  we obtain a classification accuracy of about 1%. this number is not high. in fact  if we use a na ve classifier that always classifies instances into non good class  the accuracy  i.e. one minuses the percentage of good terms  is only slightly lower. however  such a classifier is useless for our purpose because no expansion term is classified as good term. better indicators are recall  and more particularly precision. although the classifier only identifies about 1 of the good terms  i.e. recall   around 1% of the identified ones are truly good terms  i.e. precision . comparing to table 1 for the expansion terms selected by the mixture model  we can see that the expansion terms select by the svm classifier are of much higher quality. this 
coll. percentage good terms of svm accuracy rec. prec. ap 1 1 1 1 wsj 1 1 1 1 disk1 1 1 1 1 table  1. classification results of svm shows that the additional features we considered in the classification are useful  although they could be further improved in the future. 
in the next section  we will describe how the selected expansion terms are integrated into our retrieval model. 
1. re-weighting expansion terms with term classification 
the classification process performs a further selection of expansion terms among those proposed by the relevance model and the mixture model respectively. the selected terms can be integrated in these models in two different ways: hard filtering  i.e. we only keep the expansion terms classified as good; or soft filtering  i.e. we use the classification score to enhance the weight of good terms in the final query model. our experiments show that the second method performs better. we will make a comparison between these two methods in section 1. in this section  we focus on the second method  which means a redefinition of the models p w|¦Èr  for the relevance model and p w|¦Èt   for the mixture model. these models are redefined as follows: for a term e such that p +1|e  1  
p w |¦Èr  new = p e |¦Èr  old  1+¦Áp +1| e    z 
p w |¦Èt  new = p e |¦Èt  old  1+¦Áp +1| e    z                          1  
where z is the normalization factor  and  is a coefficient  which is estimated with some development data in our experiments using line search . once the expansion terms are re-weighted  we will retain the top 1 terms with the highest probabilities for expansion. their weights are normalized before being interpolated with the original query model. the number 1 is used for a fair comparison with the relevance model and the mixture model. 
name description #docs train topics dev. topicstest topics ap assoc. press 1 1 1 1 1 wsj wall 	st. 	journal
1 1 1 1 1 disk1 trec disk1 1 1 1 1 table  1. statistics of evaluation data sets 1. ir experiments 
1 experimental settings 
we evaluate our method with three trec collections  ap1  wsj1 and all documents on trec disks 1.  table 1 shows the statistics of the three collections.  for each dataset  we split the available topics into three parts: the training data to train the svm classifier  the development data to estimate the parameter  in equation 1  and the test data. we only use the title for each trec topic as our query. both documents and queries are stemmed with porter stemmer and stop words are removed.  
the main evaluation metric is mean average precision  map  for top 1 documents. since some previous studies showed that prf improves recall but may hurt precision  we also show the precision at top 1 and 1 documents  i.e.  p 1 and p 1. we also show recall as a supplementary measure. we do a query-by-query analysis and conduct t-test to determine whether the improvement on map is statistically significant.  
the indri 1 search engine  is used as our basic retrieval system. we use the relevance model implemented in indri  but implemented the mixture model following  since indri does not implement this model. 
1 ad-hoc retrieval results 
in the experiments  the following methods are compared: 
lm: the kl-divergence retrieval model with the original queries; 
rel: the relevance model; 
rel+svm: the relevance model with term classification; 
mix: the mixture model; 
mix+svm: the mixture model with term classification.  
these models require some parameters  such as the weight of original model when forming the final query representation  the dirichlet prior for document model smoothing and so on. since the purpose of this paper is not to optimize these parameters  we set all of them at the same values for all the models.  tables 1  1 and 1 show the results obtained with different models on the three collections. in the tables  imp means the improvement rate over lm model  * indicates that the improvement is statistically significant at the level of p 1  and ** at p 1. the superscripts  r  and  m  indicate that the result is statistically better than the relevance model and mixture model respectively at p 1.  
from the tables  we observe that both relevance model and mixture model  which exploit a form of prf  can improve the retrieval effectiveness of lm significantly. this observation is consistent with previous studies. the map we obtained with these two models represent the state-of-the-art effectiveness on these test collections. 
comparing the relevance model and the mixture model  we see that the latter performs better. the reason may be the following: the mixture model relies more on the difference between the feedback documents and the whole collection to select the expansion terms  than the relevance model. by doing this  one can filter out more bad or neutral expansion terms.  
on all the three collections  the model integrating term classification performs very well. when the classification model is used together with a prf model  the effectiveness is always improved. on the ap and disk1 collections  the improvements are more than 1% and are statistically significant. the improvements on the wsj collection are smaller  about 1%  and are not statistically significant.  
about the impact on precision  we can also see that term classification can also improve the precision at top ranked documents  except in the case of disk1 when svm is added to rel. this shows that in most cases  adding the expansion terms does not hurt  but improves  precision. 
let us show the expansion terms for the queries  machine translation  and  natural language processing   in table 1. the stemmed words have been restored in this table for better readability. all the terms contained in the table are those suggested by the mixture model. however  only part of them  in italic  is useful expansion terms. many of them are general terms that are not useful  for example   food    make    year    1   and so on. 
model p 1 p 1 map imp recall lm 1 1 1 ----- 1 rel 1 1 1 1%** 1 rel+svm 1 1 1r 1%** 1 mix 1 1 1 1%** 1 mix+svm 1 1 1m r 1%** 1 table  1. ad-hoc retrieval results on ap data  
model p 1 p 1 map imp recall lm 1 1 1 -------- 1 rel 1 1 1 1%** 1 rel+svm 1 1 1 1%** 1 mix 1 1 1 1%** 1 mix+svm 1 1 1r 1%** 1 table  1. ad-hoc retrieval results on wsj data  
model p 1 p 1 map imp recall lm 1 1 1 ----------- 1 rel 1 1 1 1%* 1 rel+svm 1 1 1r 1%** 1 mix 1 1 1 1%** 1 mix+svm 1 1 1m r 1%** 1 table  1. ad-hoc retrieval results on disk1 data  
 machine translation  expansion terms p ti|¦Èt  expansion terms p ti|¦Èt  compute 1 year 1 soviet 1 work 1 company 1 make 1 1 1 typewriter 1 english 1 busy 1 ibm 1 increase 1 people 1 ..... ....  natural language processing  expansion terms p ti|¦Èt  expansion terms p ti|¦Èt  english 1 publish 1 word 1 nation 1 french 1 develop 1 food 1 russian 1 make 1 program 1 world 1 dictionary 1 gorilla 1 ........ ..... table  1. expansion terms of two queries. the terms in italic are real good expansion terms  and those in bold are classified as good terms  
the classification process can help identify well the useful expansion terms  in bold : although not all the useful expansion terms are identified  those identified  e.g.  program    dictionary   are highly related and useful. as the weight of these terms is increased  the relative weight of the other terms is decreased  making their weights in the final query model smaller. these examples illustrate why the term classification process can improve the retrieval effectiveness. 
1 supervised vs. unsupervised learning 
compared to the relevance model and the mixture model  the approach with term classification made two changes: it uses supervised learning instead of unsupervised learning; it uses several additional features. it is then important to see which of these changes contributed the most to the increase in retrieval effectiveness. 
in order to see this  we design a method using unsupervised learning  but with the same additional features. the unsupervised learning extends the mixture model in the following way:  
each feedback document is also considered to be generated from the topic model  to be extracted  and the collection model. we try to extract the topic model so as to maximize the likelihood of the feedback documents as in the mixture model. however  the difference is that  instead of defining the topic model p w |¦Èt  as a multinomial model  we define it as a log-linear model that combines all the features: 
p w |¦Èt   = exp ¦Ët f w  z                                                 1  
where f w  is the feature vector defined in section 1  ¦Ëis the weight vector and z is the normalization factor to make p w |¦Èt   a real probability.  ¦Ë is estimated by maximizing the likelihood of the feedback documents. to avoid overfitting  we do regularization on¦Ëby assuming that it has a zero-mean gaussian prior distribution . then the objective function to be maximized becomes: 
	l f  =¡Æ ¡Æd¡Êf	w v¡Ê tf  w d log  1 ¦Á p w|¦Èc   +¦Áp w|¦Èt         1  
                  ¦Ët¦Ë
where   is the regularization factor  which is set to be 1 in our experiments.  ¦Á is the parameter representing how likely we use the topic model to generate the pseudo-relevant document. it is set at a fixed value as in   1 in our case . since l f  is a concave function w.r.t. ¦Ë   it has a unique maximum. we solve this unconstrained optimization problem with l-bfgs algorithm .  
table 1 shows the results measured by map. again  the superscript   m  and  l  indicate the improvement over mix and log-linear model is statistically significant at p 1.  
from this table  we can observe that the log-linear model outperforms the mixture model only slightly. this shows that an unsupervised learning method  even with additional features  cannot improve the retrieval effectiveness by a large margin. the possible reason is that the objective function  l f   does not correlate very well with map. the parameters maximizing  l f  do not necessarily produce good map.  
in comparison  the mix+svm model outperforms largely the loglinear model on all the three collections  and the improvements on ap and disk1 are statistically significant. this result shows that a supervised learning method can more effectively capture the characteristics of the genuine good expansion terms than an unsupervised method.  
model ap wsj disk1 mix 1 1 1 log-linear 1 1 1 mix+svm 1m l 1 1m l table 1. supervised learning vs unsupervised learning 1 soft filtering vs. hard filtering 
we mentioned two possible ways to use the classification results: hard filtering of expansion terms by retaining only the good terms  or soft filtering by increasing the weight of the good terms. in this section  we compare the two methods. table 1 shows the results obtained with both methods. in the table   m    r   and  h  indicate the improvement over mix  rel and hard are 
statistically significant with p 1 
from this table  we see that both hard and soft filtering improves the effectiveness. although the improvements with hard filtering are smaller  they are steady on all the three collections. however  only the improvement over mix model on the ap and disk1 data is statistically significant.  
in comparison  the soft filtering method performs much better. our explanation is that  since the classification accuracy is far from perfect  actually  it is less than 1% as shown in table 1   some top ranked good expansion terms  which could improve the performance significantly  can be removed by the hard filtering. on the other hand  in the soft filtering case  even if the top ranked good terms are misclassified  we will only reduce their relative weight in the final query model rather than removing them. therefore  these expansion terms can still contribute to improving the performance. in other words  the soft filtering method is less affected by classification errors. 
model ap wsj disk1 mix 1 1 1 mix+hard 1m 1 1m mix+soft 1m h 1 1m h rel 1 1 1 rel+hard 1 1 1 rel+soft 1r h 1 1r table  1. soft filtering vs hard filtering 1 reducing query traffic 
a critical aspect with query expansion is that  as more terms are added into the query  the query traffic  i.e. the time needed for its evaluation  becomes larger. in the previous sections  for the purpose of comparison with previous methods  we used 1 expansion terms. in practice  this number can be too large. in this section  we examine the possibility to further reduce the number of expansion terms. 
in this experiment  after a re-weighting with soft filtering  instead of keeping 1 expansion terms  we only select the top 1 expansion terms. these terms are used to construct a small query topic model p w |¦Èt  . this model is interpolated with the original query model as before. the following table describes the results using the mixture model. 
model ap wsj disk1 mix+soft-1 1 1 1 table  1.  soft filtering with 1 terms  
as expected  the effectiveness with 1 expansion terms is lower than with 1 terms. however  we can still obtain much higher effectiveness than the traditional language model lm  and all the improvements are significantly significant. 
the results with 1 expansion terms can also be advantageously compared to the mixture model with 1 expansion terms: for both ap and disk1 collections  the effectiveness is higher than the mixture model. the effectiveness on wsj is very close.  
this experiment shows that we can reduce the number of expansion terms  and even with a reasonably small number  the retrieval effectiveness can be greatly increased. this observation allows us to control query traffic within an acceptable range  and make the method more feasible in the search engines. 
1. conclusion 
pseudo-relevance feedback  which adds additional terms extracted from the feedback documents  is an effective method to improve the query representation and the retrieval effectiveness. the basic assumption is that most strong terms in the feedback documents are useful for ir. in this paper  we re-examined this hypothesis on three test collections and showed that the expansion terms determined in traditional ways are not all useful. in reality  only a small proportion of the suggested expansion terms are useful  and many others are either harmful or useless. in addition  we also showed that the traditional criteria for the selection of expansion terms based on term distributions are insufficient: good and bad expansion terms are not distinguishable on these distributions. 
motivated by these observations  we proposed to further classify expansion terms using additional features. in addition  we aim to select the expansion terms directly according to their possible impact on the retrieval effectiveness. this method is different from the existing ones  which often rely on some other criteria that do not always correlate with the retrieval effectiveness. 
our experiments on three trec collections showed that the expansion terms selected using our method are significantly better than the traditional expansion terms. in addition  we also showed that it is possible to limit the query traffic by controlling the number of expansion terms  and this still lead to quite large improvements in retrieval effectiveness. 
this study shows the importance to examine the crucial problem of usefulness of expansion terms before the terms are used. the method we propose also provides a general framework to integrate multiple sources of evidence.  
this study suggests several interesting research avenues for our future investigation: the results we obtained with term classification are much lower than with the oracle expansion terms. this means that there is still much room for improvement. in particular  improvement in classification quality could directly result in improvement in retrieval effectiveness. the improvement of classification quality could be obtained by integrating more useful features. in this paper  we have limited our investigation to only a few often used features. more discriminative features can be investigated in the future. 
