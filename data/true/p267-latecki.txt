we introduce a new em framework in which it is possible not only to optimize the model parameters but also the number of model components. a key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the em framework. while the classical em algorithm estimates model parameters empirically using the data points themselves  we estimate them using nonparametric density estimates.
﹛there exist many possible applications that require optimal adjustment of model components. we present experimental results in two domains. one is polygonal approximation of laser range data  which is an active research topic in robot navigation. the other is grouping of edge pixels to contour boundaries  which still belongs to unsolved problems in computer vision.
categories and subject descriptors
i.1  pattern recognition : general
general terms
algorithms  performance  experimentation
keywords
em  expectation maximization  kullback-leibler divergence
1. introduction
﹛our goal is to approximate the ground-truth density q x  with a member p成 x  of a parametric family {p成 x  : 成 ﹋ s} of densities. we use kullback-leibler divergence  kld  to measure dissimilarity between the ground-truth and parametric family of densities. by definition  the kld between
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  philadelphia  pennsylvania  usa.
copyright 1 acm 1-1/1 ...$1.
the ground truth q x  and the density  p成 x  is:
q x 
	d q x ||p成 x   =	log q x dx
p成 x 
	= z logq x q x dx   z logp成 x q x dx	 1 
the data itself  being noisy  do not directly correspond to the ground truth density. we demonstrate below that the ground-truth density q x  can be estimated from the data. the use of kullback-leibler divergence  kld  enables us to fit an optimal model to the ground truth rather than the noisy data.
﹛observe that kld is able to approximate the optimal number of model components of p成. this is due to the fact that kld d q||p成   viewed as a functional on the space  p成‘ of gaussian mixtures  is convex and hence has a unique minimum. however  this minimum does not have to be a finite mixture of gaussians  since the space of finite gaussian mixtures is not closed. on the other hand  the set of finite gaussian mixtures is dense in the space of continuous functions. therefore  we can approximate the minimum with any required precision when we minimize kld in the space of finite gaussian mixtures. in particular  this means that we can estimate the number of mixture components  but it is impossible to determine the optimal number of components  since this number may be large or infinite  e.g.  some ground truth model components could be very small . therefore  using kld we are able to correctly estimate the number of 'large'  or significant  model components.
﹛it is known that the expectation maximization  em  algorithm can be derived from kld  section 1 . however  in the em framework the number of model components must be known and fixed. this is due to the fact that the log likelihood function that is optimized in the em framework increases when the number of model components is increased. thus  when optimizing the log likelihood  we cannot estimate the number of model components.
﹛an important question which arises is: why is the ability to estimate the optimal number of model components lost in the derivation of em from kld  in this paper we provide an answer to this question and derive a new em target function from kld that allows us to optimize not only the model parameters but also to estimate the number of the model components. moreover  in the proposed framework  em converges to an optimal solution even if the initial values of model parameters are not close to the global optimum.
﹛there exist many possible applications that require optimal adjustment of model components. we illustrate our approach on polygonal approximation of laser range data and object contours in digital images. polygonal maps obtained by polygonal approximation of laser range data are very attractive means to represent range scan data due to their very compact size and simplicity. hence they lead to huge data compression and make it easier to access higher level features. therefore  several approaches have been proposed to obtain such maps  the most recent ones being  1  1  1 . an excellent overview can be found in . although approximation with higher order curves is possible  approximation with lines is more stable in the presence of noise  e.g.  see ch. 1 in    which is the case for laser range scans. therefore  we focus on polygonal approximation in this paper. however  the proposed em framework has a broader scope of possible applications. polygonal approximation of edge pixels in digital images can be interpreted as grouping of edge pixels to parts of object contours  which belongs to unsolved problems of computer vision. the approaches to grouping of object contours date back to the the first results of gestalt psychology in the beginning of 1th century   and they remain an active research topic in computer vision. an overview of techniques for polygonal approximations of curves  which require that the order of data points is known  can be found in .
﹛the main difficulty of fitting polylines in the above applications is that the segmentation  or correspondence  of data points to line segments as well as the order of data points are unknown. the expectation maximization  em  algorithm  provides a particullary useful framework to solve this correspondence problem. actually em applied to line fitting is known as the healy-westmacott procedure in statistics  and predates em by many years . however  polygonal approximation of point data requires that not only the model parameters but also the number of model components  line segments  are estimated  but as observed above in the em framework the number of model components must be known and fixed. moreover  em produces an optimal solution only if the number of model components is well estimated and the initial values of model parameters are close to the global optimum.
﹛we give a simple example that illustrates the fact that em yields a locally optimal solution if the initial values of model parameters are not close to globally optimal values. in fig. 1 we see data points that follow the horizontal and vertical lines in a cross like pattern. fig. 1 a  shows two diagonal lines that form the initial configuration of the standard em algorithm. the number of model components  two lines  is correctly initialized  but their initial position is not close to the global optimum. fig. 1 b  shows the final  locally optimal  result obtained by the classical em algorithm. finally  fig. 1 c  shows the globally optimal approximation obtained by the proposed method on the same input.
﹛due to the problem of getting stuck in local optima  a correct estimation of the number of components and the parameter values of a statistical model is crucial in all em applications  and therefore  belongs to one of the most challenging problems in statistical reasoning. before we describe the proposed approach  we review existing solutions.
﹛the existing solutions can be divided into two categories. the first category is based on using penalty functions like the bayesian information criterion  bic   or alternatively  the minimum description length  mdl   and akaike information criterion  aic   to determine the optimal number of model components. the approaches in this category require that em is run until it converges  whatever the initial number of components assumed  with the goal of selecting the components exemplified by the ground truth. as we show below  approaches of this sort cannot be guaranteed to correctly estimate the optimal number of model components because em may get stuck in local optima.
﹛in  the use of bic and aic to estimate the number of model components is discussed. we focus here on bic but our arguments also apply to aic and mdl. for a fixed number of data points  which is the case in our application at each given time t  the use of bic represents a trade-off between emphasizing the importance of model complexity and the likelihood of the data. typically a model that has the greatest bic values is selected by repeatedly comparing these values for all possible numbers of model components. the problem with this approach is that its success depends on the convergence of the em algorithm to the global optimum whatever the initial number of model components assumed. if  for some given initial starting configuration  em gets stuck in a local optimum  the bic estimate will incorrectly estimate the ground truth number of model components. for example  the correct number of model components could not be determined  using bic methodology  for the situation in fig. 1 a b . since em got stuck in a local optimum in  b   the likelihood of the model with two components is very low  and consequently the ground-truth model with two components is not selected. to the best of our knowledge this problem is not addressed by any existing approach designed to estimate the number of model components.
﹛moreover  in practice there is a hidden parameter that is manually adjusted to obtain the desired number of model components in bic. this parameter is the standard deviation of the measurement process. in bic this standard deviation acts as a tradeoff weighting factor between the likelihood of the data points and the model complexity. as determined experimentally on ground-truth data in   bic tends to over weight the penalty on model complexity  which leads to a too small number of model components.
﹛the second category of approaches to estimate the optimal number of components is based on steps involving splitting and merging of em model components after each algorithm iteration. our approach belongs to this category. it is important to mention that the approaches in the second category yield a quicker convergence since they adapt the number of model components and model parameters to the given environment after every algorithm iteration while bic requires convergence for each given initial number of model components.
﹛we will first show that the existing split and merge approaches cannot be guaranteed to correctly estimate the optimal number of model components due to the fact that they cannot recognize locally optimal solutions that are not globally optimal.
angle 1; standard deviation 1

	 1	 1	1.1	 1	 1	1.1	 1	 1	1.1
	 a 	 b 	 c 
figure 1:  a  shows the data points and the initial position of model lines.  b  shows the optimal approximation of the data points obtained by em.  c  shows the optimal approximation result obtained by the proposed﹛in 1 green  proposed a solution based on iterative merging and splitting components of a mixture model with the goal of obtaining a better mixture model in the case of univariate normal mixtures. green's solution is based on a fully bayesian mixture analysis that makes use of jump markov chain monte carlo  mcmc  methods. the jumps are realized by split and merge moves that are reversible. since green's merge move is evaluated using the data points  method.
it requires an additional penalty for the number of model components. the number of model components depends largely on this penalty  which is not directly related to the model quality assessment  as is the case in our approach. green's approach is used to fit polygons to contours in digital images in ; in this setting split moves correspond to inserting a new vertex into the polygon and merge moves correspond to removing a vertex. greens algorithm requires a huge number of iterations  green reported the need for 1 iterations . this is due to a random selection of vertices  which is counterintuitive from the point of view of human visual perception. humans are able to identify good and bad fitting parts of a given polygon by visual inspection. in consequence of this  it makes more sense to base algorithm moves on local visual inspection rather than on random selection.
﹛in 1 ueda et al.  proposed a split and merge extension of the em framework for mixture models. their split and merge rules do not require any penalty as is the case for greens approach. however  as we will now show  their approach is not able to recognize some locally optimal solutions that are not globally optimal. their merge criterion is based on posterior probabilities associated with the model components. two model components 肋i and 肋j are merged if they have almost equal posterior probabilities over the data points; this means that the probability of being generated by either component is approximately equal for all data points  formula  1  in  . defining model components as line segments  this means that data points are approximately the same distance to either one of components that are under consideration to be merged. observe that the two model components  diagonal line segments  in fig. 1 a  are merged by their rule. this  however  incorrectly results in a single line segment that cannot provide good support for the cross-shaped data points.
﹛a single model component is split if the local data density is significantly different from the global density; both densities are estimated using the actual component parameters of this component  formula  1  in  . this split criterion fails in our application  where the model components are line segments. the single line segment in fig. 1 is not split by this criterion  since both densities are identical  i.e.  match perfectly . however  clearly two line segments are needed to obtain an optimal fit to the data points. this critique also applies to the approach in  that uses the same split criterion.
﹛the above problems also explain why the algorithm by ueda et al.  needs a relatively large number of iterations to converge.  reports that about 1 iterations are needed to fit lines to data points. the proposed algorithm usually converges in less than 1 iterations.

figure 1: clearly two line segments are needed to obtain an optimal fit do the depicted data points.
﹛we observe that all the split and merge steps presented in the literature optimize different target function than the function optimized by the classical m step of the em algorithm. here we propose split and merge steps that optimize the same target function  sections 1 and 1 .
1. optimizing kld
it can be easily derived that the parameters 成 minimizing
 1  are given by	b
	成 =b	argmax成 z  logp成 x  q x dx‘	 1 
we obtain the classical maximum likelihood estimator by applying the mc  monte carlo  integral estimator to  1  under the assumption that the observations x1 ... xn are i.i.d.  independently and identically distributed  sample points selected from the ground truth distribution q x .
	成 =b	argmax成 xlogp成 xi 	 1 
i
however  as we derive below  equation  1    if some proportion of the observations x1 ... xn is noisy  a more accurate estimator of 成 in  1  is given by:
	成 =b	argmax牟 xlogp牟 xi sdd xi  	 1 
i
where sdd is called the smoothed data density and is defined in section 1 by the means of nonparametric density estimation.
﹛equation  1  is the basis of the proposed approach. to demonstrate the significance of  1   we consider the problem of estimating the optimal number of model components by minimizing the kld d q x ||p成 x   in 成. it is well known that  1  cannot be used to estimate the correct number of model components  since  1  increases when the number of model components increases. in contrast  we are able to determine the correct number of model components when using  1  to estimate the kld  d q x ||p成 x  . thus  the modified em algorithm that maximizes  1  is not only able to estimate model parameters but also the right number of model components.
﹛one of the key steps in the derivation of  1  is the monte carlo  mc  estimate of the integral given by the right hand side of equation  1 . let x1 ... xn be i.i.d. sample points drown from the probability density function  pdf  q x . then we can approximate the integral of a continuous function f by its mc estimate:
	 	 1 
in the usual approach to inference  it is a commonly accepted assumption that sample data points x1 ... xn are distributed according to the  estimated  density q x . this assumption is the key to insuring that maximum likelihood estimators are appropriate for purposes of estimating parameters of interest. however  in all real applications  the sample data points are corrupted by a certain amount of noise. usually the proportion of noisy points does not decrease when the number of sample points is increased. we quantify this corruption by assuming that the data follow a distribution consisting of a mixture of an unknown groundtruth distribution q x  and an unknown noise distribution 灰 x . let u x  = 汐q x  +  1   汐 灰 x  denote this mixture distribution. the quantity  汐 is the probability that an observation comes from the ground-truth distribution q x  and  1 汐  is the probability that it comes from the noise distribution. since the observed sample data points do not follow the ground truth distribution q x  but the mixture of noise and true distribution u x   we obtain a more accurate mc estimate of the integral in  1  r f x q x dx =
in section a we show that equation  1  leads to a substantially smaller mean squared error in the estimation of the integral than equation  1 . the ratio
	汐q x 	汐q x 
=	 1  u x  汐q x  +  1   汐 灰 x 
is equivalent to the conditional probability  p ground truth|x   that an observed data point x is selected from the ground truth density q x . we note that large values of p ground truth|x indicate that the data point x is of significant interest for inference purposes; small values indicate the reverse.
﹛in section 1 we show that it is possible to estimate a ratio proportional to  1  with the smoothed data density sdd x . consequently 
		 1 
by identifying sdd xi  with its normalized value psddj sdd x ix j  for i = 1 ... n  we can rewrite equation  1  in the form
	 	 1 
finally equation  1  clearly follows from  1  and  1 .
1. e and m steps
﹛we introduce latent variables z1 ... zn which serve to properly label the respective data points x1 ... xn. it is assumed that the pairs  xi zi  for i = 1 ... n are i.i.d. with common  unknown  joint  ground truth  density  q x z  = q x q z|x ; q x  is the marginal x-density and q z|x  is the conditional density of the label z given x. in this new framework  the kld between the joint density q x z  and a parametric counterpart density p成 x z  is

﹛we are now ready to introduce the expectation  e  and maximization  m  steps. both steps aim at minimizing the same target function  1  in our framework. the expectation step yields the standard em formula; considerations discussed above lead to a different solution for the maximization step.
expectation step: for a fixed set of parameters 成  we want to find a conditional density q z|x  that minimizes d q x z ||p成 x z  . since kld is always nonnegative  and the second summand in  1  is minimized for q z|x  = p成 z|x   in which case it is equal to zero   we obtain from  1  that q z|x  = p成 z|x  minimizes d q x z ||p成 x z  .
in particular  for given sample points x1 ... xn  we obtain
 
where 羽l = p zi = l|成  and 羽j = p zi = j|成  are the prior probabilities of component labels l and j correspondingly. maximization step: for the fixed marginal distribution q z|x  = p成 z|x   we want to find a set of parameters 成 that maximizes  1 . substituting q z|x  = p成 z|x  in  1   we obtain
d q x z ||p成 x z   = z log  q x dx = d q x ||p成 x   q x 
p成 x 
﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛ 1  thus  minimizing d q x z ||p成 x z   in 成 is equivalent to  minimizing d q x ||p成 x   in 成. using the estimate derived in equation  1   minimizing  1  in 成 is equivalent  in the mc setting discussed above  to maximizing the weighted marginal density
wm 成  = xsdd xi logp成 xi  = xsdd xi logp xi|成 
	n	k
= xsdd xi log xp xi|zi = l 成 p zi = l|成  
i=1	l=1 n	k
	= xsdd xi log xp xi|zi = l 成 羽l 	 1 
	i=1	l=1
where 羽l = p zi = l|成  are the prior probabilities of component labels l = 1 ... k.
﹛now we explicitly use the incremental update steps of the em framework. using the prior probabilities of component labels  obtained at stage t for l = 1 ... k  we obtain from  1  that an update of wm 成  is estimated by maximizing
	n	k
wm 成;成 t   = xsdd xi log xp xi|zi = l 成 羽l t  
	i=1	l=1
 1 
in 成 with 成 t  denoting the value of 成 computed at stage t of the algorithm. the crucial difference between this and the standard em update is that our target function is weighted with terms sdd xi . we note that the known convergence proofs for the em algorithm apply in our framework  since adding the weights sdd xi  in  1  does not influence the convergence.
1. split and merge
﹛the proposed split and merge steps adjust the number of model components by performing component split and merge steps only if they increase the value of our target function  1 . since the proposed split and merge steps are computed in the sparse em framework  the convergence of our algorithm follows from .
﹛our framework is very general in that it allows many possible selections of the candidate components for the split and merge steps. we present specific selection methods of the candidate components in section 1. they are based on a maximum a posteriori principle. in the following formulas  we assume that the candidate components are given. split: assume that we are given two candidate model components l1 l1; we consider replacing the model component l with components l1 l1. since our goal is maximizing qm 成;成 t   in formula  1   we simply need to check whether replacing l with l1 l1 increases wm  where j ﹋ {1 ... k}:
n
wm 成;成 t   = xsdd xi log xp xi|zi = j 成 羽j t  
	i	j
	 	 1 
we only need to perform 'local' computation to perform this test  i.e.  we only need to compute the corresponding probabilities for the candidate components l1 l1  subject to the condition that. the parameters are estimated following the sparse em step in neal and hinton    see equation  1  . in accordance with the results of  this local computation guarantees that the target function increases after each iteration  if  1  holds . convergence is also guaranteed in this way.
merge: given a candidate component l  we merge two existing model components l1 l1 to l if for j ﹋ {1 ... k}
n
wm 成;成 t   = xsdd xi log xp xi|zi = j 成 羽j t  
 	 1 
again we only need to perform 'local' computations to perform this test. for merge  we only need to compute the corresponding probabilities for the candidate component l  subject to the same constraint. if  1  holds and we replace l1 l1 with l  the convergence of our algorithm follows from the results of .
﹛we note that the proposed split and merge steps do not work in the classical em framework. to see this  consider sdd xi  = 1 for all the data points  i = 1 ... n . the merge inequality  1  is not satisfied even if the ground truth model is assumed to be a single component  since multiple components can better fit the data  and consequently have a larger log likelihood value. analogously  if the split inequality  1  holds for a reasonable selection of candidate component models  the classical em framework incorrectly splits ground truth components. thus  a mixture model of larger number of components is always prefered in the classical em framework. in the proposed framework  sdd represents an estimated density of the data points  estimated in a nonparametric way as described in section 1 . consequently  in the proposed split and merge steps  the divergence of parametric components l l1 l1 from the ground truth is evaluated with respect to this nonparametric density.
1. estimating the data density
﹛in this section  we construct the function sdd x  that estimates the ratio  1 . following the assumption made in calculating bootstrap samples  we can estimate the density  u x  on the observed i.i.d. sample points x1 ... xn drawn from u x  by ub x1  = ﹞﹞﹞ = ub xn  = n1.
﹛we use a kernel estimate  which is the most widely-used nonparametric density estimation method  to estimate the ground truth density q x . thus  under the assumption that x1 ... xn are i.i.d. sample points  we estimate the ratio  1  with a smoothed data density obtained by
n
sdd xj  ≦  ＞ nq xj  = nxk 	  q xj 	d xj xi  u xj 	h
i=1
n
= xg d xj xi  1 h  	 1  n
nh
i=1
where proportionality refers to the fact that psdd xi  = 1  d x y  is the euclidean distance  and g d x y  1 h  is a gaussian with mean zero and the standard deviation  std  h. an intuitive motivation for  1  is as follows:
﹛if a given data point xj was sampled from the true distribution q x   then the ratio uq  xxjj   would be large. since the ratio is proportional  see equation  1   to the probability  p groundtruth|xj   this too would be large. as a consequence of this latter fact  xj would be likely to lie in a dense region of the observed sample points and consequently sdd xj  would be large.
﹛if a given data point xj were sampled from the noise distribution 灰 x   then the ratio  uq  xxjj   would be small. for analogous reasons  this implies that xj would be likely to lie in a sparse region of the sample space  and consequently sdd xj  would be small.
﹛to estimate the bandwidth parameter h  we can draw from a large literature on nonparametric density estimation  1  1 . as we show in the presented experimental results  an accurate bandwidth estimation in not crucial in our approach. it is also possible to use variable bandwidth .
1. specific details of the m step
﹛in equation  1  of section 1 it was shown that minimizing the kullback leibler divergence in the parameters 成 amounts to maximizing the weighted marginal density wm 成 . we use this fact throughout the discussion below.
﹛the goal of this section is to show that formulas for maximizing  1  are analogous  except for multiplication by sdd  to log likelihood maximization in the standard em algorithm. to illustrate this we compute a partial derivative of  1  over one of the model parameters 牟j from the parameter vector 成 that is a parameter of j'th model component.
	 成;成 t  	 1 
	n	k
	=	sdd xi p x1i|成   牟 j xp xi|zi = l 成 羽l t 	 1 
i=1	l=1 n
	羽j	 
	=	sdd xi 	|	p xi|zi = j 成 	 1 
p xi 成   牟j
i=1
x
n
=sdd xi 	| |	logp xi|zi = j  1 成  羽jp xi zi = j 成    p xi 成 	 牟j
i=1 n
 
	=	sdd xi p zi = j|xi 成  logp xi|zi = j 成  1 
 牟j
i=1
the transitions from  1  to  1  and from  1  to  1  are based on
.
the transition from  1  to  1  is based on the bayes rule.
﹛for example  in the 1d case when 牟j is the mean of one of the gaussian mixture components  we can substitute  and set  1  equal to zero:

then we obtain in the 1d case
		 1 
1. one dimensional example
﹛below  we use the notation  g x;米;考  for the gaussian density at x with mean 米 and standard deviation 考. we generated a 1 dimensional data set x1 ... xn  with n=1  from the noisy density 
	1  g x;1 	wprob 1%
	   g x;1 	wprob 1%
	u x  =	 1 
	g x;1 	wprob 1%
	  : g x;1 	wprob 1%
see fig. 1 a  for a plot of the generated data with groundtruth groups marked with different symbols.
﹛we employed a split and merge algorithm with 1 initial groups with group labels chosen randomly. for each component considered for possible splitting  our algorithm searched for a component point  whose density  as measured by sdd  is more than 1 standard deviation below the average component density. if no such point exists  the component is not split. splits are accepted if they cause the objective function to increase its value. all pairs of components are considered for possible merging. splits and merges are accepted if they cause the objective function to increase from its former value in accord with formulas 1 and 1  correspondingly.
﹛the results obtained by the proposed algorithm are illustrated in fig. 1. to illustrate the relationship between the smoothing bandwidth h of sdd and robustness properties of the parameter estimates  we repeat our algorithm for different values of h. smaller values of  the bandwidth  h result in less smoothing; larger values result in more smoothing. the bandwidth of h = 1  calculated using least squares cross-validation  see    is optimal in this setting. this follows from a general theorem relating the optimal bandwidth to the standard deviation and sample size. the point labels obtained by our algorithm for h = 1 are shown in fig.
1 b .
﹛observe a large stability of our algorithm with respect to the bandwidth h illustrated by plots in fig. 1. for each h value  the algorithm was initialized with a randomly selected group labels consisting of 1 groups. our algorithm always converged to the correct number of three signal model components. small bandwidths did not adequately discriminate between noise and signal. already moderately large band-
widths demonstrate adequate discrimination in that component means 米j and weights 羽j  j = 1 1  are accurately estimated.
1. line segments as components
﹛we present specific details concerning our use of line segments as em model components in the applications presented below. we stress that this section applies also to hyper planes in any dimensions  but the presentation is given in terms of line segments for purposes of simplification.
﹛the proposed approach requires a minor extension of em line fitting to work with line segments  which we will call expectation maximization segment fitting  emsf . the difference between emsf and em line fitting is that our model components are line segments  rather than lines . the input  for our model  is a set of line segments and a set of data points. as with em the proposed emsf is composed of two steps:
 1  e-step the em probabilities are computed based on the distances of points to line segments instead of the distances of points to lines.
	ground truth for the simulated data	data partitioned into converged groups

	1	1	1	1	1	1	1	1	1	1
	 a 	indices	 b 	indices
figure 1:  a  a plot of the simulated data with their ground-truth component labels.  b  a plot of data points with labels to which the em algorithm converges.

	 a 	 b 	 c 
figure 1: component means  a   weights  b   and sigma  c  as function of bandwidth h used in sdd. 1  m-step given the probabilities computed in the estep  the new positions of the lines are computed by minimizing squared regression error weighted with these probabilities.
as in the case of em line fitting  the output of the m-step is a new set of lines  not line segments . since we need line segments as input to the e-step  we trim lines to line segment based on their support in the sample data. this is done by the split process described in section 1.
﹛now we describe the specific details related to line segments for steps  1  and  1 . in order to derive the solution of  1  for em model components being line segments  we introduce so called em weights. in the classical em  the weight wil t  = p zi = l|xi 成 t   represents the probability that point xi corresponds to segment sl for l = 1 ... k. we use the notation 牟l for the parameters of the line segment sl itself. in our framework wil t  ≦ sdd t  xi  ﹞ p zi = l|xi 成 t     1 
and the weights are normalized so that = 1 for each i. after the e-step associated with the t'th iteration is accomplished  we obtain a new matrix  wil t  . intuitively  each row i = 1 ... n of this matrix corresponds to weighted probabilities that the data point xi is associated with the corresponding line segments; each column l = 1 ... k can be viewed as weights representing the influence of each point on the computation of new line positions in the m-step. below  we use the notation xi =  xix xiy  with  i = 1 ... n  for the coordinates of the observed data points  and  ‘x y‘  for the coordinate averages. the line ll  constructed below  is constructed to go through the point  ‘x y‘ . to obtain the solution of  1   we perform an orthogonal regression weighted with the matrix  wil . the solution is given as the normal vector to line ll  which is the vector corresponding to the smallest eigenvalue of the matrix ml defined as

finally the parameters are given as parameters of the line segment sl t+1  obtained by trimming the line ll to the data points.
﹛we are now ready to introduce particular realization of split and merge for em model components being line segments. the proposed split and merge em segment fitting  smemsf  algorithm iterates the following four steps
 1  emsf	 1  split	 1  emsf	 1  merge
split step is presented in detail in section 1 while merge step is described in section 1. split evaluates the support in the data points of lines obtained by emsf and removes the parts that are weakly supported. since we have a finite set of data points  this has the effect of trimming the lines to line segments. finally the merge step merges similar line segments. thus  split and merge steps adjust the number of model components to better fit the data.
1 merging
﹛if inequality  1  holds  we merge two model components represented by parameters l1 l1 into one model componet given by parameter l. while components l1 l1 are present at step t  they are line segments sl1 sl1   we did not yet specify how to compute the candidate component l. now we describe a particular method to generate a candidate component l in the particular case in which the model components are line segments. we stress that other methods are possible and that inequality  1  applies to them too.
﹛a support set s sj  for a given line segment sj  model component l  is defined as set of points whose probability of supporting segment sj is the largest  i.e. 
s sj  = {xi : wij = max wi1 ... wik }.
this maps each data point to a unique segment using the maximum a posteriori principle. given two line segments sl1 sl1  the merged segment sl is obtained by trimming the straight line obtained by regression on data points in s sl1 ﹍ s sl1 . trimming is performed by line split described in section 1.
1 line split  ls 
﹛a classical case of em local optimum problem is illustrated in fig. 1 a   where the line segment is in a locally optimal position. clearly  the problem here is that we have a model consisting of one line only  while two line segments are needed. fig. 1 b  illustrates a split operation described in this section. it is based on removal of subsegments that do not have sufficient support in the data points. as the result we obtain two line segments. finally  fig. 1 c  shows the globally optimal approximation of the data points obtained by em applied to the two segments.
﹛the main idea is that higher point density along a segment indicates the presence of a linear structure in the data points around the segment. the amount of support that a line segment has is measured by the density of points around it. each line or line segment is examined regarding whether it has sufficient support in the data. only parts of segments that have this support are allowed to remain. this leads to a splitting of existing lines or segments.
﹛we use the nonparametric density estimation sdd to obtain the density along each segment. although we defined sdd only at the sample data points in  1   it is actually defined at every point. observe that sdd|sl restricted to a segment sl is a one dimensional function. we obtain split point candidates  and consequently model segment candidates  as local minima of sdd|sl.
1. applications
﹛an example application of our approach in robot mapping is outlined in fig. 1.  a  shows an original data set of laser range scan points aligned with the algorithm presented in . the original set is composed of 1 scans  each with 1 points. thus  the original input map is composed of 1 points. we initialize our algorithm with only two segments  the two diagonals  as model components.  b  shows the output of the second iteration of our algorithm. the final polygonal map in  d   obtained after 1 iterations  is composed of 1 segments  i.e.  of 1 points. thus  the proposed approach yields the data compression ratio of 1. the mean distance of scan points to the closest line segments is 1cm. we selected this map  since it contains surfaces of curved objects. the obtained polylines in  d  illustrate that the proposed approach is well suited to approximate linear as well as curved surfaces.
﹛now we apply the proposed approach to grouping edge pixels to polygonal curves representing object contours in digital images. two example applications of this kind are outlined in fig. 1.  a  shows an original input toy image.  b  shows the edges obtained by canny edge detector with a substantial amount of incorrect edge pixels  and the initial model for our algorithm. it consists of only two line segments.  c  shows an intermediate step of our algorithm. the final polygonal approximation obtained after 1 iterations is shown in  d .  e  shows a simulated image obtained by sampling 1 ground truth segments  1 points  with a substantial amount of noise  1 points .  f  shows the initial model segments for our algorithm. we present the results of our algorithm after 1 in  g  and 1 iterations in  h . we stress that we have only 1 signal points in comparison to 1 background noise points.
appendix a.	monte carlo approximations
﹛we use the notation q x  for the ground truth density of the data. we assume that the data  including noise  is disof u x . we use the notationb sdd xi ;i = 1 ... n for the nortributed as  u x  and let u x  be a standard density estimate
malized estimates of the ratio  at the given data points. as a result of theorem 1 we obtain that mean squared error  mse  for estimating the integral‵ pp r f x q x dx using
hsdd sdd xi f xi  is significantly smaller than that using hm = f xi  for any smooth function f.
﹛theorem 1. if x1 ... xn are data generated from the noisy density u x   then the approximate mse for estimating the integral r f x q x dx using hsdd ‵ psdd xi f xi  is  up to order o 1/n  
	mse hsdd  = o 1/n 	 1 
the mse for estimating the integral r f x q x dx using hm = f xi  is up to order o 1/n  
1
	mse hm  = jz f x  u x    q x ‘dxff	 1 
proof: the variances of either monte carlo approximation are of order o 1/n . hence  the mse's in either case correspond up to order o 1/n  to the squares of the bias's. the asymptotic bias for the monte carlo approximation hsdd is  via the delta method  equivalent to:
bias hsdd  ‵ j  1/n xsdd xi f xi    z q x f x dx- + n
i=1
 1/n xsdd xi    1- ﹞ z q x f x dxff	 1  n i=1
due to the normalization  the rightmost term of equation  1  in square brackets is 1. additionally  it follows from the central limit theorem that

it follows from equations  1  and  1  that
	bias hsdd 1 ‵ o 1/n 	 1 
	iteration 1  no splitting	iteration 1  no splitting	iteration 1
1
1
1
1
1
1
1
1
	1	1	1	1	1	1	1	1	1	1	1	1	1	1	1
	 a 	 b 	 c 
figure 1: it is obvious to us that the approximation in  c  of the underlying data points is significantly better then the approximation in  a .  a  shows the best possible approximation of the data points obtained by em.  b  the subsegments marked with crosses are removed  since their sdd values are too small  which results in splitting the segment to two parts.  c  shows the final approximation result obtained by em after the split.hence  by equation  1  and the remarks at the beginning of the proof 
	mse hsdd  = o 1/n 	 1 
the asymptotic bias for the monte carlo approximation hm is  equivalent to:
bias hm  = j 1/n xf xi    z f x q x dxff  1 

by the law of large numbers  up to order o 1/﹟n 
	 1/n xf xi  ‵ z f x u x dx	 1 
hence  by equations   1  and  1   it follows that  up to
order  
	bias hm  ‵ z f x  u x    q x ‘dx	 1 
as a consequence  it follows from equation  1  that up to order o 1/n  
1
	mse hm  ‵ jz f x  u x    q x ‘dxff	 1 
the result follows.
