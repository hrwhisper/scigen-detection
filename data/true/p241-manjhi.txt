for web applications in which the database component is the bottleneck  scalability can be provided by a third-party database scalability service provider  dssp  that caches application data and supplies query answers on behalf of the application. cost-effective dssps will need to cache data from many applications  inevitably raising concerns about security. however  if all data passing through a dssp is encrypted to enhance security  then data updates trigger invalidation of large regions of cache. consequently  achieving good scalability becomes virtually impossible. there is a tradeoff between security and scalability  which requires careful consideration.
　in this paper we study the security-scalability tradeoff  both formally and empirically. we begin by providing a method for statically identifying segments of the database that can be encrypted without impacting scalability. experiments over a prototype dssp system show the effectiveness of our static analysis method-for all three realistic benchmark applications that we study  our method enables a significant fraction of the database to be encrypted without impacting scalability. moreover  most of the data that can be encrypted without impacting scalability is of the type that application designers will want to encrypt  all other things being equal. based on our static analysis method  we propose a new scalability-conscious security design methodology that features:  a  compulsory encryption of highly sensitive data like credit card information  and  b  encryption of data for which encryption does not impair scalability. as a result  the security-scalability tradeoff needs to be considered only over data for which encryption impacts scalability  thus greatly simplifying the task of managing the tradeoff.
1.	introduction
　applications deployed on the internet are immediately accessible to a vast population of potential users. as a result  they tend to experience fluctuating and unpredictable load 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.

figure 1: scalable architecture for databaseintensive web applications. in this work  we focus on the database scalability service provider  dssp   the shaded cloud.
especially due to events such as breaking news  e.g.  1  and sudden popularity spikes  e.g.  the  slashdot effect  . if the application uses a commodity server  the  peak  customer load on the application may easily exceed the capacity of the server. the approach of over-provisioning-to invest in a server farm  and to hire skilled personnel to maintain it-is not only expensive but also risky because the expected customers might not show up. an appealing alternative is to contract with a scalability service that charges on a per usage basis. content delivery networks  cdns   provide such service by maintaining a large  shared infrastructure to absorb load spikes that may occur for any individual application. however  cdns currently do not provide a way for scaling the database component of a web application. hence the cdn solution is not sufficient when the database system is the bottleneck  as in several important web applications like bulletin-boards and e-commerce applications.
　to support applications where the database is the bottleneck  previous work  has proposed using a third-party database scalability service provider  dssp . a dssp caches application data and supplies query answers on behalf of the application. to be cost-effective  dssps will need to cache data from  home servers  of many applications  figure 1 shows the resulting architecture   inevitably raising concerns

figure 1: query  update  and invalidation pathways.
select toy id from toys where toyname= select qty from toys where toyid= select custname from customers where cust id= delete from toys where toyid= table 1: an example toystore application  denoted simple-toystore  with three query templates
  one update template   and two base relations: toys with attributes toy id  toy name  qty  and customers with attributes cust id  cust name. the question marks indicate parameters bound at execution time.
about security1. such concerns have been increasing lately  as borne by well-publicized instances of database theft  and the security legislation in the california senate .
to use untrusted dssps with confidence  applications must:
  prevent the dssp from tampering with master data. the dssp caches read-only copies  which are kept consistent with master copies maintained at application home servers via invalidation. all updates are applied to master copies directly.  in many web applications  updates are infrequent; so the load on home servers due to updates is low.    prevent unauthorized reading of data that passes through the dssp. a straightforward way to secure data passing through the dssp is to encrypt such data. the dssp then stores encrypted data. to permit answering of queries  one can use encryption schemes that permit query processing on encrypted data. however  recent work  has shown that only weak encryption can be used if queries are to be executed efficiently on the dssp. therefore  with this option  security of all data might be compromised. the only remaining alternative then is to store  encrypted  query results in the form of materialized views at the dssp. answering a query requires only a lookup operation  permitting arbitrarily strong encryption.
　figure 1 shows the flow of queries  updates  and invalidations in the architecture implied by the above discussion. in the figure  diagonal shading denotes information that is subject to encryption. the dssp maintains a cache of  encrypted  query results.  encrypted  queries are answered
accessible invalidationtemp-param-querylatesetersresultsconditionnononoall ofyesnonoallyesyesnoall if toy id=1yesyesyes if toy id=1  if toyid=1table 1: invalidations differ depending on the amount of information the dssp can access. the table is for update with parameter 1.
from the cache when possible; cache misses result in queries being forwarded to the home organization. all updates are routed to the home organization via the dssp  in encrypted form . the dssp monitors completed updates  and invalidates cached query results as needed to ensure consistency.  in our architecture  the home organization is free from the overhead of participating in invalidation decisions. 
1	security-scalability tradeoff
　in this work  we study the security-scalability tradeoff that arises when dssps are employed. to illustrate the presence of such a tradeoff  we introduce a simple example application called simple-toystore  specified in table 1. we focus on the application's database access templates- queries or updates missing zero or more parameter values. table 1 lists the invalidations the dssp needs to make on seeing a specific update in four different scenarios; each scenario is represented by a row of the table. the scenarios differ in what information the dssp is able to access. for example  if no information is accessible  i.e.  all data is encrypted  as in the first row  then all cached query results are invalidated on seeing an instance of update. however  if the template information is accessible  as in the second row  then cached query results of all instances of only and  are invalidated. as the information available to a dssp increases  moving down the rows   the number of invalidations it needs to make decreases  thereby increasing scalability.
　encryption of queries  updates and data for security purposes limits the information available to the dssp for making invalidation decisions. with limited information  the dssp is forced to employ conservative invalidation strategies to maintain consistency  resulting in excess invalidations and reduced scalability. this basic tradeoff between security and scalability is illustrated quantitatively in figure 1  which shows measurements of the tpc-w online bookstore benchmark executed on a prototype dssp system we have built  details are provided in section 1 . the vertical axis plots scalability  measured as the number of concurrent users that can be supported while keeping response times within acceptable limits. the horizontal axis plots a simple measure of security: the number of query templates embedded in the bookstore application for which query results are encrypted as they pass through the dssp. it is straightforward to achieve either good security or good scalability by encrypting either all data or no data. achieving good scalability and adequate security simultaneously requires more thought.

security
 number of query templates with encrypted results  
figure 1: security-scalability tradeoff  tpc-w bookstore benchmark .
1	managing the security-scalability tradeoff
　there is often room to maneuver with respect to what data needs to be encrypted. flexibility arises because in most web applications  not all data is equally sensitive. it may range from highly-sensitive data such as credit card information  to moderately sensitive data such as inventory records  to completely insensitive data such as the weekly best-seller list  which is made public anyway.
　in general  management of the security-scalability tradeoff requires careful assessment of data sensitivity  weighed against scalability goals. unfortunately  it is nontrivial to assess the scalability implications of ensuring the security of a particular portion of the database. furthermore  for data that is not entirely insensitive  it can be difficult to quantify sensitivity in a meaningful way. therefore it is not immediately clear how to best approach the task of managing the security-scalability tradeoff.
　in this paper we present a convenient shortcut  which simplifies the task substantially while avoiding undesirable compromises with respect to security or scalability. the idea is to identify portions of the database that can be encrypted while incurring no additional penalty to scalability. the outcome of applying this idea is shown in the upper-right point in figure 1  labeled  our approach.  the data that can be encrypted using our approach does not need to be considered for the security-scalability tradeoff  thus greatly simplifying the task of managing the tradeoff. hence  for the benchmark applications we have evaluated  our approach automatically achieves good security1 without compromising scalability.
1	related work
　prior work related to ours can be partitioned into two categories: database services and view invalidation. we discuss each in turn.
1.1	database services
　prior work on providing database services can be classified into two categories: the database outsourcing  do  model and the database scalability service provider  dssp  model.
　in the do model  an application outsources all aspects of management of its database to a third party . a key concern is to safeguard the application's sensitive data. since the do provider houses the application's entire database  one way to ensure security of an application's data is to store an encrypted database at the do provider  and use encryption schemes that permit query processing on encrypted data  1  1  1 . aggarwal et al.  suggest an alternative- distribute data across multiple independent providers that do not communicate with one another.
　in contrast to work in the context of the do model  we consider the dssp model  in which only database scalability  and not full-fledged database management  is outsourced to a third party . under the dssp model  application providers retain master copies of their data on their own systems  with the dssp only caching and serving read-only copies on their behalf. in our dssp approach  query execution on third party servers is not needed  so arbitrarily strong encryption of the remotely-cached data is possible. we contend that from a security and data integrity standpoint  the scalability provider model is more attractive than the do model in the case of web applications with read/write workloads  e.g.  e-commerce applications .
　other ongoing efforts to create dssp technology include the dbcache  1  1  and dbproxy  projects at ibm research  and the cacheportal project  at nec laboratories. to the best of our knowledge  no prior work has studied security issues in the dssp model  which is the focus of this paper.
1.1	view invalidation
　there has been prior work pertaining to invalidation of cached materialized views. choi and luo  proposed a technique that leverages statically-available query/update templates to speed up runtime invalidation decisions. candan et al.  introduced techniques for deciding whether to invalidate cached views in response to database updates. these techniques leverage  polling queries  to inspect portions of the database not available in the materialized view. the need to invalidate a view in response to a particular update can in some cases be ruled out by analysis of the view definition and update statements alone  without inspecting any other data. levy and sagiv  provide methods of ruling query statements  and hence view definitions  independent of updates in many practical cases  although the general query/update independence problem is undecidable. with our work  the focus is not on developing new strategies for deciding whether to invalidate cached views. rather  we develop a formal characterization of view invalidation strategies in terms of what data they access  as a basis for studying the tradeoff between data security and scalability.
1	our contributions
　we develop a formal characterization of view invalidation strategies in terms of what data they access  and use the formal characterization to cleanly formulate the securityscalability management problem. we then present a method for automatically identifying data that can be encrypted without reducing scalability at all. our method is based on static analysis of the data access templates of a given web application. it determines which query results  query statements  and update statements associated with the application can be encrypted without impacting scalability.
　our experiments over a prototype dssp system  detailed in section 1  show that several web applications can encrypt the majority of query results  as well as a substantial fraction of parameters to query and update statements  with no scalability penalty. furthermore  much of the data that is secured at no cost  falls into the moderately sensitive category. this type of data would not tend to be classified as compulsory for encryption  yet application designers may well choose to encrypt it  if armed with the knowledge that doing so does not impact scalability.
　our static analysis method enables a new scalability-conscious security design methodology that greatly simplifies the task of managing the security-scalability tradeoff: first  an administrator identifies highly-sensitive data  perhaps by applying a security law  and sets it aside for compulsory encryption. second  our static analysis method is invoked to determine which of the remaining data can be encrypted without impacting scalability. as a result  the administrator only needs to weigh the security-scalability tradeoff over the substantially reduced set of data items for which encryption may have scalability implications.
1	outline
　to underpin our study of the security-scalability tradeoff  we begin in section 1 by presenting our formal characterization of cache invalidation strategies  each of which represents a natural choice in the space of security-scalability options. section 1 describes our methodology for management of the tradeoff  while section 1 presents our main contribution: a static analysis method for determining which data can be encrypted without impacting scalability. in section 1 we present our empirical findings  which point to the effectiveness of our technique. we summarize in section 1.
1.	framework for studying the security-scalability tradeoff
　in this section we characterize when an update necessarily causes invalidation of the cached result of a query  as a function of the information that is accessible. this formal characterization underpins our study of the securityscalability tradeoff. we begin in section 1 by providing the details of our basic query and update model  and introducing the terminology and notation we use in the rest of the paper. then  in section 1 we characterize four distinct classes of invalidation strategies  i.e.  strategies for deciding when to invalidate a cached query result in response to an update  that differ in the amount of information available to them. finally  in section 1 we study the mixed invalidation strategies that arise when the information available for making invalidation decisions varies across queries and across updates.
1	query and update model
　the database components of a web application consist of a fixed set of query templates  and a fixed set of update templates  table 1 shows an example . let
and denote the set of query and update templates  respectively. a query q is composed of a query template qt to which parameters qp are attached at execution time. formally  q = qt qp . likewise  u = ut up . let q d  denote the result of evaluating query q over database d. let  d + u  denote the database state resulting from application of update u. a sequence of queries and updates issued at runtime constitutes a workload.
　based on our study of three benchmark applications  details in section 1   the query language is restricted to select-project-join  spj  queries having only conjunctive selection predicates  augmented with optional order-by and top-k constructs. spj queries are relational expressions constructed from any combination of project  select and join operations. as in previous work  1  1   the selection operations in the spj queries can only be arithmetic predicates having one of the five comparison operators {  ＋   − =}. the order-by construct affects tuple ordering in the result; and the top-k construct is equivalent to returning the first k tuples from the result of the query executed without the topk construct. we assume multi-set operation; the projection operation does not eliminate duplicates.
　the update language permits three kinds of updates: insertions  deletions and modifications. each insertion statement fully specifies a row of values to be added to some relation. each deletion statement specifies an arithmetic predicate over columns of a relation. rows satisfying the predicate are to be deleted. each modification statement modifies non-key attributes of the row  of a relation  that satisfies an equality predicate over the primary key of the relation.
1.1	assumptions for simplifying the presentation of our analysis
　to simplify the presentation of our analysis  section 1 and section 1  of which information can be encrypted without impacting scalability  we make three assumptions about the update and query templates: first  each selection predicate either compares attribute values across two relations or compares a value with a constant. second  no constants that might aid in invalidations are embedded in a query or update template. third  no queries compute cartesian products  i.e.  each query has a non-empty selection predicate. the above assumptions always hold for two of three benchmark applications we study  and are violated in less than 1% of the update/query template pairs for the third benchmark. whenever the assumptions do not hold  no encryption is recommended for the given update/query template pair. this conservative strategy ensures that our analysis never recommends encrypting any data  for which encryption impacts scalability.
　to simplify the presentation further  we make two additional assumptions about the execution of updates and queries: first  no query whose result is subject to invalidation by either an insertion or a deletion statement in the workload returns an empty result set. second  each update has some effect on the database  i.e.  for each update u 
 . in our experiments with all three of the benchmark applications we study  these assumptions always hold  and cause no loss of scalability.
1	formal characterization of
view invalidation strategies
　recall that in our current design  the dssp caches views  which are results of queries. a view invalidation strategy s is a function whose arguments possibly include an update statement  a query statement  and other information such as a cached query result. it evaluates to one of i  for  invalidate   or dni  for  do not invalidate  . a view invalidation

figure 1: relationships among classes of view invalidation strategies  in the general case.
strategy is correct if and only if whenever a view changes in response to an update  all corresponding cached instances of that view are invalidated. a formal definition of correctness is as follows:
correctness: a view invalidation strategy s is correct iff for any query q  database d  and update q d + u      s u q ...  = i .
 assume that updates are applied sequentially  and that all invalidations necessitated by one update are carried out before the next update is applied. 
　a view invalidation strategy is invoked whenever an update occurs. based on what information they access in making invalidation decisions  four classes of view invalidation strategies  one for each row of table 1  may be defined as follows:  a  blind strategy corresponding to the first row   b  template inspection strategy  tis  corresponding to the second row   c  statement-inspection strategy  sis  corresponding to the third row  and  d  viewinspection strategy  vis  corresponding to the last row. in each case  the strategy can only use the accessible information for making invalidation decisions. we provide a formal definition for each strategy in the extended technical report version of this paper .
　these four view invalidation strategies  natural points in the invalidation strategy design space  are largely based on previous work in the area of view invalidations. for example  the methods of  can be used to implement a viewinspection strategy. similarly  the methods of  can be used to implement a template- or a statement-inspection strategy. finally  implementing a blind strategy is simple: invalidate all cached query results on any update.
　also  every correct blind strategy is a correct templateinspection strategy  every correct template-inspection strategy is a correct statement-inspection strategy  and every correct statement-inspection strategy is a correct view-inspection strategy. the relationships are depicted in figure 1. we now define minimality:
minimality: a view invalidation strategy s belonging to class c is minimal if and only if it is correct and there exists no query statement q  update statement u  and database d such that s invalidates the view q d  in response to u  while another correct view invalidation strategy in class c does not. corresponding to each class of invalidation strategy  exposure levels: blind template stmt view

greater exposure  less encryption 

greater security
figure 1: security gradient.
the criterion for a minimal blind strategy  mbs   a minimal template-inspection strategy  mtis   a minimal statementinspection strategy  msis   and a minimal view-inspection strategy  mvis   can be arrived at  by applying the definition of minimality to the respective class.
　for arbitrary databases and workloads  no correct blind strategy is a minimal template-inspection strategy. similarly  no correct template-inspection strategy is a minimal statement-inspection strategy and no correct statement-inspection strategy is a minimal view-inspection strategy.  we omit formal proofs for brevity.  figure 1 depicts the relationships among classes of view invalidation strategies as a venn diagram.
　the choice of invalidation strategy determines what information can be encrypted. on the one extreme  if a viewinspection strategy is used  neither queries  nor updates  nor cached query results can be encrypted. on the other extreme  if a blind strategy is used  all queries  updates  and cached query results can be encrypted.1
1	mixed invalidation strategies
　typically  not all of an application's data is equally sensitive. an administrator may wish to control encryption of information at a per-template granularity. to control what information to encrypt  the administrator chooses an exposure level e ut  （ {blind  template stmt} for each update template ut （ ut  and an exposure level e qt  （
{blind template stmt view} for each query template qt （
q
t
 . each exposure level exposes some information of a query or an update; all information not exposed can then be encrypted. the blind exposure level exposes nothing; template exposes the template; stmt exposes the entire query or update statement  i.e.  template and parameters ; and view  only for query templates  exposes the query statement and the result of executing the query. figure 1 shows the range of exposure level options.
　figure 1 shows the possible exposure level combinations for a given ut/qt pair  the contents of the boxes may be ignored for now . when exposure level choices are made independently for every update and query template  the invalidation strategy to use may be determined at the granularity of update/query template pairs. in figure 1  the shaded boxes correspond to the four classes of invalidation strategies introduced in section 1.  we discuss the unshaded boxes shortly. 
query
blind template stmt viewblind template stmt111aijaijaij1aijbijcijfigure 1:	an invalidation probability matrix
ipm uit qtj  .
1.1	invalidation probabilities
　in our approach  exposure level choices determine the mix of invalidation strategies employed. given a workload  the invalidation strategy used for a given ut/qt pair in turn determines the invalidation probability-the likelihood that the invalidation strategy invalidates  the result of  an instance of the query template on seeing an instance of the update template  where probability distribution over template instances are derived from the workload . invalidation probabilities also depend on the database  and may change over time. in general it is difficult to estimate these  dynamic  quantities accurately  but as we will see we can find useful invariant relationships among them using static analysis alone. for the purpose of our static analysis  we represent the invalidation probabilities for different choices of exposure levels as a matrix. an invalidation probability matrix ipm  uit qtj    illustrated in figure 1  contains invalidation probability values for each combination of exposure levels for uit and qtj .  aij  bij  and cij are placeholders for invalidation probabilities that depend on workload and
database characteristics. 
ipm's obey the following properties:
property 1: the invalidation probability equals 1 if either exposure level is blind. clearly  whenever no information is available about either update u or query q  for correctness  the cached result of q must be invalidated whenever any update u occurs.
property 1: the invalidation probability is the same for all cases in which one exposure level is template and the other is some exposure level other than blind.  we denote this invalidation probability by aij （  1 .  recall from section 1.1 our assumptions that the selection predicates cannot compare two database values of the same relation and there are no constants in the update  query  templates. under these assumptions  knowledge of the query  update  parameters but not the update  query  parameters does not aid in reducing invalidations because the query  update  parameters cannot be compared to anything. similarly  knowledge of the query result but not the update parameters does not aid in reducing invalidations.  we omit formal proofs for brevity. 
property 1: the invalidation probabilities constitute a gradient as we move from top-left to bottom-right in figure 1  i.e.  1 − aij − bij − cij − 1. clearly  under minimal invalidation strategies  invalidations cannot increase if more information is available for making invalidation decisions.
　from the above discussion  it follows that invalidation strategy classes corresponding to unshaded boxes in figure 1 are of no interest since they are dominated by those corresponding to shaded boxes  i.e.  the shaded boxes permit lower exposure while offering the same invalidation probability. in certain instances  additional domination relationships can be found. first  for certain update/query template pairs uit/qtj   it can be shown that aij = 1  meaning minimal template inspection invalidation strategies are equivalent to minimal blind strategies for such update/query template pairs . similarly  in some cases bij = aij  meaning minimal statement inspection strategies are equivalent to minimal template inspection strategies for such update/query template pairs   and in some cases cij = bij  meaning minimal view inspection strategies are equivalent to minimal statement inspection strategies for such update/query template pairs . we examine how to identify and exploit such cases in section 1. before we approach this topic  we first describe our overall approach to managing the securityscalability tradeoff while meeting scalability requirements.
1.	overview of approach
　in this section we outline our approach for managing the security-scalability tradeoff  given scalability requirements. as figure 1 shows  one may control security by adjusting the exposure level of an application's update and query templates. we first provide our approach in section 1  and then present a brief example in section 1 that illustrates the approach.
1	our approach
　a natural approach to solve the security-scalability management problem is to model it as a constrained optimization problem where each potential solution  i.e.  an assignment of an exposure level to every template of the application  has an  overhead  and a  security  value; the objective is to maximize the  security  value while keeping the  overhead  below a given threshold. however  the approach is impractical because assigning meaningful security values to  and predicting overhead values of  each potential solution is virtually impossible.
　we advocate a new scalability-conscious security design methodology  which uses the following practical three-step approach for managing the security-scalability tradeoff  given a scalability requirement:
1. beginning with maximum exposure for all templates i.e.  exposure level stmt for each update template and exposure level view for each query template  reduce exposure levels  i.e.  move to the left in figure 1  based on cases in which data absolutely must be encrypted. such requirements may be decided in an ad-hoc manner  or based on a data privacy law such as .
1. using our static analysis techniques  described shortly  reduce exposure level of each template for which doing so does not impact scalability.
1. prioritize remaining exposure level reduction possibilities based on security considerations and adjust with respect to the tradeoff with scalability.
　step 1 is the focus of our work. we divide step 1 into two sub-steps:
step 1 a : characterize ipm domination relationships. determine for each uit/qtj pair whether  a  aij = 1   b  bij = aij  and  c  cij = bij. identifying these relationships is a challenge; section 1 is dedicated to this task.
step 1 b : eliminate high-exposure options whenever possible without hurting scalability. the inputs
select toy id from toys where toyname= select qty from toys where toyid= select cust name from customers  creditcard where cust id=cid and zip code= delete from toys where toyid= insert into creditcard  cid  number  zip code 
values          table 1: a more elaborate example toystore application having three query templates  two update templates  and three base relations: toys with attributes toy id  toy name  qty  customers with attributes cust id  cust name  and credit card with attributes cid  number  zip code. attribute credit card.cid is a foreign key into the customers relation. the question marks indicate parameters bound at execution time.
to this step include:  a  ipm tables with the information from ipm characterization  step 1a  plugged in  and  b  the initial exposure levels of templates based on requirements that certain data must absolutely be encrypted  step 1 . the goal of step 1b is to maximally reduce the exposure level for each template without impacting scalability. since scalability is impacted whenever invalidation probabilities change  the key idea in achieving maximal reduction of exposure levels is to ensure that the invalidation probability of no update/query template pair  as given by the ipm table  changes due to a reduction in the exposure level of a template.
　a simple greedy algorithm can be used for step 1b. it iterates over update and query templates  reducing template exposure levels when doing so incurs no increase in invalidation probability for any update/query template pair. the algorithm terminates when no further reduction is possible for any template. the order in which templates are considered does not affect the final outcome.
we next provide an example that illustrates our approach.
1	example
　consider the toystore application shown in table 1  an extension of our earlier simple-toystore application of table 1. as step 1  the administrator may well decide that credit card numbers are not to be exposed  and accordingly reduce the exposure level of template. using the notation introduced in section 1 template.
　the next step is step 1a  in which the ipm domination relationships are characterized. the results for the toystore application are provided in table 1. to understand intuitively how these relationships are determined  let us focus on the first row  i.e.  entries corresponding to. since no instance of  can affect the result of any instance of
  no instance of  will trigger invalidation of the result of any instance of qt1   so a1 = 1. however  since an instance of  can affect the result of an instance of 
or 1. as we show in section 1  whenever aij   1  aij = 1. hence  a1 = a1 = 1. further  using our analysis in section 1  it can be inferred that
b1 = a1  i.e.  knowledge of the parameters of  does not aid in reducing invalidations. also c1 = b1  i.e.  additional knowledge of the content of the result of an
a1 = 1a1 = 1a1 = 1b1 = a1b1   a1b1 = a1 i=1 c1   b1c1 = b1c1 = b1a1 = 1a1 = 1a1 = 1b1 = a1b1 = a1b1   a1 i=1 c1 = b1c1 = b1c1 = b1table 1: summary of ipm characterization for the example toystore application.
instance of   when the parameters of  and  are already known  does not aid in reducing invalidations. finally  since a1 = 1  a1 = b1 = c1 holds trivially due to property 1  section 1 .
   step 1b follows the ipm characterization step. when invoked on the toystore application  table 1  with inputs astemplate  step 1  and table 1  step 1a   the algorithm used for step 1b reduces exposure level of query template from view to template  and of query template  from view to stmt. by reducing the exposure level in this way  the inventory  quantity of toys in stock  and the customer demographic  customers in an area  are no longer exposed. an application provider may prefer not to expose this moderately sensitive information  all else being equal. further  we confirm that the additional security this reduction in exposure enables does not impact scalability. as before  cached results of instances ofare only invalidated by instances of if the toy id match  and cached results of all instances of are invalidated by any instance of . having presented our overall approach  we next describe how to determine ipm domination relationships using static analysis  step 1a .
1.	ipm characterization
　recall from section 1 that ipm characterization entails: determining statically for each uit/qtj pair  whether  a  aij = 1   b  bij = aij  and  c  cij = bij. we discuss in sections 1 - 1  how to determine for a given ut/qt pair whether each of these relationships holds. then  in section 1 we discuss how additional information  beyond those considered up to now  affect ipm values. but  first in section 1  we introduce some terminology for classifying query and update templates in a way that is useful for our analysis.
1	query and update classification
　define selection attributes of update template ut  denoted s ut   to be attributes used in any selection predicate  i.e.  a selection or a join condition  of ut.  if ut is an insertion  s ut  = {}.  further define modified attributes  m ut   of ut  selection attributes  s qt   of query template qt  and preserved attributes  p qt   of qt as in table 1. if ut is an insertion or a deletion  m ut  is defined to be the set of all attributes in the table in which the insertion or deletion takes place. for the toystore application  table 1   {toys.toy name {toys.toy id} 
s u1   = {toys.toy id}  m u1   = {toys.toyid  toys.toyname toys.qty}.
　recall from section 1 that queries are restricted to be select-project-join  spj  queries having conjunctive selection predicates  augmented with optional order-by  and topk constructs. further define two  possibly overlapping  classes
symbolmeanings ut attributes used in any of the selection predicates
 i.e.  selection and join conditions  of utm ut attributes modified by uts qt attributes used in selection predicates or order-by constructs of qtp qt attributes retained in the result of qttable 1: notation for aspects of templates.
qt
qt（ e （ nq is a query with only equality joins q is a spj query with
no top-k constructsut
ut
ut（ i
（ d
（ mu is an insertion
u is a deletion
u is a modificationqt is result-unhelpful
for ut   ut qt	 
s ut 	p qt  =		 （ h	”	{}
	table 1:	query and update classes.
of queries: ones with only equality joins or no joins  denoted e for equality   and ones with no top-k constructs  n . as before  there are three classes of updates: insertions  denoted i   deletions  d   and modifications  m . we say an update  query  template belongs to a particular update  query  class if any instance of the update  query  template belongs to the class.
　for our static analysis  it is important to know whether any instance of an update template can ever affect the result of any instance of a query template. following the terminology of   an update template ut is ignorable with respect to a query template qt if and only if no attributes modified by the update template are either preserved by the query template  or used in the selection predicate of the query template. let g denote the set of all such update/query template pairs  i.e.  
m ut ” p qt “s qt   = {}. for example  in the toystore application  table 1   update template u1t is ignorable with respect to query template.
　it is also important to know whether a query result has any information that aids in reducing invalidations. a query template qt is result-unhelpful with respect to an update template ut if and only if none of the selection attributes of the update template are preserved by the query template. let h denote the set of all such update/query template pairs  i.e.  . for example  in the toystore application  table 1   query template  is result-unhelpful for update template.
　in table 1  we summarize the different classes of templates and properties of update/query template pairs.
1	blind vs. template-inspection
 does aij = 1  
　begin by considering the case in which both update and query templates are exposed. if any instance of update template uit could cause invalidation of cached results of all possible instances of query template qtj   then aij = 1. hence  there is no advantage to using a minimal templateinspection strategy instead of a minimal blind strategy  i.e.  knowledge of the query or update templates does not aid in decreasing invalidations. for example  a1 equals 1 in the toystore application  table 1 .
　furthermore  if aij is greater than 1  then aij equals 1  i.e.  aij   1   aij = 1. the implication holds because the invalidation behavior of a template-inspection strategy is the same for all instances of an update/query template pair. so if there exists some instance of uit that causes invalidation of cached results of some instance of qtj   then
'any' instance of uit causes invalidation of cached results of 'all' instances of qtj . thus  aij either equals 1 or 1.
　lemma 1 provides the necessary and sufficient conditions for determining if aij equals 1.
　lemma 1. with assumptions as in section 1  invalidation probability aij equals 1 if and only if the update template uit is ignorable with respect to the query template qtj .
formally . otherwise  aij = 1.
proof: we omit a formal proof for brevity.
1	template-inspectionvs. statement-inspection
 does bij = aij  
　for a given update/query template pair  if whenever a minimal template-inspection strategy  mtis  evaluates to invalidate  denoted i   a minimal statement-inspection strategy  msis  also evaluates to i  then bij = aij  i.e.  knowledge of update and query parameters in addition to the update and query template does not aid in decreasing invalidations. since aij can take only two possible values  1 or 1  if bij = aij  then either bij = aij = 1 or bij = aij = 1.
　case 1  bij = aij = 1 : property 1  section 1  implies that the equality bij = aij = 1 holds if and only if aij = 1. furthermore  from lemma 1  we know the necessary and sufficient conditions for aij being 1. combining the two statements  bij = aij = 1 holds if and only if the update template is ignorable with respect to the query template 
i.e. .
　case 1  bij = aij = 1 : the equality aij = 1 is a necessary condition for bij = aij = 1. using lemma 1  the previous statement can be rewritten as: update template uit must not be ignorable with respect to query template qtj for the equality bij = aij = 1 to hold. this necessary condition for bij = aij = 1 is however not a sufficient condition since a msis also has knowledge of the parameters of the update and the query statement. this knowledge may allow the msis to infer that an instance of uit does not affect the cached query result of some instance of qtj . for example  a1 = 1 but b1   1 in the toystore application  table 1 .
　however  if s uit  ” s qtj   = {}  then knowing the parameters in addition to the update and query templates cannot aid in decreasing invalidations. hence a sufficient condition for bij = aij = 1 is: if no attribute is common to the selection predicates of both the update and query template  and the update template is not ignorable with respect to the query template  then bij = aij = 1  i.e. 
t	t	t	t
1 statement-inspection vs. view-inspection  does cij = bij  
　for a given update/query template  if whenever a minimal statement-inspection strategy  msis  evaluates to invalidate  denoted i   a minimal view-inspection strategy  mvis  also evaluates to i  then cij = bij  i.e.  knowledge of the query result in addition to the update and query statement does not aid in decreasing invalidations. from property 1  section 1   cij ＋ bij. in this subsection we provide several sufficient conditions for the equality cij = bij by iden-
tifying important classes of update/query pairs for which the equality holds. for other classes  we provide an example instance of uit and qtj for which cij   bij. next  we consider the three classes of updates in turn: insertions  deletions  and modifications.
insertions. this paragraph applies if the update is an insertion. if queries are limited to spj queries having conjunctive selection predicates  with equality as the join operator  augmented by optional order-by constructs  then whenever a msis evaluates to i  a mvis also evaluates to i  i.e.   uit （ i  …  qtj （ e ” n    cij = bij.  we provide the proof in .  this result is our most significant contribution in finding sufficient conditions for cij = bij. for example  c1 equals b1 for the toystore application  table 1   as predicted by this result. however  when the query template either has one or more of {  ＋    −} appearing in the join predicate  qtj （ e/    or has a top-k construct  qtj （ n/    cij may be less than bij  as illustrated when the update insert into toys  toyid  toy name  qty  values  1  'toyb'  1  is paired with either of the following queries:
a  select t1.toy id  t1.qty  t1.toy id  t1.qty
from toys as t1  toys as t1
where t1.toyname='toya' and t1.toy name='toyb'
and t1.qty   t1.qty
suppose the query result has just one tuple  1  1  1  1 . a minimal statement-inspection strategy will invalidate the cached query result  since a 'toya' with qty   1 might exist in the database. however  a minimal view-invalidation strategy  with the knowledge of the cached query result  which implies that there is no 'toya' with qty   1  will not invalidate the query result.
b  select max qty  from toys
suppose the result of this top-k query is 1. a minimal statement-inspection strategy will necessarily invalidate the cached query result  since the current max qty  might be less than 1. however  a minimal view-invalidation strategy  with the knowledge of the query result  will not invalidate the cached query result.
deletions. this paragraph applies if the update is a deletion. if the query template is result-unhelpful with respect to the update template  then whenever a msis evaluates to invalidate  i   a mvis also evaluates to i  i.e. 
h   cij = bij.  we provide the proof in .  for example  the equalities c1 = b1 and c1 = b1 hold for the toystore application  table 1   as predicted by this result. moreover  the pair of the toystore application is an example where the precondition of this result is not met and c1   b1.
modifications. this paragraph applies if the update is a modification. if either the update template is ignorable with respect to the query template or the query template is result-unhelpful with respect to the update template  then whenever a msis evaluates to invalidate  i   a mvis also evaluates to i  i.e. 	.  we provide the proof in . moreover  if the precondition of this result is not met  cij may be less than bij  as with the following update/query pair:
update toys set qty=1 where toyid=1
select toy id from toys where qty   1
let the toy with toyid=1 be absent from the cached query result. a minimal statement-inspection strategy will necessarily invalidate the cached query result  because the cached result could contain the toy with toyid = 1. a minimal view-inspection strategy will not invalidate it.
1	database integrity constraints
　so far the ipm values are based on the dssp's  optional  knowledge of the update statement  the query statement  and the query result. the dssp can further lower the values of the invalidation probabilities aij  bij  and cij  i.e.  increase the precision of invalidation decisions  by using database integrity constraints. database integrity constraints are conditions on the database that must be satisfied at all times  i.e.  all instances of the database must satisfy the constraints. we expect the dssp to know the basic database integrity constraints1  and thus use them for providing greater scalability to the applications. we list two such basic database integrity constraints below  and show  using the toystore application  table 1   how knowledge of the constraints can affect values of the ipm:
1. primary key constraint: consider the query tem-
plate . if toy id is the primary key of the toys relation  then the toys table cannot have more than one tuple with the same value of toy id. as a result  no insertion into the toys relation affects the cached query result of any instance of the query template.
1. foreign key constraint: consider the query template. we already assume that attribute cid of the creditcard relation is a foreign key into customers relation  i.e.  the value of the cid attribute for any tuple of the credit card relation should be the same as the value of the attribute cust id for some tuple in the customers relation. further  any insertion into the customers relation inserts a new cust id  which cannot join with any tuple in the credit card relation. as a result  no insertion into the customers relation affects the cached query result of any instance of.
　for any update/query template pair  if either of the two integrity constraints applies  aij becomes zero. furthermore  as property 1  section 1  implies  if aij = 1  then the equality aij = bij = cij = 1 holds.
1.	evaluation
　we have built a prototype dssp to gain a better understanding of the magnitude of the security-scalability tradeoff  and to see how well our scalability-conscious security design methodology works in practice. before presenting these results  we describe our benchmark applications in section 1  and our experimental methodology in section 1.
applicationnumber of ut/qt pairs for whicha = b =
= c = 1a = 1b   ab = ac   bc = bc   bc = bauction111bboard111bookstore111table 1: ipm characterization results for the three applications. the table entries denote the number of update/query template pairs for which particular ipm relationships hold.
then  in section 1 we confirm that blanket encryption of all data passing through the dssp greatly hurts scalability. finally  in section 1 we find that our scalability-conscious security design methodology enables significantly greater security without impacting scalability.
1	benchmark applications
　we sought web benchmarks that make extensive use of a the database and are representative of real-world applications. we found three publicly available benchmark applications that met these criteria: rubis   an auction system modeled after ebay.com  rubbos   a simple bulletin-board-like system inspired by slashdot.org  and tpc-w   a transactional e-commerce application that captures the behavior of clients accessing an online book store1. we used java implementation of these applications. we will henceforth refer to these applications as auction  bboard  and bookstore  respectively.
　the update/query templates of these applications differ from the assumptions outlined in section 1 in one significant way: between 1% and 1% of the query templates for each application have aggregation or group-by constructs. aggregation is one of min  max  count  sum  avg  and group-by allows application of aggregation functions to tuples clustered by some attribute. our current model does not handle aggregation and group-by queries. for our evaluation  we separately consider each update/query template pair  where the query has an aggregation or group-by construct  and manually determine the behavior of each of the four classes of minimal invalidation strategies of section 1.
1.1	ipm characterization results
　table 1 summarizes the ipm characterization results for the three applications  assuming the dssp has knowledge of the two types of database integrity constraints mentioned in section 1. each row of table 1 corresponds to an application. the table entries denote the number of update/query template pairs for which particular ipm relationships hold. the first column lists the number of update/query template
 ut/qt  pairs for which the equality a = b = c = 1 holds. for each application  the majority of ut/qt pairs fall in this category. for the remaining ut/qt pairs  invalidation probability a equals 1. these ut/qt pairs are further divided into four categories  represented by the next four columns of table 1  depending on whether b   a or b = a  and whether c   b or c = b. as table 1 shows  equalities b = a and/or c = b hold for the majority of the template pairs. accordingly  for these template pairs  reducing the exposure of templates does not increase invalidations. thus  the analysis presented in section 1 applies to the applications we studied.
1	experimental methodology
　we performed our experiments with a simple two-node configuration-a home server that runs mysql1  as its database management system  and a dssp node that provides answers to database queries using its store of the cached query results  running on emulab .  to keep the configuration simple  the dssp node also provided the functionality of a cdn node  i.e.  the ability to run web applications and to interact with a user running a web browser. we used tomcat  to provide both functionalities.  cached query results were kept consistent with the home server's database using non-transactional invalidation of cached query results.
　the home server machine had an intel p-iii 1 mhz processor with 1 mb of memory  while the dssp node had an intel 1-bit xeon processor with 1 mb of memory. in all experiments  the home server and dssp node were connected by a high latency  low bandwidth duplex link  1 ms latency  1 mbps . each client was connected to the dssp node by a low latency  high bandwidth duplex link  1 ms latency  1 mbps . these network settings model a deployment in which a dssp node  because there are many of them  is  close  to the clients  most of which are  far  from any single home server.
　since the overhead for emulating clients is low  one additional emulab node was used to emulate all clients. as in the tpc-w  specification  clients simulate human usage patterns by issuing an http request  waiting for the response  and pausing for a think time of x seconds before requesting another web page-x is drawn from a negative exponential distribution with a mean of seven seconds.
　each experiment ran for ten minutes  and the dssp node started with a cold cache each time. the database configuration parameters we used in our experiments and sample update numbers for a ten minute run are listed in . scalability was measured as the maximum number of users that could be supported while keeping the response time below two seconds for 1% of the http requests.
1	magnitude of security-scalability tradeoff
query templates:
	 1	 1	 1
　　　　　　　　query templates update templates:
	 1	 1	 1	 1
	update templates	update templates
	 a  auction	 b  bboard	 c  bookstore
figure 1:	starting with the california data privacy law  additional exposure reduction for query and update　figure 1 plots the scalability of an application as a function of the invalidation strategy used by the dssp  for all three applications. the y-axis plots scalability  measured as specified in section 1. on the x-axis  we consider an instance of each of the four classes of invalidation strategies introduced in section 1.  the same invalidation strategy is used for all update/query template pairs.  for the bboard application  in which each http request results in about ten database requests  with the poor cache behavior of a blind or a template inspection strategy  not even a small templates.

figure 1: tradeoff between security and scalability  as a function of coarse-grain invalidation strategy.
number of clients can be supported within the response time threshold specified in section 1.
　for each application  the leftmost strategy  a minimal view inspection strategy  mvis   offers the best scalability  but the worst security  full exposure of all data . on the other extreme  the rightmost strategy  a minimal blind strategy  mbs   offers the best security  full encryption of all data   but the worst scalability. figure 1 confirms the claim made in section 1 that blanket encryption of all data  thereby requiring a blind invalidation strategy  significantly hinders scalability.
1	security enhancement achieved
　in this section we show that for all three applications  the static analysis step of our scalability-conscious security design methodology enables significantly greater security without impacting scalability. recall figure 1 of section 1  which plots scalability1 versus security  for a simple metric of security that counts the number of query templates for which results can be encrypted. our static analysis identifies 1 out of the 1 query templates associated with the bookstore application  for which encrypting the results has no impact on scalability. while encouraging  that result does not tell the whole story. here we examine in greater depth the degree of security afforded by our static analysis.
　as discussed in section 1  the outcome of our static analysis  step 1  depends on the initial determination of what highly sensitive data absolutely must be encrypted  step 1 . to make this determination  we defer to the well-known california data privacy law   which  when applied to our applications  mandates securing all credit card information.
　figure 1 plots the exposure levels of query and update templates both before and after our static analysis is invoked. the top three graphs correspond to the query templates of each application  and the bottom three graphs correspond to the update templates. the y-axis of each graph plots the possible exposure levels for a template  low exposure on the bottom; high exposure on top . the x-axis plots the query or update templates associated with an application  in increasing order of exposure. the dashed lines show the initial exposure levels mandated by the california data privacy law  only a little encryption is needed to comply ; the solid lines show the final exposure levels resulting from the application of our static analysis. the area between the lines gives an idea of the reduction in exposure achieved using our approach.
　much of the data whose exposure level can be reduced due to our static analysis turns out to be moderately sensitive  and therefore the reduction in exposure would likely be a welcome security enhancement. to illustrate  we supply examples of moderately sensitive data that can be encrypted:
  auction application: the historical record of user bids  i.e.  user a bid b dollars on item c at time d .
  bboard application: the ratings users give one another based on the quality of their postings  i.e.  user a gave user b a rating of c .
  bookstore application: book purchase association rules discovered by the vendor  i.e.  customers who purchase book a often also purchase book b .
in all cases scalability is not affected-it remains the same as that of mvis in figure 1.
1.	summary
　in this paper we explored ways to secure the data of web applications that use the services of a shared dssp to meet their database scalability needs. at the heart of the problem is the tradeoff between security and scalability that occurs in this framework. when updates occur  the dssp needs to invalidate data from its cache. the amount of data invalidated varies depending on the information exposed to the dssp. the less information exposed to the dssp  the more invalidations required  and the lower the scalability.
　we presented a convenient shortcut to manage the securityscalability tradeoff. our solution is to  statically  determine which data can be encrypted without any impact on scalability. we confirmed the effectiveness of our static analysis method  by applying it to three realistic benchmark applications that use a prototype dssp system we built. in all three cases  our static analysis identified significant portions of data that could be secured without impacting scalability. the security-scalability tradeoff did not need to be considered for such data  significantly lightening the burden on the application administrator managing the tradeoff.
acknowledgments
we thank mukesh agrawal  charles garrod  phillip b. gibbons  bradley milan  and haifeng yu for their valuable feedback and suggestions.
