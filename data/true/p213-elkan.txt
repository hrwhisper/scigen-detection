the input to an algorithm that learns a binary classifier normally consists of two sets of examples  where one set consists of positive examples of the concept to be learned  and the other set consists of negative examples. however  it is often the case that the available training data are an incomplete set of positive examples  and a set of unlabeled examples  some of which are positive and some of which are negative. the problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature.
　under the assumption that the labeled examples are selected randomly from the positive examples  we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. we show how to use this result in two different ways to learn a classifier from a nontraditional training set. we then apply these two new methods to solve a real-world problem: identifying protein records that should be included in an incomplete specialized molecular biology database. our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased svm method for learning from positive and unlabeled examples.
categories and subject descriptors
h.1  database management : database applications- data mining.
general terms
algorithms  theory.
keywords
supervised learning  unlabeled examples  text mining  bioinformatics.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm 1-1-1/1 ...$1.
1. introduction
　the input to an algorithm that learns a binary classifier consists normally of two sets of examples. one set is positive examples x such that the label y = 1  and the other set is negative examples x such that y = 1. however  suppose the available input consists of just an incomplete set of positive examples  and a set of unlabeled examples  some of which are positive and some of which are negative. the problem we solve in this paper is how to learn a traditional binary classifier given a nontraditional training set of this nature.
　learning a classifier from positive and unlabeled data  as opposed to from positive and negative data  is a problem of great importance. most research on training classifiers  in data mining and in machine learning assumes the availability of explicit negative examples. however  in many real-world domains  the concept of a negative example is not natural. for example  over 1 specialized databases exist in molecular biology . each of these defines a set of positive examples  namely the set of genes or proteins included in the database. in each case  it would be useful to learn a classifier that can recognize additional genes or proteins that should be included. but in each case  the database does not contain any explicit set of examples that should not be included  and it is unnatural to ask a human expert to identify such a set. consider the database that we are associated with  which is called tcdb . this database contains information about over 1 proteins that are involved in signaling across cellular membranes. if we ask a biologist for examples of proteins that are not involved in this process  the only answer is  all other proteins.  to make this answer operational  we could take all proteins mentioned in a comprehensive unspecialized database such as swissprot . but these proteins are unlabeled examples  not negative examples  because some of them are proteins that should be in tcdb. our goal is precisely to discover these proteins.
　this paper is organized as follows. first  section 1 formalizes the scenario of learning from positive and unlabeled examples  presents a central result concerning this scenario  and explains how to use it to make learning from positive and unlabeled examples essentially equivalent to learning from positive and negative examples. next  section 1 derives how to use the same central result to assign weights to unlabeled examples in a principled way  as a second method of learning using unlabeled examples. then  section 1 describes a synthetic example that illustrates the results of section 1. section 1 explains the design and findings of an experiment showing that our two new methods perform better than the best previously suggested method for learning from positive and unlabeled data. finally  section 1 summarizes previous work in the same area  and section 1 summarizes our findings.
1. learning a traditional classifier from nontraditional input
　let x be an example and let y （ {1} be a binary label. let s = 1 if the example x is labeled  and let s = 1 if x is unlabeled. only positive examples are labeled  so y = 1 is certain when s = 1  but when s = 1  then either y = 1 or y = 1 may be true.
　formally  we view x  y  and s as random variables. there is some fixed unknown overall distribution p x y s  over triples hx y si. a nontraditional training set is a sample drawn from this distribution that consists of unlabeled examples hx s = 1i and labeled examples hx s = 1i. the fact that only positive examples are labeled can be stated formally as the equation
	p s = 1|x y = 1  = 1.	 1 
in words  the probability that an example x appears in the labeled set is zero if y = 1.
　there is a subtle but important difference between the scenario considered here  and the scenario considered in . the scenario here is that the training data are drawn randomly from p x y s   but for each tuple hx y si that is drawn  only hx si is recorded. the scenario of  is that two training sets are drawn independently from p x y s . from the first set all x such that s = 1 are recorded; these are called  cases  or  presences.  from the second set all x are recorded; these are called the background sample  or contaminated controls  or pseudo-absences.
　the single-training-set scenario considered here provides strictly more information than the case-control scenario. obviously  both scenarios allow p x  to be estimated. however  the first scenario also allows the constant p s = 1  to be estimated in an obvious way  while the case-control scenario does not. this difference turns out to be crucial: it is possible to estimate p y = 1  only in the first scenario.
　the goal is to learn a function f x  such that f x  = p y = 1|x  as closely as possible. we call such a function f a traditional probabilistic classifier. without some assumption about which positive examples are labeled  it is impossible to make progress towards this goal. our basic assumption is the same as in previous research: that the labeled positive examples are chosen completely randomly from all positive examples. what this means is that if y = 1  the probability that a positive example is labeled is the same constant regardless of x. we call this assumption the  selected completely at random  assumption. stated formally  the assumption is that
	p s = 1|x y = 1  = p s = 1|y = 1 .	 1 
here  c = p s = 1|y = 1  is the constant probability that a positive example is labeled. this  selected completely at random assumption is analogous to the missing completely at random  assumption that is often made when learning from data with missing values  1  1  1 . another way of stating the assumption is that s and x are conditionally independent given y.
　so  a training set is a random sample from a distribution p x y s  that satisfies equations  1  and  1 . such a training set consists of two subsets  called the  labeled   s = 1  and  unlabeled   s = 1  sets. suppose we provide these two sets as inputs to a standard training algorithm. this algorithm will yield a function g x  such that g x  = p s = 1|x  approximately. we call g x  a nontraditional classifier. our central result is the following lemma that shows how to obtain a traditional classifier f x  from g x .
lemma 1: suppose the  selected completely at random  assumption holds. then p y = 1|x  = p s = 1|x /c where c = p s = 1|y = 1 .
proof: remember that the assumption is p s = 1|y = 1 x  = p s = 1|y = 1 . now consider p s = 1|x . we have that
p s = 1|x =p y = 1 … s = 1|x =p y = 1|x p s = 1|y = 1 x =p y = 1|x p s = 1|y = 1 .the result follows by dividing each side by p s = 1|y = 1 .
　although the proof above is simple  the result has not been published before  and it is not obvious. the reason perhaps that the result is novel is that although the learning scenario has been discussed in many previous papers  including  1  1  1  1  1   and these papers do make the selected completely at random  assumption either explicitly or implicitly  the scenario has not previously been formalized using a random variable s to represent the fact of an example being selected.
　several consequences of the lemma are worth noting. first  f is an increasing function of g. this means that if the classifier f is only used to rank examples x according to the chance that they belong to class y = 1  then the classifier g can be used directly instead of f.
　second  f = g/p s = 1|y = 1  is a well-defined probability f ＋ 1 only if g ＋ p s = 1|y = 1 . what this says is that g   p s = 1|y = 1  is impossible. this is reasonable because the  positive   labeled  and  negative   unlabeled  training sets for g are samples from overlapping regions in x space. hence it is impossible for any example x to belong to the  positive  class for g with a high degree of certainty.
　the value of the constant c = p s = 1|y = 1  can be estimated using a trained classifier g and a validation set of examples. let v be such a validation set that is drawn from the overall distribution p x y s  in the same manner as the nontraditional training set. let p be the subset of examples in v that are labeled  and hence positive . the estimator of p s = 1|y = 1  is the average value of g x  for x in p. formally the estimator is e1 = n1 px（p g x  where n is the cardinality of p.
　we shall show that e1 = p s = 1|y = 1  = c if it is the case that g x  = p s = 1|x  for all x. to do this  all we need to show is that g x  = c for x （ p. we can show this as follows:
g x =p s = 1|x =p s = 1|x y = 1 p y = 1|x 
+ p s = 1|x y = 1 p y = 1|x =p s = 1|x y = 1  ， 1 + 1 ， 1 since x （ p=p s = 1|y = 1 .a second estimator of c is e1 =	x（p g x /	x（v g x .	p	p
this estimator is almost equivalent to e1  because
e x g x   = p s = 1  ， m = e n|m 
x（v
where m is the cardinality of v .
　a third estimator of c is e1 = maxx（v g x . this estimator is based on the fact that g x  ＋ c for all x.
　which of these three estimators is best  the first estimator is exactly correct if g x  = p s = 1|x  precisely for all x  but of course this condition never holds in practice. we can have g x  1= p s = 1|x  for two reasons: because g is learned from a random finite training set  and/or because the family of models from which g x  is selected does not include the true model. for example  if the distributions of x given y = 1 and x given y = 1 are gaussian with different covariances  as in section 1 below  then logistic regression can model p y = 1|x  and p s = 1|x  approximately  but not exactly. in the terminology of statistics  logistic regression is mis-specified in this case.
　in practice  when g x  1= p s = 1|x   the first estimator is still the best one to use. compared to the third estimator  it will have much lower variance because it is based on averaging over m examples instead of on just one example. it will have slightly lower variance than the second estimator because the latter is exposed to additional variance via its denominator. note that in principle any single example from p is sufficient to determine c  but that in practice averaging over all members of p is preferable.
1. weightingunlabeled examples
　there is an alternative way of using lemma 1. let the goal be to estimate ep x y s  h x y   for any function h  where p x y s  is the overall distribution. to make notation more concise  write this as e h . we want an estimator of e h  based on a nontraditional training set of examples of the form hx si.
clearly p y = 1|x s = 1  = 1. less obviously 
p s = 1|x y = 1 p y = 1|x 
p y = 1|x s = 1 	=	p s = 1|x 
 1   p s = 1|x y = 1  p y = 1|x 
	=	
1   p s = 1|x 
 1   c p y = 1|x 
	=	
1   p s = 1|x 
 1   c p s = 1|x /c
	=	
1   p s = 1|x 
	1   c	p s = 1|x 
	=	.
by definition	c	1   p s = 1|x e h 	= z
x y sh x y p x y s 
	1	=	 p x xp s|x xp y|x s h x y 
	x	s=1	y=1
= z p x  p s = 1|x h x 1 
x
+ p s = 1|x  p y = 1|x s = 1 h x 1  + p y = 1|x s = 1 h x 1   .
the plugin estimate of e h  is then the empirical average
1
m   x	x	  h x 1  +	w x h x 1  +  1   w x  h x 1  hx s=1i	hx s=1i
where
		 1 
and m is the cardinality of the training set. what this says is that each labeled example is treated as a positive example with unit weight  while each unlabeled example is treated as a combination of a positive example with weight p y = 1|x s = 1  and a negative example with complementary weight 1 p y = 1|x s = 1 . the probability p s = 1|x  is estimated as g x  where g is the nontraditional classifier explained in the previous section.
　there are two ways in which the result above on estimating e h  can be used to modify a learning algorithm in order to make it work with positive and unlabeled training data. the first method is to express the learning algorithm so that it uses the training set only via the computation of averages  and then to use the result above to estimate each of these averages. the second method is to modify the learning algorithm so that training examples have individual weights. then  positive examples are given unit weight and unlabeled examples are duplicated; one copy of each unlabeled example is made positive with weight p y = 1|x s = 1  and the other copy is made negative with weight 1   p y = 1|x s = 1 . this second method is the one used in experiments below.
　as a special case of the result above  consider h x y  = y. we obtain
	e y 	=	p y = 1 
1
=	  x 1 + x w x 1 +  1   w x  1  m
	hx s=1i	hx s=1i
1
=	 n + x w x  	 1  m
                             hx s=1i where n is the cardinality of the labeled training set  and the sum is over the unlabeled training set. this result solves an open problem identified in   namely how to estimate p y = 1  given only the type of nontraditional training set considered here and the selected completely at random assumption.
　there is an alternative way to estimate p y = 1 . by definition

so
.
the obvious estimator of p s = 1 … y = 1  is n/m  which yields the estimator
	n	n	n1
= m px（p g x 	mpx（p g x 
for p y = 1 . note that both this estimator and  1  are greater than n/m  as is expected for p y = 1 .

figure 1: data points lie in two dimensions. blue pluses are positive examples  while red circles are negative examples. the large ellipse is the result of logistic regression trained on all the data. it shows the set of points x for which p y|x  = 1 is estimated. the small ellipse is the result of logistic regression trained on positive labeled data versus all other data  then transformed following lemma 1. the two ellipses represent similar classifiers in practice since they agree closely everywhere that data points of both classes have high　the expectation e y  = p y = 1  is the prevalence of y among all x  both labeled and unlabeled. the reasoning above shows how to estimate this prevalence assumdensity.
ing the single-training-set scenario. in the case-control scenario  where labeled training examples are obtained separately from unlabeled training examples  it can be proved that p y = 1  cannot be identified .
　the arguments of this section and the preceding one are derivations about probabilities  so they are only applicable to the outputs of an actual classifier if that classifier produces correct probabilities as its output. a classifier that produces approximately correct probabilities is called wellcalibrated. some learning methods  in particular logistic regression  do give well-calibrated classifiers  even in the presence of mis-specification and finite training sets. however  many other methods  in particular naive bayes  decision trees  and support vector machines  svms   do not. fortunately  the outputs of these other methods can typically be postprocessed into calibrated probabilities.
　the two most common postprocessing methods for calibration are isotonic regression   and fitting a one-dimensional logistic regression function . we apply the latter method  which is often called platt scaling  to svm classifiers in section 1 below.
1. an illustration
　to illustrate the method proposed in section 1 above  we generate 1 positive data points and 1 negative data points  each from a two-dimensional gaussian as shown in figure 1. we then train two classifiers: one using all the data  and one using 1% of the positive data as labeled positive examples  versus all other data as negative examples.
　figure 1 shows the ideal trained classifier as a large ellipse. each point on this ellipse has predicted probability 1 of belonging to the positive class. the transformed nontraditional classifier is the small ellipse; the transformation following lemma 1 uses the estimate e1 of p s = 1|y = 1 . based on a validation set of just 1 labeled examples  this estimated value is e1 = 1  which is very close to the true value 1. although the two ellipses are visually different  they correspond closely in the area where both positive and negative data points have high density  so they represent similar classifiers for this application.
　given a data point hx1 x1i  both classifiers use the representationas input in order to allow the contours p y = 1|x  = 1 to be quadratic sections  as they are in figure 1. expanding the input representation in this way is similar to using a nonlinear kernel with a support vector machine. because the product x1 is not part of the input representation  the ellipses are constrained to be axisparallel. logistic regression with this input representation is therefore mis-specified  i.e. not capable of representing exactly the distributions p y = 1|x  and p s = 1|x . analogous mis-specification is likely to occur in real-world domains.
1. application to real-world data
　one common real-world application of learning from positive and unlabeled data is in document classification. here we describe experiments that use documents that are records from the swissprot database.
　we call the set of positive examples p. this set consists of 1 records obtained from a specialized database named tcdb . the set of unlabeled examples u consists of 1 records selected randomly from swissprot excluding its intersection with tcdb  so u and p are disjoint. domain knowledge suggests that perhaps about 1% of the records in u are actually positive.
　this dataset is useful for evaluating methods for learning from positive and unlabeled examples because in previous work we did in fact manually identify the subset of actual positive examples inside u; call this subset q. the procedure used to identify q  which has 1 members  is explained in . let n = u q so the cardinality of n is 1. the three sets of records n  p  and q are available at www.cs.ucsd.edu/users/elkan/posonly.
　the p and u datasets were obtained separately  and u is a sample from the whole population  as opposed to p “ u. hence this experiment is an example of the case-control scenario explained in section 1  not of the single-training-set scenario. however  we can still apply the methods suggested in that section and evaluate their success. when applied in practice  p “ u will be all of swissprot  so the scenario will be the single-training-set one.
　our experiments compare four approaches:  i  standard learning from p “ q versus n   ii  learning from p versus u with adjustment of output probabilities   iii  learning from p and u after double weighting of u  and  iv  the biased svm method from  explained in section 1 below. approach  i  is the baseline  that is the ideal: learning a standard classifier from fully labeled positive and negative training sets. we expect its accuracy to be the highest; we hope that approaches  ii  and  iii  can achieve almost as good accuracy. approach  iv  is the most successful method described in previous research. to make comparisons fair  all four methods are based on soft-margin svms with linear kernels.
　each of the four methods yields a classifier that assigns numerical scores to test examples  so for each method we can plot its receiver operating characteristic  roc  curve. each method also has a natural threshold that can be used to convert numerical scores into yes/no predictions for test examples. for methods  i  and  iii  this natural threshold is 1  since these methods yield scores that are well-calibrated probabilities. for method  iv  the natural threshold is zero  since this method is an svm. for method  ii  the natural threshold is 1c where c = p s = 1|y = 1 . as in section 1 above  we use the estimator e1 for c.
　for each of the four methods  we do cross-validation with ten folds  and obtain a combined confusion matrix from the ten testing subsets. with one confusion matrix for each approach  we compute its recall and precision. all these numbers are expected to be somewhere around 1%. crossvalidation proceeds as follows. partition p  q  and n randomly into ten subsets each of size as equal as possible. for example q will have eight subsets of size 1 and two of size 1. for each of the ten trials  reserve one subset of p  q  and n for testing. use the other nine subsets of each for training. in every trial  each approach gives one final classifier. in trial number i  this classifier is applied to the testing subsets pi  qi  and ni  yielding a confusion matrix of the following form:
positivenegativepredictedpi “ qiaibi   ni actualcidithe combined confusion matrix reported for each approach has entriesetc. in this matrix a is the number of true positives  b is the number of false negatives  c is the number of false positives  and d is the number of true negatives. finally  precision is defined as p = a/ a + c  and recall as r = a/ a + b .
　the basic learning algorithm for each method is an svm with a linear kernel as implemented in libsvm . for approach  ii  we use platt scaling to get probability estimates which are then adjusted using lemma 1. for approach  iii  we run libsvm twice. the first run uses platt scaling to get probability estimates  which are then converted into weights following equation  1  at the end of section 1. next  these weights are used for the second run. although the official version of libsvm allows examples to be weighted  it requires all examples in one class to have the same weight. we use the modified version of libsvm by ming-wei chang and hsuan-tien lin  available at www.csie.ntu.edu.tw/゛cjlin/libsvmtools/#1  that allows different weights for different examples.
　for approach  iv  we run libsvm many times using varying weights for the negative and unlabeled examples. the details of this method are summarized in section 1 below and are the same as in the paper that proposed the method originally . with this approach different weights for different examples are not needed  but a validation set to choose the best settings is needed. given that the training set in each trial consists of 1% of the data  a reasonable choice is to use 1% of the data for training and 1% for validation in each trial. after the best settings are chosen in each trial  libsvm is rerun using all 1% for training  for that trial. note that different settings may be chosen in each of the ten trials.
　the results of running these experiments are shown in table 1. accuracies and f1 scores are calculated by thresholding trained models as described above  while areas under the roc curve do not depend on a specific threshold.  relative time  is the number of times an svm must be trained for one fold of cross-validation.

false positive rate  1 negative examples 
figure 1: results for the experiment described in section 1. the four roc curves are produced by four different methods.  i  training on p “ q versus n; the point on the line uses the threshold p y = 1|x  = 1.  ii  training on p versus u; the point shown uses the threshold p s = 1|x /e1 = 1 where e1 is an estimate of p s = 1|y = 1 .  iii  training on p versus u  where each example in u is labeled positive with a weight of p y = 1|x z = 1  and also labeled negative with complementary weight.  iv  biased svm training  with penalties cu and cp chosen using validation sets. note that  in order to show the differences between methods better  only the important part of the roc space is shown.　as expected  training on p “q versus n  method  i   performs the best  measured both by f1 and by area under the roc curve. however  this is the ideal method that requires knowledge of all true labels. our two methods that use only positive and unlabeled examples   ii  and  iii   perform better than the current state-of-the-art  which is method  iv . although methods  ii  and  iv  yield almost the same area under the roc curve  figure 1 shows that the roc curve for method  ii  is better in the region that is important from an application perspective. method  iv  is the slowest by far because it requires exhaustive search for good algorithm settings. since methods  ii  and  iii  are mathematically well-principled  no search for algorithm settings is needed.
　the new methods  ii  and  iii  do require estimating c = p s = 1|y = 1 . we do this with the e1 estimator described in section 1. because of cross-validation  a different estimate e1 is computed for each fold. all these estimates are within 1% of the best estimate we can make using knowledge of
the true labels  which is
　consider the roc curves in figure 1; note that only part of the roc space is shown in this figure. each of the four curves is the result of one of the four methods. the points highlighted on the curves show the false positive/true positive trade-off at the thresholds described above. while the differences in the roc curves may seem small visually  they represent a substantial practical difference. suppose that a human expert will tolerate 1 negative records. this is represented by the black line in figure 1. then the expert will miss 1% of positive records using the biased svm  but only 1% using the reweighting method  which is a 1% reduction in error rate  and a difference of 1 positive records in this case.
1. related work
table 1: measures of performance for each of four methods.
methodaccuracyf1 scorearea under roc curverelative time i  ideal: training on p “ q versus n1111 ii  training on p versus u1111 iii  training on p versus weighted u1111 iv  biased svm1111　several dozen papers have been published on the topic of learning a classifier from only positive and unlabeled training examples. two general approaches have been proposed previously. the more common approach is  i  to use heuristics to identify unlabeled examples that are likely to be negative  and then  ii  to apply a standard learning method to these examples and the positive examples; steps  i  and  ii  may be iterated. papers using this general approach include  1  1  1   and the idea has been rediscovered independently a few times  most recently in  1  section 1 . the approach is sometimes extended to identify also additional positive examples in the unlabeled set . the less common approach is to assign weights somehow to the unlabeled examples  and then to train a classifier with the unlabeled examples interpreted as weighted negative examples. this approach is used for example by  1  1 .
　the first approach can be viewed as a special case of the second approach  where each weight is either 1 or 1. the second approach is similar to the method we suggest in section 1 above  with three important differences. first  we view each unlabeled example as being both a weighted negative example and a weighted positive example. second  we provide a principled way of choosing weights  unlike previous papers. third  we assign different weights to different unlabeled examples  whereas previous work assigns the same weight to every unlabeled example.
　a good paper that evaluates both traditional approaches  using soft-margin svms as the underlying classifiers  is . the finding of that paper is that the approach of heuristically identifying likely negative examples is inferior. the weighting approach that is superior solves the following svm optimization problem:
　　　　　minimize subject to yi w ， x + b  − 1   zi and zi − 1 for all i.
here p is the set of labeled positive training examples and u is the set of unlabeled training examples. for each example i the hinge loss is zi. in order to make losses on p be penalized more heavily than losses on u  cp   cu. no direct method is suggested for setting the constants cp and cu. instead  a validation set is used to select empirically the best values of cp and cu from the ranges cu = 1 1  1  ... 1 and cp/cu = 1 1 ... 1. this method  called biased svm  is the current state-of-the-art for learning from only positive and unlabeled documents. our results in the previous section show that the two methods we propose are both superior.
　the assumption on which the results of this paper depend is that the positive examples in the set p are selected completely at random. this assumption was first made explicit by  and has been used in several papers since  including . most algorithms based on this assumption need p y = 1  to be an additional input piece of information; a recent paper emphasizes the importance of p y = 1  for learning from positive and unlabeled data . in section 1 above  we show how to estimate p y = 1  empirically.
the most similar previous work to ours is .	their approach also makes the  selected completely at random  assumption and also learns a classifier directly from the positive and unlabeled sets  then transforms its output following a lemma that they prove. two important differences are that they assume the outputs of classifiers are binary as opposed to being estimated probabilities  and they do not suggest a method to estimate p s = 1|y = 1  or p y = 1 . hence the algorithm they propose for practical use uses a validation set to search for a weighting factor  like the weighting methods of  1  1   and like the biased svm approach . our proposed methods are orders of magnitude faster because correct weighting factors are computed directly.
　the task of learning from positive and unlabeled examples can also be addressed by ignoring the unlabeled examples  and learning only from the labeled positive examples. intuitively  this type of approach is inferior because it ignores useful information that is present in the unlabeled examples. there are two main approaches of this type. the first approach is to do probability density estimation  but this is well-known to be a very difficult task for high-dimensional data. the second approach is to use a so-called one-class svm  1  1 . the aim of these methods is to model a region that contains most of the available positive examples. unfortunately  the outcome of these methods is sensitive to the values chosen for tuning parameters  and no good way is known to set these values . moreover  the biased svm method has been reported to do better experimentally .
1. conclusions
　the central contribution of this paper is lemma 1  which shows that if positive training examples are labeled at random  then the conditional probabilities produced by a model trained on the labeled and unlabeled examples differ by only a constant factor from the conditional probabilities produced by a model trained on fully labeled positive and negative examples.
　following up on lemma 1  we show how to use it in two different ways to learn a classifier using only positive and unlabeled training data. we apply both methods to an important biomedical classification task whose purpose is to find new data instances that are relevant to a real-world molecular biology database. experimentally  both methods lead to classifiers that are both hundreds of times faster and more accurate than the current state-of-the-art svm-based method. these findings hold for four different definitions of accuracy: area under the roc curve  f1 score or error rate using natural thresholds for yes/no classification  and recall at a fixed false positive rate that makes sense for a human expert in the application domain.
1. acknowledgments
　this research is funded by nih grant gm1. tingfan wu provided valuable advice on using libsvm.
