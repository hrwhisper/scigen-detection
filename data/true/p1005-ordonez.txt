multidimensional statistical models are generally computed outside a relational dbms  exporting data sets. this article explains how fundamental multidimensional statistical models are computed inside the dbms in a single table scan exploiting sql and user-defined functions  udfs . the techniques described herein are used in a commercial data mining tool  called teradata warehouse miner. specifically  we explain how correlation  linear regression  pca and clustering  are integrated into the teradata dbms. two major database processing tasks are discussed: building a model and scoring a data set based on a model. to build a model two summary matrices are shown to be common and essential for all linear models: the linear sum of points and the quadratic sum of cross-products of points. since such matrices are generally significantly smaller than the data set  we explain how the remaining matrix operations to build the model can be quickly performed outside the dbms. we first explain how to efficiently compute summary matrices with plain sql queries. then we present two sets of udfs that work in a single table scan: an aggregate udf to compute summary matrices and a set of scalar udfs to score data sets. experiments compare udfs and sql queries  running inside the dbms  with c++  running outside on exported files . in general  udfs are faster than sql queries and udfs are more efficient than c++  due to long export times. statistical models based on the summary matrices can be built outside the dbms in just a few seconds. aggregate and scalar udfs scale linearly and require only one table scan  making them ideal to process large data sets.
categories and subject descriptors
h.1  database management : database applications- data mining; g.1  mathematics of computing : probability and statistics-multivariate statistics; h.1  database management : systems-relational databases

 this work was partially conducted at teradata  ncr.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  beijing  china.
copyright 1 acm 1-1-1/1 $1.
general terms
languages  performance  theory
keywords
dbms  sql  statistical model  udf
1.	introduction
¡¡the problem of analyzing large data sets has been extensively studied in data mining  but most research has concentrated on proposing efficient algorithms that work outside the dbms on flat files. some well-known techniques include association rules   clustering  and decision trees . only a few proposals have tackled the problem of actually integrating data mining or machine learning algorithms into the dbms  1  1  1 . integrating statistical techniques has received even less attention due to their mathematical nature  dbms complexity and the comprehensive functionality available in statistical packages. in a modern database environment  users generally export data sets to some statistical tool  build many models outside the dbms  score data sets based on the best model and then the scored data sets are imported back into the dbms. scoring means model application on a data set  generally a new one. some statistical tools can directly score data sets by generating sql queries. in general  sql is the standard language to process a data set inside a dbms  but unfortunately it has limitations to perform complex matrix operations  as required in multidimensional statistical models. this article explains a novel approach in which several fundamental statistical models are integrated into the teradata dbms with sql queries and user-defined functions  udfs .
¡¡the techniques explained in this article are used in a commercial data mining tool called teradata warehouse miner  twm . twm is a windows client program that connects to the dbms server via odbc  and automatically generates sql code based on user-specified param-
eters. twm implements statistical and machine learning algorithms combining sql queries  udfs and mathematical libraries. the linear statistical models explained in this work  implemented in twm  include correlation  linear regression  factor analysis and clustering. udfs are a standard application programming interface  api  in teradata v1. udfs in teradata are developed in the c language  compiled to object code and executed inside the dbms like any other sql function. thus udfs represent a promising alternative to extend sql with multidimensional statistics capabilities  exploiting c's flexibility and speed. therefore  sql syntax is not changed with a new primitive or select clause. the udfs presented in this article are an important component of teradata statistical and data mining functionality  but they can be used in any other dbms supporting scalar and aggregate udfs.
¡¡the article is organized as follows. section 1 introduces definitions and udfs. section 1 explains how to build statistical models and score data sets in a single table scan with sql queries and udfs. section 1 presents experiments comparing sql  udfs and c++  evaluating udf optimizations and studying time complexity. section 1 discusses related work. section 1 presents conclusions.
1.	preliminaries
1	definitions
¡¡we focus on computing multidimensional  multivariate  statistical models on a d-dimensional data set. let x = {x1 ... xn} be the input data with n points  where each point has d dimensions. x is a d ¡Á n matrix  where xi represents a column vector  equivalent to a d ¡Á 1 matrix . entry xli is the lth dimension from xi. for predictive purposes  x may have an additional dimension with a predicted variable y . to avoid confusion we use i = 1...n as a subscript for points and a b as dimension subscripts. the t superscript is used to indicate matrix transposition. in multivariate statistics the columns from a data set are called  variables . in machine learning the term  feature  is preferred. in databases the term  dimension  is the standard and that is the term used throughout this article. we will refer to the ath dimension  variable  as xa. to simplify notation sometimes we use ¦² without subscript to mean we are computing a sum over all rows i = 1...n.
¡¡in a relational database the data set x is stored on a table with another column i identifying the point  e.g. a customer id   which is not used for statistical purposes. this leads to table x being defined as  with primary key i. when there is a predicted numeric dimension y   x is defined as . statistical models are stored in tables as well. we use the j subscript to refer to the jth component  factor  or the jth cluster of a model  whose range is j = 1...k. the linear regression model is stored in a single row table having d columns  while principal component analysis and clustering model tables have k rows and d columns and use j as their primary key. notice xl  upper case  refers to the lth dimension and xi  lower case  is the ith point/observation.
1	user-defined functions
¡¡we explain user-defined functions  udfs  in the context of teradata. udfs are programmed in the c language and can be called in any  select  statement  like any other sql function. there are two classes of functions that can be used in a  select  statement:  1  scalar functions  that take a number of parameter values and return a single value. the function produces one value for each input row.  1  aggregate functions  which work like standard sql aggregate functions. they return one row for each distinct grouping of column value combinations and a column with some aggregation  e.g.  sum    . if there are no grouping columns they return one row. we omit discussion on a third class of udfs that allows using external tables.
¡¡udfs have several advantages. there is no need to modify internal dbms code. udfs are programmed in the c language and once compiled they can be used in any  select  statement like other sql functions. the udf source code can exploit the flexibility and speed of the c language. udfs work in main memory; this is a crucial feature to reduce disk i/o and reduce run time. udfs are automatically executed in parallel in teradata. this is an advantage  but also a constraint because code must be developed accordingly. on the other hand  udfs have important constraints and limitations. currently  teradata udf parameters can be only of simple types  e.g. numbers or strings  but not arrays . udfs cannot perform any i/o operation  which is a constraint to protect internal storage. udfs can only return one value of a simple data type. in other words  they cannot return a set of values or a matrix. scalar functions cannot keep values in main memory from row to row  which means the function can only keep temporary variables in stack memory. in contrast  aggregate functions can keep aggregated values in heap memory from row to row. udfs cannot internally call other udfs. udfs cannot access memory outside their allocated heap memory or stack memory. the amount of memory that can be allocated is somewhat low and it is currently limited to one 1 kb segment in the unix and windows operating systems.
1.	building models and scoring
¡¡this section presents our main contributions. we start by studying matrix computations in linear multivariate statistical models. we identify demanding matrix computations that are common to all techniques. two matrices turn out to be common for all statistical models  effectively summarizing a large data set. we study several alternatives to compute such summary matrices with sql queries. then we present an efficient aggregate udf that computes summary matrices in one table scan. we then introduce several scalar udfs that are used to score a data set when a statistical model is available. several research issues about query optimization are discussed. this section concludes with a brief time and space complexity analysis.
1	statistical models and techniques
¡¡we focus on four fundamental statistical techniques: correlation analysis  linear regression  principal component analysis and clustering. these techniques have long been used in statistics  and are commonly available in statistical packages. therefore  it is desirable to have an efficient implementation of them that can work on large data sets stored in a relational dbms.
linear correlation
a fundamental technique used to understand linear relationships between pairs of dimensions  variables  is correlation analysis. the correlation matrix is not a model  but it can be used to understand and build linear models as we shall see. all the following sums are calculated over i and therefore i is omitted to simplify equations. the pearson correlation coefficient between dimensions a and b is given by


linear regression
we extend the definitions given in section 1. the general linear regression model  assumes a linear relationship between d independent numeric variables from x and a dependent numeric variable y : y = ¦Âtx + ¦Â1  where y is a 1¡Án matrix  consistent notation   ¦Â1 is the y intercept and ¦Â is the vector of regression coefficients  ¦Â1 ¦Â1 ... ¦Âd . to allow easier mathematical manipulation it is customary to extend ¦Â with the intercept ¦Â1 and x with x1 = 1. then the linear regression equation becomes
y = ¦Âtx 
where ¦Â is unknown. the solution by the minimum least
squares method is: ¦Â =  xxt  1xy t.
¡¡the equation above requires several matrix computations. the most important partial computation is xxt that appears as a term for ¦Â. this matrix is analogous to the one used in correlation analysis  but in this case xxt is a  d + 1  ¡Á  d + 1  matrix. however  this product does not solve all computations. after computing xxt we need to invert it and multiply it by x:  xxt  1x. instead  since matrix multiplication is associative we can multiply x by y t first: xy t which produces a d¡Á1 matrix. then we can multiply the final matrix as a product of a d¡Ád matrix and a d ¡Á 1 matrix: ¦Â =  xxt  1 xy t . we use parentheses to make the order of evaluation explicit. matrix inversion and matrix multiplication are problems that will be solved outside the database system.
¡¡when ¦Â has been computed the predicted value of y can be estimated with: y  = ¦Âtx. this computation will produce a 1 ¡Á n matrix. the variance-covariance matrix of the model parameters  which is used to evaluate error  is obtained with:

¡¡we can again reuse xxt to compute the first term. the second termcan be easily computed in sql since it is just a sum of squared differences.
¡¡equations only require matrix computations. inverting a matrix is not easy to compute in sql. let z be a  d+1 ¡Án matrix that has the d+1 dimensions from x  including the 1s to be multiplied by ¦Â1  and y . then z contains the two matrices with n rows put together. z =  x y  . therefore  the product zzt  has the nice property of being able to derive xxt and xy t. there are other regression model statistics that are easily derived from xxt  y   ¦Â and y . pca and factor analysis
dimensionality reduction techniques build a new data set that has similar statistical properties  but fewer dimensions than the original data set. principal component analysis  pca   is the most popular technique to perform dimensionality reduction. pca is complemented by factor analysis  fa    which fits a probabilistic distribution to the variance of x. the output of pca and fa is a d ¡Á k dimensionality reduction matrix ¦«  where k   d. matrix ¦« is orthogonal: ¦«¦«t = id  meaning each component vector is statistically independent. pca and fa compute components  factors  from the correlation matrix and the covariance matrix  effectively centering x at its mean ¦Ì. the correlation matrix leaves dimensions in the same scale  whereas the covariance matrix maintains dimensions in their original scale. pca typically decomposes a matrix with the svd decomposition. maximum likelihood  ml  factor analysis  1  1  uses an expectation-maximization  em  algorithm  1  1  to get factors. in short  both techniques can work directly with a d ¡Á d matrix derived from x.
clustering and mixtures of distributions
k-means is the most popular clustering algorithm. the kmeans algorithm  1  1  partitions x into k disjoint subsets x1 x1 ... xk using the nearest centroid at each iteration. compared to the previous techniques  clustering cannot build an optimal model in just one scan. the standard version of k-means requires scanning x once per iteration  but there exist incremental versions that can get a good  but probably suboptimal  solution in a few or even one iteration s  . our discussion focuses on one iteration. let nj be the number of points in xj. at each iteration the centroid of cluster j is computed as:
 
¡¡where c is a d ¡Á k matrix and cj represents a column vector with one centroid. the cluster radius matrix  which measures the average squared distance per dimension to cj  is updated using:
.
¡¡each cluster weight is wj = nj/n. in general  clustering techniques assume dimensions are independent  which makes rj a diagonal matrix  with zeroes off the diagonal . in this case the sums above just require adding points and adding squared points  getting the squared of each dimension . rj is also known as the variance matrix of cluster j  since covariances are ignored. compared to the three previous techniques  we do not need to consider elements off the diagonal for rj.
1	summary matrices
¡¡some of the matrix manipulations we are about to introduce are well-known in statistics  but we exploit them in a database context. we introduce the following two matrices that are fundamental and common for all the techniques described above. let l be the linear sum of points  in the sense that each point is taken at power 1. l is a d¡Á1 matrix  shown below with sum and column-vector notation.
	.	 1 

¡¡let q be the quadratic sum of points  in the sense that each point is squared with a cross-product. q is d ¡Á d.
	.	 1 
¡¡matrix q has sums of squares in the diagonal and sums of cross-products off the diagonal:

¡¡the most important property about l and q is that they are much smaller than x  when n is large  i.e. d    n . however  l and q summarize a lot of properties about x that can be exploited by statistical techniques. therefore  the basic usage of l and q is that we can substitute every sum and every matrix product xxt for q. in other words  we will exploit l and q to rewrite equations so that they do not refer to x  which is the largest matrix.
linear correlation
the d ¡Á d correlation matrix ¦Ñ is given by:
	 	 
¡¡this equation is expressed only in terms of l and q. that is  we do not need x.
linear regression
in linear regression  taking the augmented matrix z  we can compute and let zi =  xi yi  represent the augmented vector with the dependent variable y . in this case matrix q is a  d + 1  ¡Á  d + 1  submatrix of q  with rows and columns going from 1 to d+1 containing xy t on the last row and the last column. we compute that contains l  first d + 1 values  and  last value .

¡¡based on these matrices the linear regression model can be easily constructed from. with ¦Â it is straightforward to get y . then with y  we can finally compute var ¦Â  that just requires. in short  l and q leave var ¦Â  as the only computation that requires scanning x a second time. this is consequence of not being able to derive the estimated column y  without ¦Â. in short  for linear regression l and q do most of the job  but an additional scan on x is needed to get y .
pca and factor analysis
we now proceed to study how to solve pca with l and q. as we saw above the correlation matrix can be derived from l and xxt. the variance-covariance matrix has a simpler computation than the correlation matrix. in statistics it is customary to call the variance-covariance matrix ¦²  but we will use v to avoid confusion with the sum operation. a variance-covariance matrix entry is defined as:
 
¡¡where x¡¥a is the average of xa  idem for b . this equation can be expanded to get:
 
¡¡which by mathematical manipulation reduces to the following equation based on l and q:
.
¡¡in matrix terms  v is d ¡Á d and v = q/n   llt/n1. in summary  n  l and q are enough  sufficient  to compute the correlation matrix ¦Ñ and the covariance matrix v that are the basic input to pca and ml factor analysis; then x is not needed anymore by svd or the em algorithm.
clustering
k-means and em are based on distance computation. the distance between xi and cj can be obtained with a scalar udf assuming cj is one row. we explain this udf below.
¡¡assuming we know the closest centroid to point xi  we perform a similar manipulation to the variance-covariance matrix  but we consider only diagonal matrices. the jth  j = 1...k  cluster centroid and radius  variance  are:
1
cj = lj 
nj
	1	t
rj = nj qj   1ljlj .
nj
¡¡finally  the cluster weight is wj = nj/n. we ignore elements off the diagonal in rj  thereby having two sums that are computed with d operations per point  instead of d1. notice these equations are simple and they involve ddimensional vectors and d¡Ád matrices. therefore  they can be easily computed inside the dbms with plain sql queries or scalar udfs  that will be explained later.
summary of matrix applicability
we have shown n  l and q are general enough to solve almost all demanding matrix computations in four different statistical techniques. therefore  we concentrate on analyzing how to compute them in an efficient manner in the dbms. for a large data set  where d    n  matrices l and q are comparatively much smaller than x. therefore  the cost to export them or write them to disk is minimal from a time/performance perspective. this is expected to be the typical case in a database environment.
1	implementation alternatives
¡¡in the case of the teradata dbms  there are five alternatives to evaluate matrix expressions  taking into account x is stored inside the database:  1  perform all matrix computations with sql queries  manipulating matrices as relational tables;  1  perform all matrix computations with udfs  manipulating matrices with c arrays;  1  perform no matrix operations inside the dbms  exporting the data set to an external statistical or data mining tool;  1  perform only demanding matrix computations inside the dbms combining sql queries and udfs and the rest outside;  1  integrate all matrix operations inside the dbms.
¡¡alternative  1  requires generating sql code and exploits dbms functionality. however  since sql does not provide advanced manipulation of multidimensional arrays  matrix operations can be challenging to express as sql queries. alternative  1  gives the ability to express matrix operations in the c language  but given the possibility of programming errors  the mathematical complexity of matrix computations and the parallel nature of the teradata dbms  we believe it is not advisable to program every matrix operation with udfs. in general  udfs incur on less overhead than generated sql. alternative  1  gives great flexibility to the user to analyze the data set outside the dbms with any language or statistical package. the drawbacks are the time to export the data set or subsets of it  the potentially lower processing power of a workstation compared to a large database server and compromising data security  e.g. a bank data set . alternative  1  represents a compromise among the three alternatives above. computations inside the database system can combine sql queries and udfs. from a practical point of view  there exist many off-the-shelf packages and software libraries that can build the statistical models or perform most complex matrix computations discussed above. alternative  1  represents the  ideal  scenario  where all matrix computations are done inside the database system. but at this moment this possibility is ruled out for two reasons: the parallel nature of teradata and the existence of many statistical and machine learning libraries and tools that can easily work outside the dbms  e.g. in a workstation . for instance  inverting a matrix  evaluating a long expression involving many matrices  implementing a newton-raphson method and computing singular value decomposition  svd   are difficult to program in sql or with udfs. instead  matrices l and q are used to evaluate complex  but more efficient and equivalent  matrix expressions as explained in section 1. such matrix expressions can be analyzed by a software system different from the dbms  which can reside in the database server itself or in a workstation. therefore  we focus on alternative  1   computing n  l and q for a large data set x with sql queries and udfs. the remaining matrix computations involving complex equations and numerical stability issues can be easily and efficiently solved outside the dbms.
1	summary matrices computed with udfs
¡¡we start by presenting a first approach to calculate l and q with sql queries. then this framework is used to develop an efficient aggregate udf.
summary matrices computed with sql
this is a first alternative to  push  the demanding computation of n l q inside the dbms with sql queries. it is simple and works in any relational dbms. the initial task is getting n the first time x is scanned.
select sum 1  as n from x;
¡¡the second step is computing l  which can be done with d statements:  select a sum xa  from x   giving the ability to access every l entry by subscript a. l can be equivalently computed with the sql statement below. this statement is faster than d statements  but l entries must be accessed by column names.
select sum x1  sum x1 ... sum xd  from x;/* l */
¡¡the third step requires computing q. a first straightforward approach is to get one matrix entry per  select  statement. each select statement specifies one row/column combination. these d1 statements can be accelerated synchronizing table scans.
select 1 sum x1   x1  from x; /* q */
...
select a b sum xa   xb  from x;
...
select d d sum xd   xd  from x;
¡¡given the fact that q is symmetrical we can apply a traditional numerical analysis optimization based on the fact that qab = qba: we can compute the lower triangular submatrix of q with d d + 1 /1 operations instead of d1. for k-means and em clustering we only need d computations to get the diagonal submatrix of q.
¡¡we can compute n l q more efficiently in a single sql statement based on the fact that n  l and q are independent. this is a fundamental property about sufficient statistics  that is exploited in a database context. this property will also be essential for aggregate udfs.
select sum 1  /* n */
 sum x1  sum x1  ... sum xd  /* l */
 sum x1   x1  null ... null /* q */  sum x1   x1  sum x1   x1  ... null
...
 sum xd   x1  sum xd   x1  ... sum xd   xd 
from x;
¡¡as we can see n  l and q can be computed in one table scan with one  long  sql query having 1 + d + d1 terms.
aggregate udf
we now explain how to compute summary matrices n l q inside the teradata dbms  with an efficient aggregate udf. generalizing  l and q are the multidimensional version of sum xa  and sum xa1  for one dimension xa  taking into account inter-relationships between two variables xa and xb. from a query optimization point of view  we reuse the approach used for sql by computing n  l and q in one table scan on x.
aggregate udf definition in sql
the sql definition specifies the call interface with parameters being passed at run-time and the value being returned. the aggregate udf takes as parameter the type of q matrix being computed: diagonal  clustering   triangular  correlation/pca/fa/regression  or full  querying/visualization   to perform the minimum number of operations required. udfs in teradata cannot accept arrays as parameters or return arrays. to solve the array parameter limitation  we introduce two basic udf versions: one version passes xi as a string and the second version passes xi as a list. both versions take d and pack n l q as a string and return it.
¡¡the amount of maximum heap memory allocated is specified with the  class aggregate  clause. the amount of memory required by the  big  output value is also specified here after the  returns  keyword.
aggregate udf variable storage
following the one pass principle for the sql query  the aggregate udf also computes n  l and q in one pass. the udf also computes the minimum and maximum for each dimension  which can be used to detect outliers or build histograms. a c  struct  record is defined to store n  l and q  which is allocated in main memory in each processing thread. since x is horizontally partitioned each thread can work in parallel. notice n is double precision and d is dynamic for both sql and udfs  but the udf has a threshold on d  max d  because the udf memory allocation is static.
typedef struct udf nlq storage { int d; double n; double l max d ; double q max d  max d ;
} udf nlq storage;
aggregate udf run-time execution
we omit discussion on code for handling errors  e.g. empty tables   invalid arguments  e.g. data type  and memory allocation. the aggregate udf has four main steps that are executed at different run-time stages:  1  initialization  where memory is allocated and udf arrays for l and q are initialized in each thread.  1  row aggregation  where each xi is scanned and passed to the udf  xi entries are unpacked and assigned to array entries  n is incremented and l and q entries are incrementally updated by equation 1 and equation 1. since all rows are scanned  this step is executed n times and therefore  it is the most time-consuming.  1  partial result aggregation  which is required in the parallel teradata dbms to compute totals  by adding subtotals obtained by each thread. threads return their partial computations of n l q that are aggregated into a single set of matrices by a master thread.  1  returning results  where matrices are packed and returned to the user.
¡¡in step 1 since the dimensionality d of x cannot be known at compile time  the udf  struct  record is statically defined to have a maximum dimensionality. this wastes some memory space but it does not affect speed. the reason behind this constraint is that storage gets allocated in the heap before the first row is read. an alternative to allocate only the minimum space required is to define d udfs that have d different number of parameters. the udf with k parameters s.t. k ¡Ü d would have matrices l of size k and q of size k1  respectively.
¡¡step 1 is the most intensive because it gets executed n times. therefore  most optimizations are incorporated here. the first task is to grab values of x1 ... xd. given the constraint that arrays are not allowed in teradata sql  there are two choices to pass a vector as a udf parameter:  1  packing all vector values as a long string.  1  passing all vector values as parameters individually; each of them having a null indicator parameter as required by sql. for choice  1  the udf needs to call a function to unpack xi  which takes time o d  and incurs on overhead. overhead is produced by two reasons: at run-time  floating point numbers must be cast as strings and when the long string is received  it must be parsed to get numbers back  so that they are properly stored in an array. the unpacking routine determines d. for choice  1  the udf directly assigns vector entries in the parameter list to the udf internal array entries. given the udf parameter compile-time definition  d must be passed as a parameter as well. then n in incremented and l ¡û l+xi. finally  the udf computes q ¡û q+xixti based on the desired type of matrix: diagonal  triangular or full  with triangular being the default.
1	scoring data sets with scalar udfs
¡¡once a model has been built it can be applied on data sets having the same dimensions. such data sets can be used to test the accuracy of the model using the standard train and test approach  or they can contain new points  where model application is required  i.e. predicting a numeric variable y   reducing a high dimensional data set down to a manageable dimensionality k  finding the closest representative cj . in statistical terms  applying a model on a data set is called scoring. in general  step-wise procedures for linear regression  feature selection for clustering and selecting representative dimensions in pca reduce d to some lower dimensionality d  effectively getting a subset of all dimensions in x. to simplify exposition we assume all d dimensions are used. since correlation is not a model  scoring is not needed in such case; we analyze the other statistical techniques below.
on using sql queries to score
the following discussion applies to both sql queries and udfs expressing a scoring equation. some key differences include the following. in general  sql queries require a program to automatically generate sql code given the model and an arbitrary input data set  but it is not possible to have generic sql stored procedures for such purpose  because data sets have different columns and different dimensionalities. sql arithmetic expressions are interpreted at run-time  whereas udf arithmetic expressions are compiled. linear regression
for linear regression we have ¦Â as input. thus we need to compute
y i = ¦Âtxi
for each point xi. the regression model is stored in the dbms as a table beta ¦Â1 ... ¦Âd . this table layout allows retrieving all coeficients in a single i/o. scoring a data set simply requires a dot product udf between two vectors  each having d dimensions: linearregscore x1 ... xd  ¦Â1 ... ¦Âd . this udf returns  yi. a cross-product join between beta and x is computed and then xi and beta are passed as parameters to the udf. therefore  x can be scored with a linear regression model in a single pass calling the udf once in a select statement.
pca and factor analysis
pca and factor analysis produce as output the d ¡Á k dimensionality reduction matrix ¦«  where k   d. matrix ¦« and vector xi are used to obtain the dimensionality-reduced k-dimensional vector:
 
matrix ¦« is stored as a table lambda j x1 ... xd  and

the mean is stored on table mu x1 ... xd . these table layouts allow retrieving all d dimensions in a single i/o. the scoring equation can be computed with a udf that takes xi  ¦Ì and the jth component  factor   ¦«j  as parameters and returns the jth coordinate of the  reduced  vector. vector ¦Ì corresponds to the mean of x and is used to  center  new points at the original mean. the udf call is as follows: fascore  x1 x1 ... xd  ¦Ì1 ¦Ì1 ... ¦Ìd  ¦«1j ¦«1j ... ¦«dj  . this udf is called k times in the same select statement with j = 1...k to obtain. notice the udf needs to be called k times to produce k numbers since udfs cannot return vectors. in this case x is cross-joined with lambda k times  with aliasing to avoid ambiguity  to retrieve each of the k components  factor  and then xi and component ¦«j are passed as parameters to the udf. therefore  x can be scored with a pca or factor analysis model in a single pass  calling the udf k times in a select statement.
clustering
scoring for clustering requires two steps:  1  computing distances to each centroid and  1  finding the nearest one. cluster centroids are stored on table c j x1 ... xd   their

variances are stored on table r j x1 ... xd   and weights are stored in table w  . the euclidean distance employed by k-means between xi and cj is given by dj =  xi   cj t xi   cj .
¡¡the k distances between xi and each centroid cj  can be obtained with a udf that takes two d dimensional vectors: distance x1 ... xd c1j ... cdj . by calling this distance udf k times corresponding to each cj we get k distances: d1 ... dk. in this case x and the k cluster centroids cj are cross-joined k times  with aliasing  to compute distance dj and then the k distances are passed as parameters to the scoring udf. we just need to determine the closest centroid subscript j  based on the minimum distance  which is the required score for a clustering model: j s.t. dj ¡Ü dj for j = 1...k. this udf computes the closest centroid  based on k distances clusterscore d1 ... dk . therefore  x can be scored based on a clustering model in a single table scan. both udfs could be combined into a single udf that takes xi and the k cluster centroids cj packed as a string as parameters  effectively reducing udf call overhead. but this could involve long strings for high d  exceeding the sql parser limits.
1	query optimization
¡¡optimizing queries calling our udfs is a broad problem because the udf c code does not allow manipulating the sql statement calling the udf. so far we have discussed how to efficiently compute the summary matrices and score the data set in one table scan  but we have not discussed how the data set x is derived in the first place. in general x  as defined in section 1  exists as:  1  a table or  1  a view. in either case many joins and many aggregations  are statically computed before or dynamically computed on-demand  respectively. from a query optimization view alternative  1  is easier than  1  because x is already available and the udfs just require scanning x once  without worrying how x was computed. alternative  1  represents the general query optimization problem and it is challenging because x is generally a large data set  has very high dimensionality and the view query will generally involve multiple joins and aggregations on other tables and views. therefore  we discuss relevant query optimization techniques based on alternative  1 . in general  statistical and machine learning algorithms work with tabular data sets  similar to x  where each dimension of x is one of the following:  a  some known property of point i;  b  a binary flag  indicating presence/absence of some characteristic about point i;  c  some metric. properties of point i generally come from other tables or views  by joining on the corresponding foreign keys and primary keys  keeping the desired property in x in a denormalized form  e.g. customer state  customer age . binary flags are generally derived with the sql case statement and are used to convert categorical variables into binary dimensions  e.g. is customer active  1  where 1=yes . metrics are generally computed with aggregations  being sum   and count   the most common. aggregations are used to create new dimensions  features   e.g. number of items purchased  total money spent . the three most relevant query optimizations are:  1  join elimination   which involves rewriting queries so that unnecessary joins are avoided; this optimization is particularly useful for scoring data sets  after some feature selection or step-wise procedure has been done.  1  performing group-by before join   which involves changing the default order of evaluation  join before group-by . this optimization is useful when several queries aggregate from large tables grouping by the same columns  e.g. customer id .  1  efficient star-join computation; in general x is built using left outer joins using some reference table as the left operand  containing the universe of all possible points i  populating missing values with nulls. this optimization is particularly useful when x is incrementally built with vertical partitions of its dimensions coming from multiple large tables  e.g. number of products purchased  number of complaint calls  tenure .
1	time and space complexity
¡¡space for sql and udf: n takes o 1   l takes o d   q takes o d1  in memory. the outstanding property about the udf is that i/o time to compute l and q is linear in d and n: o dn   instead of o d1n . time to compute q is o dn  for diagonal matrices  and o d1n  for triangular and full matrices. table x is scanned once with i/o time o dn   except for linear regression that requires an additional scan on x . the result table has one row and 1+d+d1 columns for sql and one row with one long string for the udf.
¡¡once matrices are exported statistical techniques take the following times to build models outside the dbms: linear correlation takes o d1   pca takes o d1   which is the time to get svd with the correlation matrix ¦Ñ  linear regression takes o d1  to invert q and clustering takes o dk  to compute k clusters. the udf approach will be efficient as long as d    n  which is expected in a database environment.
¡¡when a statistical model is available scoring with udfs takes o dn  for linear regression and o dkn  for pca  factor analysis and clustering. however  i/o time is o dn  for the three techniques since the corresponding vector/matrix operations are computed in main memory.
nlinear correlation
linear regressionpca¡Á1c++sqludfc++sqludf111111111111111111table 1: total time to build models at d = 1  secs .
1.	experimental evaluation
¡¡this section presents experimental evaluation on the teradata dbms v1. the teradata server had 1 parallel processing threads in a shared-nothing architecture  four nodes running at 1 ghz  1 mb of memory per node and 1 tb on disk. the dbms ran under the unix operating system. data sets were horizontally partitioned evenly among threads  where each thread was responsible for processing 1th of x. we also used a workstation with a 1 ghz cpu  1 mb of main memory and 1 gb on disk with a c++ implementation computing n l q. computers were linked by a 1 mbps lan. time comparisons between the dbms server and the workstation are not fair  but they illustrate a typical database scenario  where the user has the choice of processing data sets inside the dbms or outside. average time is calculated from five runs of each experiment. data sets
we generated synthetic data sets with a mixture of normal distributions that were stored as tables. we used k = 1 distributions with means in  1  and standard deviation around 1 per dimension  with about 1% of points representing uniformly distributed noise. varying these parameters does not change n or d  but we avoided data sets that produced exceptions or undefined calculations. we varied n and d to study udf optimizations and to test scalability. since the three implementations produce the same results the quality of solutions is not measured.
parameter settings and default optimizations
the c++ implementation analyzed data sets stored in text files exported out from the dbms with the odbc interface; we exclude export times with faster proprietary interfaces available in teradata since odbc is an industry standard. the c++ program was optimized to scan x once  keeping l and q in main memory at all times. the sql query and the udf compute n  l and q in a single table scan as explained in section 1 and compute the lower triangular matrix for q by default. the udf passes vector entries as a list  instead of packed string  by default. table x is read from disk every time; table x is not cached under any circumstance. 1 comparing implementation alternatives
¡¡we have two sets of comparisons:  1  the aggregate udf compared with sql and c++   1  the scalar udfs compared with sql queries producing the same result and c++  scoring outside the dbms exporting with odbc.
¡¡table 1 compares c++  sql and the udf to compute n l q. this table excludes times to export x with odbc for the c++ time  which will be analyzed below; this gives
n ¡Á 1dc++sqludfodbc111111111111111111111111table 1: time to compute n  l and q with aggregate udf and time to export x with odbc  secs .
dlinearlinearpcaclusteringcorrelationregression1111111111111table 1: time to build models with n l q; time is independent from n  secs .
an unfair advantage to c++. the three implementations take advantage of l and q thereby requiring only one scan on x. the time to build the linear regression model excludes the time to compute y   which requires a second table scan on x. the times to compute ¦Ñ for linear correlation  and the times to get ¦Â for linear regression were the same. therefore  both time measurements appear as one column. pca took slightly longer  one additional second  than the other techniques. in summary  all the techniques take practically the same time for large data sets in each respective implementation. comparing c++  sql and the udf we can see that c++ has performance comparable to sql only for smaller data sets. otherwise  c++ is much slower  especially for the largest data set  which can be explained by the difference in computing power between the parallel database server and the workstation  typical scenario . c++ and udfs show linear behavior  but sql does not; this will be illustrated with graphs. the udf shows better performance than the sql query in every case  but the sql query scalability improves as n increases.
¡¡table 1 and table 1 provide a breakdown of times shown in table 1 varying d for n = 1k and n = 1k. observe the bulk of the work in computing linear models is in getting n  l and q as illustrated in table 1. table 1 is redundant  but it illustrates the fact that the time to build a model is independent from n  when n l q are available. that is  the only scalability factor is d. from all the techniques only pca has a slightly higher time growth as d increases. but in all cases the time to build the models is negligible compared to the time to compute n l q. the column that shows numbers significantly higher than the rest is the odbc column  where the time to export the data set can be as high as two orders of magnitude higher than the time for the udf or the sql query. these results indicate that export times can become a reason not to analyze a data set outside the database. the udf shows the best performance
n ¡Á 1techniquesqludf1linear11regression111111pca11111111clustering1111111table 1: time to score x at d = 1 and k = 1  secs .

	1
1	1	1	1	1 n x 1	n x 1
figure 1: sql vs. aggregate udf varying n.
and the trend indicates that a higher d will make the gap wider compared to the sql query. the sql query comes in second place and c++ is the slowest; adding odbc times makes c++ clearly the worst.
¡¡table 1 compares sql queries and scalar udfs to score a data set based on a model. sql queries use an arithmetic expression evaluating the corresponding model equation. we can see the udf is as efficient as sql to produce a linear regression score. for pca the udf is also as efficient as sql to produce a k-dimensional vector multiplying by ¦«. finally  clustering turns out to be more challenging for sql. in this case the udf is faster than sql because sql requires two scans on a pivoted version of x in order to get distances and then determine the closest centroid. these results state that udfs are expected to be as fast or even faster than equivalent sql arithmetic expressions.
1	optimizations for aggregate udf
¡¡figure 1 compares sql and the udf as n grows. the udf shows linear behavior as n grows. sql does not show linear behavior when n ¡Ü 1  due to the overhead for parsing and evaluating long  select  statements. it is interesting sql is faster than the udf when d = 1 and d = 1. when d = 1 sql and the udf have about the same performance  although the trend indicates that sql will be slightly faster as n grows. finally  when d = 1 the udf becomes much faster than the sql query and the trend indicates the gap will be maintained as n grows.
¡¡figure 1 illustrates time growth as d grows keeping n fixed.	time growth is much slower for the udf and in

	1
1	1	1	1	1 d	d
figure 1: sql vs. aggregate udf varying d.

	1
1	1	1	1	1 n x 1	d
figure 1: comparing udf parameter passing style.
fact  its growth is almost linear. on the other hand  sql time grows much faster in a quadratic fashion. such difference in performance can be explained by the disk i/o sql needs to perform to create the  wide  table with 1 + d+ d1 columns  whereas the udf performs a quadratic number of operations in memory but it is impacted mainly by the i/o to read d dimensions. in short  these graphs indicate the udf is better at high d but it is slower at low d. sql is particularly fast for very low d.
¡¡figure 1 compares the parameter passing style for the udf  string-based or list-based from section 1  . on the left graph  when d = 1 the difference in time is marginal as n grows and d is kept fixed at a low value. on the right graph  time grows faster for the string-based version when n is fixed and d grows. we can state that for d ¡Ü 1 the stringbased and list-based version have similar performance  but when d ¡Ý 1 the list-based version becomes a much better alternative. it is interesting that growth for the list-based version seems almost constant with an almost zero slope.
¡¡figure 1 illustrates the impact of optimizing matrix computations in the aggregation step. recall from section 1 that the udf can compute a diagonal  triangular or full matrix. to our surprise  we can see on the left graph that difference in time for the three types of matrices is marginal when d = 1. on the right graph we show the trend as d grows. we can see that time grows very slowly for the diagonal matrix and it grows faster for the triangular and full matrix. in short  doing d1 operations instead of d operations makes an important difference in time at high d and a marginal difference at low d.

1	1	1	1	1 n x 1	d
figure 1: aggregate udf: matrix optimization.
n ¡Á1kstringlist111111111111111111111111table 1: using  group by  with aggregate udf varying # of groups k at d = 1  secs .
¡¡we analyze how the udf works when there is a  group by  clause. this type of query is particularly important to recompute centroids and radiuses in a clustering problem or to get several sub-models from the same data set based on different grouping columns. we partition x on k groups using the mod operator on i  computing a separate set of summary matrices for each induced group  grouping by the group subscript  j .
¡¡the data set x has d = 1  k is an integer that varies as k = 1 1 ... 1 and we compute a diagonal matrix by default. this query creates k groups with similar number of rows  each of which has its own n  l and q. in practical terms  this query can be used to compute k clusters if the nearest centroid is available in column j. table 1 shows time growth for both parameter passing styles. in every case the list-based version is faster than the string-based version  but the difference in performance is less significant at k = 1. for both versions time grows slowly when k ¡Ü 1  there is a jump at k = 1 and time grows almost four times when d doubles from 1 to 1.
summary of importance of each optimization
the first consideration about computing n  l and q inside the dbms is choosing between sql and udfs. if a dbms does not allow the creation of udfs  then sql may be a reasonable choice for low d. sql turned out to have better performance than udfs on low d. but a high d makes udfs the best choice. experiments indicate that sql becomes more

1	1	1	1	1 n x 1	d
figure 1: aggregate udf: time varying n and d.
severely affected by i/o than udfs since sql internally creates a table with one row and many columns. the style of parameter passing is the second most important factor for time performance. the overhead of converting numbers to strings becomes important. this is counter-intuitive from a database point of view because the udf works in main memory and parameters are passed on the run-time stack. packing vector entries as a string to pass them to the udf takes o d  per point  whereas triangular/full matrix operations take o d1  per point. therefore  we expected matrix operations to be the most demanding task  which was not the case in our experiments. in other words  the overhead caused by converting numbers to strings and then strings back to numbers  is greater than doing a quadratic number of arithmetic operations. therefore  if another dbms has constraints on a low maximum number of parameters  say 1   then the string-based udf version is not much slower than the udf list-based version. on the other hand  if the dbms allows a high number of parameters then the list-based version is the best choice. the third factor is naturally the time complexity of matrix computations. we have seen that even though matrix computations are done in main memory  time starts growing in a superlinear way when d ¡Ý 1. at d = 1 the difference between the diagonal and the full matrix becomes important  although not as significant as we expected. for practical purposes  this means we can perform a quadratic number of operations inside the aggregate udf without a big concern about an impact on time performance.
1	time complexity
¡¡figure 1 shows time complexity for the aggregate udf to get n l q with the most important matrix sizes: n and d. time growth is clearly linear for the three type of matrices. we can observe that the difference in performance between d values is marginal for the diagonal matrix  it is small for the triangular matrix and it becomes bigger for the full matrix. time growth for the diagonal matrix is almost zero. time grows slightly faster for the full matrix compared to the triangular matrix. but for the triangular and full matrices time growth does not look quadratic. in fact  it is almost linear. this is good news because it indicates that we can do up to d1 operations in memory with little impact on performance. however  it is also bad news because no matter how much we optimize the aggregation step  i/o will remain a bottleneck.

figure 1: scalar udfs to score: time varying n.
n ¡Á 1d# of udf callstotal time1111111111table 1: time growth for high d. times in seconds.
¡¡table 1 shows time growth for problems with highly dimensional data sets  where d ¡Ý 1. in this case  the problem of computing l and q is divided into subproblems that can fit in submatrices with d = 1  given the fact that matrices can be partitioned by row/column ranges. each udf call computes a portion of l and q  specified by the ranges for subscripts a and b. in this case all the udf calls are submitted as a single request with a synchronized table scan optimization. as can be seen  the total elapsed time is proportional to the number of calls.
¡¡figure 1 shows time complexity to score data sets with d = 1 and varying n. for pca and clustering k = 1.
the plots show the scoring udfs scale linearly with respect to n. clustering is the most demanding technique  closely followed by pca. scoring for linear regression is the fastest since it only requires a simple vector dot product.
1.	related work
¡¡although there has been considerable work in data mining to develop efficient mechanisms for computation  most work has concentrated on proposing algorithms assuming the data set is available in a flat file outside the dbms. statistics and machine learning have paid little attention to large data sets  which is precisely the primary focus of data mining. studying purely statistical techniques in a database context has received little attention. most research work has concentrated on association rules   followed by clustering  1  1  and decision trees . the importance of the linear sum of points and the quadratic sum of points  without cross-products  to decrease i/o in clustering is recognized in  1  1   but they assume the data set is directly accessible with some i/o interface. we have gone well beyond that point  showing the linear sum and the quadratic sum of points with dimension cross-products solves four fundamental statistical problems. this contribution is independent from implementation.
¡¡there exist many proposals that extend sql with data mining functionality. teradata sql  like other dbmss  provides advanced aggregate functions to compute linear regression and correlation  but it only does it for two dimensions. most proposals add syntax to sql and optimize queries using the proposed extensions. several techniques to execute aggregate udfs in parallel are studied in ; these ideas are currently used by modern parallel relational database systems such as teradata. udfs implementing common vector operations are proposed in   which shows udfs are as efficient as automatically generated sql queries with arithmetic expressions  proves queries calling scalar udfs are significantly more efficient than equivalent queries using sql aggregations and shows scalar udfs are i/o bound. data mining primitive operators are proposed in   including an operator to pivot a table and another one for sampling  useful to build data sets. sql extensions to define  query and deploy data mining models are proposed in ; this proposal focuses on managing models rather than computing them and therefore such extensions are complementary to our udfs. query optimization techniques and a simple sql syntax extension to compute multidimensional histograms are proposed in   where the grouping sets clause is optimized. computation of sufficient statistics for classification in a relational dbms is proposed in . developing data mining algorithms  rather than statistical techniques  using sql has received moderate attention. some important approaches include  1  1  to mine association rules   1  1  to cluster data sets using sql queries  and  to define primitives for decision trees. sql syntax is extended to allow spreadsheet-like computations in   letting an end-user express complex equations in sql  but such approach is not as flexible and efficient as udfs to express matrix equations.
1.	conclusions
¡¡we explained how linear multidimensional statistical models are integrated into the teradata dbms with sql queries and udfs. the techniques presented in this work are used in a commercial data mining tool called twm. we focused on four well-known statistical techniques including correlation  linear regression  principal component analysis and clustering. udfs are presented in two groups: an aggregate udf that computes summary matrices to build models and a set of scalar udfs to score data sets given a model as input. in general  udfs can process a data set in a single table scan  except clustering . to build models  statistical techniques take advantage of two sufficient statistics matrices that effectively summarize a large  high dimensional data set. the first matrix contains the linear sum of points and the second one contains the quadratic sum of points with dimension cross-products. complex matrix equations for a model based on sufficient statistics can be efficiently evaluated outside the dbms in a few seconds. two alternatives to compute sufficient statistics were discussed: with plain sql queries and with a faster udf. both alternatives require only one table scan. we then presented a set of scalar udfs to score data sets in a single pass based on linear regression  pca and clustering models. udfs are efficient since they work in main memory  but they are constrained by the dbms architecture. experiments compare udfs and sql queries  running inside the dbms  and c++  running outside the dbms . long export times using odbc represent a limitation to analyze a data set outside the dbms with c++. the aggregate udf can compute a diagonal  triangular or a full summary matrix to improve performance. there is a small performance gain at low dimensionality when computing a diagonal instead of a triangular matrix or when computing a triangular instead of a full matrix  but such gain becomes important at high dimensionality. scalar udfs to score data sets and the aggregate udf to compute summary matrices exhibit linear scalability  highlighting their remarkable efficiency.
¡¡these are several issues for future research. other statistical techniques can benefit from the same approach: finding matrices that summarize large data sets to build a model  efficiently computing such matrices with aggregate udfs and scoring data sets through scalar udfs. studying in depth optimization of queries calling udfs that directly access views is an important next step. disk i/o remains a bottleneck to develop even faster udfs  but there are further optimizations like main or cache memory processing  block read-ahead and synchronized table scans on query steps accessing the same table that may further reduce time.
acknowledgments
the author thanks mike rote  tim miller  the twm development team and the data mining consulting team for many interesting discussions and their support.
