we present a novel approach for efficient and self-tuning query expansion that is embedded into a top-k query processor with candidate pruning. traditional query expansion methods select expansion terms whose thematic similarity to the original query terms is above some specified threshold  thus generating a disjunctive query with much higher dimensionality. this poses three major problems: 1  the need for hand-tuning the expansion threshold  1  the potential topic dilution with overly aggressive expansion  and 1  the drastically increased execution cost of a high-dimensional query. the method developed in this paper addresses all three problems by dynamically and incrementally merging the inverted lists for the potential expansion terms with the lists for the original query terms. a priority queue is used for maintaining result candidates  the pruning of candidates is based on fagin's family of top-k algorithms  and optionally probabilistic estimators of candidate scores can be used for additional pruning. experiments on the trec collections for the 1 robust and terabyte tracks demonstrate the increased efficiency  effectiveness  and scalability of our approach.
categories and subject descriptors: h.1: search process.
general terms: algorithms  performance  experimentation.
keywords: top-k ranking  query expansion  incremental merge  probabilistic candidate pruning.
1. introduction
　query expansion is an indispensable technique for evaluating difficult queries where good recall is a problem. examples of such queries are the ones in the trec robust track 

 
 partially supported by the eu within the 1th framework program under contract 1  dynamically evolving  large scale information systems   delis 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
e.g.  queries for  transportation tunnel disasters  or  ship losses  on the aquaint news corpus. typical approaches use one or a combination of the following sources to generate additional query terms: thesauri such as wordnet with concept relationships and some form of similarity measures  explicit user feedback or pseudo relevance feedback  query associations derived from query logs  document summaries such as google top-1 snippets  or other sources of term correlations. in all cases  the additional expansion terms are chosen based on similarity  correlation  or relative entropy measures and a corresponding threshold. for difficult retrieval tasks like the above  query expansion can improve precision top-k  recall  as well as uninterpolated mean average precision  map  by a significant margin  see  e.g.   1  1  . however  in contrast to a mere benchmark setting such as trec  applying these techniques in a real application with unpredictable ad-hoc queries  e.g.  in digital libraries  intranet search  or web communities  faces three major problems  1  1 : 1  the threshold for selecting expansion terms needs to be carefully hand-tuned  and this is highly dependent on the application's corpus and query workload. 1  an inappropriate choice of the sensitive expansion threshold may result in either not achieving the desired improvement in recall  if the threshold is set too conservatively  or in high danger of topic dilution  if the query is expanded too aggressively . 1  the expansion may often result in queries with 1 or 1 terms which in turn leads to very high computational costs in evaluating the expanded queries over inverted index lists.
　this paper addresses the above three issues and provides a practically viable  novel solution. our approach is based on the paradigm of threshold algorithms  proposed by fagin and others  1  1  1  1  1   with a monotonic score aggregation function  e.g.  weighted summation  over inverted index lists. in this framework  each per-term list
is precomputed and stored in descending order of per-term scores  e.g.  some form of tf，idf-style index-term weights or the term's contribution to a probabilistic relevance measure such as okapi bm1  1  1  . the specific algorithm that we employ processes index lists using efficient sequential disk i/o  prunes top-k result candidates as early as possible  and terminates without having to scan the entire index lists  often after processing only fairly short prefixes of the lists . in addition  we use score distribution statistics to estimate aggregated scores of candidates and may prune them already when the probability of being in the top-k result drops below an acceptable error threshold  e.g.  1 percent .
　our key techniques for making query expansion efficient  scalable  and self-tuning are to avoid aggregating scores for multiple expansion terms of the same original query term and to avoid scanning the index lists for all expansion terms. for example  when the term  disaster  in the query  transportation tunnel disaster  is expanded into  fire    earthquake    flood   etc.  we do not count occurrences of several of these terms as additional evidence of relevance. rather we use a score aggregation function that counts only the maximum score of a document for all expansion terms of the same original query term  optionally weighted by the similarity  or correlation  of the expansion term to the original term. furthermore and most importantly for efficiency  we open scans on the index lists for expansion terms as late as possible  namely  only when the best possible candidate document from a list can achieve a score contribution that is higher than the score contributions from the original term's list at the current scan position or any list of expansion terms with ongoing scans at their current positions. the algorithm conceptually merges the index lists of the expansion terms with the list of the original query term in an incremental on-demand manner during the runtime of the query. for further speed-up  probabilistic score estimation can be used  considering score distributions and term selectivities.
　the novel contributions of the paper are threefold: 1  the incremental merge algorithm for highly efficient query expansion  1  its integration with top-k query processing and probabilistic prediction of aggregated scores and selectivities  and 1  a comprehensive experimental evaluation using the robust track and terabyte track benchmarks of trec 1.
1. related work
1 query expansion
　there is a rich body of literature on query expansion  see  e.g.   1  1  1  1  1  1  1  1  1  1  1  1  . all methods aim to generate additional query terms that are  semantically  or statistically related to the original query terms  often producing queries with more than 1 or 1 terms and appropriately chosen weights. given the additional uncertainty induced by the expansion terms  such queries are usually considered as disjunctive queries and incur very high execution costs for a dbms-style query processing  1  1 . the various methods differ in their sources that they exploit for inferring correlated terms: explicit relationships in thesauri  explicit relevance feedback  pseudo relevance feedback  query associations derived from query logs and click streams  summary snippets of web search engine results  extended topic descriptions  available in benchmarks   or combinations of various techniques. in all cases some similarity  correlation  or entropy measure between the original query terms and the possible expansion terms should be quantified  usually in some statistical manner   and a carefully tuned threshold needs to be determined to eliminate expansion candidates that are only weakly related to the original query. while such manual tuning is standard in benchmarks like trec  it is almost black art to find robust parameter settings for real applications with highly dynamic corpora and continuously evolving query profiles   e.g.  intranets  web forums  etc. . this calls for automatic and self-adaptive query tuning.
　 generate google queries from the original query and use the summary snippets on the top-1 result page to generate alternative query formulations and performed very successful in recent trec benchmarks. the final query expansion is a weighted combination of the original and the alternative queries.  uses a suite of techniques for extracting phrases and word sense disambiguation  wsd   with wordnet  as a background thesaurus and source of expansion candidates. both of these techniques seem to require substantial hand-tuning for achieving their very good precisionrecall performance. in the current paper  where the emphasis is on efficiency rather than effectiveness  we use a rather simple form of wsd and wordnet-based expansions. the techniques of the current paper could be easily carried over to more sophisticated expansion methods as mentioned above.
1 efficient top-k query processing
　there is also a good amount of literature on efficient topk query processing for ranked retrieval of text  web  and semistructured documents as well as multimedia similarity search  preference queries on product catalogs  and other applications  see  e.g.   for an overview . most algorithms scan inverted index lists and aggregate precomputed per-term or per-dimension scores into in-memory  accumulators   one for each candidate document. the optimizations in the ir literature aim to limit the number of accumulators and the scan depth on the index lists in order to terminate the algorithm as early as possible  1  1  1  1  1  1  1  1 . this involves a variety of heuristics for pruning potential result candidates and stopping some or all of the index list traversals as early as possible  ideally after having seen only short prefixes of the potentially very long lists. to this end  it is often beneficial that the entries in the inverted lists are kept in descending order of score values rather than being sorted by document identifiers  1  1 . in the current paper  we assume exactly this kind of index structure.
　a general framework for this kind of methods has been developed by fagin et al. in  and others  1  1  1   in the context of similarity search on multimedia and structured data. this family of so-called threshold algorithms  ta  operates on score-sorted index lists and assumes that the score aggregation function is monotonic  e.g.  a weighted summation . it uses a threshold criterion that is provably optimal on every possible instance of the data . many variations and extensions of the ta method have been developed in the last few years  1  1  1  1  1  1  1  1 . the current paper builds on the particular variant developed in   with sequential index accesses only and a probabilistic score prediction technique for early candidate pruning  when approximate top-k results  with probabilistic error bounds  are acceptable by the application. the rationale for avoiding  minimizing  or limiting random accesses is that for very large corpora index lists are disk-resident and sequential disk i/o is an order of magnitude more efficient than random i/o  because of lower positioning latencies .
1. the prob-k algorithm
　the basic query processing on which this paper builds is the prob-k algorithm developed in . a query scans precomputed index lists  one for each query term  that are sorted in descending order of scores such as tf，idf or bm1 weights of the corresponding term in the documents that contain the term. unlike the original ta method of   prob-k avoids random accesses to index list entries and performs only sequential accesses. the algorithm maintains a pool of candidate results and their corresponding score accumulators in memory  where each candidate is a data item d that has been seen in at least one list and may qualify for the final top-k result based on the following information  we denote the score of data item d in the i-th index list by si d   and we assume for simplicity that the score aggregation is summation :
  the set e d  of evaluated lists where d has been seen 
  the global worstscore d  := pi（e d  si d  of d based on the known scores si d   and
  the global bestscore d  := worstscore d +
pi/（e d  highi that d could possibly still achieve based on the upper bounds highi for the score values in the yet unvisited parts of the index lists.
　the algorithm terminates and is guaranteed to obtain the exact top-k result  when the worstscore of the kth rank in the current top-k result is at least as high as the highest bestscore among all other candidates. the prob-k algorithm implements the candidate pool using a priority queue. in addition  it can optionally replace the conservative threshold test by a probabilistic test for an approximative top-k algorithm with probabilistic precision and recall guarantees. the most flexible implementation uses histograms to cap-
ture the score distributions in the individual index lists and computes convolutions of histograms1 in order to predict the probability that an item d has a global score above some value δ: p pi（e d  si d  + pj/（e d  sj d    δ | sj d  ＋ highj   where sj d  denotes the random variable for the score of d in list j. here  δ is set to the score threshold that the candidate has to exceed  namely to the worstscore of the kth rank in the current top-k result  coined min-k . when this probability for some candidate document d drops below a threshold   e.g.  set to 1   d is discarded from the candidate set. the special case corresponds to the conservative threshold algorithm.
　figure 1 shows pseudo code for this algorithm. the candidates and their accumulators are kept in a hash table  whereas only a small number of top candidates has to be organized in a priority queue in order to maintain the threshold condition for algorithm termination  using their bestscore values as priorities. the probabilistic pruning test is performed only every b index scan steps  where b is chosen in the order of a few hundred. at these points the priority queue is rebuilt  i.e.  all candidates' bestscore values are updated taking the current highi values into account.
1. dynamic query expansion
1 thesaurus-based expansion terms
　we generate potential expansion terms for queries using a thesaurus based on wordnet . wordnet concepts  i.e.  word senses  and their hypernymy/hyponymy relationships are represented as a graph. the edges in the graph are weighted by precomputing statistical correlation measures on large text corpora  e.g.  the various trec corpora . more specifically  we compute the dice coefficient for each pair of adjacent nodes  counting a document as a algorithm 1 prob-k query q =  t1 ... tm  :
1: top-k :=  ; candidates :=  ; min-k := 1;
1: //round-robin schedule for sorted accesses
1: for all index lists li with i=1..m do1:si d  := li.getnext  ; //next sorted access to li1:e d  := e d  “ {i};1:highi := si d ;1:worstscore d  := pi（e d  si;1:bestscore d  := worstscore d  + pi/（e d  highi;1:if  worstscore d    min-k  then1:replace min{worstscore d1 |d1 （ top-k} by d;1:remove d from candidates;1:else if  bestscore d    min-k  then1:candidates := candidates “ d;1:else1:drop d from candidates if present;1:end if1:min-k := min{worstscore d1 |d1 （ top-k};1:if  scandepth % b == 1  then1:for all d1 （ candidates do1:update bestscore d1  using current highi;1:if  bestscore d1  ＋ min-kor then
1:drop d1 from candidates;1:end if1:end for1:end if1:if  candidates =   or max{bestscore d1 |d1 （ candidates} ＋ min-k  then1:return top-k;1:end if1: end for
co-occurrence of the two concepts  if it contains at least one term or phrase denoting a concept from each of the two sets of synonyms  synsets  that wordnet provides for the concepts. for non-adjacent nodes the similarity weight is computed on demand by multiplying edge weights along the best connecting path  using a variant of dijkstra's shortest path algorithm.
　a query term t is mapped onto a wordnet concept c by comparing the textual context of the query term  i.e.  the description of the query topic or the summaries of the top-1 results of the original query  against the synsets and glosses  i.e.  short descriptions  of concept c and its hyponyms  and optionally siblings in the graph . this comparison computes a cosine similarity between the two contexts con t  and con c  for each potential target concept c whose synset contains t. the mapping uses a simple form of word sense disambiguation by choosing the concept with the highest cosine similarity. here con t  and con c  are basically bags of words  but we also extract noun phrases  e.g.   fiber optics  or  crime organization   and include them as features in the similarity computation by forming 1-grams and 1-grams and looking them up in wordnet's synsets. once we have mapped a query term t onto its most likely concept c  we generate a set exp t  of potential expansion terms. each tj （ exp t  has a weight sim t tj  set to the dice coefficient for adjacent concepts c and cj  or 1 for the special case of t and tj being synonyms  or an aggregation of edge weights on the shortest connecting path between c and cj as described above.
1 incremental merge algorithm
　at query run-time  for a query with terms t1 ...tm we first look up the potential expansion terms tij （ exp ti  for each ti with sim ti tij    θ  where θ is a fine-tuning threshold for limiting exp ti . note that θ merely defines an upper bound for the number of expansion terms; in contrast to a static expansion  our algorithm does not necessarily exhaust the full set  such that θ could even be set to 1 in the incremental
expansion setup.
　the top-k algorithm is extended such that it now merges multiple index lists lij in descending order of the combined score that results from the local score sij d  of an expansion term tij in a document d and the thesaurus-based similarity sim ti tij . moreover  to reduce the danger of topic drift  we use the following modified score aggregation for a query t1 ...tm that counts only the best match for any one of the expansion terms per document:

with analogous formulations for the worstscore d  and bestscore d  bounds.
　the incremental merge algorithm provides sorted access to a  virtual  index list that results from merging a set of stored index lists in descending order of the combined sim ti tij  ， sij d  values. the index lists for the expansion terms for a given query term are merged on demand  hence  incrementally  until the min-k threshold termination is reached  by using the following scheduling procedure for the index scan steps: the next scan step is always performed on the list lij with the currently highest value of sim ti tij  ， highij  where highij is the last score seen in the index scan  i.e.  the upper bound for the unvisited part of the list . this procedure guarantees that index entries are consumed in exactly the right order of descending sim ti tij ，sij d  products. index lists for potential expansion terms are opened and added to the set of active expansions activeexp ti  for term ti  only if they are beneficial for identifying a top-k candidate. often the scan depth on these lists is much smaller than on the index lists for the original query terms  and for many terms in exp ti  the index scans do not have to be opened at all  i.e.  activeexp ti    exp ti . the precomputed sim ti tij  values are stored in a metaindex table which is fairly small and typically fits into main memory  e.g.  all wordnet nouns together with their initial high scores in the inverted lists . the sim ti ti  is defined to yield the maximum similarity value of 1  such that the incremental merge scans are initialized on the index lists for the original query conditions ti first. the meta-index also contains the maximum possible scores of all expansion candidates  i.e.  the initial highij values at the start of the query execution. this way  we are in a position to open the scans on the expansion-term index lists as late as possible  namely  when we actually want to fetch the first index entry from such a list. fig. 1 gives pseudo code for the incremental merge algorithm. fig. 1 illustrates the merged index scans for an expansion exp t .
　when we want to employ probabilistic pruning based on score predictors  we face an additional difficulty with query expansion using the incremental merge technique. before computing the convolution over the still unknown scores for a subset of the original query's terms  we need to consider the possible expansions for a query term. for term ti with a random variable si capturing the score distribution in the not yet visited part  we are interested in the probability p si ＋ x|si ＋ highi . with expansion into exp ti  = {ti1 ... tip}  this becomes p max{simi1，si1 ... simip，sip}
＋ x | si1 ＋ highi1 … ，，， … sip ＋ highip   where simij is shorthand for sim ti tij . as the individual factors of this product are captured by the precomputed histograms algorithm 1 incrementalmerge query q =  t1 ... tm   meta-index with exp t1  ... exp tm  and sim ti tij  :

1: top-k :=  ; candidates :=  ; min-k := 1; 1: //initialize active expansions
1: for all ti with i=1..m do
1:	activeexp ti  := {ti};
1:	nextexp ti  := tij （  exp ti  ti  with maxtij{sim ti tij };
1:	nextexphigh ti  := sim ti nextexp ti   ， highi nextexp ti ;
1: end for
1: //round-robin schedule for sorted accesses
1: for all i=1..m do
1:	best := tij （ activeexp ti 
1:	with maxtij{sim ti tij  ， highij};
1:	if  sim ti best  ， highbest   nextexphigh ti   then
1:	best := nextexp ti ;
1:	activeexp ti  := activeexp ti  “ {best};
1:	nextexp ti  := tij （  exp ti    best 
1:	with maxtij{sim ti tij  ， highij};
1:	nextexphigh ti  :=
1: sim ti nextexp ti   ， highi nextexp ti ; 1: end if
1: //next sorted access to lbest 1: si d  := sim ti best  ， lbest.getnext  ;
1:	//continue as in prob-k algorithm
1: end for

per index list  with the constant sim ti tij  coefficients simply resulting in a proportionally adjusted distribution   we can easily derive a new combined histogram for the maxdistribution. this meta histogram construction is performed prior to query execution  and it is updated whenever a new scan on another index list is opened with linear costs in the number of histogram cells and the current number of active query expansions. then this meta histogram is fed as input into the convolution with other  meta  histograms for the original query terms  or their expansion sets . in our experiments the overhead for this dynamic histogram construction was negligible compared to the costs saved in physical disk accesses.

figure 1: example schedule for incremental merge
1. phrase matching
　for phrase matching and computing phrase scores  we cannot directly apply the algorithms described so far  because it is rarely feasible to identify potentially query-relevant phrases in advance and create inverted index lists for entire phrases. we store term positions in a separate index  not in the inverted index lists to keep the inverted index as compact as possible and minimize disk i/o when scanning these lists. thus  testing the adjacency condition  or other kinds of proximity condition and computing an additional proximity factor for the overall score contribution  requires random i/o. we therefore treat the adjacency tests as expensive predicates in the sense of  and postpone their evaluation as much as possible.
　in our approach we assume that the score for a phrase match of a phrase t1 ...tp is the sum of the scores for the individual words  possibly normalized by the phrase length.
the key idea to minimizing the random i/os for adjacency tests is to eliminate many candidates based on their upper score bounds  or estimated quantile of their scores  early before checking for adjacency. consider a phrase t1 ...tp and suppose we have seen the same document d in the index lists for a subset of the phrase's words  say in the lists for t1 ...tj  j   p . at this point we know the bestscore1 d  with regard to this phrase match alone  which is part of a broader query   and we can compare this bestscore1 d  or the total bestscore d  for the complete query against the worstscore d1  of other candidates d1 that have been collected so far. in many cases we can eliminate d from the candidate pool without ever performing the adjacency test. even if we have seen d in all p index lists for the phrase's words  we may still consider postponing the adjacency test further  as we gain more information about the total bestscore d  for the entire query  not just the phrase  and the evolving min-k threshold of the current top-k documents.
　this consideration is implemented using a top-k algorithm for both the phrase matching and the entire query  leading to a notion of nested top-k operators that dynamically combine the index lists for each of several phrases of the full query. as indicated by the example expansion fig. 1  this method allows a very fine grained modeling of semantic similarities directly in the query processing. the algorithm works by running outer and inner top-k operators in separate threads  with inner operators periodically reporting their currently best candidates along with the corresponding  worstscorei d   bestscorei d   intervals. then the aggregated score interval of a candidate d at each operator simply becomes the sum of the score interval boundaries propagated by each of the subordinate operators  pi=1..m worstscorei d   pi=1..m bestscorei d   for a top-k join or the maximum of the respective interval boundaries  maxi=1..m worstscorei d   maxi=1..m bestscorei d   for an incremental merge join which remains a monotonous score aggregation at any level of the operator tree. at the synchronization points  the outer operator integrates the reported information into its own candidate bookkeeping and considers pruning candidates from the outer priority queue. this technique also allows a lazy scheduling of random i/os for phrase tests by the top-level operator only which can further decrease the amount of phrase tests by an order of magnitude for large phrase expansions.
1. experimental evaluation
　the presented algorithms are implemented in a java prototype using jdk 1. inverted index lists are stored as tables with appropriate b+-tree indexes in an oracle 1g database; in addition  a term-to-position index is kept in a second table for phrase matching. the top-k engine accesses the index lists via jdbc  with a large prefetching buffer. index scans are multi-threaded  with periodic thread synchronization and queue updates after every b = 1 index scan steps. all experiments were run on an intel dual xeon with 1 gb ram and a large raid-1 disk array.
1 setup & datasets
　we performed experiments with two different trec data sets and ad-hoc query tasks:  1  the distinct set of 1 queries of the trec 1 robust track marked as hard  explicitly identified by trec  on the aquaint news corpus 

figure 1: phrase matching with multiple nested topk operators
consisting of 1 text documents and a raw data size of 1 gb and yielding about 1 million distinct  term  docid  score  tuples; and  1  the 1 queries of the trec 1 terabyte track on recently crawled web documents of the .gov internet domain  consisting of 1 1 documents and a raw data size of 1 gb and yielding about 1 billion distinct  term  docid  score  tuples.
　for both collections we used a standard okapi bm1  1  1  model with the exact parameters k1 and b set to
1 and 1  respectively. query expansion was based on wordnet concepts and the statistically quantified hypernym/hyponym relations in addition to the concept synsets  see sec. 1 . we used both the benchmark query titles  which are merely 1 to 1 keywords  and the descriptions  which are a few sentences describing the query topic. we mostly focus on results for the 1 hard queries of the robust track data as these have become the gold standard for query expansion; we will discuss results for the terabyte corpus only in sec. 1 on scalability. we compared both efficiency and effectiveness of the following methods:
a  a baseline method without query expansion  with the option of probabilistic candidate pruning and variation of the control parameter   with being the conservative case where all speculative and approximative techniques are disabled  
b  a static expansion method  where we generate a large disjunctive query by adding all expansion terms with similarity to at least one of the original query terms above a threshold θ  with θ = 1 being the special case where only synonyms are added   and
c  the incremental merge method for dynamic on-demand expansion  with an upper bound on the number of expansion terms controlled by θ.
　with static expansion the score aggregation function was the summation of scores for the whole expansion; with incremental merge we used the score aggregation function introduced in sec. 1. for both expansion methods we varied the control parameter  for probabilistic candidate pruning. in addition to these methods under comparison  we also indicate the number of relevant entries of index lists for a dbms-style join&sort query processing  where all inverted index lists that are relevant to a query are fully scanned  however  disregarding random i/os for phrase matching ; this merely serves as a reference point. we measured the following metrics for assessing efficiency and effectiveness:
  the total number of sequential accesses to inverted index lists for all benchmark queries  i.e.  the number of  docid  score  tuples read from the inverted lists 
  the total number of random accesses to the index lists and the position index  for phrase matching  
  the total cpu time for all benchmark queries 
  the maximum memory consumption during the benchmark run  for the candidate pool  probabilistic predictors  etc.  
  the macro average precision 1 for the top-1 result of each query  using the official trec 1 relevance assessments 
  the uninterpolated mean average precision  map  for the top-1 of each query  using separate runs to obtain the top-1 results   and
  the macro average relative precision rprec r1 r1  :=  for the top-1 of each query compared to the conservative algorithm with  a relative overlap measure for the probabilistic pruning .
1 baseline runs
　the first part of tab. 1 shows the results for the baseline run  i.e.  without query expansion. the maximum query dimensionality max m  was 1  the average query length avg m  was 1 terms; there was no consideration of phrases. we see that top-k query processing is generally much more efficient than the join&sort technique could possibly be  in terms of inverted index lists accesses. the rows for the top-k baseline method differ in their settings for the  control parameter. allowing a modest extent of probabilistic pruning led to noticeable performance gains  while it hardly affects effectiveness. this makes the prob-k method an excellent choice for a system where high precision at the top ranks is required and efficiency is crucial.
1 expansion strategies
　the second part of tab. 1 shows the efficiency and effectiveness results for the 1 hard queries of the trec robust track  comparing the incremental merge method with static query expansion. a fixed expansion technique using only synonyms and first-order hyponyms of noun-phrases from titles and descriptions already produced fairly highdimensional queries  with up to 1 terms  many of them marked as phrases ; the average query size was 1 terms.
　compared to the baseline without query expansion  all expansion techniques significantly improved the result quality in terms of precision 1 and map. the quality is still below the very best trec runs  1  1  which achieved about 1 precision 1  but the results are decent. recall that our basis for query expansion  wordnet  is certainly not the most suitable choice for ad-hoc query expansion  at least not unless combined with other sources and techniques   and the emphasis of this paper is on efficiency with good  but not hand-tuned  effectiveness. the best result quality in our experiment  1 precision 1  was achieved by the incremental merge technique with  set to 1. probabilistic pruning with reduced the number of sorted accesses by about 1 percent  but in terms of run-time  in cpu seconds  it gained a factor of 1 as it also incurred fewer random accesses  mostly for phrase matching  and lower memory overhead. for the static expansion technique  the cost savings by the probabilistic pruning were much higher  more than a factor of 1 in all major efficiency metrics  but this came at the expense of a significant loss in query result quality. in terms of run-time  however  both incremental merge and static expansion performed almost equally well when probabilistic pruning was used. the absolute performance numbers of less than 1 seconds per query  with an academic prototype in java and the high overhead of jdbc sessions  are very encouraging. it is particularly remarkable that for the incremental merge method  the probabilistic pruning affected the effectiveness only to a fairly moderate degree  reducing precision 1 from 1 to 1. this seems to be a very low price for a speed-up factor of 1.
　fig. 1 and 1 show how the effectiveness and efficiency measures change as the control parameter  is varied from 1 and 1 towards higher  more aggressive values  with the extreme case 1 corresponding to a fixed amount of indexscan work  looking only at the b = 1 highest-score entries of each index list . we see that the relative precision of most variants drops linearly with   which is just the expected behavior  see    but in terms of objective result quality  as measured by the trec relevance assessments  the pruning technique performed much better: both precision 1 and map decrease only very moderately with increasing . for example  for the best incremental merge method could still achieve a precision 1 of about 1 while its run-time cost  in terms of sorted accesses  was reduced by a factor of more than 1. in fig. 1 and 1 the curves for static expansion with query titles and descriptions reveal that the score predictor degenerates for very high-dimensional disjunctive queries because of neglecting feature correlations. this led to the sudden drop in the number of sorted accesses already for  being as low as 1  but this came at the expense of a significant loss in retrieval quality. this phenomenon did not occur for the incremental merge method  where expansions are grouped into multiple nested top-k operators  such that the maximum number of query dimensions at the top level was only 1 compared to 1 for the static expansion  including phrases .
　fig. 1 and 1 show the efficiency and effectiveness results as a function of varying the θ parameter  i.e.  the aggressiveness of generating candidate terms and phrases for query expansion. tab. 1 gives detailed figures for various combinations of θ and  values. the charts demonstrate the robustness and self-tuning of the incremental merge method: even with very low θ  meaning very aggressive expansion  both precision 1 and map values stayed at a very good level. the execution cost did significantly increase  however  but this is not surprising when you consider that the expansion candidate sets for some queries had up to 1 terms or phrases - quite a stress test for any search engine. in combination with moderate probabilistic pruning  the incremental merge method was still able to answer all 1 queries in a few minutes  less than 1 seconds per query.
1 scalability
max m avg m # seq# randcpu secmemoryp 1maprprecrobust baseline runsjoin&sort
 title only 1.1 1title only11.1 11.1 kb11111.1 11.1 kb111robust fixed expansions
join&sort
 static expansion 1 1 1static expansion11.1 1111 kb11111.1 1111 kb111incr. merge11.1 1111 kb11111.1 1111 kb111terabyte fixed expansions
join&sort
 static expansion 1.1 1static expansion11.1 111.1 1 kb11111.1 111.1 1 kb111incr. merge11.1 111.1 1 kb11111.1 111.1 1 kb111table 1: baseline and fixed expansions for the 1 hard robust and the 1 terabyte queries

	figure 1: precision as a function of 	figure 1: efficiency as a function of 　the evaluation on the terabyte corpus and queries served as a proof of scalability for the developed top-k query processing and expansion methods. the third part of tab. 1 shows the results of some of our runs  using the same fixed expansion technique as in sect. 1 for the robust track. the performance gap for static expansions without vs. the one with probabilistic pruning indicates the same overly aggressive behavior of the score predictor as for the high dimensional queries in the robust setup - however  again at the expense of retrieval quality. the overall efficiency gain in terms of access rates for the incremental merge method compared to the static expansion without probabilistic pruning is even more impressive by a factor of more than 1 and a factor of 1 compared to full scans  respectively  while achieving higher precision 1 and map values. this makes the incremental merge approach the method of choice in terms of both retrieval robustness and efficiency.
1. conclusion
　this paper has presented a suite of novel techniques for efficient query expansion and tested them on the trec robust and terabyte benchmarks. the incremental merge technique clearly outperformed the traditional method of static expansion  and proved that it can achieve decent queryresult quality while exhibiting very good execution cost and run-time behavior. especially in combination with probabilistic score prediction and candidate pruning  it is a very efficient and effective  scalable method that could be of interest for industrial-strength search engines. our emphasis has been on efficiency  so we used only a relatively simple basis for generating expansion terms and phrases. our future work may consider combining our methods with more advanced techniques for expansion  using  for example  document summaries from google top-1 snippets and query associations from query logs. a second line of ongoing and future work is to study more sophisticated scheduling strategies for random accesses to inverted index lists and position indexes. our current strategies all have a greedy-heuristics flavor; a more advanced approach could be to drive the scheduling of sorted and random accesses by considering score distributions and an explicit cost model.
