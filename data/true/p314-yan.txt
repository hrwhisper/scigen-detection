frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets  sequences  and graphs. however  the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability  due to the huge number of patterns generated by the mining process.
¡¡in this paper  we examine how to summarize a collection of itemset patterns using only k representatives  a small number of patterns that a user can handle easily. the k representatives should not only cover most of the frequent patterns but also approximate their supports. a generative model is built to extract and profile these representatives  under which the supports of the patterns can be easily recovered without consulting the original dataset. based on the restoration error  we propose a quality measure function to determine the optimal value of parameter k. polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement. empirical studies indicate that we can obtain compact summarization in real datasets.
categories and subject descriptors: h.1  database
management : database applications - data mining
general terms: algorithms
keywords: frequent pattern  summarization  probabilistic model
1. introduction
¡¡mining frequent patterns is an important data mining problem with broad applications  including association rule mining  indexing  classification  and clustering  see e.g.   1  1  1  1  1  . recent studies on frequent-pattern mining

 this work was supported in part by the u.s. national science foundation nsf iis-1 and iis-1. any opinions  findings  and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect the views of the funding agencies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  chicago  illinois  usa.
copyright 1 acm 1-1-x/1 ...$1.
have seen significant performance improvements on efficient identification of various kinds of patterns  e.g.  itemsets  sequences  and graphs   1  1  1  1  . patterns with other interesting measures were also examined extensively  see e.g.   1  1  1  1  1  . however  the major challenge of frequent-pattern mining is not at the efficiency but at the interpretability: the huge number of frequent patterns makes the patterns themselves difficult to explore  thus hampering the individual and global analysis of discovered patterns.
¡¡there are two sources leading to the interpretability issue. first  the rigid definition of frequent patterns often generates a large number of redundant patterns  most of which are slightly different. a pattern is frequent if and only if it occurs in at least ¦Ò fraction of a dataset. according to this definition  any subset of a frequent itemset is frequent. this downward closure property leads to an explosive number of frequent patterns. for example  a frequent itemset with n items may generate 1n sub-itemsets  all of which are frequent. the introduction of closed frequent itemsets  and maximal frequent itemsets  1  1  can partially alleviate this redundancy problem. a frequent pattern is closed if and only if a super-pattern with the same support does not exist. a frequent pattern is maximal if and only if it does not have a frequent super-pattern. unfortunately  for any pattern ¦Á  as long as there is a small disturbance on the transactions containing ¦Á  it may generate hundreds of subpatterns with different supports. we term the original pattern  ¦Á  a master pattern and its deviated subpatterns  derivative patterns. intuitively  it is more interesting to examine the master patterns  rather than the derivative patterns. unfortunately  there is no clear boundary between master patterns and their derivatives.
¡¡secondly  as long as the number of discovered patterns is beyond tens or hundreds  it becomes difficult for a user to examine them directly. a user-friendly program should present the top-k distinct patterns first and arrange the remaining patterns in a tree structure so that a user can start quickly from a small set of representative patterns. the patterns delivered by the existing top-k mining algorithms such as  are the most frequent closed itemsets  but not distinct ones. users often prefer distinct patterns with little overlap for interactive exploration.
¡¡in this paper  we intend to solve the pattern interpretability issue by summarizing patterns using k representatives. there are three subproblems around this summarization task: what is the format of these representatives  how can we find these representatives  and what is the measure of their quality 
¡¡a number of proposals have been made to construct a concise and lossless representation of frequent patterns. for example  pasquier et al.  introduced the concept of closed frequent patterns  and calders et al.  proposed mining all non-derivable frequent itemsets. these kinds of patterns are concise in the sense that all of the frequent patterns can be derived from them. unfortunately  the number of patterns generated in these two approaches is still too large to handle. researchers have also developed lossy compression methods to summarize frequent patterns: maximal patterns by gunopulos   top-k patterns by han et al.   errortolerant patterns by yang et al.   and condensed pattern bases by pei et al. . nevertheless  the discrimination between patterns is not emphasized in these studies. generally speaking  a user may not only be interested in a small set of patterns  but also patterns that are significantly different. a recent approach proposed by afrati et al.  uses k itemsets to cover a collection of frequent itemsets. their solution is interesting but leaves the support integration issue open: it is unknown how to cover the support information in a summarization. in this paper  we investigate this issue and further advance the summarization concept. given a set i of items o1 ... od and a transaction dataset d = {t1 ... tn}  where each transaction is a subset of i. the pattern collection f is a set of patterns ¦Á1 ... ¦Ám  ¦Ái   i. we are interested in partitioning the pattern set into k groups such that the similarity within each group is maximized and the similarity between the groups is minimized. frequent patterns are distinguished from each other not only because they have different composition  but also because they have different supports. suppose ¦Ái and ¦Áj exhibit strong similarity on these two criteria. it is likely that ¦Ái and ¦Áj can be merged to one pattern  ¦Ái ¡È¦Áj. this is the intuition behind merging two patterns  or clustering them in the same group . we develop a generative model m to measure these two similarity criteria simultaneously. based on this model  we are able to evaluate the quality of such merging by measuring the probability that ¦Ái and ¦Áj are generated by m.
¡¡using the above generative model  we can arrange all of the patterns in a tree structure using a hierarchical agglomerative clustering method  where patterns with the highest similarity are grouped together first. in this hierarchical tree  a user can flexibly explore patterns with different summarization granularity. our methods can successfully compress thousands of frequent patterns into hundreds or even tens of distinct patterns.
our major contributions are outlined as follows.
1. we propose a statistical model which is good not onlyat summarizing patterns  but also at integrating their supports. after compressing numerous patterns to k representatives  our methods are able to recover these patterns and their supports from the k representatives.
1. a principled similarity measure based on kullbackleibler divergence is developed to group highly correlated patterns together. we show that the summarization algorithm based on this measure can complete in polynomial time.
1. we use k representatives to summarize the pattern set and devise a mechanism to estimate the support of frequent patterns from these k representatives only. in addition  the estimation error is used to evaluate the summarization quality. we monitor quality changes over different ks in order to determine the optimal number of representatives for a given pattern set. to the best of our knowledge  ours is the first algorithm that can guide the selection of k  thus eliminating the obstacle for the applicability of pattern summarization.
1. empirical studies indicate that the method can buildvery compact pattern summarization in many real data sets. for example  on a typical mushroom dataset1  the method can summarize thousands of frequent patterns accurately using around 1 patterns.
¡¡the rest of the paper is organized as follows. section 1 introduces the concept of pattern profile for similar frequent patterns. the details of the similarity measure  the quality evaluation function  as well as the summarization algorithms are introduced in section 1. we report our experimental results in section 1  discuss related work in section 1  and conclude our study in section 1.
1. pattern profile
¡¡let i be a set of items o1 o1 ... od. a subset of i is called an itemset. a transaction dataset is a collection of itemsets  d = {t1 ... tn}  where ti   i. for any itemset ¦Á  we write the transactions that contain ¦Á as d¦Á = {ti|¦Á   ti and ti ¡Ê d}.
¡¡definition 1  frequent itemset . for a transaction dataset d  an itemset ¦Á is frequent if   where is called the support of ¦Á in d  written s ¦Á   and ¦Ò is the minimum support threshold  1 ¡Ü ¦Ò ¡Ü 1.
¡¡frequent itemsets have the apriori property: any subset of a frequent itemset is frequent . since the number of subsets of a large frequent itemset is explosive  it is more efficient to mine closed frequent itemsets only.
¡¡definition 1  closed frequent itemset . a frequent itemset ¦Á is closed if there does not exist an itemset ¦Â such that ¦Á   ¦Â and d¦Á = d¦Â.
¡¡figure 1 shows a sample dataset  where the first column represents the transactions and the second the number of transactions. for example  1 transactions have only items a  c  and d; and 1 transactions have only items b  c  and d. there are 1 transactions in d1. if we set the minimum support at 1%  itemset habcdi is frequent  and so are its sub-itemsets. there are 1 frequent itemsets  among which 1 are closed. as one can see  the number of closed frequent itemsets is much less than that of frequent itemsets.
	transaction	number
acd1bcd1abcd1figure 1: d1
¡¡note that in definition 1  for a transaction that contributes to the support of a pattern  it must contain the entire pattern. the rigid definition of closed frequent patterns causes a severe problem. a small disturbance within the transactions may result in hundreds of subpatterns that could have different supports. there exists significant pattern redundancy since many patterns are actually derived from the same pattern. on the other hand  a pattern could be missed if its support is below ¦Ò while its derivative subpatterns pass the threshold. in this situation  it is necessary to assemble small correlated subpatterns together to recover it. if we relax the exact matching criterion in the support definition and allow one item missing in a pattern  habcdi becomes the only pattern in d1.
¡¡inspired by this example  we find that it is very important to group similar itemsets together to eliminate redundant patterns. we can merge the itemsets in each group to a master pattern. a master pattern is the union of all the itemsets within a group. before we formalize the format of pattern summarization  let us first examine what a good summarization means.
¡¡suppose ¦Á and ¦Â can be grouped together to form a master pattern  ¦Á¡È¦Â. that means the supports of ¦Á  ¦Â  and ¦Á¡È¦Â should not be significantly different from each other. or more fundamentally  d¦Á and d¦Â should be highly correlated so that the transactions containing ¦Á will likely contain ¦Â  and vice versa.
	transaction	number
a1bcd1abcd1figure 1: d1
¡¡although the support similarity is one of the major criteria in determining whether we should summarize two patterns into one  there is another measure that determines how good a summarization is. let us compare d1 with another dataset d1 shown in figure 1. for these two datasets  we can summarize all the subpatterns of habcdi as habcdi because their supports are very close to each other. however  the quality of these two summarizations is different. if we allow approximate matching between a pattern and a transaction  pattern habcdi is more likely contained by transactions  abc  in d1  one item missing  than by transactions  a  in d1  three items missing . that means  the summarization of d1 as habcdi has better quality. this intuition will be clarified when we explain our pattern profile model below.
¡¡according to the above discussion  we propose using a probability profile to describe a representative  instead of using an itemset only. the profile has a probability distribution on items.
	definition 1	 bernoulli distribution vector .
let i = {o1 ... od} be a set of items  and xi be a boolean random variable indicating the selection of oi. p x  =  p x1   ... p xd   is a bernoulli distribution vector over d dimensions  where x1  ...  and xd are independent boolean random variables.
suppose patterns ¦Á1 ¦Á1 ... ¦Ál are grouped together to form a master pattern ¦Á1 ¡È ¦Á1 ¡È ... ¡È ¦Ál. we can estimate the distribution vector that generates the dataset d¡ä = sli=1 d¦Ái.
d
	p d¡ä|¦È  = y y p xi = tij  	 1 
tj¡Êd¡ä i=1
where tij is the value of xi in the jth transaction and ¦È is a set of probability {p xi }. when tij = 1  it means that the jth transaction has item oi.
¡¡according to maximum likelihood estimation  mle   the  best  generative model should maximize the log likelihood l ¦È|d¡ä  = logp d¡ä|¦È   which leads to
	= 1	 1 
the well-known result is
	.	 1 
that is  p xi = 1  is the relative frequency of item oi in d¡ä.
¡¡we use a probability distribution vector derived from mle to describe the item distribution in a set of transactions. here we formulate the concept of pattern profile.
¡¡definition 1  pattern profile . let ¦Á1 ¦Á1 ... ¦Ál be a set of patterns and d¡ä = si d¦Ái. a profile m over ¦Á1 ¦Á1 ...  and ¦Ál is a triple hp ¦Õ ¦Ñi. p is a probability distribution vector learned through eq.  1 . pattern
is taken as the master pattern of is regarded as the support of the profile  also written as s m .
abcdd1.1.1.1.1d1.1.1.1.1table 1: pattern profile
¡¡if we want to build a pattern profile for habcdi in d1 and d1  we can derive the distribution vectors for the sample datasets in figures 1 and 1. the resulting profiles are shown in table 1. for example  1. table
1 demonstrates that the profile derived from d1 has higher quality since it is much closer to a perfect profile  a profile with p xi = 1  = 1 for oi ¡Ê ¦Õ. in addition  without accessing the original dataset  we can conclude that hbcdi in d1 is more frequent than the other size-1 subpatterns of habcdi. pattern profile actually provides more information than the master pattern itself; it encodes the distribution of subpatterns. the key difference between our profile model and the model proposed by afrati et al.  is that our profile model can accommodate the patterns themselves as well as their supports.
¡¡the support of a pattern in a dataset d can be regarded as the average probability of observing a pattern from a
transaction 
	p ¦Á 	= x p ¦Á|t    p t  
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡t¡Êd where and p ¦Á|t  = 1 if ¦Á   t  1 otherwise.
¡¡in our profile model  we can regard the probability of observing a pattern as the probability that the pattern is generated by its corresponding profile times the probability of observing this profile from a transaction 
             p ¦Á|t  ¡« p ¦Á|m    p m|t    1  where we assume the conditional independence p ¦Á|m t  = p ¦Á|m . according to this model  we can estimate the support for a given pattern ¦Á from the profile it belongs to.
¡¡definition 1  estimated support . let m be a profile over a set of patterns {¦Á1 ¦Á1 ... ¦Ál}. the estimated support of ¦Ák is written as s  ¦Ák  
	s  ¦Ák  = s m  ¡Á y p xi = 1  	 1 
oi¡Ê¦Ák
where  p is the distribution vector of m and xi is the boolean random variable indicating the selection of item oi in pattern ¦Ák.
¡¡surprisingly  the calculation of an estimated support is only involved with d + 1 real values: the d-dimensional distribution vector of a profile and the number of transactions that support the profile. this result becomes one of the most distinguishing features in our summarization model. it means that we can use very limited information in a profile to recover the supports of a rather large set of patterns.
1. pattern summarization
¡¡our pattern profile model shows how to represent a set of patterns in a compact way and how to recover their supports without accessing the original dataset. however  the problem of selecting a set of similar patterns for summarization is not yet solved. we formalize this summarization problem as follows.
¡¡definition 1  pattern summarization . given a set of patterns f = {¦Á1 ¦Á1 ... ¦Ám} that are mined from a database d = {t1 t1 ... tn}  pattern summarization is to find k pattern profiles based on the pattern set f.
a potential solution to the summarization problem is to group frequent patterns into several clusters such that the similarity within clusters is maximized and the similarity between clusters is minimized. once the clustering is done  we can calculate a profile for each cluster.
¡¡we can construct a specific profile for each pattern that only contains that pattern itself. using this representation  we can measure the distance between two patterns based on the divergence between their profiles. the distance between two patterns should reflect the correlation between the transactions that support these two patterns. namely  if two patterns ¦Á and ¦Â are correlated  d¦Á and d¦Â likely have large overlap; and the non-overlapping parts exhibit high similarity. several measures are available to fulfill this requirement. a well-known one is the kullback-leibler divergence between the distribution vectors in the profiles of ¦Á  m¦Á  and ¦Â  m¦Â  
d
	kl p 	 1 
i 1
where p is the distribution vector of m¦Á and q is the distribution vector of m¦Â.
¡¡when p xi  and q xi  have zero probability  kl p||q  = ¡Þ. in order to avoid this situation  we smooth the probability of p xi   and q xi   with a background prior  p¡ä xi  = ¦Ëu +  1   ¦Ë p xi  
where ¦Ë is a smoothing parameter  1   ¦Ë   1  and u could be the background distribution of item oi.
¡¡when kl p||q  is small  it means that the two distribution vectors p and q are similar  and vice versa. this could be justified by taylor's formula with remainder  kl 

where ¦È = p xi = 1  and ¦Ç = q xi = 1 . when 1   ¦È   1 and 1   ¦Ç   1  it implies that
kl p xi ||q xi         p xi  ¡« q xi .
¡¡note that for any oi ¡Ê ¦Á  p xi = 1  = 1 and for any oi ¡Ê ¦Â  q xi = 1  = 1  after smoothing  both of them are close to 1  but not equal to 1 . when two distribution vectors p and q of patterns ¦Á and ¦Â are similar  transactions containing ¦Á will likely contain ¦Â too  and vice versa. it indicates that these two patterns are strongly correlated  i.e.  p xi  ¡« q xi    d¦Á ¡« d¦Â. likely  patterns ¦Á and ¦Â are derivative patterns of the same pattern ¦Á ¡È ¦Â. therefore  kl-divergence can serve as a distance measure for the pattern summarization task. this conclusion can further be justified by the connection of kl-divergence with the following generative model.
¡¡given several profiles {m¦Á}  and a pattern ¦Â  we have to decide how likely ¦Â is generated by a profile m¦Á  or how likely d¦Â is generated by m¦Á. if d¦Â is generated by m¦Á  then we cannot tell the difference between d¦Â and the transactions d¦Á covered by m¦Á. in this case  we can put patterns ¦Á and ¦Â in one cluster. the best profile in {m¦Á} should maximize
d
p d¦Â|m¦Á  = y y p xi = tij  
tj¡Êd¦Â i=1
where p is the distribution vector of m¦Á. it is equivalent to maximizing l m¦Á  = logp d¦Â|m¦Á . let q be the distribution vector of pattern ¦Â.
l m
 1 
where n = |d¦Â| and ni is the number of transactions  in d¦Â  having item oi. we may add the entropy of q 
d
h q x   =  x x q xi logq xi  
i=1 xi¡Ê{1}
on the left and right side of eq.  1 . hence 
l 

	=  kl q x ||p x  	 1 
¡¡according to eq.  1   the best profile maximizing p d¦Â|m¦Á  is the profile that minimizes kl q x ||p x  . that means that kl-divergence can measure how likely a pattern is generated from a profile  indicating that it is a reasonable distance measure for grouping profiles.
¡¡in the rest of this section  we will introduce our summarization methods based on hierarchical clustering and k-means clustering  as well as the potential optimization heuristics. we are not going to explore the details of clustering algorithms  since essentially any clustering algorithm based on kl-divergence can be used. we will focus on the practical issues raised by pattern summarization.
1 hierarchical agglomerative clustering
¡¡hierarchical clustering produces a dendrogram where two clusters are merged together at each level. the dendrogram allows a user to explore the pattern space in a top-down manner and provides a global view of patterns.
algorithm 1 pattern summarization: hierarchical clus-
tering
input: transaction dataset d 
¡¡¡¡¡¡pattern set f = {¦Á1 ... ¦Ám}  number of representatives k  output: a set of pattern profiles mc1 ... mck.
1: initialize k = m clusters  each of which has one pattern;
1: compute the pairwise kl divergence among c1 ... ck  dij = kl mci||mcj ;
1: while  k   k 
1:	select dst such that s t = argmini jdij;
1:	merge clusters cs and ct to a new cluster c;
1:	dc = dcs ¡È dct;
1:	ic = ics ¡È ict;
1:	update the profile of c over dc by eq.  1 ;
1:	calculate the kl-divergence between c and the remaining clusters;
1:	k = k   1;
1: return;

¡¡algorithm 1 outlines the pattern summarization process using a hierarchical clustering approach. at the beginning  it calculates the kl-divergence between any pair of patterns. in the following iterations  it repeats lines 1 by selecting the two clusters which have the smallest kl-divergence  line 1  and merging them into one cluster. the iteration procedure terminates when the total number of clusters becomes k.
¡¡a cluster c is defined as a collection of patterns  c   f. the set of transactions that contain a pattern in c is written as dc = ¡È¦Ád¦Á ¦Á ¡Ê c and the master pattern in c is written as ic = ¡È¦Á¦Á ¦Á ¡Ê c. the newly merged cluster inherits the transactions that support the original clusters and the patterns that are owned by the original clusters. it has a newly built profile over the merged transactions  line
1 .
¡¡the profiling construction has to scan the dataset once  thus taking o nd  for each merge operation  where n is the number of transactions in d  and d is the size of the global itemset i. the initial kl-divergence construction  line 1  takes o m1d   where m is the number of patterns. for each cluster ci  we can maintain a distance list between ci and other clusters and sort them in increasing order. whenever a new cluster c is generated  the two merged clusters are deleted from the distance lists in time o m . a new distance list is created for c and sorted in time o mlogm . note that we need not insert the distance from c in the existing distance lists. the minimum kl-divergence can be found by checking the first element in o m  distance lists. therefore  hierarchical clustering itself can be done in o m1logm . hence  algorithm 1 can finish in o m1logm+m1d+mnd .
¡¡when the dataset is very large  it may be expensive to recompute the profile by scanning the whole dataset. fortunately  it is effective to profile a new cluster through sampling based on hoeffding bound .
1 k-means clustering
¡¡one major computation cost in hierarchical clustering is the pair-wise kl-divergence calculation. in each iteration  algorithm 1 has to calculate o m  times of kl-divergence. in total  algorithm 1 has to do o m1  kl-divergence computation. one approach to eliminate the quadratic cost is to adopt k-means clustering. using k-means  we can achieve very fast clustering for a large number of patterns.
algorithm 1 pattern summarization: k-means
input: transaction dataset d 
pattern set f = {¦Á1 ... ¦Ám} 
¡¡¡¡¡¡number of representatives k  output: a set of pattern profiles mc1 ... mck.
1: randomly select k patterns as the initial clusters;
1: for each pattern ¦Á do assign its membership to the cluster that has the smallest kl-divergence kl m¦Á||mcj ;
1: update the profiles of newly formed clusters by eq.  1 ;
1: repeat lines 1 until small change in mc1 ...mck or the summarization quality does not increase;
1: return;

¡¡algorithm 1 outlines the major steps of the k-means algorithm. in the initial step  algorithm 1 randomly selects k patterns as the initial cluster centers. in the following iterations  it reassigns patterns to clusters according to the kl-divergence criterion. the profiles of newly formed clusters are then updated  line 1 algorithm 1 . this procedure will terminate until there is only small change in mc1 ...  and mck or it meets other stop conditions  e.g.  the summarization quality does not increase any more  see section
1 .
¡¡conceptually  algorithm 1 is similar to distributional clustering  and divisive clustering . the first difference is that we treat each dimension as a separate distribution while other approaches put all dimensions together and create a multinomial model. the second difference is that our quality evaluation function  see section 1  is different from the mutual information loss function proposed by dhillon et al. . the third difference is that distributional clustering or divisive clustering mixes the profiles directly by assigning equal weight to each instance  while we prefer to recompute the profiles through the original dataset. in the experiment section  we will illustrate the performance difference between these two strategies.
¡¡the time complexity of algorithm 1 is o  mkd+knd r   where m is the number of patterns  k is the number of clusters  d is the number of distinct items  n is the number of transactions  and r is the number of iterations. generally kmeans clustering can complete clustering faster than hierarchical clustering. however  it cannot provide a hierarchical clustering tree for pattern navigation. another drawback of k-means is that its output is highly related with the seed selection  line 1  algorithm 1   which is undesirable in some applications.
1 optimization heuristics
¡¡we develop two optimization heuristics to speed up the clustering process.
1.1 closed itemsets vs. frequent itemsets
¡¡we can start the clustering process either from closed frequent itemsets or from frequent itemsets. the following lemma shows that either way generates the same result.
¡¡lemma 1. given patterns ¦Á and ¦Â  if ¦Á   ¦Â and their supports are equal  then kl m¦Â||m¦Á =kl m¦Á||m¦Â  = 1.
proof. if ¦Á   ¦Â and s ¦Á  = s ¦Â   then d¦Á = d¦Â  which leads to the above lemma.
¡¡any frequent itemset must have a corresponding closed frequent itemset. according to lemma 1  their profiles have zero kl-divergence  indicating that the clustering based on frequent itemsets will be the same as the clustering based on closed frequent itemsets. therefore  we can summarize closed frequent itemsets instead of frequent itemsets. since the number of closed frequent itemsets is usually less than that of frequent itemsets  summarizing closed itemsets can significantly reduce the number of patterns involved in clustering  thus improving efficiency.
1.1 approximate profiles
¡¡another issue is whether we should rebuild the profile from scratch as algorithm 1  line 1  and algorithm 1  line 1  do. the profile updating dominates the computation in both algorithms since it has to access the original dataset. a potential solution is to mix two profiles directly without checking the original dataset in algorithm 1 
	p 	 1 
or weigh each pattern's profile equally in algorithm 1 
	p.	 1 
¡¡this approximation can significantly improve clustering efficiency. however  it may affect the summarization quality since the mixed profile may no longer reflect the real distribution. traditional clustering algorithms usually assume the instances are sampled independently  i.i.d . however  in our case  the i.i.d. assumption for frequent patterns is not valid. most of the frequent patterns are not independent at all. it may be incorrect to have an equal weight for each pattern or weigh two clusters according to their sizes.
1 quality evaluation
¡¡one way to evaluate the quality of a profile is to calculate the probability of generating its master pattern from its profile  p ¦Õ  = qoi¡Ê¦Õ p xi = 1 . the closer p ¦Õ  to 1  the better the quality. because the master patterns from different profiles have different sizes  it is pretty hard to combine multiple p ¦Õ s to give a global quality assessment. we are going to examine an additional measure in this section.
¡¡through eq.  1  in section 1  we show that the support of a pattern covered by one profile can be estimated through its distribution vector directly. a high quality profile should have this estimation as close as possible to the real support. when we apply this measure to a set of profiles  we encounter an ambiguity issue since one pattern may have different estimated supports according to different profiles. suppose we are given minimum information: a set of profiles  each of which is a triple hdistribution vector  master pattern  supporti. the information about which pattern belongs to which profile is not given. for any pattern ¦Á  it could be a subset of several master patterns. in this situation  we may get multiple support estimations for ¦Á. which one should we select  a simple strategy is to select the maximum one 
	s  ¦Ák  = maxm s m  ¡Á y pm xi = 1 .	 1 
oi¡Ê¦Ák
this strategy is consistent with the support recovery for a frequent pattern given a set of closed frequent patterns. let f be a set of closed frequent patterns. for any frequent pattern ¦Á  its support is the same as the maximum support of its super-pattern ¦Â  ¦Á   ¦Â and ¦Â ¡Ê f.
¡¡definition 1  restoration error . given a set of profiles m1 ... mk and a testing pattern set t = {¦Á1 ¦Á1  ... ¦Ál}  the quality of a pattern summarization can be evaluated by the following average relative error  called restoration error 
	.	 1 
¡¡restoration error measures the average relative error between the estimated support of a pattern and its real support. if this measure is small enough  it means that the estimated support of a pattern is quite close to its real support. a profile with small restoration error can provide very accurate support estimation.
¡¡the measure in the above definition is determined by the testing pattern set. we may choose the original patterns  which have been summarized into k master patterns  as the testing case. we can also assess the quality over the itemsets that are estimated to be frequent  i.e. 
	 	 1 
where t¡ä is the collection of the itemsets generated by the master patterns in profiles and  s ¦Ák  ¡Ý ¦Ò.
¡¡the measure j tests  frequent patterns   some of which may be estimated as  infrequent   while jc tests  estimated frequent patterns   some of which are actually  infrequent . therefore  these two measures are complementary to each other. as long as jc is relatively small  we can obtain the support of a generated itemset with high accuracy and determine whether it is frequent or not. the following lemma shows that if we summarize closed frequent itemsets using k profiles  the subsets generated by the master patterns in these k profiles will cover all of the frequent itemsets.
¡¡lemma 1. let {m1 ... mk} be a set of profiles learned over a collection of closed frequent itemsets {¦Á1 ... ¦Ám} using hierarchical clustering or k-means clustering. for any frequent itemset ¦Ð  there must exist a profile mk such that ¦Ð   ¦Õk  where ¦Õk is the master itemset of mk.
proof. for any frequent itemset ¦Ð  there exists a closed frequent itemset ¦Ái such that ¦Ð   ¦Ái. ¦Ái must belong to one cluster  say mk. hence  ¦Ái   ¦Õk  where ¦Õk is the master itemset of mk. therefore  ¦Ð   ¦Õk.
1 optimal number of profiles
¡¡the summarization quality is related to the setting of k  i.e.  the number of profiles. a smaller k is always preferred. nevertheless  when the summarization is too coarse  it may not provide any valuable information. in order to determine the optimal number of profiles  we can apply a constraint on the summarization result. for example  for any profile m =  p ¦Õ ¦Ñ   we require p xi  ¡Ý 1 for any i such that oi ¡Ê ¦Õ. the optimal number is the smallest k that does not violate this constraint. in this section  we examine the summarization quality change to determine the optimal value of
k.
¡¡when two distribution vectors p and q calculated from patterns ¦Á and ¦Â are close to each other  transactions containing ¦Á will likely contain ¦Â too  and vice versa. thus  d¦Á is similar to d¦Â and d¦Á ¡Èd¦Â is similar to both d¦Á and d¦Â. let r be the probability distribution vector over d¦Á ¡È d¦Â. when p and q are close  the mixture r will be close to them too. therefore  the support estimation of any pattern according to eq.  1  will not change much when we merge ¦Á and ¦Â. it implies that the merge of two similar profiles will not significantly change the summarization quality.
¡¡on the other hand  if a clustering algorithm has to merge two profiles m¦Á and m¦Â that have a large kl-divergence  it may dramatically change the estimated support of a given pattern. therefore  when we gradually decrease the value of k  we will observe the deterioration of the summarization quality. by checking the derivative of the quality over k 
 
we can find the optimal value of k practically: if j increases suddenly from k  to k  1  k  is likely to be a good choice for the optimal number of profiles.
¡¡figure 1 shows the summarization quality along the number of profiles for a real dataset  mushroom. the support threshold is set at 1%. we use hierarchical clustering to summarize this pattern set. the curve indicates that there are three optimal candidates to choose: 1  1  and 1. a user can select one of them for examination based on their
need on the summarization quality. figure 1 shows the quality change along the number of profiles. the derivative of j clearly indicates three huge quality changes.

figure 1: mushroom 1%: j

figure 1: mushroom 1%: ¡÷j
1. empirical study
¡¡in this section  we provide the empirical evaluation for the effectiveness of our summarization algorithm. we use two kinds of datasets in our experiments: three real datasets and a series of synthetic datasets. the clustering algorithms are implemented in visual c++. all of our experiments are performed on a 1ghz  1gb-memory  intel pc running windows xp.
1 real datasets
mushroom. the first dataset  mushroom  is available in the machine learning repository of uc irvine. we obtained a variant from fimi repository. this dataset consists of 1 hypothetical mushroom samples with 1 distinct features. each sample has 1 features. a support threshold of 1% was used to collect 1 closed frequent patterns  1frequent itemsets .
¡¡figure 1 shows the average restoration error over the closed frequent patterns  j  and the average restoration error over the frequent itemsets generated by the resulting profiles  jc . the two restoration errors j and jc are quite close. this indicates that we can use the k representative profiles to properly estimate the supports of the original closed patterns as well as the supports of the patterns not in the original set but derivable from the profiles. we also examined the standard deviation of the two restoration errors and found they are pretty close to j or jc. from this aspect  our sum-

figure 1: mushroom: hierarchical clustering
marization method is stable and accurate in the restoration of the patterns and their supports.

figure 1: mushroom
¡¡figure 1 shows the average restoration error  j  over hierarchical clustering and k-means clustering with or without applying the profile approximation heuristics. in kmeansapx and hierarchical-apx  we do not go back to the dataset to rebuild the cluster profilers. instead  we directly interpolate the existing profiles as described in section 1.1. overall  the 1 closed patterns can be successfully summarized into 1 profiles with good quality - the average restoration error at 1 profiles is less than 1. in other words  the error of estimating the support for a pattern is less than 1% of that pattern's real support  and even as low as 1% when we summarize them into 1 profiles or over. bms-webview1. the second dataset  bms-webview1  is a web click-stream dataset from a web retailer company: gazelle.com. the data was provided by blue martini software . in this experiment  we set the minimum support threshold at 1% and got 1 closed itemsets. bmswebview1 is completely different from the mushroom dataset. it consists of many small frequent itemsets over a large set of items  itemsets of size 1 make up 1% of the total 1 patterns versus 1% for mushroom   which makes the summarization more difficult.
¡¡figure 1 shows the average restoration error over hierarchical clustering and k-means clustering with or without applying the profile approximation heuristics. as shown in the figure  when we use the profile approximation heuristics in k-means  the summarization quality is much worse than that of building the profiles from scratch. the restoration

figure 1: bms-webview1
at k = 1 is 1% for k-means with the profile approximation while it is around 1% for hierarchical clustering or k-means without profile approximation.
¡¡overall  the summarization quality for bms-webview1 patterns is worse than that of mushroom. when we use 1 profiles to summarize the patterns  the restoration error is 1%. the difference is due to the underlying distribution of patterns. there is much redundancy between patterns in mushroom. by examining the 1 closed patterns of mushroom  we can easily identify  rough  groups of patterns  where patterns in each group look very similar in composition and differ in support by only a very small number. intuitively  these patterns can be summarized accurately. in bms-webview1  patterns are much shorter and sparser. apparently  it is not good to put two itemsets into one cluster while they have very little overlap in composition. such pattern distribution can be also explained from the data characteristics  i.e.  the click-stream dataset usually contains random and short user access sessions  where the pattern explosive problem is not as serious as in dense datasets like mushroom.

figure 1: replace
replace. the third dataset  replace  is a program trace dataset collected from the  replace  program  one of siemens programs  which are widely used in software engineering research . we recorded the program call and transition information of 1 correct executions. each type of program calls and transitions is taken as one item. overall  there are 1 kinds of calls and transitions. the frequent patterns mined in this dataset may reveal the normal program execution structures  which can be compared with the abnormal executions for bug isolation.
¡¡we set the minimum support threshold at 1% in this experiment and obtained 1 closed frequent execution structures. figure 1 shows the average restoration error over hierarchical clustering and k-means clustering.
¡¡all methods except k-means clustering with profile approximation achieve good summarization quality. the quality of the three methods is quite close and the restoration error is about 1% when we use 1 clusters. the curves indicate that there are two optimal k values to choose: 1 and 1  since the error decreases significantly at these points compared with their neighboring points. for k-means with profile approximation  we further examine the summarization quality. though the average restoration error does not decrease as we increase the number of profiles  the standard deviation of the error does lower - the standard deviation is 1% at k = 1 versus 1% at k = 1. it means that the summarization quality improves as we use more profiles.
1 synthetic datasets
¡¡in this experiment  we want to study how the underlying distribution in patterns can affect the summarization quality. we used a series of synthetic datasets with different distributions to test it. the synthetic data generator is provided by ibm and is available at http://www.almaden.ibm. com/software/ quest/resources/index.shtml. users can specify parameters like the number of transactions  the number of distinct items  the average number of items in a transaction  etc.  to generate various kinds of data.
¡¡we generated seven transaction datasets  where we vary the number of items to control the distribution of patterns in these datasets. each dataset has 1 transactions  each of which has an average of 1 items. the number of distinct items in each dataset varies from 1  1  up to 1. since it may not be fair to compare the result using a fixed support threshold in these datasets  we intentionally obtained the top-1 frequent closed patterns from each dataset and summarize them into 1 and 1 profiles using hierarchical clustering. figure 1 shows the average restoration error j.

figure 1: synthetic data: hierarchical clustering
¡¡as shown in figure 1  the summarization quality deteriorates as the number of distinct items in the datasets increases. when the number of items is small  the dataset has a dense distribution. there exists much redundancy between patterns. so  they can be summarized with small restoration errors; while for a dataset with a large number of items  the patterns are sparsely distributed. thus  it is harder to summarize them with reasonably good quality. this experiment shows that the summarization quality has close relation with the patterns themselves. this result is also observed in the previous real datasets. dense data with high redundancy can be summarized with good quality while sparse patterns with little overlap cannot be grouped together very well.

figure 1: synthetic data: efficiency
¡¡we also tested the running time of our pattern summarization methods over six synthetic datasets by varying the number of transactions from 1  1 up to 1. a set of about 1 closed patterns is obtained from each dataset using a minimum support of 1%. we tested hierarchical clustering and k-means clustering with or without applying the profile approximation heuristics over these datasets. figure 1 shows the running time. the running time of hierarchical-apx and kmeans-apx does not change with the transaction number because we simply interpolate the profiles as described in section 1.1. the running time of hierarchical clustering and k-means clustering without using profile approximation heuristics increases linearly with the number of transactions. this figure shows that profile approximation can really improve the efficiency.
1. related work
¡¡lossless methods have been proposed to reduce the output size of frequent itemset patterns. pasquier et al.  developed the concept of closed frequent patterns  and calders et al.  proposed mining non-derivable frequent itemsets. these kinds of patterns are concise in the sense that all of the frequent patterns can be derived from these representations. lossy compression methods were also developed in parallel: maximal patterns by gunopulos   error-tolerant patterns by yang et al.  and pei et al.   and top-k patterns by han et al. . these methods can reduce the pattern set size further. for example  in maximal pattern mining  all of the frequent subpatterns are removed so that the resulting pattern set is very compact. besides lossless and lossy methods  other concepts like support envelopes  were also proposed to explore association patterns.
¡¡our pattern approximation model is also related to the probabilistic models developed by pavlov et al.  for query approximations  where frequent patterns and their supports are used to estimate query selectivity. mielik¡§ainen and mannila  proposed an approximation solution based on ordering patterns. the closest work to our study is a novel pattern approximation approach proposed by afrati et al.   which uses k frequent  or border  itemsets to cover a collection of frequent itemsets. their result can be regarded as a generalization of maximal frequent itemsets. in  afrati et al. mentioned the support integration issue: it is unknown how to integrate the support information with the approximation. in this paper  we solved this problem  thus advancing the summarization concept. interestingly  the k representatives mined by our approach can be regarded as a generalization of closed frequent itemsets.
1. conclusions
¡¡we have examined how to summarize a collection of itemset patterns using only k representatives. the summarization will solve the interpretability issue caused by the huge number of frequent patterns. surprisingly  our profile model is able to recover frequent patterns as well as their supports  thus answering the support integration issue raised by afrati et al. . we also solved the problem of determining the optimal value of k by monitoring the change of the support restoration error. empirical studies indicate that we can obtain very compact summarization in real datasets. our approach belongs to a post-mining process; we are working on algorithms that can directly apply our profiling model to the mining process.
