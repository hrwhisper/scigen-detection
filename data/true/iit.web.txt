this year's trec 1 web task incorporated two retrieval tasks into a single set of experiments for known-item retrieval.  we hypothesized that not all retrieval tasks should use the same retrieval approach when a single search entry point is used.  we applied task classifiers on top of traditional web retrieval approaches.  our traditional retrieval is based on fusion of result sets generated by query runs over independent parts of the document structure. our task classifiers combine query term analysis with known information resources and url depth. this approach to task classification shows promise: our classified runs improved overall mrr effectiveness over our traditional retrieval results by ~1%; provided an mrr of .1;  ranked 1% of relevant results in the top 1; correctly ranked the #1result  1% of the time.  1% of the queries performed above the average  and 1%  above the median. 
 
keywords: known-item search  document structure retrieval  query task classification 
introduction 
many years of research have been devoted to examining the question of what is the best retrieval strategy for retrieving information.  this year we explore a variation on the task in which a specific home/named page or known-item is sought after given a query or topic.  our research this year builds on prior knownitem and homepage retrieval techniques by examining the question of whether these two tasks should be treated differently. 
 
basic retrieval work has focused on ranking strategies: for example  some of the most studied algorithms include pdln  pivoted document length normalization    okapi bm1   self-relevance   and language models .  all these ranking strategies try and find better ways to estimate relevance  as do many of the newer language models.  in our tests  bm1 has consistently outperformed the other strategies  so we use it in our experiments. 
 
web retrieval extends basic full-text retrieval by using link and document structures to provide various document representations .  this multi-document representation approach was shown to be effective in the top web track systems at the 1 trec conference.  the basic hypothesis is that content developers use html elements/tags to improve the readability of their documents  thus using that information during the ranking process via multiple document representations will improve effectiveness.  examples of these representations could be title  section headers  anchor text  bold  underlines  comments  referring page anchor text  etc.  we initially focus on title  anchor text  and referring anchor text.   
 
given multiple document representations  the most fitting method of using and combining those representations for a given query becomes a research question.  in recent years  the category of work known as data fusion  or multiple-evidence  describes a range of techniques in information retrieval whereby multiple pieces of information are combined to achieve improvements in retrieval effectiveness.  these pieces of information can take many forms including different query representations  different document representations  and different retrieval strategies used to obtain a measure of relationship between a query and a document.   
 
several researchers have used combinations of different retrieval strategies to varying degrees of success in their systems  1  1 .  belkin et al. examined the effects of combining several different query representations to achieve improvements in effectiveness  1  1 .  lee examined the effect of using different weighting schemes to retrieve different sets of documents using a single query and document representation  and a single retrieval strategy .  fox and shaw examined combination algorithms that increase the score of a 
document based on repeated evidence of its relevance in .   
 
one of the algorithms designed by fox and shaw  combmnz  has proven to be a simple  effective method for combining result sets.  it was used by lee in his fusion experiments  and has become the standard by which newly developed result combination algorithms are judged.  more recent research in the area of meta-search engines has led to the proposal of several new result combination algorithms  1  1  1 .  although these algorithms were shown to be comparable  and on occasion superior  to combmnz  we use the widely-used combmnz for this work  leaving other approaches as a topic of further research. 
 
our traditional web search approach fuses the results from different document structure indices to produce a single ranked list for the known-item task.  the results were fused using linear combinations based on estimated mrr values in order to maximize mutual evidence . 
 
in the next section we describe our basic search approach in more detail.  in the task classification section we present our approach to using task information to improving task and overall system effectiveness.  lastly  we present our experimental results and conclude with future possible research directions. 
1 traditional search approach 
to conduct our research we use the iit retrieval system aire  http://ir.iit.edu/projects/aire.html  .  this system builds a traditional inverted index based on a given document structure s .  for stemming  our system uses conflation classes  instead of a more commonly used stemmer such as porter .  those classes have been modified over the years as problem term variants have been encountered.  additionally  aire uses a generated statistical phrase list  where the statistical phrases were generated with a news collection and idf filtering to reduce the final phrase list size.  phrases are generated via a bi-gram sliding window algorithm and weighted with 1% importance in relation to keyword weighting for retrieval.  basic term weighting uses the okapi bm1  equation 1. 
 
 
¡Ælog    n n +n. 1+ .1           k 1k++1 tf* tf * k k1++1 qtf*qtf      
 
k = k1*  1 b  +b*dl /avdl  
equation 1: okapi bm1  
where:  
  tf = frequency of occurrences of the term in the document 
  qtf = frequency of occurrences of the term in the query 
  dl = document length 
  avdl = average document length 
  n = is the number of documents in the collection 
  n = is the number of documents containing the word 
  k1 = 1 
  b = 1 or 1  we use .1 for full text and .1 for shorter representations  see appendix    	k1 = 1  set to 1 or 1  controls the effect of the query term frequency on the weight. 
 
1 parsing 
we indexed the 1gb .gov collection producing a full-text index  an html title term index  and an anchor text index.  the anchor text index differed from the other indexes  in that an additional mapping stage was required so referencing anchor text data can be linked to the referenced trec document name.  for our experimental layout we first produced a baseline run using bm1  conflation classes  phrases  and full-text indexing  referred to as the  full text  run in the results summarized in table 1 .  
 
1 fusion 
our linear combination consists of the following steps.  first  for each document representation retrieved  the scores are normalized using exponential z-score normalization  as in equation 1.  the advantage of this method is that it preserves all relationships of the values exactly; it does not introduce any potential bias into the data.  the final scores are calculated using combmnz  as in equation 1  where each individual score is biased via weights assigned to the document structure by prior mrr estimates. 
 
 
combmnz = sum individual similarities  * number of nonzero similarities 
 
equation 1: combmnz 
 
norm score d x  =   e  orig score d   - mean x   / stddev x   
 
equation 1: exponential z-score normalization for document d and document representation x 
1 task classification 
to explore our hypothesis  we identify home pages via two techniques.  the first technique uses known information resources and seeks to match those resources to queries.  the second approach classifies queries based on keywords like  homepage   and then uses probability distributions of url length to improve the classification. 
 
1 known-resource matching 
as many of the homepages in the .gov domain are government agencies  we hypothesized that simply pairing queries with homepages by matching names and acronyms of agencies would be effective.  we searched the web for lists of government agencies and their associated acronyms and homepages  choosing http://www.ulib.iupui.edu/subjectareas/gov/docs abbrev.html because it provided all three pieces of information  was reasonably large  and was easy to parse with a simple regular expression.   
 
we matched queries to this parsed list of agency name  acronym  and url tuples using the matching algorithm below.  our system matched 1 of the 1 queries  found the correct homepage for 1 of them  improving our results for 1 queries over our traditional web approach combined with url normalization.  we combined these matching homepages with the final result sets by simply inserting them at rank one.  of the matching queries  1 already had the matched result at rank one in our final fused  url lengthweighted result set and 1 had not previously been found in the top 1 results.  the other 1 queries matched the relevant homepage  so inserting that homepage at the first result instead of its previous lower position in the result set offered an improvement.  in total  mrr was improved by 1.  our complete known-resource matching algorithm is shown in figure 1. 
 
known-resource matching algorithm: 
 
step 1. strip  home    homepage   and  page  from the query. strip  the  if it appears as the first word. 
step 1. if the remaining query is an acronym  any sequence of capital letters and spaces   look it up in the list of acronyms by case-insensitive exact string matching.  else  remove any acronyms that might be present alongside other terms from the query  normalize the spacing in the query  and look it up in the list of agency names by case-insensitive exact string matching. 
step 1. if we found a matching acronym or agency name  convert its url to a canonical form by stripping  http://    www   trailing slashes  etc. and look it up in a list of all the urls in the .gov by case-insensitive exact string matching. 
step 1. if we found a matching acronym  but could not find its corresponding url in the .gov  look for its corresponding url with the last path element stripped off and just the matching acronym as  http://www.acronym.gov  in the .gov 
 
figure 1: known-resource matching algorithm 
1 task classification 
kraaij  westerveld  and hiemstra  previously examined differences in the distributions of url depth  the length of the path in the url  between known home pages  from trec-1 answers  and the wt1g test collection. they showed that these distributions were very different  and that this could be used to improve the ranking of the results for home page queries. thus it appeared that if we would be able to successfully classify queries as either home page queries or something else  named page queries in this case   we should be able to improve the results for the homepage queries.  
 
we used a definition of url depth that was slightly different from the one used by kraaij et al. but confirmed the differences in distributions. we removed from the url the leading parts  including host  domain  port  etc.  up to the path. we then removed trailing occurrences of  index.htm  and  index.html   and counted the number of path elements remaining to determine the url depth. the graph below shows the url depth distribution for the wt1g collection and the correct answers for the trec-1 homepage task. 
table 1: wt1g collection url distributions 
u r l d e p t h d i s t r i b u t i o n 1 1

 
for trec-1 we ran the same analysis against the .gov collection and the now known correct answers  qrels  for both the homepage queries and the named-page queries. the analysis shows that for homepage queries  the same distribution differences can be seen between the correct answers and the collection as a whole that were observed in trec-1. in addition  it shows that the url depth distributions for the named page query results are virtually identical to that of the collection as a whole  and thus no advantage can be gained for named page queries.  
 
 
table 1: gov collection url distribution u r l d e p t h d i s t r i b u t i o n

 
 
to determine if there were other variables we could utilize  we examined last-modified-date  and in-domain and out-domain link information  and found no significant difference in distributions for the correct answers versus the .gov collection as a whole.  
 
to be able to take advantage of the url depth information for home page queries without disturbing the rankings for the named page queries  we attempted to classify the queries into one of these two groups. we created a list of 1 keywords that we believed were good indicators of a home page query. this list includes words like  home    homepage    administration    agency   etc. some of these terms were generic  but many would likely be specific to the gov collection. we parsed the queries looking for these words. our algorithm categorized 1  1%  queries  out of 1 combined  as home page queries.  of those 1  1 were false positives  and 1 were correctly classified.  of the 1 home page queries in the query set  1 did not match any of the criteria in the classifier and were not marked as home page queries  false negatives .  
 
we took the results from the fusion run and modified the scores of the documents for those queries that our classifier marked as home page queries. the algorithm for the score boosting is shown in equation 1: 
 
 
si* = si + ¦Á p di  
equation 1: score boosting formula 
 
where si* is the newly assigned score of document i  si is the original score of document i  after fusion   ¦Á is a constant  di is the url depth of document i  and p di  is the probability that a document would have 
url depth di if it was given that it was a home page. after some experimentation we set the value of ¦Á to 1.  
1 results 
our approach is to build on prior approaches to known-item retrieval.  to that end  we first examined the effectiveness of a full text approach based solely on bm1 ranking.  in the following table we see that our estimated  .1  and actual  .1  effectiveness of this approach can be improved. 
 
we next followed the structured approach that others have shown to be effective by exploiting html structure.  in the second set of experiments we fused the title  anchor  and full text indices with the combmnz algorithm with linear weighting based on estimated mrr values.  the appendix displays the results of those experiments; while we did not have the final qrels  our estimated qrels provided equivalent results to real probabilities.  the overall improvement of using document structure over full text retrieval by using combmnz with mrr linear combinations improved our effectiveness by 1%. 
 
we next examined our use of known resource information to our traditional web based search approach.  by using our known resource information that is based on that task  our mrr is .1.  
 
next  we examined our classification approach over prior web techniques.  with the usage of prior probability factoring with our task classifier  we improved the effectiveness of the system by 1% estimated and 1% actually.  we then examined this effectiveness assuming a perfect classifier  and found that our mrr increased to .1  or an additional 1% improvement. 
 
finally  we examined the improvements of combining both our known resource and url factoring on the overall effectiveness: we found that by combining those approaches our mmr was raised to .1 and with a perfect classifier .1  an improvement of 1% over our fused results and 1% over our full text results.   
 
features run tag description training 
mrr actual 
mrr w/ 
perfect 
classifier & p dist full text iit1wp1 full text using 
statistical phrases weighted at 1  bm1 with b=1 .1 .1 n/a fusion iit1wtaez combmnz fulltext  b=1  title b=1  anchor b=1  using 
z-score and exponential 
normalization .1 .1 n/a fusion  
known-resources iit1sa same fusion  insert known resources with matching names or acronyms at first position .1 .1 .1 fusion   
url length weighting iit1su same fusion  re-weight results using prior probabilities of relevance given url lengths calculated by maximum likelihood of 
training qrels .1 .1 .1 fusion  
 url length weighting  
known-resources iit1sau same fusion  same reweighting based on url length priors  and 
same known-resource 
insertion .1 .1 .1  iit1wp1 iit1wtaez iit1sa iit1su iit1sau mrr .1 .1 .1 .1 .1 in top 1 .1 .1 .1 .1 .1 not found .1 .1 .1 .1 .1 = median .1 .1 .1 .1 .1  = mean .1 .1 .1 .1 .1 table 1: submitted runs 
 	
table 1: submitted runs official evaluation 
our final iit1sau approach for the known-item task was 1% of the time equal or above the median and 1% above the mean score of submitted runs.  additionally  our approach produced the item in the top 1 results 1% of the time and only missed 1% of the results.   
 
these results provide validation of the robustness of our task algorithm; more research needs to be conducted to find other task specific information to determine how that information should be incorporated into the ranking strategy. 
1 preliminary failure analysis 
examining table 1 gives some indication of how each approach performs.  notice for the iit1sa and iit1sau runs that relevant documents are not found for approximately 1% of the queries  and are found in the top ten for 1% of the queries  leaving 1% for which the relevant document is found  but is poorly ranked.  for failure analysis we focus on these runs  as they achieve the highest mrr. 
we examined the queries where the relevant document was not found or poorly ranked  found  but not in the top ten .  for each of these queries from the iit1sa and iit1sau runs  we examined the relevant documents and noted where in the each document the query text was present.  our hope was that this analysis would give us an idea of which of our document representations was most likely to contain query text for queries that performed poorly  and we could use that knowledge to improve our parsers.  this analysis is shown in table 1  where the left side represents queries where the relevant document was poorly ranked  and the right side represents queries for which the relevant document was missing.  
from table 1 we see that for queries where the relevant document is missing or poorly ranked  the query text appears within the relevant document approximately 1% of the time. since the title is most likely to contain the query text  1%   it is reasonable to conclude that our title parser might be failing often on these documents.  query text is also likely to appear in the body of the document  1%   and particularly in the anchor text for queries where the relevant document is missing  1% .  from these results we conclude that our title and body parsers may be at fault and worthy of further examination. 
to examine this further  we attempted to determine where our parsers were failing  paying particular attention to the anchor text and title parsers.  we noticed that although the title parser extracts most of the titles  there are several instances where it fails for various idiosyncratic reasons.  likewise  the anchor text parser also worked fairly well  extracting all the data from within the  a  tags.  however  it did not extract any data where the  href  parameter of the  a  tag was pointing to itself.  for example  when the  #  sign is used with hyperlinks it is usually followed by redirection within the page such as  top  or  back .  however  in some cases  there are hyperlinks with  #  sign followed by meaningful text. in the case of   a href =  #1  u.s.s. monitor  /a    our anchor text parser would ignore  u.s.s. monitor .  by improving the efficiency of both the title and anchor text parsers  we believe the accuracy of each individual result file can be improved  thus improving the accuracy of the final results file. 
we also did some analysis to try and determine whether our fusion process was causing relevant documents to be pushed down in the result set to poor ranks.  to test this  we examined the three result files used in the fusion process: iit1wp1  iit1t np  and iit1a np.  once again we examined two cases: poorly ranked documents and missing documents.  in the final result file there are 1 queries that perform poorly and 1 queries that don't return relevant documents. for each of these queries  we calculated the distribution of relevant documents over each individual result file. in addition  we computed the percentage of relevant documents whose ranks occurred in the top ten  in between ten and fifty  and over fifty.  the results of this analysis are given in table 1.   
for queries with poor performance  relevant documents are most likely to occur in the title  1%  and word  1%  result files.  overall  1% of the relevant documents occurred without overlap in the top ten of the individual result files  and the fusion process increases these ranks  thereby damaging overall mrr.  
similarly  for queries whose relevant documents are missing  the word and title result files contain a high percentage of relevant documents.  unfortunately in this case the majority of the relevant documents occurring in the word and title result files have ranks over fifty prior to fusion  therefore  performance on these queries was initially bad and the fusion process is not likely to have had a significantly negative impact in this case.  from the analysis we can conclude that a more finely tuned fusion method may have the potential to help cases where the relevant documents end up poorly ranked in the final result set. 
 
     poorly ranked missing   iit1sa iit1sau iit1sa 	iit1sau title .1 .1 .1 .1 word .1 .1 .1 .1 imgalt .1 .1 .1 .1 anchortext .1 .1 .1 .1 meta .1 .1 .1 .1 not found .1 .1 .1 .1 table 1: presence of query terms in documents  
rank  in resultfile     poorly ranked iit1wp1 	iit1t np 	iit1a np ¡¡¡¡¡¡¡¡¡¡missing iit1wp1 	iit1t np iit1a np top 1 .1 .1 	.1 	.1 	.1 .1 1 .1 .1 	.1 	.1 	.1 .1 over 1 .1 .1 	.1 	.1 	.1 .1 total .1 .1 	.1 	.1 	.1 .1 table 1: presence of relevant documents in individual result files  
1 conclusion 
this year we participated in the homepage and known-item web retrieval task.  we explored the concept of multiple tasks being issued via the same interface.  to that end we explored using a task classification approach where we could use task specific information to improve those queries.  this approached showed promise in that by using task specific information our results improved ~1% over our baseline traditional web retrieval approach and would have improved by 1% given an optimal classifier.  given the simplicity of our classifier this approach seems to help the overall system effectiveness.  our future work will continue examining other features that can help in the other tasks. 
appendix 
table 1: b-value = .1 
	index 	run description 	run name 	est. mrr actual mrr 
gov.anchor .gov anchor terms only  good html parser  no phrases iit1a np.dat 1 1 gov.anchor .gov anchor terms only  good html parser  with phrases iit1a p.dat 1 1 gov.title 	.gov title terms only  good html parser  no phrases iit1t np.dat 1 1 gov.title 	.gov title terms only  good html parser  with phrases iit1t p.dat 1 1 gov.bigtext .gov bigtext terms only  good html parser  no phrases iit1b np.dat 1 1 gov.bigtext .gov bigtext terms only  good html parser  with phrases iit1b p.dat 1 1 gov.word 	.gov conglomerate  words only  good html parser  no phrases iit1w np.dat1 1 
gov.word 	.gov conglomerate  words only  good html parser  with phrases iit1w p.dat 1 1 gov.meta 	.gov meta  words only  good html parser  no phrases 	iit1m np.dat1 1 gov.meta 	.gov meta  words only  good html parser  with phrases 	iit1m p.dat 1 1 gov.imgalt .gov img/alt  words only  good html parser  no phrases 	iit1i np.dat 1 1 gov.imgalt .gov img/alt  words only  good html parser  with phrases 	iit1i p.dat 1 1 table 1: b-value = .1 
	index 	run description 	run name 	est. mrr actual mrr 
gov.anchor .gov anchor terms only  good html parser  no phrases 	iit1a np1.dat 1 1 gov.anchor .gov anchor terms only  good html parser  with phrases 	iit1a p1.dat 1 1 gov.title 	.gov title terms only  good html parser  no phrases 	iit1t np1.dat 1 1 gov.title 	.gov title terms only  good html parser  with phrases 	iit1t p1.dat 1 1 gov.bigtext .gov bigtext terms only  good html parser  no phrases 	iit1b np1.dat 1 1 gov.bigtext .gov bigtext terms only  good html parser  with phrases 	iit1b p1.dat 1 1 gov.word 	.gov conglomerate  words only  good html parser  no phrases 	iit1w np1.dat1 1 gov.word 	.gov conglomerate  words only  good html parser  with phrases iit1w p1.dat 1 1 gov.meta 	.gov meta  words only  good html parser  no phrases 	iit1m np1.dat1 1 gov.meta 	.gov meta  words only  good html parser  with phrases 	iit1m p1.dat 1 1 gov.imgalt .gov img/alt  words only  good html parser  no phrases 	iit1i np1.dat 1 1 gov.imgalt .gov img/alt  words only  good html parser  with phrases 	iit1i p1.dat 1 1  
