machine learning is the mainstay for text classification.  however  even the most successful techniques are defeated by many real-world applications that have a strong time-varying component. to advance research on this challenging but important problem  we promote a natural  experimental framework-the daily classification task-which can be applied to large time-based datasets  such as reuters rcv1.  
in this paper we dissect concept drift into three main subtypes. we demonstrate via a novel visualization that the recurrent themes subtype is present in rcv1. this understanding led us to develop a new learning model that transfers induced knowledge through time to benefit future classifier learning tasks. the method avoids two main problems with existing work in inductive transfer: scalability and the risk of negative transfer. in empirical tests  it consistently showed more than 1 points f-measure improvement for each of four reuters categories tested.   
categories and subject descriptors 
h.1  information search & retrieval : information filtering; i.1  pattern recognition : design methodology  classifier design and evaluation. 
general terms 
algorithms  performance  experimentation. 
keywords 
text classification  topic identification  concept drift  time series  machine learning  inductive transfer  support vector machine. 
1. introduction 
advanced technology for supervised machine learning is making its way into commercial applications. for example  we have used it to categorize millions of hp technical support documents into hundreds of topic categories for improved customer support. but real-world deployment of techniques that have proven successful in the laboratory often meet with challenging practical problems. machine learning research typically assumes the manuallylabeled training cases are random samples-independently and identically distributed  iid -from a stationary test distribution.  in contrast  commercial applications of machine learning often desire to apply trained classifiers to make predictions on a stream of future samples that may vary over time.  unfortunately  the success of machine learning classification pales for real-world  time-varying streams of data. 
despite the difficulty  this is nonetheless an economically important problem to tackle.  although controlled concept drift scenarios have been devised for individual investigations  the lack of a large  real-world benchmark problem to share  innovate from  and compare against has been a detriment to progress in this area.  towards this end  we define and promote a research framework called the daily classification task  dct  in which to conduct studies and perhaps competitions.  among other datasets  it can be applied to the large reuters rcv1 corpus  which has 1 news articles over 1 days that are classified into many topics  industries and country categories . it is publicly available from nist   unlike many industrial datasets exhibiting similar concept drift. 
we subdivide the notion of concept drift into three main types  and demonstrate that reuters exhibits the recurrent themes subtype  illustrated via a new visualization technique.  armed with this understanding  we then go on to develop a classification model that is able to leverage training data from many previous days  even if the target concept has drifted substantially.  finally  we present an empirical dct evaluation that reveals the strong success of this new model.  it effects a form of inductive transfer across time  and does so in a way that avoids many of the common problems inherent in the existing vein of research on inductive transfer  as described in the related work section. 
1. analysis of concept drift 
we subdivide the notion of concept drift into three main types: 
1. shifting class distribution:  the relative proportion of cases in the different categories may change over time  but the samples within a given class are iid stationary.  for example  the proportion of hepatitis a cases may increase with an epidemic  but the symptoms/features of the disease are invariant over time .  even so  this will change the optimal decision threshold for an imperfect classifier . for a robust method to track shifting class distributions with very limited training data  see  or . 
1. shifting subclass distribution:  a category may be comprised of a union of  potentially undiscovered  subclasses or 

 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  august 1  1  seattle  washington  usa. 
copyright 1 acm 1-1/1...$1. 
themes  and the class distribution of these subclasses may shift over time.  as above  the feature distribution given a particular subclass is stationary  but the feature distribution of the super-class will vary over time  because its mixture of subclasses varies. 
1. fickle concept drift:  individual cases may take on different ground truth labels at different times. this setting is appropriate for recommender systems-the user may initially find some case relevant that is later not relevant  such as an interest in boston weather that wanes after one's trip there.  this appears to be the most difficult setting of class drift.  if some assumption can be placed on how slowly or suddenly concepts shift  one may have some notion of how prior training labels may still be useful.  in the general case  old training data is no better than unlabeled samples. 
a variant on each of these types is when the domain has some recurrent or even periodic behavior. for example  in spam classification  there is a periodic theme of christmas-related spam every december. in the remainder of this paper  we will focus on recurrent themes:  a subtype of type 1 concept drift  in which we cannot expect strict periodic behavior of resurfacing subclasses  and furthermore  we do not know the subclasses that compose the positive class of interest.  moreover  in problems of interest  new subclass themes may crop up that also belong to the class  but have never before been witnessed. 
1. fxtime visual analysis 
we present here a novel visual analysis of the feature space for binary text classification datasets. its goal is to determine how stable or shifting the most predictive features are over time.  it helps one characterize the degree and nature of concept drift in a dataset. 
in each time period independently  we determine the most predictive features-in our case  the top 1 word features for predicting a particular reuters topic.  if the concept is completely stable  the top 1 words will be the same each day; otherwise  they will vary.  given the total set of top words over all time  we sort the words by the date they first appeared in the top 1.  we allocate each word to a column of pixels in an image  where each pixel in the column represents one day  going downward.  we color a pixel black if the word was among the top 1 predictive words on that day; otherwise we color it grayscale according to its predictiveness  gray being predictive  white non-predictive.  figure 1 shows this visualization for three reuters topic categories: gcat  government & social issues  1% of articles   gspo  sports  1%   and ecat  economics  1% .  we selected the top 1 words according to bi-normal separation   which like information gain  is a feature scoring metric used in feature selection.  for each time period  we used a sliding window covering the most recent 1 days of data  in order to avoid weekend-related effects  which otherwise make a distracting horizontal grating pattern that visually obscures other patterns.  
first  notice that the top predictive words for ecat are less stable over time. it has 1% more top words  1  over the course of the year than either of the other two classes.  later we will see that it also exhibits much lower f-measure.  all three images show a few words on the far left that remained predictive throughout the year.  for gcat  these are words such as police  troops  arrested  and peace.  
the frontier of black pixels shows when a feature first makes it into the top 1.  notice by the changes in slope that on some days many new top words are generated as a new hot news topic is introduced  and other times several days pass without many new features  indicating the rate of drift.  for all three images  observe that most top words lose their predictiveness after a few days  downward   reinforcing the slogan that yesterday's news is like stale bread.  in some cases the popularity of some few predictive keywords lasts several weeks  as in the right-hand oval in gcat  when laurent  mobutu  seko  sese  and kabila became predictive in may  1  as laurent kabila led rebel forces to expel the president of zaire  mobutu sese seko  from the country. 
interestingly  many or most predictive words resurface again later among the top words.  for example  in the left-hand oval  the words hostage s   tupac  amaru  mrta  and fuimori were introduced on december 1  1  when the tupac amaru revolutionary movement  mrta  occupied a japanese embassy.  later we see this group of words resurface at the bottom of the oval when the alberto fujimori regime in peru massacred 1 mrta commandos on april 1  1.  this is a striking example  but there are many other predictive words that come back later when there is additional news on their topic. 
examples like these represent recurrent themes of type 1 concept drift: recurrent shifting subclass priors  where we do not know the subclasses.  this is not fickle concept drift  because articles that fit a category continue to belong to the class  supposing more news on that topic arises later.   
because of the highly dynamic nature of news streams and other industrial datasets  a classifier built from training cases up to day 1 is unlikely to be effective on day 1 when the top predictive words have mostly changed.  to cope with this problem  any operational setting must provide an ongoing stream of additional labeled training cases  but hopefully need as few as possible. 
1. daily classification task   dct  
concept drift is admittedly a difficult research area.  to promote its study  we define a conducive problem formulation we call the daily classification task.  in it  time is discretized into periods  e.g. days  and of the many cases each day  a limited size iid random sample is provided as a labeled training set  as in .  a performance objective of interest  such as classification accuracy or f-measure  is computed on each day of a benchmark dataset  and the average is reported over all days.  using all days gives a natural preference for methods that improve quickly with only a few past days available  as opposed to beginning the average after day 1  for example  when steady state may have been achieved.  for research purposes  the size t of the daily training set should be selected so that the learning curve is still climbing. 
as an optional variant  some percentage h of the ground truth test labels may be revealed for past days.  we call this variant setting hindsight dct-a reasonable assumption for certain real-world settings.  for example  in the reuters setting  some of the predictions that were wrong may get noticed and corrected by people after the prediction errors have been committed.  in some settings there is no cost to obtain past labels. this is common where the classification task constitutes a prediction about the future  e.g. whether now is a good time to spin down the laptop disk drive to spare the battery; after the fact  we can determine from disk demand whether the prediction was good.  
the strawman learning model is simply to train a state-of-the-art classifier  on  each  daily training set of size t   and  then use it  to 

1-1 
 
 
 
1-1 
 
 
 
 
 
 
	 	1-1 
category gcat   government & social issues  1% :   1 top predictive words 
1-1 
 
 
 
1-1 
 
 
 
 
 
 
	 	1-1 
category gspo   sports  1% :  1 top predictive words 
 
category ecat   economics  1% :  1 top predictive words 
figure 1.  fxtime visualization of predictive features  columns   revealing recurring predictive features over time  downward . 
classify the rest of that day's cases. to surpass this strawman  we the most recent p previous days.  our empirical evaluation ahead would like to leverage the available past training data somehow.  refutes this popular idea for reuters classification  because of the an obvious idea is to use a sliding window that retains data from rampant concept drift from day to day. 

p=1 additional features for each case
figure 1.  temporal inductive transfer  tix  model 
1. temporal inductive transfer  tix  model 
ideally we would like to be able to leverage the learned models from the past  not just reuse past data.  this is the research area of inductive transfer  which has met with limited success. here we desire to apply it in a new way: temporally within a single  changing classification task to help cope with its concept drift. 
we propose the following method  refer to figure 1 :  like the strawman  each day we learn a new classifier from the t training examples using whatever state-of-the-art induction algorithm is available.  but the input feature vector  in addition to the usual bag-of-words features  we augment with p additional binary features.  these are generated by the predictions that the p previous daily classifiers would have made for today's cases. these predictions are computed both for the training data and for today's testing data. clearly this can be done without knowing the ground truth labels for the testing data. 
these additional p features may be potentially predictive in today's daily learning task. if a news theme recurs that was popular within p days ago  the new classifier may be able to leverage the predictions made by the old classifiers that were trained while the theme was previously popular.  this suggests some pressure to maximize p for greater long-term memory  if we can afford the computational cost.  but note  old classifiers are just reapplied  which is much faster than their original training.  even if we were to let p go as high as 1 days  it is still dwarfed by the large number of bag-of-words features generated by the training sets. 
now  if some or all of the p features end up being worthless with regard to the daily learning task  then the state-of-the-art classifier will be able to ignore them  just as today's text classifiers are easily able to ignore a large number of non-predictive words. hence  we expect that these p features will not make the text classifier perform any worse  but sometimes may help it improve. 
a detail:  if p=1  then today's classifier depends on yesterday's classifier only.  but that classifier depends on the one from the day before it  and so on.  the recurrence relation implies that all classifiers remain in use for all time. intelligent pruning may someday be devised  but for the purposes of this paper  we break the recurrence by simply substituting a classifier trained on yesterday's data  having no additional tix features. hence  by this tweak  the p classifiers operate independently of one another. 
1. empirical evaluation 
we conducted a series of daily classification task experiments on reuters for each of the four classes: gcat  gspo  ecat and 

	 1	 1	 1	 1	 1	 1	 1
	number of daily training cases t	 
figure 1.  learning curves for strawman svm. 
m1  money markets  1% positives .  that is  for gcat  we focused on the binary classification task of predicting which of each day's news articles belong to the gcat topic  government/social  ~1% .  each day we made available t=1 training cases.  rather than the overkill of testing on the many thousands remaining each day  we considered only the first 1 articles each day  selecting the t training cases from these at random. we report f-measure macro-averaged over all days. 
the base classifier we used is a linear support vector machine  svm  trained on binary bag-of-words features  title+body text lowercased  alpha only  max 1k words from each training set   as implemented by the weka v1 library   with bns feature scaling .  we chose this classifier for its state-of-the-art performance and for its ability to tolerate many useless features  to which tix may add some useless features.  for the tix model  the added features are each binary predictions  preliminary tests with real-valued score features showed worse results . 
1 results 
figure 1 shows the average f-measure for strawman  i.e. simply training on the t random samples each day and testing on the rest of the day's cases. this establishes a baseline f-measure performance for t=1  used hereafter.  this graph confirms for each class that the choice of t=1 is sufficient for some learning to occur  but not so much that additional training data or predictive features would provide no benefit. 
figure 1 contains the four graphs corresponding to the independent experiments on the four reuters classes. the top two graphs share a common y-axis scale  but the bottom two share a much lower f-measure scale. we are less concerned about absolute performance for each task than about improvement. 
each graph shows the average f-measure performance of all models for t=1 daily training cases. the leftmost point shows strawman  which leverages none of the past training data available.   the sliding window technique  labeled window  adds p previous days of training cases to the daily training set  i.e. the training set grows to t* 1+p  cases.  as we increase p  its performance consistently declines  even below the visible chart. for larger p  we see that increasingly stale training data misleads the classifier badly about what the concept today is. hindsight would only worsen this. although sliding window is not a viable method for highly drifting concepts  the shape of its performance decline curve might be used as a way to characterize the pace of concept drift in a dataset.   
1	 1	 1	 1	 1	 1	 1	 1
	previous days of training data used	 
1	 1	 1	 1	 1	 1	 1	 1
previous days of training data used
strawmanoracle
¡¡¡¡tixtix+h windowt=1 tix+h 
1	 1	 1	 1	 1	 1	 1	 1
	previous days of training data used	 
1	 1	 1	 1	 1	 1	 1	 1
previous days of training data used
figure 1.  results for gcat and gspo  top   ecat and m1  bottom .  

turning now to the tix model  with t training cases and no additional training labels revealed from the past  h=1%   it performs exactly equal to the strawman  regardless of p.  one might conclude that the predicted classes from past classifiers have no bearing on the daily learning task at hand.  but this is not so.  if we increase the training sets of those p past classifiers by revealing all past training labels  h=1%   then they are able to generate predictive features that are more accurate  and they indeed become useful to the daily learning task  see the climbing 

 
figure 1.  tix  dot  vs. strawman  whisker  for 1 classes. curve labeled tix+h .  we observe a consistent and substantial rise of more than 1 points of f-measure for all four classes as we increase p to 1 past days.   we repeated this measurement for 1 of the most common reuters topics: from ccat at ~1% prevalence down to c1 at ~1%  roughly correlating with classification difficulty. tix+h at p=1 shows a substantial gain for all but the most difficult topics  as shown in figure 1. the classes are arranged along the x-axis according to tix performance. we return our attention to figure 1 hereafter.  
oracle: for an upper-bound performance comparison  we also evaluated an 'oracle' model:  we train the daily classifier on only the t=1 training cases  like the strawman  but we include one additional binary feature that gives away the true class label for each case. the learned classifier is not perfect  since svm does not memorize its training set  as k-nearest-neighbors would. but this gives an upper bound on the performance we could expect from the base classifier we used  if the tix predictive features had been 1% perfect.  
in the case of gspo  top right   tix+h rises to match the performance of the oracle model.  for gcat and m1  tix+h rises over half way to the oracle from strawman performance.   

figure 1.  f-measure improvement by tix with hindsight  dot  over strawman  end of whisker  for each day of gcat. the  1-day moving average of differences is shown by the dotted line near the bottom  averaging +1 f-measure points. varying daily training size t: for gcat and gspo  we nearly match the performance of tix+h even with half as many training cases each day  see curve labeled t=1 tix+h   at least when p=1 past days of hindsight memory are allowed.  this suggests the tix features we add are worth nearly as much as doubling the number of rows in the training set.  for some real-world settings  halving t could cut in half a substantial daily manual effort in labeling cases.   
in contrast  for ecat and m1 where the overall classification accuracy is much worse  tix+h with t=1 never approaches the performance with t=1.  one might suppose that the p tix classifiers have difficulty in predicting accurately  however  with h=1% hindsight  they may be getting roughly the accuracy of the oracle model.  for ecat this is quite high  so rather than blame the p=1 tix features for being inaccurate  instead we may reason that their applicability to the new daily classification task is less than for classes such as gcat or gspo. 
in these difficult situations  there is little substitute for fresh training data  though even here more days of memory helps monotonically.  if we had a dataset that covered a longer time period  it would be interesting whether p=1 or more could eventually bring the performance up to that of the oracle. 
1 time series view 
drilling down on the gcat results for t=1  figure 1 shows the performance improvement for each day of the year. the dot indicates the f-measure of tix with hindsight using p=1 days of memory  and the other end of the whisker indicates strawman. the 1-day moving average of the improvement is shown by the dotted line at the bottom  averaging about +1 f-measure points. also notice that when strawman performance is very low or even failing  tix using hindsight often led to large improvements. 
what is more striking is that all the differences are positive  with the minor exception of a small loss that occurred on day two . this substantiates our claim that the daily induction task can leverage tix features when they are useful and successfully ignore them when they are irrelevant. this is a property of the base classifier we have chosen.  if we had used a base classifier that was very sensitive to feature selection  such as na ve bayes  then we would expect to see some losses as well. 
1 reduced hindsight 
so far  we have only considered full hindsight or no hindsight.  figure 1 shows for each category  the f-measure of tix as we vary hindsight: 1%  1%  1%  and 1% of past test case labels revealed. recall that at 1%  its performance happens to match strawman.  each day there are 1 articles; t=1 are used for training  leaving 1 for testing.  at 1% hindsight  1 of the test cases later have their true labels revealed  and the old daily classifiers are retrained for their use as feature generators.  for gcat and gspo  1% hindsight yields most of the benefit of full hindsight.  this non-linear behavior is typical of learning curves  and practically useful in many real-world domains where there continues to be some cost for obtaining hindsight labels. but for the difficult class ecat  1% hindsight gives no benefit. 
1 runtime analysis 
the size of the available training data grows linearly over time.  for the sliding window algorithm  it can accumulate a very large training set.  for an induction algorithm such as svm  this results in an o n1  training time. in practice  we do not see worst case performance.  we found consistently that doubling p results in tripling the time to run the sliding window experiment with no hindsight.  in contrast  doubling p for tix with 1% hindsight only increases the time by 1x.  concretely  sliding window for p=1  1 training cases  took ~1 hours on modern hardware to complete the experiment  while tix with 1% hindsight took ~1 hours  which effectively leverages 1 training cases.  we did not even attempt sliding window with hindsight: besides slowing down the training time tremendously  we reason that the  1	 1	 1	 1	 1	 1	 1	 1
previous days of training data used
1%
1%
figure 1.  f-measure for tix  varying % hindsight. 
additional stale cases would only further drown out the signal from the t=1 fresh daily training cases. 
the secret to the efficiency of tix is that it brings forward its induced linear classifiers for future uses  which are quick to evaluate on new cases.  it need not save or re-train on past data.  it trains once for t=1  and once more for the hindsight cases.  hence  its runtime is only linear in p. 
1. discussion 
it is somewhat disappointing that the tix model provided no benefit without hindsight.  we believe the reason is that its inductive transfer features have no greater accuracy than what the induction algorithm can already learn from the bag-of-words features.  an alternate hypothesis is that the daily classifier does learn to depend on the tix features  but their unreliability results in no performance gain. but by painstakingly examining the feature weights on the tix features  we found they were not valued.  with some additional hindsight data  they then obtain greater accuracy and become useful in the daily learning task  even though they may be for a somewhat drifted concept.  
recall that for this study  each past tix classifier did not build on previous classifiers  in order to break the recurrence chain to the first day.  if they had instead each leveraged previous classifiers  one possibility is that they would then perform more accurately  and hence eventually provide more reliable features for future daily classifiers. in this case  tix without hindsight might surpass strawman. we are skeptical of this: the outputs of strawman performed equal to those of the p=1 tix classifier  i.e. there would presumably be no difference in performance if we were to replace the past strawman classifiers in tix with the daily tix classifiers that are trained with the additional p=1 feature.   
nonetheless  this idea should be tried  and if indeed successful  some intelligent pruning method will eventually be required to avoid linear slowdown over thousands of days' data. 
regarding sliding windows: in additional experiments not recorded here  we tried sliding window for training sizes t=1...1. as expected  the greatest effect of reducing t is a loss in accuracy.  but a minor effect is that for very small training sets  expanding the window to include a few past days of data can be beneficial. for example  for gcat with t=1  sliding window peaked at 1% f-measure with p=1  up from 1% for strawman.  
finally  although our results discourage sliding window  we acknowledge that in settings without the time discretization of the daily classification task formulation  some sort of sliding window scheme may be needed to select a sample of recent data. our rcv1 results suggest strong pressure to minimize the window width  and instead use tix to leverage older data. 
1. related work 
in most settings in the concept drift literature  concepts change rarely or else gradually. this has led to many heuristic methods to detect when a significant change occurs.  but this is a non-issue in the reuters data and industrial datasets of interest where substantial change is the rule  rather than the exception.  
the remaining research challenge is how to produce an effective classifier despite the concept having just drifted. the prevailing approach in the literature is to completely throw away the old classifier and most of its training data  and then build a new one on more recent data  e.g. cvfdt  and a variety of sliding window techniques  some with adaptive window sizes  .  two approaches stand out by their ability to re-use older information.  one selects various old training sets for inclusion  if they appear to be similar to the most current training data .  the other reuses previously learned concepts  rather than re-using the training data they represent.  examples include reactivating a single old model  if it seems more appropriate on recent data than the current classifier  e.g. flora1    while other models use ensembles of old classifiers and prune or adapt the weights according to recent data .  a direct empirical comparison would be interesting; though in none of these approaches does a previously learned classifier benefit the training of a new base classifier  as in tix. 
there has been a great deal of research on inductive transfer under many names  e.g. multi-task learning  lifelong learning  bias learning  representation learning  and notably hierarchical bayes. these efforts show consistently that transferring knowledge helps from 'similar enough' task data  but if the related task is 'too dissimilar' it hurts  politely called 'negative transfer' .  this was one of the greatest concerns voiced at the recent nips workshop on inductive transfer .  by contrast  it is noteworthy that the tix model never harms prediction accuracy.  this property is designed into the model by the ability of the base svm classifier to successfully ignore useless features.  this is its great strength.  
another difference is that many existing approaches to inductive transfer do not actually transfer previously induced models  but instead re-use the old training data to help condition the induction of new classifiers.  this requires ever more cpu time for retraining on growing training sets  which unfortunately is superlinear. 
for text learning  there has been a great deal of experimentation with different feature vectors.  besides the many variants that try stemming  phrases  and other linguistic techniques  some replace the feature vector with a representation thought to model the dataset better  e.g. latent semantic indexing  distributional clustering  and cluster centroids  e.g.  .  some of these methods have the advantage that they can leverage large bodies of unlabeled text-semi-supervised learning.  but again  if the unlabeled data are 'too dissimilar' then the changed representation may instead defeat the learning task at hand.   
some related work uses the output of classifiers as features  e.g. stacking  voting  and various ensemble methods.  these methods all train their subclassifiers on the same input training set. sequential prediction methods use the output of classifiers trained with previous  overlapping subsequences of items  assuming some predictive value from adjacent cases  as in language modeling. 
1. conclusion 
we have shown the success of temporal inductive transfer for the dct setting when the ground truth labels for some past test cases are eventually revealed-hindsight.  while useful in many realworld situations  in others the past labels are not available without additional expense. thus  further research is called for in the dct setting without hindsight.  
a promising avenue for future work includes hybridizing the temporal inductive transfer idea with related work in semisupervised learning.  the past labeled data provides for a richer setting than traditional semi-supervised learning.  interestingly  gabrilovich and markovitch recently tried augmenting the bag-ofwords feature vector with the output of classifiers trained on the open directory hierarchy  and found some benefit .  while their inductive transfer is not through time and not from markedly similar tasks  the benefit of augmenting vs. replacing the raw features we believe is the right approach. 
1. acknowledgments  
ian witten's weka software library for machine learning  made this research a pleasure-only 1 lines of code. we thank eric anderson and the hp labs utility datacenter for providing ample computing horsepower to meet the conference deadline. 
