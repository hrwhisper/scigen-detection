in many classification tasks training data have missing feature values that can be acquired at a cost. for building accurate predictive models  acquiring all missing values is often prohibitively expensive or unnecessary  while acquiring a random subset of feature values may not be most effective. the goal of active feature-value acquisition is to incrementally select feature values that are most cost-effective for improving the model's accuracy. we present two policies  sampled expected utility and expected utility-es  that acquire feature values for inducing a classification model based on an estimation of the expected improvement in model accuracy per unit cost. a comparison of the two policies to each other and to alternative policies demonstrate that sampled expected utility is preferable as it effectively reduces the cost of producing a model of a desired accuracy and exhibits a consistent performance across domains.
general terms
algorithms
keywords
machine learning  data mining  active learning  cost-sensitive learning
1. introduction
　in many predictive modeling problems  feature values for training data are missing  but can be acquired at a cost. often the cost of acquiring the missing information varies according to the nature of the information or of the particular instance for which information is missing. consider  for example  patient data used to induce a model to predict whether or not a treatment will be effective for a given patient. some patient data may have missing demographic
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
ubdm '1  august 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
information that can be obtained at a low cost. in contrast  acquiring diagnostic test results from different health-care providers can be significantly more expensive and time-consuming. various solutions are available for learning models from incomplete data  such as imputation methods   and learners that ignore missing feature values such as the naive bayes classifier. however  these solutions almost always undermine model performance as compared to that of a model induced from complete information. since obtaining all missing values may be prohibitively expensive  it is desirable to identify what information would be most cost-effective to acquire. in this paper we address this generalized version of the active feature-value acquisition  afa  task for classifier induction : given a model built on incomplete training data  select feature values that would be most cost-effective to acquire for improving the model's accuracy. the problem of feature-value acquisition is different from traditional active learning  in which class labels rather than feature values are missing and are costly to acquire.
　unlike prior work   we study afa in a setting where the total cost to be spent on acquisitions is not determined a priori  but rather can be determined on-line based on the model's performance as learning progresses. this setting is motivated by the inherent uncertainty regarding the tradeoff between costs and improvement in model accuracy. an incremental spending strategy enables a decision maker to re-evaluate the desirability of further expenditures by incrementally exploring the performance curve resulting from a series of acquisition decisions. for example  one may choose not to acquire additional information if the current model accuracy is satisfactory  or if additional information is unlikely to provide a significant improvement in the model. we propose a general setting for afa that specifies an incremental acquisition schedule. given the current model  an afa strategy identifies feature-value acquisitions that are estimated to be most cost-effective with respect to model accuracy.
　we present a solution to the afa task that ranks alternative featurevalue acquisitions based on an estimation of the expected improvement in model performance per unit cost. our approach is general  i.e.  it can be applied to select acquisitions for any learner  and to attempt to improve any performance metric. experimental results on decision tree induction to improve classification accuracy demonstrate that our method does consistently result in significantly improved model accuracy per unit cost compared to random feature-value acquisition. the method is particularly advantageous in challenging tasks for which there is a significant variance across potential acquisitions with respect to their contribution to learning per unit cost.
1. task definition and algorithm
1 active feature-value acquisition
　assume a classifier induction problem where each instance is represented with n feature values and a class label. a training set of m instances can be represented by the matrix f  where fi j corresponds to the value of the j-th feature of the i-th instance. initially  the class label  yi  of each instance is known  and the matrix f is incomplete  i.e.  it contains missing values. the learner may acquire the value of fi j at the cost ci j. we use qi j to refer to the query for the value of fi j. the general task of active featurevalue acquisition is the selection of these instance-feature queries that will result in building the most accurate model  classifier  at the lowest cost. the framework for the generalized afa task is presented in algorithm   ach step the learner builds a classifier trained on the current data  and scores the available queries based on this classifier. the query with the highest score is selected and the feature value corresponding to this query is acquired. the training data is appropriately updated and this process is repeated until some stopping criterion is met  e.g. a desirable model accuracy has been obtained. to reduce computation costs in our experiments  we acquire queries in fixed-size batches at each iteration.

algorithm 1 general active feature-value acquisition framework
given:
f - initial  incomplete  instance-feature matrix
y = {yi : i = 1 ... m} - class labels for all instances
t - training set =   f y  
l - base learning algorithm b - size of query batch
c - cost matrix for all instance-feature pairs
1. initialize totalcost to cost of f
1. initialize set of possible queries q to {qi j
1 ... m;j = 1 ... n; such that fi j is missing}
1. repeat until stopping criterion is met
1. generate a classifier  m = l t 
1.  qi j （ q compute score m qi j l t 
1. select a subset s of b queries with the highest score
1.  qi j （ s 
1. acquire values for fi j
1. totalcost = totalcost + ci j
1. remove s from q
1. return m = l t :	i=　alternate problem settings of feature-value acquisition have been explored in the literature. in particular  melville et al.  studied a specialized version of the afa problem addressed here  where all the missing feature values for an instance are acquired at once and an acquisition policy selects the instances for which acquiring all missing values would result in the most accurate classifier. lizotte et al.  studied the budgeted learning scenario  in which the total cost  budget  to be spent on feature-value acquisitions is determined a priori. we discuss these and other related research in more detail in the related work section.
1 expected utility estimation
　specific solutions to the afa problem differ based on the method used to score and rank queries. in our approach we provide scores based on the expected utility of each query  defined below . for now we assume all features are nominal  i.e.  they can take on values from a finite set of values. assume feature j has k distinct values v1 ... vk. the expected utility of the query qi j can be computed as:
k
	e qi j  = xp fi j = vk u fi j = vk 	 1 
                           k=1 where p fi j = vk  is the probability that fi j has the value vk  and u fi j = vk  is the utility of knowing that the feature value fi j is vk  given by:
		 1 
where a f  is the accuracy of the current classifier; a f fi j = vk  is the accuracy of the classifier trained on f assuming fi j = vk; and ci j is the cost of acquiring fi j. for this paper  we define the utility of an acquisition in terms of improvement in model accuracy per unit cost. depending on the objective of learning a classifier  alternate utility functions could be used.
　if we were to plot a graph of accuracy versus model cost after every iteration of afa  our expected utility approach would correspond to selecting the query that is expected to result in the largest slope for the next iteration. if all feature costs are equal  this corresponds to selecting the query that would result in the classifier with the highest expected accuracy.
　since the true distribution of each missing feature value is unknown  we estimate p fi j = vk  in eq. 1 using a learner that produces class probability estimates. for each feature j  we train a classifier mj  using this feature as the target variable and all other features along with the class as the predictors. when evaluating the query qi j  the classifier mj is applied to instance i to produce the estimate p  fi j = vk .
　in eq. 1  the true values of a .  are also unknown. however  since the class labels for the training data are available at selection time we can estimate a f  and a f fi j = vk  based on the training set accuracy. in our experiments  we used 1 loss to measure the accuracy of the classifiers. however  other measures such as class entropy or gini index could also be used . in our preliminary studies we did not observe a consistent advantage to using entropy.
　when the expected utility method described here is applied to learn a naive bayes classifier and feature costs are assumed to be equal  it is similar to the greedy loss reduction approach presented in . similar approaches to expected utility estimation have also been used in the related task of traditional active learning  1  1  1 .
　computing the estimated expectation e  .  for query qi j requires training one classifier for each possible value of feature j. selecting the best from all available queries would require exploring  in the worst case  mn queries. so exhaustively selecting a query that maximizes the expected utility is computationally very intensive and is infeasible for most interesting problems. we make this exploration tractable by reducing the search space to a random sub-sample of the available queries. we refer to this approach as sampled expected utility. this method takes a parameter α
  which controls the complexity of the search. to select a batch of b queries  first a random sub-sample of αb queries is selected from the available pool  and then the expected utility of each query in this sub-sample is evaluated. the value of α can be set depending on the amount of time the user is willing to spend on this process. one can expect a tradeoff between the amount of time spent and the effectiveness of the selection scheme.
1 instance-based active feature-value acquisition
　in sampled expected utility we use a random sample of the pool of available queries to make the expected utility estimation feasible. however  it may be possible to improve performance by applying expected utility estimation to a sample of queries that is better than a random sample. one approach could be to first identify potentially informative instances  and then select candidate queries only from these instances. in previous work we studied a specialized version of afa  where all the missing feature values for an instance are acquired at once and an acquisition policy selects the instances for which acquiring all missing values would result in the most accurate classifier. the method proposed in this work  error sampling  es   can be readily used to identify informative instances from which we can then choose candidate queries. error sampling orders incomplete instances in terms of potential informativeness in the following way. it ranks instances that have been misclassified by the current model as the most informative. next  it ranks correctly classified instances in order of decreasing uncertainty in the model's prediction. error sampling requires building only one model at each step of afa  and hence is not too computationally intensive to use in place of random sampling in our sampled expected utility approach. we call this new approach expected utility-es  in which error sampling is used to rank instances from which the first αb missing instance-feature pairs are selected as candidate queries. where b is the desired batch size and α is the exploration parameter.
　though error sampling was designed for selecting instances  it can also be modified to acquire single feature values in our general afa setting. the method ranks instances for acquisition  but does not provide a mechanism for selecting the most informative features for a given instance. we therefore examine a version of error sampling in which instances are ordered using the error sampling ranking  and the first b missing feature values are selected for acquisition.
1. experimental evaluation
1 methodology
　we begin by evaluating our proposed approaches on four datasets from the uci repository   the details of which are presented in table 1. for the sake of simplicity  we selected datasets that have only nominal features. in the future work section  we describe how we can extend our approach to handle numeric features. none of the uci datasets provide feature acquisition costs - in our initial experiments we simply assume all costs are equal. later  we present additional experiments with different cost structures.
table 1: summary of data sets
nameinstancesfeaturesclassesvote11car11lymph11audio11　we compare all the proposed methods to random feature acquisition  which selects queries uniformly at random to provide a representative sample of missing values. for the sampled expected utility and expected utility-es we set the exploration parameter α to 1. given the computational complexity of expected utility it is not feasible to run the exhaustive expected utility approach on all datasets. however  we did run expected utility on the vote dataset. for all methods  as a base learner we used j1 decisiontree induction  which is the weka  implementation of c1 . laplace smoothing was used with j1 to improve class probability estimates.
　the performance of each acquisition scheme was averaged over 1 runs of 1-fold cross-validation. in each fold of cross-validation  we generated learning curves in the following fashion. initially  the learner is given a random sample of feature values  i.e. the instance-feature matrix is partially filled. the remaining instancefeature pairs are used to initialize the pool of available queries. at each iteration  the system selects a batch of queries  and the values for these features are acquired. this process is repeated until a desired number of feature values is acquired. classification accuracy is measured after each batch acquisition in order to generate a learning curve. one system  a  is considered to be significantly better than another system  b  if the average accuracy across the points on the learning curve of a is higher than that of b according to a paired t-test  p   1 . as in   the test data contains only complete instances  since we want to approximate the true generalization accuracy of the constructed model given complete data for a test instance. for each dataset  we selected the initial random sample size to be such that the induced model performed at least better than majority class prediction. the batch size for the queries was selected based on the difficulty of the dataset. for problems that were harder to learn  we acquired a larger number of feature-values and consequently used larger batch sizes.
1 results
　our results are presented in figure 1. for all datasets  sampled expected utility builds more accurate models than random sampling for any given number of feature acquisitions. these results demonstrate that the estimation of the expected improvement in the current model's accuracy enables effective ranking of potential queries. consequently  sampled expected utility selects queries that on average are more informative for the learner than an average query selected at random. the differences in performance between these two systems on all datasets is significant  as defined above. since sampled expected utility was proposed in order to reduce the computational costs of our original expected utility approach  we also examined the performance and computational time of the exhaustive expected utility algorithm for vote. we computed the average time it took to select queries in each iteration for each of the methods. these timing results are summarized in table 1. the results show that constraining the search in expected utility by random sampling  or error sampling  can significantly reduce the selection time  by two orders of magnitude in this case  without a significant loss in accuracy.


	lymph	vote
figure 1: comparing alternative active feature-value acquisition approaches.　while error sampling can rank acquisitions of complete instances effectively  it does not consider the value of individual feature values. despite this  we observed that error sampling performs quite well. in particular  it often performs significantly better than random sampling and it sometimes performs better than sampled expected utility. however  the performance of error sampling in this general setting of afa is inconsistent  as it may perform significantly worse than random selection  as is seen on the lymph dataset. the performance of expected utility-es shows that the method table 1: average selection times on vote.
afa methodselection time  msec random1expected utility1 〜 1sampled expected utility1 〜 1error sampling1expected utility-es1 〜 1can effectively benefit from each of its components. when error sampling performs better than random sampling  the acquisitions made by expected utility-es result in better models than those induced with sampled expected utility. the vote dataset seems to be an exception  in which error sampling can at times perform even better than expected utility  so the combined expected utilityes method does not outperform error sampling here. error sampling's inconsistent performance can also undermine the expected utility-es acquisition policy  so that when error sampling fails to improve upon random acquisitions  expected utility-es produces inferior models than those induced with sampled expected utility. these results suggest that the use of error sampling in our current afa setting is a promising direction for future work  but is dependent on improving the error sampling strategy such that error sampling consistently performs better than random selection. note that  in the instance-completion setting of afa for which error sampling was originally designed  it always performs better than random .
　in summary  expected utility-es often exhibits superior performance with respect to sampled expected utility and random selection. however  it is susceptible to the inconsistent performance of error sampling and thus may potentially perform worse than random sampling. on the other hand  sampled expected utility exhibits consistent improvements over random sampling on all datasets.
1 artificial data and feature costs

 1 1 1 1 1 1 1 1 total cost of feature-values acquired total cost of feature-values acquired
	 a  feature cost structure 1	 b  feature cost structure 1
figure 1: comparing different algorithms on artificial data under different cost structures　as no feature-acquisition costs are provided for the domains we employ here  we initially assumed uniform feature costs. in addition  some of the features in the data are equally discriminative so that there may be little value in selecting between them. in the extreme case  where feature costs are uniform and all features provide equal information about the target concept  random sampling is likely to be a very effective strategy. in order to make the problem setting more challenging  we constructed artificial data in the following way. we took the lymph dataset  which is composed of 1 features  and added an equal number of binary features with randomly-selected values  so as to provide no information about the class variable. in addition  we experimented with different cost structures. for the sake of simplicity  instead of having a cost associated with each instance-feature pair  we assume that the cost of acquiring a particular feature is the same irrespective of the instance. with each feature  we associate a cost selected uniformly at random from 1 to 1. experiments were run as before for 1 different assignments of feature costs. along with recording the accuracy after each batch acquisition of queries  we also record the current model cost based on the cost of the features acquired. since random sampling does not take feature costs into account  we also compare sampled expected utility with a simple baseline strategy that incorporates feature costs. this approach  which we call cheapest-first  selects feature values for acquisition in order of increasing costs. given the inconsistent performance of error sampling and expected utility-es  we do not apply them to these datasets.
　figure 1 presents plots of accuracy versus model cost for two representative cost structures. the results for all randomly assigned costs structures show that for the same cost  sampled expected utility consistently builds more accurate models than random sampling. the differences in performance between these two systems is more substantial than those observed for the uci datasets with uniform costs. in contrast  the performance for cheapest-first is quite varied for different cost assignments. when highly informative features are assigned low costs  cheapest-first can perform quite well  figure 1 a  . since the underlying assumption of the cheapestfirst strategy  that the cheapest features are also informative  often holds in this case  it sometime performs better than sampled expected utility  which imperfectly estimates the expected improvement in accuracy from each acquisition. however  when many inexpensive features are also uninformative  cheapest-first performs worse than a random acquisition policy  figure 1 b  . sampled expected utility  however  estimates the tradeoff between cost and expected improvement in accuracy  and although the estimation is clearly imperfect  it consistently selects better queries than random acquisitions for all cost structures.
1. related work
to the best of our knowledge  the methods we propose here are the first approaches designed for the general problem of incrementally ranking and selecting feature values for inducing any classifier under a general acquisition cost structure. in this section  we discuss alternate settings for the afa task.
　lizotte et al.  study afa in the budgeted learning scenario  in which the total cost to be spent towards acquisitions is determined a priori and the task is to identify the best set of acquisitions for this cost. in contrast  our setting aims to enable the user to stop the acquisition process at any time  and as such the order in which acquisitions are made is important. given this criterion  we attempt to select the next acquisition that will result in the most accurate model per unit cost. lizotte et al. also assume that feature values are independent given the class  and as such consider queries of the form  give me the value of feature j for any instance in class k.  however  our approach evaluates feature-value acquisitions of specific instances  which allows us to 1  incorporate feature-value costs that vary per instance; and 1  to better estimate the expected value of an acquisition by capturing improvements from better modeling of feature interactions. note that a set of features may exhibit different interactions for different instances  in which case evaluating potential acquisitions for individual instances is critical.
　in this paper  we explored the use of the error sampling policy designed for the instance-completion setting  in which all missing feature values are acquired for a selected training instance  1  1 . sampled expected utility selects individual features  and hence can be also employed in the instance-completion setting  e.g.  by selecting the instance with the highest sum of utilities of individual feature-value acquisitions.
　some work on cost sensitive learning  has addressed the issue of inducing economical classifiers when there are costs associated with obtaining feature values. however  most of this work assumes that the training data are complete and focuses on learning classifiers that minimize the cost of classifying incomplete test instances. an exception  cs-id1   also attempts to minimize the cost of acquiring features during training; however  it processes examples incrementally and can only request additional information for the current training instance. cs-id1 uses a simple greedy strategy that requests the value of the cheapest unknown feature when the existing hypothesis is unable to correctly classify the current instance. it does not actively select the most useful information to acquire from a pool of incomplete training examples. the lac* algorithm  also addresses the issue of economical feature acquisition during both training and testing; however  it also adopts a strategy that does not actively select the most informative data to collect during training. rather  lac* simply requests complete information on a random sample of instances in repeated exploration phases that are intermixed with exploitation phases that use the current learned classifier to economically classify instances.
　traditional active learning  1  1  assumes access to unlabeled instances with complete feature data and attempts to select the most useful examples for which to acquire class labels. active featurevalue acquisition is a complementary problem that assumes labeled data with incomplete feature data and attempts to select the most useful additional feature values to acquire.
1. limitations and future work
　in sampled expected utility we used a random sample of the pool of available queries to make the expected utility estimation feasible; and in expected utility-es  we explored the possibility of limiting the set of candidate queries to only potentially informative instances. alternatively  we can restrict the set of candidate queries to only the most informative features. a subset of such features could be picked using a feature selection technique that can capture the interactions among feature values  such as the wrapper approach of john et al. .
　the performance of expected utility relies on having good estimates of the feature-value distributions and of the improvement in model accuracy for each potential acquisition. thus expected utility is likely to benefit from improving upon the methods we applied to perform these estimations. for example  we could use probability estimation methods that better approximate the feature-value distributions  specifically when there are many missing values.
　the expected utility framework allows us to consider model performance objectives other than accuracy. for example  when the benefits from making different accurate predictions and the error costs are specified  expected utility can be applied to identify acquisitions that result in the highest growth in benefits per unit cost. experimenting with such alternate measures of model performance is an avenue for future work.
　our current study was restricted to datasets that are composed of only nominal features. since many interesting domains include both numeric and nominal features  we would like to extend this study to datasets which also have numeric features. we could apply our current expected utility method after converting the numeric features to nominal features using a discretization technique  as in
.
1. conclusion
　in this paper  we propose an expected utility approach to active feature-value acquisition  that obtains feature values based on the estimated expected improvement in model accuracy per unit cost. we demonstrate how this computationally intensive method can be made significantly faster  without much loss in performance  by constraining the search to a sub-sample of potential feature-value acquisitions. experiments with uniform feature costs show that this sampled expected utility approach consistently builds more accurate models than random sampling for the same number of featurevalue acquisitions  and exhibits consistent performances across domains as compared to policies employing an instance-based ranking of features. additional experiments on artificial datasets with different cost structures demonstrate that for the same cost  sampled expected utility builds more accurate classifiers than the costagnostic random feature acquisition approach. its performance is also more consistent than that of a simple cost-sensitive method which acquires feature values in order of increasing cost.
acknowledgments
prem melville and raymond mooney were supported by darpa grant hr1-1.
