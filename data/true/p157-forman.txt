this paper promotes a new task for supervised machine learning research: quantification-the pursuit of learning methods for accurately estimating the class distribution of a test set  with no concern for predictions on individual cases. a variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers. these tasks cover a large and important family of applications that measure trends over time. 
the paper establishes a research methodology  and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications.  in empirical tests  median sweep methods show outstanding ability to estimate the class distribution  despite wide disparity in testing and training conditions.  the paper addresses shifting class priors and costs  but not concept drift in general. 
categories and subject descriptors 
h.1  information systems applications : decision support.  
i.1  pattern recognition : design methodology  classifier design and evaluation 
general terms 
algorithms  measurement  design. 
keywords classification  quantification  cost quantification  text mining. 
1. introduction 
tracking trends over time constitutes a very large family of business and scientific applications  e.g. monitoring the prevalence of hepatitis b.  if the cases are labeled accurately  either by humans or by supervised machine learning  then a simple histogram of the class labels gives accurate counts.  but commonly such classifiers have some degree of error that leads  perhaps surprisingly  to a large and systematic bias in the counts. 
there is a tremendous literature in machine learning that focuses 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd'1  august 1  1  philadelphia  pennsylvania  usa. 
copyright 1 acm 1-1/1...$1. 
 
on optimizing the correctness of individual predictions  accuracy  f-measure  etc. .  while useful  it is not sufficient for this large family of applications whose objective is different:  accurate estimation of the histogram counts.  interestingly  it does not matter that the count may involve both false positives and false negatives  as long as they balance one another well to cancel each other out.  this brings about two challenging and valuable tasks for ongoing research: 
1. the quantification task for machine learning:  given a labeled training set  induce a quantifier that takes an unlabeled test set as input and returns its best estimate of the class distribution. 
at first  this task may seem almost trivial  but experience proves otherwise.  the ubiquitous practices of random sampling and cross-validation in machine learning research completely hide the problem  since the training class distribution is chosen to match that of testing.  this has perhaps suppressed recognition of this important research setting  as well as its more complicated testing methodology  which we address in section 1 to evaluate the quantification methods proposed in section 1. 
1. the cost quantification task for machine learning:  given a labeled training set  induce a cost quantifier that takes an unlabeled test set as input and returns its best estimate of the total cost associated with each class  as determined by a cost attribute on each record  which in some settings may have missing values . 
for example  consider the task of estimating the total time spent  labor cost  by technical support agents dealing with calls related to problem x each month.  one solution is to train a binary 
table 1. parameters varied in the experimental comparison 
p = 1...1 	positives in training set n = 1...1 negatives in training set p  = 1...1% 	percent positives in test set benchmark =  	1 binary text classification tasks  x 1 splits 
learning algorithms: 
svm 	linear support vector machine 
nb 	multinomial naive bayes 
performance metrics: 
abs.err  |estimated p  -  actual p| 
bias 	  estimated p  -  actual p ce 	  normalized cross-entropy 
methods: 
cc 	classify & count 
ac 	adjusted cc  plus variants with selected thresholds 
ms 	median sweep of ac at all thresholds 
mm 	mixture model 
classifier to recognize problem x  and for each monthly batch of call logs  return the sum of the cost values of all cases predicted positive.  partly due to reasons stated above  this is a poor approach.  we address this and propose methods in section 1. 
a flip-side benefit of quantification technology is that to obtain an equivalent accuracy of the count  a much less accurate classifier can be used. this enables some applications of machine learning where otherwise its classification accuracy would be unacceptable. this can also result in savings in the labor cost to develop labeled datasets to train classifiers.  at first blush  this savings may not seem substantial.  however  at hewlett-packard we train thousands of classifiers to analyze technical support logs to track the many different types of support issues that arise for our many different product lines . as a concrete example: monitoring for an increase in the incidence rate of cracked screens on hp ipaq handheld products.  further  the training needs are ongoing because of concept drift  as well as the introduction of product lines and new support issues.  so labor savings from quantification technology continues to accumulate over time. 
we finish this section with a discussion of related work  before we introduce and then test methods for quantification in the subsequent sections.  in this paper  without much loss of generality  we primarily focus on two-class tasks in order to simplify the exposition and resolve a key sub-problem for the multi-class setting  section 1 .  in this binary setting we can speak of estimating the number of positives in a test set. 
1 related work 
most supervised machine learning research attempts to optimize the correctness of individual classifications in one way or another.  hence  each classification can be judged independently  and the success of a method can be judged by its accuracy  error rate or fmeasure averaged over a large benchmark of tasks.  in contrast  quantification produces a single output for a whole batch of items.  these outputs must be aggregated over many batches in a large benchmark to evaluate methods. hence  the research methodology is unusual. 
it bears a superficial resemblance to the batching used in research for probability estimating classifiers.  for example  if a meteorologist predicts the chance of rain at 1% on certain days of the year  we would like it to rain on 1% of those days.  the correctness of a prediction on a single day cannot be judged.  some methods even require examination of the entire test set before producing any output. however  probability estimation  like traditional classification  continues to make individual predictions on each item  and judge them in aggregate.  by contrast  quantification makes a single prediction based on an entire batch-a single scalar for two-class tasks.  this batching requirement calls for a different research methodology  section 1 . 
intuitively  not having to make individual predictions should make the estimation task easier.  an insurance company can estimate how many cars will have accidents next year  but cannot predict which ones.  the nature of the uncertainty is shifted from the individual cases to the aggregate count.   
regarding probability estimation:  one obvious idea for a quantification method is to induce a classifier that outputs calibrated probability estimates  and then to sum these probabilities over the test set to estimate the count for each class.  this has an intuitive advantage over simply counting discrete predictions made by a traditional classifier  which loses information about the uncertainty of individual predictions.  nonetheless  this obvious method is ill-posed: the calibration depends critically on the class distribution of the training set  which does not generally match that of the test set in quantification  cf. it always matches under cross-validation . 
estimating the class distribution of a target dataset is not new.  but existing work in machine learning estimates the test class distribution in order to adjust the classification threshold  e.g. 1 1 . again  the objective metric in such research has been the correctness of the individual classifications.  to our knowledge  ours is the first work to empirically compare and determine machine learning methods that excel in estimating the class distribution.  this paper extends our recent publication  with superior methods  as well as a more focused experiment protocol.   
of course  once accurate and robust methods are established for estimating the distribution  they can be used as a subroutine for the traditional purposes of calibrating probability estimating classifiers  or optimizing the classification decision threshold to minimize cost  e.g. in roc analysis . 
as a side note  there is unsupervised work in tracking shifting topic distributions  e.g. 1 . it naturally has uncalibrated cluster boundaries  having no bearing on supervised quantification. 
1. quantification methods 
as a strawman method  consider simply learning a state-of-the-art binary classifier from the training set  and counting the number of items of the test set for which it predicts positive. we call this simple method classify & count  cc . the observed count of positives from the classifier will include true positives tp and false positives fp.  ideally  we would like to adjust the observed count somehow for the false positives and false negatives. by the following characterization  we derive such a quantifier  the adjusted count  ac  method : 
 	 	 	 	   classifier prediction: actual class: 	 	 	pos 	neg 
 	tp = tpr * positives fn  	fp = fpr * negatives 
 	 	= fpr *  total - positives  tn positives 
 negatives 
where tpr is the true positive rate of the classifier  p predict + | actual pos   and fpr is its false positive rate  p predict + | actual neg .  the observed count of positives is then: 
 	pos   = tpr * positives  +  fpr *  total - positives  
 	 	=  tpr - fpr  * positives  +  fpr * total 	 
solving for the actual number of positives  we get: 
positives =   pos -  fpr * total   /   tpr - fpr  
finally  dividing both sides by the total  we express the equation in terms of percentages of positives: 
	adjusted estimate p'  =   observed % positives  - fpr   	 1 
                           tpr - fpr 
it remains only to estimate the fpr and tpr characteristics of the classifier  which is accomplished via standard cross-validation on the training set. these characteristics are independent of the training class distribution because they treat positives and negatives separately. since these estimates may deviate somewhat 

	raw classifier scores  uncalibrated 	 
figure 1.  various threshold selection policies. 
from the actual tpr fpr rates in testing  we must clip the output of formula  1  to the range 1% to 1% in order to avoid occasional infeasible estimates.  in summary  the ac method trains a binary classifier  estimates its tpr fpr characteristics via cross-validation on the training set  and then when applied to a given test set  adjusts its quantification estimate by formula  1 . 
1 class imbalance problem 
the ac method estimates the true class distribution well in many situations  but its performance degrades severely if the training class distribution is highly imbalanced  e.g. figure 1 in  .  for example  with p=1 positive training cases and n=1 negative training cases  the induced classifier is very conservative about voting positive.  if the positive class is rare enough  it will simply vote negative always  i.e. tpr=1%.  while this may result in optimal classification accuracy  for the distribution in the training set   it is useless for estimating the distribution of the test set.  backing off from this extreme  consider a classifier that is very conservative but not entirely negative.  its true positive rate tpr would be very low  and its false positive rate fpr would be nonzero.  this forces the denominator  tpr - fpr  of formula  1  to a small range  making the quotient highly sensitive to any error in the estimate of tpr and fpr  giving bad estimates under imbalance. 
unfortunately  high class imbalance is pandemic  esp. to our business applications  and so the need to operate well under these conditions is important.  a well known workaround is simply to disregard many cases in the majority class of the training set to bring it back into balance.  but this throws away information. we can do better. having investigated the reason for the degradation  we have devised new methods that are resilient under class imbalance and take advantage of any surfeit of negative training cases  which are often freely available. 
1 imbalance tolerant methods 
the solution to the class imbalance problem lies in recognizing that accuracy on individual classifications is not important to our objective.  we will select a different decision threshold for the classifier that will provide better estimates via formula  1   although the threshold may be completely inappropriate for maximizing classification accuracy. specifically  we will admit many more false positives to avoid a threshold in the tails of the curve  where estimates of tpr fpr are poorer. 
the remaining question is what policy to use for selecting a threshold.  to consider the possibilities  we use the illustration in figure 1.  the x-axis represents the spectrum of thresholds  i.e. the scores generated by the raw classifier.  they are uncalibrated and may take any range  e.g. the probability output by na ve bayes  which is notorious for its poor probability calibration  or the signed distance from the separating hyperplane of an svm  calibrating the svm output via a fitted logistic regression model would have no effect on the methods  since they do not use the magnitude of the x-axis except as a decision threshold .   
the descending curve shows the false positive rate fpr and the ascending curve shows the inverse of the true positive rate  false negative rate = 1 - tpr .  the inversion of tpr is visually useful to see the tradeoff with fpr.  for a perfect classifier  there would be perfect separation between these two curves  this never occurred in our benchmark tasks .  these example curves represent an svm classifier whose natural threshold of zero delivers 1% classification accuracy for an imbalanced training set having 1 positives and 1 negatives.  because negatives abound  svm naturally optimized for a very low false positive rate  even at the cost of a 'few' misclassified positives  1 of 1 . this explains its poor f-measure of 1%. 
the basic ac method uses the classifier's default threshold  which will be far in the fpr tail if positives are rare in training. an intuitively better threshold policy is where the two curves cross  where fpr = 1-tpr  labeled x in figure 1 .  this  x  method nicely avoids the tails of both curves.  considering the earlier discussion of small denominators  another likely policy is where the denominator is maximized: method max = argmax tpr-fpr .  more traditionally  a neyman-pearson criterion would select the threshold at a particular true positive rate  method t1 = 1%  t1 = 1%   or false positive rate  f1 = 1% . we tested all these threshold policies and others. 
1 median sweep  ms  
all the threshold selection methods above run the risk that the tpr fpr estimates from cross-validation at their chosen threshold do not happen to match the actual rates encountered on the test set.  for this reason  we consider an additional approach: obtain an estimate at every threshold  and return a mean or median of these estimates. median is preferred  as it is less sensitive to outliers. specifically  the median sweep  ms  method computes the distribution estimate via formula  1  for all thresholds  and returns the median.  considering the earlier discussion about high sensitivity when the denominator becomes small  we also evaluate a variant  ms1  that considers only thresholds where the denominator  tpr-fpr  is greater than 1. 
1 mixture model  mm  
for comparison  we also include a robust quantification method from our earlier work  which is based on completely different principles.  due to space limitations  we cannot describe the method fully here  but we refer readers to .  in short  it models the distribution of raw classifier scores generated on the test set as a mixture of two distributions: the classifier scores on the training negatives  and those on the training positives-as determined by cross-validation.  an optimization step determines the mixture that results in the best fit  and then it returns this as the estimated % positive.  this method is surprisingly robust with even as few as 1 training positives  ! .  while it does not apparently suffer from the class imbalance problem  we shall see that the median sweep methods often surpass it. 
1. experiment methodology 
as mentioned in the introduction  quantification research necessitates a substantially different experiment methodology.  in particular  the class distribution must be varied independently and dramatically between the training and testing sets.  further  since each batch produces only a single estimate  we must test on many batches and aggregate measurements to identify trends. the ideal quantification method will generate accurate estimates  despite wide variation in training and testing conditions. 
to vary the training conditions  we randomly select p=1...1 positive training cases  and n=1 or 1 negative training cases from the benchmark dataset at hand.  these sizes are selected to cover common operating ranges of interest to our business applications  and are reasonable for many other situations.  the larger number of negatives represents a common multi-class case where we consider one class at a time against many others that each has 1...1 example cases.     
to vary the testing conditions  we select as many positives and negatives from the remaining benchmark dataset such that the percent positives matches our target p.  for a specific dataset having 1 positives and 1 negatives  we first take 1 positives and 1 negatives out for training  subsetting these for varied training situations   leaving 1 positives and 1 negatives.  when targeting p=1% positive  we test with all 1 positives and a random subset of 1 negatives; for p=1%  we use a random subset of 1 positives against all 1 negatives.  our business datasets often have  1 cases to quantify  but are not publishable and rarely labeled with ground truth. 
our prior study measured performance on test sets ranging from p=1%...1% positive  stepping by 1%.  while reasonable scientifically  this does not focus on the area of interest for the business problems we face: 1...1% positives is a more challenging and more important range in which to estimate well.  furthermore  we speculate that this range is preferable to study in general.  four loose arguments:   a  for  1% positive  one might simply reverse the meaning of positive and negative.   b  a common type of quantification task has many mutually exclusive classes  therefore most classes are moderately rare in order to sum to 1%.   c  for ~1...1% positive  class imbalance is not a problem  and classifiers tend to operate better in this region.   d  finally  to get tight confidence intervals when estimating the percent positive  e.g. by manual testing  many more cases must be examined if positives are rare-so  the labor savings of automatic quantification is much greater in the tails.  
1 error metrics 
a natural error metric is the estimated percent positives minus the actual percent positives. by averaging across conditions  we can determine whether a method has a positive or negative bias.  but even a method that guesses 1 percentage points too high or too low equally often will have zero bias. for this reason  absolute error is a more useful measure. but it is unsatisfactory in this way: estimating 1% when the ground truth is 1% is not nearly as 'bad' as estimating 1% when the ground truth is 1%.  for this reason  cross-entropy is often used as an error measure. to be able to average across different test class distributions  however  it needs to be normalized so that a perfect estimate always yields zero error.  hence  we use normalized cross-entropy  defined as: normce p q   =  ce p q  - ce p p       1  
ce p q   =  -p log1 q   -   1-p  log1-q  
table 1.  benchmark classification tasks. 
# datasetclass positives negatives total 1 fbis 1 1 1 1 1 fbis 1 1 1 1 1 fbis 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 la1 1 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1 1 ohscal 1 1 1  
where q is the estimate of the actual percent positives p in testing.  since cross-entropy goes to infinity as q goes to 1% or 1%  we back off any estimate in these situations by half a count out of the entire test set.  matching our intuition  this back-off will increasingly penalize a method for estimating zero positives for larger test sets: it is worse to mistakenly estimate zero positives among thousands of test cases than among ten. 
1 datasets 
the benchmark text classification tasks are drawn from ohsumed abstracts  ohscal   the los angeles times  la   and the foreign broadcast information service  fbis  ; the feature vectors are publicly available for download from the journal of machine learning research . see table 1. for this suite of experiments  we consider the binary classification task of one class vs. all others. only 1 of the 1 potential binary tasks suited our needs  because we required a large number of positives and negatives for this study.  our prior study went up to 1 training negatives  but this depletes the supply of negatives for testing  leaving only 1 suitable binary tasks.   f-measure for these tasks averages in the mid-1's. 
1 learning algorithms 
we use the linear support vector machine  svm  implementation provided by the weka library v1 .  we also repeated the experiment with the multinomial na ve bayes classifier  which has respectable performance in the text domain.  
the adjusted count methods and the mixture model all require cross-validation on the training set to generate the distribution of scores for positives and negatives  in order to characterize tpr and fpr.  we chose 1-fold stratified cross-validation.   note that if there are fewer than 1 positives in the training set  it feels 

p training positives   n=1 negatives 	p training positives   n=1 negatives 	 
figure 1.  absolute error for target p=1% positives only. 

	p training positives   n=1 negatives 	p training positives   n=1 negatives 
figure 1.  cross-entropy for targets p=1...1% averaged. 

undesirable that some test folds will contain no positives. but we found only degradation trying 1- or min 1 p n -folds instead.  
a great deal of computer time can be saved by sharing the computation that is common to all methods:  a  the 1-fold cross validation on the training set   b  training the final classifier on the training set  and  c  applying that classifier to the test set to generate scores for the positives and for the negatives.  different subsets of these scores can then be used to evaluate various quantification methods under different test class distributions.  sharing steps 1 reduced the overall computational load of this experiment by a factor of ~1. as it was  the experiment consumed several days on hundreds of fast 1-bit cpus in the hp labs utility datacenter. the complete experiment protocol is listed in pseudo-code in the tech report version of this paper.  
1. experiment results 
given the large number of parameters in this experiment  we break down the results into sections where we hold some conditions constant as we vary others.  for each figure  we will have a pair of graphs: n=1 training negatives on the left  and n=1 training negatives on the right; we take care that the yaxis range is identical for easy comparison. every data point represents an average performance over the 1 benchmark text classification tasks times 1 random splits. except where stated otherwise  we focus on the svm base classifier. 
1 varied training   fixed target p=1%  
we begin by examining how resilient the various quantification methods are to wide variations in the training set  while we hold the test conditions fixed at p=1% positive.  figure 1 shows the accuracy of the quantifications  as measured by absolute error from the 1% target. overall we see the median sweep methods dominate  ms and ms1  bold lines .  note the absolute scale: the ms methods estimated on average within two percent given only p=1 positives and n=1 negatives  or within one percent given p=1 & n=1  e.g. estimating 1% positive when the ground truth is 1% .  note the performance with p=1 is nearly as good as at p=1. when labor costs are involved to manually label training cases  this can amount to significant savings. 
in the graph for n=1 training negatives  left   the simple classify & count  cc  method achieved the lowest absolute error  but only for very specific training conditions. we seek methods with consistently good predictions  despite training variations.  

 1 1 1 1 1 1 1 target p% positives     n=1 negatives  target p% positives     n=1 negatives 
figure 1.  absolute error for targets p=1...1% individually.   p=1 training positives. 

	target p% positives     n=1 negatives 	target p% positives     n=1 negatives 
figure 1.  bias. average signed error for targets p=1..1% individually.   p=1 training positives. next consider n=1 negatives  right : cc appears competitive when given a large enough set of training positives  but this is illusory. deeper examination reveals that for smaller p it underestimates and for greater p it progressively overestimates. the training class prior is simply moving the decision threshold  resulting in more positive predictions  not better quantification. by contrast  the basic adjusted count  ac  method does converge to better quantification once sufficient training positives are available. with p 1 & n=1  however  the class imbalance is so great that the ac method gets severe error.  our prior work highlighted the remarkable stability of the mixture model  mm  method even with great imbalance. mm is dominated  however  by many of the methods in this paper designed to address the class imbalance problem. interestingly  even without class imbalance  p¡Ön=1 at left   the ms and t1  tpr=1%  methods surpass prior methods. we return to this point in the discussion. 
 a few mediocre methods were omitted from the graphs to improve readability. the tech report version of this paper available at hp has a color appendix containing all the results.  
1 varied training   varied target 
the analysis above was for a single target of p=1% positives. does the strong result for median sweep generalize for p=1...1% positives   to check this  we average the performance over this whole range.  as discussed in the methodology section  to average different targets together  we must not simply use absolute error  but rather normalized cross-entropy. see figure 1 for these results. though the y-axis scale is now different than figure 1  the rankings are qualitatively similar.  median sweep continues to dominate on average. 
to determine whether median sweep dominates for all target distributions  we expanded the study up to 1% test positives.  
instead of averaging over this range  we spread out p=1..1% along the x-axis in figure 1; but to keep the focus on the lower region  we used a log-scale x-axis.  since we are not averaging results across different values of p  the y-axis shows absolute error  which is more intuitive than cross-entropy.  to view the results in two dimensions  we must hold some other variable constant:  we fix p=1 training positives  where performance is relatively insensitive to changes in p.   
in the low p range  median sweep methods excel.  in the higher range  two methods excel consistently:  max  maximize tpr - fpr  or x  where fpr and 1-tpr cross .  but in the following analysis we shall see that the max method suffers from systematic bias.   
finally  the absolute error grows substantially for all methods as p approaches 1%. but this is not especially concerning:   a  it is a rare operating range   b  if positives are so prevalent  it is more likely that the classifier would be trained with a positive majority rather than a negative majority as we have done here  and  c  in order to experiment at 1% positives  we end up with very few test negatives  due to the shortage of positive training cases.  for example  if only 1 positives are available for testing  we end up with merely 1 test negatives in order to achieve p=1%.  so  the experimental results have naturally higher variance in this region. if one needed to research this region more effectively  larger benchmark datasets are called for  or else the meaning of positives and negatives might be reversed. 

	target p% positives     n=1 negatives 	target p% positives     n=1 negatives 
figure 1.  like figure 1  but for the held-out dataset  and greater training class imbalance.  p=1 training positives.  1 bias analysis 
next we analyze the other component of accuracy-bias-by presenting the average signed error for each method.  figure 1 shows the bias under varied training  left vs. right  and testing conditions  p% positives on x-axis .  we abandoned the log-scale here in order to show the strongly linear bias of two methods:  max and classify & count.  for the classifier trained with 1% positives  p=n=1  at left   the cc method progressively overestimates when p 1%  and underestimates when p 1%  as expected.  when trained with 1% positives  p=1  n=1  at right   this balance point is shifted accordingly  but not proportionately-it is unbiased only at p=1% instead of 1%.  we have seen this behavior consistently: svm exaggerates the training class imbalance in testing. although the ubiquitous suggestion is to bias the svm cost penalty c  it proves ineffective and has been better addressed recently by wu and chang . 
it is surprising that the max method  being an adjusted count variant  also exhibits a linear bias  albeit to a lesser degree than cc. this means that the max method consistently finds thresholds such that the tpr and fpr characterization does not hold well for the test set.  all the other methods have a relatively stable bias over a wide range.  
we see greatly increasing bias at the tails  which is expected:  if a method's estimates vary by a few percent and p is close to zero  any infeasible negative estimates are adjusted up to 1%  resulting in a positive bias.  as we approach p=1% positives  the even greater negative bias is similarly caused by clipping estimates which have greater variance  as shown previously. 
1 failure analysis  
although an induced classifier should learn to separate cases well enough that its true positive rate tpr is greater than its false positive rate fpr  they nonetheless fail sometimes. this usually happens under great class imbalance in training.  for example  for one of the ten splits on one of the tasks trained with p=1 and n=1  the induced classifier's natural threshold gave tpr=fpr=1. it learned to classify everything as negative  which results in a troublesome zero denominator in the adjusted count method. the commonness of this problem was part of the impetus for this research:  tpr was less than or equal to fpr in 1 of 1 cases for ac.  in progressively decreasing occurrence of failure  we have: ac  t1  f1  f1  t1 and x.   the max method never experienced a failure  exactly because it seeks to maximize the denominator.  naturally  this is a non-problem for either cc or the mixture model. 
1 na ve bayes vs. svm 
we do not present graphs for the na ve bayes classifier because every quantification method under every training/testing condition performed substantially better on average with svm as the base classifier.  it is well established for text classification that svm usually obtains better accuracy than na ve bayes.  but our finding further suggests its tpr and fpr characteristics may be more stable as well.  
1 greater imbalance   held-out dataset 
given the consistent performance of the median sweep methods  we would like to ensure their continued performance in situations with even greater training class imbalance  ~1%   such as we face in practice.  this study so far has been limited to n=1 training positives  in order to have 1 benchmark tasks for study.  although we could increase class imbalance by simply reducing p  this results in degenerate classifiers.  instead  we would like to consider a greater number of negatives.  in addition  we want to validate these results against other classification problems. 
for these two purposes  we held back a dataset: new1 from . it has 1 cases partitioned into 1 classes. we repeated our study on its 1 classes that have at least 1 positives  setting aside 1 negatives for training.  figure 1 shows these results for p=1 and n=1 negatives  ~1% positives in training  right  and for a subset of n=1 negatives  ~1%  left .  the log-scale x-axis shows p  and the y-axis shows average absolute error.  although perhaps uninteresting for its similar results to figure 1 with n=1  it is encouraging that the conclusions generalize to heldout tasks and to greater training imbalance.  the median sweep methods continue to estimate well for low p; they have  1% absolute error for p =1% in both graphs of figure 1 and for n=1 in figure 1.  the competitive methods max and x have somewhat improved performance for this hold-out benchmark  and now slightly beat ms for p as low as ~1%. 
1. discussion 
observe in figure 1 that the curves cluster into two shapes: concave upward  ms ms1 t1  and s-curve  max x mm .  
interestingly  the ac method under high imbalance  n=1 and 

all of figure 1  belongs to the concave upward group  but under balanced training  n=1 in figure 1  belongs to the s-curve group.  as discussed previously  the ac method under high imbalance uses thresholds with many false negatives  i.e. closer to t1 in the concave upward group.   recall that t1 selects the threshold at tpr=1%.   but under more balanced conditions  ac uses thresholds closer to the x crossover point in figure 1  which results in the s-curve grouping.  looking now at ms  its consistent grouping with t1 suggests it may be using many estimates derived from tpr rates nearer to 1% than near the cross-over point x. 
we set out to address a problem that occurred under class imbalance  and we ended up discovering methods that estimate substantially better even under balanced training  e.g. median sweep and t1.  see p~n=1 in figures 1.   since the adjusted count formula is unchanged from the basic ac method  this implies t1 selects a threshold for which tpr fpr characterization on the training set is more reliable than the default threshold.  this may provide a clue to the separate problem of estimating tpr fpr well for other purposes. 
we believe the reason that median sweep works so well is that instead of relying on the accuracy of a single tpr fpr estimate  it takes in information from all the estimates  of which many are likely close. in some sense  it has the advantage of bootstrapping  without the heavy computational cost of repeating the 1-fold cross-validation for many different random samplings of the available training set. 
until now we have referred to the median sweep methods together.  overall they perform very similarly  which we should expect since the median is very insensitive to outlier clipping  unlike the mean.  even so  the ms1 variant-which only considers estimates that come from larger  more stable denominators-shows a slight benefit  particularly in lower bias and over a broader range. this validates the idea that small denominators result in poorer estimates.  putting ms1 into production runs the risk that on some tasks there may no estimates with a sufficiently large denominator  although this never happened in our benchmark tasks. at the very least  it could fall back to ms in such cases.  further research may develop a more robust outlier clipping method that could improve median sweep methods.  
one motivation mentioned in the introduction for quantification research is reduced training effort to obtain a given level of accuracy.  to illustrate this  note in the right-hand graph of figure 1 that median sweep methods with p=1 positives achieve similar accuracy to ac with p=1.  but for the basic classify & count method  additional training does not lead to an accurate  unbiased quantifier. furthermore  in the left-hand graph of figure 1 we see that additional training examples mislead ac.  the point is this: quantification research is essential because accurate estimates cannot be achieved by simple methods like cc or ac just by providing more training data  unlike active learning research where all methods produce the same classification accuracy given enough training cases . 
although we are pleased to have reduced the absolute error of the estimate to less than 1% in many situations  we need to quantify increasingly rare events  where the bias and the relative error both grow.  to conduct experiments in the tail of the distribution requires much larger labeled datasets made available for research.  
1. extensions 
the implications of this work extend to trending over time  multiclass quantification  and quantification of costs  which we describe in sequence. 
1 trending 
measuring trends over time was listed as a primary motivation  but so far we have only discussed quantifying the class distribution of a single test set.  in order to apply this technology for trending  the cases are partitioned into discrete bins  e.g. daily or monthly groupings  and the quantification is performed independently on each batch to obtain accurate estimates.  these may then be plotted together in one graph  optionally with a fitted trend line to project into the future where no cases are yet available.  as is typical with such applications  if there are too many bins for the volume of data  the counts in each bin become small and noisy.  the quantification methods we describe are intended to work on large batches. they will produce noisy estimates given only a handful of items. for more accurate quantification in these situations  we have used a sliding window technique to aggregate cases from adjacent bins into each batch.  at the same time  this provides smoothing like a moving average  e.g. to smooth over weekend-effects. 
note that this work addresses changes in the class distribution but not general concept drift  where the definition of what counts as positive may gradually or suddenly change arbitrarily .  when trending over time  concept drift is often implicated  and can be difficult to cope with. ideally the quantifier used on each bin is given a training set appropriate to the class concept in effect for that bin.  naturally this can be hard to determine  and requires ongoing training data.   
regardless of concept drift  if additional training cases become available later-e.g. some cases are labeled in a new monthly batch of data-it is best to redo the trend quantification over all bins. the additional training data may improve the quantifier's estimates on the old bins as well.  if instead one applies the improved quantifier only to the new batch of data  this estimate should not be appended to pre-existing trend lines.  to do so would compare estimates that are not calibrated to one another. 
1 multi-class quantification 
in our use of quantification  we usually want to track the trends for many classes  e.g. different types of technical support issues.  since most customers call with a single problem  these classes are competing  and therefore may be treated as a 1-of-n multi-class problem. on the other hand  occasionally multiple issues do apply to a single case. if there were a rampant trend of coinciding issues  we would not want to have designed the system to be blind to it. hence  we quantify each issue independently  i.e. as an mof-n multi-class topic recognition task. 
nonetheless  there are situations where the 1-of-n multi-class setting is called for.  to treat this  one should not simply apply a multi-class classifier to the dataset.  if some classes are much rarer than others either in the training set or in the test set  the test set counts predicted for those classes may be very rare. the adjusted count method applied then to each class will not lead to good multi-class estimates. 
instead we recommend performing independent quantifications for each class vs. all others  and then normalizing the estimates so they sum to 1%.  in this way  each independent quantification compensates for imperfect classification and for class imbalance. 
1 cost quantification 
simply estimating the number of cases belonging to a category may not correlate with importance.  a relatively uncommon issue having a high cost can be more important to delve into than a more frequent issue having low cost.   
if the average cost per positive case c+ is known in advance  it can simply be multiplied into the quantifier's estimate to obtain the total cost of the positive cases.  more commonly c+ is not known  and we must analyze the cost attribute attached to each case  e.g. the parts & labor cost to repair each problem.  consider a rare subclass of repairs whose costs climbed substantially in the new month of data. 
1.1 cost quantification methods 
classify & total:  the obvious solution  akin to cc  is to train a classifier and total the cost attribute associated with all cases predicted positive. but unless that classifier is perfectly accurate  it will result in poor and systematically biased cost estimates. 
grossed-up total:  the next obvious solution is to perform the total as above  but then to adjust it up or down according to a factor f determined by quantification.  if the binary classifier predicted 1 positives and the quantifier estimates 1 positives  then the cost total would be multiplied by f=1/1.  but this method suffers from similar problems as ac:  it runs the risk that the binary classifier may select zero or very few cases to include in the total  if positives happen to be rare in its training or test sets.  else if positives were overly common in the training set  then the induced liberal classifier will include in its total the costs of many negatives  polluting the result.  this pollution occurs even at the perfect ratio if there are some false positives. 
conservative average * quantifier  caq :  we can reduce the false-positive pollution by selecting the classifier's decision threshold to be more conservative-a classic precision-recall tradeoff.  using a smaller set of highly precise predictions  we can average their costs to estimate c+  and then multiply it by the estimated size of the class from a quantifier.  ideally we would like a threshold with 1% precision  but often there is no such threshold.  furthermore  a highly conservative threshold based on precision may predict only a few cases as positive  esp. if positives are rare.  given too few items to average over  the variance of the c+ estimate will be large  giving a poor overall cost estimate. to avoid this problem  one might instead always take the top  say  1 most strongly predicted positives for the average. but this cannot ensure high precision-some test sets might have only 1 positives. 
precision-corrected average * quantifier  pcaq :  despite decreased precision  there is pressure to use a less conservative threshold for the reasons above  and also because at high precision/low recall the classifier's precision characterization from cross-validating the training set has high variance.  in perfect analogy to the first part of this paper  we select a classification threshold with worse precision  but having more stable characterization as well as providing a sufficient number of predicted positives to average over.  we then adjust the average according to a simple equation that accounts for the false-positive pollution: 
	precision-corrected average c+  =   1-q  ct  -  1-pt  call  	 1 
                                                 pt - q 
where q is a quantifier's estimate of the percentage of positives in the test set  pt is an estimate of the precision at a given threshold t  ct is the average cost of all cases predicted positive up to the threshold t  and call is the average cost of all cases.  the derivation is in the appendix of the tech report version of this paper available online. the remaining design decision is which threshold to use-for example  the t1 or x thresholds shown in figure 1.  we suggest avoiding max  given our earlier bias discussion about its choosing thresholds with poor tpr fpr characterization. 
median sweep pcaq:  rather than use a single threshold and hope that its precision characterization is accurate  we may sweep over many thresholds and select the median of the many pcaq estimates of c+. this has some of the benefit of bootstrapping without the computational cost.  just as the ms1 method excludes estimates that are likely to have high variance  a profitable variant on this method might exclude estimates from thresholds where  a  the number of predicted positives falls below some minimum  e.g. 1   b  the confidence interval of the estimated c+ is overly wide  and/or  c  the precision estimate pt was calculated from fewer than  say  1 training cases predicted positive in cross-validation. 
mixture model average * quantifier  mmaq :  finally  rather than try to determine an estimate at each threshold  we can model the shape of the ct curve over all thresholds as the mixture  
	ct  = pt c+ +  1-pt  c- 	 1 
where c- is the average cost of a negative case  which is also unknown . this method estimates c+  and c-  via linear regression of the points generated at many different thresholds.  the same thresholds omitted by median sweep can be omitted here as well  in order to eliminate some outliers that may have a strong effect on the linear regression.  alternately  one may use regression techniques that are less sensitive to outliers  e.g. that optimize for l1-norm instead of mean squared error. 
1.1 evaluation 
we found mmaq outperformed caq in a small test. the next logical research step is to evaluate all these methods against one another. unfortunately  any such empirical experiment depends strongly on the cost distribution for positives vs. the cost distribution for negatives  including their relative variances   in addition to variation in the training set makeup and the test class distribution.  besides its being a high dimensional experiment  we must first have a large publishable benchmark with costs of reasonable interest to a family of applications. this is an open invitation to the research community. 
1.1 missing costs 
in some settings  especially those in worldwide enterprises  cost values may be missing or detectably invalid for some cases. given that most of the above methods begin by estimating the average cost for positives c+  such cases with missing cost may simply be omitted from the analysis. that is  the estimate of c+ is determined by the subset of cases having valid cost values  and the count is estimated by a quantifier run over all the cases.  this can be effective if the data are missing at random  mar . 
however  if the mar-assumption does not hold  the missing values should first be imputed by a regression predictor. 
1.1 cost-confounded prediction 
the methods above implicitly assume that the cost of positive cases is not correlated with the prediction strength of the base classifier.  as an assurance  one may check the correlation between cost and the classifier scores over the positive cases of the training set. if the classifier predicts the most expensive positives strongest  then the methods above  esp. caq  will overestimate badly.  negative correlation results in underestimates.  this problem also arises if the classifier's scores have substantial correlation with cost for negative cases.  
to avoid these problems  we recommend the cost attribute not be given as a predictive feature to the classifier. if the average cost for the positive class c+ is similar to the overall average  then this attribute will generally be non-predictive. but in the interesting case where it is substantially different from the background  this feature may be strongly predictive  e.g. a rare but relatively expensive subclass.  in this case  it is tempting to provide cost as a predictive feature to improve the classifier.  but it is better not to: the methods are explicitly designed to function despite imperfect classifiers. 
1. conclusion 
it is fortunate that quantification can be made to compensate for the inaccuracy of a classifier  yielding substantially more precise and less biased estimates. this requires only small amounts of training data  which can reduce labor costs compared with having to train highly accurate classifiers. these factors can lead to greater acceptance of machine learning technology for business use.  we have been pushing machine learning within our company for years  but have never before experienced the business pull we find for quantification .  to data mining researchers who wish to apply and develop advanced prediction models  this comes as some surprise  since the task seems so simple-at least on the surface. 
though the median sweep  max and x methods all show great improvement over prior technology  they are surely not the last word.  future work will involve research further down the tail toward greater class imbalance.  chemists easily talk about parts per million  but machine learning is currently nowhere near up to the task. to research the tail will require very large benchmark datasets  ideally publishable ones for repeatability and experimentation by others. studying high class imbalance requires that the data set labels not have mistakes  for the conclusions are more sensitive to any noise in the answer key. ideally  such a dataset would include individual costs to support research in cost quantification.  the most effective methods may depend strongly on the characteristics of the data  so hopefully such a dataset would suit a popular family of applications. other research directions are in multi-class methods  possibly including class hierarchies  or quantification under various constraints  such as having less tolerance for underestimating the size or cost of a subclass  as motivated by some business applications. finally  trending over time naturally introduces concept drift  which is a challenging but important area for research. 
 
1. acknowledgments 
i wish to thank my colleagues jaap suermondt  evan kirshenbaum  jim stinger  tom tripp  and farzana wyde for their contributions in conceiving and developing this application.  thanks also to bin zhang for pre-reviewing this paper. 
