automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legacy sources and text collections into a data warehouse for subsequent querying  analysis  mining and integration. in this paper  we mine tables present in data warehouses and relational databases to develop an automatic segmentation system. thus  we overcome limitations of existing supervised text segmentation approaches  which require comprehensive manually labeled training data. our segmentation system is robust  accurate  and efficient  and requires no additional manual effort. thorough evaluation on real datasets demonstrates the robustness and accuracy of our system  with segmentation accuracy exceeding state of the art supervised approaches.
categories and subject descriptors
h.1  database management : database applications-data mining; i.1  artificial intelligence : learning
general terms
algorithms  design  performance  experimentation
keywords
text segmentation  machine learning  text management  information extraction  data cleaning
1. introduction
　information in unstructured text needs to be converted to a structured representation to enable effective querying and analysis. for example  addresses  bibliographic information  personalized web server logs  and personal media filenames are often created as unstructured strings that could be more effectively queried and analyzed when imported into a structured relational table. building and maintaining large data warehouses by integrating data from such sources requires automatic conversion  or segmentation of text into structured records of the target schema before loading them into relations.
　informally  the problem of segmenting input strings into a structured record with a given n-attribute schema is to partition the string into n contiguous sub-strings and to assign each sub-string to a unique attribute of the schema. for instance  segmenting the input string  segmenting text into structured records v. borkar  deshmukh and

 work done at microsoft research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
sarawagi sigmod  into a bibliographic record with schema  authors  title  conference  year  requires the assignment of the substring  v. borkar  deshmukh and sarawagi  to the authors attribute  the sub-string  segmenting text into structured records  to the title attribute   sigmod  to the conference attribute  and the null value to the year attribute.
　current techniques for automatically segmenting input strings into structured records can be classified into rule-based and supervised model-based approaches. rule-based approaches require a domain expert to design a number of rules and maintain them over time. this approach does not scale as deployment for each new domain requires designing  crafting  deploying  and maintaining a new set of rules. supervised approaches alleviate this problem by automatically learning segmentation models from training data consisting of input strings and the associated correctly segmented tuples . however  it is often hard to obtain training data  especially data that is comprehensive enough to illustrate all features of test data. this problem is further exacerbated when input test data as in our target data warehouse scenario is error prone; it is much harder to obtain comprehensive training data that effectively illustrates all kinds of errors. these factors limit the applicability and the accuracy of supervised approaches.
　in this paper  we exploit reference relations-relations consisting of clean tuples-in typical data warehouse environments. for example  most data warehouses include large customer and product tables  which contain examples that are specific to the domain of interest. reference relations can be a source of rich vocabularies and structure within attribute values. our insight is to exploit such widely available reference tables to automatically build a robust segmentation system. note that we do not attempt to match the newly segmented records with the tuples in the reference table. record matching and deduplication are different problems and are not the focus of this paper.
　our approach relies only on reference tables: we do not need the association between unsegmented input strings and the corresponding segmented strings  labeled data  that supervised systems require for training. current operational data warehouses and databases do not maintain such associations between input data and the actual data in the database  because input data goes through a series of potentially complex transformations before it is stored in the database.
　building segmentation models from clean standardized tuples in a large reference table requires three critical challenges to be addressed. the first challenge is that information in reference relations is typically clean whereas input strings may contain a variety of errors: missing values  spelling errors  use of inconsistent abbreviations  extraneous tokens  etc. . therefore  the challenge is to learn from clean reference relations  segmentation models that are robust to input errors. the second challenge is that we do not know the order in which attribute values in an input string are specified. in the data warehouse maintenance scenario  the order in which attribute values are concatenated by an address data source may be  state  zipcode  city  name  address  while another source may concatenate it in the order  name  address  city  zip  state . another common example is bibliographic data: some sources may order attributes for

figure 1: architecture of the cram system
each article as  authors  title  conference  year  pages  while other sources may order them as  title  authors  conference  pages  year . therefore  for an unsupervised segmentation system to be deployed over a variety of data sources it has to deal with differences in input orders by automatically detecting the attribute order. the third challenge is that reference tables can usually be very large and consist of millions of tuples. therefore  to effectively exploit large vocabularies and rich structural information in large reference tables the algorithm for building a segmentation model from large reference tables has to be efficient and scalable. note that training efficiency is not usually an issue with supervised approaches: manually labeling data is time-consuming and expensive and hence training datasets are much smaller than sizes of available reference tables.
　due to the above challenges  it is not possible to directly adapt existing supervised approaches to the  learn from reference table only  scenario. first  we cannot use data in a reference table to prepare a training dataset for current supervised approaches because we do not know the order in which attribute values would be observed in test input strings. in fact  current supervised models heavily rely on the knowledge of input attribute order to achieve high accuracy . second  data in reference tables is usually clean whereas input data observed in practice is dirty. and  making a design goal of robustness to input errors during training itself improves the overall segmentation accuracy. third  current supervised text segmentation systems are designed to work with small labeled datasets  e.g.  by using cross validation  and hence tend not to scale to training over large reference tables.
　the architecture of our automatic segmentation system cram1 is shown in figure 1. we adopt a two-phased approach. in the first pre-processing phase  we build an attribute recognition model over each column in the reference table to determine the probability with which a  sub- string belongs to that column. this process can be customized with a domain-specific tokenizer and a feature hierarchy  i.e.  token classification scheme such as  number    delimiter   etc. . for example  the recognition model on the  zip code  column of an address relation indicates that the probability of a fivedigit number  e.g.  1  being a valid zip code is 1 whereas that of a word like timbuktoo is only 1. models on all columns can be used together to determine the best segmentation of a given input string into sub-strings. in the second run-time segmentation phase  we segment an input string s into its constituent attribute values s1 ... sn and assign each si to a distinct column such that the quality of the segmentation is the best among all possible segmentations.
contributions: in this paper  we develop a robust segmentation system which can be deployed across a variety of domains because it only relies on learning the internal attribute structure and vocabularies from widely available reference tables. we introduce robust and accurate attribute recognition models to recognize attribute values from an attribute  and develop efficient algorithms for learning these models from large reference relations  section 1 . we then develop

1
cram: combination of robust attribute models
an algorithm that effectively uses robust attribute recognition models to accurately and efficiently segment input strings  section 1 . finally  we present a thorough experimental evaluation of cram on real datasets from a variety of domains and show that our domainindependent segmentation system outperforms current state of the art supervised segmentation system  section 1 .
1. related work
　extracting structured information from unstructured text has been an active area of research  culminating in a series of message understanding conferences  mucs   1  1  and more recently ace evaluations . named entity recognition systems extract names of entities from the natural language text  e.g.  person names  locations  organizations . detecting entities in natural language text typically involves disambiguating phrases based on the actual words in the phrase  and the text context surrounding the candidate entity. explored approaches include hand-crafted pattern matchers  e.g.     rule learners  e.g.   1  1    and other machine learning approaches  e.g.   . in contrast  strings in our segmentation scenario are not necessarily natural language phrases  and do not contain surrounding contextual text.
　work on wrapper induction  e.g.   1  1  1   exploits layout and formatting to extract structured information from automatically generated html pages  as well as heuristics and specialized feature hierarchies to extract boundaries between records . in contrast  the input strings in our problem are short and have no obvious markers or tags separating elements. furthermore  all of the string except for the delimiters must be assigned to some of the target attributes. as previously shown by borkar et al.   the required techniques therefore differ from traditional named entity tagging and from wrapper induction.
　hidden markov models  hmms  are popular sequential models   and have been used extensively in information extraction and speech recognition. since the structure of hmms is crucial for effective learning  optimizing hmm structure has been studied in the context of ie and speech recognition  e.g.   1  1 .
　the problem of robustness to input errors has long been studied in speech recognition. some approaches include filtering out noise during pre-processing and training the system in artificially noisy conditions  error injection  . noise filtering from speech recognition cannot be adapted to text segmentation directly  since in our scenario the input errors are not separable from actual content. to the best of our knowledge  we are the first to explicitly address input errors in the context of text segmentation.
　past work on automatic text segmentation most closely related to ours is the datamold system  and related text segmentation approaches  e.g.   and  . these are supervised approaches and hence share the limitations discussed earlier.
　in this paper  we present a novel  scalable  and robust text segmentation system cram. our system requires only the target reference table and no explicitly labelled data to build accurate and robust models for segmenting the input text strings into structured records.
1. segmentation model
　in this section  we define the segmentation problem and introduce preliminary concepts required in the rest of the paper. we define the  quality  of segmentation of an input string into attribute value sub-strings as a function of the  probabilities  of these sub-strings belonging to corresponding attribute domains. our goal now is to select the segmentation with the best overall probability. the probability of a sub-string assignment to an attribute domain is estimated by a model called attribute recognition model  described below  associated with each attribute. consider an input string  walmart 1
s. randall ave madison 1 wi  which has to be segmented into organization name  street address  city  state  and zipcode attribute values. for example  the attribute recognition model for organization name may assign respective probabilities of 1 and 1 to substrings  walmart  and  walmart 1.  if the combination  say  product  of individual probabilities of the segmentation  walmart  as organization name   1 s. randall ave  as street address   madison  as city   1  as zipcode  and  wi  as state has the highest numeric value  we output this segmentation of the given input string. the combination of probabilities has to be invariant to changes in the order in which attribute values were concatenated. this is in contrast to traditional  tagging  approaches  which rely heavily on the order in which attribute values are concatenated  e.g   1  1  . in the rest of the paper  we use r to denote a reference relation with string-valued attributes  e.g.  varchar types  a1 ... an.
attribute recognition model  arm : an attribute recognition model armi for the attribute ai is a model for the domain of ai such that armi r  for any given string r is the probability of r belonging to the domain of ai.
optimal segmentation of an input string: let r be a reference relation with attributes a1 ... an and arm1 ... armn be their respective attribute recognition models. given an input string s  the segmentation problem is to partition s into s1 ... sn and to map them to distinct attributesas1 ... asn such that ni {armsi si }. is maximized over all valid segmentations of s into n substrings. a similar model that assumes partial independence of attribute segmentations was independently developed in .
　note that the order of attributes as1 ... asn may be different from the order of the attributes a1 ... an specified in the reference table. attribute constraints for r  e.g.  maximum attribute length  can also be incorporated into this model. and  information about the order in which attribute values are usually observed can also be incorporated. for example  if we know that the street address value usually follows the name attribute value  we can potentially bias the assignment of consecutive sub-strings  say  walmart  and  1 s. randall ave   to name and to street address attributes  respectively.
1 preliminaries
　let tok be a tokenization function which splits any string into a sequence tok s  of tokens based on a set of user-specified delimiters  say  whitespace characters . the token set of a string s is the set of all tokens in s. for example  tok v  of the tuple  boeing company  seattle  wa  1  is  boeing  company   and {boeing  company} is the token set. observe that we ignore case while generating tokens. the dictionary di of the attribute ai of r is the union of token sets of all attribute values in the projection r i  of r on ai. in the rest of the paper  we assume that strings can be segmented only at token boundaries.
hidden markov models: a hidden markov model  hmm  is a probabilistic finite state automaton encoding the probability distribution of sequences of symbols each drawn from a discrete dictionary . figure 1 a  shows an example hmm. for a sequence s of symbols each drawn from the probability distribution encoded by a hmm  we can compute the probability of observing s. a hmm comprises a set of states and a dictionary of output symbols. each state can emit symbols from the dictionary according to an emission probability distribution for that state and pairs of states are connected by directed edges denoting transitions between states. further  edges
are associated with transition probabilities. hmms have two special states: a start state and an end state. the probability of observing a string s = o1 ... ok of symbols drawn from the dictionary  is the

figure 1: a sample generalized dictionary instantiated for the street address column
sum of probabilities of all paths from the start state to the end state with k transitions. the probability of any path p is the product of all transition probabilities on each transition in p and the emission probabilities of observing the ith symbol oi at the ith state on p. the path with the highest probability is usually considered the path that generated the string s. the set of states and the set of transitions constitute the topology of a hmm. for any given application  the topology is usually fixed a priori.1 the emission and transition probabilities are then learned during a training phase over the training data.
feature hierarchy: a hmm built over a dictionary of an attribute cannot be directly used for computing probabilities of sequences with unknown tokens. however  the set of base tokens in a dictionary can be generalized to recognize unknown tokens . for example  it may be sufficient to see a 1-digit number optionally followed by a 1-digit number to recognize zip codes. the successive generalization of features  e.g.  from 1-digit numbers to all numbers  is usually encoded as a feature hierarchy. in this paper  we use a hierarchy which is similar to the feature hierarchy employed in . in our hierarchy the lower levels are more specific than higher levels. at the top level there is no distinction among symbols; at the next level they are divided into classes  words    numbers    mixed   and  delimiters.   words    numbers  and  mixed  are then divided into sub-classes based on their lengths. for example  the class of words consisting of 1 or less characters  denoted  a-z {1}  is above the class of words consisting of 1 or less characters  denoted  a-z {1} . all base tokens are at the leaf levels of the feature hierarchy. to distinguish base tokens from the generalized elements in the discussion  we refer to the non-leaf elements in the feature hierarchy as feature classes. we say that that a token t minimally belongs to a feature class f if t belongs to f but not to any feature class that is a descendant of f. for example  the zipcode value 1 is said to minimally belong to the feature class 1-digit numbers.
　observe that it is possible to input domain-specific feature hierarchies to cram in the pre-processing phase  but as we show experimentally  the default hierarchy is sufficient for a wide range of domains.
generalized dictionary: the generalized dictionary consists of all elements in the feature hierarchy in addition to the dictionary of base tokens. formally  the generalized dictionary of an attribute ai in r is the union of the dictionary di of ai and the set of feature classes in the feature hierarchy. henceforth  we use the term dictionary to denote the generalized dictionary unless otherwise specified. note that while the feature hierarchy itself is fixed across domains  the generalized dictionary is instantiated for each attribute automatically during training from the provided reference tables. a sample of a

1
 recently  techniques based on cross validation were developed to identify a good topology from among a target class of topologies .

figure 1: example hmm model  a   and our arm model  b 
generalized dictionary instantiated for the street address attribute is shown in figure 1.
1. attribute recognition models
　in this section  we discuss the efficient construction of robust attribute recognition models from a reference relation. recall that an attribute recognition model assigns probability with which a string or a sequence of tokens belongs to the attribute domain. therefore  we adopt the class of hidden markov models  hmms   a popular class for modelling sequences of elements  for instantiating attribute recognition models. instantiating an hmm requires us to define  i  the topology consisting of a set of states and the set of transitions among them  and  ii  the emission probabilities at each state and the transition probabilities between states. in this section  we describe the topology we adopt for instantiating arms and the computation of emission and transition probabilities. our primary focus in this design is  i  to improve the robustness of segmentation to input errors  and  ii  to develop an efficient and scalable algorithm for building robust attribute recognition models.
　we now discuss the intuition behind the principle  which we call specificity  that we exploit to make arms more robust to input errors. a more  specific  attribute recognition model assigns higher probabilities only to very few selective token sequences. arms can be specific in three aspects: positional specificity  sequential specificity  and token specificity. we illustrate these notions with an example. consider an hmm example in figure 1 a . that a token in the street address value ending in  th-st  can only be in the second position is an example of positional specificity. the probability of acceptance is much lower if such a token ending in  th-st  appears in the third position instead of the second position. a token ending in  th  or  st  can only follow a short word and tokens  st  rd  wy  blvd  can only follow a token ending in  th  or  st  are examples of sequential specificity. note that sequential specificity stresses the sequentiality-of a token following another-and is orthogonal to the positionality of the tokens. that the last state can only accept one of  st  rd  wy  blvd  is an example of token specificity.
　even though highly specific models may be required for some applications  attribute recognition models need only be specific to the extent of being able to identify an attribute value as belonging to the correct attribute and distinguish it from other domains. moreover  being overly specific in recognizing attribute values may cause the attribute recognition model to reject  i.e.  assign very low probability 
attribute values with errors  thereby resulting in incorrect segmentations. often  we can trade specificity off for achieving robustness to input errors. however  the challenge is to make the tradeoff without losing segmentation accuracy and at the same time being able to build the model efficiently.
　in this section  we instantiate an accurate and robust attribute recognition model. we first describe the topology of arms  and then describe the techniques for relaxing sequential and token specificity. finally  we describe a procedure for learning such a model from a large reference table.
1 arm topology
　the topology of a hidden markov model  consisting of the set of states and valid transitions between these states  has a big impact on the accuracy of the model. recently  techniques based on crossvalidation and stochastic optimization have been proposed to automatically decide good topologies  1  1 . however  these structure optimization techniques require several scans of the training data  and hence are slow when training data is large. moreover  these techniques-if trained on  clean  data-can result in positionally specific topologies  e.g.  figure 1 a    conditioned to accept tokens in specific positions  say  first or third or last. such topologies  even though accurate for segmenting clean input data  can be less robust towards erroneous input. in this section  we discuss a statically fixed topology that  i  enables efficient model building and  ii  relaxes positional specificity in favor of robustness to input errors.
　we observe that collapsing positional information into a small number of distinct categories results in a more flexible  compact  and robust arm topology. more specifically  we categorize tokens in attribute values into three positions: beginning  middle  and trailing positions  resulting in what we will call the bmt topology  shown in figure 1 b .1 in the example  1th nw 1th st  string corresponding to a street address  the token  1th  is categorized as beginning token   st  as the trailing token  and the rest as middle tokens.
　collapsing token positions into these three categories  we gain efficiency while building arms by avoiding a computationally expensive search for  optimal  topology. we also gain robustness to several common types of input errors-token deletions  token insertions  and token re-orderings. for example  the probability of observing a token 1th as the second or third token in  nw 1th 1th st  is the same for both occurrences of the token. observe that we still are specific about the positionality of the beginning and trailing tokens because these are critical for correctly recognizing boundaries between attribute values.1 and  by not grouping boundary tokens with the middle tokens  we are able to collect more specific statistics on the emission and transition probabilities for boundary tokens. our empirical evaluation shows that the bmt topology captures the salient structure required for robust segmentation  and performs as well as  and sometimes better than  other topologies involving multiple middle positions.
　the categorization of tokens into positions induces a categorization on the  generalized  dictionary of an attribute. the dictionary di corresponding to an attribute ai is now categorized into the beginning  middle  and trailing dictionaries dib  dim  and dit. for example  a token occurring in the beginning position of an attribute value of any tuple in r belongs to the dib dictionary.
set of states and possible transitions: the set of states in an arm model is also categorized into beginning  middle  and trailing states. each category consists of a state s for each element e  base token or feature class  in the corresponding categorized  generalized  dictionary  and s emits only e with non-zero probability. the union of all three categorized states along with the special start and end states constitutes the set of states in an arm. the broad structure of the set of allowed transitions is shown in figure 1 b . each category-

1
 the categorization can be generalized to more sophisticated topologies with multiple middle positions. however  our experiments  not included due to space constraints  showed that the simplest bmt topology is equally good.
1
 this intuition is corroborated by the human ability to recognize words even if all but boundary characters are randomly rearranged:  aoccdrnig to a rscheearch at an elingsh uinervtisy  it deosn't mttaer in waht oredr the ltteers in a wrod are  the olny iprmoetnt tihng is taht frist and lsat ltteer is at the rghit pclae. the rset can be a toatl mses and you can sitll raed it wouthit porbelm. tihs is bcuseae we do not raed ervey lteter by it slef but the wrod as a wlohe. 
beginning  middle  and trailing-may consist of several states in the hmm but the transitions among these state categories are restricted to non-backward transitions  as indicated by the arrows in figure 1 b . that is  beginning states can only transition to either middle or to trailing or to the end states  middle states to middle or to trailing or the end states  and trailing states only to the end state.
　observe that by assigning a state to each token or feature class  we encode transition probabilities more accurately than the usually adopted approach grouping all base tokens into one state. thus  we are better able to exploit large sets of examples in reference tables. for example  grouping base tokens  st  hwy  into one basetoken state also collapses all transitions from previous states  say   1th  hamilton  sr1   to any of these individual tokens into one transition. it is possible that the states  e.g.   sr1   transitioning into the token  hwy  are very different from the states  e.g.   1th  hamilton   transitioning into the token  st.  grouping several base tokens into one basetoken state loses the ability to distinguish among transitions. therefore  we associate one base token per state. the cost of associating a token per state is that the number of transitions increases. however  we show in section 1 that the resulting transition matrices are very sparse  and hence the comprehensive arm models easily fit in main memory.
emission and transition probabilities: to complete the instantiation of an attribute recognition model armi on attribute ai  we need to define the emission probabilities at each state and the transition probabilities between states. since we in include in armi a state s per element  base token or feature class  e in the categorized feature hierarchy  the emission probability distribution at s is: p x|e  = 1 if x = e and the position of x within armi and that of e within the attribute value are identical  and 1 otherwise. in section 1  we describe an algorithm for learning  during the preprocessing phase  the transition probability distribution from the reference table. in the next section  where we describe the relaxation of sequential specificity for robustness  we assume that these probabilities are known.
1 sequential specificity relaxation
　consider the example path in figure 1 a  consisting of a  short word   a  number ending in th or st   and a token in the set rd  wy  st  blvd  which accepts the string  nw 1th st  with a high probability. however  its erroneous versions  1th st    nw1th st    nw 1th    nw 1th 1th st  have a very low acceptance probability due to the sequential specificity of the model in figure 1 a : that a specific token has to follow another specific token. therefore  erroneous transitions  from the state accepting  1th  to the end state  from the start state to the state accepting  1th   have a very low probability.
　our approach for trading sequential specificity for robustness is to  adjust  attribute recognition models trained on clean data by creating appropriate states and transitions in an arm to deal with some of the commonly observed types of errors: token insertions  token deletions  and missing values . our adjustment operations exploit the uniform bmt topology and are illustrated in figure 1. in order to deal with erroneous token insertions  we copy states along with transitions to and from these states in the beginning and trailing positions to the middle position  as shown in figure 1 a  . we add low probability  according to the good-turing estimate   transitions to the states copied from the beginning position from all states in the beginning position. similar transitions to trailing states are added from the states copied from the trailing position. in order to deal with erroneous token deletions  we copy states and associated transitions from the middle position to the beginning and trailing positions  as shown in figure 1 b  . in order to deal with missing attribute values  we create a transition  as shown in figure 1 b   directly from the
	a 	a' 	b 

	begin 	middle 	trailing 	begin 	middle trailing 	begin 	middle trailing 
	 a   	 b  	 c  
figure 1: illustration of robustness operations: recover insertions  a   deletions  b   missing values  c 
start state to the end state. observe that the implementation of these operations on a non-uniform topology would be very complex. other common errors are addressed by other characteristics of arms. our begin-middle-trailing topology is designed to be robust to token reorderings  especially  those in the middle position. spelling errors are handled through token specificity relaxation as we describe next.
1 token specificity relaxation
　we now describe an approach for relaxing token specificity to account for unknowns or rare tokens. our approach is a departure from the smoothing approach used by borkar et al. : during training  we propagate base token statistics to all matching feature classes. at runtime  if the observed token t is in the dictionary it is mapped directly to the appropriate state in the arm; otherwise  the token is generalized to the minimal feature class that accepts t. in order to explicitly address spelling errors  we have experimented with matching unseen base tokens to states corresponding to known base tokens according to standard distance measures such as edit distance  e.g.   street  with  streat  . however  our experiments did not show any accuracy improvement over our original method.
1 arm training
　we now describe the training procedure for computing transition probabilities between states in the bmt topology. the importance of each transition in recognizing whether or not a string exhibiting this transition belongs to the attribute depends on two factors:  i  generative factor: how often is the transition observed in the current attribute   ii  discriminative factor: how unique is the transition to the current attribute 
　many traditional approaches for learning hmm transition probabilities rely only the generative factors. that is  the  generative  transition probability between two states s1 and s1 in armi is the probability of observing token pairs t1 and t1 that can be emitted from s1 and s1. the generative approach results in high probability transitions between higher level feature classes  e.g.  transitions between the w+ states that accept any token  even though such transitions may not discriminate an attribute from other attributes. for example  consider an erroneous bibliographic input string  editorial wilfred hodges of logic and computation  that has to be segmented into attributes  title  authors  journal 1. in our experiments  purely generative models segment this string as   editorial    wilfred hodges of logic    and computation   because the token  and  generalizes to a three-character string which is often observed as the beginning token for a journal name  e.g.   acm tods  .
　we ensure that the transition probabilities depend on both the generative and the discriminative factors. we now formalize this intuition to define the transition probabilities between pairs of states. in the following description  let s1 and s1 be two states in the attribute recognition model armi. let the position posi s  of a state s denote the position-beginning  middle  or trailing-of s in armi. the position pos t v  of a token t in an attribute value v is the position-beginning  middle  trailing-of t in the string v. given two states s1 and s1  we say that the transition t s1 s1  from s1 to

1
 the original record before corruption was   editorial    wilfred hodges    journal of logic and computation  .
procedure buildarm  table r  column c  f 
1 first pass: scan r  build dictionary c 
1a	prune dictionary f 
1 second pass: scan r  compute transition frequencies
1 generalize: propagate base transitions up the hierarchy
1 compute transition probabilities
1 apply robustness transformations
figure 1: arm training procedure
s1 is valid only if it is a non-backward transition. that is:
  if posi s1  = beginning then posi s1  （{middle trailing end}
  if posi s1  = middle then posi s1  （{middle trailing end}
  if posi s1  = trailing then posi s1  （{end}
given an attribute value v from the attribute ai and the states of armi  we say that v supports a valid transition t s1 s1  if there exists a pair of consecutive tokens t1 and t1 in v such that pos t1 v  = posi s1   pos t1 v  = posi s1   and either t1 and t1  i  are emitted with non-zero probability by s1 and s1  respectively or  ii  belong to the feature classes emitted by s1 and s1  respectively.
positive frequency: given a reference table r  the positive frequency fi+ t s1 s1   of a transition t s1 s1  with respect to attribute ai is the number of attribute values in the projection of r on ai that support t s1 s1   and is 1 for all non-feasible transitions.
overall frequency: given a reference table r  the overall frequency f t s1 s1   of a transition t s1 s1  is the number of attribute values from any attribute that support the transition t s1 s1 . that is  f t s1 s1   = i fi+ t s1 s1  .
generative transition probability: given a reference table r  the generative transition probability gp t s1 s1 |ai   of transition t s1 s1  with respect to an attribute ai is the ratio fi+ +t s1 s1   .
j fi  t s1 sj  
transition probability: given a reference table r  the transition probability p t s1 s1 |ai of a transition depends on its generative probability and its ability to distinguish attribute ai. assuming independence between the two aspects  we compute the transition probability as the product: . note that we do not re-normalize the transition probabilities. an em-style baum-welch  algorithm or other discriminative training methods  e.g.    can be used to further optimize the transition probabilities. in our implementation  we used the efficient two-pass procedure described next  figure 1 .
　the pseudocode for the training procedure is shown in figure 1. the overall procedure requires two passes: the first pass builds the dictionary and the second pass computes transition probabilities and applies robustness adjustments.
arms summary: the crucial aspects of arms are  i  the adoption of a fixed bmt topology that allows us to efficiently learn them from large reference tables and to gain robustness to input errors   ii  the association of a state per base token to accurately encode transition probabilities and exploit large dictionaries   iii  the relaxation of sequential specificity by adjusting an arm learned from clean reference data  and  iv  the computation of transition probabilities to distinguish target attributes on which arms are built.
1. segmentation algorithm
　in this section  we describe an efficient algorithm for segmenting input strings. the segmentation problem has two components: first 

figure 1: segmentation algorithm
determining the sequence in which attribute values are concatenated in an input string and second  determining the best segmentation of an input string into the corresponding ordered sequence of attribute values. previous supervised approaches learned the attribute value order from the training data. for example  borkar et al.  model the probabilistic attribute order using a hidden markov model. for instance  the author attribute immediately precedes the title attribute with probability 1 and the year attribute immediately precedes the booktitle attribute with probability 1. once such a probabilistic order is known  a dynamic programming algorithm based on the viterbi approximation can be employed to determine the best segmentation . therefore  in the rest of this section  we only discuss the solution for the first sub-problem of determining the attribute value order. for the second sub-problem  a dynamic programming algorithm can be employed.  however  our implementation uses an exhaustive search.  figure 1 illustrates our approach: first learn the total order of attribute values over a batch of input strings  and then segment each individual string using this  fixed  attribute order.
1 determining attribute value order
　we now describe an efficient algorithm for determining the attribute value order in input strings. our algorithm is based upon the following observation. attribute value orders of input strings usually remain same within batches of several input strings. for example  a data source for bibliographic strings may concatenate authors  title  conference name  year  pages in this order  preserving order across strings within the same page. therefore  we need to recognize and recover this order only once for the entire batch of input strings. we validated this assumption on real data sources on the web for the media  address  and citation domains: content creators do tend to preserve the order of attributes at least within the same page.
　we now formalize the above intuition. we first estimate the probability of attribute ai preceding  not necessarily immediately  attribute aj  and the use these estimates to determine the most likely total ordering among all attributes. we now describe these two steps of the process.
pairwise precedence estimates
intuitively  the precedence estimate prec ai aj  of an attribute ai preceding attribute aj is the fraction of input strings where the attribute value for ai is before the attribute value for aj. the precedence order among attributes for a single input string is determined as follows. for each attribute  we determine the token in the input string s at which it is most likely to start. for a pair of attributes ai and aj  if the token at which ai is most likely to start precedes the token at which aj is most likely to start  then we say that ai precedes aj with respect to the input string s. we break ties by picking one of the two attributes with equal probability. for example  consider an input string consisting of 1 tokens  walmart 1 s. randall ave madison 1 wi.  we compute an 1-coordinate vector  1  1  1 1  1 1  1 1  for the city attribute. the first component 1 in the vector denotes the probability of the city attribute starting at the token  walmart.  because the 1th coordinate is the maximum among all coordinates  the city attribute is most likely to start at the token  madison.  suppose the vector for the street attribute is  1  1 1 1  1 1 1 1 . the maximum for the city vector occurs at the 1th coordinate and that for the street occurs at the 1th coordinate. therefore  street attribute value precedes the city attribute value for this input string. the fraction of input strings in the entire batch where attribute ai precedes aj is an estimate for ai preceding aj.
　formally  let s be a given input string within a batch s of strings. we tokenize s into a sequence t1 ... tm of tokens and associate with each attribute ai  1 ＋ i ＋ n  a vector v s ai  =  vi1 ... vim . the component vij is an estimate of the attribute value for ai starting at the token tj; vij is the maximum probability with which armi accepts any prefix of  tij ... tim . let max v s ai   denote the coordinate corresponding to the maximum among values vi1 ... vim. that is  max v s ai   = argmaxj{vij}. the precedence estimate prec ai aj  is:

　at the end of this phase  we possess the pairwise precedence estimates between all pairs of attributes. computationally  this procedure requires invoking the arms for determining acceptance probabilities of sub-sequences of tokens from each input string in a batch. if the average number of tokens in an input string is m  this computation involves o m1  calls to arms. these acceptance probabilities can be cached and later used during the actual segmentation  thus avoiding repeated invocation of arms.
determining total attribute order
using the directed attribute precedence probabilities estimated as described above  we can now estimate the best total order among attributes. the quality of an attribute order is the product of precedence probabilities of consecutive pairs of attributes in the given order. when the number of target attributes is small  say  less than 1   we can exhaustively search all permutations for the best total order. when the number of attributes is large  we can use more efficient heuristic search techniques  e.g.   .
1. experimental evaluation
　in this section  we evaluate cram using a variety of real datasets from real operational databases to show that cram is a robust  accurate  and efficient unsupervised domain-independent segmentation system. we first describe our experimental setup  section 1 . we present results on accuracy in sections 1 and 1  and those on scalability in section 1.
1 experimental setup
we now describe the datasets and the evaluation metrics we adopted.
reference relations: we consider reference relations from three different domains: addresses  media  music album records   and bibliography domains.
  addresses: the address relation consists of 1 1 clean and
standardized individual and organization addresses from united states and puerto rico with the schema:  name  number1  number1  address  city  state  zip .
  media: the media reference relation consists of 1 clean and standardized records describing music tracks with the schema:  artistname  albumname  trackname .
  bibliography: the bibliography relation consists of 1 bibliography records from the dblp repository and has the following schema:  title  author  journal  volume  month  year .
errordescriptionspellinga randomly chosen token is corrupteddeletionsa randomly chosen token is deletedinsertionsinsert a random token from dictionaryreordersmove randomly chosen token to new positionmissingreplace randomly chosen attribute with nullall 1 errorsapply one or more of the above errors to a tuplenaturalnaturally erroneous data  manually entered by users table 1: error model: general attribute error types used for corrupting the test datasets.
test datasets: we evaluated cram over both naturally concatenated strings obtained from the web and from internal company sources. further  in order to allow controlled experiments  we generated extensive test sets by concatenating  error-injected  real records into strings. all test datasets are disjoint from training datasets.
  naturally concatenated test sets: company addresses obtained from the rise repository of information extraction sources  publicly available ; individual addresses and media filenames from internal company sources  not publicly available ; 1 most cited papers from citeseer  publicly available . we will refer to these collectively as natural datasets.
  controlled test data sets: both clean and erroneous strings are obtained by concatenating attribute values of records in a test relation held aside for validation. therefore  the training and test sets are disjoint. to automate evaluation  we fix the order in which attribute values are concatenated to be a randomly chosen permutation of all attributes. erroneous input strings are generated from test tuples by passing them through an error injection phase  as in   before they go into the concatenation phase described above. the error injection phase is a controlled injection of a variety of errors  which are commonly observed in real world dirty data   into the test tuples. the attribute value s  to introduce an error into is chosen randomly from among all attribute values. the types of errors and their effects on attribute values are listed in table 1. while generating input strings with errors  we introduce at least one error into every tuple. for the mixed error model  all 1 errors in table 1   we assume that each error type occurs with equal probability.
systems compared: we compare our system cram with a state of the art supervised text segmentation system datamold   which was designed for automatic text segmentation and was shown to outperform other competing systems such as rapier . thus  our comparison is definitive  since by outperforming datamold  cram is virtually guaranteed to outperform other systems as well.
1 evaluation metrics
　as discussed earlier in section 1  we separate the problem of determining attribute order from that of arriving at the best segmentation given the order. reflecting this separation  we also split the evaluation of the attribute order determination from that of segmentation accuracy given the order.
segmentation accuracy: we measure segmentation accuracy as a fraction of attributes segmented correctly. from an input string s  let n1  out of a maximum of n attributes  be the number of attributes correctly identified by a segmentation algorithm alg. we define the segmentation accuracy acc s alg  on the input string  or the fraction of target attributes correctly identified. for a set s of input strings  the overall segmentation accuracy of alg is defined as the average fraction of correctly identified attributes over all strings in s  or  more formally as:
	accuracy	 1 

figure 1: relative accuracy gain of cram over datamold.
attribute order: we compute the accuracy of our order determination as the fraction of attributes in the input batch of strings which were assigned the correct absolute position. for example  if the correct order is number  address  city and our algorithm proposes number  city  address  the order accuracy is 1 or 1%.
1 robustness of cram
　we now evaluate the accuracy of cram over real datasets. we show that  i  the cram system substantially outperforms the state of the art supervised text segmentation system   ii  the attribute order determination technique is accurate  and  iii  cram scales to large reference tables.
segmentation accuracy
we compare the segmentation accuracy of cram with that of datamold over both erroneous and clean test datasets discussed in section 1. for this experiment  we fix the order in which attribute values are concatenated to focus on the improvements due to using arms.1 we report the segmentation accuracy in figure 1  a  b  and c . observe that cram substantially improves accuracy  often by 1%  for all error types and over all datasets illustrating that arms are accurate in modelling attribute value domains. in order to put the accuracy improvements in a relative perspective  we report the same results in figure 1 but in the form of the percentage relative error reduction over datamold for the addresses  dblp  and media datasets. for many of the noisy datasets  cram reduces segmentation errors by over 1%. these improvements directly result in the significant reduction of required manual intervention and increased accuracy while loading the data into relations.
accuracy of attribute order determination
we now evaluate our attribute order determination technique over batches of natural  i.e.  user-entered or web-derived  strings. figure 1 shows the high accuracy of our total order determination algorithm: in many cases we can determine order with 1% for a relatively small number  around 1  of tuples. this is a reasonable requirement  since many personal media libraries  and address and citation collections  and legacy data sources will have that many tuples.
exploiting large reference tables
we now study the impact of reference table sizes on segmentation accuracy. figure 1 shows the results. observe the increase in segmentation accuracy with the size of the reference table  especially for the media dataset. in fact  note that accuracy of over 1% for the media dataset is only achieved when the reference table size ex-

1
 because of privacy restrictions  we were not able to ship  real  individual addresses data to be evaluated by datamold. all of the remaining results are performed over the natural dataset  unless otherwise indicated. when this was not possible for privacy concerns  our all 1 errors dataset provides a good approximation of the segmentation accuracy of the real data entered by real users.

figure 1: accuracy of attribute order determination for the webderived addresses and media  and the corrupted dblp datasets.

figure 1: accuracy of segmentation usingcram for increasing reference table size  addresses  dblp  and media datasets .
ceeds 1 tuples. therefore   i  exploiting rich dictionaries from large reference tables is important to achieve higher segmentation accuracy  and  ii  a segmentation system must scale to large reference table sizes. cram takes just a few minutes    1  to learn arms over a reference table of 1 tuples. in contrast  supervised systems relying on cross-validation approaches would be much slower.
1 effectiveness of robustness techniques
　in this section we evaluate the impact on segmentation accuracy of the bmt topology  the association of a state for each base token  and the robustness operations.
fixed arm topology and robustness operations
we now evaluate the effectiveness of our bmt topology-which alleviates the need for expensive cross-validation approaches for optimizing hmm topology-to show that it results in high segmentation accuracy  and is often better than topologies obtained by using expensive cross-validation approaches. we compare the accuracy of segmentation using two representative arm topologies:  i  1-pos topology only models the sequential information between states by even collapsing all begin  middle  and trailing positions into one category and  ii  our bmt  the 1-pos  topology. in fact  we found that increasing the number of positions in the fixed topology further does not increase accuracy. due to space constraints  we omit the results. figure 1 shows that the fixed bmt topology is usually more accurate than the 1-pos topology that completely ignores positional specificity.
　figure 1 also reports results of sequential specificity relaxation operations over arms. these include the adjustment operations  e.g.  recovering from token deletions  and the transition probability computation described in section 1. as we can see  our robustness operations do provide a consistent improvement in accuracy over both erroneous and clean data on all datasets. the improvements on clean test datasets are the result of modified transition probability computation.

figure 1: accuracy of cram over addresses  a   dblp  b   and media  c  datasets with different arm topologies  1-pos and bmt  and with robustness operations  bmt-robust .

figure 1: complete vs. collapsed token states over addresses  a   dblp  d   and media  m  datasets.
modelling individual base tokens
as discussed in section 1  we associate a state with each base token that is retained in the arm. a common alternative is to collapse many base tokens together into one state of the hmm  which results in the loss of transitional information by collapsing transitions together. figure 1 shows that collapsing base tokens together into one state  as is done in datamold   results in substantially lower  sometimes by 1%  segmentation accuracy. the price of this accuracy gain is a larger model size. however  we show in section 1 that cram achieves high accuracy even if we retain only important base tokens. in related experiments  which we omit due to space constraints  we studied the use of different hierarchies. however  they all resulted in similar segmentation accuracies.
hypothetical supervised approach
we now demonstrate that making robustness to input errors a design criterion is essential. we consider a hypothetical scenario where we know the error model with the percentages of each error type expected in the test input.  of course  this knowledge is not available in practice.  we use this knowledge to prepare a training dataset that reflects the variety of errors and percentages. as shown in figure 1  datamold:hypothetical  corresponding to datamold trained over this hypothetical dataset  has significantly higher segmentation accuracy than datamold trained over clean data  but is still outperformed by cram trained on clean reference data. thus  robustness to input errors is essential for deploying a segmentation system on real data.

figure 1: varying fraction of tokens retained: segmentation accuracy  a   #transitions  b   and total model size  mb   c .
1 scalability
　we now study the sizes of models with respect to reference table sizes and the impact of constraining model sizes on the segmentation accuracy. we validate our hypothesis that  i  the number of transitions grows much slower than a function that is quadratic in the number of states  and  ii  that we can achieve comparable accuracy by considering only a fraction of the most frequent base tokens.
　figure 1 shows results of varying the fraction f of base tokens retained while keeping the reference table size fixed. as shown in figure 1 a   retaining only a fraction of the base tokens in arms gets us the same accuracy as that of retaining all base tokens. at f = 1  all tokens are retained and the model size  shown in fig-

figure 1: varying reference table sizes: segmentation accuracy  a   total model size  in mb  on the primary axis  and #model states on the secondary axis  b .
ure 1 c   is still less than a few mb for all datasets. and  the number of transitions  shown in figure 1 b   is around 1 times that of the number of states-usually 1-in the model; hence  it is orders of magnitude less than |s|1. thus  we can significantly reduce memory requirements without compromising on segmentation accuracy.1
　we now study the impact on accuracy of increasing reference table sizes while constraining the size of the cram model to be under 1mb  using the media dataset . figure 1 a  shows the improvement in accuracy as the reference table size increases from 1 to 1  1 tuples. figure 1 b  shows the corresponding model size  which is constrained to below 1mb  and the number of states. note that even under constrained model size  the resulting segmentation accuracy of 1% for 1 tuples matches that of the larger unconstrained model trained over the same reference table. thus  cram scales to large reference tables while maintaining a small memory footprint without compromising on segmentation accuracy.
　in summary  we have shown cram to be robust  scalable  and domain independent. it is accurate on both clean and erroneous test data  and gracefully scales up to large reference table sizes.
1. conclusions
　in this paper  we exploit widely available reference tables to develop a domain-independent  robust  scalable  and efficient system for segmenting input strings into structured records. we exploit rich dictionaries and latent attribute value structure in reference tables to accurately and robustly model attribute value domains. to easily deploy cram over a variety of data sources  we address the problem of automatically determining the order in which attribute values are concatenated in input strings. using real datasets from several domains  we established the overall accuracy and robustness of our system vis-a-vis state of the art supervised systems.
acknowledgments: we thank sunita sarawagi for helping us compare our system with datamold. we also thank theo vassilakis for several thoughtful comments on the paper.
