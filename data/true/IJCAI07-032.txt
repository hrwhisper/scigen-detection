multiagent distributed resource allocation requires that agents act on limited  localized information with minimum communication overhead in order to optimize the distribution of available resources. when requirements and constraints are dynamic  learning agents may be needed to allow for adaptation. one way of accomplishing learning is to observe past outcomes  using such information to improve future decisions. when limits in agents' memory or observation capabilities are assumed  one must decide on how large should the observation window be. we investigate how this decision influences both agents' and system's performance in the context of a special class of distributed resource allocation problems  namely dispersion games. we show by numerical experiments over a specific dispersion game  the minority game  that in such scenario an agent's performance is non-monotonically correlated with her memory size when all other agents are kept unchanged. we then provide an information-theoretic explanation for the observed behaviors  showing that a downward causation effect takes place.
1 introduction
an important class of natural problems involving distributed resource allocation mechanisms may be modeled by dispersion games . problems in this class are of anti-coordination nature: agents prefer actions that are not taken by other agents. stock exchange and load balancing in networks are instances of such problems . in iterated versions of such games learning is an important component of the decisionmaking process. while each agent may try and learn individual behaviors of all agents  this is a complex task especially when the total number of agents in the system is unknown  not constant or very large. a possible solution is to consider other agents' actions as indistinguishable from the environment - an idea behind the so-called 1-level agents  which  for some scenarios  yields the best results . in such cases  only the joint actions of all agents are considered and they may be represented as a single global outcome. the computational cost of analyzing one single global outcome is expected to be much smaller than that of analyzing multiple outcomes produced by individual agents. more importantly  such a cost does not depend on the number of agents  and its computation may be feasible even without knowledge of such number.
　in repeated dispersion games  agents may learn from a series of past outcomes in order to improve their decisions. a natural issue is how large should the observation window be. while bounds in agents' memory or observation capabilities are often assumed   there is a paucity of explanation about how such limits are established. in this paper we tackle this problem in a specialized version of a dispersion game known as the minority game  mg  . in the mg  a finite number of agents decide over a  also finite  set of actions and those who choose the action chosen by the smallest number of agents are rewarded. when the game is repeated  agents have the chance to learn from previous rounds and improve their performance by considering a finite window of past outcomes  they have finite memory  in order to try and learn temporal patterns.
　our contribution is twofold. first  we systematically simulate cases in the mg composed of agents with different memory sizes and compare their performances to a fixed class of  other  agents with fixed memory sizes  using two different learning algorithms. the results show a clear non-monotonic relation between access to information and agent's performance  unveiling cases where more information translates into higher gains and cases where it is harmful. further  we relate those findings to the system's global efficiency state. second  we provide an information-theoretic explanation to the observed behaviors. we show that the size of the generated outcome patterns does not always match the system's memory size  thus allowing agents with larger memory sizes to exploit this information.
　the remainder of the paper is organized as follow. we start by discussing relevant related work. we then present the mg model and terminology. next  we detail our methodology and the results of the experimental studies using two different learning algorithms. finally  we analyze the results and point out directions for future research.
1 related work
while previous works have focused on problems relating homogeneous memory size to global system performance  little effort has been put into understanding the relation between individual memory size and individual agent's performance.
homogeneous agents involved in self-play are typically assumed  with emphasis in system dynamics rather than individual agents. however  as argued in   a more appropriate agenda for multi-agent learning  asks what the best learning strategy is for a fixed class of the other agents in the game   as this allows the construction of effective agents. we follow this methodology here. in the context of the mg  little work so far has been directed to and concerned with individual agent's performance. an exception to that is   in which a genetic algorithmic-based agent is shown to outperform traditional ones  but little investigation about the conditions on why this happens is done. other learning algorithms have been applied to the mg  1  1  1 ; however  homogeneous agents are assumed with focus on global properties.
　heterogeneous memory sizes are studied in   where an evolutionary setup is used to search for the best memory size for the game; however  they do not fix a class of agents  allowing every agent to evolve independently. a similar setup is used in   but their concern is average memory size. we shall provide alternative explanation for some of their results. in  it is stated that agents with larger memories perform better than agents with smaller ones. nonetheless  they consider a varying number of agents endowed with only one extra bit of memory; we will show that having larger memories may not lead to better performance.
1 a multiagent market scenario
the minority game  mg  was initially proposed in the context of econophysics  as a formalization of the el farol bar problem . in these models  the main interest is the type of information structure created by multiple interacting bounded rational agents. in the mg  there are two groups and an odd number  n  of agents. at each round  agents must choose concurrently and without communication to be in only one of the two groups. at each round t  after all decisions are made  a global outcome is calculated: a t  =   where ai t  is the decision of agent i in round t and ai t  （ {+1  1}. agents' wealth is distributed according to wi t  = wi t   1  + ai t sign a t   1    where sign ，  is +1 or  1 according to a t   1  being positive or negative  other functions of a t  may be used . initial values are set to zero. a t  encodes which group had the minority of agents and only those in this group are rewarded  while those in the majority are penalized. there is no equilibrium point in which all agents profit  and the expectation of the majority is always frustrated due to the anti-coordination nature of the game: if many believe a particular group will be the minority one  then they will actually end up in the majority. in spite of its simplicity  the mg offers a rich and complex behavior that was shown to represent several scenarios of decentralized resource allocation  such as financial markets and traffic commuters scenarios . in   this model was generalized and shown to belong to a broader class of dispersion games.
　one of the main concerns in the mg is with the game's global efficiency  i.e. the number of agents winning the game. from the viewpoint of the system as a whole  one is interested in maximizing the number of winners  so that resources are better distributed. when the mg was originally introduced  agents were modeled using a simple inductive learning algorithm to make decisions  corresponding to a simplified model of how human decisions in complex environments are actually made . since this algorithm has been widely used and studied in other applications  it is of our interest to make use of it in a straightforward way  we refer to it as the traditional learning algorithm . in this algorithm  each agent has access to the m last outcomes of the game and she is given a set s of hypotheses  randomly selected from a fixed hypotheses space h. a hypothesis maps all possible patterns of past outcomes over m to a decision  i.e. what group to choose in the next round . m is known as the memory size of the game. a hypothesis is effectively represented by a binary string of length 1m and |h| = 1m. the learning algorithm keeps a measure fi j t  of the effectiveness of each hypothesis j held by agent i in time t  which is updated by fi j t  = fi j t   1  + aisign a t   1  . this measure is often called virtual points. an agent then uses her hypothesis with the maximum number of virtual points argmaxjfi j t  to commit to a decision in the next round of the game. ties are broken by coin toss. this algorithm is very simple and provides more adaptation than learning  since agents are unable to better explore strategies outside their initial set. in order to overcome the limitations of this algorithm and to allow for comparisons with a different learning strategy  we shall introduce a new evolutionary-based learning algorithm.
1 methodology and experiments
due to the nature of our interest in the model  we consider agents with different hypotheses space sizes  i.e. different memory sizes   in order to observe the effects of such differences in agents' performance1. we split the mg into two parts. the environment consists of a traditional mg with a set n of agents  as described above  with homogeneous agents . the control group g is a group of agents which sample hypotheses from a different space from that of the agents in the environment  differing only in its size |h|. since hypotheses space size in our setup is defined only by each agent's memory size  we shall denote the environment's by me and the control group's by mg.
　in this work we assume |g| = 1.  investigates larger groups in similar fashion; however in their experiments the memory size is not systematically modified and their interest is on the effects of changes in group sizes. we will observe the performance for the resulting control agent when her hypotheses space size mg varies in relation to me. we also want to verify how her performance  defined by the average wealth wg  changes with mg and compare it to the environment's performance  defined by the average wealth we of a random agent in the environment . in order to do so  we measure the control agent's gain: wg/we. all results are averages over 1 randomly initialized games and they have been statistically tested for significance using a standard t-test with 1% confidence interval. each game was run for 1 rounds and we have used the standard values found in the literature for all other parameters  namely: |n| + |g| = 1

figure 1: control agent gains for combinations of me and mg. lighter shades denote higher gains.
and |s| = 1  so comparisons could be easily made 1. we shall detail experiments using the traditional and the evolutionary learning algorithms.
1 traditional learning algorithm
we start with the traditional learning algorithm in both the control agent and agents in the environment. in fig. 1 we show the control agent's gain for every combination of me （  1  and mg （  1 . these ranges may seem somewhat limited  but are adequate for our purposes. the exponential increase of hypotheses space with m and the large number of agents often makes experimentation with larger memories intractable. in the resulting topography  we observe some interesting behaviors. for me ＋ 1  higher than unit gains are obtained whenever mg   me. on the other hand  for values immediately above me = 1 we can observe that whenever mg   me  wg falls below unit  showing that the target agent does worse by having a larger hypotheses space. for larger me  wg is always below unit except where mg = me  when the gain is  as expected  exactly unitary.
　let us now observe in detail what happens in specific points of the observed regions by sectioning the topography  see fig. 1 . for the first case  we take me = 1. we can see that our control agent benefits from a larger hypotheses space. interestingly  having smaller spaces seem to cause no harm and the agent performs as if mg = me. we can also observe that the transition from one of the above cases to another is quite sharp and further increases in mg provide no additional gains. instead  a logarithmic decrease of wealth may be observed with further increases in mg. the highest gain occurs precisely where mg = me + 1  where a spike may be observed. as for the second case  we detail the behavior for me = 1. we observe a logarithmic drop in wg for mg   me. for mg   me  a small decrease of gains can be observed. thus  in this region no mg does better than unit and  interestingly  having access to a larger input window is harmful to the agent. for larger values of me  losses become larger whenever mg   me. having access to a larger information win-

figure 1: control agent gain for me = 1  solid line  and me = 1  dashed line  with agents using the traditional learning algorithm

figure 1: control agent gain for varying me and mg = me+1
dow is harmful to the agent  except when the environment has small memory sizes. to observe such phenomena  we plot the control agent's gain when me varies and we set mg = me+1. this is shown in fig. 1  where it is made clear that there is a non-monotonic relation between memory size and agent's performance. we observe that gains are high for small values of me  then they become smaller than unit  reaching a minimum and later increasing again to finally converge to a value slightly below unit. having worse performance when accessing more information may seem counter-intuitive. one could argue that this is due to the larger hypotheses space  which makes finding a good hypothesis harder. even though this may be part of the explanation  it does not account for all of it since we observe a non-linear relationship between gains and hypotheses space size  as we shall further investigate.
1 evolutionary learning
it could be argued that the observed behavior is only but a peculiar effect of the learning algorithm used  whose main limitation is the inability to explore the hypotheses space during the game  i.e. an agent may only use the hypotheses given at the start of a run. in order to address this concern  we have repeated the experiments using an evolutionary-based learning algorithm so as to allow agents to further explore the hypotheses space. some different evolutionary-based algorithms for the mg have been proposed  1  1  1 . we chose an adaptation of the one proposed in   due to its simplicity and good
algorithm evol-learning w ○ 1;
s ○ random hypotheses （h; foreach s（s do
fitness s  ○ 1;
end
while not end of the game do obs ○ window of size m of past outcomes; h ○ arg maxs fitness s ;
decision ○ decision of h using observation obs; commit decision ;
if decision = outcome then w ○ w + 1; foreach s（s do if decision of s using obs = outcome then fitness s  ○ fitness s  + 1;
else fitness s  ○ fitness s  - 1;
end
end
with probability pr do worst hypothesis s○ best hypothesis k; with probability pm  flip bits in s;
end
   end endfigure 1: evolutionary learning algorithm
performance. while we could have applied a more elaborated algorithm used in similar games  such as   we have chosen to create an algorithm that is closely related to the traditional one  but presenting better learning characteristics. by doing so we are able to better understand and compare results obtained using both algorithms.
　in this algorithm  depicted in fig. 1   each agent starts with s hypotheses and  at every round  with probability pr  she discards her worst performing hypothesis and replaces it with a copy of her best performing one. each bit of this copy is then flipped with probability pm. this allows agents to search for better hypotheses  continually introducing new ones to the game. it is interesting to note that  differently from other evolutionary learning algorithms such as   this one does not require a global ordering of agents based on their performances. agents retain their autonomous characteristics by not relying on a central authority to decide which hypotheses among all agents are to be replaced  thus preserving the distributed nature of the game. figure 1 shows a t  for a typical run when agents are using the new proposed learning algorithm and pr = 1 and pm = 1. clearly  learning is taking place  when using the traditional algorithm  no decrease in oscillations is observed  even for very long runs . for all experiments using this algorithm we consider only results after 1 rounds  in order to observe the  steady  state of the system. figure 1 shows the control agent's gain for every combination of me （  1  and mg （  1  when agents use the evolutionary-based learning algorithm. we now observe a different behavior when compared to our previous case. there are no regions where larger memories are beneficial  each extra bit beyond the environment's memory size is harmful to the target agent. a logarithmic decrease with mg is observed for mg   me  for all tested values of me. in fig. 1 we detail the behavior for some values of me.

figure 1: a t  for agents using evolutionary-based algorithm

figure 1: control agent gains for combinations of me and mg. lighter shades denote higher gains.
as with the traditional learning algorithm  for mg   me no considerable losses or gains are observed.
1 analyzing the results
1 dynamics of the game and its efficiency regions
as stated above  one of the main concerns with the mg is its global efficiency  i.e. how many agents are winning the game during a run. a typical way of measuring the temporal efficiency of resource distribution is by means of the statistical variance σ1 of a t  . the larger the variance is  the larger the waste of resources  making the system less efficient. the variance σ1 is a function only of the number of agents in the game and their  homogeneous  memory size . since we keep the number of agents fixed and the control group is unitary  we consider σ1 only as a function of me  the memory size of agents within the environment. figure 1 shows σ1 as a function of me when all agents use the traditional learning algorithm. the same plot is observed for any mg. three regions of efficiency can be observed. for small me  high variance is observed  thus low efficiency characterize the system. for large me  the system has precisely the variance expected if all agents were deciding randomly  i.e. the random case game - rcg . intermediate values of me are correlated with smaller  better than random  variances. this last case is often the main subject of interest in the mg  since it indicates that agents are able to self-organize to improve efficiency.

figure 1: target agent's gain versus mg for me = 1  solid line  and me = 1  dashed line   using the evolutionary learning algorithm
　comparing fig. 1 with the plotted gain using the traditional algorithm  fig. 1   we observe that higher than unit gains are associated with regions with high variances  gains below unit are associated with regions with small variance and increases in gains follow increases in variance. this indicates a correlation between system's global efficiency and exploitation possibilities of individual agents  where larger memories are beneficial when the system is behaving inefficiently  worse than random  whereas when it is efficient having larger memories becomes harmful. the same behavior is observed in the evolutionary setup. we can also observe in fig. 1 that the variance curve is quite different from the one using the traditional algorithm - there is a smooth transition between a very low variance and the expected for the rcg. since there is no inefficient region we would expect  following analogous reasoning  that no agent with mg   me would perform better than the agents in the environment and this was actually observed  see fig. 1 . such results are interesting  since they relate the efficiency of the system as a whole with individual exploitation possibility  by means of larger memories  and indicate the existence of stability points in the system's efficiency regions. for instance  if we take an evolutionary version of the game where all agents start with a small memory and are allowed to increase or decrease their memory size during a run  we would expect that there would be an initial incentive towards larger memories  leading the system towards higher efficiency points. however  such incentive would stop when the system reaches an efficient point as larger memories then become harmful. thus  this efficient point would be a stability point. such experiment was conducted in  where initial memory growth was observed  halting at the predicted memory size. such behavior was attributed to the simple nature of the game. here we propose a more detailed  distinctive explanation relating individual agent gain to the efficiency region of the system. this is a case of downward causation  where game dynamics are initially fixed by agents but such dynamics end up fixing possible  or profitable  agents' behaviors.
1 mutual information
in  homogeneous memory sizes and the traditional learning algorithm are used to show that there is different information available to the agents for different memory sizes. they

figure 1: σ1 as a function of me for the traditional  solid line  and the evolutionary algorithm  dashed line 
have measured this information by the conditional probability p 1|μk  - the probability of having a  1  following a binary sequence μk of length k. they have shown that the inefficient region is actually informationally efficient  in the sense that p 1|μk  is exactly 1 for all μk whenever all agents have memory of size k. on the other hand  in the efficient region there is an asymmetry in the probability distribution and there are μk that offer predictive power. the information analysis in  focused in cases where k = m  i.e. the measurement of information available to agents with the same memory size as their peers. we wish to access the information available within  since it represents our target agent with different memory sizes. in order to do so  we have to define a more precise information measure. to measure the asymmetry in the distribution of the predictive power of each hypothesis  we use the concept of mutual information  between a string μk and its outcome  computing the average information contained in each string: i k  =
 
where p1 and p1 are the probabilities of a  1  or a  1  occurring  respectively; p1 μk  and p1 μk  are the probabilities of  1  or a  1  immediately occurring after a string μk （ h  respectively; pu μk  = p1 μk +p1 μk . in the above equation  i k  is zero whenever the probabilities of observing  1  or  1  are the same for all μk. the highest the value of i k   the highest the average asymmetry of probabilities  indicating the existence of hypotheses with predictive power. we have measured the information by executing runs of 1 rounds and recording each outcome  resulting in a binary string of 1 symbols. mutual information is then measured over this string and averaged over 1 independent runs.
　we are now in position to measure the information available for some hypotheses of different lengths when agents are using the traditional learning algorithm. for each value of me  we let k assume values below  equal to  and above me. figure 1 shows results for me = 1 and me = 1  the same values detailed in the previous sections . we observe that i k  for me = 1 remains close to zero for k ＋ me. information available to memory sizes below me is almost the same as for me  which explains why agents with smaller memories do not present lower gains in fig. 1. for k = me + 1 we observe a substantial increase in the available information and further

figure 1: i k  versus k for me = 1  solid line  and me = 1
 dashed line   using the traditional learning algorithm
increases in k lead to a logarithmic decrease in i k . this  again  is in accord with the observed behavior for the target agent's gains and is evidence that agent's interactions are creating patterns that are of greater length than their memory sizes. for me = 1  the mutual information plot also closely follows the target agent's gain plot  fig. 1   where the highest information is present precisely at k = me. in both cases the highest value of i k  is followed by a logarithmic fall with further increases in k. this fall is expected: take kp as the value of k associated with the highest value of i k ; if we increase the hypotheses such that k   kp  the informative hypotheses previously at kp becomes more spread out through the hypotheses space. for example  if kp = 1  we could have a hypothesis that detects the pattern  1  but  for k = 1  this pattern is found in two hypotheses -  1  and  1  - but only one of them may be actually happening. we conclude that mutual information provides a good explanation for the target agent's gains with different memory sizes. it is worth observing that for some cases using the traditional learning algorithm  patterns of lengths different from the agent's memory size are created - as can be inferred from the target agent's exploitation possibility when having mg   me.
1 conclusions
the number of game rounds considered by each agent is a central issue in several dispersion games  such as the mg. by means of a methodology including extensive simulations  we have analyzed emerging patterns resulting from the agents' interaction  relating them to the possibility of exploitation of certain setups. we have shown that having access to more information is not always an advantage and that it can actually be harmful. experiments with the traditional learning algorithm have shown that there is a region  related to smaller memory sizes  where an agent with larger memory could exploit the system and obtain higher gains  whereas the same agent could have its gain reduced at another region. we have also measured the mutual information associated to different strings at the outcome history. we have related this information to the observed behavior  showing that agents in the system often generate patterns that are not of the same size of their memory sizes  but which can be exploited by an agent with larger memory. on the other hand  larger strings make information sparser in the space of possible patterns  resulting in decreasing agent performance when we increase her memory above an optimal size. the results presented here lead to a better understanding of the emergent patterns created from multiagent learning interactions using an informationtheoretic analysis. in particular  we have provided arguments to help choosing the best memory size when designing an agent or strategy to play the mg. while it is well known that more memory is not necessarily better for collective performance  we showed that this is also true for individual agent performance. this result contributes for the construction of better algorithms  that may take into account the system's efficiency region when deciding between different hypotheses space to consider.
　acknowledgments: this work has been partly supported by cnpq and capes.
