relevance feedback is a powerful technique to enhance contentbased image retrieval  cbir  performance. it solicits the user's relevance judgments on the retrieved images returned by the cbir systems. the user's labeling is then used to learn a classifier to distinguish between relevant and irrelevant images. however  the top returned images may not be the most informative ones. the challenge is thus to determine which unlabeled images would be the most informative  i.e.  improve the classifier the most  if they were labeled and used as training samples. in this paper  we propose a novel active learning algorithm  called laplacian optimal design  lod   for relevance feedback image retrieval. our algorithm is based on a regression model which minimizes the least square error on the measured  or  labeled  images and simultaneously preserves the local geometrical structure of the image space. specifically  we assume that if two images are sufficiently close to each other  then their measurements  or  labels  are close as well. by constructing a nearest neighbor graph  the geometrical structure of the image space can be described by the graph laplacian. we discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images  which gives us the most amount of information. experimental results on corel database suggest that the proposed approach achieves higher precision in relevance feedback image retrieval.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-relevance feedback; g.1  mathematics of computing : probability and statistics-experimental design
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
general terms
algorithms  performance  theory
keywords
image retrieval  active learning  relevance feedback
1. introduction
¡¡in many machine learning and information retrieval tasks  there is no shortage of unlabeled data but labels are expensive. the challenge is thus to determine which unlabeled samples would be the most informative  i.e.  improve the classifier the most  if they were labeled and used as training samples. this problem is typically called active learning . here the task is to minimize an overall cost  which depends both on the classifier accuracy and the cost of data collection. many real world applications can be casted into active learning framework. particularly  we consider the problem of relevance feedback driven content-based image retrieval  cbir  .
¡¡content-based image retrieval has attracted substantial interests in the last decade . it is motivated by the fast growth of digital image databases which  in turn  require efficient search schemes. rather than describe an image using text  in these systems an image query is described using one or more example images. the low level visual features  color  texture  shape  etc.  are automatically extracted to represent the images. however  the low level features may not accurately characterize the high level semantic concepts. to narrow down the semantic gap  relevance feedback is introduced into cbir .
¡¡in many of the current relevance feedback driven cbir systems  the user is required to provide his/her relevance judgments on the top images returned by the system. the labeled images are then used to train a classifier to separate images that match the query concept from those that do not. however  in general the top returned images may not be the most informative ones. in the worst case  all the top images labeled by the user may be positive and thus the standard classification techniques can not be applied due to the lack of negative examples. unlike the standard classification problems where the labeled samples are pregiven  in relevance feedback image retrieval the system can actively select the images to label. thus active learning can be naturally introduced into image retrieval.
¡¡despite many existing active learning techniques  support vector machine  svm  active learning  and regression based active learning  have received the most interests. based on the observation that the closer to the svm boundary an image is  the less reliable its classification is  svm active learning selects those unlabeled images closest to the boundary to solicit user feedback so as to achieve maximal refinement on the hyperplane between the two classes. the major disadvantage of svm active learning is that the estimated boundary may not be accurate enough. moreover  it may not be applied at the beginning of the retrieval when there is no labeled images. some other svm based active learning algorithms can be found in   .
¡¡in statistics  the problem of selecting samples to label is typically referred to as experimental design. the sample x is referred to as experiment  and its label y is referred to as measurement. the study of optimal experimental design  oed   is concerned with the design of experiments that are expected to minimize variances of a parameterized model. the intent of optimal experimental design is usually to maximize confidence in a given model  minimize parameter variances for system identification  or minimize the model's output variance. classical experimental design approaches include a-optimal design  d-optimal design  and e-optimal design. all of these approaches are based on a least squares regression model. comparing to svm based active learning algorithms  experimental design approaches are much more efficient in computation. however  this kind of approaches takes only measured  or  labeled  data into account in their objective function  while the unmeasured  or  unlabeled  data is ignored.
¡¡benefit from recent progresses on optimal experimental design and semi-supervised learning  in this paper we propose a novel active learning algorithm for image retrieval  called laplacian optimal design  lod . unlike traditional experimental design methods whose loss functions are only defined on the measured points  the loss function of our proposed lod algorithm is defined on both measured and unmeasured points. specifically  we introduce a locality preserving regularizer into the standard least-square-error based loss function. the new loss function aims to find a classifier which is locally as smooth as possible. in other words  if two points are sufficiently close to each other in the input space  then they are expected to share the same label. once the loss function is defined  we can select the most informative data points which are presented to the user for labeling. it would be important to note that the most informative images may not be the top returned images.
¡¡the rest of the paper is organized as follows. in section 1  we provide a brief description of the related work. our proposed laplacian optimal design algorithm is introduced in section 1. in section 1  we compare our algorithm with the state-or-the-art algorithms and present the experimental results on image retrieval. finally  we provide some concluding remarks and suggestions for future work in section 1.
1. related work
¡¡since our proposed algorithm is based on regression framework. the most related work is optimal experimental design   including a-optimal design  d-optimal design  and eoptimal design. in this section  we give a brief description of these approaches.
1 the active learning problem
¡¡the generic problem of active learning is the following. given a set of points a = {x1 x1 ¡¤¡¤¡¤  xm} in rd  find a subset b = {z1 z1 ¡¤¡¤¡¤  zk}   a which contains the most informative points. in other words  the points zi i = 1 ¡¤¡¤¡¤  k  can improve the classifier the most if they are labeled and used as training points.
1 optimal experimental design
we consider a linear regression model
		 1 
where y is the observation  x is the independent variable  w is the weight vector and  is an unknown error with zero mean. different observations have errors that are independent  but with equal variances ¦Ò1. we define f x  = wtx to be the learner's output given input x and the weight vector w. suppose we have a set of labeled sample points  z1 y1  ¡¤¡¤¡¤   zk yk   where yi is the label of zi. thus  the maximum likelihood estimate for the weight vector  w   is that which minimizes the sum squared error
		 1 
the estimate w  gives us an estimate of the output at a novel input:  y = w  tx.
¡¡by gauss-markov theorem  we know that w    w has a zero mean and a covariance matrix given by ¦Ò1hsse 1  where hsse is the hessian of jsse w 

where z =  z1 z1 ¡¤¡¤¡¤  zk .
¡¡the three most common scalar measures of the size of the parameter covariance matrix in optimal experimental design are:
  d-optimal design: determinant of hsse.
  a-optimal design: trace of hsse.
  e-optimal design: maximum eigenvalue of hsse.
since the computation of the determinant and eigenvalues of a matrix is much more expensive than the computation of matrix trace  a-optimal design is more efficient than the other two. some recent work on experimental design can be found in   .
1. laplacian optimal design
¡¡since the covariance matrix hsse used in traditional approaches is only dependent on the measured samples  i.e. zi's  these approaches fail to evaluate the expected errors on the unmeasured samples. in this section  we introduce a novel active learning algorithm called laplacian optimal design  lod  which makes efficient use of both measured  labeled  and unmeasured  unlabeled  samples.
1 the objective function
¡¡in many machine learning problems  it is natural to assume that if two points xi  xj are sufficiently close to each other  then their measurements  f xi   f xj   are close as well. let s be a similarity matrix. thus  a new loss function which respects the geometrical structure of the data space can be defined as follows:

where yi is the measurement  or  label  of zi. note that  the loss function  1  is essentially the same as the one used in laplacian regularized regression  lrr   . however  lrr is a passive learning algorithm where the training data is given. in this paper  we are focused on how to select the most informative data for training. the loss function with our choice of symmetric weights sij  sij = sji  incurs a heavy penalty if neighboring points xi and xj are mapped far apart. therefore  minimizing j1 w  is an attempt to ensure that if xi and xj are close then f xi  and f xj  are close as well. there are many choices of the similarity matrix s. a simple definition is as follows:
¡¡¡¡1 	if xi is among the p nearest neighbors of xj  sij =or xj is among the p nearest neighbors of xi; 1 	otherwise.   1 
let d be a diagonal matrix   and l = d s.
the matrix l is called graph laplacian in spectral graph theory . let y =  y1 ¡¤¡¤¡¤  yk t and x =  x1 ¡¤¡¤¡¤  xm . following some simple algebraic steps  we see that:

w
=	yty   1wtzy + wtzztw w
	=	yw
the hessian of j1 w  can be computed as follows:

in some cases  the matrix zzt +¦Ëxlxt is singular  e.g. if m   d . thus  there is no stable solution to the optimization problem eq.  1 . a common way to deal with this ill-posed problem is to introduce a tikhonov regularizer into our loss function:
 1 
the hessian of the new loss function is given by:

where i is an identity matrix and ¦« = ¦Ë1xlxt + ¦Ë1i. clearly  h is of full rank. requiring that the gradient of j w  with respect to w vanish gives the optimal estimate w :
w  = h 1zy
the following proposition states the bias and variance properties of the estimator for the coefficient vector w.
w  cov w   =
¦Ò  	 	¦«
	proof. since y	  = 1  it follows that
	e w    w 	 1 
	=	h 1zztw   w
	=	h 1 zzt + ¦«   ¦« w   w
	=	 i   h 1¦« w   w
	=	 h 1¦«w	 1 
notice cov y  = ¦Ò1i  the covariance matrix of w  has the expression:
	cov w  	=	h 1zcov y zth 1
	=	¦Ò1h 1zzth 1
	=	¦Ò1h 1 h   ¦« h 1
	=	¦Ò1 h 1   h 1¦«h 1 	 1 

therefore mean squared error matrix for the coefficients w is
	e w	w   w	w  t	 1 
	=	h 1¦«
for any x  let  y = w  tx be its predicted observation. the expected squared prediction error is
x
	=	¦Ò1 + xt h 1¦«wwt¦«h 1 + ¦Ò1h 1   ¦Ò1h 1¦«h 1 x
clearly the expected square prediction error depends on the explanatory variable x  therefore average expected square predictive error over the complete data set a is

since
tr xt h 1¦«wwt¦«h 1   ¦Ò1h 1¦«h 1 x 
		tr ¦Ò1xth 1x  
our laplacian optimality criterion is thus formulated by minimizing the trace of xth 1x. definition 1. laplacian optimal design

where z1 ¡¤¡¤¡¤  zk are selected from {x1 ¡¤¡¤¡¤  xm}.
1. kernel laplacian optimal design
¡¡canonical experimental design approaches  e.g. a-optimal design  d-optimal design  and e-optimal  only consider linear functions. they fail to discover the intrinsic geometry in the data when the data space is highly nonlinear. in this section  we describe how to perform laplacian experimental design in reproducing kernel hilbert space  rkhs  which gives rise to kernel laplacian experimental design  klod .
¡¡for given data points x1 ¡¤¡¤¡¤  xm ¡Ê x with a positive definite mercer kernel k : x ¡Áx ¡ú r  there exists a unique
rkhs hk of real valued functions on x. let kt s  be the
.
function of s obtained by fixing t and letting kt s  = k s t .
hk consists of all finite linear combinations of the form with ti ¡Ê x and limits of such functions as the
ti become dense in x. we have
1 derivation of lod in reproducing kernel hilbert space
¡¡consider the optimization problem  1  in rkhs. thus  we seek a function f ¡Ê hk such that the following objective function is minimized:

 1  we have the following proposition.
¡¡proposition 1. let a subspace of hk  the solution to the problem  1  is in h.
¡¡proof. let h¡Í be the orthogonal complement of h  i.e. hk = h ¨’ h¡Í. thus  for any function f ¡Ê hk  it has orthogonal decomposition as follows:
f = fh + fh¡Í
now  let's evaluate f at xi:

notice that kxi ¡Ê h while fh¡Í ¡Ê h¡Í. this implies that
= 1. therefore 

this completes the proof.	
proposition 1 tells us the minimizer of problem  1  admits a representation . please see  for the details.
¡¡let ¦Õ : rd ¡ú h be a feature map from the input space rd to h  and k xi xj  =  ¦Õ xi  ¦Õ xj   . let x denote the data matrix in rkhs  x =  ¦Õ x1  ¦Õ x1  ¡¤¡¤¡¤  ¦Õ xm  . similarly  we define z =  ¦Õ z1  ¦Õ z1  ¡¤¡¤¡¤  ¦Õ zk  . thus  the optimization problem in rkhs can be written as follows: mintr z
since the mapping function ¦Õ is generally unknown  there is no direct way to solve problem  1 . in the following  we apply kernel tricks to solve this optimization problem. let x 1 be the moore-penrose inverse  also known as pseudo inverse  of x. thus  we have:
xx
x x
x
=	kxx	¡Á	+ ¦Ë1	l	+ ¦Ë1	  kxx
where kxx is a m m matrix  kxx ij = k xi xj    kxz is a m¡Ák matrix  kxz ij = k xi zj    and kzx is a k¡Ám matrix  kzx ij = k zi xj  . thus  the kernel laplacian optimal design can be defined as follows:
definition 1. kernel laplacian optimal design
kxxkxzkzx + ¦Ë1kxxlkxx
	¦Ë1kxxkxx	 1 
1 optimization scheme
¡¡in this subsection  we discuss how to solve the optimization problems  1  and  1 . particularly  if we select a linear kernel for klod  then it reduces to lod. therefore  we will focus on problem  1  in the following.
¡¡it can be shown that the optimization problem  1  is np-hard. in this subsection  we develop a simple sequential greedy approach to solve  1 . suppose n points have been selected  denoted by a matrix zn =  z1 ¡¤¡¤¡¤  zn . the  n +
1 -th point zn+1 can be selected by solving the following optimization problem:
	max	kxx
zn+1= zn zn+1 
	¦Ë1kxxlkxx + ¦Ë1kxx	kxx
the kernel matrices kxzn+1 and kzn+1x can be rewritten as follows:
kxzn+1 
thus  we have:
kxzn+1kzn+1x = kxznkznx + kxzn+1kzn+1x
we define:
a = kxznkznx + ¦Ë1kxxlkxx + ¦Ë1kxx
a is only dependent on x and zn. thus  the  n + 1 -th point zn+1 is given by:
¡¡¡¡¡¡¡¡zn+1	  zn+1 = argmintr kxxkxzkxx
 1 
each time we select a new point zn+1  the matrix a is updated by:
a ¡û a + kxzn+1kzn+1x
if the kernel function is chosen as inner product k x y  =
  then hk is a linear functional space and the algorithm reduces to lod.
1. content-based image retrieval using laplacian optimal design
¡¡in this section  we describe how to apply laplacian optimal design to cbir. we begin with a brief description of image representation using low level visual features.
1 low-level image representation
¡¡low-level image representation is a crucial problem in cbir. general visual features includes color  texture  shape  etc. color and texture features are the most extensively used visual features in cbir. compared with color and texture features  shape features are usually described after images have been segmented into regions or objects. since robust and accurate image segmentation is difficult to achieve  the use of shape features for image retrieval has been limited to special applications where objects or regions are readily available.
¡¡in this work  we combine 1-dimensional color histogram and 1-dimensional color texture moment  ctm    to represent the images. the color histogram is calculated using 1 ¡Á 1 ¡Á 1 bins in hsv space. the color texture moment is proposed by yu et al.   which integrates the color and texture characteristics of the image in a compact form. ctm adopts local fourier transform as a texture representation scheme and derives eight characteristic maps to describe different aspects of co-occurrence relations of image pixels in each channel of the  svcosh  svsinh  v  color space. then ctm calculates the first and second moment of these maps as a representation of the natural color image pixel distribution. please see  for details.
1 relevance feedback image retrieval
¡¡relevance feedback is one of the most important techniques to narrow down the gap between low level visual features and high level semantic concepts . traditionally  the user's relevance feedbacks are used to update the query vector or adjust the weighting of different dimensions. this process can be viewed as an on-line learning process in which the image retrieval system acts as a learner and the user acts as a teacher. they typical retrieval process is outlined as follows:
1. the user submits a query image example to the system. the system ranks the images in database according to some pre-defined distance metric and presents to the user the top ranked images.
1. the system selects some images from the database andrequest the user to label them as  relevant  or  irrelevant .
1. the system uses the user's provided information to rerank the images in database and returns to the user the top images. go to step 1 until the user is satisfied.
our laplacian optimal design algorithm is applied in the second step for selecting the most informative images. once we get the labels for the images selected by lod  we apply laplacian regularized regression  lrr    to solve the optimization problem  1  and build the classifier. the classifier is then used to re-rank the images in database. note that  in order to reduce the computational complexity  we do not use all the unlabeled images in the database but only those within top 1 returns of previous iteration.
1. experimental results
¡¡in this section  we evaluate the performance of our proposed algorithm on a large image database. to demonstrate the effectiveness of our proposed lod algorithm  we compare it with laplacian regularized regression  lrr     support vector machine  svm   support vector machine active learning  svmactive    and a-optimal design  aod . both svmactive  aod  and lod are active learning algorithms  while lrr and svm are standard classification algorithms. svm only makes use of the labeled images  while lrr is a semi-supervised learning algorithm which makes use of both labeled and unlabeled images. for svmactive  aod  and lod  1 training images are selected by the algorithms themselves at each iteration. while for lrr and svm  we use the top 1 images as training data. it would be important to note that svmactive is based on the ordinary svm  lod is based on lrr  and aod is based on the ordinary regression. the parameters ¦Ë1 and ¦Ë1 in our lod algorithm are empirically set to be 1 and 1. for both lrr and lod algorithms  we use the same graph structure  see eq. 1  and set the value of p  number of nearest neighbors  to be 1. we begin with a simple synthetic example to give some intuition about how lod works.
1 simple synthetic example
¡¡a simple synthetic example is given in figure 1. the data set contains two circles. eight points are selected by aod and lod. as can be seen  all the points selected by aod are from the big circle  while lod selects four points from the big circle and four from the small circle. the numbers beside the selected points denote their orders to be selected. clearly  the points selected by our lod algorithm can better represent the original data set. we did not compare our algorithm with svmactive because svmactive can not be applied in this case due to the lack of the labeled points.
1 image retrieval experimental design
¡¡the image database we used consists of 1 images of 1 semantic categories  from corel data set. it is a large and heterogeneous image set. each image is represented as a 1-dimensional vector as described in section 1. figure 1 shows some sample images.
¡¡to exhibit the advantages of using our algorithm  we need a reliable way of evaluating the retrieval performance and the comparisons with other algorithms. we list different aspects of the experimental design below.
1.1 evaluation metrics

	 a  data set	 b  aod	 c  lod
figure 1: data selection by active learning algorithms. the numbers beside the selected points denote their orders to be selected. clearly  the points selected by our lod algorithm can better represent the original data set. note that  the svmactive algorithm can not be applied in this case due to the lack of labeled points.

	 a 	 b 	 c 
figure 1: sample images from category bead  elephant  and ship.¡¡we use precision-scope curve and precision rate  to evaluate the effectiveness of the image retrieval algorithms. the scope is specified by the number  n  of top-ranked images presented to the user. the precision is the ratio of the number of relevant images presented to the user to the scope n. the precision-scope curve describes the precision with various scopes and thus gives an overall performance evaluation of the algorithms. on the other hand  the precision rate emphasizes the precision at a particular value of scope. in general  it is appropriate to present 1 images on a screen. putting more images on a screen may affect the quality of the presented images. therefore  the precision at top 1  n = 1  is especially important.
¡¡in real world image retrieval systems  the query image is usually not in the image database. to simulate such environment  we use five-fold cross validation to evaluate the algorithms. more precisely  we divide the whole image database into five subsets with equal size. thus  there are 1 images per category in each subset. at each run of cross validation  one subset is selected as the query set  and the other four subsets are used as the database for retrieval. the precisionscope curve and precision rate are computed by averaging the results from the five-fold cross validation.
1.1 automatic relevance feedback scheme
¡¡we designed an automatic feedback scheme to model the retrieval process. for each submitted query  our system retrieves and ranks the images in the database. 1 images were selected from the database for user labeling and the label information is used by the system for re-ranking. note that  the images which have been selected at previous iterations are excluded from later selections. for each query  the automatic relevance feedback mechanism is performed for four iterations.
¡¡it is important to note that the automatic relevance feedback scheme used here is different from the ones described in   . in     the top four relevant and irrelevant images were selected as the feedback images. however  this may not be practical. in real world image retrieval systems  it is possible that most of the top-ranked images are relevant  or  irrelevant . thus  it is difficult for the user to find both four relevant and irrelevant images. it is more reasonable for the users to provide feedback information only on the 1 images selected by the system.
1 image retrieval performance

	 a  feedback iteration 1	 b  feedback iteration 1
figure 1: the average precision-scope curves of different algorithms for the first two feedback iterations. the lod algorithm performs the best on the entire scope. note that  at the first round of feedback  the svmactive algorithm can not be applied. it applies the ordinary svm to build the initial classifier.

	 a  precision at top 1	 b  precision at top 1	 c  precision at top 1
figure 1: performance evaluation of the five learning algorithms for relevance feedback image retrieval.  a  precision at top 1   b  precision at top 1  and  c  precision at top 1. as can be seen  our lod algorithm¡¡in real world  it is not practical to require the user to provide many rounds of feedbacks. the retrieval performance after the first two rounds of feedbacks  especially the first round  is more important. figure 1 shows the average precision-scope curves of the different algorithms for the first two feedback iterations. at the beginning of retrieval  the euclidean distances in the original 1-dimensional space are used to rank the images in database. after the user provides relevance feedbacks  the lrr  svm  svmactive  aod  and lod algorithms are then applied to re-rank the images. in order to reduce the time complexity of active learning algorithms  we didn't select the most informative images from the whole database but from the top 1 images. for lrr and svm  the user is required to label the top 1 images. for svmactive  aod  and lod  the user is required to label 1 most informative images selected by these algorithms. note that  svmactive can only be apconsistently outperforms the other four algorithms.
plied when the classifier is already built. therefore  it can not be applied at the first round and we use the standard svm to build the initial classifier. as can be seen  our lod algorithm outperforms the other four algorithms on the entire scope. also  the lrr algorithm performs better than svm. this is because that the lrr algorithm makes efficient use of the unlabeled images by incorporating a locality preserving regularizer into the ordinary regression objective function. the aod algorithm performs the worst. as the scope gets larger  the performance difference between these algorithms gets smaller.
¡¡by iteratively adding the user's feedbacks  the corresponding precision results  at top 1  top 1  and top 1  of the five algorithms are respectively shown in figure 1. as can be seen  our lod algorithm performs the best in all the cases and the lrr algorithm performs the second best. both of these two algorithms make use of the unlabeled images. this shows that the unlabeled images are helpful for discovering the intrinsic geometrical structure of the image space and therefore enhance the retrieval performance. in real world  the user may not be willing to provide too many relevance feedbacks. therefore  the retrieval performance at the first two rounds are especially important. as can be seen  our lod algorithm achieves 1% performance improvement for top 1 results  1% for top 1 results  and 1% for top 1 results  comparing to the second best algorithm  lrr  after the first two rounds of relevance feedbacks.
1 discussion
¡¡several experiments on corel database have been systematically performed. we would like to highlight several interesting points:
1. it is clear that the use of active learning is beneficialin the image retrieval domain. there is a significant increase in performance from using the active learning methods. especially  out of the three active learning methods  svmactive  aod  lod   our proposed lod algorithm performs the best.
1. in many real world applications like relevance feedback image retrieval  there are generally two ways of reducing labor-intensive manual labeling task. one is active learning which selects the most informative samples to label  and the other is semi-supervised learning which makes use of the unlabeled samples to enhance the learning performance. both of these two strategies have been studied extensively in the past       . the work presented in this paper is focused on active learning  but it also takes advantage of the recent progresses on semi-supervised learning . specifically  we incorporate a locality preserving regularizer into the standard regression framework and find the most informative samples with respect to the new objective function. in this way  the active learning and semi-supervised learning techniques are seamlessly unified for learning an optimal classifier.
1. the relevance feedback technique is crucial to imageretrieval. for all the five algorithms  the retrieval performance improves with more feedbacks provided by the user.
1. conclusions and future work
¡¡this paper describes a novel active learning algorithm  called laplacian optimal design  to enable more effective relevance feedback image retrieval. our algorithm is based on an objective function which simultaneously minimizes the empirical error and preserves the local geometrical structure of the data space. using techniques from experimental design  our algorithm finds the most informative images to label. these labeled images and the unlabeled images in the database are used to learn a classifier. the experimental results on corel database show that both active learning and semi-supervised learning can significantly improve the retrieval performance.
¡¡in this paper  we consider the image retrieval problem on a small  static  and closed-domain image data. a much more challenging domain is the world wide web  www . for web image search  it is possible to collect a large amount of user click information. this information can be naturally used to construct the affinity graph in our algorithm. however  the computational complexity in web scenario may become a crucial issue. also  although our primary interest in this paper is focused on relevance feedback image retrieval  our results may also be of interest to researchers in patten recognition and machine learning  especially when a large amount of data is available but only a limited samples can be labeled.
