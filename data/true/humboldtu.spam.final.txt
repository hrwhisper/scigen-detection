   this paper discusses several lessons learned from the spamtrec 1 challenge. we discuss issues related to decoding  preprocessing  and tokenization of email messages. using the winnow algorithm with orthogonal sparse bigram features  we construct an efficient  highly scalable incremental classifier  trained to maximize a discriminative optimization criterion. the algorithm easily scales to millions of training messages and millions of features. we address the composition of training corpora and discuss experiments that guide the construction of our spamtrec entry. we describe our submission for the filtering tasks with periodical re-training and active learning strategies  and report on the evaluation on the publicly available corpora.
1 introduction
spam filtering remains a technological challenge; the commercial incentive for spam senders results in an arms race between filtering methods and spam obfuscation techniques. naive bayes  1  1  and rule based learners  have been very popular; discriminative approaches like support vector machines  logistic regression as well as maximum entropy have also been studied for spam filtering. most discriminatively trained methods are non-incremental and often scale poorly to large training samples. regular updates of the classifier which are necessary due to topic drift and the adversarial nature of spam  are consequently costly or even infeasible.
   p-norm algorithms such as winnow  1  1  are incremental and can be implemented to scale to large amounts of training data. these methods have proven to be very robust  and highly scalable in practice. a number of alternative approaches such as network-based spam detection  collaborative filtering strategies  or email batch detection using graph theoretical methods have been studied but cannot be applied to the spamtrec challenge because of the nature of the data that is provided.
   in this paper  we address challenging issues for the construction of practical filtering systems. we discuss how a large-scale filter can be trained using a discriminative optimization criterion. we address preprocessing  decoding  and tokenization issues  and questions regarding the assembly of training corpora. we report on experiments that guide the construction of our filtering system. we discuss the evaluation of this system on the public spamtrec corpora and conclude with a number of lessons learned.
   the rest of this paper is structured as follows. we address challenges for practical spam filters in section 1 and report on our experiments. section 1 describes the system that we used in the competition; section 1 concludes.
1 challenges in email classification
in this section  we address design issues for practical spam filtering systems and report on experiments that guide the construction of a system that performs well for the spamtrec challenge.
1 scalable discriminative training
text classification requires highly scalable methods as it involves large amounts of high dimensional data. winnow  a perceptron-like algorithm with multiplicative updates   can handle a huge amount of data very efficiently  outperforming other well-established filters such as naive bayes at the same time . however  practical difficulties and implementation issues still remain.
   beyond preprocessing and tokenization  an efficient implementation has to compute the features - hash values of the parsed tokens or n-grams - and spam scores on the fly. computation of the score involves huge hash structures with several million buckets. our implementation is an efficient version of winnow which scales to large corpora and feature vectors. a related version of our filtering system is used by a commercial webspace and email service provider and filters about 1 million email messages per day.
1 preprocessing and tokenization
encoding schemes provide spam senders with a rich set of tools to obfuscate tokens. for example  the word  caf¡äe  would be html-encoded as  caf&eacute;   url-encoded as  caf=e1   or base1-encoded as  y1fm1q== . in order to avoid an inflation of the attribute space  it is highly desirable to map all these representations to the same token. this can be achieved by transforming tokens into a canonical encoding scheme  such as utf-1.
   our spam filtering system includes modules for a variety of different preprocessing measures. they incorporate procedures to analyze the structure  and conformity to several standards  of each email and its parts. they include procedures that transform the email contents into the canonical utf-1 encoding scheme. these transformation steps result in a representation of the message text that is independent of the particular encapsulation method used.
   the conformity checks and the attachment dissection provide additional features that can be used for classification. for example  spammers often forge the date of the email  such that the message appears user as the most recent item the user's inbox for a long time. in detail  prior to tokenization each email is subjected to the following actions:
  parsing structure of mime-parts;
  decoding mime-part contents  e.g.  base1 or  quoted/printable  decoding ;
  transforming the character set into the utf-1 encoding scheme;
  decoding the subject-string according to rfc1 and rfc1;
  transforming html- or url-encoded characters into utf-1;
  plausibility checks of the  received  time according to rfc1;
  extraction of language information based on used character sets;
  extraction of information about attachment types;
  checking of standard conformity of mime structure and attached files.
   we conduct a set of experiments in order to study the benefit of these pre-processing and feature extraction steps. first  we study the effectiveness of pre-processing and its contribution to the classification results. therefore we train two classifiers using identical training sets containing 1 randomly selected emails from our english corpus  table 1 . for the first classifier  the itemized preprocessing steps are carried out  whereas they are disabled for the second classifier. we use an evaluation set of again 1 emails from the same corpus. we observe an auc performance of 1 when preprocessing is employed and 1 when it is disabled. that is  the preprocessing step decreases the risk  1   auc  by 1% from 1 to 1. in this experiment  clearly the accuracy is high for both classifiers. nevertheless  a 1% reduction of the risk is a significant finding that emphasizes the importance of the preprocessing step.
   preprocessing has a noticeable effect on the subsequent feature generation step. without preprocessing  trained on 1 english documents  the filter has 1 million features with nonzero weights. when preprocessing is employed  only 1 million features are used.
   the stream of preprocessed characters has to be tokenized in a way that facilitates the extraction of discriminative attributes. whereas tokenization can be based on whitespace characters and punctuation symbols for european languages  such whitespace characters are absent for many asian languages such as chinese  japanese and korean languages. for asian languages  we treat each character as an individual token  resulting in syllabic tokens. the resulting loss of inter-syllabic context is compensated for by the use of n-gram features in the following feature extraction step.
table 1: data used for experiments
sourceenglish docschinese docsccert data set  www.ccert.edu.cn/spam 1disclosed enron emails 1guenter spam trap  untroubled.org/spam 1imc mailing lists  www.imc.org 1various newsletters1spamarchive.org1spamassassin collection1trec 1 corpus1various moderated usenet groups1wouters archive  www.xtdnet.nl/paul/spam 1own collection11 text feature extraction
from the stream of preprocessed tokens  features have to be extracted that provide the classification method with sufficiently discriminative information to allow for a highly accurate decision. in many cases  contextual information that spans across multiple tokens hints at the semantics of sentences. this is particularly true for asian languages for which words span across multiple tokens. in addition  it has become popular among spam senders to blur the bag-of-word-view of the messages by appending random sets of good words  individually drawn for each message. orthogonal sparse bigrams provide a mechanism for obtaining discriminative features . empirically  they have shown to be an effective and scalable mechanism to represent information table 1: dimensionality of the filter  based on training corpus
corpusnumber of documentsfeaturesenglish & chinese11 1english11 1english11 1english11 1english11 1english  no preprocessing11 1chinese11trec fully trained classifier11 1trec weakly trained classifier11 1contained in n adjacent tokens. a window of fixed width  in our case n = 1  slides over the sequence of tokens. for each window position  the set of all two-elementary combinations of the n tokens is generated. each combination is constrained to always include the left-most token and zero or one other token; the remaining tokens in the window are replaced by a special skip symbol    . the resulting n orthogonal sparse bigram combinations uniquely represent the content of the current window position.
   for example  the window containing the sequence  all medications at low price   is represented by the orthogonal sparse bigram features halli  hall medicationsi  hall   ati  hall    lowi  and hall     pricei. the combination of these five features represents the entire content of the window. information is lost  however  when the orthogonal sparse bigram features of the entire message are pooled into a single feature vector. thus  orthogonal sparse bigrams implement an appealing and empirically proven trade-off between scalability and representational richness. table 1 displays the number of features that the filter employs  based on the training corpus. between 1 and 1 million orthogonal sparse bigram features have nonzero weights.
1 large training corpora
the high dimensionality of the feature representation and the required high accuracy of the resulting classifier call for an extremely large training corpus. in this section  we want to clarify to which extent the winnow algorithm can benefit from additional training data and which amount of data is tractable. therefore we trained four classifiers on between 1 and 1 emails of the english corpus  table 1 . another 1 emails were kept for testing. table 1 and figure 1 show the results of the experiments. they confirm the linear runtime behavior of winnow as well as a significant improvement of accuracy when using larger training sets. it can be seen that the number of training examples has a much greater impact on classification performance than preprocessing.
table 1: impact of training set size on classifier performance and training time
training set sizeaucmaxspam precision attrainingf-measure1% non-spam errortime in s1 emails111 %11 emails111 %11 emails111 %11 emails111 %1	accuracy	execution time
	 1	 1	 1
training set size
figure 1: plot of classifier quality  left  and training time  right  depending on the size of the training set
1 differences in distributions underlying training and test data
most machine learning methods assume that training data be governed by the exact same distribution that the classifier is exposed to at application time. in the spam filtering application  control over the data generation process is less perfect. a number of public sources of spam messages and a much more limited number of sources of non-spam emails are available. on the other hand  american  chinese  professional  recreational and many other groups of email users receive emails from a range of diverging distributions. questions arise about the effect of such divergence between training and testing data on the classifier  and about the optimal way of dealing with this divergence. for a discussion of these generally under-studied questions  see
.
   it is relatively easy to identify the language used in a message. we study the impact that training a classifier on an english  chinese  or mixed corpus will have on its performance on english  chinese  or mixed testing data. our goal is to obtain guidance on the optimal training corpus for a classifier that may be exposed to additional  unforeseen languages. we would also like to know whether it is advisable to use a joint classifier  or separate classifiers that perform well for only one language.
   we perform several experiments using a chinese and an english email corpus containing 1 emails each. after splitting the data in 1% training and 1% test data we train three classifiers using both data sets and a mixed corpus. the experimental results  given in table 1  indicate that a classifier trained on emails of both languages performs very similar to the monolingually trained classifiers on each individual language.
   recently  the compensation of sample selection bias is being studied  e.g.   . in order to explicitly account for a divergence between training and testing data  the  unlabeled  testing data has to be available at training time. unfortunately  this is not the case in the spamtrec challenge.
   from our experiments  we conclude that in the absence of unlabeled testing data  a classifier trained on a heterogeneously mixed corpus is generally preferable  because it does not lose accuracy on the individual languages while gaining additional robustness. we therefore compile a training corpus from maximally heterogeneous sources. it includes mailing lists  newsletters  several distinct spam traps  personal emails  moderated usenet groups  public email corpora such as the enron dataset  and many more. for details  see section 1.
table 1: performance of classifier depending on language
chinese test dataenglish test datatraining dataauc	max f-measureauc	max f-measurechinese1	11	1english1	11	1both1	11	11 trec 1 spam track tasks
the trec 1 spam track consists of two different tasks. in the online filtering task  the spam filter classifies each message from the test corpus  and subsequently receives the true label of each message for training. there are two sub-tasks  the first with immediate feedback  and the second with delayed feedback; for the second sub-task  training is carried out after a randomly sized chunk of messages has been processed. in the active filtering task  the spam filter gets to choose a number of emails for which the label is then disclosed.
   all tasks are carried out on four distinct evaluation corpora. there are two private corpora  tagged b1 and x1  one public corpus with english emails and one public corpus with chinese messages. a summary of the corpus sizes is shown in table 1.
table 1: evaluation corpora
dataset#non-spams#spamsb1  private 1x1  private 1english  public 1chinese  public 1   the rules of the spam track allow each contestant to submit four different filter configurations for each of the two main tasks. in the following  we describe the configurations of our entries  and the results on the different evaluation corpora.
1 pre-training
to take advantage of the capability of the winnow algorithm to handle large sets of known features and large training corpora  we assemble a corpus of emails to pre-train our filter prior to submission. table 1 gives an overview of the sources of our training emails and their numbers of spam and non-spam emails.
   extensive pre-training imposes a risk if the chosen training data only poorly reflects the distribution at application time. therefore we submit one weakly trained filter configuration for each task. for this  only a single iteration of the winnow algorithm over the data is exercised  instead of re-iteration until convergence.
1 online classification with periodical re-training
the winnow algorithm performs best when the model is trained by iterating several times over all training emails. however  all trec tasks are designed to expose the filter to each training email only once. to overcome this discrepancy  we experiment with several strategies to cache all seen training emails and re-train the classifier on them periodically. our experiments on different corpora show a notable influence of the re-training strategy on overall accuracy  but table 1: training corpus composition
source#non-spams#spamsccert data set  www.ccert.edu.cn/spam 1disclosed enron mails 1guenter spam trap  untroubled.org/spam 1imc mailing lists  www.imc.org 1various newsletters1nazario corpus  monkey.org/¡«jose/phishing 1spamarchive.org1spamassassin collection1trec 1 corpus1various moderated usenet groups1wouters archive  www.xtdnet.nl/paul/spam 1own collection1no single strategy can outperform the others consistently. therefore  we choose to vary the strategies over the four allowed submissions. we include two submissions where a re-training is executed after each misclassified email  one configuration with re-training after every 1th email  and one submission with no re-training at all.
   the results on the four evaluation datasets in table 1 show that the periodical re-training is indeed successful in improving the performance in the online setting. on all but one dataset the configuration without re-training performs worst.
   the private b1 corpus seems to be the most difficult dataset with the most mistakes made. presumably this is due to the corpus differing strongly from our training data. this assumption is also consistent with the second configuration performing best on this dataset. the weak pre-training allows it to faster adapt to the different evaluation set.
table 1: auc results for the online filter tasks  on the two private corpora  b1  x1   the public english  e   and the public chinese  c  corpus
number11pre-trainingfullweakfullfullre-train aftermistakesmistakesneverevery 1thb1 immediate1111b1 delayed1111x1 immediate1111x1 delayed1111e immediate1111e delayed1111c immediate1111c delayed1111   figures 1  for immediate feedback  and 1  for delayed feedback  show learning curves for all participants of spamtrec  aggregated over all  public and private  evaluation corpora. the data has kindly been provided godon cormak. for all participants  the best of all submitted filters was used. when few or no training examples have been provided  the auc mesaure of our filter exceeds the auc of other submitted systems in both cases. for larger training samples  the filter falls back behind other submissions. we believe that this behavior is at

figure 1: results of the evaluation aggregated over all corpora for immediate feedback.

figure 1: results of the evaluation aggregated over all corpora for delayed feedback.
least partly explained by the large-scale pre-training that we used for all of our submissions- even the weakly trained filter was pre-trained with many emails. pre-training leads to a better performance from the start but slows down the adaptation to the evaluation corpora. a careful comparative evaluation of all submitted filter is provided by .
1 active learning task
in the active learning task  the spam filtering system has to decide which emails from a given set are to be labeled for training. there are various possible methods to select the next email. the idea behind most of them is to select those training emails which are expected to provide the most knowledge about the class labels of the test emails. one standard approach is uncertainty sampling   where one selects the training item which lies closest to the current decision boundary. this captures the intuition that an update of the decision boundary with this item has the largest effect on the shape of the decision regions  and therefore the greatest knowledge gain.
   besides uncertainty sampling  we evaluate several ad-hoc strategies  such as selecting the email with the highest number of unknown features  or selecting the email with the best feature coverage over all available training mails. but our experiments show that none of them can outperform uncertainty sampling. therefore  three of our four submissions for task 1 use this standard method. the remaining  weakly trained configuration uses random sampling  because uncertainty sampling is unlikely to yield good results if the distance to the decision boundary depends only on very few update steps.
   in contrast to our preliminary experiments  uncertainty sampling does not consistently outperform random sampling on the evaluation data of the competition. in combination with the periodical re-training strategy  the spam classifier tends to degrade to a highly imbalanced state  yielding almost useless spam scores. apparently the selection of some disadvantageous training emails shifted the score of a large portion of spam emails after re-training far below the decision threshold.
private corpus b1private corpus x1
	public english corpus	public chinese corpus

figure 1: results on the active learning task
   this problem does not occur with our two submitted configurations without pre-training  one with uncertainty sampling and the other with random sampling. in figure 1 one can again see that the b1 corpus has different characteristics than the others. as one would expect on an evaluation corpus which is highly different from the training corpus  the uncertainty sampling strategy performs worse than random sampling due to strong initial fluctuations of the decision boundary.
1 conclusion
the winnow classifier in conjunction with orthogonal sparse bigram features proves to be highly scalable and capable of efficiently handling  and benefiting from  hundreds of thousands of training messages and millions of features. our experiments emphasize the importance of large  diverse training sets; accuracy and robustness still improve when the training data is already very large. the accuracy of the classifier is further improved  and the resulting dimensionality of the classifier reduced  by character set decoding and normalization.
   for the incremental spamtrec tasks  using the cache of all previously seen messages in each model update step outperforms an update based on just the newly seen mail. in both tasks  the weakly trained classifier managed to adapt faster to the apparently most difficult evaluation corpus  as we expected. we make an interesting observation with respect to the benefit of uncertainty sampling versus random sampling. uncertainty sampling is beneficial for the public  but detrimental for the private corpora that apparently deviate from the training data more strongly. negative results for uncertainty sampling are rare in the literature  possibly because usually the case of identically distributed training and testing data is studied.
   recently  image spam is challenging spam filters. even state-of-the-art filters are nearly helpless as the number of image spam messages explodes. visual data requires different feature extraction procedures; the efficiency of the processing steps is crucial.
   it would be interesting for future spamtrec competitions to include evaluation corpora with emails from many different users with complete  unmodified headers. this would permit non-text based approaches to spam filtering  such as classification based on social networks or information about the sending servers. to explore the benefits of collective classification approaches  an additional task could expose the spam filters to more than one email at once instead of one at a time  and thus allow the incorporation of relationships between each other.
acknowledgment
this work was supported by a grant from strato ag. t.s. is supported by grant sche 1-1 of the german science foundation dfg. we wish to thank gordon cormack for his great support!
