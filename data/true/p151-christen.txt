the task of linking databases is an important step in an increasing number of data mining projects  because linked data can contain information that is not available otherwise  or that would require time-consuming and expensive collection of specific data. the aim of linking is to match and aggregate all records that refer to the same entity. one of the major challenges when linking large databases is the efficient and accurate classification of record pairs into matches and non-matches. while traditionally classification was based on manually-set thresholds or on statistical procedures  many of the more recently developed classification methods are based on supervised learning techniques. they therefore require training data  which is often not available in real world situations or has to be prepared manually  an expensive  cumbersome and time-consuming process.
모the author has previously presented a novel two-step approach to automatic record pair classification  1  1 . in the first step of this approach  training examples of high quality are automatically selected from the compared record pairs  and used in the second step to train a support vector machine  svm  classifier. initial experiments showed the feasibility of the approach  achieving results that outperformed k-means clustering. in this paper  two variations of this approach are presented. the first is based on a nearestneighbour classifier  while the second improves a svm classifier by iteratively adding more examples into the training sets. experimental results show that this two-step approach can achieve better classification results than other unsupervised approaches.
categories and subject descriptors
i.1  artificial intelligence : learning; h.1  database management : database applications-data mining
general terms
algorithms  experimentation  performance
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm 1-1-1/1 ...$1.
keywords
data matching  data linkage  deduplication  entity resolution  nearest neighbour  support vector machine
1. introduction
모as increasingly large amounts of data are being collected by many organisations  techniques that enable efficient mining of massive databases have in recent years attracted interest from both academia and industry. sharing of large databases between organisations is also of growing importance in many data mining projects  as data from various sources often has to be linked and aggregated in order to improve data quality  or to enrich existing data with additional information  1  1 . similarly  detecting duplicate records that relate to the same entity within one database is commonly required in the data preparation step of many data mining projects . the aim of such linkages and deduplications is to match all records that relate to the same entity. these entities can be  for example  patients  customers  businesses  product descriptions  or publications.
모traditionally  record linkage has been employed in the health sector and within statistical agencies . today  many public and private sector organisations use deduplication and linkage techniques to improve the quality of their databases. government agencies use record linkage to  for example  identify people who register for assistance multiple times  or who collect unemployment benefits despite being employed. national security  and crime and fraud detection  are other areas where record linkage is increasingly being employed. security agencies often require fast access to files of a particular individual in order to solve crimes or to prevent terror through early intervention .
모if all databases to be linked contain common entity identifiers  or keys   then the problem of linking at the entity level can be solved by a standard database join. in most situations  however  no such entity identifiers are available  and therefore more sophisticated linkage techniques have to be applied. broadly  these techniques can be classified into deterministic  probabilistic  and modern approaches .
모figure 1 outlines the general record linkage process. an important initial step for successful linkage is data cleaning and standardisation  as in most real-world databases noisy  incomplete and incorrect information is common . a lack of high quality data can be one of the biggest obstacles to successful record linkage. the main tasks of data cleaning and standardisation are the conversion of the raw input data into well defined  consistent forms  and the resolution of inconsistencies in the way information is represented.

figure 1: the general record linkage process. the output of the blocking step are candidate record pairs  while the field  attribute  comparison step generates weight vectors with matching weights.
모when linking two databases  a and b  potentially each record in a should be compared with all records in b. therefore  the total number of potential record pair comparisons equals |a| 뫄 |b|  with | 몫 | denoting the number of records in a database. similarly  when deduplicating a database  a  the total number of potential record pair comparisons is |a| 뫄  |a|   1 /1  as each record potentially has to be compared to all others. however  as the performance bottleneck in a record linkage or deduplication system is normally the expensive detailed comparison of field  or attribute  values between records  1  1   it is impossible to compare all pairs when the databases are large. additionally  assuming there are no duplicate records in the databases to be linked  i.e. one record in a can only be a true match to one record in b  and vice versa   then the maximum number of true matches corresponds to min |a| |b| . thus  when linking larger databases the computational efforts potentially increase quadratically while the maximum number of true matches only increases linearly. this also holds for deduplication  where the number of true duplicate records is always less than the number of records in a database.
모to reduce the potentially very large number of comparisons to be conducted between pairs of records  some form of indexing or filtering technique  collectively known as blocking   is employed by most record linkage systems. a single record attribute  or a combination of attributes  often called the blocking key  is used to split the databases into blocks. all records that have the same value in the blocking key will be inserted into the same block  and candidate record pairs are only generated from records within the same block. even though blocking will remove many of the record pairs that are obvious non-matches  some true matches will likely also be removed in the blocking process  because of errors or typographical variations in record attribute values .
모the two records in a candidate pair are compared using similarity functions applied to selected record attributes  fields . these functions can be as simple as an exact string or a numerical comparison  can take typographical variations into account   can be specialised for example for date or time values  or they can be as complex as a distance comparison based on look-up tables of geographic locations  longitudes and latitudes . there are also various approaches to learn such similarity functions from training data  1  1 . each similarity function returns a numerical
christinesmith1mainstreetchristinasmith1mainstbobo'brian1smithrdrobertbryce1smytheroadr1:
r1:
r1:
r1:
wv r1 r1 :  1  1  1  1  1 
wv r1 r1 :  1  1  1  1  1 
wv r1 r1 :  1  1  1  1  1 
wv r1 r1 :  1  1  1  1  1 
wv r1 r1 :  1  1  1  1  1 
wv r1 r1 :  1  1  1  1  1 
figure 1: four example records  made of given name and surname; and street number  name and type  and the corresponding weight vectors  wv  resulting from the comparisons of these records.
matching weight that is usually normalised  such that 1 corresponds to exact similarity and 1 to total dissimilarity  with attribute values that are somewhat similar having a matching weight somewhere in between 1 and 1.
모as illustrated in figure 1  for each compared record pair a weight vector is formed that contains the matching weights calculated for that pair. using these weight vectors  candidate pairs are classified into matches  non-matches  and possible matches  depending upon the decision model used  1  1  1 . pairs of records that are not compared due to the blocking process are implicitly assumed to be non-matches. assuming there are no duplicate records in the databases to be linked  then the majority of candidate pairs are likely non-matches  because the maximum possible number of true matches corresponds to the number of records in the smaller of the databases that are linked. classifying record pairs is therefore often a very imbalanced problem .
모two records that have equal or very similar values in all their attributes will likely refer to the same entity  as it is unlikely that two entities have very similar values  or even the same value  in all their record attributes. all matching weights calculated when comparing such a pair of records will be 1  or close to 1. on the other hand  weight vectors that contain matching weights of only 1  or weights close to 1  were with high likelihood calculated when two records that refer to two different entities were compared  as it is unlikely that two records that refer to the same entity have totally different values in all their attributes. for example  when somebody moves  most of that person's address details will change  while the person's name  gender and date of birth will stay the same. therefore  there would be several record attributes that keep their values.
모based on these observations  it is often easy to accurately classify record pairs as matches when their weight vectors contain only matching weights close to or equal to 1  and as non-matches when their weights are all close to or equal to 1. on the other hand  it is more difficult to correctly classify record pairs that have some similar and some dissimilar attribute values. in figure 1  for example  records r1 and r1 are very similar to each other  with only two small differences in their given name and street type attributes  and thus very likely refer to the same person. on the other hand  records r1 and r1 are more different from each other  and it is not obvious if they refer to the same person or not.
모it follows that it is possible to automatically select training examples  weight vectors  from the set of all weight vectors that with high likelihood correspond to true matches or true non-matches  and to then train a supervised binary classifier using these training examples as 'seeds'. for example  of the weight vectors shown in figure 1  wv r1 r1  can be selected as a match training example  while wv r1 r1  and wv r1 r1   possibly even wv r1 r1  and wv r1 r1   can be used as non-match training examples.
모this two-step approach to automated record pair classification has first been proposed by the author in   with initial experiments indicating its feasibility. the contribution of this paper is the evaluation of two improved classification methods to be used in the second step of the approach. the first method is based on nearest-neighbour classification  and the second improves svm classification by iteratively adding more weight vectors into the training sets.
모the remainder of this paper is structured as follows. an overview of related work is given next. then  the proposed two-step approach to record pair classification is presented in detail in section 1  with the new classification methods discussed in section 1. these two methods are then evaluated experimentally in section 1 using both real and synthetic data sets  and the paper is concluded in section 1 with an outlook to future work.
1. related work
모the classic probabilistic record linkage approach  as formalised in the 1s by   has in recent years been improved by applying the expectation-maximisation  em  algorithm for better parameter estimation in record pair classification   and by using approximate string comparisons to calculate partial agreement weights when record attribute  field  values have typographical variations  1  1 .
모since the mid 1s  researchers have investigated a variety of approaches to record linkage  originating from artificial intelligence  database technology  information retrieval  machine learning  and data mining  1  1   with the aims of improving the linkage quality and the scalability of the linkage process. many of these approaches are based on supervised learning techniques and require training data  record pairs with known true match or true non-match status . such training examples  however  are often not available in real world situations  or they have to be prepared manually. this is a laborious process  and often the training data generated is not 1% accurate  as even humans are not always able to clearly determine weather two records are a true match or not  without having access to further information. one supervised approach is to learn similarity measures for approximate string comparisons  such as the costs for edit-distance operations  1  1   with the aim to adapt similarity calculations to a particular data domain. decision tree induction  1  1  and support vector machines  are two popular supervised machine learning techniques that have been employed successfully for record pair classification. as expected  these techniques usually achieve better linkage quality compared to unsupervised approaches.
모three methods for record pair classification have been implemented in tailor : the first is based on supervised decision tree induction  the second is using unsupervised kmeans clustering  with three clusters  one each for matches  possible matches and non-matches   and the third is a hybrid approach that combines the first two to overcome the problem of lack of training data. it first clusters a sub-set of the weight vectors  again into matches  possible matches and non-matches   and then uses the match and non-match clusters to train a supervised decision tree classifier. both the fully supervised and hybrid approach outperformed k-means clustering in experimental studies. in section 1  a variation of the hybrid tailor approach  employing a svm instead of a decision tree classifier  will be compared experimentally to the proposed two-step classification approach.
모active learning is an approach that aims to overcome the problem of lack of training data. a system that presents a difficult to classify record pair to a user for manual classification is discussed in . after such a pair has been manually classified  it is added to the training data and the classifier is re-trained. this process is repeated until all record pairs are successfully classified. using this approach  manually classifying less than 1 training pairs provided better results than a fully supervised approach that required 1 randomly selected examples. a similar approach is presented in   where a committee of decision trees is used to learn a set of rules that describe linkages.
모unsupervised clustering techniques have been investigated both to improve blocking  1  1  and for automatic record pair classification  1  1  1 . the clustering techniques kmeans and farthest-first were compared in  with supervised decision tree induction using both synthetic and real data. surprisingly  farthest-first clustering outperformed kmeans and achieved results comparable to decision trees. in   the k-means clustering algorithm has been employed to group weight vectors into matches and non-matches. in this approach  a user can also identify a 'fuzzy' region halfway in between the two cluster centroids where the difficult to classify record pairs are located. these pairs will then be handed to the user for manual clerical review. using synthetic data  it was shown that this approach can significantly reduce the number of record pairs that have to be reviewed manually  while keeping high linkage quality.
모recently  unsupervised techniques based on relational clustering  have been explored for entity resolution of relational data. while traditional record linkage techniques assume that only similarities between attribute values are available  in relational data the entities have additional relational information that can be used to improve the quality of entity resolution. relational information is present  for example  in census databases that include a family relationship attribute  with values like 'married to'  'dependent of'  or 'parent of' ; or in bibliographic data where  besides the name of a paper  a publication record also contains a list of one or more authors that can indicate co-author relationships. experimental results  showed that relational entity resolution outperforms traditional record linkage based only on record attribute similarities. however  non-relational data is still available in many real world applications  such as in databases that contain hospital patient or customer information  and this paper concentrates on improving unsupervised classification of such non-relational data.
모the two-step classification approach presented in this paper has been inspired by similar methods for text classification  where commonly only a limited number of positive labeled examples  besides many unlabeled examples  are available for training. in such situations the aim is to learn a classifier from these positive and unlabeled examples. in   the tc-won approach is described  which iteratively trains a svm using the positive and a selected set of strong negative examples. additional unlabeled examples are included into the training sets as the trained classifier becomes more accurate  until all unlabeled examples are classified.
	 a 	 b 	 c 	 d 	 e 
1
wv
11
wv
11
wv
11
wv
11
wv
1
	1	wv	1	1	wv	1	1	wv	1	1	wv	1	1	wv	1
figure 1: example of the seeded nearest-neighbour classification process with 1-dimensional weight vectors and k = 1. weight vectors classified as matches are shown with a   non-matches with a    and unclassified weight vectors with a circled question mark. the seed training examples are shown as bold circles. in each step  the unclassified weight vector closest to k already classified neighbours is added to one of the training sets. details of this process are described in section 1.1 and algorithm 1.1. two-step classification
모the idea of seeded record pair classification is based on the following two assumptions. first  weight vectors that contain exact or high similarity values in all their matching weights were with high likelihood generated when two records that refer to the same entity were compared. second  weight vectors that contain mostly low similarity values were with high likelihood generated when two records that refer to different entities were compared. as a result  selecting such weight vectors in a first step as seeds for generating training data  and training a classifier using these seed training examples in a second step  should enable automatic  efficient and accurate record pair classification.
모previously  in  and   the author has shown the feasibility of this proposed approach  and investigated several variations of how to select the initial seed training examples. this paper concentrates on the second step of the approach  which will be discussed in detail in section 1. first  an overview of the first step of the approach is given.
1 step 1: selection of training examples
모let w be the set of weight vectors that were generated in the comparison step  after blocking has been applied to reduce the total number of detailed record pair comparisons . the aim of the first step of the proposed approach is to select weight vectors from w that with very high likelihood correspond to true matches and true non-matches. the selected weight vectors are inserted into the match seed training examples set  wm  and the non-match seed training examples set  wn  respectively  with wm 뫌wn =   . there are two main approaches to selecting training examples  either using distance thresholds or nearest-based .
모the threshold based approach selects weight vectors that have all their matching weights within a certain distance threshold to the exact similarity or total dissimilarity values  respectively. for example  using the weight vectors from figure 1 and a distance threshold of 1  only wv r1 r1  will be selected into wm  and wv r1 r1  and wv r1 r1  into wn. the remaining three weight vectors will not be selected  as at least one of their matching weights is further than the 1 distance threshold away from 1 or 1.
모in the nearest based approach  on the other hand  weight vectors are sorted according to their distances  using  for example  manhattan or euclidean distance  from the vectors containing only exact similarities and only total dissimilarities  respectively  and the respectively nearest vectors are selected into the training sets. in figure 1  wv r1 r1  is closest to the exact similarities vector   1  1  1  1  1    followed by wv r1 r1 ; while wv r1 r1  and wv r1 r1  only contain total dissimilarity values  and wv r1 r1  and wv r1 r1  are the next vectors closest to them.
모experiments  showed that the nearest based approach generally outperforms threshold based selection. one reason is that nearest based selection allows explicit specification of the number of weight vectors to be included into wm and wn. because weight vector classification is often a very imbalanced problem  the number of true non-matches in w is commonly much larger than the number of true matches   and thus more weight vectors should be selected into wn than into wm. an estimation  r  of the ratio of true matches to true non-matches can be calculated using the number of records in the two databases to be linked  a and b  and the number of weight vectors in w:
	 	 1 
with | 몫 | denoting the number of elements in a set or a database. the problem with balanced training set sizes is that weight vectors that likely do not correspond to true matches will be selected into wm .
1 step 1: classification of record pairs
모once the seed training example sets for matches  wm  and non-matches  wn  have been generated  they can be used to train any binary classifier. in the following two sections  a nearest-neighbour based classifier and an iterative svm classifier are presented. the set of weight vectors not selected into the seed training example sets will be denoted with wu  with wu = w    wm 뫋 wn .
1.1 nearest-neighbour classification
모the basic idea of this classifier is to iteratively add unclassified weight vectors from wu into the training sets until all weight vectors are classified. in each iteration  the unclassified weight vector closest to k already classified weight vectors is classified according to a majority vote of its classified neighbours  i.e. if the majority is either matches or non-matches . using the seed training example sets  this nearest-neighbour based classifier can be implemented efficiently as illustrated in figure 1 and detailed in algorithm 1.

algorithm 1: seeded k-nn classification

input:
- set of weight vectors generated in comparison step: w
- seed training examples match set: wm
- seed training examples non-match set: wn
- distance function: dist - number of nearest-neighbours to consider: k
output:
- weight vectors classified as matches: zm
- weight vectors classified as non-matches: zn
1:	zm := wm and zn := wn
1:	wt :=  wm 뫋 wn  and wu := w   wt
1: initialise empty heap h 1: m :=  n :=  u := 
1:	for wm 뫍 wm:
1:	m wm  := { k + 1  nearest wu 뫍 wu 
sorted according to dist wm wu }
1:	end for
1:	for wn 뫍 wn:
1:	n wn  := { k + 1  nearest wu 뫍 wu 
sorted according to dist wn wu }
1:	end for
1:	for wu 뫍 wu:
1: u wu  := { k + 1  nearest wt 뫍 wt  sorted according to dist wu wt }
1:	s := pwt뫍u wu  1:k  dist wu wt 
1:	insert  s wu  into h 1:	end for
1:	while wu 1=  :
1:	 s wt  := first element in h
1:	wu := wu   wt
1: if u wt  contains more weight vectors from zm than zn:
1:	zm := zm + wt
1:	else:
1:	zn := zn + wt 1:	end if
1: xu := 뫋wu뫍u wt  m wu  뫋 n wu   1: for wu 뫍 xu:
1:	d := dist wt wu 
1:	if wu is one of  k + 1  nearest to wt: 1:	update wt in u wu  with d
1:	s := pwv뫍u wu  1:k  dist wu wv 
1:	update  s wu  in h
1:	end if
1:	end for
1:	if wt 뫍 zm:
1:	m wt  := { k + 1  nearest wu 뫍 xu 
sorted according to dist wt wu }
1:	else:
1:	u wt  := { k + 1  nearest wu 뫍 xu 
sorted according to dist wt wu }
1:	end if
1:	end while

모in algorithm 1  the number of nearest weight vectors to be considered when nearest neighbours are selected is denoted with k  with k 뫟 1 . the function dist calculates the distance between two weight vectors  and can be any distance function such as euclidean  manhattan  canberra  or cosine distance. in the first line of the algorithm  the output sets of classified match and non-match weight vectors  zm and zn  are initialised to the seed training example sets. next  in line 1  the set wt of all seed training examples and the set wu of all unclassified weight vectors are generated. an empty heap data structure  h  is then initialised next. a heap has the property that its first element is always the smallest element. it will be used in the second phase of the algorithm to iteratively get the next unclassified weight vector that has the smallest distance to the training sets. in line 1  three lists  m  n and u  are initialised that will be used to store nearest weight vectors as detailed below.
모lines 1 to 1 constitute the first phase of algorithm 1. in lines 1 and 1  for each weight vector in the match seed training set wm  the closest  k + 1  not classified weight vectors from wu are stored in the list m. the same is done in lines 1 and 1 for the weight vectors in the nonmatch seed training set wn  with nearest neighbours from wu stored in list n. these  k + 1  nearest neighbours  in m and n  are represented in figure 1 using dashed arrowed lines. in lines 1 and 1  for each unclassified weight vector in wu  the closest  k + 1  weight vectors from the overall seed training set wt are stored in the list u. these nearest neighbours  in u  are represented in figure 1 using black arrowed lines  with the nearest neighbour indicated using a bold black line . additionally  in lines 1 and 1  the sum of the distances  s  of the k closest training set neighbours for each wu in wu are calculated and inserted into the heap h. therefore  at the end of this first phase of the algorithm  the first element of h will be the weight vector from wu with the smallest distance sum to vectors from wt.
모phase two of algorithm 1  line 1 onwards  iterates until all weight vectors in wu are classified. in line 1  the first element in h  i.e. the weight vector with the smallest sum of distances to classified weight vectors  is taken from h and removed from wu in line 1. depending upon if the majority of neighbours of wt are matches or non-matches  it is added to the set of classified matches  zm  or classified non-matches  zn  respectively  lines 1 to 1 .
모in line 1  the set u wt  of nearest training set weight vectors of wt is used to create the set xu of nearest unclassified weight vectors. xu is retrieved via the corresponding nearest sets in the lists m and n. line 1 then loops over each of the unclassified weight vectors wu in xu that are nearest to the newly classified weight vector wt. in line 1  the distance d from wu to wt is calculated  and the list of nearest classified weight vectors for wu  u wu   is updated with this new distance d in line 1 if d is one of the  k + 1  smallest distances. in this case  the heap element for wu also needs to be updated in h with the newly calculated distance sum s  lines 1 and 1 . finally  depending upon if wt was classified as a match or a non-match  the set of nearest unclassified weight vectors for wt is updated in the corresponding list m or n in lines 1 to 1.
모this process is illustrated in figures 1  b  and  c . the middle upper  unclassified weight vector is classified as a match  because its nearest neighbour is a match. its list of nearest matches  solid lines  is used to get its nearest classified neighbours  the two seed matches in the top right   which in turn have lists of their nearest unclassified neighbours  dotted lines . the union of these lists becomes the new list of unclassified nearest neighbours  represented in figure 1  c  with the new dotted lines that point from the newly classified match to its two unclassified neighbours.
모na몮 ve nearest-neighbour based classification would involve calculating the distances between all pairs of weight vectors  and thus have complexity o |w|1 . this could be improved significantly by employing data reduction or fast searching and indexing techniques for nearest-neighbour classification   work that is left for future improvements.
모the seeded training sets also allow a reduction of distance calculations to be done through the use of the nearest lists m  n and u. in the first phase of algorithm 1  distances are only calculated between the weight vectors in the overall training example set wt and those in wu. if a fraction of t  t   1  of all weight vectors is included in wt  then the number of distance calculations in the first phase of the algorithm is |w|1뫄 t t1 . the maximum number of distance calculations will have to be done if half the weight vectors are in wt and half are in wu  i.e. t = 1: |w|1 뫄 1; while with t = 1  for example  the number of distance calculations to be done is only |w|1 뫄 1. the second phase of the algorithm involves calculating a maximum of  k + 1  뫄 k distances for each of the weight vectors in wu  as for each of the k nearest training set weight vectors the  k +1  nearest vectors will have to be checked for closeness.
1.1 iterative svm classification
모the iterative svm classifier is similar to the tc-won  approach for text and web page classification based on only positive labeled training examples. the basic idea is to train an initial svm using the seed training example sets wm and wn  and to then iteratively add the strongest positive and negative classified weight vectors from wu into the training sets of subsequent svms.
모algorithm 1 details the steps involved in this approach. the input parameter ip determines what percentage of unclassified weight vectors will be added into the training sets in each iteration  and tp determines the total percentage of weight vectors that will be added into the training sets. for example  if tp = 1% then all weight vectors from w will be used in the last iteration to train the final svm.
모the algorithm starts with initialising the training sets tm for matches and tn for non-matches  and by creating the set wu of all unclassified weight vectors. the initial svm svm1 is trained in line 1 using tm and tn. the main loop then starts in line 1 and iterates until tp percent of all weight vectors have been included into the training sets.
모each iteration starts in line 1 by classifying the weight vectors in wu using the previously trained svm svmi. the function svm classify returns two sets  xm and xn  that contain the weight vectors from wu classified as matches and non-matches  respectively. these classified weight vectors are sorted in line 1 according to how far away they are from the svm decision boundary  and in lines 1 and 1 the strongest positive and negative weight vectors are extracted into the sets ym of new matches  and yn of new non-matches. the size of these sets is determined by the increment percentage parameter ip. for example  if ip = 1%  then in each iteration half of the weight vectors in xm are inserted into ym and half of xn into yn. in lines 1 and 1  the new training examples in ym and yn are added to the training sets tm and tn  and a new svm svmi is trained next in line 1 using these expanded training sets. finally  in the last step within the iteration  in line 1  the new training examples from ym and yn are removed from the set wu of unclassified weight vectors.

algorithm 1: seeded iterative svm classification

input:
- set of weight vectors generated in comparison step: w
- seed training examples match set: wm
- seed training examples non-match set: wn
- increment percentage: ip - total training percentage: tp
output:
- weight vectors classified as matches: zm
- weight vectors classified as non-matches: zn
1:tm := wm and tn := wn1:wu := w    wm 뫋 wn 1:svm1 := train svm tm tn 1:i := 1:while  |tm| + |tn|     |w|   tp/1 :1:xm xn := svm classifiy svmi wu 1:sort xm and xn according to distance from svmi decision boundary  hyperplane 1:ym := |xm|    ip/1  vectors from xm furthest away from decision boundary1:yn := |xn|    ip/1  vectors from xn furthest away from decision boundary1:tm := tm 뫋 ym1:tn := tn 뫋 yn1:i := i + 1:svmi := train svm tm tn 1:wu := wu    ym 뫋 yn 1:end while1:xm xn := svm classifiy svmi wu 1:zm := tm 뫋 xm and zn := tn 뫋 xn
모the final svm is then used in line 1 to classify the weight vectors in wu that have not been classified so far  this step is not required if tp = 1%   and in line 1 the final two sets of matches and non-matches  zm and zn  are created. assuming that training a svm is of quadratic complexity in the number of training examples   then the overall complexity of algorithm 1 is o |w|   i   with i being the number of times a svm is trained. the value of i depends upon the ip and tp parameter values. for example  if ip = 1% and tp = 1% then i = log1 |wu|   while if ip = 1% and tp = 1% then i = 1  as in each step 1% of weight vectors from wu will be added to the training sets. training of svms will become increasingly time consuming as more weight vectors are added into the training sets.
1. experimental evaluation
모the two record pair classifiers presented above were evaluated and compared with two other classification methods. the first is a fully supervised svm that has access to the true match status of all weight vectors. nine parameter variations were evaluated: three kernel methods  linear  polynomial and rbf   and three values for the cost parameter  c   1  1  1 . the second method is based on the hybrid approach implemented in the tailor  toolbox. it first employs k-means  with one cluster each for matches  possible matches and non-matches   and then uses the match and non-match clusters to train a svm  in  a decision tree classifier has been used instead . two distance functions  manhattan and euclidean  were evaluated for the k-means step  while for the svm classifier step the same nine parameter variations as for the supervised svm were used.
table 1: data sets used in experiments. see section 1 for more details.
data setnumber oftaskpairsreductionnumber of weightratio r as in equation  1 recordscompletenessratiovectors  i.e. |w| and true ratio from datacensus1 + 1linkage1111 / 11 / 1restaurant1deduplication1111 / 11 / 1cora1deduplication1111 / 11 / 1ds-gen-a1deduplication1111 / 11 / 1ds-gen-b1deduplication1111 / 11 / 1ds-gen-c1deduplication1111 / 11 / 1ds-gen-d1deduplication1111 / 11 / 1
figure 1: weight vector histograms. only one is shown for the synthetic data sets as they all are very similar.모for the two-step classification approach  the imbalanced nearest based selection method  was used  with the number of seed training examples in wn selected as 1% or 1% of all weight vectors in w  respectively  and the number of training examples in wm calculated according to the ratio r as given in equation  1 . for the nearest-neighbour based two-step classifier  as described in section 1.1   again manhattan and euclidean distances were evaluated in combination with k set to 1 and 1. for the iterative svm based two-step classifier  described in section 1.1   the same nine parameter variations as for the supervised svm were evaluated  and the parameters ip and tp were set to the pairs  1   no iterative refinement    1    1   and  1 .
모the experiments for the supervised svm and tailor classifiers were conducted using 1-fold cross validation  1% used for training and 1% for testing   while this was not possible for the two-step classifier  because the selection of seed training examples requires all weight vectors in w.
모all classifiers are implemented in the febrl  record linkage system  which is written in python. the libsvm library was used for the svm classifier . all experiments were run on a 1 ghz dual-core cpu with 1 gbytes of main memory  running linux 1.1 and using python 1.1.
모experiments were conducted using both real and synthetic data  as summarised in table 1. three real data sets from the secondstring toolkit1 were used  while four synthetic data sets of various sizes were created using the febrl data set generator . this synthetic data contains name and address attributes that are based on real-world frequency tables  and includes 1% original and 1% duplicate records. the duplicates were randomly created through modification of record attributes  like inserting  deleting or substituting characters  and swapping  removing  inserting  splitting or merging words   again according to real-world error characteristics. up to nine duplicates were generated for one original record  with a maximum of three modifications per attribute and a maximum ten modifications per record.

1
http://secondstring.sourceforge.net
모standard blocking  was applied in all experiments to reduce the number of detailed record pair comparisons  with the blocking keys being combinations of name  address and postcode values. in the record pair comparison step  the winkler  approximate string comparator  commonly used in record linkage  was employed for name  address  paper title and conference name attribute  field  comparisons. additionally  character difference comparisons  were used on attributes such as postcode  street number  and publication year. figure 1 shows histograms based on the summed weight vectors that were generated in the comparison step. the imbalance between the number of matches and nonmatches can be seen clearly  especially for the 'restaurant' data set. the ratios of matches to non-matches  both calculated according to equation  1  and based on the data itself  are shown in the last column of table 1.
모the quality of the compared record pairs is shown in table 1 using the pairs completeness measure  which is the number of true matched record pairs generated by blocking divided by the total number of true matched record pairs   and the complexity of the record pair comparison step is shown using the reduction ratio measure  which is one minus the number of record pairs generated by blocking divided by the total possible number of record pairs   1  1 .
모due to the imbalanced distribution of matches and nonmatches in the weight vector set w  the accuracy measure commonly used for evaluating classifier performance is not suitable for assessing the quality of record pair classification : the large number of non-matches would dominate accuracy  and show results that are too optimistic. instead  the f-measure  f  the harmonic mean of precision  p  and recall  r  is used for measuring classifier quality:
f = 1 p 뫄 r / p + r   with p = tp/ tp + fp  and r = tp/ tp + fn . tp is the number of true positives  true matched record pairs classified as matches   fn the number of false negatives  true matched record pairs classified as non-matches   and fp the number of false positives  true non-matched record pairs classified as matches .
table 1: quality of nearest-based seed training example selection as described in section 1. each pair of values shows the quality of wm / wn as percentage of correctly selected training examples. the seed size gives the percentage of weight vectors from w selected into wn.
seed sizecensusrestaurantcorads-gen-ads-gen-bds-gen-cds-gen-d1%1%/1%1%/1%1%/1%1%/1%1%/1%1%/1%1%/1%1%1%/1%1%/1%1%/1%1%/1%1%/1%1%/1%1%/1%1%1%/1%1%/1%1%/1%1%/1%1%/1%1%/1%1%/1%모table 1 shows the quality of the seed training example sets generated in the first step of the proposed two-step classification approach  as described in section 1   given as the percentage of correctly selected weight vectors in these sets  i.e.  1 뫄 |true matches in wm|/|wm|  and 1 뫄 |true non-matches in wn|/|wn| . for the second step  figure 1 shows the average f-measure results  together with the minimum and maximum f-measure results over all parameter variations discussed above  i.e. 1 supervised svms  1 tailor classifiers  1 nearest-neighbour based two-step classifiers  and 1 iterative two-step svm classifiers - 1 each for the four variations of the  ip tp  parameter pairs . these average results  rather than the 'best' results using a certain parameter setting  are presented as they provide a more realistic picture of the overall performance of a classifier  which likely depends upon the characteristics of a data set .
1 results and discussion
모as can be seen in table 1  the seed training example sets selected in the first step of the proposed two-step classification approach are mostly of very high quality  with the match training example set wm only containing true matches in all but one case  1% seed size for the 'restaurant' data set . while overall the 1% training set selection contains the highest quality seed training data  the size of these sets  especially wm  is very small  and the resulting classifiers based on these 1% seeds generated in the second step were much worse in most experiments compared to the classifiers generated based on 1% or 1% seed sizes. therefore  the classifiers based on 1% seed size were not used further and are not included in the results presented.
모figure 1 shows the f-measure results for all data sets  with the results for the four synthetic data sets averaged  as they all had very similar f-measure results . as can be seen  there is a large variety of f-measure result values for certain classifiers  and very different f-measure result values for the same classifier on the different data sets.
모as expected  the supervised svm  which can be seen as an 'oracle' as it knows the true match status of all record pairs  performs best on all data sets. for the 'census' and 'restaurant' data sets  there are  however  parameter settings that lead to svm classifiers that perform worse than the best two-step classifier using the iterative svm approach. the tailor hybrid classifier approach has the lowest performance on most data sets. only on 'cora' did it achieve better results than most two-step classifier variations.
모the nearest-neighbour based two-step classifier performs better than all iterative svm variations for all synthetic data sets  while for the real data sets the iterative svm generally achieves better classification results. looking at the different values of the parameter pairs  ip tp   a noticeable improvement when including more weight vectors into the training sets is only visible for 'cora'  while the improvements are very small for the synthetic data sets  and mixed for the 'census' data. for the 'restaurant' data set  the results are getting worse when more training data is added.
모the results for the 'restaurant' data set are very low for all unsupervised classifiers compared to the supervised svm. one reason for this is that for this data set the weight vector set w only contains 1 matches  duplicates   but 1 non-matches  a ratio of 1 to 1   making it very difficult to extract true match examples. another reason is that the attributes in this data set contain addresses with a large proportion of either abbreviations or completely different values  such as 'los angeles' versus 'beverly hills'  for the same restaurant. therefore  the weight vectors generated when such attribute values were compared have a very overlapped distribution of matches and non-matches  as can be seen in figure 1   that are hard to classify without knowing the true match status of these weight vectors. this can also be seen in table 1  where with the 1% seed size the match training set wm already contains more than 1% non-matches.
모the experiments presented in this paper show that the proposed two-step classification approach can achieve results that outperform other unsupervised record pair classification techniques  such as the hybrid tailor approach which previously has shown to be better than k-means clustering . on the other hand  these experiments also showed the limitations of unsupervised classification based on only pair-wise record attribute similarities.
1. conclusions and future work
모this paper presented a novel unsupervised two-step approach to record pair classification that is aimed at automating the record linkage process. this approach combines automatic selection of seed training examples with training of a binary classifier. the two two-step classifiers discussed achieve improved record pair classification results compared to other unsupervised classifiers  such a the hybrid tailor  approach. thus  the proposed approach can be used in situations where no training data is available.
모future work will include to conduct more experiments using different data sets  including run-time tests on data sets of various sizes in order to experimentally get scalability results. related to this is the implementation of data reduction and fast searching and indexing techniques for the nearest-neighbour based classifier   and similar approaches for the iterative svm  with the aim to reduce training times while keeping a high record pair classification quality. another area of research will be to investigate active learning techniques  1  1  and combine them with the seeded training example selection approach presented here. active learning can  for example  be used in the iterative two-step svm classifier to select the hardest to classify examples and hand them to a user for manual classification  assuming additional information might be available .

figure 1: average f-measure results  shown with minimum and maximum values . '1s' stands for two-step classifier  and the numbers given with '1s-svm' refer to the parameters ip and tp as used in algorithm 1.1. acknowledgements
모this work is supported by an australian research council  arc  linkage grant lp1 and partially funded by the new south wales department of health  sydney.
