for the short  factoid questions in trec  the query terms we get from the original questions are either too brief or often do not contain most relevant information in the corpus. it will be very difficult to find the answer  especially exact answer  in a large text document collection because of the gap between the query space and the document space. in order to bridge this gap  there is a need to expand the original queries to include the terms in the document space. in this research  we investigate the integration of both the web and wordnet in performing local context and lexical correlations to bridge the gap. in order to minimize the noise introduced by the external resources  we explore detailed question classes  fine-grained named entities  and successive constraint relaxation. 
1. 	introduction 
we are participating in this year's question answering  qa  main task and it's our first time to take part in trec. question answering has recently received attention from many natural language processing communities . our goal is to retrieve the exact answers for the short  factoid questions in trec. in our system  several modules have been developed. they are question processing  external resources adoption  document retrieval  candidate sentence selection and exact answer extraction.  
during question parsing  the detailed question classes  answer types  original content query terms and nlp roles of the query terms are analyzed. we derive detailed question class ontology that corresponds to fine-grained named entities. this enables us to extract exact answer from the candidate sentences more accurately.  
the original query terms can be used as the basis to locate potential answer candidates in the corpus. however  one major problem of doing this is that the query terms do not have sufficient coverage to locate most answer candidates. this is known as the semantic gap between the query space and document space. in order to bridge this gap  we use the knowledge of both the web and lexical resources to expand the original query. we first use the original query to search the web for top n web documents and then extract terms that co-occur frequently in the local context of the n-gram query terms. we next use wordnet to find other terms in the retrieved documents that are lexically related to the expanded query terms. the new query therefore contains terms that are related to the local context in the web and the lexical context through wordnet. finally  we use the expanded query to search for answer candidates through the mg system . 
candidate answer sentences are selected from the top returned documents and are ranked based on certain criteria to maximize the answer recall and precision. nl analysis is performed on these candidate sentences to extract pos  base noun phrases  named entities  etc. answer selection is done by matching the expected answer type to the nl results. the nearest string with the expected answer type in the candidate sentence is returned as the final answer. figure 1 gives the overview of our system architecture 
question  
classification 
figure 1: overview of system architecture 
in this system  we focus on the techniques to expand the original query to locate most answer candidates. the resulting approach is efficient and has been found to be effective. our experiments on trec qa main task show good results when combining both local context and lexical information. 
1. 	question processing 
the purpose of our question processing is to find the specific nature of each question and to make full use of all the information in the question in order to find the best answer.  
1. question classification 
question classification in our system is based on question focus and answer type. a rule-based question classifier is developed to determine the question focus and question class. there are seven main question classes in our system. they are: hum  human   loc  location   tme  time   num  number   obj  object   des  description  and unknown  unknown . the last type unknown is used to group questions that cannot be categorized into the other classes. different types of the questions are treated slightly differently in the following answer extraction module.  
 example 1:   which city is the capital of canada     q-class: loc  
 example 1:   which province is the capital of canada in     q-class: loc  
obviously  both of the questions belong to the type of loc  location  and their content words are almost the same  i.e.  capital and canada. however  they are expecting different answers  which should fall in different categories  i.e.  city or state. the first question's answer will be ottawa but the second's answer should be ontario.  in order to detect the subtle differences in the questions  we further classify the first 1 major question classes into 1 sub classes  see table 1 below . under each main class  there is a special sub-class called xxx basic  which is designed for questions that fall in the major class but do not suit any of the sub-classes. our question classification is similar to the learning classifier developed by li and roth . currently  our rule-based classifier can reach an accuracy of over 1%. 
 q-class q-sub-class #trec1q #trec1q example hum hum person 1 1 who is the governor of colorado   hum org 1 1 what car company invented the edsel   hum basic 1 1 who is tom cruise married to   loc loc planet 1 1 which planet did the spacecraft magellan enable scientists to research extensively   loc city 1 1 what is the capital city of algeria   loc continent 1 1 what continent is scotland in   loc country 1 1 what country is berlin in   loc county 1 1 what county is elmira   ny in   loc state 1 1 which state has the longest coastline on the atlantic ocean   loc province 1 1 what province is calgary located in   loc town 1 1 the hindenburg disaster took place in 1 in which new jersey town   loc river 1 1 what river is called  china 's sorrow    loc lake 1 1 what is the deepest lake in the world   loc mountain 1 1 what is the name of the volcano that destroyed the ancient city of pompeii   loc ocean 1 1 what body of water does the colorado river flow into   loc island 1 1 what is the world 's second largest island   loc basic 1 1 where is devil 's tower   num num count 1 1 how many chromosomes does a human zygote have   num price 1 1 how much does it cost to register a car in new hampshire   num percent 1 1 what percent of the u.s . is african american   num distance 1 1 what is the height of the tallest redwood   num weight 1 1 what is the average weight of a yellow labrador   num degree 1 1 what is the boiling point of water   num age 1 1 how old was nolan ryan when he retired   num range 1 1 what is the range for the number of passengers a boeing 1 airplane can carry   num speed 1 1 how fast does a cheetah run   num frequency 1 1 how often does the united states government conduct an official population census   num size 1 1 what 's the capacity of the superdome   num area 1 1 how much area does the everglades cover   num basic 1 1 how much vitamin c should you take in a day   tme tme year 1 1 what year was alaska purchased   tme month 1 1 in what month are the most babies born   tme day 1 1 what day did neil armstrong land on the moon   tme basic 1 1 when was the telegraph invented   obj obj currency 1 1 what is the currency used in china   obj music 1 1 what was aaron copland 's most famous piece of music   obj animal 1 1 what is the state bird of alaska   obj plant 1 1 what is the major crop grown in arizona   obj breed 1 1 what breed was roy rogers ' horse trigger    obj color 1 1 what are the colors of the italian flag   obj religion 1 1 what is the chief religion for peru   obj war 1 1 what war is connected with the book   charge of the light brigade    obj language 1 1 what language do they speak in new caledonia   obj work 1 1 which long lewis carroll poem was turned into a musical on the london stage   obj profession 1 1 what was william shakespeare 's occupation before he began to write plays   obj entertain 1 1 what tv series did pierce brosnan play in   obj game 1 1 what card game uses only 1 cards   obj basic 1 1 what is the chemical formula for sulphur dioxide   des des abb 1 1 what does cpr stand for   des meaning 1 1 what does   e pluribus unum   mean   des manner 1 1 how did mahatma gandhi die   des reason 1 1 what are hiccups caused by   des basic 1 1 what do you call a baby sloth   table 1 : question classes 
1. question parsing 
besides question classes  some important information are also extracted when we parse the question. they are crucial for the later processes. detailed analysis is performed here in order to get as much useful information as possible. there are several kinds of word groups we extract from the original question. they are: 
 1  content words: these include nouns  adjectives  numbers  and some non-trivial verbs  which appear in the question string. part of speech tagging is performed before we select the content words. for example:  what mythical scottish 
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　 1  town appears for one day every 1 years     the content word vector will be q :  mythical  scottish  town  appears  one  day  1  years  
 1  basic noun phrases: we use noun phrase recognizer to identity all basic noun phrases appear in the question. for the above example  the noun phrase vector n :    mythical scottish town     
 1  head of the first noun phrase: it refers to the noun follows the question header  e.g. what  which  how  etc  and carries the main meaning of the question focus. it can be the next noun or the last word in the next noun phrase after the question header. for the above example h :  town . usually  there is only one such head for each question sentence.  
 1  quotation words: for some of the questions  quotations appear in the question string. they should be given special treatment. the string inside quotation marks usually is longer than a noun phrase  sometimes it could be a full sentence. for example    what broadway musical is the song   the story is me   from    the quotation word vector will be u :   the story is me   
1. 	query expansion 
after question processing  we need to locate the relevant documents and sentences from the trec corpus. the most common way is to apply information retrieval techniques to find the relevant documents and candidate answer sentences. for the short  factual questions in trec  the query terms we get from the original questions are either too brief or do not fully cover the terms used in the corpus.  
	 1 	 1 	 1 	 1 
given a short query  q	 =  q1	 q1	 ...qk	   usually with k =1   the problem for retrieving all the documents relevant to 
 1 
q is that the query does not contain most of the terms used in the document space to represent the same concept. thus there is thus a need to expand the original query to bridge the gap between the query space and document space.  
we use general open resources to overcome this problem. the external general resources that can be readily used include the web  wordnet  knowledge bases  and query logs. many groups working on qa have recently used the web  and wordnet  as resources for question answering. in our system  we integrate the external resources to expand the query. the new query is then used to look for the relevant documents and sentences in the qa text collection. 
1. using web as the generalized external resource 
the web is the most rapidly growing and complete knowledge resource in the world now. the terms in the relevant documents retrieved from the web are likely to be similar or even the same as those in the qa text collection since they are both news articles.  
original content words in the question are passed to the online search engine  e.g. google  to search for documents in the web. the terms in the relevant web documents that are highly correlated with the original query terms will be considered as candidates to expand the context of the original query. the steps are:  
a  get original query q 1  =  q1   q1  ...  qk  1    . 
 1 
b  for q	  retrieve the top n documents from the web. 
              1  （ q 1   extract wi  which contains non-trivial words in the same sentence or within p words away from qi 1   in c   qi the retrieved web documents. 
d  rank all wik （wi by computing its probability of co-occurrence with qi 1   as: 
 	 	 	pr wik   = d s wik …qi  1     	 	 	 	 	 	 1  
d s wik ‥qi  
where  ds wik/  qi 1   is the number of instances that wik and qi 1   appear together  and ds wik / qi 1   is the number of 
                                  1   appears. instances that either wik or qi
 1 
e  merge all wi to form cq for q . therefore  cq contains the list of words that are highly correlated with the original query from web documents.  
1. use wordnet as the generalized external resource 
the web can only provide us the words that occur frequently with the original query terms in the local context. it however  lacks information on lexical relationships between these terms. to overcome this problem  we look up wordnet to find words that are lexically related to the original query terms. the glosses  synonyms  and hypernyms are considered to be useful in relating words. in this work  we consider glosses and synonyms only to relate terms. for example  from the glosses  
 definition of plant: a living organism lacking the power of locomotion  
      definition of animal: a living organism characterized by voluntary movement the common concept here is living organism  which will link concept plant to concept animal.  
 1 
from wordnet  we can find gloss words gq and synset words sq for q . if we expand the query by appending all the terms in the glosses and synsets  it tends to be too general and contain too many terms out of context. in general  we need to restrict gq and sq to those terms found in the web documents  i.e.  those found in cq. thus we circumvent this problem by using gloss and synset relations to increase the weights of appropriate context terms wk（cq by: 
 if wk（ gq  increase wk by α 
	 if wk（ sq  increase wk by β    1 β α 1  	 	 	 	 	 	 	  1  
the final weight for each term in cq is normalized for ranking. the new query is formed as q 1  = q 1  + {top m terms from cq whose weights are below the selection threshold } 	 	 	 1  
currently  we plan to use the semantic perceptron net approach  to derive semantic groups in cq  gq and sq in order to derive a structured approach to utilize external knowledge. 
1. 	document & candidate answer sentence retrieval  
we use the mg tool  in our system to index the documents. we choose boolean retrieval because of the short queries 
                                                                                                1   to retrieve the top m documents  m = and the need to maximize precision. after performing boolean retrieval by using q
 1   does not return sufficient number of relevant documents  we reduce the extra terms added and repeat the 
1   if q
boolean search. therefore  we successively relax the constraints to ensure precision in document retrieval.  
the sentence is chosen as the basic unit for processing in our system. after performing sentence boundary detection  we use 
　　　　　　 1  the following criteria to rank the relevance of a sentence to the question:  recall from query processing  we extracted q	  n  h  u . for each sentence sentj  we match it with 
  quotation words:  wuj = % of term overlap between u and sentj 
  noun phrases:  wnj = % of phrase overlap between n and sentj  
  head of first noun phrase:  whj = 1 if there is a match and 1 otherwise 
  original content words: wcj = % of term overlap between q 1  and sentj  
  expanded content words: wej = % of term overlap between q 1  and sentj   where q 1  = q 1   -  q 1  
the final score for the sentence is    s j = ‘a i       w ij where ‘ai=1  wij（{ wuj   wnj   whj   wcj   wej }. the top k sentences are then selected as the candidate answer sentences based on i sj. 
1. 	answer extraction 
finally we perform the tagging of fine-grained named entities  on the top k sentences extracted from the previous steps. 
from these sentences  we extract the string that matches the question classes  answer target  as the answer. once an answer is found within the top ith sentence  the system will terminate the search for the rest of  k-i  sentences. when there is more than one matching strings in a single sentence  we will choose the string that is nearest to the original query terms. for example: for question  where did dr. king give his speech in washington     we get: 
  q-class: loc basic  
   loc basic washington  king-dream    loc basic washington    in the  num period 1 years  since dr .  hum person martin luther king  jr . delivered his `` i have a dream '' speech at the  loc basic lincoln memorial    how have economic and social conditions changed for  loc continent african  americans   
for question class loc basic  we look for all the sub categories under loc and we will get washington  washington  lincoln memorial and african as answer candidates. among them  lincoln memorial is the nearest  string to original content word speech  and hence is picked as the exact answer.  
for some questions  we cannot find any answer. our solution is to reduce the number of the expanded query terms and repeat the document/sentence retrieval and answer extraction process for up to m iterations  m=1 . if we still cannot find an exact answer  nil is returned as the answer. we call this method successive constraint relaxation  which helps to increase the recall while preserving precision. 
1. 	result analysis 
we answered 1 questions correctly with un-interpolated average precision of 1. figure 1 shows that our system works well for most of the easy questions  right side of the figure   and has reasonable performance for the difficult ones. 

figure 1: question difficulty distribution 

figure 1: answer accuracy of all the question types 
we also found that the accuracy of the exact answers differ for different type of questions  see figure 1 . for some question classes  like time  location and human  our system gives quite high performance. for description  number and object questions  we still need to find better techniques to improve the performance.  
another problem is that we have too many questions with nil answers. the precision for recognizing nil answer is low: 
1 / 1 = 1  although the recall for nil answer is satisfactory: 1 / 1 = 1. as a result  the overall system recall  consider both questions with non-nil and those with nil answer  is not satisfactory as compared to precision. this is because we use the boolean search to look for relevant trec documents. only the documents containing all the query terms are returned. this restriction might be too strict. 
1. 	future work  
we are currently refining our approach in several directions. first  we are refining our terms correlation by considering a combination of local context  global context and lexical correlations. second  we are working towards a template-based approach on answer selection that incorporates some of the current ideas on question profiling and answer proofing  etc. third  we will explore the structured use of external knowledge using the semantic perceptron net approach . our longer-term research plan includes interactive qa  and the handling of more difficult analysis and opinion question types. 
