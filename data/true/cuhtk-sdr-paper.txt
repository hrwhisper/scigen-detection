this paper presents work done at cambridge university on the trec-1 spoken document retrieval  sdr  track. the 1 hours of broadcast news audio was filtered using an automatic scheme for detecting commercials  and then transcribed using a 1-pass htk speech recogniser which ran at 1 times real time. the system gave an overall word error rate of 1% on the 1 hour scored subset of the corpus  the lowest in the track. our retrieval engine used an okapi scheme with traditional stopping and porter stemming  enhanced with part-of-speech weighting on query terms  a stemmer exceptions list  semantic 'poset' indexing  parallel collection frequency weighting  both parallel and traditional blind relevance feedback and document expansion using parallel blind relevance feedback. the final system gave an average precision of 1% on our transcriptions.
for the case where story boundaries are unknown  a similar retrieval system  without the document expansion  was run on a set of  stories  derived from windowing the transcriptions after removal of commercials. boundaries were forced at  commercial  or  music  changes and some recombination of temporally close stories was allowed after retrieval. when scoring duplicate story hits and commercials as irrelevant  this system gave an average precision of 1% on our transcriptions.
the paper also presents results for cross-recogniser experiments using our retrieval strategies on transcriptions from our own first pass output  at&t  cmu  1 nist-run bbn baselines  limsi and sheffield university  and the relationship between performance and transcription error rate is shown.
1. introduction
the trec-1 spoken document retrieval  sdr  track showed that successful retrieval of information where the original source of the documents is audio is possible for small collections  1  1 . the results showed that although retrieval performance degraded when recogniser performance worsened  the fall off was rather gentle and good retrieval can still be achieved on transcriptions with over 1% processed term error rate   corresponding to 1% word error rate  wer  . further work has shown that various extensions to our retrieval system can increase performance across the whole range of error rates  with an average precision  avep  of 1 obtained on reference transcriptions  1 on our own transcriptions  1% wer  and 1 on transcriptions from dera   1% wer  on the trec-1 task .
although by speech recognition standards  the 1 hourtest data for trec-1 represented a large task  the 1 stories and 1 queries providedonly a small collection to test retrieval systems. the conclusions which could be drawn about sdr were therefore limited and a larger collection was needed to confirm the results. the 1 hours of trec-1 data  with 1 stories and 1 queries  represents such a collection and the results presented in this paper show how our methods adapt to a larger task.
an additional feature of our trec-1 system is that no knowledge about story boundaries is used for recognition  and two retrieval runs are made for each set of transcriptions. for the first run  manual  story  boundaries have been added and commercials have been manually removed  story-known  whilst for the second  no such information was used and the retrieval system attempted to find relevant passages in the document collection  story-unknown . this led to added challenges in recognition as well as retrieval  with a pre-processing stage being added to remove some data automatically labelled as commercials before recognition began.
this paper firstly describes the trec-1 sdr tasks and the data used in both development and evaluation of our trec-1 sdr system. the commercial-detectionscheme and the speech recogniser are described in detail in sections 1 and 1 respectively  with the performance of all the sites participating in the crossrecogniser runs also given in the latter. the retrieval engine is then described in section 1  along with a detailed analysis of how the individual retrieval components interacted and affected the overall results. section 1 focuses on the development of the story-unknown system using concatenated trec-1 data and describes the final evaluation system  giving the results for the trec-1 task. cross-recogniser experiments are presented in section 1  where the influence of transcription quality on both the story-known and story-unknown tasks is investigated. finally  conclusions are offered in section 1.
1. description of trec-1 sdr tasks
the trec-1 sdr track contains two main tasks. the first  story-known  sk  sdr  is similar to the trec-1 sdr track  with audio from american broadcast radio and tv news programs provided along with a list of manually-generated story  or document  boundaries. natural language text queries  such
as  what natural disasters occurred in the
world in 1 causing at least 1 deaths  
are then provided and participating sites must submit a ranked list of potentially relevant documents after running a recognition and retrieval system on the audio data. real relevance assessments generated by humans are then used to evaluate the ranked list in terms of the standard ir measures of precision and recall. for trec-1  sites may also run their retrieval system on a  reference  transcription which uses manually-generated closed-caption data  and on other automatically generated transcriptions fromnist  baselines or fromother participatingsites  cross-recogniser .
the second trec-1 task assumes no knowledge of the story boundariesat both recognitionand retrievaltime  story-unknown case . the end points of the shows are given as the start time of the first  story  and end time of the last  story  but no other story information  including the location of commercial breaks within the show  can be used. retrieval then produces a ranked list of shows with time stamps  which are mapped in the scoring procedure to their corresponding story identifiers  ids . all but the first occurrence of each story is marked irrelevant  as are commercials  before the standard scoring procedure is applied.
for both tasks in trec-1  the recognition is an on-line task  i.e. for any given audio show  only data and information derived from before the day of broadcast can be used. therefore  unlike for trec-1  unsupervised adaptation on the test collection can only use data up to and including the current day. retrieval however is retrospective and can use any data up until the last day of the document collection  june 1th 1 . further details can be found in the trec-1 specification .
1. description of data
there are two main considerations when describing the data for sdr. firstly the audio data used for transcription  and secondly the query/relevance set used during retrieval. table 1 describes the main properties of the former  whilst table 1 describes the latter  for the development  trec-1  and evaluation  trec-1  data sets.1
trec-1  dev trec-1  eval nominal length of audio1 hours1 hours  su number of documents11  sk approx. number of words11 1  sk 
1 1  su average doc length1 words1 words  sk table 1: description of data used

trec-1  dev trec-1  eval number of queries1average length of query1 words1 wordsmean # rel docs per query1 docs1 docs  sk table 1: description of query and relevance sets used
1. automatic detection of commercials
to enable both the case of known and unknown story boundary sdr to be investigated  the recognition must be run on all of the 1 hours of audio without using any knowledge of the story boundaries. since a substantial portion of the data to be transcribed was known to be commercials and thus irrelevant to broadcast news queries  an automatic method of detecting and eliminating such commercials would potentially reduce the number of false matches  thereby increasing the precision of the overall system. removing commercials early on in processing would also reduce the amount of data that needed to be transcribed and hence speed up the overall recognition system. the first stage of our sdr system was thus a commercial detector designed to eliminate automatically some sections of audio thought to correspond to commercials  whilst retaining all the information-rich news stories.
1. development on trec-1
the commercial detector was based on finding segments of repeated audio using a direct audio search  described in    making the assumption that  usually  only commercials are repeated. experiments were performed on the 1 hours of trec1 sdr data from abc by searching for segments of similar audio within the data. the results from using 1 sliding window systems with length and skip to generate the initial segments are given in table 1 along with a system which uses the automatically generated wideband segments from our 1 hub-1 segmenter . since the segmentation and commercial detection processes interact  results after both stages are given.
segmentscut-offalone+segmentationgenerationnon-storystorynon-storystoryautomaticlow1%1%1%1%wb segsmedium1%1%1%1%high1%1%1%1%slidelow1%1%1%1%l=1smedium1%1%1%1%s=1shigh1%1%1%1%slide low1%1%1%1%l=1smedium1%1%1%1%s=1shigh1%1%1%1%table 1: proportion of story/non-story rejected by direct search on coded audio for trec-1 abc data
a low cut-off threshold on the shorter window-length system was chosen to maximise the rejection of commercials whilst keeping the rejection rate of genuine stories below 1%. the effect of relabelling segments shorter than a certain smoothing
length    which appeared between two segments labelled as commercials was investigated  with the results given in table 1.
this shows that smoothing for up to a minute between detected commercial segments increases the performanceof the commercial rejection system.
 s non-story rejectionstory rejection1  none 1%1%1.1%1%1.1%1%1.1%1%table 1: effects of smoothing on trec-1 abc data
these general principles were used in the design of the trec-1 system  but some changes and additions were made to reflect the different nature of the trec-1 story unknown task: for example  only data broadcast before the current day can be used to identify commercials in trec-1.
1. the trec-1 system
in a more realistic scenario  the user is not likely to be interested in retrieving information which has been re-broadcast   i.e. repeats  whether it be a commercial or a news story. however  the trec-1 evaluation set-up meant it was better to retain segments containing news content even if they were repeats  whilst eliminating those repeated segments which correspond to commercials. safeguards were therefore added to try to reduce the probability of any matching audio which was not a commercial being falsely rejected during the commercial detection stage.

figure 1: the commercial detection process
a block diagram of the commercial detection process used for the trec-1 evaluation is given in figure 1. audio of the current show was analysed into 1 second windows with a window shift of 1s. each window was characterised by the covariance matrix of the  wideband  plp cepstral coefficients as used in the subsequent speech recognition passes. a broadcast history was built up which consisted of the windows for a certain amount of broadcast data  typically 1 hours  from that broadcaster  running up to a few days before the date of the current show. the delay was introduced to reduce the probability of an actual news story occurring in the broadcast history being directly re-broadcast in the current show. the broadcast history was initialised using the january 1 tdt-1 data and rolled through the trec-1 sdr evaluation data as the data was processed.
each segment in the current show was then compared to the segments in the broadcast history. if the arithmetic harmonic sphericity distance  between the covariance matrices of the segments was less than a threshold  then the pair was marked as  matching . note that a non-zero threshold was necessary  even when looking for identical audio  since there is no guarantee that the sampling and window shifts in each case are synchronous with the audio event in question.
for a segment to be marked as a true repeat  the number of matches between the segment and the broadcast history had to be above a given threshold  to reduce the number of false alarms due to similar  but not identical audio  for example for segments which overlappedby say 1%  matching erroneously. the probability of a re-broadcast story being labelled as a repeat was further reduced by defining the number of different days in the broadcast history which must be involved in the match before the segment was accepted as a repeat.
the merging process was then applied which relabelled as intermediates any small gaps which occurred between two segments already labelled as repeats. the intermediates were then relabelled as commercials  only if the resulting smoothed  commercial  was less than a critical length  the repeats always being relabelled as commercials. for the cnn shows a show  grammar   constructed from the cnn trec-1 data  was used to constrain the locations in the audio that could be labelled as commercials. due to the limited time resolution of the commercial labelling process  conservative start and end points were also used. 1. results for the trec-1 system
since the audio was eliminated at an early stage and could not be recovered later during processing  a very conservative system  comm-eval  which removed 1% of the audio  was used for the evaluation. a contrast run  comm-1  which removed 1% of the audio  was later made to see the effect of relaxing the tight constraints on the system. the breakdown of data removed using these systems compared to the manually generated story labels is given in table 1. note that these  reference  labels are not an exact reflection of the story/commercial distinction  since a few commercials have been wrongly labelled as stories and some portions of genuine news have not had story labels added and hence are erroneously scored as commercials; however they offer a reasonable indicator of the performance of the commercial detector within the context of this evaluation.
the results show that automatic commercial elimination can be performed very successfully for abc news shows. more false rejection of stories occurs with cnn data  due to the frequency of short stories  such as sports reports  occurring between commercials. the amount of commercial rejection with the voa data is low  due mainly to the absence of any voa broadcast history from before the test data. however  overall the scheme worked well  since 1% of the 1 hours of data removed by the comm-eval system  and 1% of the 1 hours removed by the contrast comm-1 run  were labelled as non-story in the reference.
broad.non-storiesstoriestotalcnn1hr=1%1s=1%1hrs=1%commabc1hr=1%1s=1%1hrs=1%evalpri1hr=1%1s=1%1hrs= 1%voa1hr= 1%1s=1%1hrs= 1%all1hrs=1%1hrs=1%1hrs=1%cnn1hr=1%1s=1%1hrs=1%commabc1hr=1%1s=1%1hrs=1%- 1pri1hr =1%1s=1%1hrs= 1%voa1hr= 1%1s=1%1hrs= 1%all1hrs=1%1hrs=1%1hrs=1%table 1: amountof data rejected during commercialelimination
1. the trec-1 htk broadcast news transcription system
after the commercial detection and elimination  the data is automatically segmented and classified by bandwidth and gender. the segmenter initially classifies the data as either wideband  wb  speech  narrowband  nb  speech or pure music/noise  which is discarded. the labelling process uses gaussian mixture models and incorporatesmllr adaptation. a gender-dependent phone recogniser is then run on the data and the smoothed gender change points and silence points are used in the final segmentation. putative segments are clustered and successive segments in the same cluster are merged  subject to the segment length remaining between 1 and 1 seconds . the trec-1 segmenter  which ran in approximately 1x real time  included a revised mixture model for music and applied new insertion penalties  but is essentially similar to the system described in  with the modifications for faster operation from .
since silence  music and noise are discarded during segmentation  it is interesting to note the interaction between this stage and the commercial elimination phase. the results  given in table 1  show that the proportion of data discarded by the segmenter decreases from 1% to 1% if applied after the commercial elimination stage.
before seg.after seg.original
commercial elim1
111table 1: number of hours of audio retained during processing
the main transcription system used a continuous mixture density  tied-state cross-word context-dependenthmm system based on the cuhtk-entropic 1 hub1xrt system . the speech was coded into 1 static cepstral coefficients  including c1  and their first and second derivatives. cepstral mean normalisation was applied over each segment. after commercial detection and segmentation  a 1-pass recognition system was applied. the initial transcription pass through the data  denoted cuhtk-p1  used gender-independent  bandwidth-specific triphone models  with a 1 word 1-gram language model to produce a single best hypothesis. the gender of each segment was then labelled by choosing the most likely alignment of this transcriptionusing male and female hmms. top-downcovariancebased clustering  was then applied on a gender and bandwidth specific basis to all the segments broadcast on a given day and mllr transforms were generated for these clusters using the first pass transcriptions.
the second pass used the mllr-adapted gender-dependent triphone models with a 1 word 1-gram mixture language model to generate lattices from which a one-best output was generated using a 1-gram model. this transcription  denoted cuhtk-s1u  was used for the story-unknown retrieval experiments  whilst the story-known transcription  cuhtk-s1  was simply generated by filtering this output using the known story boundaries. the overall system gave a word error rate of 1% on the november 1 hub1 evaluation data and 1% on the 1-hour scored subset of the trec-1 evaluation data and runs in about 1xrt on a single processor of a dual processor pentium iii 1mhz running linux.
the hmms were trained using 1 hours of broadcast news audio running up to 1st january 1  supplied by the ldc and used for the 1 hub-1 task. the gender-independent wideband models were generated initially  then narrowband models were created by single pass retraining using a band-limited  1hz to 1hz  analysis. gender-specific models were generated using a single training iteration to update the mean and mixture weight parameters.
three fixed backoff word-based language models were trained  from broadcast news text  newspaper texts and acoustic transcriptions  which were all generated using data from before 1st january 1. the first model was built using 1 million words of broadcast news text  covering 1  supplied by the ldc   nov. 1 to jan. 1  from the primary source media broadcast news collection  and jan. 1  from the tdt-1 corpus transcriptions . the ldc also supplied the 1m words from the washington post and los angeles times covering 1 to jan. 1  which were used for the newspaper texts model. the third model was built using 1m words from the 1 and 1 acoustic training transcriptions and 1 marketplace transcriptions. single merged word based models were created which resulted in effectively interpolating the three models  forming a single resultant language model. the final 1k language model had 1m bigrams  1m trigrams and 1m 1-grams  whilst the 1k model had 1m  1m and 1m respectively.
1. wer results from cross-recogniser runs
as well as our own transcriptions  cuhtk-s1  we used several alternative sets to assess the effect of error rate on retrieval performance. these came frommanually generated closed-captions  both unprocessed  cc-unproc  and with some standard text processing of numbers  dates  money amounts and abbreviations  cc-proc ; two baselines produced by nist using the bbn rough'n'ready transcription system   nist-b1 and nist-b1   including a fixed and dynamically updated language model respectively; transcriptionsfrom recognisersfromlimsi sheffield university  at&t  and carnegie mellon university  cmu ; and the output of the first pass of our system  cuhtk-p1 .
a 1-hour subset of the trec-1  story-known  evaluation data was taken and detailed transcriptions made by the ldc for scoring the recognisers. the results are given in table 1.
recognisercorr.sub.del.ins.errcc-proc11111cc-unproc11111cuhtk-s1.1.1.1.1.1limsi11111cuhtk-p1.1.1.1.1.1nist-b1.1.1.1.1.1nist-b1.1.1.1.1.1at&t11111sheffield11111cmu11111table 1: wer on 1 hour subset of trec-1 evaluation data
the results show that the cuhtk-s1 automatic transcriptions are very good  suggesting that the error rate  though some distance from that for the manually-generated closed caption transcriptions  is still low enough not to degrade retrieval performance substantially. it is pleasing to note that the relatively simple cuhtk-p1 system  which uses a smaller vocabulary  has no adaptation and runs in around 1 times real time  gives a reasonably low word error rate.
1. retrieval system
the basic system we used for sk retrieval in trec-1 is similar to that presented at trec-1   but the final system also contains several new devices. these include semantic poset indexing  spi  and blind relevance feedback for query expansion  both on the test collection itself  brf  and a parallel corpus  pbrf   all of which have been shown to increase performance on the trec-1 task  1  1 . a new technique called parallel collection frequency weighting  pcfw  is also presented along with an implementation of document expansion using the parallel corpus within the framework of the probabilistic model.
1. system description
1.1. preprocessing
a term is a set of words or word sequences from queries or documents which are considered to be a unique semantic unit. we call the first set of operations which define the relationship between terms and their components preprocessing. the following preprocessing techniques are sequentially applied on all transcriptions and queries before indexing and retrieval.
the words are first made lower case and some punctuation characters are removed. hyphens and digital numbers were kept even though they do not occur in the asr-transcribed documents.1 some sequences of words are then mapped to create single compound words. and some single-word mappings are also

applied to deal with known stemming exceptions and alternative  possibly incorrect  spellings in the manual transcriptions. the list of compound words and mappings was created manually for our trec-1 sdr system . a set of non-content  stop  words was removed from all documents and queries  with an additional set also being removed from just the queries  e.g. find documents .. . abbreviations   in several forms 
are mapped into single words  e.g.  c. n. n. -  cnn .
the use of porter's well-established stemming algorithm  allows several forms of a word to be considered as a unique term  e.g. = train  training  trainer  trains  ... . unlike the mapping techniques  this algorithm is not limited by the use of a fixed thesaurus and therefore every new word in a test collection can be associated with its various forms.
1.1. indexing
the index  inverted  file contains all the information about a given collection of documents that is needed to compute the document-query scores. for the collection  each term in the term-vocabulary has an associated:
collection number : the number of documentswhich at least one of the components of occurs in.
list of term frequencies   which is the number of occurrences of all of the components of in document .
the index file also contains the number of documents in the collection    and the length of each document .
semantic poset indexing  spi   is used to allow and to take into account some semantic relationships between terms. more specifically  semantic poset structures based on unambiguous noun hyponyms from wordnet  and a manuallybuilt geographiclocations tree were made. a term occurring in a poset is then redefined as the union of itself and all more specific terms in the poset associated with that term  before the statistics are calculated. for example  the term frequency for a term thus becomes the sum of the frequencies of occurrence of itself and all more specific related terms within a given document.
1.1. retrieval
a part-of-speech  pos  tagger is run over the queries and the weight of each query term is scaled by a factor using the pos weighting scheme from our trec-1 system . the score for a document with respect to a given query is then obtained by summing the combined weights  cw   for each query term according to the following formulae: cw 

where	is the term vocabulary for the whole document collec-
tion	; and	and	are tuning constants
1.1. blind relevance feedback  brf 
when the documents in the collection are ranked according to a given query  it is possible to expand the query by adding several terms which occur frequently within the top documents but rarely within the whole collection. the terms which obtain the highest offer weight are added to the query. the offer weight of a term is :

where is the number of top documents which are assumed to be relevant; the number of assumed relevant documents in which at least one component of occurs; the total number of documents in which at least one component of occurs; and is the total number of documents in the collection.
1.1. document parallel blind relevance feedback  dpbrf 
the method of document expansion described within the vector model in  at trec-1  can also be used within the probabilistic framework. by considering a document as a pseudo-query  it is possible to expand that document using brf on a parallel collection. for a given document  the 1 terms with the lowest are used as the pseudo-query. brf is then applied on the parallel collection  with   and the top 1 terms are added to the original document with a term frequency based on their offer weight.
1.1. parallel collection frequency weighting  pcfw 
if the test collection is small or contains many transcription errors  the values of may not be sufficiently reliable to use in the prediction of relevance. it is possible to exploit the larger  higher quality parallel collection to obtain better estimates for
      and    to use within the combined weights formula. the collection number    for a given term is therefore replaced by the sum of the collection number for that term on the test corpus and the parallel corpus; with the number of documents    being adjusted accordingly.
1.1. the final system
the index file was made as follows:
1. preprocess & apply spi to the test collection to give
1. preprocess & apply spi to parallel collection to give
1. perform dpbrf using the pseudo queries from the testcollection documents on and add the new terms into the index file .
1. replace the collection frequency weights in with the pcfws derived from and and update accordingly.
the query file was produced by:
1. preprocess the original natural language request file andattach a pos weight  posw  to each query term.
1. perform pbrf using	and add the new terms to the query.
1. perform brf on	and add the new terms to the query.
1.1. the parallel collection
the parallel collection used in dpbrf  pbrf and pcfw is composedof stories extracted fromthe l.a. times  washington post and new york times over the period of jan 1st to june 1th 1. this contains the trec-1 sdr test collection period  feb 1st to june 1th 1 .
1. experiments on trec-1 sk sdr
the avep results for our final system on all the sets of transcriptions made available is given in table 1 in section 1. here we concentrate on the effect on performance of each aspect of the system on our own cuhtk-s1 transcriptions.
1.1. results on the cuhtk-s1 transcriptions
it is important to try to understand the contribution of each individual device towards the overall performance of the ir system. table 1 gives the values of avep we obtain by progressively decomposing the system.
lines 1 and 1 show that the addition of all these devices together led to a relative increase in avep of . lines 1 show that adding just pbrf or brf individually improve the performance over a system with no blind relevance feedback  but applying pbrf alone gives better results than their combination.
lines 1 show that the improvement due to pcfw is reduced by the use of pbrf. brf degrades the performance even more when pcfw is present. a similar behaviour can be observed on lines 1 for posw  namely that adding posw increases performance on the basic system  but degrades when all the other devices are also included. however  this is not the case for dpbrf  as lines 1 show that including dpbrf when all other devices are present increases avep by 1% relative.
spi exhibits a rather different behaviour. it has no significant effect on the baseline system  see lines 1   but since the parallel corpus was indexed with spi  all the devices apart from posw were affected by the use of this technique. lines 1 and 1 show that avep reached when spi was not used and thus spi actually degraded the performance by relative. by comparing lines 1 and 1  we can see that the poor contribution of brf was due to the inclusion of spi.
in summary  the inclusion of the techniques discussed increased avep by 1% relative. some interaction between the devises was found and it was noted that an avep of 1% could be achieved if spi had not been included. the corresponding avep on the processed closed-caption data was 1%.
1. the story-unknown  su  system
for the su evaluation  no knowledge of the manually-labelled story boundaries can be used either in retrieval or recognition. the system must present a ranked list of show:time stamps  which are mapped to the corresponding story  or commercial  ids before retrieval performance evaluation  with commercials and duplicates scored as irrelevant.
spidpbrfpcfwposwpbrfbrfavepp 1------111yyyyyy111yyyy--111yyyyy-111yyyy-y111yy-y--111yyyy--111yy-yy-111yyyyy-111yy-yyy111yyyyyy111------111---y--111yyy-yy111yyyyyy111y-yyyy111yyyyyy111------111y-----111-yyyyy111yyyyyy111-yyyy-11table 1: breakdown of results on the cuhtk-s1 transcriptions showing different combinations of the retrieval techniques
two main approaches to the su task exist  the first consists of labelling story boundaries automatically and then running the standard retrieval engine; whilst the second never explicitly finds the story boundaries  but rather locates the relevant passages in the transcriptions and performs some merging of temporally close relevant passages to reduce the possibility of producing multiple hits from the same story source. we investigated one technique from each approach  namely hearst's texttiling  for topic boundary detection and a windowing/ recombination system.
for development  the 1 hours of trec-1 sdr test data was used. this did not exactly model the trec-1 su task  since the commercials had already manually been removed from the data  but offered a reasonable basis to compare the different systems. two methods of scoring were used  the first is the official evaluation scoring procedure  where all instances of a story other than the first one are scored as irrelevant  named dup-irrel . the second  by removing all duplicates before scoring  was more lenient and provided an indication of the  best  performance that could be achieved if a perfect merging system  that removed duplicates  but did not re-score or re-order the ranked list  were added after retrieval. this was named dup-del and represents a reasonable indication of the potential of any given system.
a simple experimentwas conductedto comparea text-tiling system with a windowing system. text-tiling was originally designed to group paragraphs in long textual reports together and thereforeis not ideally suited to the su-sdr task  since the transcriptions contain no case  sentence or paragraph information.  pseudo  paragraphs of 1s of speech were made for each show and the default text-tiling parameters  were used along with some additional abbreviations processing  to obtain the  tile  boundaries. our standard retriever  similar to our trec-1 system   was then used to produce the final ranked list. the windowing system made pseudo-stories of a given length and skip before running the retriever as before. the results are given in table 1. the windowing system seemed to offer greatest potential and hence the basis of the su system was chosen to be a sliding window of length 1 seconds and skip 1 seconds.
systemdup-irreldup-del# stories trec-1 story-known111text-tiling111windowing - 1s 1s111windowing - 1s 1s111windowing -	1s 1s111windowing -	1s 1s111table 1: avep for simple su systems on the trec-1 data
the standard retrieval engine was then replaced by a more complicated system  similar to the one described in   and forcedbreaks were added during the windowing to prevent windows being formed over gaps of more than 1 seconds in the audio. any very short windows   1 seconds or 1 words  were removed at this stage. the results are given in table 1. the increase in performance due to a more sophisticated retrieval engine  which includes spi and relevance feedback  is clearly shown. forcing breaksat gaps in the audio did not havemuch effect on the trec-1 data  which contained no commercials   but it was hoped that these gaps  generally formed by music/silence removal in the segmentation  or commercial elimination for the trec-1 system  would offer a good indication of story boundary for the trec-1 data  and hence should be enforced as hard breaks.
systemdup-irreldup-delbaseline from table 1.1.1improved retriever11improved retriever + forced-breaks11table 1: su avep improvements on the trec-1 data
post-processing the retrieval output in order to prevent multiple hits of the same story was then examined. smoothing was added such that for any given query  any stories which were returned as relevant and originated from within a certain time    in the same broadcast were pooled  with only the highest scoring window being retained. the others were placed in order at the bottom of the ranked list. the results for different values of are given in table 1.
the results show that the best performance using the trec-1 evaluation measure  dup-irrel  for the trec-1 data is obtained with a smoothing time of 1s. this is surprisingly high  but it was thought that the probability that two temporally close windows both being retrieved for a given query but not being
111dup-irrel11111dup-del1111111dup-irrel11
11dup-del1111table 1: avep for different merge times for post-processing on the trec-1 data
from the same story was quite low. since the trec-1 collection contained more data and had a greater proportion of cnn broadcasts  which generally produce shorter stories  the parameter was set to the sub-optimal  but shorter 1s for the trec-1 evaluation.
attempts were made to modify the score from the retriever of any window which represented a merged group of windows  before re-ordering during the post-processing phase  but this proved not to be beneficial for the trec-1 data. finally hard breaks  as defined by a certain length gap in the audio  were also enforced in the post-processing phase  so that no merging could take place over such a break. the results are given in table 1 for a of 1 seconds and 1s.
audio gap=1s=1sfor boundarydup-irreldup-deldup-irreldup-del1s or11111s11111s
1s1
11
111
11table 1: effect of enforcing hard boundariesin post-processing on trec-1 data
no real benefit is shown for the trec-1 data when the smoothing is relatively conservative  but for the case of =1s  when the smoothing time is greater than optimal value  the enforcement of boundaries for audio gaps of 1s does increase performance slightly. since the problem of over-smoothing was thought to be greater for trec-1 as the commercials had not been manually removed  the enforcement of boundaries at 1s gaps in the audio was maintained.
the final system  summarised in figure 1  gavean avep of 1  r-prec=1  on our own transcriptions on the trec-1 task. a more detailed analysis of the su results for trec-1 can be found in .
1. cross-recogniser experiments
several sets of transcriptions from other participating sites were offered to allow comparisons to be made between retrieval using different recognition systems. the detailed breakdown of the word error rate of these transcriptions is given in table 1 in section 1. the avep for both the sk and su runs  along with
eliminate commercials eliminate music and silence
generate 1s windows   1s enforce boundaries at gaps  1s truncate shorter windows remove short/few-word windows
as for sk system with no document expansion
enforce boundaries at gaps   1s take only first for windows within 1s output top 1 per query
figure 1: the trec-1 su system
the term error rate  after stopping and stemming  sster  and word error rate  wer  is given in table 1. the avep for a benchmark system with no relevance feedback  document expansion or parallel collection frequency weights  base  is given as a comparison. 1
the term error rate after document expansion  deter  is also given in table 1 as a comparison. to calculate this measure  pre-processing  poset mapping and then document expansion are performed on both the reference and hypothesis transcriptions before the standard term error rate is calculated.1
error rate on 1hr subsetaverage precisionrecogniserwerssterdeterskbasesucc-proc11111--cc-unproc11111--cuhtk-s1.1.1.1.1.1.1limsi111111cuhtk-p1.1.1.1.1.1.1nist-b1.1.1.1.1.1.1nist-b1.1.1.1.1.1.1at&t11111--sheffield111111cmu11111--table 1: avep for sk and su cross-recogniser evaluation conditions with corresponding transcription error rates
figure 1 shows the relationship between stopped-stemmed term error rates  sster  and avep. whilst the benchmark  base  performancecan be predicted reasonablywell fromsster  there is more  seemingly unpredictable  variation for the case of the complete sk system. in particular  the avep for the nist-b1


figure 1: relationship between avep and sster
and cc-unproc runs is much worse than that predicted by the sster. however  the deter for both these cases is unusually high  suggesting the problem for these runs lay in the document expansion process.1
it is interesting to note that the best-fit lines for both the complete sk system and the benchmark sk cases are almost parallel    gradients -1 and -1 respectively   showing that the inclusion of relevance feedback for query and document expansion and parallel collection frequency weights improves the overall avep by around1% absolute across the complete range of transcription error rates.
the su results follow a roughly similar pattern  suggesting that generally transcriptions which work well for the sk case also work well for the su case. it is pleasing to note that the output from the first pass of our system  cuhtk-p1  does better than might be predicted from its error rate. this is due in part to the reduction in false alarms because of the elimination of commercials in the system. this is confirmed by the results given in table 1  which show that the avep on cuhtk-p1 transcriptions would have fallen by 1% if the commercial detector had not been used  whereas the performance on limsis transcriptions increases by over 1% when the detected commercials are filtered out during the post-processing stage  see  for more details .
runno commercialscomm-evalremovedremovedcuhtk-p1.1%1%limsi1%1%table 1: effect on avep for the su case when automatic commercial detection is included
1. new ters to predict performance
term error rates were introduced in  to model the input to the retriever more accurately than the traditional word error rate.

if knowledge about the retrieval process itself is known in advance  then the ter can be modified to exploit this information to model the retrieval process more closely and therefore hopefully provide a better predictor of the performance of the final system. an example of this is using sster  where the stopping  mapping and stemming processes used in the first stage of indexing the transcriptions  is incorporated into the error rate calculation.
if more information is known about how the scores are generated within the retriever for a given term  then new ters can be defined which incorporate this information. the generic ter function thus becomes:

where is some function which generally depends on the word   is the reference and the hypothesis. this can be seen to reduce to the standard ter when is the identity function. some other possibilities for the function which allow the collection frequency weighting  inverse document frequency  1 or the combined weights formula to be included directly are:
		 1 
ndl
where       and have the same meaning as in section 1.1. it is also possible to include the frequency of each term in the query as a scale factor within if the queries are known  but this makes the score query-dependent which may be undesirable  and care must be taken in defining the query terms if relevance feedback is used for query expansion.
the ters using  1   including stopping  stemming  mapping  posets  document expansion and parallel collection frequency weights within the combined weighting formula are given in table 1. unfortunately these numbers do not appear to offer a better predictor for our avep results. this may be because the words added to the  reference  during document expansion may not be the best in terms of retrieval performance  or that only the query terms themselves should be taken into account  or simply the overall performance on the entire 1 hour collection cannot be predicted well using the scored 1 hour subset.
rec.htkcc-prochtk-p1limsinist-b1error11111rec.sheffat&tcc-unprocnist-b1cmuerror11111table 1: term error rate modelling stopping  stemming  mapping  posets  document expansion and pcfw with combined weighting on the scored 1 hour subset

1. conclusions
this paper has described the systems developed at cambridge university for the 1 trec-1 sdr story known and story unknown evaluations.
a new method of automatically detecting commercials has been shown to work well  with 1% of the 1 hours of data automatically labelled as commercials being marked as non-story informationby humans. by automatically eliminating these  commercials  at an early stage  the computational effort required during speech recognition was reduced by 1% and the average precision for the story unknown task was increased by 1% relative.
two htk-based transcription systems were made. the first ran in 1 times real time and gave a word error rate  wer  of 1% on the scored 1 hour subset of the data. the second ran at 1 times real time and included a second pass with a 1k vocabulary and speaker adaptation  giving a wer of 1%  the lowest in the track by a statistically significant margin.
severalextensionsto ourretrieverhave been described and shown to increase average precision on our best transcriptions for the story-knowncase by 1% relative  giving a final valueof 1%. these included semantic poset indexing  blind relevance feedback  parallel blind relevance feedback for both query and document expansion and parallel collection frequency weighting.
the system developed for the case where story boundaries were not known included automatic detection and elimination of commercials  windowing using the segmentation information  retrieval using all the strategies developed for the story-known case except document expansion  and post-filtering to recombine multiple hits from the same story. the final system gave an average precision of 1% on both sets of our transcriptions.
finally  experiments were described using other transcriptions and the relationship between transcription error rate and performance was investigated. the results from trec-1 showing that the degradation of performance with increasing error rate was fairly gentle were confirmed on this significantly larger data set.
acknowledgements
this work is in part funded by an epsrc grant reference gr/l1. thanks to tony robinson for the initial idea that repetitions of audio could help to indicate the presence of commercials.
