outlier detection has recently become an important problem in many industrial and financial applications. in this paper  a novel feature bagging approach for detecting outliers in very large  high dimensional and noisy databases is proposed. it combines results from multiple outlier detection algorithms that are applied using different set of features. every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set. as a result  each outlier detector identifies different outliers  and thus assigns to all data records outlier scores that correspond to their probability of being outliers. the outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers. experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm. 
categories and subject descriptors 
h.1  database management : database applications  data mining  scientific databases  spatial databases  
general terms 
algorithms  performance  design  experimentation. 
keywords 
outlier detection  bagging  feature subsets  integration  detection rate  false alarm. 
1. introduction 
the explosion of very large databases and the world wide web has created extraordinary opportunities for monitoring  analyzing and predicting global economical  geographical  demographic  medical  political and other processes in the world. however  despite the enormous amount of data being available  particular events of interests are still quite rare.  these rare events  very often called outliers or anomalies  are defined as events that occur very infrequently  their frequency ranges from 1% to less than 1% depending on the application . detection of outliers  rare events  has recently gained a lot of attention in many domains  ranging from detecting fraudulent transactions and intrusion detection to direct marketing  and medical diagnostics. for example  in the network intrusion detection domain  the number of cyber attacks on the network is typically a very small fraction of the total network traffic. in medical databases  when classifying the pixels in mammogram images as cancerous or not  abnormal  cancerous  pixels represent only a very small fraction of the entire image. among all users that visit an e-commerce web site  those that actually purchase are quite rare - for example less than 1% of all people who visit amazon.com's website make a purchase  and this is much higher than the industry average. although outliers  rare events  are by definition infrequent  in each of these examples  their importance is quite high compared to other events  making their detection extremely important. 
the problem of detecting outliers  rare events  has been variously called in different research communities: novelty detection   chance discovery   outlier/anomaly detection  1  1  1  1  1  1   exception mining   mining rare classes  1  1   etc. data mining techniques that have been developed for this problem are based on both supervised and unsupervised learning. supervised learning methods typically build a prediction model for rare events based on labeled data  the training set   and use it to classify each event  1  1  1 . the major drawbacks of supervised data mining techniques include  1  necessity to have labeled data  which can be extremely time consuming for real life applications  and  1  inability to detect new types of rare events. on the other hand  unsupervised learning methods typically do not require labeled data and detect outliers  rare events  as data points that are very different from the normal  majority  data based on some measure . these methods are typically called outlier/anomaly detection techniques  and their success depends on the choice of similarity measures  feature selection and weighting  etc. outlier detection algorithms can detect new types of rare events as deviations from normal behavior  but on the other hand suffer from a possible high rate of false positives  primarily because previously 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd'1  august 1  1  chicago  illinois  usa 
copyright 1 acm 1-1-x/1...$1. 
unseen  yet normal  data are also recognized as outliers/anomalies  and hence flagged as interesting. in this paper  we focus on unsupervised methods for outlier detection. 
many outlier detection algorithms  1  1  1  1  1  attempt to detect outliers by computing the distances in full dimensional space. however  in very high dimensional spaces  the data is very sparse and the concept of similarity may not be meaningful anymore  1  1 . in fact  due to the sparse nature of distance distributions in high dimensional spaces  the distances between any pair of data records may become quite similar . thus  by using the notion of similarity in high dimensional spaces  each data record may be considered as potential outlier. it has been shown recently that by examining the behavior of the data in subspaces  it is possible to develop more effective algorithms for cluster discovery  and similarity search in high dimensional spaces  1  1  1 . it has been shown that this is also true for the problem of outlier detection   since in many applications only the subset of attributes is useful for detecting anomalous behavior. in the example shown in fig. 1  data records a and b can be seen as outliers only when certain two dimensions are selected  in fig. 1b data record a is seen as outlier  in figure 1c data record b is observed as outlier  in figure 1d both data records a and b may be detected as outliers   while in other two-dimensional projections they show average behavior  fig. 1a  . in addition  when significant number of features in a database is considered noisy  finding outliers in all dimensions typically do not result in effective detection of outliers  while at the same time it is difficult to identify a few relevant dimensions where the outliers may be observed. 
furthermore  it is well known in machine learning that ensembles of classifiers can be effective in improving overall prediction performance. these combining techniques typically manipulate the training data patterns single classifiers use  e.g. bagging   boosting   or the class labels  e.g. ecoc  . in general  an ensemble of classifiers must be both diverse and accurate in order to improve prediction of the whole. in addition to classifiers' accuracy  diversity is also required to ensure that all the classifiers do not make the same errors. however  it has been shown that standard combining methods  e.g. bagging  do not improve the prediction performance of simple local classifiers  e.g. k-nearest neighbor  due to correlated predictions across the outputs from multiple combined classifiers  1  1  and their low sensitivity to data perturbation. nevertheless  local classifiers are extremely sensible to the selection of features that are used in the learning process  and prediction of their ensembles can be decorrelated by selecting different feature representations  e.g different set of features   1  1 . since many outlier detection techniques that compute full dimensional distances are also local in their nature  they are also sensitive to the selection of features used in distance computation. in addition  presence of noisy and irrelevant features can significantly degrade the performance of outlier detection. in this paper  we propose a novel feature bagging framework of combining predictions from multiple outlier detection algorithms for detecting outliers in high-dimensional and noisy data sets. unlike standard bagging approach where the classification/regression models that are combined use randomly sampled data distributions  in this approach outlier detection algorithms are combined and their diversity is improved by sampling random subsets of features from the original feature set. due to aforementioned sensitivity of outlier detection algorithms to the selection of features used in distance computation  each outlier detector identifies different outliers and assigns different outlier scores to data records. the outlier scores are then combined in order to find the better quality outliers than the outliers identified by single outlier detection algorithms. 
it is important to note that the proposed combining framework can be applied to the set of any outlier detection algorithms or even to the set of different outlier detection algorithms. our experimental results performed on synthetic and real life data sets have shown that the combining outlier detection algorithms provide non-trivial improvement over the base algorithm. 
1. background and related work 

o 	o o o 
c  o 
o 
o 
o 	o 
o o o 
 b 	o 
o o o o a 
o o 
x a o o 
o o 	o 
a 
x 
 o o 
o o o o o 
o 
o  b 
o o 	o 	o o 	o o 	o 	o o o 
o o b  
 o o o o 	 b 
o o 
o o o o o o 
o o 
o o o x a o o o o o o o o o o 
	o 	o 
x a	o o o 	o 	o   b o 	o 
o o 	b 
o o 	o x a o o o 	o 
o o o o o figure 1. different two-dimensional projections of data space reveal different set of outliers or may not reveal outliers at all. outlier detection algorithms are typically evaluated using the detection rate  the false alarm rate  and the roc curves . in order to define these metrics  let's look at a confusion matrix  shown in table 1. in the outlier detection problem  assuming class  c  as the outlier or the rare class of the interest  and  nc  as a normal  majority  class  there are four possible outcomes when detecting outliers  class  c    namely true positives  tp   false negatives  fn   false positives  fp  and true negatives  tn . 
table 1. confusion matrix defines four possible scenarios when classifying class  c  
 predicted outliers - class c predicted normal class nc actual outliers - class c true positives   tp  false negatives  fn  actual normal class nc false positives   fp  true negatives  tn   
from table 1  detection rate and false alarm rate may be defined as follows: 
detection rate  	=  tp /  tp + fn  false alarm rate =  fp /  fp + tn  
detection rate gives information about the number of correctly identified outliers  while the false alarm rate reports the number of outliers misclassified as normal data records  class nc . the roc curve represents the trade-off between the detection rate and the false alarm rate and is typically shown on a 1-d graph  fig. 1   where false alarm rate and detection rate are plotted on x-axis  and y-axis respectively. the ideal roc curve has 1% false alarm rate  while having 1% detection rate  figure 1 . however  the ideal roc curve is hardly achieved in practice  and therefore researchers typically compute detection rate for different false alarm rates and present results on roc curves. very often  the area under the curve  auc  is also used to measure the performance of outlier detection algorithm. the auc of specific algorithm is defined as the surface area under its roc curve. the auc for the ideal roc curve is typically set to be 1  while aucs of  less than perfect  outlier detection algorithms are less than 1. in figure 1  the shaded area corresponds to the auc for the lowest roc curve. 
most of outlier detection techniques can be categorized into four groups:  1  statistical approaches   1  distance based approaches   1  profiling methods and  1  model-based approaches. in statistical techniques  1  1  1   the data points are typically modeled using a stochastic distribution  and points are determined to be outliers depending on their relationship with this model. however  most statistical approaches have limitation with higher dimensionality  since it becomes increasingly difficult and inaccurate to estimate the multidimensional distributions of the data points . distance based approaches  1  1  1  1  1  1  attempt to overcome limitations of statistical techniques and they detect outliers by computing distances among points. several recently proposed distance based outlier detection algorithms are based on  1  computing the full dimensional distances of points from one another using all the available features  1  1  or only feature projections   and  1  on computing the densities of local neighborhoods . in addition  a few clustering-based techniques have also been used to detect outliers either as side products of the clustering algorithms  points that do not belong to clusters   1  1  or as clusters that are significantly smaller than others . in profiling methods  profiles of normal behavior are built using different data mining techniques or heuristic-based approaches  and deviations from them are considered as intrusions. finally  model-based approaches usually first characterize the normal behavior using some predictive models  e.g. replicator neural networks  or unsupervised support vector machines  1  1    and then detect outliers as the deviations from the learned model. 
roc curves for different outlier detection techniques

figure 1. the roc curves for different detection algorithms 
on the other hand  extensive research was devoted to classifier ensembles in recent years. there were numerous techniques proposed in literature for combining classification algorithms  1  1  1  1  1 . however  it is important to note here that the problem of combining outlier detection algorithms is not exactly the same to the problem of classifier ensembles due to several reasons. first  in classifier ensembles  classification algorithms deal with combining discrete outputs  class labels  typically using different types of voting techniques. in combining outlier detection algorithms  the outlier scores or rankings of the algorithms are combined instead of class labels  although some classifier ensembles also combine rankings  or class probability estimates  from single classifiers through averaging. second  classifiers that are combined typically have complete knowledge of training data records and their labels  supervised learning  while outlier detection algorithms typically deal only with data records without any labels  unsupervised learning . however  some classifier ensembles that do not use class labels effectively  e.g. bagging  are very similar to combining outlier detection algorithms. finally  certain classifier ensembles  e.g. boosting   can control the combining process by observing the error rate  which is not possible in combining outlier detection algorithms since the label is not given and it is not known in advance what data records are really outliers. 
1. outlier detection techniques 
outlier detection algorithms that we utilize in this study are based on computing the full dimensional distances of the points from one another as well as on computing the densities of local neighborhoods. in our previous work    we have experimented with numerous outlier detection algorithms in the problem of network intrusion detection  and we have concluded that the density based outlier detection approach  e.g. lof  typically achieved the best prediction performance. therefore  in this study  we have chosen the lof approach to illustrate our findings.  
1 density based local outlier factor  lof  
detection approach 
the main idea of this method  is to assign to each data example a degree of being outlier. this degree is called the local outlier factor  lof  of a data example. data points with high lof have more sparse neighborhoods and typically represent stronger outliers  unlike data points belonging to dense clusters that usually tend to have lower lof values. 
to illustrate advantages of the lof approach over the simple nearest neighbor approach  consider a simple two-dimensional data set given in figure 1. it is apparent that the density of the cluster c1 is significantly higher than the density of the cluster c1. due to the low density of the cluster c1 it is apparent that for every example p1 inside the cluster c1  the distance between the example p1 and its nearest neighbor is similar to the distance between the example p1 and the nearest neighbor from the cluster c1  and the example p1 will not be considered as outlier using the simple nearest neighbor  nn  scheme. on the other hand  lof approach is able to capture the example p1 as outlier due to the fact that it considers the density around the points. nevertheless  the example p1 may be detected as outlier using both nn and lof approaches  since it is too distant from both clusters. 

figure 1. advantages of the lof approach 
1. combining outlier detection outputs 
we propose two novel techniques for combining outlier detection algorithms. their general framework is shown in fig. 1. the procedure for combining outlier detection techniques proceeds in a series of t rounds  although these rounds may be run in parallel for faster execution. in every round t  the outlier detection algorithm is called and presented with a different set of features ft that is used in distance computation. the set of features ft is randomly selected from the original data set  such that the number of features in ft is also randomly chosen between  d/1  and  d-1   where d is the number of features in original data set. when the number of features nt in ft is selected  nt features are randomly selected without replacement from the original feature set. 
every outlier detection algorithm  as a result  outputs different outlier score vector ast that reflects the probability of each data record from the data set s being an outlier. for example  if ast i    ast j   data record xi has higher probability of being outlier than data record xj. at the end of the procedure  after t rounds  there are t outlier score vectors each corresponding to a single outlier detection algorithm. the function combine  figure 1  is then 
used to coalesce these t outlier score vectors ast   t = 1 t into a unique anomaly score vector asfinal  which is lastly used to assign a final probability of being an outlier to every data record from the data set. 
  given: set s { x1  y1   ...    xm  ym } xi ¡Êxd  with labels yi ¡Êy = {c  nc}  where c corresponds to outliers  nc corresponds to a normal class  and d corresponds to the dimensionality  number of features  of vector x.   normalize data set s 
  for t = 1  1  1  1  ... t 
1. randomly chose the size of the feature subset nt from a uniform distribution between  d /1  and  d-1  
1. randomly pick  without replacement  nt features to create a feature subset ft  
1. apply outlier detection algorithm ot by employing the feature subset ft 
1. the output of the outlier detection algorithm ot is anomaly score vector ast  
  combine the anomaly score vectors ast and output a final anomaly score vector  asfinal as: 
     asfinal = combine ast   t = 1  ...  t figure 1. the general framework for combining outlier detection techniques 
the problem of combining outlier score vectors is conceptually quite similar to the problem of meta search engines  1  1  1  where different rankings returned by individual search engines are combined in order to provide the pages that are most relevant to the search string. in both problems  there is no label that helps to understand how relevant the search results are and the rank of results from individual algorithms is important in the combining process  since it gives the notion of result relevance. motivated by several approaches used in meta search engines  in this paper we explore two variants of the function combine that integrates the outputs of multiple outlier detection algorithms. the first variant  denoted as breadth first approach  is presented in figure 1.  
  given: ast  t = 1  ...  t  and m is the size of data set s and each vector ast 
  sort all outlier score vectors ast into the vectors sast and return indices indt of the sorted vectors  such that sast 1  has the highest score and indt 1  is the index of the data record in s with the highest score sast 1  
  let asfinal and indfinal be empty vectors. 
  for i = 1 to m 
  for t = 1 to t 
  if the index indt i  of the data record that is ranked at the i-th place by t-th outlier detection algorithm and that has the outlier score ast i  does not exist in the vector indfinal  
  insert indt i  at the end of the vector indfinal 
  insert ast i  at the end of the vector asfinal 
  return indfinal and asfinal figure 1. the breadth-first scheme for combining outlier detection scores 
algorithm 1 	algorithm 1 	... 	algorithm t

figure 1. illustration of the breadth-first approach for combining outlier detection scores. 
the breadth-first combining method first sorts all the outlier detection vectors ast into the sorted vectors sast and returns indices indt that give the correspondence between the sorted elements of the score vectors and the original elements of the sorted vectors. for example  indt 1  = k means that in the t-th outlier detection score vector ast  data record xk has the highest anomaly score ast k . thus in figure 1  as1 corresponds to the data record that is ranked as the most probable outlier by algorithm 1  as1 corresponds to the data record that is ranked as the second most probable outlier by algorithm 1   and so on. 
after sorting all outlier score vectors ast  the breadth-first approach simply takes the data records with the highest anomaly score from all outlier detection algorithms  scores as1  as1  as1  ...  ast 1 in figure 1  and inserts their indices in the vector indfinal  then takes data records with the second highest anomaly score  scores as1  as1  as1  ...  ast 1 in figure 1  and appends their indices at the end of the vector indfinal  and so on. if the index of the current data record is already in the vector indfinal  it is not appended again. at the end of the breadth-first method  the index indfinal contains indices of the data records that are sorted according to their probability of being outlier  and the vector asfinal contains these probabilities. 
the final results of the breadth-first method are in general sensitive to the order of outlier detection algorithms. however  the differences are minor since variations may happen only within t rankings  t is generally much smaller than the total number of data records   since at every i-th pass we go through t indices for data records ranked at i-th place in the outlier detection vector. 
the second variant of the function combine  denoted as cumulative sum approach  is presented in the figure 1.  
  given: ast  t = 1  ...  t  and m is the size of each vector ast 
  sum all anomaly scores ast from all t iterations as follows: 
  for i = 1 to m 
 	 asfinal i = ¡Æt ast  i  
t=1
  return asfinal figure 1. the cumulative sum approach for combining  
outlier detection scores 
this combining method first creates the final outlier score vector asfinal by summing all the outlier score vectors ast from all t iterations  then sorts the vector asfinal and finally identifies the data records with the highest outlier scores as outliers. for example  data record nc1 in figure 1 may be ranked as the first outlier by algorithm 1  ranked as fourth by algorithm 1  ...  and ranked as second by algorithm t. in the cumulative sum approach we sum all the scores that correspond to data record nc1  namely scores as1  as1  ...  and ast 1  and then sort all data records nci  i = 1  ...  m according to newly computed score. 
as1 - nc1 
as1 
as1 
as1 - nc1 
... 
as1 k as1 
as1 
as1 
as1 - nc1 
... 
as1 k - nc1  
 
 
 
 
 ast 1 
ast 1 - nc1
ast 1 
ast 1 - nc1
... 
ast k algorithm 1 	algorithm 1 	... 	algorithm t
figure 1. illustration of the breadth-first approach for  combining outlier detection scores 
it is important to note that this method is analogue to the ranking method in the meta search engines where the ranks are summed  but it is more flexible since in the ranking method an outlier detected by a single algorithm may not be detected in the final decision especially if it is ranked low by other detection algorithms. on the other hand  in the cumulative sum approach  the outlier that is detected by a single algorithm may have very large outlier score  and after all summations are performed may still have sufficiently large final outlier score to be detected. this fact is extremely important in the scenarios where outliers are visible only in a few dimensions  since in that case it is sufficient to select relevant features only in a small number of iterations  compute high outlier scores for these feature subsets and thus cause that these outliers are ranked high in the final score. 
1. experiments 
our experiments were performed on several synthetic data and real life data sets summarized in table 1. in all our experiments  we have assumed that we have information about the normal behavior  class  in the data set. therefore  in the first training phase  we have applied outlier detection algorithms only to the normal data set  without any outliers  in order to set specific false alarm rates  and in the second  testing  phase  we have applied outlier detection algorithms to test data sets  with all outliers . using this procedure we can achieve better detection performance that using completely unsupervised approach. 
1 experiments on synthetic data sets 
our first synthetic data set  synthetic -1 in table 1  has 1 data records  wherein 1 data records correspond to normal  majority  behavior  and 1 data records represent outliers. the data set has five original  contributing  features that determine which data records are outliers  figure 1 . normal behavior  blue points in figure 1  is modeled as a gaussian distribution of five original contributing features  while the outliers  red crosses in figure 1  are points that are far from the generated gaussian distribution. we added 1 noisy features in order to test robustness of  feature bagging  approach to the detection performance. 
our experiments on the synthetic-1 data set were performed using only lof approaches. the computed roc curves for this scenario for lof approach  breadth-first and cumulative sum approaches employing lof as single outlier detection algorithm are presented in figure 1. 
table 1. summary of data sets used in experiments 
dataset modifications made  in the data set size of dataset number of features number of outliers 
 rare class records  percentage of outliers continuous discrete synthetic -1 - 1 1 1 1 1% synthetic -1 - 1 1 1 1 1% satimage smallest class vs. rest 1 1 1 1 1% coil 1 - 1 1 1 1 1 % rooftop - 1 1 1 1 1 % lymphography merged classes 1 vs. rest 1 1 1 1 1 % mammography - 1 1 1 1 1 % kddcup 1 u1r vs. normal 1 1 1 1 1 % ann-thyroid class1 vs. class1 1 1 1 1 1% ann-thyroid class1 vs. class1 1 1 1 1 1% led each class vs. rest 1 1 1 ~1 ~1% letter recognition each class vs. rest 1 1 1 1 1% segment each class vs. rest 1 1 1 1 1% shuttle classes 1  1  1  1 & 1 vs. class 1 1 1 1 1 - 1 1% - 1% 
 
figure 1. distribution of two contributing features for the synthetic-1 data set  blue points represent normal behavior  red crosses represent outliers . 
analyzing roc curves from figure 1  it can be observed that lof approach applied with five original and five noisy features has much worse roc curve than lof approach that used only five original features. this was reasonable to assume since density computations in lof approach are significantly influenced by noisy and/or irrelevant features  and thus the lof performance also degrades. however  when the proposed methods for combining outlier detection algorithms are applied on the synthetic-1 data set with five original and five noisy features  it can be observed that they were able to alleviate the effect of noisy features and to outperform single lof approach. furthermore  the cumulative sum combining method has very similar roc curve as the lof approach only with 1 original contributing features. 
 
figure 1. roc curves for single lof approach and two combining methods employing lof approach when applied to the synthetic-1 data set with 1 original and 1 noisy features. the number of combined outlier detection algorithms for all data sets was set to 1. the figures are best viewed in color on the other hand  the breadth first approach is slightly worse than the cumulative sum  but still better than lof approach with both contributing and noisy features. that means that if there are irrelevant features in the data sets  combining methods are able to decrease the influence of noisy features regarding the detection performance. depending on the number of relevant and irrelevant features this decrease can vary. our earlier experiments also show that this decrease is rather small if the number of irrelevant features significantly outnumbers the number of relevant features. to investigate the influence of the noisy features to the detection performance  we have created two additional synthetic data sets with 1 and 1 noisy features in addition to five contributing features. instead of roc curves  for these two data sets we have reported areas under the curve  auc   since auc allows us to easier compare all three scenarios. from table 1  it can be observed that with increasing number of noisy features  the gap between single lof and the combining methods is indeed decreasing. that means that the combining methods can alleviate the influence of the noisy features only till a certain level. the auc of ideal roc curve corresponds to 1  and it is computed using the trapezoidal rule. 
table 1. auc  areas under the curve  for single lof  cumulative sum and the breadth first approaches depending on the number of noisy features in the data set. 
number of noisy features single lof cumulative sum approach breadth first approach 1 1 1 1 1 1 1 1 1 1 1 1 our second synthetic data set  synthetic-1 data set  has also 1 data records  wherein 1 data records correspond to normal  majority  behavior  and 1 data records represent outliers. this data set has 1 features and all 1 features are responsible for determining the outliers  i.e. the data set does not have any noisy features. like in the synthetic-1 data set  see figure 1   the normal behavior in this data set corresponds to a gaussian distribution of eight contributing features  while analogously to the first data set the outliers are data points far from the normal behavior. the computed roc curves for this data set for lof approach  breadth-first and cumulative sum approaches are presented in figure 1. note that roc curves for the synthetic-1 data set use different axis scale than roc curves for the synthetic-1 data set in order to observe true differences. 

figure 1. roc curves for single lof approach and two combining methods employing lof approach when applied to the synthetic-1 data set. the number of combined outlier detection algorithms for all data sets was set to 1. 
it can be observed from figure 1 that in the scenario when all features that determine the outliers are important  there is a slight decrease in detection performance of combining methods. however  this decrease is minor  e.g. for false alarm = 1%  detection rate was decreased approximately only 1% for the breadth first approach and only 1% for the cumulative sum approach. for the false alarm of 1% all three methods achieve 1% detection rate  so the only differences are for the lower false alarm rates. the degradation in performance of the combining methods compared to the single lof approach is understandable since combining methods do not use all the features in any of the iterations  but at the same time due to the nature of the generated data set all the features are important for detecting outliers. however  in real life scenarios  it is hardly the case that all the features are relevant for detecting outliers. to check this claim  we also performed experiments on numerous real life data sets. 
1 experiments on real life data sets 
all real life data sets used in our experiments have been used earlier by other researchers for the problem of detecting rare classes  1  1  1  1 . these data sets are summarized in table 1. since rare class analysis is conceptually the same problem as the outlier detection  we employed those data sets for the purpose of outlier detection  where we detected rare classes as outliers. in addition to the data sets reported in table 1  we have also used several data sets from uci repository  that do not directly correspond to the rare class problems or outlier detection problems but can be converted into binary problems by taking one small class  with less than ~1% proportion present in the data set  and remaining data records or the biggest remaining class as a second class. therefore  we selected the following data sets for the conversion into binary data sets: ann-thyroid  led  letter recognition  segment  and shuttle. the same procedure was used earlier  when experimenting with the rare class learning. using this technique  we have formed additional 1 data sets. some of the data sets selected to perform the experiments have both continuous and discrete features. since lof approach is based on computing distances between data records  measuring a distance between two discrete  categorical  values is not always straightforward. in our implementation  for computing distances between data records that have discrete attributes we have used the concept of inverse document frequency  idf  already used in outier detection problems   where each value of categorical attribute is represented with the inverse frequency of its appearance in the data set. when performing experiments on coil 1   mammography  and rooftop  data sets  we did not change any class distribution. however  in the original lymphography data set   there are four classes  but two of them are quite small  1 and 1 data records   so we merged them and considered them as outliers compared to other two large classes  1 and 1 data records . when performing experiments on kddcup'1 data set  we selected to detect the smallest intrusion class  u1r   which had only 1 instances. since the outliers are detected as deviations from the normal behavior  we have modified original data set  1 data records with five classes  such that the new data set contained only the data records from the normal class  1 data records  and from the u1r class. in such modified data set  we have tried to detect the u1r class using outlier detection algorithms. finally  for satimage data set we chose the smallest class to represent outliers and collapsed the remaining classes into one class as was done in . this procedure gave us a skewed 1-class dataset  with 1 majority class examples and 1 minority class examples  outliers . for 1 created binary data sets  we have typically selected one of the smallest classes and then converted either the remaining data records or the biggest remaining class into the majority class. therefore  for ann-thyroid data set we have detected classes 1 and classes 1 as outliers vs. the class 1 as the normal  majority  class. similarly  for shuttle data set we have created five data sets by selecting classes 1  1  1  1 and 1 to be detected as outliers compared to the biggest remaining class 1. for other real life data sets  led  letter recognition  and segment   we have simply selected each of the classes to be detected as outliers and merged all remaining classes to correspond to the normal  majority  class. 
for our experiments performed on first six real life data sets from 
table 1  the computed roc curves for lof approach  breadthfirst and cumulative sum approaches are presented in figure 1. due to the lack of space the experimental results for remaining 1 created binary data sets were presented using areas under the curves  auc   table 1 . the computed aucs  for chess  led  letter  segment and shuttle data sets have been averaged over all generated data sets for the original data set. for example  there were 1 binary data sets generated from the original letter data set  since there are 1 classes   and aucs were averaged over all these 1 data sets when reporting experimental results in table 1. 
table 1. auc  areas under the curve  for single lof  cumulative sum and breadth first approaches for 1 real life data sets obtained by converting original data into binary problems. 
data set single lof approach cumulative sum approach breadth first approach ann-thyroid  
class1 vs. class 1 1 1 1 ann-thyroid  class1 vs. class1 1 1 1 led  average  1 1 1 letter  average  1 1 1 segment  average  1 1 1 shuttle  average  1 1 1 analyzing figure 1 and table 1  it can be observed that both  cumulative sum and breadth first combining methods outperformed single lof outlier detection approach on all real life data sets. the improvements in the detection performance were the smallest  approximately 1% in detection rate for chosen false alarm rate  on the coil 1 data set  figure 1a  and on the satimage data set  figure 1f . this was probably due to the poor performance of individual outlier detection algorithms on these two data sets  so combining their outputs could not lead to significant improvements. when detecting outliers on the rooftop data set  figure 1b   the improvements were slightly better than for the coil 1 data set  but again not large due to weak performance of individual outlier detection algorithms. nevertheless  the improvements in detection rate for the false alarm rates ranging from 1% to 1% are not small and they vary from 1% to 1%. 
the greatest enhancements in outlier detection were achieved for the mammography  fig. 1d  and kdd cup'1  figure 1e  data sets. for those data sets single outlier detection results had respectively reasonable detection performance  so combining their outputs further improved overall results. however  when performing experiments on lymphography data set  figure 1c   the detection rate of a single lof approach was 1% already at 1% false alarm rate  so the combining methods could not improve detection performance very much. in order to illustrate even such a slight improvement of combining methods for this data set  we reported their roc curves only for small false alarm rates  less than 1 . 
from table 1  it can be observed that the small improvements were also achieved for those binary data sets that were created by taking one small class as outlier class and remaining data records as a second class. this can be explained by the fact that the remaining classes that were merged together to form a single majority class were quite different  so it was not possible to distinct separated class from the remaining data. it can be also observed that in two data sets when the binary data sets were created by taking the small class as outlier class and the biggest one as the normal class  the improvements of the combining methods were more apparent. 
finally  it can be observed that for all 1 real life data sets used in our experiments and for all values of false alarm rate  both combining methods were consistently better than the single lof approach. the only exceptions are the lymphography data set  kddcup'1 data set and certain generated data sets from led and letter data sets  where for low false alarm rates  less than 1 for lymphography data set  less than 1 for kddcup'1 data set and less than 1 for data sets created from led and letter data sets  detection rates of all three approaches were quite similar. 
1. conclusions 
a novel general framework for combining outlier detection algorithms was presented. experiments on several synthetic and various real life data sets indicate that proposed combining methods can result in much better detection performance than the single outlier detection algorithms. the proposed combining methods successfully utilize benefits from combining multiple outputs and diversifying individual predictions through focusing on smaller feature projections. data sets used in our experiments contained different percentage of outliers  different sizes and different number of features  thus providing a diverse test bed and showing wide capabilities of the proposed framework. the universal nature of the proposed framework allows that the combining schemes can be applied to any combination of outlier detection algorithms thus enhancing their usefulness in real life applications. 
although performed experiments have provided evidence that the proposed methods can be very successful for the outlier detection task  future work is needed to fully characterize them especially in very large and high dimensional databases  where new algorithms for combining outputs from multiple outlier detection algorithms are worth considering. it would also be interesting to examine the influence of changing the data distributions when detecting outliers in every round of combining methods  employing not only the distance-based but also other types of outlier detection approaches. 
1. acknowledgments 
this work was partially supported by army high performance 
computing research center contract number daad1-1  by the arda grant ar/f1-c-1 and by the nsf grant iis-1. the content of the work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred. access to computing facilities was provided by the ahpcrc and the minnesota  supercomputing institute. 
	coil 1 data set	rooftop data set
	1.1.1.1.1.1.1.1.1.1	1.1.1.1.1.1.1.1.1.1
	false alarm rate	  	 	false alarm rate	 
lymphography data set	mammography data set
1.1.1.1	1	1	1	1	1	1	1	1	1	1 false alarm rate	  	 	false alarm rate	 kdd cup 1 data set	satimage data set
	1.1.1.1.1.1.1.1.1.1	1.1.1.1.1.1.1.1.1.1
	false alarm rate	 	 	 	false alarm rate	 
figure. 1. roc curves for single lof approach and two combining methods employing lof approach when applied to all five data sets. the number of combined outlier detection algorithms for all data sets was set to 1  except for the mammography data set when this number was 1 due to small number of features  1  in the data set. the figures are best viewed in color.  
