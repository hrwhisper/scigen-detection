many techniques have been proposed to scale web applications. however  the data interdependencies between the database queries and transactions issued by the applications limit their efficiency. we claim that major scalability improvements can be gained by restructuring the web application data into multiple independent data services with exclusive access to their private data store. while this restructuring does not provide performance gains by itself  the implied simplification of each database workload allows a much more efficient use of classical techniques. we illustrate the data denormalization process on three benchmark applications: tpc-w  rubis and rubbos. we deploy the resulting service-oriented implementation of tpc-w across an 1-node cluster and show that restructuring its data can provide at least an order of magnitude improvement in the maximum sustainable throughput compared to master-slave database replication  while preserving strong consistency and transactional properties.
categories and subject descriptors
c.1  computer-communication networks : distributed systems; c.1  performance of systems : design studies; h.1  information storage and retrieval : systems and software.
general terms
performance.
keywords
scalability  web applications  data denormalization.
1. introduction
　the world-wide web has taken an important place in everyday's life. many businesses rely on it to provide their customers with immediate access to information. however  to retain a large number of customers  it is important to guarantee a reasonable access

 
　zhou wei and jiang dejun also work at vrije universiteit amsterdam. this work is supported by the china national science foundation project #1 and 1 project #1aa1- z1.
copyright is held by the international world wide web conference committee  iw1 . distribution of these papers is limited to classroom use  and personal use by others.
www 1  april 1  1  beijing  china.
acm 1-1-1/1.
performance regardless of the request load that is addressed to the system. web application hosting systems therefore need the ability to scale their capacity according to business needs. many research efforts have been made to provide scalable infrastructures for static content. however  scaling web applications that dynamically generate content remains a challenge.
　web applications are commonly used to generate dynamic content that is personalized to individual clients. each user's request triggers the execution of specific business logic  which issues any number of queries to an underlying database before returning a result to the client. queries can simply extract information from the database  but udi queries will also update  delete or insert information to the database. queries may also be grouped into database transactions.
　many techniques have been proposed to improve the scalability of web applications. scaling application-specific computation is relatively easy as requests can be distributed across any number of independent application servers running identical code. similarly  one can reduce the network bottleneck between the application and database servers . the main challenge  however  is to scale access to application data. besides classical techniques such as master-slave database replication  new techniques exploit knowledge of the application data access behavior. database query caching relies on high temporal locality  and uses prior knowledge of data overlap between different query templates to efficiently implement invalidations  1  1  1  1 . a query template is a parametrized sql query whose parameter values are passed to the system at runtime. partial replication techniques use similar information to reduce the data replication degree and limit the cost of database updates  1  1 . however  we observe that these techniques work best under very simple workloads composed only of a few different query templates. when the number of templates grows  an increasing number of constraints reduces their efficiency: database caching mechanisms need to invalidate more cached queries upon each update to maintain consistency  and partial replication is increasingly limited in the possible choices of functionally correct data placements.
　in this paper  we claim that scalable web applications should not be built along the traditional monolithic three-tier architecture. instead  restructuring the application data into independent data services  each of which having exclusive access to its private data store  allows one to reduce the workload complexity of each of the services. while this restructuring by itself does not lead to any performance improvements  it does allow for a more effective application of the aforementioned optimization techniques  thus leading to significantly better scalability. importantly  this does not imply any loss in terms of transactional or consistency properties.
　restructuring a monolithic web application composed of web pages that address queries to a single database into a group of independent web services querying each other requires to rethink the data structure for improved performance - a process sometimes named denormalization. future web applications should preferably be designed from the start along a service-oriented architecture. however  for existing monolithic applications we show how one can denormalize the data into data services.
　to demonstrate the effectiveness of our proposal  we study three web application benchmarks: tpc-w   rubis  and rubbos . we show how these applications can be restructured into multiple independent data services  each with a very simple data access pattern. we then focus on the udi-intensive data services from tpc-w and rubis to show how one can host them in a scalable fashion. for rubbos  this is almost trivial. finally  we study the scalability of tpc-w  the most challenging of the three benchmarks  and demonstrate that the maximum sustainable throughput grows linearly with the quantity of hosting resources used. we were thus able to scale tpc-w by an order of magnitude more than traditional systems.
　this paper is structured as follows. section 1 presents related work. then  section 1 details our system model and the issues that we need to face. section 1 presents the process of data denormalization  while section 1 shows how individual data services can be scaled. section 1 presents performance evaluations. finally  section 1 concludes.
1. related work
　in the past years  many techniques have been proposed to improve the scalability of web applications. the simplest one is edgeserver computing where requests are distributed among several edge servers running the same code  1  1 . although this technique is very effective at scaling the computation part of the applications  the main challenge is to scale the access to the application data.
　replication is a common technique to improve the throughput of a rdbms. many rdbms replication solutions aim at replicating data across multiple servers within a cluster  1  1  1  1 . database replication allows one to distribute read queries among the replicas. however  in these solutions  all udi queries must first be executed at a master database  then propagated and re-executed at all other  slave  databases using 1-phase commit or snapshot isolation mechanisms. a few commercial database systems such as oracle allow one to optimize the re-execution of udi queries at the slaves by transferring a log of the execution at the master. however  these techniques do not improve the maximum throughput as they require a single master server to execute all udi queries. the throughput of the master server then determines the total system's throughput.
　as extensively discussed in   a number of techniques have been developed specifically to scale web applications. most of these techniques exploit the fact that web applications issue a fixed number of query templates to the database. several systems propose to cache the result of database queries at the edge servers. static knowledge of conflicts among different query templates allows one to invalidate caches efficiently when a database update occurs. however  such systems work best under workloads with high query locality  few udi queries  and few dependencies between query templates. furthermore  the efficiency of caches does not grow linearly with the quantities of resources assigned to it  so caches alone cannot provide arbitrary scalability.
　another approach based on query templates relies on partial database replication  where not all tables get replicated to all database servers . this allows one to reduce the number of udi queries that must be issued to each database server. however  although this technique allows one to improve the system throughput  its efficiency is constrained by the templates that query multiple tables simultaneously  because of join queries or database transactions . in contrast  our proposal also relies to a certain extent on partial data replication  but at a much finer granularity which allows one to reduce the data placement constraints.
　in   the authors propose an edge computing infrastructure where the application programmers can choose the best suited data replication and distribution strategies for the different parts of application data. by carefully reducing the consistency requirements and selecting the replication strategies  this approach can yield considerable gains in performance and availability. however  it requires that the application programmers have significant expertise in domains such as fault-tolerance and weak data consistency. in contrast  we strive to present a systematic method to denormalize and restructure data  with no implied loss in consistency or transactional properties. note that  although we only focus on performance issues in this paper  improving availability can be realized by applying classical techniques to each individual service.
　several recent academic and industrial research efforts have focused on the design of specialized data structures for scalable and highly available services  1  1  1 . these systems usually provide a simple key-based put/get interface and focus on scalability and availability properties rather than rich functionality or transactional properties. these design choices mean that these systems implicitly or explicitly assume that an application has been decomposed into separate entities with very simple data access patterns. in contrast  this paper demonstrates how one can design a web application along such a service-oriented architecture with simplified workload.
　data fragmentation techniques have been commonly used in the design of distributed relational database systems  1  1  1  1 . in these works  tables are partitioned either vertically or horizontally into smaller fragments. partitioning schemes are determined according to a workload analysis in order to optimize access time. however  these techniques do not fundamentally change the structure of the data  which limits their efficiency. furthermore  changes in the workload require to constantly re-evaluate the data fragmentation scheme . we consider that dynamic environments such as web applications would make such an approach impractical. in contrast  we propose a one-time modification in the application data structure. further workload fluctuations can be handled by scaling each service independently according to its own load.
　even though data denormalization is largely applied to improve the performance of individual databases  few research efforts have been made to systematically study them  1  1 . data denormalization often creates data redundancy by adding extra fields to existing tables so that expensive join queries can be rewritten into simpler queries. this approach implicitly assumes the existence of a single database  whose performance must be optimized. in contrast  we apply similar denormalization techniques in order to scale the application throughput in a multi-server system. denormalization in our case allows one to distribute udi queries among different data services  and therefore to reduce the negative effects of udis on the performance of replicated databases.

figure 1: system model
1. system model
1 goal
　the idea behind our work is that the data access pattern of traditional monolithic web applications is often too complex to be efficiently handled by a single technique. indeed  proposed techniques work best under specific simple access patterns. data replication performs best with workloads containing few or no udi; query caching requires high temporal locality and not too many udis; partial replication or even data partitioning demand that queries do not span multiple partitions.
　we claim that major gains in scalability can be obtained by restructuring web application data into a collection of independent data services  where each service has exclusive access to its private data store. while such restructuring does not provide any performance improvement by itself  it considerably simplifies the data access pattern generated by each service. this allows one to apply appropriate scaling techniques to each service.
　figure 1 shows the system model of a web application after restructuring. instead of being hosted in a single database  the application data are split into three separate databases db1  db1 and db1. each database is encapsulated into a data service which exports a service interface to the application business logic. each data service and its database can then be hosted independently using the technique that suits it best according to its own data access pattern. here  db1 is replicated across two database servers  db1 is hosted by only one server  while db1 has been further partitioned into db1a and db1b. note that splitting the application data into independent services also improves separation of concerns: details about the internal hosting architecture of a data service are irrelevant to the rest of the application.
1 data denormalization constraints
　denormalizing an application's data into independent data services requires deep changes to the structure of the data. for example  a table containing fields hkey attr1 attr1i and queried by templates  select key from table where attr1=  
and  select key from table where attr1=   may be split into two tables hkey attr1i and hkey attr1i  which may belong to two different data services.
　however  not all tables can be split arbitrarily. in practice  data accessed by different queries often overlap  which constrains the denormalization. we identify two types of constraints: transactions and query data overlap.
　although database transactions are known as an adversary to performance  they sometimes cannot be avoided. an example is a checkout operation in an e-commerce application where a product order and the corresponding payment should be executed atomically. acid requirements provide a strong motivation for maintaining all data accessed by one transaction inside a single database  and therefore inside a single data service. splitting such data into multiple services would impose executing distributed transactions across multiple services  for example  using protocols such as 1phase commit. we expect that this would negate the performance gains of the data decomposition.
　another source of constraints is created by ordinary queries executed outside transactions. similar to constraints created by transactions  it seems logical to cluster the data accessed by each query. however  in most cases the overlap of different queries would lead to creating a single data service. instead  we can apply two other transformations. first  certain complex database queries can be rewritten into multiple  simpler queries. doing this reduces the data inter-dependency and allows better data restructuring. second  data dependencies induced by overlapping queries can also be reduced by replicating certain data to multiple services. however  this implies a trade-off between the gains of splitting the data into more services and the costs of replicating update queries to these data over multiple services.
1 scaling individual data services
　in all our experiments  we noticed that the services resulting from data denormalization maintain extremely simple data structures and are queried by very few query templates. such a simple workload considerably simplifies the task of hosting services in a scalable fashion. for example  some data services receive very few or even no udi queries at all. such services can therefore benefit from massive caching or replication. on the other hand  some other services concentrate on large number of udi queries  often grouped together inside transactions. such services are clearly harder to scale. however  they at least benefit from the fact that they receive less queries than the database of a monolithic application would. additionally  we show in section 1 that such services can often be partitioned so that udi queries are distributed across multiple database servers.
1. data denormalization
　service-oriented data denormalization exploits the fact that udi queries and transactions often access only a part of the columns of a table. decomposing such tables into multiple smaller ones helps distributing udi queries and transactions to more data services  and thereby simplifies their workload. as discussed in section 1  two main constraints must be taken into account when denormalizing an application's data. first  one should split the data into the largest possible number of services  such that no transaction or udi query in the workload spans multiple services. second  one must make sure that read queries can continue to operate over the then partitioned data.
1 denormalization and transactions
　as discussed in previous sections  we need to cluster the data into services such that no transaction overlaps multiple data services. to this end  we first mark which data columns are accessed by each transaction. then  simple clustering techniques can be applied to decompose the data into the largest possible number of independent data services.
　we distinguish three types of  transactions  that must be taken into account here. first  real database transactions require acid properties. this means that all the data they access must be accessed atomically and must be placed into the same service. one exception to this rule is formed by data columns that are never up-

figure 1: different denormalization techniques for read queries
dated  neither by the transaction in question nor by any other query in the workload. an example is the table that matches zipcodes to local names. such read-only data does not need to be placed in the same data service  and can be abstracted as a separate data service.
　the second type of transaction is a so-called  atomic set   where only the atomicity property of a normal transaction is necessary. atomic sets appear  for example  in tpc-w  where a query that reads the content of a shopping cart and the one that adds another element must be executed atomically . for such atomic sets  only the columns that are updated must be local to the same data service to be able to provide atomicity. columns that are only read by the atomic set can reside outside the service  as they are not concerned by the atomicity property1.
　finally  udi queries that are not part of a transaction must be executed atomically  and therefore must be considered as an atomic set composed of a single query.
　once one has marked each transaction  udi query and atomic set with the data columns that should be kept in a single service  simple clustering techniques can provide the first step of decomposition of the database columns into services. however  this step is not functional  as it accommodates only the needs of transactions and udi queries. to become functional  one must further update this data model to take read queries into consideration.
1 denormalization and read queries
　clearly  one can consider read queries similarly to udi queries and transactions  and cluster data services further such that no read query overlaps multiple services. however  applying this method would increase the constraints to the data decomposition and lead to coarse-grain data services  possibly with a single data service for the whole application.
　instead  as shown in figure 1  two different operations can be applied. first  certain read queries can be rewritten into a series of multiple sub-queries  where each sub-query can execute in one data service. for example  in tpc-w  the customer and order tables are located in different data services  whereas the following query spans both tables with a join operation:  select o id from customer  orders where customer.c id = orders.o c id and c uname =   . however  this query can be easily rewritten into two sub-queries that access only one table: i   select c id from customer where c uname
=   ; and ii   select o id from orders where
o c id=  . the returned result of the first query is used as input for the second one and the final result is returned by the second query.
　another transformation often applied in traditional database denormalization techniques consists of replicating data from certain database tables to other tables. this allows one to transform join queries into simpler queries. note that traditional denormalization applies this technique to optimize the efficiency of query execution within a single database whereas we apply this technique to be able to split the data into independent data services. for example  the following query accesses two tables in two different data services:  select item.i id item.i title from item order line where item.i id = order line.ol i id and item.subject=  limit 1 . replicating column i subject from table item to the other data service allows one to transform this query and target a single data service. the only constraint is that any update to the i subject column must be applied at both data services  preferably within a  distributed  transaction. this scheme is therefore applicable only in cases where the data to be replicated are rarely updated.
　to conclude  complex query rewriting should be the preferred option if the semantics of the query allows it. otherwise  column replication may be applied if the replicated data are never or seldom updated. in last resort  when neither query rewriting nor column replication is possible  merging the concerned data services is always correct  yet at the cost of coarse-grain data services.
1 case studies
　to illustrate the effectiveness of our data denormalization process  we applied it to three standard web applications: tpc-w  rubis and rubbos.
1.1 tpc-w
　tpc-w is an industry standard e-commerce benchmark that models an online bookstore similar to amazon.com . its database contains 1 tables that are queried by 1 transactions  1 atomic sets  1 udi queries that are not part of a transaction  and 1 read-only queries.
　first  the transactions and atomic sets of the tpc-w workload impose the creation of four sets of transactions whose targeted data do not overlap. the first set contains transaction purchase  and the two atomic sets docart and getcart; the second set contains the adminconfirm transaction  the third set contains only the updaterelated transaction. finally  the last set contains
addnewcustomer  refreshsession and enteraddress. this means for example that the original item table from tpc-w must be split into five tables: item stock contains the primary key i id and the column i stock; table item related contains i id and i related1; table item dynamic contains i id  i cost  i thumbnail  i image and i pub date; the last table contains all the read-only columns of table item.
the result of the first denormalization step is composed of five
data services: a financial data service contains tables orders 
order entry  cc xacts  shopping cart 
data servicedata tables included columns requestsfinancialorders	order entry
i stock i stock 	shopping cart
ping cart entrycc xacts shop-getlastestorderinfo  createemptycart  additem  refreshcart  resetcarttime  getcartinfo  getbesterids  computerelateditems  purchasecustomercustomer address countrygetaddress  setaddress  getcustomerid  getcustomername  getpassword  getcustomerinfo  login  addnewcustomer  refreshsessionitem dynamicitem dynamic i cost	i pub date
i image i thumbnail i subjectgetitemdynamicinfo  getlatestitems  setitemdynamicinfoitem basicitem basic i title i subject  authorgetitembasicinfo 	searchbyauthor 	searchbytitle  searchbysubjectitem relateditem related i related1 getrelateditems  setitemrelateditem publisheritem publisher i publisher getpublishersitem detailitem detail i srp i backing getitemdetailsitem otheritem other i isbn	i page	i dimensions	i desc i avail getitemotherinfotable 1: data services of the denormalized tpc-wshopping cart entry and item stock; data service item related takes care of items that are related to each other  with table item related; data service item dynamic takes care of the fields of table item that are likely to be updated by means of table item dynamic; finally  data service  customer  contains customer-related information with tables customer  address and country. the remaining tables from tpc-w are effectively read-only and are clustered into a single data service. this readonly data service can remain untouched  but for the sake of the explanation we split it further during the second denormalization step.
　the second step of denormalization takes the remaining read queries into account. we observe that most read queries can either be executed into a single data service  or be rewritten. one read query cannot be decomposed: it fetches the list of the best-selling 1 books that belong to a specified subject. however  the list of book subjects i subject is read-only in tpc-w  so we replicate it to the financial data service for this query1; i subject is also replicated to the item dynamic data service for a query that obtains the list of latest 1 books of a specified subject.
　the remaining read-only data columns can be further decomposed according to the query workload. for example  the  search  web page only accesses data from columns i title  i subject and table author. we can thus encapsulate them together as the item basic service. we similarly created three more read-only data services.
　the final result is shown in table 1. an important remark is that  although denormalization takes only data access patterns into account  each resulting data service has clear semantics and can be easily named. this result is in line with observations from   where examples of real-world data services are discussed.
1.1 rubbos
　rubbos is a bulletin-board benchmark modeled after slashdot. org . it consists of 1 tables requested by 1 udi queries and 1 read-only queries. rubbos does not contain any transactions.
six tables incur udi workload  while the other two are read-only. furthermore  all udi queries access only one table. it is therefore easy at the end of the first denormalization step to encapsulate each table incurring udi queries into a separate data service.
data servicetablestransactionsuserusers u storecomment u c comments c registeruser u auctionitems i storebuynow i n buy now n registeritem i bids b storebid i b categoriescategories-regionsregions-table 1: data services of rubis
　all read queries can be executed in only one table except two queries which span two tables: one can be rewritten into two simpler queries; the other one requires to replicate selected items from old stories into the users table. the old stories table  however  is read-only so no extra cost is incurred from such replication. finally  the two read-only tables are encapsulated as separate data services.
　rubbos can therefore be considered as a very easy case for data denormalization.
1.1 rubis
　rubis is an auction site benchmark modeled after ebay.com . it contains 1 tables requested by 1 update transactions. except for the read-only tables regions and categories  the other five tables are all updated by insert queries  which means that they cannot be easily split. this means that the granularity at which we can operate is the table. the transactions impose the creation of two data services: the  users  data service contains tables users and comments  while the  auction  data service contains tables buy now  bids and items. the final result of data denormalization is shown in table 1.
　rubis is a difficult scenario for denormalization because none of its tables can be split following the rules described in section 1. we note that in such worst-case scenario  denormalization is actually equivalent to the way globetp  would have hosted the application. we will show however in the next section that scaling the resulting data services is relatively easy.
1. scaling individual data services
　in all cases we examined  the workload of each individual data service can be easily characterized. some services incur either
1 insert into order with o id=id;
1 insert into cc xacts with cx o id=id;
1 foreach item i within the order do
1insert into order entry with ol o id=id  ol i id=i;
1update i stock set i stock=i stock-qty i  where i id=i;
1 end
1 update shopping cart where sc id=id;
1 delete from shopping cart entry where scl sc id=id;algorithm 1: the purchase transaction
read-only or read-dominant workload. these services can be scaled up by classical database replication or caching techniques . other services incur many more udi queries  and deserve more attention as standard replication techniques are unlikely to provide major performance gains. furthermore  update-intensive services also often incur transactions  which makes the scaling process more difficult. instead  partial replication or data partitioning techniques should be used so that update queries can be distributed among multiple servers. we discuss two representative examples from tpc-w and rubis and show how they can be scaled up using relatively simple techniques.
1 scaling the financial service of tpc-w
　the denormalized tpc-w contains one update-intensive service: the financial service. this service incurs a database update each time a client updates its shopping cart or does a purchase. however  all tables from this service  except one  are indexed by a shopping cart id and all queries span exactly one shopping cart. this suggests that  instead of replicating the data  one can partition them according to their shopping cart id.
　the financial data service receives two types of updates: updates on a shopping cart  and purchase transactions. the first one accesses tables shopping cart and shopping cart- entry. table shopping cart contains the description of a whole shopping cart  while shopping cart entry contains the details of one entry of the shopping cart. if we are to partition these data across multiple servers  then one should keep a shopping cart and all its entries at the same server.
　the second kind of update received by the financial service is the purchase transaction. we present this transaction in algorithm 1. similar to the updatecart query  the purchase transaction requires that the order made from a given shopping cart is created at the same server that already hosts the shopping cart and its entries. this allows one to run the transaction within a single server of the financial service rather than facing the cost of a distributed transaction across replicated servers.
one exception to this easy data partitioning scheme is the
item stock table  in which any element can potentially be referred to by any shopping cart entry. one simple solution would be to replicate the item stock table across all servers that host the financial service. however  this would require to run the purchase transaction across all these servers. instead  we create an item stock table in each server of the financial service in which all item details are identical except the available stock which is divided by the number of servers. this means that each server is allocated a part of the stock that it can sell without synchronizing with other servers. only when the stock available at one server is empty  does it need to execute a distributed transaction to re-distribute the available stock.
　the financial service receives two more read queries that access data across multiple data clusters. these queries retrieve respectively the 1 and 1 latest orders from tables orders and order entry in order to obtain either the list of best-selling items or the items most related to a given other item. we implement these queries in a similar way to distributed databases. each query is first issued at each server. the results are then merged into a single result set  and the relevant number of most recent orders is re-selected from the merged results.
　in our implementation  we wanted to balance the load imposed by different shopping carts across all servers of the financial service. we therefore marked each row of tables shopping cart  shopping cart entry and orders with a key equal to the shopping cart id. we then hash this id to h =  1id + 1 %m  where m is the number of servers  to determine which server h should be responsible for that row. our experiments show that this very simple hash function balances the load effectively in terms of data storage size and computational load.
　this example shows that  even for relatively complex data services  the fact that each service has simple semantics and receives few different queries allows one to apply application-specific solutions. the resulting relative complexity of the service implementation  however  remains transparent to other parts of the application  which only need to invoke a simple service interface.
1 scaling rubis
　the denormalized rubis implementation contains two updateintensive services:  auction  and  user.  similar to the previous example  most queries address a single auction or user by their respective ids. we were thus able to partition the data rows between multiple servers. a few read-only queries span multiple auctions or users  but we could easily rewrite them such that individual queries would be issued at every server  before their results can be merged.
1. performance evaluation
　as we have seen  rubbos and rubis are relatively simple to host using our denormalization technique. rubbos can be decomposed into several rarely updated data services. on the other hand  rubis requires coarser-grain update-intensive services  but they can be scaled relatively easily. we present here performance evaluations of tpc-w  which we consider as the most challenging of the three applications.
　our evaluations assume that the application load remains roughly constant  and focus on the scalability of denormalized applications. to support the fluctuating workloads that one should expect in real deployments  a variety of techniques exist to dictate when and how extra servers should be added or removed from each individual data service of our implementations  1  1  1 .
　we compare three implementations of tpc-w.  otw  represents the unmodified original tpc-w implementation. we then compare its performance to  dtw   which represents the denormalized tpc-w where no particular measure has been taken to scale up individual services. finally   stw   scalable tpc-w  represents the denormalized tpc-w with scalability techniques enabled. all three implementations are based on the java implementation of tpc-w from the university of wisconsin . for performance reasons we implemented the data services as servlets rather than soap-based web services.
　we first study the performance of otw and dtw to investigate the costs and benefits of data denormalization with no scalability techniques being introduced. we then study how replication and data partitioning techniques allow us to scale individual data services of tpc-w. finally  we deploy the three implementations on an 1-node cluster and compare their scalability in terms of throughput.
1 experimental setup
　all experiments are performed on the das-1  an 1-node linuxbased server cluster . each machine in the cluster has a dualcpu / dual-core 1 ghz amd opteron dp 1  1 gb of memory and a 1 gb ide hard drive. nodes are connected to each other with a gigabit lan such that the network latency between the servers is negligible. we use tomcat v1.1 as application servers  postgresql v1.1 as database servers  and pound 1 as load balancers to distribute http requests among multiple application servers.
　before each experiment  we populate the databases with 1 customer records and 1 item records. other tables are scaled according to the benchmark requirements. the client workload is generated by emulated browsers  ebs . we use the number of ebs to measure the client workload. the workload model incorporates a think time parameter to control the amount of time an eb waits between receiving a response and issuing the next request. according to the tpc-w specification  think times are randomly distributed with exponential distribution and average value 1 seconds.
　tpc-w defines three standard workloads: the browsing  shopping and ordering mixes  which generate 1%  1% and 1% update interactions respectively. unless otherwise specified  our experiments rely on the shopping mix.
1 costs and benefits of denormalization
　the major difference between a monolithic web application and its denormalized counterpart is that the second one is able to distribute its udi workload across multiple machines. even though such an operation implies a performance drop when hosting the application on a single machine  it improves the overall system scalability when more machines are used. in this section  we focus on the costs and benefits of data denormalization when no special measure is taken to scale the denormalized tpc-w.
　we exercise the otw and dtw implementations using 1 ebs  under each of the three standard workload mixes. both systems are deployed over one application server and 1 database servers. in the case of otw  the database servers are replicated using the standard postgresql master-slave mechanism. dtw is deployed such that each data service is hosted on a separate database server.
　we measure the system performance in terms of wirt  web interaction response time  as well as wips  web interactions per second . according to the tpc-w specification  we defined an sla in terms of the 1th percentile of response times for each type of web interaction: namely  1% of web interactions of each type must complete under 1 ms. the only exception is the  admin confirm  request type  which does not have an sla requirement. this request is issued only by the system administrator  and therefore does not influence the client-perceived performance of the system.
　figure 1 shows the performance of the different systems under each workload. figure 1 a  shows the achieved system throughput  whereas figure 1 b  shows the number of query types for which the sla was respected.
　the browsing mix contains very few udi queries. both implementations sustain roughly the same throughput. however  the denormalized tpc-w fails to meet its sla for two out of the 1 interaction types. this is due to the fact that the concerned interactions heavily rely on queries that are rewritten to target multiple  different data services. these calls are issued sequentially  which explains why the corresponding request types incur higher latency.
　at the other extreme  the ordering mix contains the highest fraction of udi queries. here  dtw sustains a high throughput and respects all its slas  while otw simply crashes because of over-

 a  average throughput comparison

 b  sla-satisfied web interaction type number comparison
figure 1: throughput and performance comparison between original tpc-w and denormalized tpc-w. note that the ordering mix for the original tpc-w overloaded and subsequently crashed the application.
load. this is due to the fact that dtw distributes its udi queries across all database servers while otw replicates them to all servers. finally  the shopping mix constitutes a middle case where both implementations behave equally good.
　we conclude that data denormalization improves the performance of udi queries at the cost of a performance degradation of rewritten read queries. we note  however  that the extra cost of read queries does not depend on the number of server machines  whereas the performance gain of udi queries is proportional to the size of the system. this suggests that the denormalized implementation is more scalable that the monolithic one  as we will show in the next sections.
1 scalability of individual data services
　we now turn to study the scalability of each data service individually. we study the maximum throughput that one can apply to each service when using a given number of machines  such that the sla is respected.
　since we now focus on individual services rather than the whole application  we need to redefine the sla for each individual data service. as one application-level interaction generates on average five data service requests  we roughly translated the interactionlevel sla into a service-level sla that requires 1% of service requests to be processed within 1 ms. the financial service is significantly more demanding than other services  since about 1% of its requests take more than 1 ms irrespectively of the workload. we therefore relax its sla and demand that only 1% of queries return within 1 ms.

figure 1: scalability of individual tpc-w services
　we measure the maximum throughput of each data service by increasing the number of ebs until the service does not respect its sla any more. to generate flexible reproducible workloads for each data service  we first ran the tpc-w benchmark several times under relatively low load  1 ebs  and collected the logs of the invocation of data service interfaces. we obtained 1 query logs  each representing the workload of 1 ebs for a duration of 1 minutes. we can thus generate any desired workload  from 1 ebs to 1 ebs step by 1 ebs  by replaying the right number of elementary log files across one or more client machines concurrently.
　figure 1 shows the throughput scalability of three representative data services from the scalable tpc-w. the item basic data service is read-only. it is therefore trivial to increase its throughput by adding database replicas. similarly  the item dynamic service receives relatively few udi queries  and can be scaled by simple master-slave replication.
　on the other hand  the financial service incurs many database transactions and udi queries  which implies that simple database replication will not produce major throughput improvements. we see  however  that the implementation discussed in section 1 exhibits a linear growth of its throughput as the number of database servers increases.
　to conclude  we were able to scale all data services to a level where they could sustain a load of 1 ebs. different services have different resource requirements to reach this level  with the item basic  item dynamic and financial services requiring 1  1  and 1 database servers  respectively.
　we believe that all the data services can easily be scaled further. we stopped at that point as 1 ebs is the maximum throughput that our tpc-w implementation reaches when we use the entire das-1 cluster for hosting the complete application.
1 scalability of the entire tpc-w
　we conclude this performance evaluation by comparing the throughput scalability of the otw  dtw and stw implementations of tpc-w. similar to the previous experiment  we exercised each system configuration with increasing numbers of ebs until the sla was violated. in this experiment  we use the application-level definition of the sla as described in section 1.
　figure 1 a  compares the scalability of otw  dtw and stw when using between 1 and 1 server machines. in all cases we started by using one application server and one database server. we then added database server machines to the configurations. in otw  extra database servers were added as replicas of the monolithic application state. in dtw  we start with one database server for all services  and eventually reach a configuration with one database server per service. in stw  we allocated the resources as depicted in figure 1 b . note that in all cases  we deliberately over-allocated the number of application servers and client machines to make sure that the performance bottleneck remains at the database servers.
　when using very few servers  otw slightly outperforms dtw and stw. with increasing number of servers  otw can be scaled up until about 1 ebs when using 1 servers. however  when further adding servers  the throughput decreases. in this case  the performance improvement created by extra database replicas is counterbalanced by the extra costs that the master incurs to maintain consistency.
　as no individual scaling techniques are applied to dtw  it can be scaled up to at most 1 database servers  one database server per service . the maximum throughput of dtw is around 1 ebs.
note that this is only about half of the maximum achievable throughput of otw. this is due to the extra costs brought by data denormalization  in particular the rewritten queries. adding more database servers per service using database replication would not improve the throughput  as most of the workload is concentrated in the financial service.
　finally  stw shows near linear scalability. it reaches a maximum throughput of 1 ebs when using 1 server machines  1 database servers for the financial service  1 database servers for the other services  1 application servers and 1 load balancers . taking into account the 1 client machines necessary to generate a sufficient workload  this configuration uses the entire das-1 cluster. the maximum throughput of stw at that point is approximately 1 times that of otw  and 1 times that of a single database server.
　we note that the stw throughput curve seems to start stabilizing around 1 server machines and 1 ebs. this is not a sign that we reached the maximum achievable throughput of stw. the explanation is that  as illustrated in figure 1  1 ebs is the point where many small services start violating their sla with two database servers  and need a third database server. in our implementation each database server is used for a single service  which means that several extra database servers must be assigned to the small data services to move from 1 ebs to 1 ebs. we expect that using more resources the curve would grow faster again up to the point where the small data services need four servers.
1. conclusion
　most approaches toward scalable hosting of web applications consider the application code and data structure as constants  and propose middleware layers to improve performance transparently to the application. this paper takes a different stand and demonstrates that major scalability improvements can be gained by allowing one to denormalize an application's data into independent services. while such restructuring introduces extra costs  it considerably simplifies the query access pattern that each service receives  and allows for a much more efficient use of classical scalability techniques. we applied this methodology to three standard benchmark applications and showed that it allows tpc-w  the most challenging of the three  to scale by at least an order of magnitude compared to master-slave database replication. importantly  data denormalization does not imply any loss in terms of consistency or transactional properties. this aspect makes our approach unique compared to  for example  .
	 1	 1
	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1	 1
	number of server machines	number of server machines
	 a  maximum system throughput	 b  allocation of machine resources for stw
figure 1: scalability of tpc-w hosting infrastructure　in our experience  designing the data denormalization of an application from its original data structure and query templates takes only a few hours. on the other hand  the work required for the actual implementation of the required changes highly depends on the complexity of each data service.
　data denormalization exploits the fact that an application's queries and transactions usually target few data columns. this  combined with classical database denormalization techniques such as query rewriting and column replication  allows us to cluster the data into disjoint data services. although this property was verified in all applications that we examined  one cannot exclude the possible existence of applications with sufficient data overlap to prevent any service-oriented denormalization. this may be the case of transaction-intensive applications  whose acid properties would impose very coarse-grained data clustering. it is a well-known fact that database transactions in a distributed environment imply important performance loss  so one should carefully ponder whether transactions are necessary or not.
　the fact that denormalization is steered by prior knowledge of the application's query templates means that any update in the application code may require to restructure the data to accommodate new query templates. however  the fact that all data services resulting from denormalization have clear semantics makes us believe that extra application features could be implemented without the need to redefine data services and their semantics. one can also imagine to fully automate denormalization such that any necessary change in the data structure could be applied transparently to the application  using a proxy layer to translate the original application query templates into their data service-specific counterparts. we leave such improvements for future work.
