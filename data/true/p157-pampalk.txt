using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays. several visualization techniques have been developed to study the cluster structure of data  i.e.  the existence of distinctive groups in the data and how these clusters are related to each other. however  only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed. understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase.
¡¡in this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure. we demonstrate the application of our approach in two music related data mining projects.
categories and subject descriptors
i.1  pattern recognition : clustering-similarity measures  algorithms
keywords
high-dimensional data  interactive data mining
1. introduction
¡¡a common problem in data mining is to extract and select the right features for further analysis. this is particularly true for complex high-dimensional data such as images or music. in many applications the different options of the preprocessing and feature extraction procedures can be described in terms of parameters which are adjusted to fit specific needs. for example  such a parameter can define the
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigkdd '1  august 1  1  washington  dc  usa copyright 1 acm 1-1/1 ...$1.
value of the exponent of a power-law used to compress the range of specific values  the weighting between features  or if the data is variance normalized or not.
¡¡in exploratory data analysis there is generally no specific target function given which could be used to optimize these parameters. furthermore additional parameters might be introduced by the distance function  e.g. minkowski metric  which is necessary for cluster analysis. finding appropriate values for all of these parameters and in consequence defining the feature extraction procedure remains a task which requires domain expertise and human intelligence.
¡¡in this paper we present a new technique to visualize the influence of such parameters on the cluster structure of the data. the intention is to offer domain experts the possibility to interactively explore the influence of the parameters  gain new insights  and inspire new hypothesis for further analysis. the technique we propose is particularly applicable to very high-dimensional data represented by low-level features. it is based on a new extension to the self-organizing map  som   algorithm and on smoothed data histograms  sdh  . in particular  we present aligned-soms  i.e.  multiple soms stacked on top of each other and aligned so that they are organized in a similar way. the soms are trained to represent the same data items in different data spaces defined by slightly different values for the parameters mentioned above. we use aligned-soms in combination with sdh to visualize how gradual changes in the feature extraction procedure slowly change the organization of the data in the 1-dimensional visualization space.
¡¡to demonstrate our approach we present a system designed in cooperation between a data miner and a musicologist to analyze expressive performances of classical piano music by internationally renowned pianists. furthermore  we apply the same concept to create an interface for exploring the contents of digital libraries. in particular  we demonstrate how a music collection is organized and visualized based on a combination of timbre  i.e.  sound characteristics which distinguish different instruments  and rhythmic characteristics and how this organization gradually changes when the focus of interest is changed in favor of one of these.
¡¡the remainder of this paper is organized as follows. in section 1 we review related work. in section 1 we review the som and the sdh visualization. in section 1 we present aligned-soms. in section 1 we present two applications of aligned-soms to explore expressive performances of classical music and to explore archives of popular music. finally  in section 1 we draw some conclusions.
1. related work
¡¡in general  visualization techniques are powerful tools that are frequently employed in knowledge discovery processes. visualizations can make complex relationships easily understandable and stimulate visual thinking . especially  tools which visualize the cluster structure of data are valuable for exploring and understanding data. such tools include data histograms for one-dimensional data as well as algorithms which project high-dimensional data to a two-dimensional visualization space trying to preserve the topology  i.e.  trying to ensure that distances between data items in the visualization space correspond to the distances in the highdimensional data space.
¡¡a popular choice for non-linear projections is the selforganizing map  som  . alternatives include non-linear multi-dimensional scaling  mds   or linear principal component analysis . for example  in  an approach was presented where a hyperbolic metric is combined with mds. the user can interactively change the focus to different regions in the data space  thus  viewing the relationship of items in the chosen region with a relatively high resolution while maintaining the overall context. however  this approach is based on given distances between data items  while our aim is to aid the user in defining how these distances shall be calculated. once an appropriate definition for the distances is found  focusing on details in different regions of the data space would be one of the next steps.
¡¡the distances in the data space depend on the metric  the extracted features  and how the individual features are normalized and weighted. in the experiments presented in this paper we use the euclidean distance metric. however  in the same way the features can be weighted or normalized differently it is also possible to change the distance metric. the impact of distance metrics on high-dimensional data has been studied  for example  in . the problem of defining a similarity between data items can be simplified if for some of the data the similarity is known .
¡¡changing the distances between data items changes the structure of the data. for example  if the data consists of several piano pieces which vary in tempo  the structure would be one big cluster when focusing on sound characteristics only. one the other hand  the structure might consist of several small clusters if the focus is on rhythm.
¡¡to understand the relationship between different ways of weighting features  it is useful to visualize the gradual changes in the structure when shifting focus from one feature to another. previous work in this direction includes star coordinates   which are based on scatter plots where the data is projected onto a non-orthogonal coordinate system representing the multi-dimensional data space. the resulting ambiguities are resolved when the user interacts with the visualization. the user can emphasize a particular feature by giving a single dimension more space  i.e.  increasing the length of the respective axes  and rearranging all other axis so that they are orthogonal to the emphasized one. the main difference to our approach is  that we do not assume each data dimension by itself to be meaningful but rather assume many low-level attributes which as a whole resemble an abstract concept.
a different approach to combining human intuition with the processing power of computers to find a suitable projection of the data is  1  1 . the approach is based on polarized projections  i.e.  the data is projected into a subspace defined by polarization anchors which have a similar function as the model vectors in the som. given a polarized projection the data is visualized using kernel density estimators allowing the user to easily identify clusters. although this allows an interactive search for the best way to project the high-dimensional data  there are several differences to our work. foremost  the approach focuses on finding clusters in the data  while we try to understand and find the right parameters for the feature extraction process. the projections which are defined through the polarization anchors cannot be interpreted directly in such a way that would allow direct feedback to the feature extraction process. furthermore  in contrast to the polarized projections  our emphasis is on linking different views of the same data so that the data density visualization changes smoothly between slightly different projections of the same data.
¡¡recently  a framework for visualizing changes in the density distribution of data was presented in . in contrast to the approach we present  the framework was applied to understand changes in evolving data streams using differential kernel density estimation with various window sizes. in evolving data streams the same data spaces are used at different points in time while the data items change. other approaches analyzing the changes in data characteristics include  where the focus is on measuring the effects on data mining models instead of intuitively visualizing changes.
1. self-organizing maps
¡¡the self-organizing map  som   1  1   an unsupervised neural network  has successfully been applied in exploratory data analysis  with applications in various domains such as finances . the som is a powerful tool for visual clustering  and analyzing correlations in the data . furthermore  a variant of the som  the adaptive subspace som  has been developed to automatically detect invariant features in dynamic signals.
¡¡one of the best known applications of the som is the websom project  1  1  where millions of high-dimensional text documents are organized according to their similarity to create an intuitive user interface for interactive exploration. alternatives to the som include multi-dimensional scaling   sammon's mapping   and generative topographic mapping  gtm  . the approach we present can be reformulated to use either of these  however  we have chosen the som because of its computational efficiency.
¡¡the idea of the som is to map the high-dimensional data to a 1-dimensional map in such a way that similar items are located close to each other. the resulting mapping reflects the cluster structure of the data  i.e.  the clusters and their relationship to each other.
¡¡the som consists of an ordered set of units which are arranged in a 1-dimensional visualization space  referred to as map. common choices to arrange the map units are rectangular or hexagonal grids. each unit is defined through its distance to the other units  and is assigned a model vector in the high-dimensional data space. to map a data item from the data space to the map it is necessary to calculate the distance between the data item and all model vectors to find the model vector with the smallest distance  i.e.  the most similar model vector. the data item is then mapped to the respective unit  also referred to as best matching unit. thus  every point in the data space can be assigned to a location on the 1-dimensional map.
1 the batch-som algorithm
¡¡the som can be initialized randomly  i.e.  random vectors in the data space are assigned to each model vector. several alternatives using  for example  a principal component analysis to initialize the som can be found in .
¡¡the som training is basically a loop which is repeated until convergence. in each iteration the best matching unit for each data item is calculated. then the model vectors of the respective units are updated so that they fit the data better. this would be identical to k-means clustering  were it not for a constraint which forces the model vectors of neighboring units to represent similar data items. the units which are considered to be in the neighborhood of a particular unit are defined through a neighborhood function based on the distances between the units on the map. an important aspect of the som is that the size of the neighborhood decreases slowly with each iteration to finally end up with very small neighborhoods allowing each unit to perfectly adapt to the data it represents.
¡¡to formalize the batch-som algorithm we define the data matrix d  the model vector matrix mt  the distance matrix u  the neighborhood matrix nt  the partition matrix pt  and the spread activation matrix st. the data matrix d is of size n¡Ád where n is the number of data items and d is the number of dimensions. each row represents one data item. the model vector matrix mt is of size m¡Ád  where m is the number of map units. the values of mt change with each iteration t. the distance matrix u of size m¡Ám defines the squared distances between the units on the map. thus  u is symmetrical with zeros on the diagonal. the neighborhood matrix nt can be calculated  for example  as
 u/rt1
	nt = e	 	 1 
where rt defines the neighborhood radius and monotonically decreases with each iteration. n is of size m ¡Á m  symmetrical  with high values on the diagonal  and represents the influence of one unit on another. the sparse partition matrix pt is calculated given d and mt. in particular  at iteration t 
 
	1 	if unit j is the best match for item i 
pt i j  =	 1  1 	otherwise.
thus  pt is of size n ¡Á m and the sum over all columns of each row equals 1. the spread activation matrix st  with size n ¡Á m  defines the responsibility of each unit for each data item at iteration t. thus  st i j  will be high if j is the best matching unit for item i  and depending on how large the neighborhood is  all units in the neighborhood of unit i will have relatively high values too for i. the spread activation is calculated as 
	st = ptnt.	 1 
at the end of each loop the new model vectors mt+1 are
calculated as 
	mt+1 = s td 	 1 
where s t denotes the spread activation matrix which has been normalized so that the sum over all rows in each column equals 1 except for units without responsibilities. note  that if a unit is not responsible for any data item  i.e.  the sum over all rows in the respective column equals 1  the normalization would cause divisions by zero. thus  the columns representing units without responsibilities are not normalized and the respective model vectors are not updated.
¡¡there are two main parameters which need to be adjusted by the user. the first is the number of map units  i.e.  how large the som should be. this decision is basically a computational one. more map units require more training iterations and for each iteration the computation of the best matching units becomes more intense. on the other hand  more units lead to a higher resolution of the mapping.
¡¡the most difficult parameter to adjust is the final neighborhood radius relative to the number of map units. to adjust this correctly it is necessary to know the level of noise in the data. very noisy data requires a large final radius. the final radius defines the smoothness of the mapping.
1 smoothed data histograms
¡¡various methods to visualize clusters based on the som have been developed. the most prominent method visualizes the distances between the model vectors of adjacent units and is known as the u-matrix . we use smoothed data histograms  sdh   where each data item votes for the map units which represent it best based on some function of the distance to the respective model vectors. all votes are accumulated for each map unit and the resulting distribution is visualized on the map.
¡¡as voting function we use a robust ranking where the map unit closest to a data item gets n points  the second n-1  the third n-1 and so forth  for the n closest map units. all other map units are assigned 1 points. the parameter n can interactively be adjusted by the user. the concept of this visualization technique is basically a density estimation  thus the results resemble the probability density of the whole dataset on the 1-dimensional map  i.e. the latent space . the main advantage of this technique is that it is computationally not more expensive than one iteration of the batch-som algorithm. on the other hand  it does not offer a clear statistical interpretation as  for example  the probability density defined by the gtm algorithm.
1 illustration of som and sdh
¡¡figure 1 illustrates characteristics of the som and the cluster visualization using a synthetic 1-dimensional dataset. although in general it does not make sense to use the som to analyze 1-dimensional data this dataset allows us to illustrate some aspects of the som algorithm which would be difficult to visualize otherwise.
¡¡one important aspect is the topology preservation. map units next to each other on the grid represent similar regions in the data space. this can be seen by the arrangement of the model vectors  which are connected by lines indicating which model vectors are assigned to neighboring map units  cf. figure 1c . if k-means had been used instead of the som  the connecting lines would be drawn randomly between points  while the som has learned to represent the data in such a way that the model vectors are arranged according to the organization of the map units.
¡¡another important aspect  which is illustrated in figure 1c  is that the som defines a non-linear mapping from the data space to the 1-dimensional map. the distances between neighboring model vectors are not uniform. areas in b d
figure 1: illustration of som and sdh   a  probability distribution in the 1-dimensional data space   b  sample drawn from this distribution   c  model vectors of the som in the data space   d  map units of the som in the visualization space with clusters visualized using sdh  n=1 with spline interpolation . high density areas are visualized with white  low density with gray. the model vectors and the map units of the som are represented by the nodes of the rectangular grid.
the data space with a high density are represented by more model vectors  thus  in higher detail than sparse areas. this characteristic is exploited by the u-matrix visualization.
¡¡however  not all model vectors are located in dense areas and some model vectors remain in sparse areas to maintain the overall structure of neighboring units. these units which might not represent any data are known as interpolating units. the fact that not every unit represents the same amount of data items is exploited by the sdh  cf. figure 1d .
1. aligned-soms
¡¡the goal is to understand the relationship between different ways of representing the same data by visualizing changes in the cluster structure. thus  we assume that the dataset can be represented in different but related ways depending on various parameters in the feature extraction process.
¡¡aligned-soms are a new approach to visualizing the influence of such parameters by training multiple soms  i.e.  we stack several soms on top of each other and obtain several som layers representing the same data from different points of view. each layer has a slightly different point of view than its neighbors. the main constraint we apply to the layers is that they map the same data to similar locations as their neighbors. then the user can move through the layers and see how the distribution of the data gradually changes as the parameters defining the feature extraction process are changed.
¡¡the aligned-soms are trained in such a way that each layer maps similar data items close to each other within the layer  and that neighboring layers map the same items to similar locations. to ensure that two neighboring som layers have a similar organization we define a distance between two layers  which we choose to be smaller than the distance between two adjacent units on each map. for example  while the distance between two adjacent units within a layer is set to 1  the distance between two layers can be set to 1. thus  two layers separated spatially by 1 other layers would be constrained as strongly as two adjacent units within a layer to represent the same data items. note that it is not sufficient to use the normal som training algorithm to map the data to 1-dimensional grids   because each som layer represents not only the same data  but also has a different data space. thus  a model vector from one layer cannot directly be interpreted in a different layer.
1 training algorithm for aligned-soms
¡¡we formulate the aligned-soms training algorithm based on the formulation of the batch-som in section 1. to train the som layers we extend the distance matrix u to contain the distances between all units in all layers  thus the size of u is ml ¡Á ml  where m is the number of units per layer and l is the total number of layers. each layer i has its own model vectors mit of size m ¡Á d and data di of size n ¡Á d. the neighborhood matrix is calculated according to equation 1. the sparse partition matrix pt is of size n¡Áml and calculated using equation 1 with the extension that the best matching unit for a data item is calculated for each layer. thus  the sum over all columns in each row equals the number of layers. the spread activation matrix st is calculated as in equation 1. the updated model vectors mit+1 are calculated as 
	m 	 1 
where s it denotes the normalized columns of st which represent the model vectors of layer i.
¡¡to initialize the aligned-soms in our experiments we have first trained the layer representing the most complex data space  e.g.  the layer in which sound characteristics  timbre  and rhythm are equally weighted  and then initialized the spread activation of all layers based on s it of the most complex layer.
¡¡the necessary resources in terms of cpu time and memory are proportional to the number of layers and depend on the complexity of the feature extraction parameters analyzed. thus  the overall computational load is of a higher magnitude than training a single som. in practice  we have experienced that standard hardware  p1ghz with 1mb ram  is sufficient to run aligned-soms on our datasets. for example  the experiments we discuss in subsection 1  which are calculated from over 1 multivariate time series segments  run within 1 hour including the time it takes matlab to create over 1 image files for the html interface. for larger datasets several optimizations of the algorithm are possible  in particular  applying an extended version of the fast winner search  would improve the efficiency drastically  since there is a high redundancy in the multiple layer structure.
1 illustration of aligned-soms
¡¡to illustrate the aligned-soms we use a simple dataset containing 1 animals with 1 boolean features describing their appearance and activities . the dataset is depicted in table 1. we assume  that it is not clear how to best represent the animals and that the weighting ratio between appearance and activity features is of interest. thus  we have trained 1 layers of soms using the aligned-som training algorithm. the first layer uses a weighting ratio between appearance and activity features of 1. the 1th layer  i.e. 
	appearance	activities
dove¡Á¡Á¡Á¡Áchicken¡Á¡Á¡Áduck¡Á¡Á¡Á¡Ágoose¡Á¡Á¡Á¡Á¡Áowl¡Á¡Á¡Á¡Á¡Áhawk¡Á¡Á¡Á¡Á¡Áeagle¡Á¡Á¡Á¡Á¡Áfox¡Á¡Á¡Á¡Ádog¡Á¡Á¡Á¡Áwolf¡Á¡Á¡Á¡Á¡Á¡Ácat¡Á¡Á¡Á¡Átiger¡Á¡Á¡Á¡Á¡Álion¡Á¡Á¡Á¡Á¡Á¡Áhorse¡Á¡Á¡Á¡Á¡Á¡Ázebra¡Á¡Á¡Á¡Á¡Á¡Ácow¡Á¡Á¡Á¡Átable 1: 1 animals described by 1 attributes.
the center layer  weights both feature groups equally. the last layer uses a weighting ratio of 1  thus  focuses only on activities. the weighting ratios of all other layers are linearly interpolated.
¡¡from the resulting aligned-soms 1 layers are depicted in figure 1. for interactive exploration a html version with all 1 layers is available on the internet.1 when the focus is only on appearance all small birds are located together in the lower right corner of the map. the eagle is an outlier because of its size. on the other side  all mammals are located in the upper half of the map separating the medium sized ones on the left from the large ones on the right. as the focus is gradually shifted to activity features the structure changes. in particular  the animals are arranged in such a way that predators or located on the left and others on the right. although there are several significant changes regarding individuals  the overall structure has remained largely the same  enabling the user to easily identify similarities and differences between two different ways of viewing the same data.
1. applications
¡¡in this section we present two applications of alignedsoms. the first application is the identification of distinctive sequences in multivariate time series data representing musical performance strategies. the second application is the content-based organization and visualization of a music collection for interactive exploration. for both applications we use a html based user interface with javascript and many images to conveniently interact with the alignedsoms. a demonstration is available on the internet.1
1 exploringmusicalperformancestrategies
¡¡the first application is part of a large data mining project1whose goal is to study fundamental principles of expressive

figure 1: part of a trajectory corresponding to an expert performance of chopin etude op. 1  no. 1. the loudness and tempo curves are smoothed. the bar boundaries are indicated through the black sections. the time dimension is visualized through the thickness and shading of the trajectory.
music performance  1  1 . performances by concert pianists are measured with respect to timing and loudness fluctuations. the goal is to find characteristic patterns that give insight into typical interpretation strategies used by pianists.
¡¡the dataset used for this particular experiment consists of performances of mozart piano sonatas  played by 1 internationally renowned pianists  daniel barenboim  roland batik  glenn gould  maria jo ao pires  andr¡äas schiff  mitsuko uchida . each performance is characterized by two series of numeric values that represent the measured tempo and loudness  respectively  over the course of the performance. an example of one such time series in the form of a smoothed trajectory in the two-dimensional tempo-loudness space is shown in figure 1  with tempo on the vertical axis and loudness on the horizontal axis. details of this form of performance visualization can be found in . the various trajectories are cut into overlapping segments each represented by 1 low-level features. the purpose of the whole procedure is to find out whether there are indeed characteristic and interpretable classes of tempo-loudness strategies that pianists apply consistently  and whether these are different between performers.
¡¡at the current state of our research it is not clear how to best represent the performance trajectories to capture the main characteristics. some of the open questions are related to the weighting of the tempo and loudness dimensions  strength of the trajectory smoothing  and the normalization of the data.
 e
figure 1: aligned-soms trained with the animal dataset   a  first layer with weighting ratio 1 between appearance and activity features   b  first quarter layer with ratio 1   c  center layer with ratio 1   d  last quarter layer with ratio 1   e  last layer with ratio 1. the shadings represent the density calculated¡¡regarding the normalization we have found 1 forms to be of particular interest which can be categorized in 1 levels. the first level is no normalization  the second level is normalizing the mean  the third level is to normalize mean and variance. the effect of the second level is that we focus only on absolute changes regarding loudness or tempo. for example  did the pianist speed up by 1 beats per minute  bpm   in the third level we focus only on relative changes  for example  has the pianist played 1% faster or slower  within the second and third levels we distinguish between 1 ways of normalizing the data  namely  normalizing over a short segment of the trajectory  'local'  or normalizing over using sdh  n=1 with linear interpolation .

figure 1: the feature extraction options and their relationships.
a longer sonata part  'global' .
¡¡figure 1 shows the connection between the 1 forms of normalizing the data. in addition  two dimensions are included which control the weighting between loudness and tempo  and the degree of smoothing the trajectories. for each of these different ways to extract features we analyze the effects on the cluster structure of the data. in particular  we analyze changes in the density distribution of trajectories in different sonatas  and in different pianists.
1.1 user interface
¡¡a screenshot of the html user interface is shown in figure 1. the data used consists of several fast mozart piano sonatas with a segment length of 1 beats. the user interface is divided into 1 parts. the first part contains the navigation unit  below it an eigenvalue indicator  and a visualization of the som model vectors  namely the codebook. the second part contains the sdh visualization for each pianist. to make differences between the 1 sdh visualizations more apparent  in addition  we visualize the individual sdhs after subtracting the average sdh. from this contrasting visualization it is easily possible to identify which patterns are used particularly often or seldom by a pianist compared to the average usage of the patterns. the third part contains the sdh visualizations of each sonata part. although we are not primarily interested in finding patterns which distinguish sonatas from each other  this visualization has proven to be useful in evaluating the normalizations.
¡¡the navigation unit is the same as in figure 1. by moving the mouse over the circles  the corresponding images are shown in the html viewer using javascript. three markers are used to indicate the current position to the user. one marker indicates the normalization  one the weighting with respect to loudness and tempo  and the third marker indicates the degree of smoothing. between two extremes we use 1 intermediate positions. therefore  given the overall structure  over 1 soms would need to be trained to allow every possible combination of the three markers. due to computational considerations we have limited the combinations such that a marker can only be moved to the small circles if the other two markers are located on big circles  thus  reducing the number of soms to 1.
¡¡the eigenvalue indicator displays the first  the second  and the sum of all other eigenvalues. the eigenvalues serve as indication of the complexity of the cluster structure in the data space. for example  for the sonatas used in figure 1 if no normalization is applied most of the variance can be explained using a linear projection into a 1-dimensional space. the reason is that the relative variations within a segment are negligible compared to the large variations of the absolute loudness and tempo given several sonatas played by different pianists. on the other hand  the most complex data space is the one illustrated in figure 1  i.e.  local mean and variance normalization with unsmoothed trajectories and an equal weighting between loudness and tempo.
¡¡the codebook shows the model vector of each unit. the trajectory segments are visualized by blue lines with a red dot marking the end. the blue shading represent the variance of the data items mapped to the unit. the number in the lower right of each unit displays the number of data items mapped to the respective unit. for example  in figure 1 there are 1 items mapped to the second unit in the first row. the trajectory starts fast and loud and almost linearly moves to slow and soft  ritardando-decrescendo . the codebook gives valuable insights into frequent patterns  however  it depends on the data analyzed if it is possible to visualize the codebook. when analyzing  for example  a text document collection it might be interesting to use a list of frequent words or other summarization strategies instead  1  1 .

figure 1: screenshot of the user interface used for exploring the effects of the feature extraction on theexpressive performances of mozart piano sonatas.
1.1 first results
¡¡studying the influence of the low-level features on the distinguishability of performance strategies is ongoing research. using the interactive interface presented above we have been able to understand the data better. moreover  visualizing the effects of normalization and weighting have helped communicate ideas between data miners and musicologists in our research project. we invite the reader to verify the following observations by interacting with the visualization provided at http://www.oefai.at/ elias/kdd1/mozart/.
¡¡one example of a rather trivial result is the influence of the loudness-tempo weighting when no normalization is applied. when the focus is only on loudness major differences between the pianists are clearly recognizable in the sdh visualization. in particular  the recordings by batik are much louder than the others while schiff is by far the softest. on the other hand  the sdh visualizations of the sonatas are very similar and hardly distinguishable. both observations are intuitive since the cd recordings vary in terms of average loudness. thus  the distinctions made with this normalization are not due to specifics of pianists  but rather to specifics of the recordings.
¡¡on the other hand  if the focus is on tempo  then the differences between the pianists diminish  except for gould. comparing the sdh with the codebook reveals that the performances by gould are outliers since they are either extremely fast or slow. the sdh of the sonatas reveals that they are very distinguishable in terms of tempo which is obvious since some are simply faster than others.
¡¡this distinction between sonatas is lost when normalizing the mean either locally for a segment or globally for a whole sonata part. when normalizing the mean locally the sdh of the pianists reveals that gould plays very frequently patterns of relative constant loudness and tempo. when removing the average from the sdh visualization  we can see that pires is quite the opposite to gould and frequently performs very strong modulations of loudness and tempo. schiff seems to modulate more the tempo than the loudness. this is underlined when viewing the effect of the loudness-tempo weighting. when focusing only on loudness the performances of schiff is very similar to the performances by gould while focusing on tempo reveals that there are strong similarities between pires and schiff. note that although the visualization gives valuable insights into the data which would be difficult to obtain otherwise  it is necessary to quantify any observations and test them on new data.
1 exploring music archives
¡¡the second application is part of the project islands of music1 whose goal is to create intuitive interfaces to digital libraries of music by automatically analyzing  organizing  and visualizing pieces of music based on their perceived acoustic similarities  1  1 .
¡¡the islands of music are calculated using the som with a sdh visualization and a specific color scale. similar pieces are located close to each other on the map. the resulting clusters are visualized as islands. subclusters within clusters are visualized as mountains and hills. a parameter defines the coastline which separates the water from the islands and helps to find distinctive clusters faster. a similar effect we obtain through the coastline has been used with polarized projections for visual clustering .
¡¡the main problem when organizing pieces of music is to automatically calculate the similarities between them. music similarity can be viewed from several different perspectives. for example  the similarity can be based on the instruments used  the melody  or the rhythm. although it is an easy task for a human listener to judge the general similarity between two pieces  there are currently no satisfactory computational models available.
¡¡several approaches to calculate music similarities are based on mel frequency cepstrum coefficients  mfccs   e.g.   1  1  1 . the mfccs describe sound characteristics in terms of frequency band and energy at a specific point in time and are used to model the timbre.
¡¡in our previous work we presented rhythm patterns  1  1  to calculate similarities. the rhythm patterns capture dynamic characteristics in the loudness modulation of specific frequency bands based on psychoacoustic models .
¡¡instead of defining a specific way to calculate the similarity between two pieces of music  we are developing interfaces which allow the users to define what they individually consider to be relevant aspects of similarity. a screenshot of a prototype is shown in figure 1. the user can interactively change the weights on features describing rhythm properties and sound characteristic  timbre  using a sliding bar.
¡¡the visualization shown in figure 1 is a prototype we use to analyze the similarity measures. beneath the islands and the sliding bar the model vectors of each layer are visualized. the model vectors contain two types of information which are displayed separately. on the left  red color  are the rhythm patterns and on the right  blue color  the mfcc patterns.
¡¡the current position of the sliding bar is on the right side  thus  the focus is on timbre characteristics. from this point of view there are five islands in the data  each representing a specific type of music. for example  on the island in the lower left peaceful classical pieces are located such as fu¡§r elise or the mondscheinsonata by beethoven. on the other side  in the upper right of the map is an island where we find pieces by the aggressive rock group papa roach.
¡¡although the details of the model vectors are irrelevant for the targeted user it allows us in the current stage of development to analyze and understand why a particular piece of music is located in a specific region. for example  when looking at the model vectors of the map in figure 1 there are several insights into the organization of the map we can gain. for example  we can see that the rhythm patterns are organized so that patterns with overall low energy can be found on the left. while the patterns with the highest energy can be found around the island in the lower right corner  which represents music with strong beats such as the songs by bomfunk mc's.
¡¡furthermore  we use the whole system to analyze and understand the relationships between different similarity measures and  thus  to evaluate them in an intuitive manner. for example  we can simplify the calculation of the rhythm patterns and compare the simplified version to the original version. if there are no significant changes in the organization of the collection  then the simplified version is likely to be just as good. however  it is more likely to have some sort of changes in the cluster structure  which can then be easily identified. an alternative approach would be to use objective and qualitative evaluations. however  due to a lacking ground truth such evaluations of music similarity measures are currently an unsolved problem in the music information retrieval community.
¡¡one of the observations we have made with this visualization is that focusing on timbre structures the data more in terms of types of instruments or artist. for example  in the lower left of the map there are some classical pieces of music. when focusing on timbre  there is a distinction between slow piano and slow string pieces. on the other hand  when focusing on rhythm this distinction is not made. another example is the music in the upper right of the map. when focusing on timbre  for example  pieces by papa roach are clearly separated from others  while these are mixed together with other pieces from the same style when weighting the rhythm patterns more strongly.
1. conclusions and discussion
¡¡we have presented a novel approach to visualizing changes in the cluster structure of data when the features describing the data are changed. using aligned self-organizing maps the user is able to gradually and smoothly change focus between feature extraction procedures to explore how they are related and what the differences are. we demonstrated the application in two data mining projects.
¡¡in the first application where the goal is to analyze musical performance strategies the main result was that the visualization helped communicate ideas between data miners and domain experts. in particular  it helped the data miners explain why it is necessary to consider weighting of different dimensions and different forms of normalization. on the other hand  the domain experts were able to help the data miners find interesting aspects of the data to analyze in more detail.

figure 1: screenshot of the user interface used for content-based exploration of music archives.¡¡in the islands of music project where communicating ideas is not a critical issue there were two main results. the first result is that the visualization can be used to study differences and similarities between different ways of computing similarity between music  i.e. studying how different ways of extracting features are related. furthermore  preliminary results indicate that being able to change focus from one similarity aspect to another might be an interesting tool for browsing and exploring digital libraries of music.
1. acknowledgments
¡¡this research has been carried out in the project y1-inf  sponsored by the austrian federal ministry of education  science and culture  bmbwk  in the form of a start research prize. the bmbwk also provides financial support to the austrian research institute for artificial intelligence.
