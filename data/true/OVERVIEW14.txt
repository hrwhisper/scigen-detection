1.1 documents
the document set of a test collection should be a sample of the kinds of texts that will be encountered in the operational setting of interest. it is important that the document set reflect the diversity of subject matter  word choice  literary styles  document formats  etc. of the operational setting for the retrieval results to be representative of the performance in the real task. frequently  this means the document set must be large. the primary trec test collections contain 1 to 1 gigabytes of text and 1 to 1 1 documents . the document sets used in various tracks have been smaller and larger depending on the needs of the track
 num  number: 1
 title  embryonic stem cells
 desc  description: what are embryonic stem cells  and what restrictions are placed on their use in research 
 narr  narrative: explanation of the nature of embryonic stem cells is relevant. their usefulness in research is relevant. sources for them and restrictions on them also are relevant.
figure 1: a sample trec 1 topic from the terabyte track test set.
and the availability of data. the terabyte track was introduced in trec 1 to investigate both retrieval and evaluation issues associated with collections significantly larger than 1 gigabytes of text.
   the primary trec document sets consist mostly of newspaper or newswire articles. high-level structures within each document are tagged using sgml or xml  and each document is assigned an unique identifier called the docno. in keeping of the spirit of realism  the text was kept as close to the original as possible. no attempt was made to correct spelling errors  sentence fragments  strange formatting around tables  or similar faults.
1.1 topics
trec distinguishes between a statement of information need  the topic  and the data structure that is actually given to a retrieval system  the query . the trec test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. the format of a topic statement has evolved since the earliest trecs  but it has been stable since trec-1  1 . a topic statement generally consists of four sections: an identifier  a title  a description  and a narrative. an example topic taken from this year's terabyte track is shown in figure 1.
   the different parts of the trec topics allow researchers to investigate the effect of different query lengths on retrieval performance. for topics 1 and later  the  title  field was specially designed to allow experiments with very short queries; these title fields consist of up to three words that best describe the topic. the description   desc   field is a one sentence description of the topic area. the narrative   narr   gives a concise description of what makes a document relevant.
   participants are free to use any method they wish to create queries from the topic statements. trec distinguishes among two major categories of query construction techniques  automatic methods and manual methods. an automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. the definition of manual query construction methods is very broad  ranging from simple tweaks to an automatically derived query  through manual construction of an initial query  to multiple query reformulations based on the document sets retrieved. since these methods require radically different amounts of  human  effort  care must be taken when comparing manual results to ensure that the runs are truly comparable.
   trec topic statements are created by the same person who performs the relevance assessments for that topic  the assessor . usually  each assessor comes to nist with ideas for topics based on his or her own interests  and searches the document collection using nist's prise system to estimate the likely number of relevant documents per candidate topic. the nist trec team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors.
1.1 relevance judgments
the relevance judgments are what turns a set of documents and topics into a test collection. given a set of relevance judgments  the ad hoc retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. trec usually uses binary relevance judgments-either a document is relevant to the topic or it is not. to define relevance for the assessors  the assessors are told to assume that they are writing a report on the subject of the topic statement. if they would use any information contained in the document in the report  then the  entire  document should be marked relevant  otherwise it should be marked irrelevant. the assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.
   relevance is inherently subjective. relevance judgments are known to differ across judges and for the same judge at different times . furthermore  a set of static  binary relevance judgments makes no provision for the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. despite the idiosyncratic nature of relevance  test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments .
   the relevance judgments in early retrieval test collections were complete. that is  a relevance decision was made for every document in the collection for every topic. the size of the trec document sets makes complete judgments utterly infeasible-with 1 documents  it would take over 1 hours to judge the entire document set for one topic  assuming each document could be judged in just 1 seconds. instead  trec uses a technique called pooling  to create a subset of the documents  the  pool   to judge for a topic. each document in the pool for a topic is judged for relevance by the topic author. documents that are not in the pool are assumed to be irrelevant to that topic. pooling is valid when enough relevant documents are found to make the resulting judgment set approximately complete and unbiased.
   the judgment pools are created as follows. when participants submit their retrieval runs to nist  they rank their runs in the order they prefer them to be judged. nist chooses a number of runs to be merged into the pools  and selects that many runs from each participant respecting the preferred ordering. for each selected run  the top x documents per topic are added to the topics' pools. since the retrieval results are ranked by decreasing similarity to the query  the top documents are the documents most likely to be relevant to the topic. many documents are retrieved in the top x for more than one run  so the pools are generally much smaller than the theoretical maximum of x¡Á the-number-of-selected-runs documents  usually about 1 the maximum size .
   the use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents.
   zobel demonstrated that the quality of the pools  the number and diversity of runs contributing to the pools and the depth to which those runs are judged  does affect the quality of the final collection . he also found that the trec collections were not biased against unjudged runs. in this test  he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. for the trec-1 ad hoc collection  he found that using the unique relevant documents increased a run's 1 point average precision score by an average of 1 %. the maximum increase for any run was 1 %. the average increase for the trec-1 ad hoc collection was somewhat higher at 1 %.
   a similar investigation of the trec-1 ad hoc collection showed that every automatic run that had a mean average precision score of at least 1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents . that investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs  an effect noted by the organizers of the ntcir  nacsis test collection for evaluation of information retrieval systems  workshop who performed their own manual runs to supplement their pools .
   the uniquely-retrieved-relevant-documents test can fail to indicate a problem with a collection if all the runs that contribute to the pool share a common bias-preventing such a common bias is why a diverse run set is needed for pool construction. while it is not possible to prove that no common bias exists for a collection  no common bias has been demonstrated for any of the trec collections until this year. the retrieval test collection built in the trec 1 hard and robust tracks has a demonstrable bias toward documents that contain topic title words. that is  a very large fraction of the known relevant documents for that collection contain many topic title words despite the fact that documents with fewer topic title words that would have been judged relevant exist in the collection.  details are given in the robust track overview paper later in this volume . 
   the bias results from pools that are shallow relative to the number of documents in the collection. many otherwise diverse retrieval methodologies sensibly rank documents that have lots of topic title words before documents containing fewer topic title words since topic title words are specifically chosen to be good content indicators. but a large document set will contain many documents that include topic title words. to produce an unbiased  reusable collection  traditional pooling requires sufficient room in the pools to exhaust the spate of title-word documents and allow documents that are not title-word-heavy to enter the pool. the robust track contained one run that did not concentrate on topic title words and could thus demonstrate the bias in the other runs. no such  smoking-gun  run exists for the collections built in the trec 1 and 1 terabyte track  but a similar bias must surely exist in these collections. the biased collections are still useful for comparing retrieval methodologies that have a matching bias  and the results of the 1 tracks are valid since the runs were used to build the collections   but results on these collections need to be interpreted judiciously when comparing methodologies that do not emphasize topic title words.
1 evaluation
retrieval runs on a test collection can be evaluated in a number of ways. in trec  ad hoc tasks are evaluated using the treceval package written by chris buckley of sabir research . this package reports about 1 different numbers for a run  including recall and precision at various cut-off levels plus singlevalued summary measures that are derived from recall and precision. precision is the proportion of retrieved documents that are relevant  number-retrieved-and-relevant/number-retrieved   while recall is the proportion of relevant documents that are retrieved  number-retrieved-and-relevant/number-relevant . a cut-off level is a rank that defines the retrieved set; for example  a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. the treceval program reports the scores as averages over the set of topics where each topic is equally weighted.  the alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important. 
   precision reaches its maximal value of 1 when only relevant documents are retrieved  and recall reaches its maximal value  also 1  when all the relevant documents are retrieved. note  however  that these theoretical maximum values are not obtainable as an average over a set of topics at a single cut-off level because different topics have different numbers of relevant documents. for example  a topic that has fewer than ten relevant documents will have a precision score at ten documents retrieved less than 1 regardless of how the documents are ranked. similarly  a topic with more than ten relevant documents must have a recall score at ten documents retrieved less than 1. at a single cut-off level  recall and precision reflect the same information  namely the number of relevant documents retrieved. at varying cut-off levels  recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.
   of all the numbers reported by treceval  the interpolated recall-precision curve and mean average precision  non-interpolated  are the most commonly used measures to describe trec retrieval results. a recall-precision curve plots precision as a function of recall. since the actual recall values obtained for a topic depend on the number of relevant documents  the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. the particular interpolation method used is given in appendix a  which also defines many of the other evaluation measures reported by treceval. recallprecision graphs show the behavior of a retrieval run over the entire recall spectrum.
   mean average precision  map  is the single-valued summary measure used when an entire graph is too cumbersome. the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . the mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. the average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents  and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. geometrically  average precision is the area underneath a non-interpolated recall-precision curve.
   as trec has expanded into tasks other than the traditional ad hoc retrieval task  new evaluation measures have had to be devised. indeed  developing an appropriate evaluation methodology for a new task is one of the primary goals of the trec tracks. the details of the evaluation methodology used in a track are described in the track's overview paper.
1 trec 1 tracks
trec's track structure was begun in trec-1  1 . the tracks serve several purposes. first  tracks act as incubators for new research areas: the first running of a track often defines what the problem really is  and a track creates the necessary infrastructure  test collections  evaluation methodology  etc.  to support research on its task. the tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. finally  the tracks make trec attractive to a broader community by providing tasks that match the research interests of more groups.
   table 1 lists the different tracks that were in each trec  the number of groups that submitted runs to that track  and the total number of groups that participated in each trec. the tasks within the tracks offered for a given trec have diverged as trec has progressed. this has helped fuel the growth in the number of participants  but has also created a smaller common base of experience among participants since each participant tends to submit runs to a smaller percentage of the tracks.
   this section describes the tasks performed in the trec 1 tracks. see the track reports later in these proceedings for a more complete description of each track.
1 the enterprise track
trec 1 was the first year for the enterprise track  which is an outgrowth of previous years' web track tasks. the purpose of the track is to study enterprise search: satisfying a user who is searching the data of an organization to complete some task. enterprise data generally consists of diverse types such as published table 1: number of participants per track and total number of distinct participants in each trec
tracktrec1111111ad hoc1111------routing111--------interactive--111111---spanish--11---------confusion---1---------merging---1---------filtering---11111---chinese----1--------nlp----1--------speech-----11-----xlingual-----1111---high prec-----1-------vlc------1------query------11-----qa-------111111web-------11111-video--------	-11---novelty--------	--111-genomics--------	---111hard--------	---111robust--------	---111terabyte--------	----11enterprise--------	-----1spam--------	-----1participants1111111reports  intranet web sites  and email  and the goal is to have search systems deal seamlessly with the different data types.
   the document set used in the track was the w1c test collection  see http://research. microsoft.com/users/nickcr/w1c-summary.html . this collection  created by nick craswell  was created from a crawl of the world-wide web consortium web site and includes email discussion lists  web pages  and the extracted text from documents in various formats  such as pdf  postscript  word  powerpoint  etc. . because of the technical nature of the documents  and hence the topics that could be asked against those documents  topic development and relevance judging for the enterprise track were performed by the track participants.
   the track contained three search tasks: a known-item search for a particular message in the email lists archive; an ad hoc search for the set of messages that pertain to a particular discussion covered in the email lists; and a search-for-experts task. the motivation for the expert-finding task is being able to determine who the correct contact person for a particular matter is in a large organization. for the track task  the topics were the names of w1c working groups  e.g.   web services choreography    and the correct answers were assumed to be the members of that particular working group. systems were to return the names of the people themselves  not documents that stated the people were members of the particular working group.
   twenty-three groups participated in the enterprise track  1 groups in the discussion search task  1 groups in the expert-finding task  and 1 groups in the known-item search task. while groups generally attempted to exploit the thread structure and quoted material in the email tasks  the effectiveness of the searches was generally dominated by traditional content factors. thus  more work is needed to understand how best to support discussion search.
1 the genomics track
the goal of genomics track is to provide a forum for evaluation of information retrieval systems in the genomics domain. it is the first trec track devoted to retrieval within a specific domain  and thus a subgoal of the track is to explore how exploiting domain-specific information improves retrieval effectiveness. as in trec 1  the 1 genomics track contained an ad hoc retrieval task and a categorization task.
   the document set for the ad hoc task was the same corpus as was used in the 1 genomics ad hoc task  a 1-year subset  1 to 1  of medline  the bibliographic database of biomedical literature maintained by the us national library of medicine. the corpus contains about 1 million medline records  which include title and abstract as well as other bibliographic information  and is about 1gb of data. the topics were developed from interviews from real biologists who were asked to fill in a  generic topic template  or gtt. the gtts were used to produced more structured topics than traditional trec topics so systems could make better use of resources such as ontologies and databases. the 1 test topics contain ten instances for each of the following five gtts  where the underlined portions represent the template slots:
1. find articles describing standard methods or protocols for doing some sort of experiment or proce-

dure.
1. find articles describing the role of a gene involved in a given disease.

1. find articles describing the role of a gene in a specific biological process.

1. find articles describing interactions  e.g.  promote  suppress  inhibit  etc.  between two or more genes

in the function of an organ or in a disease.

1. find articles describing one or more mutations of a given gene and its biological impact.

for example  a topic derived from the mutation gtt might be provide information about mutation of ret in thyroid function. relevance judgments were made by assessors with backgrounds in biology using a threepoint scale of definitely relevant  probably relevant  and not relevant. both definitely relevant and probably relevant were considered relevant when computing evaluation scores.
   the genomics domain has a number of model organism database projects in which the literature regarding a specific organism  such as a mouse  is tracked and annotated with the function of genes and proteins. the classification task used in the 1 track focused on one of the tasks in this curation process  the  document triage  task. the document triage task is essentially a filtering task in which a document passes through the filter only if it should receive more careful examination with respect to a specific category. four different categories were used in the track: gene ontology  go  annotation  tumor biology  embryologic gene expression  and alleles of mutant phenotypes. the document set was the same document set used in the trec 1 genomics categorization task  the full text articles from a two-year span of three journals made available to the track through highwire press. the truth data for the task came from the actual annotation process carried out by the human annotators in the mouse genome informatics  mgi  system.
   the genomics track had 1 participants  with 1 groups participating in the ad hoc search task and 1 participating in the categorization task. retrieval effectiveness was roughly equivalent across the different topic types in the ad hoc search task. in contrast  system effectiveness was strongly dependent on the specific category in the triage task.
1 the hard track
the goal of the  high accuracy retrieval from documents   hard  track is improving retrieval system effectiveness by personalizing the search to the particular user. for the 1 track  the method for obtaining information about the user was through clarification forms  a limited type of interaction between the system and the searcher.
   the underlying task in the hard track is an ad hoc retrieval task. participants first submit baseline runs using the topic statements as is. they may then collect information from the searcher  the assessor who judged the topic  using clarification forms. a clarification form is a single  self-contained html form created by the participating group and specific to a single topic. there were no restrictions on what type of data could be collected using a clarification form  but the searcher spent no more than three minutes filling out any one form. an example use of a clarification form is to ask the searcher which of a given set of terms are likely to be good search terms for the topic. finally  participants make new runs using the information gathered from clarification forms.
   the same document set  topics  and hence relevance judgments were used in both the hard and robust tracks. the document set was the aquaint corpus of english news text  ldc catalog number ldc1  see www.ldc.upenn.edu . the 1 test topics were a subset of the topics used in previous trec robust tracks  which had been demonstrated to be difficult topics for systems when used on the trec disks 1 document set. relevance judgments were performed by nist assessors based on pools of both hard and robust runs.
   the motivation for sharing the test collection between the two tracks was partly financial-nist did not have the resources to create a separate collection for each track-but sharing also had technical benefits as well. one hypothesis as to why previous years' hard tracks did not demonstrate as large a difference in effectiveness between baseline and final runs as expected was that many of the topics in those test sets did not really need clarification. using topics that had been shown to be difficult in the past was one way of constructing a test set that had room for improvement. the design also allows direct comparison between the largely automatic methods used in the robust track with the limited searcher feedback of the hard track.
   sixteen groups participated in the hard track. the majority of runs that used clarification forms did improve over their corresponding baseline runs  and a few such runs showed noticeable improvement. while this supports the hypothesis that some forms of limited user interaction can be effective in improving retrieval effectiveness  many questions regarding how best to use it remain. note  for example  that the best automatic run from the robust track  that used no interaction  was more effective than any of the automatic runs from the hard track.
1 the question answering  qa  track
the goal of the question answering track is to develop systems that return actual answers  as opposed to ranked lists of documents  in response to a question. the main task in the trec 1 track was very similar to the trec 1 task  though there were additional tasks as well in trec 1.
   the questions in the main task were organized into a set of series. a series consisted of a number of  factoid   questions with fact-based  short answers  and list questions that each related to a common  given target. the final question in a series was an explicit  other  question  which systems were to answer by retrieving information pertaining to the target that had not been covered by earlier questions in the series. the score for a series was computed as a weighted average of the scores for the individual questions that comprised it  and the final score for a run was the mean of the series scores.
   the document set used in the track was again the aquaint corpus. the test set consisted of 1 series of questions where the target was either a person  an organization  an entity to be defined  e.g.   kudzu    or an event. events were new to the trec 1 task.
   one of the concerns expressed at both the sigir 1 ir1qa workshop and the qa track workshop at the trec 1 meeting was a desire to build infrastructure that would allow a closer examination of the role document retrieval techniques play in supporting qa technology. to this end  participants in the main task were required to submit a document ranking of the documents their system used in answering the question for each of 1 individual questions  not series . while not all qa systems produce a ranked list of documents as an initial step  some ranking  even if it consisted of only a single document  was still required. the submitted document rankings were pooled as in a traditional ad hoc task  and nist assessors judged the pools using  contains an answer to the question  as the definition of relevant. the judged pools thus give the number of instances of correct answers in the collection  a statistic not computed for other qa test sets. the ranked lists will also support research on whether some document retrieval techniques are better than others in support of qa.
   the relationship task was an optional second task in the track. the task was based on a pilot evaluation that was run in the context of the arda aquaint program  see http://trec.nist.gov/ data/qa/add qaresources.html . aquaint defined a relationship as the ability of one entity to influence another  including both the means to influence and the motivation for doing so. eight spheres of influence were noted  including financial  movement of goods  family ties  communication pathways  organizational ties  co-location  common interests  and temporal. systems were given a topic statement that set the context for a final question asking about one of the types of influence. the system response was a set of  information nuggets  that provided the evidence  or lack thereof  for the relationship hypothesized in the question. the relationship task test set contained 1 topics. submissions to the relationship task were allowed to be either automatic  no manual processing at all  or manual.
   thirty-three groups participated in the main task  including three groups that performed only the document ranking task. six groups participated in the relationship task as well. the document ranking task results demonstrated only a weak correlation between the effectiveness of the initial document ranking as measured by r-precision and the ability of the system to answer factoid questions.
1 the robust track
the robust track looks to improve the consistency of retrieval technology by focusing on poorly performing topics. previous editions of the track have demonstrated that average effectiveness masks individual topic effectiveness  and that optimizing standard average effectiveness usually harms the already ineffective topics.
   the task in the track is an ad hoc retrieval task where effectiveness is measured as a function of worstcase behavior. measures of poor performance used in earlier tracks were problematic because they are relatively unstable when used with as few as 1 to 1 topics. a new measure developed during the final analysis of the trec 1 robust track results appears to give appropriate emphasis to poorly performing topics in addition to being stable with as few as 1 topics. this  gmap  measure is based on a geometric  rather than arithmetic  mean of average precision over a set of topics  and was the main effectiveness measure used in this year's track.
as discussed in the hard track section  the hard and robust tracks used the same test collection in
1. the collection consists of the aquaint document set and 1 topics that had been used in previous years' robust tracks. the 1 topics were topics that had low median effectiveness  across trec submissions  when run against trec disks 1 and are therefore considered difficult topics. the topics were selected from a larger set by choosing only those topics that had at least three relevant documents in the aquaint collection as judged by nist assessors. different assessors judged the topics this year against the aquaint document set from those that initially judged the topics against the disks 1 collection.
   as in the robust 1 track  a second requirement in the track was for systems to submit a ranked list of the topics ordered by perceived difficulty. a system assigned each topic a number from 1 to 1 where the topic assigned 1 was the topic the system believed it did best on  the topic assigned 1 was the topic the system believed it did next best on  etc. this task is motivated by the hope that systems will eventually be able to use such predictions to do topic-specific processing. the quality of a prediction is measured using the area between two curves each of which plots the map score computed over all topics except the run's worst x topic. x ranges from 1  so  all topics are included  to 1  so  the average is computed over the best half of the topics . in one curve  the worst topics are defined from the run's predictions  while in the second curve the worst topics are defined using the actual average precision scores.
   seventeen groups participated in the robust track. as in previous robust tracks  the most effective strategy was to expand queries using terms derived from resources external to the target corpus. the relative difficulty of different topics  as measured by the average score across runs  differed between the disks 1 collection and the aquaint collection.
1 the spam track
the spam track is a second new track in 1. the immediate goal of the track is to evaluate how well systems are able to separate spam and ham  non-spam  when given an email sequence. since the primary difficulty in performing such an evaluation is getting appropriate corpora  longer term goals of the track are to establish an architecture and common methodology for a network of evaluation corpora that would provide the foundation for additional email filtering and retrieval tasks.
   there are a number of reasons why obtaining appropriate evaluation corpora is difficult. obviously making real email streams public is not an option because of privacy concerns. yet creating artificial corpora is also difficult. most of the modifications to real email streams that would protect the privacy of the recipients and senders also compromises the information used by classifiers to distinguish between ham and spam. the track addressed this problem by having several corpora  some public and some private. the track also made use of a test jig developed for the track that takes an email stream  a set of ham/spam judgments  and a classifier  and runs the classifier on the stream reporting the evaluation results of that run based on the judgments.
   track participants submitted their classifiers to nist. track coordinator gord cormack and his colleagues at the university of waterloo used the jig to evaluate the submitted classifiers on the private corpora. in addition  the participants used the jig themselves to evaluate the same classifiers on the public corpora and submitted the raw results from the jig on that data back to nist.
   several measures of the quality of a classification are reported for each combination of corpus and classifier. these measures include ham misclassification rate: the fraction of ham messages that are misclassified as spam; spam misclassification rate: the fraction of spam messages that are misclassified as ham; ham/spam learning curve : error rates as a function of the number of messages processed;
roc curve: roc  receiver operating characteristic  curve that shows the tradeoff between ham/spam
misclassification rates;
roc ham/spam tradeoff score: the area above an roc curve. this is equivalent to the probability that the spamminess score of a random ham message equals or exceeds the spamminess score of a random spam message.
   thirteen groups participated in the spam track. in addition  the organizers ran several existing spam classifiers on the various corpora and report those results as well in the spam track section of appendix a. on the whole  the filters were effective  though each had a misclassification rate that was observable on even the smallest corpus  1 messages . steady-state misclassification rates were reached quickly and were not dominated by early errors  suggesting that the filters would continue to be effective in actual use.
1 the terabyte track
the goal of the terabyte track is to develop an evaluation methodology for terabyte-scale document collections. the track also provides an opportunity for participants to see how well their retrieval algorithms scale to much larger test sets than other trec collections.
   the document collection used in the track was the same collection as was used in the trec 1 track: the gov1 collection  a collection of web data crawled from web sites in the .gov domain during early 1. this collection contains a large proportion of the crawlable pages in .gov  including html and text  plus extracted text of pdf  word and postscript files. the collection contains approximately 1 million documents and is 1 gb. while smaller than a full terabyte  this collection is at least an order of magnitude greater than the next-largest trec collection. the collection is distributed by the university of glasgow  see http://ir.dcs.gla.ac.uk/test collections/.
   the track contained three tasks  a classic ad hoc retrieval task  an efficiency task  and a named-pagefinding task. manual runs were encouraged for the ad hoc task since manual runs frequently contribute unique relevant documents to the pools. the efficiency and named page tasks required completely automatic processing only.
   the ad hoc retrieval task used 1 information-seeking topics created for the task by nist assessors. while systems returned the top 1 documents per topic so various evaluation strategies can be investigated  pools were created from the top 1 documents per topic.
   the efficiency task was an extension of the ad hoc task and was designed as a way of comparing the efficiency and scalability of systems given participants all used their own  different  hardware. the  topic  set was a sample of 1 queries mined from web search engine logs plus the title fields of the 1 topics used in the ad hoc task. systems returned a ranked list of the top 1 documents for each query plus reported timing statistics for processing the entire query set. to measure the effectiveness of the efficiency runs  the results for the 1 queries that corresponded to the ad hoc topic set were added to the ad hoc pools and judged by the nist assessors during the ad hoc judging.
   since the document set used in the track is a crawl of a cohesive part of the web  it can support investigations into tasks other than information-seeking search. one of the tasks that had been performed in the web track in earlier years was a named-page finding task  in which the topic statement is a short description of a single page  or very small set of pages   and the goal is for the system to return that page at rank one. the terabyte named page task repeated this task using the gov1 collection.
   nineteen groups participated in the track  including 1 groups participating in the ad hoc task  1 groups in the efficiency task  and 1 groups in the named page task. while there was a wide spread in both efficiency and effectiveness across groups  runs submitted by the same group do demonstrate that devoting more queryprocessing time can increase retrieval effectiveness.
1 the future
a significant fraction of the time of one trec workshop is spent in planning the next trec. two of the trec 1 tracks  the hard and robust tracks  will be discontinued as tracks in trec 1. a variant of the hard track's clarification form task will continue as a subtask of the question answering track; the evaluation methodology developed in the robust track will be incorporated in other tracks with ad hoc tasks. the discontinued tracks make room for two new tracks to begin in trec 1. the blog track will explore information seeking behavior in the blogosphere. the goal in the legal track is to develop search technology that meets the needs of lawyers to engage in effective discovery in digital document collections.
acknowledgements
special thanks to the track coordinators who make the variety of different tasks addressed in trec possible. the analysis of the pools from the hard/robust tracks and the terabyte track was done in collaboration with chris buckley and ian soboroff.
