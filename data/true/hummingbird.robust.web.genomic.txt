   hummingbird participated in 1 tasks of trec 1: the ad hoc task of the robust retrieval track  find at least one relevant document in the first 1 rows from 1gb of news and government data   the navigational task of the web track  find the home or named page in 1 million pages  1gb  from the .gov domain   the topic distillation task of the web track  find key resources for topics in the first 1 rows from home pages of .gov   and the primary task of the genomics track  find all records focusing on the named gene in 1gb of medline data . in the ad hoc task  searchserver found a relevant document in the first 1 rows for 1 of the 1 new short  title-only  topics. in the navigational task  searchserver returned the home or named page in the first 1 rows for more than 1% of the 1 queries. in the distillation task  a searchserver run found the most key resources in the first 1 rows of the submitted runs from 1 groups.
1	introduction
hummingbird searchserver1 is an indexing  search and retrieval engine for embedding in windows and unix information applications. searchserver  originally a product of fulcrum technologies  was acquired by hummingbird in 1. founded in 1 in ottawa  canada  fulcrum produced the first commercial application program interface  api  for writing information retrieval applications  fulcrum¡ãr ful/texttm. the searchserver kernel is embedded in many hummingbird products  including searchserver  an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.
   searchserver supports a variation of the structured query language  sql   searchsqltm  which has extensions for text retrieval. searchserver conforms to subsets of the open database connectivity  odbc  interface for c programming language applications and the java database connectivity  jdbc  interface for java applications. almost 1 document formats are supported  such as word  wordperfect  excel  powerpoint  pdf and html.
   searchserver works in unicode internally  and supports most of the world's major character sets and languages. the major conferences in text retrieval evaluation  trec   clef  and ntcir   have provided opportunities to objectively evaluate searchserver's support for more than a dozen languages.
   this paper looks at experimental work with searchserver for robust retrieval  robustness of ad hoc search across topics   web navigation  find the one page the user wanted  i.e. a known-item search task   web distillation  find key resource pages for broad topics   and genomic retrieval  a domain-specific task . for the submitted runs in august 1  an experimental post-1.x development build of searchserver was used. 1	robust retrieval
the document set of the trec 1 robust retrieval track was a subset of the news and government data of trec disks 1 and 1. it consisted of 1 documents totaling 1 1bytes  1gb . the average document size was 1 bytes. for more information  see the track overview paper.
   for this ad hoc task  participants were asked to focus not just on mean average precision but on at least one other measure indicative of  robustness  across results  such as the number of topics for which at least one relevant was retrieved in the first 1 rows.
1	indexing
the custom text reader called ctrec  described in our first trec paper   already supported detailed handling of the trec disk collections. for example  it allowed indexing of text following particular tags  such as  headline  and  text   and disabled indexing for text surrounded by other tags  such as  page ... /page   and for the tags themselves. as this year's guidelines did not restrict the fields allowed for indexing  we used the /k option of ctrec to allow indexing of text tagged as keywords  in particular  text tagged by  in  or  subject  in the case of the disks used this year . past experiments suggest that this detailed handling does not affect the results much.
   we used the mygov.stp stopword list  1 english stopwords  first used for a web task last year . the option to support inflections from lexical english stemming was enabled. we also experimented with an option to construct term vectors for result list clustering for this task.
1	searching
the submitted humr1d run was a  plain  searchserver run on the description field of each topic. it used searchserver's intuitive searching  i.e. the is about predicate of searchsql . here is an example searchsql query for topic 1:
select relevance 'v1'  as rel  docno
from robust1
where ft text is about 'commercial harvesting of marine vegetation such as algae  seaweed and kelp for food and drug purposes.'
order by rel desc;
   searchserver's relevance value calculation is the same as described last year . briefly  searchserver dampens the term frequency and adjusts for document length in a manner similar to okapi  and dampens the inverse document frequency using an approximation of the logarithm. searchserver's relevance values are always an integer in the range 1 to 1.
   before the queries were run  various set statements were issued.  set max search rows 1  ensured the resulting working table would contain at most 1 rows. inflections from english stemming were enabled by  set vector generator 'word!ftelp/lang=english/base/noalt | * | word!ftelp/lang=english/inflect'    for more details on stemming for several european languages  see our clef paper  . the importance of document length to the relevance value calculation was set with  set relevance dlen imp 1   scale of 1 to 1 .
   we automatically removed  query stop words  such as  find    relevant  and  document  from the topics before presenting them to searchserver  i.e. words which are not stop words in general but were commonly used in previous years' trec and clef topics as general instructions  this year's topics were not reviewed . an evaluation in last year's clef paper  found this step to be of only minor impact in several european languages including english.
   the submitted humr1t run was the same as humr1d except that the title field of the topic was used instead of the description. for example  for topic 1  the where clause was just  where ft text is about 'marine vegetation'  . this run represented a  plain  searchserver run for title queries.
   the submitted humr1de run used query expansion from blind feedback. the first two rows of the humr1d run were used to find additional query terms. only terms appearing in at most 1% of the documents  based on the most common inflection of the term  were included. mathematically  the approach is similar to rocchio feedback with weights of one-half for the original query and one-quarter for each of the 1 expansion rows. this was the same blind feedback approach as used for arabic experiments at trec last year  except that we just used 1 rows for expansion this time instead of 1  diagnostics on past clef and trec ad hoc tasks suggested fewer rows may be more effective  perhaps because most tasks have fewer relevant documents to feed back in per topic than the arabic task did . blind feedback from the top retrieved documents is often effective at increasing recall later in the result list without sacrificing early precision  important components of the average precision measure. however  the large number of additional query terms negatively impacts performance. in practice  users could manually add terms to the query rather than work blindly. for this run  the measure in mind was average precision.
   the submitted humr1dc run re-ordered the top 1 rows of humr1d so that the first 1 rows were from different clusters to see if that increased the chance of a relevant document in the top 1 rows. the steps were as follows:
   first  the parameter settings were the same as for humr1d except that  set max search rows 1  was used instead of 1. hence a relevant document had to appear in the top 1 for re-ordering to have a chance of moving it into the top 1.
   next  a result list clustering query was run on the 1 row working table  producing a set of up to 1 clusters. each of the 1 rows appeared in exactly one cluster; the number of rows in each cluster could differ. within each cluster  the rows were ordered by the original relevance value. the clusters themselves were ordered by the average relevance value of the rows of the cluster. the clustering was based on the term vectors of the documents built at index-time. other than its impact on which 1 documents were clustered  the query had no impact on the clustering.
   finally  a round-robin of the clusters was followed  with the row of highest remaining relevance score of each cluster placed into the final result.  in the submitted file  the first 1 digits after the decimal point are the relevance value of the document  and the 1th digit is the cluster number from 1. 
   the top 1 rows of humr1d and humr1dc should be the same except for the order. the first row for a topic in humr1d would appear somewhere in the top 1 for the topic in humr1dc. the other rows in the top 1 might differ.
   the final result of humr1dc had at most 1 rows per topic. we did not bother to pad to 1 rows. hence for this run there was a bias against measures which consider documents retrieved past 1 rows  such as recall and average precision. for this run  the measure in mind was the 'relevant in the top 1 rows' measure.
   the submitted humr1tc run was the same as humr1dc except that it was based on humr1t instead of humr1d.
1	results
for this task  there were 1  old  topics and 1  new  topics.
   the 1 old topics were selected  by the task organizers  from past years' ad hoc trec topics 1 to produce a set of  tough  topics  i.e. topics on which few systems produced a high precision score when they were originally used  though there may have been a bias against topics on which all systems produced a low score; the track overview paper may elaborate more . as they were already in the public domain  the guidelines allowed groups to continue to study these topics for this year's submissions  which might also help lead to techniques for improving results on tough topics. one must be cautious however at reading too much into results on these topics  even  statistically significant  results  because of the possibility that the techniques are tuned to this data.
   for the 1 new topics   automatic  systems were not allowed to be altered based on examination of the topics  so in that sense the results may be more meaningful. but there was no reason to expect these topics to be as  tough  as the specially-selected older set  it is very hard to predict which topics will be tough in advance  so for the purpose of this track  robustness across topics  there may not be enough challenging topics to distinguish the techniques.
   we separately list the results for each of these sets of topics  we do not bother to look at the combined scores . also  for the 1 new topics  the relevance assessors distinguished  highly relevant  documents from table 1: precision of submitted runs
runavgpp 1p 1p 1rec1rec1p r%rel1 humr1te-old 11%1%1%111%1humr1t-old11%1%1%111%1humr1tc-old11%1%1%111%1humr1de-old11%1%1%111%1humr1d-old11%1%1%111%1humr1dc-old11%1%1%111%1 humr1te-new 11%1%1%111%1humr1t-new11%1%1%111%1humr1tc-new11%1%1%111%1humr1de-new11%1%1%111%1humr1d-new11%1%1%111%1humr1dc-new11%1%1%111%1 humr1te-newh 11%1%1%111%1humr1t-newh11%1%1%111%1humr1tc-newh11%1%1%111%1humr1de-newh11%1%1%111%1humr1d-newh11%1%1%111%1humr1dc-newh11%1%1%111%1table 1: impact of clustering on percentage of topics with a relevant in top 1
experimentavgdiff1% confidencevs.1 largest diffs  topic d-old-rel1.1  1  1 1-1.1  1   1  1 d-new-rel1 1  1  1 1-1.1  1    1  1 t-old-rel1 1  1  1 1-1 1  1    1  1 d-newh-rel1 1  1  1 1-1.1  1    1  1 t-new-rel1 1  1  1 1-1 1  1    1  1 t-newh-rel1 1  1  1 1-1.1  1   1  1 just  relevant  documents. 1 of the 1 topics had at least one  highly relevant  document. we list the scores averaged over those 1 topics when just considering highly relevants as relevant  tagged with  newh  .
   table 1 gives an overview of several precision scores for each submitted run  also  in brackets  is an unsubmitted run  because of the 1-run submission limit  produced at the same time  humr1te  an analog of humr1de for titles . listed for each run are its mean average precision  avgp   the mean precision after
table 1: impact of blind feedback on average precision
experimentavgdiff1% confidencevs.1 largest diffs  topic  t-new-avgp 1  1  1 1-1.1  1   1  1 d-new-avgp1  1  1 1-1 1  1   1  1  t-newh-avgp 1  1  1 1-1.1  1   1  1  t-old-avgp 1  1  1 1-1.1  1   1  1 d-old-avgp1  1  1 1-1.1  1   1  1 d-newh-avgp1  1  1 1-1 1  1    1  1 table 1: impact of blind feedback on percentage of topics with a relevant in top 1
experimentavgdiff1% confidencevs.1 largest diffs  topic d-newh-rel1.1  1  1 1-1.1  1   1  1  t-newh-rel1 1  1  1 1-1.1  1   1  1 d-old-rel1 1  1  1 1-1.1  1    1  1  t-old-rel1  1  1  1 1-1.1  1   1  1  t-new-rel1  1  1  1 1-1.1  1    1  1 d-new-rel1 1  1  1 1-1 1  1    1  1 1  1 and 1 documents retrieved  p 1  p 1 and p 1 respectively   the mean interpolated precision at 1% and 1% recall  rec1 and rec1 respectively   the mean precision after r documents retrieved  p r  where r is the number of relevant documents for the topic  and the ratio of the number of topics with at least one relevant retrieved in the top 1 vs. the total number of topics  %rel1 .  definitions of the measures are in last year's paper   and they likely are also in an appendix of the conference proceedings.  it appears that for every measure listed  the score on the  new  topics is higher than the corresponding score for the  old  topics  i.e. as expected  the  old  topics were more challenging  on average .
for tables focusing on the impact of one particular difference in approach  the columns are as follows:
   experiment  indicates whether the title or description topics were used   t  or  d  respectively  and whether the score is based on the old topics   old    the new topics when treating all relevants the same   new    or the new topics just counting highly relevants as relevant   newh  .
   avgdiff  is the average  mean  difference in the score.
   1% confidence  is an approximate 1% confidence interval for the average difference calculated using efron's bootstrap percentile method1   using 1 iterations . if zero is not in the interval  the result is  statistically significant   at the 1% level   i.e. the feature is unlikely to be of neutral impact  though if the average difference is small  e.g.  1  it may still be too minor to be considered  significant  in the magnitude sense.
   vs.  is the number of topics on which the score was higher  lower and tied  respectively  with the feature enabled. these numbers should always add to the number of topics  1 or 1 .
   1 largest diffs  topic   lists the two largest differences in the score  based on the absolute value  with each followed by the corresponding topic number in brackets  the old topic numbers range from 1 to 1 and the new topic numbers from 1 to 1 .
   table 1 shows the impact of the clustering-based technique on the percentage of topics with a relevant in the first 1 rows. for description queries  this is based on subtracting the scores of humr1d from humr1dc  and for the title queries  subtracting the scores of humr1t from humr1tc. as you can see  there was a net gain of 1 topics with a relevant in the top 1 on the old description queries  1 gained but 1 lost   though this was not statistically significant  and the loss of 1 on the old title queries was statistically significant. on the new queries  the finding was similar; the differences were not significant on the description queries but were on the title queries  both when counting all relevants or just highly relevants.
   table 1 shows the impact of the blind feedback technique on the average precision score  based on subtracting humr1d from humr1de  and humr1t from  unsubmitted run  humr1te . the increase was statistically significant for 1 of the 1 cases  the exception being for highly relevants on the new description topics.
   table 1 shows the impact of the same blind feedback technique on the perentage of topics with a relevant in the first 1 rows  based on the same runs as table 1 . none of the impacts were statistically significant.
table 1: examples of url type and depth values
urltypedepthdepth termhttp://nasa.gov/root1urldepthahttp://www.nasa.gov/root1urldepthahttp://jpl.nasa.gov/root1urldepthabhttp://fred.jpl.nasa.gov/root1urldepthabchttp://nasa.gov/jpl/subroot1urldepthabhttp://nasa.gov/jpl/fred/path1urldepthabchttp://nasa.gov/index.htmlroot1urldepthahttp://nasa.gov/fred.htmlfile1urldepthabtable 1: number of pages of each url type and depth
	type	#pages	depth	#pages	depth	#pages
	root	1	1	1 1
	subroot	1	1 1	1
	path	1	1 1	1
	file	1 1	1	1 1
	1 1+	1
   for the plain searchserver runs  more topics found a relevant in the first 1 rows using the titles than the descriptions  1 to 1 for the old topics  1 to 1 for the new topics  though tied at 1 when restricting to highly relevants  as per table 1 . these results might suggest that shorter queries are more  robust   perhaps extra details throw off a system more often than missing details  even though the longer queries score higher on the other listed measures which reward recall more . however  these changes in the number of topics with a relevant in the first 1 rows did not pass a significance test.
1	web retrieval
both tasks of the trec 1 web track used the same .gov collection as last year. it consists of pages downloaded from the .gov domain of the world wide web in early 1. uncompressed  it is 1 1bytes  1gb  and a total of 1 1 documents. the average document size is 1bytes. for more information on the .gov collection  see .
1	indexing
the indexing approach was the same as described in last year's paper   except that a newer version of the software was used which may have contained an updated english lexicon for stemming .
   briefly: in addition to full-text indexing  the custom text reader ctrec populated particular columns such as title  if any   url  url type and url depth. the url type was set to root  subroot  path or file  based on the convention which worked well in trec 1 for the twente/tno group  on the entry page finding task  also known as the home page finding task . the url depth was set to a term indicating the depth of the page in the site. table 1 contains url types and depths for example urls  and table 1 shows the number of .gov pages of each url type and depth. the exact rules we used are given in last year's paper .
1	searching
even though the 1 web tasks are potentially quite different  the navigational task is a known-item task  one right answer   while the topic distillation task is focused on distilling broad topics to key resource pages   we used the same techniques for both tasks for each of the 1 submitted runs  and most of the techniques used were the same as last year . this allows us to compare the impact of the techniques on different tasks.
   the submitted humnp1l and humtd1l runs used the same approach as the diagnostic base run described in last year's paper  which was just to search the content  ft text column  using the is about predicate  i.e. the same approach as used for the  plain  runs of the robust task .
   the submitted humnp1pl and humtd1pl runs used the same approach as last year's hum1pd run. below is an example searchsql query. the queries differed from humnp1l and humtd1l in that that properties and phrases in properties were given a little extra weight.  the all props column contained the title  url  first heading and some meta tags  but not most of the document content; see last year's paper for the details.  note that the ft text column also indexed all of the properties except for the url.
select relevance 'v1'  as rel  docno
from gov
where
 all props contains 'visiting pandas national zoo' weight 1  or
 all props is about 'visiting pandas national zoo' weight 1  or
 ft text is about 'visiting pandas national zoo' weight 1 
order by rel desc;
   the contains predicate does phrase searching  so the listed terms would have to occur adjacently in the specified order  except stop words .  set phrase distance 1  was previously specified so that there could be up to 1 characters between adjacent terms  plus additional whitespace . by default  the contains predicate does exact searching  i.e. no inflections from stemming   though some unicodebased normalizations  e.g. decompositions and conversion to upper-case  are still done. the motivation for including the query as a phrase was that it seemed the query might often be in the title or other property information of the document  e.g. a query in mind was  washington state legislature   which was not one of the 1 official queries last year  . the phrase searching was just given one-tenth the weight of content searching for relevance ranking purposes. experiments on the trec 1 entry page finding task suggested a small weight was helpful  on average  but a strong weight had a negative impact.
   the is about predicate uses searchserver's intuitive searching. it by default matches inflections froms english stemming and just requires one of the terms to have a match. it was used with weight 1 on the all props column to increase the ranking of documents with query terms in the title or other property information. it was used with weight 1 on the ft text column  which represents the external document . again  these weights were chosen based on what worked well on the trec 1 entry page finding task.
   the submitted humnp1upl and humtd1upl runs used the same approach as last year's hum1upd run. the 'u' indicates a higher weight was given to urls of particular type and depth. see last year's paper for an example of the searchsql syntax .
   the submitted humnp1uhpl and humtduhpl runs used the same approach as last year's hum1uhp run except for using a document length importance of 1 instead of 1  1 was used for all submitted web runs this year . the 'h' indicates an even higher weight was given to url type  the 1 terms of weight 1 were given weight 1 . on the trec 1 entry page finding task  the stronger url type weights gave similar mrr scores to the lower ones.
   the submitted humnp1up and humtd1up runs were the same as humnp1upl and humtd1upl  respectively  except that linguistic expansion from english stemming was disabled  i.e. matching of inflections was disabled  by  set vector generator ''  .
   for the navigational  humnp1*  runs  the statement  set max search rows 1  was previously executed so that the working table would contain at most 1 rows  whereas for the topic distillation  humtd1*  runs  the statement  set max search rows 1  was previously executed.
table 1: scores of submitted navigational runs

humnp1up11%1%11%1%11%1%humnp1upl11%1%11%1%11%1%humnp1pl11%1%11%1%11%1%humnp1uhpl11%1%11%1%11%1%humnp1l11%1%11%1%11%1%table 1: impact of submitted navigational techniques on reciprocal rank
experimentavgdiff1% confidencevs.1 largest diffs  topic hp u  upl - pl 1  1  1 1-1.1  1   1  1 hp p  pl - l 1  1  1 1-1.1  1   1  1 hp l  upl - up 1  1  1 1-1.1  1   1  1 hp h  uhpl - upl  1  1  1 1-1 1  1    1  1 np u  upl - pl  1  1  1 1-1 1  1   1  1 np p  pl - l 1  1  1 1-1 1  1   1  1 np l  upl - up  1  1  1 1-1.1  1    1  1 np h  uhpl - upl  1  1  1 1-1 1  1    1  1    for the web queries  no query terms were discarded  e.g. there was no expectation that discarding the words  find    relevant  and  document  would be beneficial  unlike for the robust task . of course  the index omitted a few stop words  e.g.  the    by   as previously mentioned.
   searchserver's relevance value calculation is the same as described for the robust task. additionally  when multiple predicates are combined  as was done for some of the web approaches  searchserver currently does not normalize by query length. for example  the url type clauses would have a lot less relative impact if the topic query contained 1 words instead of 1.
1	results
the evaluation measures are likely explained in an appendix of this volume. briefly  for the navigational task   reciprocal rank  for a topic is one divided by the rank in which the home or named page was found  using the smallest rank if there were duplicates of the page   or zero if the page was not found.  mean reciprocal rank   mrr  is the average of the reciprocal ranks over all the topics.  %top1  is the percentage of topics for which the home or named page was found in the first 1 rows.  %fail  is the percentage of topics for which the home or named page was not found in the first 1 rows. the topic distillation measures are the same as described previously in the robust section.
   table 1 shows the scores of the submitted navigational runs in descending order by mean reciprocal rank over all 1 queries. the hp columns show the scores just for the 1 home page queries. the np columns show the scores just for the 1 named page queries.  the topics did not state whether they were of hp or np type; that information was provided by the organizers after the submission date for use in analysis.  table 1 shows the impact when isolating each technique distinguishing the submitted navigational runs:
  the 'u' factor  extra weight for url type and depth  increased mrr dramatically on the home pages  1 points  but  like last year  was detrimental on the named pages  1 points . more diagnostics are below.
  the 'p' factor  extra weight for html properties and phrases in properties  increased mrr 1 points on both home and named pages. more diagnostics are below.
table 1: diagnostics of extra weight on document structure  navigational task  reciprocal rank 
experimentavgdiff1% confidencevs.1 largest diffs  topic hp v  vl-l 1  1  1 1-1.1  1   1  1 hp q  ql-l 1  1  1 1-1.1  1   1  1 hp qv  vql-l 1  1  1 1-1.1  1   1  1 hp v  vql-ql 1  1  1 1-1.1  1   1  1 hp q  vql-vl 1  1  1 1-1.1  1   1  1 hp other  pl-vql 1  1  1 1-1.1  1    1  1 np v  vl-l 1  1  1 1-1.1  1   1  1 np q  ql-l 1  1  1 1-1 1  1   1  1 np qv  vql-l 1  1  1 1-1 1  1   1  1 np v  vql-ql 1  1  1 1-1.1  1    1  1 np q  vql-vl 1  1  1 1-1 1  1    1  1 np other  pl-vql 1  1  1 1-1 1  1    1  1 table 1: diagnostics of extra weight on url structure  navigational task  reciprocal rank 
experimentavgdiff1% confidencevs.1 largest diffs  topic hp d1  d1pl-pl 1  1  1 1-1.1  1   1  1 hp d1  d1pl-pl 1  1  1 1-1.1  1   1  1 hp d1  d1pl-pl 1  1  1 1-1.1  1   1  1 hp d1  d1pl-pl 1  1  1 1-1.1  1   1  1 hp r1  r1pl-pl 1  1  1 1-1.1  1   1  1 hp r1  r1pl-pl 1  1  1 1-1.1  1   1  1 hp r1  r1pl-pl 1  1  1 1-1.1  1   1  1 np d1  d1pl-pl  1  1  1 1-1.1  1   1  1 np d1  d1pl-pl  1  1  1 1-1 1  1   1  1 np d1  d1pl-pl  1  1  1 1-1 1  1    1  1 np d1  d1pl-pl  1  1  1 1-1 1  1    1  1 np r1  r1pl-pl  1  1  1 1-1.1  1   1  1 np r1  r1pl-pl  1  1  1 1-1 1  1    1  1 np r1  r1pl-pl  1  1  1 1-1.1  1    1  1   the 'l' factor  linguistic expansion  inflections  from lexical english stemming  made little difference  on average .
  the 'h' factor  even more extra weight for url type  was detrimental even on the home page queries  even though it had a neutral impact on the trec 1 entry page task.
   table 1 isolates the components of the 'p' factor. 'v' denotes that the run included title is about  i.e. vector  matching with weight 1  and 'q' denotes that the run included title contains  i.e. phrase  matching with weight 1. adding the 'v' factor  to a full content search with weight 1  increased mrr significantly for both home pages and named pages  1 and 1 points respectively as per the  v  vl-l   rows . the 'q' factor had significant  though smaller  increases  1 and 1 points respectively as per the  q  ql-l   rows . if one of these was already done  adding the other still led to a significant increase except in the case of adding phrase matching to a vector match for named pages  as per the  np q  vql-vl   row . using the all props column instead of the title column did not lead to a further significant increase as per  other  pl-vql   rows. so for the 'p' factor  like last year  most of the benefit appears to have come from the title weighting  but unlike last year  both vector and phrase matching helped significantly  not just vector table 1: scores of submitted topic distillation runs
runavgpp 1p 1p 1rec1rec1p rtopicshumtd1upl11%1%1%111%1humtd1up11%1%1%111%1humtd1uhpl11%1%1%111%1humtd1pl11%1%1%111%1humtd1l11%1%1%111%1table 1: impact of topic distillation techniques on precision 1
experimentavgdiff1% confidencevs.1 largest diffs  topic u  upl - pl 1  1  1 1-1.1  1   1  1 p  pl - l 1  1  1 1-1.1  1   1  1 l  upl - up 1  1  1 1-1.1  1    1  1 h  uhpl - upl  1  1  1 1-1 1  1    1  1 matching  perhaps the topics this year happened to be part of the title of the desired page more often than last year .
   table 1 isolates the components of the 'u' factor. 'r' denotes the weight assigned to the url type values  root  subroot  path  and 'd' denotes the weight assigned to the url depth values  'u' was 'r1' and is in table 1 . a small weight on either the url depth or type increased the home page score substantially without a significant drop in the named page score  as per the 'd1' and 'r1' rows . so it may be reasonable to include a small weight on url structure in a general web page search system  regardless of the expected frequency ratio of home page and named page queries. higher weights may be reasonable if home page queries are expected to be a lot more common.
   table 1 shows the scores of the submitted topic distillation runs in descending order by precision 1. the humtd1upl run had the highest precision 1 score of any submitted run from the 1 groups  even though its score means it found on average just more than 1 key resource page in the first 1 rows  the judgements contained 1 key resource pages per topic on average . the topics were broad  e.g.  science  was an example in the task guidelines  and the top retrieved rows may have been filled with many more pages that were  relevant  to the topic even though they were not judged  key resources  by the assessors.
   tables 1  1 and 1 show the impact of the submitted topic distillation techniques on precision 1  average precision and precision r respectively:
  the 'u' factor  extra weight for url type and depth  increased precision 1 by 1 points and produced a statistically significant increase for all 1 examined measures. this is not surprising because the key resources were required to be home pages this year.
  the 'p' factor  extra weight for html properties and phrases in properties  did not have a significant impact on precision 1  but tables 1 and 1 show it led to a significant increase in the average
table 1: impact of topic distillation techniques on average precision
experimentavgdiff1% confidencevs.1 largest diffs  topic p  pl - l 1  1  1 1-1.1  1   1  1 u  upl - pl 1  1  1 1-1.1  1   1  1 l  upl - up 1  1  1 1-1.1  1    1  1 h  uhpl - upl  1  1  1 1-1 1  1    1  1 table 1: impact of topic distillation techniques on r-precision
experimentavgdiff1% confidencevs.1 largest diffs  topic u  upl - pl 1  1  1 1-1.1  1   1  1 p  pl - l 1  1  1 1-1.1  1   1  1 l  upl - up 1  1  1 1-1.1  1    1  1 h  uhpl - upl  1  1  1 1-1 1  1    1  1 precision and precision r measures.
  the 'l' factor  linguistic expansion  inflections  from lexical english stemming  made little difference for most topics except for some measures for topic 1   polygraphs  .
  the 'h' factor  even more extra weight for url type  had a significant negative impact on the examined measures.
   overall  the impacts on the distillation scores were much more like the impacts on the home page finding scores than the named page finding scores.
1	genomic retrieval
for the primary task of the genomics track  find all records focusing on the named gene   the medline data consisted of 1 documents  records   all in one file   trec-medline   of 1 1 bytes  1gb  uncompressed. the average record length was 1 bytes. more information should be in the track overview paper.
1	indexing
the ctrec text reader  described in the robust section  was enhanced to include a /m option for identifying the medline records  documents  during table expansion from the  trec-medline  file. for the individual records  the /p option of ctrec was used  i.e. we just passed through all of the text for indexing  including identifiers such as  ui    pmid    mh  etc.  maybe next year we will enhance the text reader to populate columns from particular fields  such as the title  allowing experiments with the record structure like we did for the web data. 
   a different stopfile  mynum.stp  was used for this task. it contained just one instruction  al =  1   which means to treat the digits 1 to 1 as alphabet characters. for example  this would cause the symbol  cdkn1a  to be indexed as 1 term instead of 1. experiments on the training queries found the scores were a little higher with this indexing change. the mynum.stp stopfile did not contain any stop words as the training queries did not seem to use much natural language.
punctuation characters  including hyphens and parentheses  were still treated as term separators.
1	searching
the submitted runs used is about queries based on combining just 1 of the 1 query fields: the 1 name fields  official gene name  preferred gene name  and the 1 symbol fields  official symbol  alias symbol  preferred symbol . the other 1 fields were omitted  preferred product  alias prot  product  because they were found to be harmful on the training topics. also  the species information was ignored for the submitted runs.
   the submitted humg1ns run gave equal weight to the five fields. an example searchsql query is below  from run humg1ns test topic 1 . the parentheses between query fields were just added for readability and did not affect the is about search:
table 1: precision of genomics runs
runavgpp 1p 1p 1rec1rec1p rtopicshumg1ns11%1%1%111%1humg1ns1.1.1%1%1%111%1base  diag. 11%1%1%111%1+1x phrases11%1%1%111%1select relevance 'v1'  as rel  docno
from med1n
where ft text is about 'activating transcription factor 1    atf1    hb1    creb1    treb1    cre-bp1'
order by rel desc
   the submitted humg1ns1 run gave 1 times the weight to the three symbol fields  by repeating them 1 times in the query  rather than using the weight clause   which was modestly helpful on the training topics  though not significantly so .
   inflections from stemming were disabled for both runs. the document length importance was set to 1 for both runs. more details of the relevance ranking are in the robust and web sections.
1	results
table 1 shows various scores of the submitted runs  humg1ns and humg1ns1  the column headings are explained in the robust section .
   at the conference  some groups found that filtering by species  organism  was helpful  presumably because of the artificial way the relevance assessments were created for this first genomics task. the nlm paper  described how to convert the species name in the topic statement to the mh field in the medline records  map 'homo sapiens' to 'human'  map 'mus musculus' to 'mice'  map 'rattus norvegicus' to 'rats'  map 'drosophila melanogaster' to 'drosophila' . the nrc reported that just 1 of the official right answers were discarded if restricting to fields of the same organism .
   the diagnostic  base run  is the same as humg1ns except that it adds a phrase-match restriction to just include documents of the specified species  e.g.  and  ft text contains 'mh - human' weight 1   was added to the query if the species was 'homo sapiens'. it was assigned  weight 1  so that it would not affect the relevance calculation. table 1 shows that the base run scored a 1 mean average precision  an increase of more than 1 points over humg1ns.
   the  +1x phrases  diagnostic run of table 1 additionally boosted the scores of records which contained any of the query fields as complete phrases  by use of the contains predicate. in the contains predicate  hyphenated terms match not just terms separated with different punctuation or white space  but also concatenations of the terms  e.g. a contains search for 'cre-bp1' would additionally match not just 'cre bp1 '  'cre bp1'  etc.  but also 'crebp1' . the where clause for topic 1 was
where   ft text is about 'activating transcription factor 1   
atf1    hb1    creb1    treb1    cre-bp1'
or  ft text contains 'activating transcription factor 1' weight 1 
or  ft text contains 'atf1' weight 1 
or  ft text contains 'hb1' weight 1 
or  ft text contains 'creb1' weight 1 
or  ft text contains 'treb1' weight 1 
or  ft text contains 'cre-bp1' weight 1   
and  ft text contains 'mh - human' weight 1 
   table 1 compares a number of diagnostic runs to the base run  always subtracting the base run's scores in average precision from the listed run . for example  the first row shows that the  +1x phrases  run table 1: impact of genomics techniques on average precision
experimentavgdiff1% confidencevs.1 largest diffs  topic +1x phrases1  1  1 1-1.1  1   1  1 +1x phrases1  1  1 1-1.1  1   1  1 +1x phrases1  1  1 1-1.1  1   1  1 1x sym1  1  1 1-1.1  1   1  1 idf squared  v1 1  1  1 1-1.1  1    1  1 1x sym1  1  1 1-1.1  1   1  1 phrases only1  1  1 1-1 1  1    1  1 dlen 1 1  1  1 1-1 1  1    1  1 stemming on 1  1  1 1-1 1  1    1  1 omit names 1  1  1 1-1 1  1   1  1 number parsing 1  1  1 1-1.1  1    1  1 all fields 1  1  1 1-1 1  1    1  1 vector species 1  1  1 1-1 1  1    1  1 omit symbols 1  1  1 1-1 1  1    1  1 terms count  1  1  1  1 1-1 1  1    1  1 omit species 1  1  1 1-1 1  1    1  1 hits count  1  1  1  1 1-1 1  1    1  1 scored on average 1 points higher than the base run  1 minus 1 is 1   and this difference was statisticially significant  see the robust section for a detailed explanation of the column headings of table 1 .
   phrasing: the  +1x phrases  and  +1x phrases  runs used 'weight 1' and 'weight 1' for the phrases instead of 'weight 1'  and they also produced significant 1 point gains. the  phrases only  run just used the phrases  dropping the is about predicate  and scored about the same as the base run on average  though with a lot of variance. the  number parsing  run used a table with the default parsing of alphanumerics  e.g.  cdkn1a  would be treated as 1 terms  cdkn  1  a  instead of 1   in a sense removing the natural phrasing of symbols  and the 1 point drop in score passed the significance test.  note that if the symbols matched as phrases when names did not  the effect would be the same as just increasing the symbol weight  described below   which may be why topic 1 shows a similar increase in table 1 for both phrases and symbol weighting.  there is probably room for improvement in this term matching area  e.g. a search for 'cdkn1a' will not match 'cdkn 1a' with the parsing rules used for this task .
   query fields: the  1x sym  and  1x sym  runs were the same as the diagnostic base run except that the symbols were each listed twice and five times  respectively  to boost their impact on the score. table 1 shows the 1 point gain of  1x sym  was statistically significant  while the 1 point gain of  1x sym  was not. just using the symbols  omitting the name fields  scored 1 points lower on average  as per the  omit names  run   but with a lot of variance. just using the names and not the symbols scored a significant 1 points lower  as per the  omit symbols  run . adding in the 1 other fields  preferred product  product  alias prot  scored a significant 1 points lower  as per the  all fields  run . overall  it appears the symbols are the most useful of the query fields for this task  though perhaps we're not making as effective use of the names as we could  as the phrase experiments suggested .
   relevance ranking: squaring the importance of inverse document frequency to the relevance calculation  by using searchserver relevance method 'v1' instead of 'v1'  scored 1 points higher  but did not quite pass the significance test  as per the listed  idf squared  v1   run. enabling document length normalization or matching of inflections from english stemming made little difference for this task as per the listed  dlen 1  and  stemming on  runs. simpler ranking techniques  such as just counting the number of query terms matched  relevance method '1'  or simply counting all the matches in a record  relevance method '1'  scored dramatically lower  1 and 1 points respectively  as per the listed  terms count  1   and  hits count  1   runs  indicating that a combination of term frequency dampening and inverse document frequency is still valuable for this task  though we have not separated the impact of these techniques .
   as previously mentioned  not restricting to the species given in the topic scored more than 1 points lower as per the listed  omit species  run  the difference of the humg1ns run and the base run . adding the species to the is about vector instead of using a strict contains match scored 1 points lower  as per the listed  vector species  run . at the conference it was stated that in a real task it can be useful to find the gene in different species  so these species results apparently are examples of misleading conclusions from the artificial nature of the judgements used this year.
