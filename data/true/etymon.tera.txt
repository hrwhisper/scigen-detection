   the trec 1 terabyte track evaluated information retrieval in largescale text collections  using a set of 1 million documents  1 gb . this paper gives an overview of our experiences with this collection and describes amberfish  the text retrieval software used for the experiments.
1 preface
this is the first year of the terabyte track and the first time we have participated directly in the trecconference. it is also the first time that the amberfish software  etymon.com/tr.html  has been used in trec. our goals for this ambitious track were simply to complete the task and to gain some rudimentary experience with evaluation and the terabyte collection.
   this paper presents a summary of the amberfish software followed by a brief discussion of this year's terabyte track.
1 amberfish
amberfish is open source text retrieval software developed by the author starting in 1 and distributed by etymon systems  inc. the project was based on lessons learned from previous implementation experiences with isearch  1  and freewais  1  at the clearinghouse for networked information discovery and retrieval  cnidr . isearch was the search component of isite  an open source z1 implementation  1  1   and freewais was an open source version of wais . more recently  amberfish has been coupled with gir  and z1 for experiments in distributed searching.
   the core of amberfish provides a set of general purpose indexing and searching operations  with conventional support for boolean queries  right truncation  phrase searching  relevance ranking  multiple documents per file  incremental indexing  and stemming. some novel features are indexing of semi-structured text  xml   structured queries for selecting field path subtrees  hierarchical results showing field relationships  and automatic searching across multiple indexes.
   the software consists of a c/c++ library and a unix-like command line interface on the front end. the central data structures are a dictionary and linked postings lists   with a simple prefix b-tree used to store the inverted file. the postings can be converted to sequential lists with an optional second indexing pass. several speed optimizations are used in indexing  such as merge update of the b-tree . additional files optionally store word positions  for phrase/proximity  and field structures  both associated with individual postings.
1 terabyte track
the first few weeks of handling the terabyte collection primarily entailed discovering all of the approaches that were not going to work. the collection consisted of 1 million web pages from the .gov domain  or about 1 gb. the initial plan was to process the collection using html-to-xml conversion software such as html tidy and html1text  so that the structure could be indexed by amberfish. however  the software was unable to convert successfully more than 1% of the documents. with limited time remaining and in order to avoid confusion  it was determined to index the documents as plain text  although this unfortunately meant that tag element and attribute names would be indexed as words. in addition  the relevance scoring function for  bag of words  queries was not implemented in time for submitting query runs. as a result  the plan changed to finishing the task and leaving evaluation until next year.
   three runs were submitted. the first was produced by the simple union of documents matching title terms from the topic. the second was a simple intersection. the third was a combination of union and intersection  with results from the intersection weighted more heavily. the results were poor enough that they need not be included here. in particular the lack of attention to word proximity seems to have hurt precision. however  this could also be a result of our incomplete scoring function  which in effect was a very simplified variation of ntc for document weights with no term weighting.
   although a porter stemmer was used  no stopwords were removed. even so the indexing process was very fast  under 1 hours with fairly modest hardware: 1 gb ram on a xeon 1 ghz system  storing 1 gb of index files.  the system had quite a bit more memory which was not used in these experiments.  the index was partitioned into 1 subindexes and the searches distributed over them.
   all indexing and searching was done using the available  stock  version of amberfish  which includes options for outputting results in trec-run format. it is hoped that prospective participants  especially students  will find the software to be a useful tool while learning about trec.
1 acknowledgements
i owe special thanks to gregory newby  of the arctic region supercomputing center  who generously offered his time  suggestions  and access to computer systems over several months. many others have contributed directly or indirectly to the amberfish project  and a few of them must be gratefully mentioned here: kevin gamiel  james fullton  erik scott  edward zimmermann  and david green.
