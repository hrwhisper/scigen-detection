text is ubiquitous and  not surprisingly  many important applications rely on textual data for a variety of tasks. as a notable example  information extraction applications derive structured relations from unstructured text; as another example  focused crawlers explore the web to locate pages about specific topics. execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan  or  crawl   the text database or  alternatively  we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. the choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output  completeness   e.g.  in terms of recall . nevertheless  this choice is typically ad-hoc and based on heuristics or plain intuition. in this paper  we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed  cost-based way. towards this goal  we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. we adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. we identify these properties and present efficient techniques for estimating the associated parameters of the cost model. overall  our approach helps predict the most appropriate execution plans for a task  resulting in significant efficiency and output completeness benefits. we complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets.
1. introduction
 text is ubiquitous and  not surprisingly  many applications rely on textual data for a variety of tasks. for example  information extraction applications retrieve documents and extract structured relations from the unstructured text in the documents. reputation management systems download web pages to track the  buzz  around companies and products. comparative shopping agents locate e-commerce web sites and add the products offered in the pages to their own index.
to process a text-centric task over a text database  or the web   we
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
can retrieve the relevant database documents in different ways. one approach is to scan or crawl the database to retrieve its documents and process them as required by the task. while this approach guarantees that we cover all documents that are potentially relevant for the task  it might be unnecessarily expensive in terms of execution time. for example  consider the task of extracting information on disease outbreaks  e.g.  the name of the disease  the location and date of the outbreak  and the number of affected people  as reported in news articles. this task does not require that we scan and process  say  the articles about sports in a newspaper archive. in fact  only a small fraction of the archive is of relevance to the task. for tasks such as this one  a natural alternative to crawling is to exploit a search engine index on the database to retrieve -via careful querying- the useful documents. in our example  we can use keywords that are strongly associated with disease outbreaks  e.g.   world health organization    case fatality rate   and turn these keywords into queries to find news articles that are appropriate for the task.
 the choice between a crawl- and a query-based execution strategy for a text-centric task is analogous to the choice between a scan- and an index-based execution plan for a selection query over a relation. just as in the relational model  the choice of execution strategy can substantially affect the execution time of the task. in contrast to the relational world  however  this choice might also affect the quality of the output that is produced: while a crawl-based execution of a text-centric task guarantees that all documents are processed  a querybased execution might miss some relevant documents  hence producing potentially incomplete output  with less-than-perfect recall. the choice between crawl- and query-based execution plans can then have a substantial impact on both execution time and output recall. nevertheless  this important choice is typically left to simplistic heuristics or plain intuition.
 in this paper  we introduce fundamental building blocks for the optimization of text-centric tasks. towards this goal  we show how to rigorously analyze query- and crawl-based plans for a task in terms of both execution time and output recall. to analyze crawl-based plans  we apply techniques from statistics to model crawling as a document sampling process; to analyze query-based plans  we first abstract the querying process as a random walk on a querying graph  and then apply results for the theory of random graphs to discover relevant properties of the querying process. our cost model reflects the fact that the performance of the execution plans depends on fundamental taskspecific properties of the underlying text databases. we identify these properties and present efficient techniques for estimating the associated parameters of the cost model.
in brief  the contributions and content of the paper are as follows:
  a novel framework for analyzing crawl- and query-based execution plans for text-centric tasks in terms of execution time and output recall  section 1 .

figure 1: extracting diseaseoutbreaks tuples
  a description of four crawl- and query-based execution plans  which underlie the implementation of many existing text-centric tasks  section 1 .
  a rigorous analysis of each execution plan alternative in terms of execution time and recall; this analysis relies on fundamental task-specific properties of the underlying databases  section 1 .
  an optimization strategy that estimates the database properties that affect the execution time and recall of each plan and selects the best execution plan for the task description and target recall requirements  section 1 .
  an extensive experimental evaluation showing that our optimization strategy is accurate and results in significant performance gains. our experiments include three important textcentric tasks and multiple real-life data sets  sections 1 and 1 . finally  section 1 discusses related work  while section 1 provides further discussion and concludes the paper.
1. examples of text-centric tasks
 in this section  we briefly review three important text-centric tasks that we will use throughout the paper as running examples  to illustrate our framework and techniques.
1 task 1: information extraction
 unstructured text  e.g.  in newspaper articles  often embeds structured information that can be used for answering relational queries or for data mining. the first task that we consider is the extraction of structured information from text databases. an example of an information extraction task is the construction of a table diseaseoutbreaks diseasename  date  country  of reported disease outbreaks from a newspaper archive  see figure 1 . a tuple hyellow fever  1  malii might then be extracted from the news articles in figure 1.
 information extraction systems typically rely on patterns -either manually created or learned from training examples- to extract the structured information from the documents in a database. the extraction process is usually time consuming  since information extraction systems might rely on a range of expensive text analysis functions  such as parsing or named-entity tagging  e.g.  to identify all person names in a document . see  for an introductory survey on information extraction.
 a straightforward execution strategy for an information extraction task is to retrieve and process every document in a database exhaustively. as a refinement  an alternative strategy might use filters and do the expensive processing of only  promising  documents; for example  the proteus system  ignores database documents that do not include words such as  virus  and  vaccine  when extracting the diseaseoutbreaks relation. as an alternative  query-based approaches such as qxtract  have been proposed to avoid retrieving all documents in a database; instead  these approaches retrieve appropriate documents via carefully crafted queries.
1 task 1: content summary construction
 many text databases have valuable contents  hidden  behind search interfaces and are hence ignored by search engines such as google.

figure 1: focused resource discovery for botany pages
metasearchers are helpful tools for searching over many databases at once through a unified query interface. a critical step for a metasearcher to process a query efficiently and effectively is the selection of the most promising databases for the query. this step typically relies on statistical summaries of the database contents  1  1 . the second task that we consider is the construction of a content summary of a text database. the content summary of a database generally lists each word that appears in the database  together with its frequency. for example  figure 1 shows that the word  xbox  appears in 1 documents in the forbes.com database. if we have access to the full contents of a database  e.g.  via crawling   it is straightforward to derive these simple content summaries. if  in contrast  we only have access to the database contents via a limited search interface  e.g.  as is the case for  hidden-web  databases    then we need to resort to query-based approaches for content summary construction  1  1 .
1 task 1: focused resource discovery
 text databases often contain documents on a variety of topics. over the years  a number of specialized search engines  as well as directories  that focus on a specific topic of interest have been proposed  e.g.  findlaw . the third task that we consider is the identification of the database documents that are about the topic of a specialized search engine  or focused resource discovery.
 as an example of focused resource discovery  consider building a search engine that specializes in documents on botany from the web at large  see figure 1 . for this  an expensive strategy would crawl all documents on the web and apply a document classifier  to each crawled page to decide whether it is about botany  and hence should be indexed  or not  and hence should be ignored . as an alternative execution strategy  focused crawlers  e.g.   1  1  1   concentrate their effort on documents and hyperlinks that are on-topic  or likely to lead to on-topic documents  as determined by a number of heuristics. focused crawlers can then address the focused resource discovery task efficiently at the expense of potentially missing relevant documents. as yet another alternative  cohen and singer  propose a querybased approach for this task  where they exploit search engine indexes and use queries derived from a document classifier to quickly identify pages that are relevant to a given topic.
1. describing text-centric tasks
 while the text-centric examples of section 1 might appear substantially different on the surface  they all operate over a database of text documents and also share other important underlying similarities.
 each task in section 1 can be regarded as deriving  tokens  from a database  where a token is a unit of information that we define in a task-specific way. for task 1  the tokens are the relation tuples that are extracted from the documents. for task 1  the tokens are the words in the database  accompanied by the associated word frequencies . for task 1  the tokens are the documents  or web pages  in the database that are about the topic of focus.
 the execution strategies for the tasks in section 1 rely on taskspecific document processors to derive the tokens associated with the task. for task 1  the document processor is the information extraction system of choice  e.g.  proteus   dipre   snowball  : given a document  the information extraction system extracts the tokens  i.e.  the tuples  that are present in the document. for task 1  the document processor extracts the tokens  i.e.  the words  that are present in a given document  and the associated document frequencies are updated accordingly in the content summary. for task 1  the document processor decides  e.g.  via a document classifier such as naive bayes  or support vector machines   whether a given document is about the topic of focus; if the classifier deems the document relevant  the document is added as a token to the output and is discarded otherwise.
 the alternate execution strategies for the section 1 tasks differ in how they retrieve the input documents for the document processors  as we will discuss in section 1. some execution strategies fully process every available database document  thus guaranteeing the extraction of all the tokens that the underlying document processor can derive from the database. in contrast  other execution strategies focus  for efficiency  on a strict subset of the database documents  hence potentially missing tokens that would have been derived from unexplored documents. one subcategory applies a filter  e.g.  derived in a training stage  to each document to decide whether to fully process it or not. other strategies retrieve via querying the documents to be processed  where the queries can be derived in a number of ways that we will discuss. all these alternate execution strategies thus exhibit different tradeoffs between execution time and output recall.
﹛definition 1.  execution time  consider a text-centric task  a database of text documents d  and an execution strategy s for the task  with an underlying document processor p. then  we define the execution time of s over d  time s d   as
time s d  =
	 	 
	x	x	x
tt s +	tq q +	tr d +tf d 	+	tp d  q﹋qsent	d﹋dretr	d﹋dproc where
  qsent is the set of queries sent by s 
  dretr is the set of documents retrieved by s  dretr   d  
  dproc is the set of documents that s processes with document processor p  dproc   d  
  tt s  is the time for training the execution strategy s 
  tq q  is the time for evaluating a query q 
  tr d  is the time for retrieving a document d    tf d  is the time for filtering a retrieved document d  and
  tp d  is the time for processing a document d with p.
assuming that the time to evaluate a query is constant across queries  i.e.  tq = tq q   for every q ﹋ qsent  and that the time to retrieve  filter  or process a single document is constant across documents  i.e.  tr = tr d   tf = tf d   tp = tp d   for every d ﹋ d   we have:
time s d  = tt s +tq﹞|qsent|+`tr+tf∩﹞|dretr|+tp﹞|dproc|
1
﹛definition 1.  recall  consider a text-centric task  a database of text documents d  and an execution strategy s for the task  with an underlying document processor p. let dproc be the set of
input: database d  recall threshold 而  document processor p
output: tokens tokensretr
tokensretr =    dretr =    recall = 1 while recall   而 do
retrieve an unprocessed document d and add d to dretr process d using p and add extracted tokens to tokensretr recall = |tokensretr|/|tokens|
end
return tokensretrfigure 1: the scan strategy
documents from d that s processes with p. then  we define the recall of s over d  recall s d   as
recall s d  = |tokens p dproc |	 1  |tokens p d |
where tokens p d  is the set of tokens that the document processor p extracts from the set of documents d. 1
 our problem formulation is close  conceptually  to the evaluation of a selection predicate in an rdbms. in relational databases  the query optimizer selects an access path  i.e.  a sequential scan or a set of indexes  that is expected to lead to an efficient execution. we follow a similar structure in our work. in the next section  we describe the alternate evaluation methods that are at the core of the execution strategies for text-centric tasks that have been discussed in the literature.1then  in subsequent sections  we analyze these strategies to see how their performance depends on the task and database characteristics.
1. execution strategies
 in this section  we review the alternate execution plans that can be used for the text-centric tasks described above  and discuss how we can  instantiate  each generic plan for each task of section 1. our discussion assumes that each task has a target recall value 而  1   而 ≒ 1  that needs to be achieved  see definition 1   and that the execution can stop as soon as the target recall is reached.
1 scan
 the scan  sc  strategy is a crawl-based strategy that processes each document in a database d exhaustively until the number of tokens extracted satisfies the target recall 而  see figure 1 .
 the scan execution strategy does not need training and does not send any queries to the database. hence  tt sc  = 1 and |qsent| = 1. furthermore  scan does not apply any filtering  hence tf = 1 and |dproc| = |dretr|. therefore  the execution time of scan is:
	time sc d  = |dretr| ﹞  tr + tp 	 1 
 the scan strategy is the basic evaluation strategy that many textcentric algorithms use when there are no efficiency issues  or when recall  which is guaranteed to be perfect according to definition 1  is important. we should stress  though  that |dretr| for scan is not necessarily equal to |d|: when the target recall 而 is low  or when tokens appear redundantly in multiple documents  scan may reach the target recall without processing all the documents in d. in section 1  we show how to estimate the value of |dretr| that is needed by scan to reach a target recall 而.
 a basic version of scan accesses documents in random order. variations of scan might impose a specific processing order and prioritize  say   promising  documents that are estimated to contribute many new tokens. another natural improvement of scan is to avoid processing altogether documents expected not to contribute any tokens; this is the basic idea behind filtered scan  which we discuss next.
input: database d  recall threshold 而  classifier c  document processor p output: tokens tokensretr
tokensretr =     dretr =    recall = 1 while recall   而 and |dretr|   |d| do
retrieve an unprocessed document d and add d to dretr use c to classify d as useful for the task or not if d is useful then
﹛process d using p and add extracted tokens to tokensretr end
recall = |tokensretr|/|tokens|
end
return tokensretrfigure 1: the filtered scan strategy
1 filtered scan
 the filtered scan  fs  strategy is a variation of the basic scan strategy. while scan indistinguishably processes all documents retrieved  filtered scan first uses a classifier c to decide whether a document d is useful  i.e.  whether d contributes at least one token  see figure 1 . given the potentially high cost of processing a document with the document processor p  a quick rejection of useless documents can speed up the overall execution considerably.
 the training time tt fs  for filtered scan is equal to the time required to build the classifier c for a specific task. training represents a one-time cost for a task  so in a repeated execution of the task  i.e.  over a new database  the classifier will be available with tt fs  = 1. this is the case that we assume in the rest of the analysis. since filtered scan does not send any queries  |qsent| = 1. while filtered scan retrieves and classifies |dretr| documents  it actually processes only c考﹞|dretr| documents  where c考 is the  selectivity  of the classifier c  defined as the fraction of database documents that c judges as useful. therefore  according to definition 1  the execution time of filtered scan is:
	time fs d  = |dretr| ﹞ `tr + tf + c考 ﹞ tp∩	 1 
in section 1  we show how to estimate the value of |dretr| that is needed for filtered scan to reach the target recall 而.
 filtered scan is used when tp is high and there are many database documents that do not contribute any tokens to the task at hand. for task 1  filtered scan is used by proteus   which uses a hand-built set of inexpensive rules to discard useless documents. for task 1  the filtered scan strategy is typically not applicable  since all the documents are useful. for task 1  the filtered scan strategy corresponds to a  hard  focused crawler  that prunes the search space by only considering documents that are pointed to by useful documents.
 both scan and filtered scan are crawl-based strategies. next  we describe two query-based strategies  iterative set expansion  which emulates query-based strategies that rely on  bootstrapping  techniques  and automatic query generation  which generates queries automatically  without using the database results.
1 iterative set expansion
 iterative set expansion  ise  is a query-based strategy that queries a database with tokens as they are discovered  starting with a typically small number of user-provided seed tokens tokensseed. the intuition behind this strategy is that known tokens might lead to unseen tokens via documents that have both seen and unseen tokens  see figure 1 . queries are derived from the tokens in a task-specific way. for example  a task 1 tuple hcholera 1 nigeriai for diseaseoutbreaks might be turned into query  cholera and nigeria ; this query  in turn  might help retrieve documents that report other disease outbreaks  such as hcholera 1 senegali and hmeasles 1 nigeriai.
 iterative set expansion has no training phase  hence tt ise  = 1. we assume that iterative set expansion has to send |qsent| queries to reach the target recall. in section 1  we show how to estimate this value of |qsent|. also  since iterative set expansion processes all the
input: database d  recall threshold 而  tokens tokensseed  document processor p
output: tokens tokensretr
tokensretr =    dretr =    recall = 1 while recall   而 and tokensseed 1=   do
remove a token t from tokensseed
transform t into a query q and issue q to d retrieve up to maxd documents matching q foreach newly retrieved document d do
add d to dretr
process d using p and add newly extracted tokens to tokensretr and tokensseed recall = |tokensretr|/|tokens|
if recall ≡ 而 then return tokensretr
end
end
return tokensretrfigure 1: the iterative set expansion strategy
input: database d  recall threshold 而  document processor p  queries q output: tokens tokensretr
tokensretr =    dretr =    recall = 1 foreach query q ﹋ q do
retrieve up to maxd documents matching q foreach newly retrieved document d do
add d to dretr
process d using p and add extracted tokens to tokensretr recall = |tokensretr|/|tokens|
if recall ≡ 而 then return tokensretr
end
end
return tokensretrfigure 1: the automatic query generation strategy
documents that it retrieves  tf = 1 and |dproc| = |dretr|. then  according to definition 1:
time ise d  = |qsent| ﹞ tq + |dretr| ﹞ `tr + tp∩  1 
informally  we expect iterative set expansion to be efficient when tokens tend to co-occur in the database documents. in this case  we can start from a few tokens and  reach  the remaining ones.  we define reachability formally in section 1.  in contrast  this strategy might  stall  and lead to poor recall for scenarios when tokens occur in isolation  as was analyzed in .
 iterative set expansion has been successfully applied in many tasks. for task 1  iterative set expansion corresponds to the tuples algorithm for information extraction   which was shown to outperform crawl-based strategies when |duseful  where duseful is the set of documents in d that  contribute  at least one token for the task. for task 1  iterative set expansion corresponds to the query-based sampling algorithm by callan et al.   which creates a content summary of a database from a document sample obtained via query words derived  randomly  from the already retrieved documents. for task 1  iterative set expansion is not directly applicable  since there is no notion of  co-occurrence.  instead  strategies that start with a set of topic-specific queries are preferable. next  we describe such a querybased strategy.
1 automatic query generation
 automatic query generation  aqg  is a query-based strategy for retrieving useful documents for a task. automatic query generation works in two stages: query generation and execution. in the first stage  automatic query generation trains a classifier to categorize documents as useful or not for the task; then  rule-extraction algorithms derive queries from the classifier. in the execution stage  automatic query generation searches a database using queries that are expected to retrieve useful documents. for example  for task 1 with botany as the topic  automatic query generation generates queries such as  plant and phylogeny  and  phycology .  see figure 1. 
 the training time for automatic query generation involves downloading a training set dtrain of documents and processing them with p  incurring a cost of |dtrain|﹞ tr+tp . training time also includes the time for the actual training of the classifier. this time depends on the learning algorithm and is  typically  at least linear in the size of dtrain. training represents a one-time cost for a task  so in a repeated execution of the task  i.e.  over a new database  the classifier will be available with tt aqg  = 1. this is the case that we assume in the rest of the analysis. during execution  the automatic query generation strategy sends |qsent| queries and retrieves |dretr| documents  which are then all processed by p  without any filtering1  i.e.  |dproc| = |dretr| . in section 1  we show how to estimate the values of |qsent| and |dretr| that are needed for automatic query generation to reach a target recall 而. then  according to definition 1:
time aqg d =|qsent| ﹞ tq + |dretr| ﹞ `tr + tp∩  1 
 the automatic query generation strategy was proposed under the name qxtract for task 1 ; it was also used for task 1 in  and for task 1 in .
 the description of the execution time has so far relied on parameters  e.g.  |dretr|  that are not known before executing the strategies. in the next section  we focus on the central issue of estimating these parameters. in the process  we show that the performance of each strategy depends heavily on task-specific properties of the underlying database; then  in section 1 we show how to characterize the required database properties and select the best execution strategy for a task.
1. estimating execution plan costs
 in the previous section  we presented four alternative execution plans and described the execution cost for each plan. our description focused on describing the main factors of the actual execution time of each plan and did not provide any insight on how to estimate these costs: many of the parameters that appear in the cost equations are outcomes of the execution and cannot be used to estimate or predict the execution cost. in this section  we show that the cost equations described in section 1 depend on a few fundamental task-specific properties of the underlying databases  such as the distribution of tokens across documents. our analysis reveals the strengths and weaknesses of the execution plans and  most importantly  provides an easy way to estimate the cost of each technique for reaching a target recall 而. the rest of the section is structured as follows. first  section 1 describes the notation and gives the necessary background. then  sections 1 and 1 analyze the two crawl-based techniques  scan and filtered scan  respectively. finally  sections 1 and 1 analyze the two query-based techniques  iterative set expansion and automatic query generation  respectively.
1 preliminaries
 in our analysis  we use some task-specific properties of the underlying databases  such as the distribution of tokens across documents. we use g d  to represent the  degree  of a document d for a document processor p  which is defined as the number of distinct tokens extracted from d using p. similarly  we use g t  to represent the  degree  of a token t in a database d  which is defined as the number of distinct documents that contain t in d. finally  we use g q  to repre-

figure 1: modeling scan as multiple sampling processes  one per token  running in parallel over d
sent the  degree  of a query q in a database d  which is defined as the number of documents from d retrieved by query q.
 in general  we do not know a-priori the exact distribution of the token  document  and query degrees for a given task and database. however  we typically know the distribution family for these degrees  and we just need to estimate a few parameters to identify the actual distribution for the task and database. for task 1  the document and token degrees tend to follow a power-law distribution   as we will see in section 1. for task 1  token degrees follow a power-law distribution  and document degrees follow roughly a lognormal distribution ; we provide further evidence in section 1. for task 1  the document and token distributions are  by definition  uniform over duseful with g t  = g d  = 1. in section 1  we describe how to estimate the parameters of each distribution.
1 cost of scan
 according to equation 1  the cost of scan is determined by the size of the set dretr  which is the number of documents retrieved to achieve a target recall 而.1 to compute |dretr|  we base our analysis on the fact that scan retrieves documents in no particular order and does not retrieve the same document twice. this process is equivalent to sampling from a finite population . conceptually  scan samples for multiple tokens during execution. therefore  we treat scan as performing multiple  sampling from a finite population  processes  running in parallel over d  see figure 1 . each sampling process corresponds to a token t ﹋ tokens. according to probability theory  1  page 1   the probability of observing a token t k times in a sample of size s follows the hypergeometric distribution. for k = 1  we get the probability that t does not appear in the sample  which is
. the complement of this value is the probability that t appears in at least one document in the set of s retrieved documents. so  after processing s documents  the expected number of retrieved tokens for scan is:
e |tokensretr
g
t﹋tokens
hence  we estimate the number of documents that scan should retrieve to achieve a target recall 而 as:
	tokensretr|  ≡ 而|tokens|}	 1 
the number of documents |dretr| retrieved by scan depends on the token degree distribution. for many databases  the distribution of g t  is highly skewed and follows a power-law distribution: a few tokens appear in many documents  while the majority of tokens can only be extracted from only a few documents. for example  the task 1 tuple hsars  1  chinai can be extracted from hundreds of documents in the new york times archive  while the tuple hdiphtheria  1  afghanistani appears only in a handful of documents. by estimating the parameters of the power-law distribution  we can then compute the expected values of g t  for the  unknown  tokens in d and use equations 1 and 1 to derive the expected cost of scan. in section 1  we show how to perform such estimations on-the-fly.
 the analysis above assumes a random retrieval of documents. if the documents are retrieved in a special order  which is unlikely for the task scenarios that we consider  then we should model scan as  stratified  sampling without replacement: instead of assuming a single sampling pass  we decompose the analysis into multiple  strata   i.e.  into multiple sampling phases   each one with its own g ﹞  distribution. a simple instance of such technique is filtered scan  which  conceptually  samples useful documents first  as discussed next.
1 cost of filtered scan
 filtered scan is a variation of the basic scan strategy  therefore the analysis of both strategies is similar. the key difference between these strategies is that filtered scan uses a classifier to filter documents  which scan does not. the filtered scan classifier thus limits the number of documents processed by the document processor p. two properties of the classifier c are of interest for our analysis:
  the classifier's selectivity c考: if dproc is the set of documents in d deemed useful by the classifier  and then processed by p   then.
  the classifier's recall cr: this is the fraction of useful documents in d that are also classified as useful by the classifier. the value of cr affects the effective token degree for each tuple t: now each token appears  on average  cr ﹞ g t  times1 in dproc  the set of documents actually processed by p.
using these observations and following the methodology that we used for scan  we have:
e |tokensretr
	考	r g	考
t﹋tokens
again  similar to scan  we have:
|dretr| =	=	tokensretr|  ≡ 而|tokens|}	 1 
	c考	c考
 equations 1 and 1 show the dependence of filtered scan on the performance of the classifier. when c考 is high  almost all documents in d are processed by p  and the savings compared to scan are minimal  if any. when a classifier has low recall cr  then many useful documents are rejected and the effective token degree decreases  in turn increasing |dretr|. we should also emphasize that if the recall of the classifier is low  then filtered scan is not guaranteed to reach the target recall 而. in this case  the maximum achievable recall might be less than one and |dretr| = |d|.
1 cost of iterative set expansion
 so far  we have analyzed two crawling-based strategies. before moving to the analysis of the iterative set expansion query-based strategy  we define  queries  more formally as well as a graph-based representation of the querying process  originally introduced in .
﹛definition 1.  querying graph  consider a database d and a document processor p. we define the querying graph qg d p 
	t	d

figure 1: portion of the querying and reachability graphs of a database
of d with respect to p as a bipartite graph containing the elements of tokens and d as nodes  where tokens is the set of tokens that p derives from d. a directed edge from a document node d to a token node t means that p extracts t from d. an edge from a token node t to document node d means that d is returned from d as a result to a query derived from the token t. 1
for example  suppose that token t1  after being suitably converted into a query  retrieves a document d1 and  in turn  that processor p extracts the token t1 from d1. then  we insert an edge into qg from t1 to d1  and also an edge from d1 to t1. we consider an edge d ↙ t  originating from a document node d and pointing to a token node t  as a  contains  edge  and an edge t ↙ d  originating from a token node t and pointing to a document node d  as a  retrieves  edge.
 using the querying graph  we analyze the cost and recall of iterative set expansion. as a simple example  consider the case where the initial tokensseed set contains a single token  tseed. we start by querying the database using the query derived by tseed. the cost at this stage is a function of the number of documents retrieved by tseed: this is the number of neighbors at distance one from tseed in the querying graph qg. the recall of iterative set expansion  at this stage  is determined by the number of tokens derived from the retrieved documents  which is equal to the number of neighbors at distance two from tseed. following the same principle  the cost in the next stage  after querying with the tokens in distance two  depends on the number of neighbors at distance three and recall is determined by the number of neighbors at distance four  and so on.
 the previous example illustrates that the recall of iterative set expansion is bounded by the number of tokens  reachable  from the tokensseed tokens; the execution time is also bounded by the number of documents and tokens that are  reachable  from the tokensseed tokens. the structure of the querying graph thus defines the performance of iterative set expansion. to compute the interesting properties of the querying graph  we resort to the theory of random graphs: our approach is based on the methodology suggested by newman et al.  and uses generating functions to describe the properties of the querying graph qg. we define the generating functions gd1 x  and gt1 x  to describe the degree distribution1 of a randomly chosen document and token  respectively:
	gd1 x  = xpdk ﹞ xk 	gt1 x  = xptk ﹞ xk	 1 
                         k	k where pdk is the probability that a randomly chosen document d contains k tokens  i.e.  pdk = pr{g d  = k}  and ptk is the probability that a randomly chosen token t retrieves k documents  i.e.  ptk = pr{g t  = k}  when used as a query.
 in our setting  we are also interested in the degree distribution for a document  or token  respectively  chosen by following a random edge. using the methodology of newman et al.   we define the functions gd1 x  and gt1 x  that describe the degree distribution for a document and token  respectively  chosen by following a random edge:
		 1 
where is the first derivative of is the first derivative of gt1 x   respectively.  see  for the proof. 
 for the rest of the analysis  we use the following useful properties of generating functions :
  moments: the i-th moment of the probability distribution generated by a function g x  is given by the i-th derivative of the generating function g x   evaluated at x = 1. we mainly use this property to compute efficiently the mean of the distribution described by g x .
  power: if x1 ... xm are independent  identically distributed random variables generated by the generating function g x   then the sum of these variables   has generating function  g x  m.
  composition: if x1 ... xm are independent  identically distributed random variables generated by the generating function g x   and m is also an independent random variable generated by the function f x   then the sumhas generating function f g x  .
 using these properties and equations 1 and 1  we can proceed to analyze the cost of iterative set expansion. assume that we are in the stage where iterative set expansion has sent a set q of tokens as queries. these tokens were discovered by following random edges on the graph; therefore  the degree distribution of these tokens is described by gt1 x   equation 1 . then  by the power property  the distribution of the total number of retrieved documents  which are pointed to by these tokens  is given by the generating function:1
	gd1 x  =  gt1 x  |q|	 1 
 now  we know that dretr in equation 1 is a random variable and its distribution is given by gd1 x . we also know that we retrieve documents by following random edges on the graph; therefore  the degree distribution of these documents is described by gd1 x   equation 1 . then  by the composition property1  the distribution of the total number of tokens |tokensretr| retrieved by the dretr documents is given by the generating function:1
	gt1 x  = gd1 gd1 x   =  gt1 gd1 x   |q|	 1 
 finally  we use the moments property to compute the expected values for |dretr| and |tokensretr|  after iterative set expansion sends q queries.
 1 
	e |tokensretr	 1 
hence  the number of queries |qsent| sent by iterative set expansion to reach the target recall 而 is:
	| qsent| = min{q : e |tokensretr|  ≡ 而|tokens|}	 1 
 our analysis  so far  did not account for the fact that the tokens in a database are not always  reachable  in the querying graph from the tokens in tokensseed. as we have briefly discussed  though  the ability to reach all the tokens is necessary for iterative set expansion to achieve good recall. before elaborating further on the subject  we describe the concept of the reachability graph  which we originally introduced in  and is fundamental for our analysis.
﹛definition 1.  reachability graph  consider a database d  and an execution strategy s for a task with an underlying document processor p and querying strategy r. we define the reachability graph rg d s  of d with respect to s as a graph whose nodes are the tokens that p derives from d  and whose edge set e is such that a directed edge ti ↙ tj means that p derives tj from a document that r retrieves using ti. 1
 figure 1 shows the reachability graph derived from an underlying querying graph  illustrating how edges are added to the reachability graph. since token t1 retrieves document d1 and d1 contains token t1  the reachability graph contains the edge t1 ↙ t1. intuitively  a path in the reachability graph from a token ti to a token tj means that there is a set of queries that start with ti and lead to the retrieval of a document that contains the token tj. in the example in figure 1  there is a path from t1 to t1  through t1. this means that query t1 can help discover token t1  which in turn helps discover token t1. the absence of a path from a token ti to a token tj in the reachability graph means that we cannot discover tj starting from ti. this is the case for the tokens t1 and t1 in figure 1.
 the reachability graph is a directed graph and its connectivity defines the maximum achievable recall of iterative set expansion: the upper limit for the recall of iterative set expansion is equal to the total size of the connected components that include tokens in tokensseed. in random graphs  typically we observe two scenarios: either the graph is disconnected and has a large number of disconnected components  or we observe a giant component and a set of small connected components. chung and lu  proved this for graphs with a powerlaw degree distribution  and also provided the formulas for the composition of the size of the components. newman et al.  provide similar results for graphs with arbitrary degree distributions. interestingly for our problem  the size of the connected components can be estimated for many degree distributions using only a small number of parameters  e.g.  for power-law graphs we only need an estimate of the average node out-degree  to compute the size of the connected component; in section 1 we explain how we obtain such estimates . by estimating only a small number of parameters  we can thus characterize the performance limits of the iterative set expansion strategy.
 as discussed  iterative set expansion relies on the discovery of new tokens to derive new queries. therefore  in sparse and  disconnected  databases  iterative set expansion can exhaust the available queries and still miss a significant part of the database  leading to low recall. in such cases  if high recall is a requirement  different strategies are preferable. the alternative query-based strategy that we examine next  automatic query generation  showcases a different querying approach: instead of deriving new queries during execution  automatic query generation generates a set of queries offline and then queries the database without using query results as feedback.
1 cost of automatic query generation
 section 1 showed that the cost of automatic query generation consists of two main components: the training cost and the query-
ing cost. training represents a one-time cost for a task  as discussed in section 1  so we ignore it in our analysis. therefore  the main component that remains to be analyzed is the querying cost.
 to estimate the querying cost of automatic query generation  we need to estimate recall after sending a set q of queries and the number of retrieved documents |dretr| at that point. each query q retrieves g q  documents  and a fraction p q  of these documents is useful for the task at hand. assuming that the queries are biased only towards retrieving useful documents and not towards any other particular set of documents  the queries are conditionally independent1 within the set of documents duseful and within the rest of the documents  duseless. therefore  the probability that a useful document is retrieved by a query. hence  the probability that a useful document d
﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛useful is retrieved by at least one query is:
q
1 pr{d not retrieved by any query}=1 y 1   p qi  ﹞ g qi 
so  given the values of p qi  and g qi   the expected number of useful documents that are retrieved is:
1
	e |dretruseful|  = |duseful| ﹞		 1 
and the number of useless documents retrieved is:
e |dretruseless| =|duseless	 1     i   ﹞   i 
	|	|
i=1
assuming that the  precision  of a query q is independent of the number of documents that q retrieves 1 we get a simpler expression:
﹛! e |dretruseful| =|duseful 1 
where e p q   is the average precision of the queries and e g q   is the average number of retrieved documents per query. an analogous expression follows for e |dretruseless| . the expected number of retrieved documents is then:
	e |dretr|  = e |dretruseful|  + e |dretruseless| 	 1 
 to compute the recall of automatic query generation after issuing q queries  we use the same methodology that we used for filtered scan. specifically  equation 1 reveals the total number of useful documents retrieved  and these are the documents that contribute to recall. these documents belong to duseful. hence  similarly to scan and filtered scan  we model automatic query generation as sampling without replacement; the essential difference now is that the sampling is over the duseful set. therefore  we have an effective database size |duseful| and a sample size equal to |dretruseful|.1 by modifying equation 1 appropriately  we have:
e |tokensretr|  =x 1   |duseful|   g t  ! |duseful|   s !  1   |duseful|   g t    s !|duseful|!
t﹋tokens
where s = |dretruseful|. a good approximation of the average value of |tokensretr| can be derived by setting s to be the mean value of the |dretruseful| distribution  equation 1 . similarly to the analysis for iterative set expansion  we have:
	tokensretr|  ≡ 而|tokens|}	 1 
 in this section  we analyzed four alternate execution plans and we showed how their execution time and recall depend on fundamental task-specific properties of the underlying text databases. next  we show how to exploit the parameter estimation and our cost model to significantly speed-up the execution of text-centric tasks.
1
 the conditional independence assumption implies that the queries are only biased towards retrieving useful documents  and not towards any subset of useful documents.
1
we observed this assumption to be true in practice.
1. putting it all together
 in section 1  we examined how we can estimate the execution time and the recall of each execution plan by using the values of a few parameters  including the target recall 而 and the token  document  and query degree distributions. in this section  we summarize our overall optimization approach  and show how we estimate -on-the-fly- the parameters needed. as we will show in our experimental evaluation in section 1  our optimization approach leads to efficient executions of the text-centric tasks for the target recall value.
our cost model of section 1 relies on a number of parameters. for
example  the value of |tokens|  i.e.  the number of tokens in the database  is generally unknown before executing a task  and we need it both  1  to decide when we reach the desired recall for the task  to stop execution; and  1  to provide an  educated  estimate to bootstrap our estimation techniques. a robust estimation method for a database and a task is to retrieve multiple document samples from the data-
base and analyze the token overlap across the samples to determine a |tokens| estimate. similar estimation methods have been proposed for task 1  and for task 1  1  page 1 . in our experiments  we do not rely on estimates but rather use the actual value of |tokens|.
 some parameters of our cost model  such as classifier selectivity and recall  section 1   can be estimated accurately over a relatively small sample of database documents. in fact  the classifier characteristics for filtered scan and query degree and precision for automatic query generation can be easily estimated during classifier training using cross-validation . to estimate the token and document distributions  we rely on the fact that  for many tasks  we know the general family of these distributions  as we discussed in section 1. hence  our estimation task reduces to estimating a few parameters of well-known distribution families 1 which we discuss below.
 to estimate the parameters of a distribution family for a concrete text-centric task and database  we could resort to a  preprocessing  estimation phase before we start executing the actual task. for this  we could follow chaudhuri et al.   and continue to sample database documents until cross-validation indicates that the estimates are accurate enough. an interesting observation is that having a separate preprocessing estimation phase is not necessary in our scenario  since we can piggyback such estimation phase into the initial stages of an actual execution of the task. in other words  instead of having a preprocessing estimation phase  we can start processing the task and exploit the retrieved documents for  on-the-fly  parameter estimation. this estimation relies on an initial  manual  assignment of tasks to distribution families  which are often natural for the tasks or have been investigated in the literature  as we discussed in section 1. the basic challenge in this scenario is to guarantee that the parameter estimates that we obtain during execution are as accurate as the estimates that we would derive through random sampling. this is straightforward for scan  since scan effectively performs random sampling over the databases. automatic query generation performs random sampling over the duseful documents and is thus equivalent to random sampling for the token degree distribution. for the document degree distribution  automatic query generation then underestimates pd1  the probability that g d  = 1  i.e.  that a document d is useless  see section 1   and overestimates pdk for k ≡ 1. filtered scan has a similar bias  introduced by the classifier: since the classifier is not perfect  the observed token and document degrees are typically underestimates of the real values. fortunately  for both filtered scan and automatic query generation  we can compensate for the introduced bias using a confusion matrix adjustment   which we use for our
input: database d  recall threshold 而  alternate strategies s1 ...sn output: tokens tokensretr statistics =  
while recall   而 and |dretr|   |d| do
/* locate best possible strategy */ esttime = +﹢
foreach available execution strategy s do
compute the time s d  for reaching target recall 而 using the available statistics
if esttime ≡ time s d  then strategy = s esttime = time s d  end
end
/* execute strategy */
   continue execution using strategy  for a batch of documents update statistics using dretr and tokensretr end
return tokensretrfigure 1: choosing execution strategies adaptively
experiments.  due to space restrictions  we omit the details.  finally  for iterative set expansion  we should notice that the execution plan samples the distributions generated by the functions gt1 x  and gd1 x   see section 1   while unbiased random sampling samples from the distributions generated by the functions gt1 x  and gd1 x . equations 1 and 1 show how to convert the observed estimates to the real ones. again  due to space restrictions  we omit the details.
 using the observations above  we can now describe our overall optimization approach. the optimization starts by choosing one of the execution plans described in section 1  based on some  prior knowledge  about the token and document distributions  e.g.  that the token and document degrees follow a power-law distribution for task 1 . then  during execution  the adaptive strategy keeps updating the estimates for the token and document distributions and checks for their robustness using cross-validation. at any point in time  if the estimated execution time for reaching the target recall  time s d   of a competing strategy s is smaller than that of the current strategy  then the optimizer switches to executing the less expensive strategy  continuing from the execution point reached by the current strategy. figure 1 summarizes this algorithm.
 next  our experimental evaluation shows that our optimization approach accurately predicts the cost of each execution strategy and -in many cases- manages to choose the strategy that reaches the target recall 而 with the minimum execution time.
1. experimental setting
 we now describe the experimental setting for each text-centric task of section 1  including the real-world data sets for the experiments. we also present interesting statistics about the task-specific distribution of tokens in the data sets.
1 information extraction
document processor: for this task  we use the snowball information extraction system  as the document processor  see section 1 . we use two instantiations of snowball: one for extracting a diseaseoutbreaks relation  task 1a  and one for extracting a headquarters relation  task 1b . for task 1a  the goal is to extract all the tuples of the target relation diseaseoutbreaks  diseasename  country   which we discussed throughout the paper. for task 1b  the goal is to extract all the tuples of the target relation headquarters  organization location   where a tuple ho li in headquarters indicates that organization o has headquarters in location l. a token for these tasks is a single tuple of the target relation  and a document is a news article from the new york times archive  which we describe next. data set: we use a collection of newspaper articles from the new york times  published in 1  nyt1  and 1  nyt1 . we

figure 1: token distribution for task 1's diseaseoutbreaks

figure 1: document distribution for task 1's diseaseoutbreaks
use the nyt1 documents for training and the nyt1 documents for evaluation of the alternative execution strategies. the nyt1 database contains 1 documents  with 1 tokens for task 1a and 1 tokens for task 1b. figures 1 and 1 show the document and token degree distribution  section 1  for task 1a: both distributions follow a power-law  a common distribution for information extraction tasks. the distributions are similar for task 1b.
execution plan instantiation: for filtered scan we use a rule-based classifier  created using ripper . we train ripper using a set of 1 useful documents and 1 not useful documents from the nyt1 data set. we also use 1 documents from the nyt1 data set as a training set to create the queries required by automatic query generation. finally  for iterative set expansion  we construct the queries using the conjunction of the attributes of each tuple  e.g.  tuple htyphus belizei results in query  typhus and belize  .
1 content summary construction
document processor: for this task  the document processor is a simple tokenizer that extracts the words that appear in the eligible documents  defined as a sequence of one or more alphanumeric characters and ignoring capitalization.
data set: we use the 1 newsgroups data set from the uci kdd archive . this data set contains 1 messages from 1 usenet newsgroups. we also randomly retrieve additional usenet articles to create queries for automatic query generation. figures 1 and 1 show the document and token degree distribution  section 1  for this task. the document degree follows a lognormal distribution  and the token degree follows  as expected   a power-law distribution. execution plan instantiation: for this task  filtered scan is not directly applicable  since all documents are  useful.  for iterative set expansion  the queries are constructed using words that appear in previously retrieved documents; this technique corresponds to the learned resource description strategy for vocabulary extraction presented by callan et al. . finally  for automatic query gener-

figure 1: token distribution for task 1

figure 1: document distribution for task 1
ation  we constructed the queries as follows: first  we separate the documents into topics according to the high-level name of the newsgroup  e.g.   comp    sci   and so on ; then  we train a rule-based classifier using ripper  which creates rules to assign documents into categories  e.g.  cpu and ram ↙ comp means that a document containing the words  cpu  and  ram  is assigned to the  comp  category . the final queries for automatic query generation contain the antecedents of the rules  across all categories. this technique corresponds to the focused probing strategy for vocabulary extraction presented by ipeirotis and gravano .
1 focused resource discovery
document processor: for this task  the document processor is a multinomial naive bayes classifier  which detects the topic of a given web page . the topic of choice for our experiments is  botany.  data set: we retrieved 1 web pages listed in open directory1under the category  top ↙ science ↙ biology ↙ botany.  we selected 1 out of the 1 documents as training documents  and created a multinomial naive bayes classifier that decides whether a web page is about botany. then  for each of the downloaded botany pages  we used google to retrieve all its  backlinks   i.e.  all the web pages that point to that page ; again  we classified the retrieved pages and for each page classified as  botany  we repeated the process of retrieving the backlinks  until none of the backlinks was classified under botany. this process results in a data set with approximately 1 pages about botany  pointed to by approximately 1 useless documents deemed irrelevant to the botany topic. to augment the data set with additional useless documents  we picked 1 more random topics from the third level of the open directory hierarchy and we downloaded all the web pages listed under these topics  for a total of approximately 1 pages. after downloading the backlinks for these pages  our data set contained a total of approximately 1

figure 1: actual vs. estimated execution times for task 1a  as a function of the target recall 而

figure 1: actual vs. estimated execution times for task 1b  as a function of the target recall 而
pages  out of which 1 are relevant to botany.
execution plan instantiation: for this task  the scan plan corresponds to an unfocused crawl  with a classifier deciding whether each of the retrieved pages belongs to the category of choice. as an instantiation of filtered scan  we use the  hard  version of the focused crawler described in . the focused crawler starts from a few botany web pages  and then visits a web page only when at least one of the documents that points to it is useful. finally  to create queries for automatic query generation  we train a ripper classifier using the training set  and create a set of rules that assign documents into the botany category. we use these rules to query the data set and retrieve documents.
1. experimental evaluation
 in this section  we present our experimental results. our experiments focus on the execution times of each alternate execution strategy  section 1  for the tasks and settings described in section 1. we compute the actual execution times and compare them against our estimates from section 1. first  we compute our estimates with exact values for the various parameters on which they rely  e.g.  token degree distribution . then  we measure the execution time using our optimization strategy  which relies on approximations of these parameters  as described in section 1.
 accuracy of cost model with correct information: the goal of the first set of experiments is to examine whether our cost model of section 1 captures the real behavior of the alternate execution strategies of section 1  when all the parameters of the cost model  e.g.  token and document degree distributions  classifier characteristics  are known a-priori. for this  we first measure the actual execution time of the strategies  for varying values of the target recall 而. the lines sc time  fs time  ise time  aqg time in figures 1  1  1  and 1 show the actual execution time of the respective strategies for the tasks described in section 1. then  to predict the execution time of each

figure 1: actual vs. estimated execution times for task 1  as a function of the target recall 而

figure 1: actual vs. estimated execution times for task 1  as a function of the target recall 而
strategy  we used our equations from section 1. the lines sc pred  fs pred  ise pred  aqg pred in figures 1  1  1  and 1 show our execution time estimates for varying values of the target recall 而. the results were exceptionally accurate  confirming the accuracy of our theoretical modeling. the prediction error is typically less than 1% for all values of target recall 而.
 furthermore  our modeling captures well the limitations of each execution plan. for example  for task 1a  figure 1  automatic query generation is the fastest execution plan when target recall 而   1. however  due to the limited number of queries generated during the training phase  automatic query generation cannot reach high recall values.  we generated 1 queries for this task.  our analysis correctly captures this limitation and shows that  for higher recall targets  other strategies are preferable. this limitation also appears for the iterative set expansion strategy  confirming previously reported results . the results are similar for task 1: our analysis correctly predicts the execution time and the recall limitations of each strategy. quality of choice of execution strategies: after confirming that our cost models accurately capture the actual execution time of the alternate execution strategies  we examine whether the cost model leads to the choice of the fastest plan for each value of target recall 而. we start executing each task by using the strategy that is deemed best for the target recall and the available statistics. these statistics are the expected distribution family of the token and document degrees for the task  with some  default  parameters  such as 汕 =  1 for power-law distributions  see section 1 . our experiments also assume knowledge of the actual value of |tokens|  as discussed. during the actual execution  the available statistics are refined; if the acquired statistics show that an alternative strategy is preferable at some point in the execution  then we switch to the strategy that is deemed best  figure 1 . the optimized line in figures 1  1  1  and 1 shows the actual execution time  for different recall thresholds  using our optimization approach. typically  our optimizer finishes the task in the same time as the best possible strategy  resulting in execution times that can be up to 1 times faster than alternative plans that we might have picked based on plain intuition or heuristics. for example  consider task 1b with recall target 而 = 1  figure 1 : without our cost modeling  we might select iterative set expansion or automatic query generation  both reasonable choices given the relatively low target recall 而 = 1. however  automatic query generation cannot achieve a recall of 1 and iterative set expansion is more expensive than filtered scan for that task. our optimizer  on the other hand  correctly predicts that filtered scan should be the algorithm of choice. in this example  our optimizer initially picked iterative set expansion  but quickly revised its decision and switched to filtered scan after gathering statistics from only 1% of the database. in some cases  our prediction algorithm overestimates the achievable recall of a strategy  e.g.  automatic query generation . in such cases  our  incorrectly picked  strategy runs to completion; then  naturally  our technique picks the  next best  strategy and continues the execution from the point reached by the  incorrectly picked  strategy. in such cases  we sometimes even observed a small performance gain derived from this initial mistake  since the  incorrect  strategy outperforms the  correct  strategy for the first part of the execution. this result outlines an interesting future research direction: instead of picking a single strategy for a target recall  we could instead build multi-strategy executions explicitly  by choosing different strategies for different parts of the execution.
 conclusions: we demonstrated how our modeling approach can be used to create an optimizer for text-centric tasks. the presented approach allows for a better understanding of the behavior of query- and crawl-based strategies  in terms of both execution time and recall. furthermore  our modeling works well even with on-the-fly estimation of the bulk of the required statistics  and results in close-to-optimal execution times. our work provides fundamental building blocks towards a full query optimizer for text-centric tasks: given a specific target recall  e.g.   find 1% of all disease outbreaks mentioned in the news    the query optimizer can automatically select the best execution strategy to achieve this recall.
1. related work
 in this paper  we analyzed and estimated the computational costs of text-centric tasks. we concentrated on three important tasks: information extraction  task 1   text database content summary construction  task 1   and focused resource discovery  task 1 .
 implementations of task 1  section 1  traditionally use the scan strategy of section 1  where every document is processed by the information extraction system  e.g.   1  1  . some systems use the filtered scan strategy of section 1  where only the documents that match specific url patterns  e.g.    or regular expressions  e.g.    are processed further. agichtein and gravano  presented query-based execution strategies for task 1  corresponding to the iterative set expansion strategy of section 1 and automatic query generation strategy of section 1. more recently  etzioni et al.  used what could be viewed as an instance of automatic query generation to query generic search engines for extracting information from the web. cafarella and etzioni  presented a complementary approach of constructing a special-purpose index for efficiently retrieving promising text passages for information extraction. such document  and passage  retrieval improvements can be naturally integrated into our framework. for task 1  the execution strategy in  can be cast as an instance of iterative set expansion  as discussed in section 1. another strategy for the same task  can be considered an instance of automatic query generation  section 1 . interestingly  over large crawlable databases  where both query- and crawlbased strategies are possible  query-based strategies have been shown to outperform crawl-based approaches for a related database classification task   since small document samples can result in good categorization decisions at a fraction of the processing time required by full database crawls. for task 1  focused resource discovery systems typically use a variation of filtered scan  1  1  1  1   where a classifier determines which links to follow for subsequent  expensive  retrieval and processing. other strategies such as automatic query generation may be more effective for some scenarios .
 other important text-centric tasks can be modeled in our framework. one such task is text filtering  i.e.  selecting documents in a text database on a particular topic    which can be executed following either filtered scan  or  if appropriate  automatic query generation. another task is the construction of comparative web shopping agents . this task requires identifying appropriate web sites  e.g.  by using an instance of automatic query generation  and subsequently extracting product information from a subset of the retrieved pages  e.g.  by using an implementation of filtered scan . as another example  web question answering systems  usually translate a natural language question into a set of web search queries to retrieve documents for a subsequent answer extraction step from a subset of the retrieved documents. this process can be viewed as a combination of automatic query generation and filtered scan. recently  ntoulas et al.  presented query-based strategies for exhaustively  crawling  a hidden web database while issuing as few queries as possible.
 estimating the cost of a query execution plan requires estimating parameters of the cost model. we adapted common database sampling techniques  e.g.   1  1   for our problem  as we discussed in section 1. our work is similar in spirit to query optimization over structured relational databases  adapted to the intrinsic differences of executing text-centric tasks. our work is complementary to previous research on optimizing query plans with user-defined predicates   in that we provide a robust way of estimating costs of complex textcentric  predicates . our work can then be regarded as developing specialized  efficient techniques for important special-purpose  operators   e.g.  as was done for fuzzy matching  .
 closest to this paper  in  we presented results on modeling and estimating the achievable recall of iterative set expansion  for task 1  information extraction  and task 1  database content summary construction . our current work extends  in several ways. first  we develop rigorous cost models for iterative set expansion  as well as for three additional general execution strategies  namely scan  filtered scan  and automatic query generation. we also present a principled  cost-based method for selecting the most efficient execution strategy automatically  whereas  only provided upper bounds on the possible recall that each strategy could achieve if run to completion. finally  we thoroughly evaluated our cost estimation models and adaptive execution strategy over multiple tasks and multiple data sets.
1. conclusion
 in this paper  we introduced a rigorous cost model for several queryand crawl-based execution strategies that underlie the implementation of many text-centric tasks. we complement our model with a principled cost estimation approach. our analysis helps predict the execution time and output completeness of important query- and crawlbased algorithms  which until now were only empirically evaluated  with limited theoretical justification. we demonstrated that our modeling can be successfully used to create an optimizer for text-centric tasks  and showed that the optimizer can adaptively select the best execution strategy to achieve a target recall  resulting in executions that can be orders of magnitude faster than alternate choices.
 our work can be extended in multiple directions. for example  the current framework assumes that the document processors have perfect  precision   in that they always produce accurate results. relaxing this assumption and  correspondingly  predicting the precision of the output produced by different strategies is a natural next step. another interesting direction is to apply our model to other text-centric tasks and also study how to minimize our reliance on task-specific prior knowledge of the token and document distributions for our analysis.
