model management is a generic approach to solving problems of data programmability where precisely engineered mappings are required. applications include data warehousing  e-commerce  object-to-relational wrappers  enterprise information integration  database portals  and report generators. the goal is to develop a model management engine that can support tools for all of these applications. the engine supports operations to match schemas  compose mappings  diff schemas  merge schemas  translate schemas into different data models  and generate data transformations from mappings. 
much has been learned about model management since it was proposed seven years ago. this leads us to a revised vision that differs from the original in two main respects: the operations must handle more expressive mappings  and the runtime that executes mappings should be added as an important model management component. we review what has been learned from recent experience  explain the revised model management vision based on that experience  and identify the research problems that the revised vision opens up. 
categories and subject descriptors 
h.1  heterogeneous databases  
general terms 
algorithms  design  theory 
keywords 
data exchange  data integration  data translation  model management  schema evolution  schema matching  schema mapping  engineered mapping 
1. introduction 
one of the main goals of database management is to make it easier for users to write programs that access large shared databases. we call this the data programmability problem. one reason why data programmability is not easy is that it often requires complex mappings between different representations of data. those different representations arise for two main reasons: heterogeneity and impedance mismatch. heterogeneity arises because data sources are independently developed by different people and for different purposes and subsequently need to be integrated. the data sources may use different data models  different schemas  and different value encodings. impedance mismatches arise because the logical schemas required by applications are different from the physical ones exposed by data sources . in both cases  much of the work to access the data involves designing  implementing  testing  and using mappings between these different data representations. the subject of this paper is how to make this work easier. 
integrating heterogeneous data is among the oldest of database problems. it predates sigmod  which was called sigfidet  for file description and translation  before being renamed sigmod in 1. every database research self-assessment has listed interoperability of heterogeneous data as one of the main problems where more research is needed  .  
the database field has been quite successful in addressing the data programmability problem. data integration  the problem of providing access to heterogeneous data sources  has been a popular research topic for 1 years . there is a huge research literature on solutions to the heterogeneity and impedance mismatch problems. and there are many products to help solve those problems. 
however  despite this progress  coping with heterogeneity and impedance mismatch remains one of the most time-consuming data management problems. anecdotal evidence suggests that it is 1% or more of the work in enterprise it departments. one study of development projects found that coding and configuring objectto-relational mappings was 1% of the effort . it is a large and growing part of scientific  engineering and medical computing. it is needed for many web searches. and it is the essence of the semantic web vision. in short  it is a problem in need of better solutions. 
1 the nature of schema mappings 
to cope with heterogeneity and impedance mismatch  the core problem is in developing and using complex mappings between schemas. the nature of the problem depends a lot on the amount of precision required in the mapping specification. 
in enterprise it and many other domains  one needs to specify an engineered mapping between the schemas of the data to be accessed or integrated. by engineered  we mean that the mapping is precisely specified and tested for each application. 

permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod 1  june 1  1  beijing  china. 
copyright 1 acm 1-1-1/1...$1. 
 
we ll use the term data architect for the role of the person developing engineered mappings.  
at the other end of the spectrum lies approximate mappings  where users find relationships between data as they go  as in web search or in mining a heterogeneous set of data sources. in these cases  imprecision is tolerable since there is usually no welldefined notion of correct answer. in some cases  a probabilistic analysis may be able to give a formal estimate of the accuracy of the mapping. but in the end  it is usually up to the user often an end-user  not a skilled data architect to determine if the retrieved data is useful.  
in between these two ends of the spectrum are cases where both engineered and approximate mappings are developed. for example  data integration is sometimes performed incrementally  where some mappings are carefully engineered and others are done as a best-effort. this approach  called dataspaces in    arises in data exploration scenarios  such as the management of scientific data  personal information  and military command and control  and in schema extraction from text .  
the spectrum from engineered to approximate mappings is quite broad. many points along that spectrum are described in position papers at a recent workshop on data integration . although the entire spectrum is of great practical importance  we will focus on just one end of it  that of engineered mappings.  
there are many usage scenarios that require engineered mappings. one way to characterize them is to list the types of tools used to support them. the following are some common tools where engineered mappings play a central role1: 
  extract-transform-load  etl  tools  to simplify the programming of scripts to extract data from sources  clean it  reshape it  and load it into a data warehouse . 
  message mapping tools  to simplify the programming of message translation between different formats. these are often embedded in message-oriented transactional middleware  such as enterprise application integration  eai  environments . 
  query mediators to access heterogeneous databases. in database research  this is called data integration . in commercial it  it is called enterprise information integration  eii    where there are many variations  e.g.  supporting web services and updates . there are custom implementations for bio-informatics and medical informatics . this usage scenario may also be served by keyword search. 
  wrapper generation tools  for example  to produce an objectoriented wrapper for a relational database . unlike query mediators  wrappers often need to support incremental updates. some enterprise application products include custom tools for this  since the wrappers are such a large piece of the application. 
  graphical query design tools  to define a mapping between source and target schemas .  
  portal design tools  to map data sources to controls that can be conveniently displayed . 
  forms managers to map between structured data sources and forms . many enterprise application products include custom tools for this. 
  report writers that map between structured data sources and a report format .  
  olap databases  which map data sources into data cubes that are suitable for olap queries . 
  data translation tools for moving data between different  applications . for commercial applications  this role has been partly subsumed by etl tools. however  for design 
tools it is a separate product category. for example  mechanical cad tools need to translate between different geometric coordinate systems  assembly structures  and data formats .  
despite the obvious overlap in mapping functionality between these tools  there is little shared mechanism between them  in some cases even when offered by the same vendor. 
1 the problem 
given the existence of all these tools  why is it still so laborintensive to develop engineered mappings  to some extent  it is an unavoidable consequence of ambiguity in the meaning of the data to be integrated. if there is a specification of the schemas  it often says little about integrity constraints  units of measure  data quality  intended usage  data lineage  etc. given that the specification of meaning is weak and the mapping must be precisely engineered  it seems hopeless to fully automate the process anytime soon. a human must be in the loop.  
since human designers are required  the solution must lie in raising the level of abstraction in which engineered mappings are specified and in offering better tools to do that specification. we need better tools to help the data architect understand the semantics of the data to be integrated  select data sources  extract schema from unstructured sources  deduplicate overlapping data  clean up inconsistencies  choose among different types of integration tools  etl  eii  replication   design and implement mappings  debug mappings  expose mapping provenance  and revise mappings when schemas evolve. these problems and others were nicely summarized by laura haas in . 
most of these problems are hard. a lot of engineering effort is required to build tools to solve them. to maximize the functionality of the tools that can be built with a given engineering budget  we need reusable components that can be applied to a wide variety of scenarios. we already do this for the execution environment  notably with query execution engines  which are usually part of a database system or middleware framework. we need to do this for the design-time environment too we need to produce reusable components of tools. 
1 the initial research agenda 
one component that is present in all of the tools listed in section 1 is a mapping designer. this component helps the data architect design a mapping between schemas in a high-level notation. it generates code that implements the mapping  typically in a programming language or query language  depending on the scenario. ideally  it should also help the user evolve a mapping after one of the mapped schemas changes  though this is not commonly offered today.  
the need for a more powerful mapping designer was recognized by miller  haas  and hern ndez  in the first of a long series of papers about the clio project  e.g.   . the project has explored ways to simplify the data architect s job by proposing mappings based on simple correspondences between elements of the source and target schema  generating code from the mappings  and updating the mappings when one of the schemas changes . the tool can be used to generate executable mappings in a range of languages  such as sql  xquery  or xslt. some of the technology is now available in ibm rational data architect . 
an alternative to building a general-purpose mapping designer is to build an engine for schema and mapping manipulation functions that are common to a wide variety of tools for data programmability. in  we proposed such an engine  called a model management system  and refined the proposal in . model management supports operations to match schemas  merge schemas  translate schemas  diff schemas  and compose mappings. it is generic in the sense that it supports multiple metamodels and mapping languages.  
1 the revised research agenda 
initially  this model management approach seemed rather different than the mapping designer approach of clio. however  over time  the two approaches have converged and are exploring essentially the same problem space. let us see how this came about. 
the original model management proposal was influenced by the first author s experience with microsoft repository . that system was meant to support tools for application and database design and development. the tools were meant to use microsoft repository for impact analysis  dependency management  configuration management  static lineage  and other functions that required only simple relationships between artifacts.  
since the manipulation of simple relationships is well understood  the initial model management proposal used a mapping language based on them  rather than a highly expressive mapping language that would require solutions to difficult mathematical problems  such as composing and merging mappings expressed in a predicate calculus language. the simple mapping language was designed to be easy to manipulate  factoring out the problem of manipulating complex expressions that have instance-level semantics. indeed  the first implementation of a model management system followed this approach . we hoped that implementations could eventually offer extensibility hooks for plugging in and manipulating more expressive languages. 
so far  this hope has not been realized by most of our experience in applying model management to practical problems. instead  we have usually found it easier to build custom implementations for expressive mapping languages and solve the mathematical problems that this implies. in part  this is due to the choice of practical problems we have tackled problems of data integration and wrapper generation  not of design and development tools which require expressive mappings. in part  it is also due to the difficulty of developing a generic expressive mapping language and applying it to different metamodels. some would argue that this result is inevitable; a variety of approaches to generating and manipulating engineered mappings is necessary due to the wide range of data programmability problems being addressed. while it may turn out this way  we still have reason to believe that a generic model management engine is feasible. but it requires developing model management operators that manipulate highly expressive mappings  which was not the original vision. this is one reason why our vision for model management has changed. 
another outcome of our experience in applying model management to practical problems is the need for more focus on the runtime system that supports the execution of mappings. the runtime system does not simply execute queries over mappings. it must also propagate updates  notifications  exceptions  and access rights  and provide other services  such as debugging  synchronization  and provenance. these problems are sensitive to the expressiveness of mappings and to the capabilities of the model management operators that generate the mappings. that is  the ability to support a certain amount of expressiveness in mappings depends not only on design-time capabilities of a model management system to manipulate those mappings but also on runtime capabilities to provide services over those mappings. given these interdependencies  the runtime support for mappings needs to be considered as part of the model management system. this too has caused us to rethink our vision. 
recent published work from the clio group at ibm and their university collaborators has evolved in a similar direction  but from a different starting point. their early work focused mostly on the mapping design tool. however  since then they have done seminal work on two of the model management operations  compose  and inverse   and on the semantics of query answering   which is closely related to code generation. a summary of this work appears in . model management research has landed in the same place: starting with operations on schemas and simple mappings  it has evolved to focus on highly expressive mappings  like clio.  
given our experience and that of others  it is time to revisit the model management vision to review what has been learned from that experience  to revise the vision based on that experience  and to identify the research problems that the revised vision opens up. necessarily  much of this will involve summarizing our own work and that of the clio group. 
the next section introduces the abstractions and capabilities of a model management system. sections 1 explore those capabilities in more detail  describing the main operators of model management and summarizing what is known about them. 
section 1 is the conclusion. 
1. model management 
a model management system is a component that supports the creation  compilation  reuse  evolution  and execution of mappings between schemas represented in a wide range of metamodels. the user-oriented goal is to simplify the development and maintenance of applications that perform data programming. however  a model management system  mms  is not a useroriented tool. rather  it is a reusable component that can be embedded  with relatively modest customization  into useroriented tools for data warehouse loading  message mapping  query mediation  wrapper generation  report writing  and other data programmability problems.  
the main abstractions supported by an mms are schemas and mappings. since an mms should be generic  the choice of languages in which to express schemas and mappings is important. 
a schema is an expression that defines a set of possible instances  that is  database states. a metamodel is a language for expressing schemas. to enable reuse for a wide enough range of scenarios  an mms must support schemas expressed in all popular metamodels. today  that means sql  xml schema  xsd   entity-relationship  er   and object-oriented  oo  metamodels  e.g.  java  odmg   and .net   and perhaps service modeling language  sml    resource description framework  rdf   and web ontology language  owl  
. ideally  a basis set of data type constructs that are common to many metamodels could cover most of their features  with only a few specials that are included for one metamodel only. it is not a trivial undertaking to define such a universal metamodel that is elegant and has precise semantics that can be succinctly specified. however  it is clearly doable with some effort and not what stands in the way of building a powerful mms.  
the harder part is in developing technology for an mms to support mappings between many popular metamodels. it is unclear how best to go about this. one could develop a language that can express mapping constraints between schemas in the universal metamodel. while it is beneficial to have one mechanism like this  the mapping language might have to be rather complex to handle so many different types. or one could use multiple languages. for example  to map xml to sql  one could use sql as a mapping language to pull shredded data from a sql database  compose that mapping with a default xml representation of the data  and compose the result with an xquery mapping to reshape the xml.  
a mapping expresses a relationship between the instances of two schemas . we can formally define the instance-level semantics of a mapping as follows: if d1 and d1  are the sets of possible instances of schemas s1 and s1 respectively  then a mapping between s1 and s1 defines a subset of d1  d1 . usually  a mapping is expressed as a set of mapping constraints  sometimes called inter-schema constraints    each of which is a formula in some mapping language; it defines the subset of d1  d1 for which the formula holds. 
an mms must support a rich mapping language so it can be applied to a wide variety of scenarios. given the tension between the expressiveness of mapping constraints and the tractability of manipulating them  choosing the mapping language is a major design challenge. if tractability were not a consideration  one would want a mapping language that includes first-order logic with aggregation  with set and bag semantics  user-defined functions  regular expressions  rich type constructors  e.g.  to construct xml fragments   and even heuristic operations such as deduplication.  
a transformation is a functional mapping constraint  such as a query or view definition. if mappings are restricted to be transformations  and the mms needs to do nothing more than compile the transformation into executable code  then a highly expressive mapping language may be tractable. however  as we will see  an mms may need to allow non-functional mapping constraints which it can translate into transformations. moreover  an mms must do more than compile mappings. this translation and additional manipulation operations are tractable only if compromises are accepted  such as constraining the expressiveness of mappings or using algorithms that are slow or that make a best effort to solve an intractable problem. 
a closely related challenge is the choice of a common language for defining integrity constraints  that is  constraints on one schema  as opposed to mapping constraints that relate two schemas . it needs to be powerful enough to express integrity constraints supported by popular metamodels. yet it must be feasible to reason over the integrity constraints across mappings. for example  for a given source and target database that are related by a given mapping  we might need to check that if the source database satisfies the source integrity constraints then the target database also satisfies the target integrity constraints.  
the functionality of a model management system is encapsulated in its design-time and runtime operations. the main components are shown in figure 1. they are presented in sections 1  organized as follows: 
  section 1 discusses the generation of mappings either between two given schemas or between a given schema s 

 figure 1: model management system architecture and a schema generated from s. the main operations are match and modelgen. 
  section 1 discusses the generation of transformations from mapping constraints. the main operation is transgen. 
  section 1 discusses the runtime functions that are needed to support mappings. 
  section 1 discusses problems that arise from schema evolution. the solutions require several additional operations: compose  diff  extract  merge  and inverse. 
1. the origin of mappings 
there are two main scenarios for mapping generation  each with variations. in the first scenario  the source and target schemas are given and the data architect defines a mapping between them. for example  the schemas could be a data source and data warehouse schema or message schemas from two business partners. the second scenario is defined by the model management operation called modelgen; given only one of the two schemas  the other is  semi-  automatically derived along with a mapping between the given schema and the derived schema. for example  the input schema could be a data source schema and the derived schema could be an oo wrapper or form definition. we now discuss each scenario in more detail. 
1 given two schemas  generate a mapping 
a good way to think about mapping design is as a three-step process that produces mappings in three successively more refined representations: correspondences  mapping constraints  and transformations. correspondences are pairs of elements from the two schemas that are believed to be related in some unspecified way. usually  correspondences do not define a mapping. rather  they are hints that tell which elements of the two schemas need to be related by a mapping. the second step is to translate those correspondences into mapping constraints . in some cases  the mapping constraints are transformations  so step two completes the process. in other cases  the mapping constraints are not functions  so a third step is required to translate them into transformations.  
1.1 schema matching 
the problem of generating correspondences is called schema matching. there is a big literature on this topic  offering many different algorithms to compute correspondences . they include ways to exploit lexical analysis of element names  schema structure  data types  value distributions  thesauri  ontologies  and previous matches. most recent work has focused on improving the precision and recall of a schema matcher based on certain types of 

select p.id  p.name  from persons as p  
where p is of  only person   
   or p is of  only employee  =select id  name from dbo.hr select e.id  e.dept  
from persons as e  
where e is of employee =select id  dept from dbo.empl select c.id  c.name     
    c.creditscore  c.billingaddr from persons as c  
where c is of customer =select id  name   
              score  addr 
from dbo.clientfigure 1: mapping constraints between an er and sql schema
schema and instance information. such work is valuable for approximate data integration  especially in unsupervised settings like the semantic web  and for ontology integration.  
however  it is unlikely that improved precision and recall will yield big productivity gains for the data architect who is developing an engineered mapping between independently developed schemas. this is especially true for mapping tasks that are unrelated to previous ones  where there are no validated mappings to reuse. the reason is that much of the data architect s time is spent reading documentation  learning application requirements  writing functions that combine or split element values  and running tests with sample data activities that are currently beyond the reach of algorithmic solutions. for engineered mappings  we expect that the main value of the matcher is to avoid the need for tedious scrolling around large schemas by offering candidate matches to consider. thus  a better goal for this setting is to ensure that a matcher returns all viable candidates for a given element  rather than only the best one for every element  . 
the above beliefs are only educated guesses  based on a limited number of discussions we have had with product developers and users. what is missing from the literature are more comprehensive and controlled investigations of how people spend time using a schema matching tool for engineered mappings and  hence  what kinds of features would be most likely to improve their productivity. we believe the biggest productivity gains will come from better user interfaces   not from more accurate schema matching algorithms. examples include helping the user focus on the schema elements of interest by dynamically reorganizing them to fit on one screen and providing workflow assistance to track what the user knows about elements that he has already examined. 
1.1 mapping constraint generation 
given a set of correspondences between two schemas  the data architect needs to generate a transformation  such as a query or view definition over the source schema that populates the target schema.  
select value -- constructing persons 
        case 
            when  t1. from1 and not t1. from1    
               then person t1.person id  t1.person name              
            when  t1. from1 and t1. from1   
               then employee t1.person id  t1.person name   
                                           t1.employee dept  
                else customer t1.person id  t1.person name   
                       t1.customer creditscore   
                       t1.customer billingaddr  
        end 
    from     
      select t1.person id  t1.person name   
            t1.employee dept   
            cast null as sqlserver.int  as customer creditscore   
            cast null as sqlserver.nvarchar  as  
                     customer billingaddr  false as  from1   
             t1. from1 and t1. from1 is not null  as  from1   
            t1. from1 
       from    
             select  
                 t.id as person id   
                 t.name as person name                   true as  from1 
             from dbo.hr as t  as t1 
             left outer join   
             select  
                 t.id as person id   
                 t.dept as employee dept                    true as  from1 
             from dbo.empl as t  as t1 
             on t1.person id = t1.person id  
       union all   
       select  
             t.id as person id   
             t.name as person name   
             cast null as sqlserver.nvarchar  as employee dept   
             t.score as customer creditscore   
             t.addr as customer billingaddr   
             true as  from1                false as  from1   
             false as  from1 
       from dbo.client as t  
      as t1 
figure 1: a query to populate persons based on constraints in figure 1 
some tools automatically generate transformations directly from correspondences. thus  the correspondences amount to a visual programming language. in some tools the semantics of that language is unclear  so the data architect needs to read the generated transformation to understand the meaning of the correspondences . 
in our opinion  a better approach is for the mapping design tool to help the data architect translate correspondences into mapping constraints. each constraint should specify a small enough portion of the desired mapping that the data architect can easily understand what it does and hence determine whether it is what she wants .  
for example  consider the is-a hierarchy in figure 1  where employee and customer are specializations of person  which need to be mapped to relational tables hr  empl  and client . we can express the mapping constraints as equalities of simple queries  shown in the figure. the queries are expressed in entity sql   an extension of sql that can deal with inheritance and other er concepts. its syntax uses the keywords is of or is of only to test whether a variable is of a particular type. the first constraint maps the id and name of entities that are either of type person or employee to the hr table. the second constraint maps the id and dept of entities that are of type employee to the empl table. the third maps id  name  creditscore  and billingaddr of entities of type customer to the client table. each of these constraints is relatively easy to express and understand. however  these constraints imply a rather complex hard-to-understand query on tables that returns data to populate the person entity set  shown in figure 1. 
this problem of going from correspondences to mapping constraints or queries was explored in ibm s clio project. in the first clio paper   transformations are generated directly from correspondences. value correspondences are taken as input  which may include selection predicates and computations over source elements that generate a target element. with some optional user guidance  clio produces a query. for example  if the source is a relational database schema and the target is a relation schema  then the problem boils down to selecting source relations that have correspondences to the target  choosing joins between the source relations  and possibly adding selections over some of the source relations. 
in later papers from the clio project  mapping constraints are generated from correspondences. for example  in  they propose using constraints expressed as source-to-target tuplegenerating dependencies  which correspond to global-and-localas-view  glav  formulas .  a more technical definition 
appears later  in section 1.  
melnik et al. give a case where correspondences can be unambiguously interpreted as mapping constraints . intuitively  if the source and target schemas are snowflake schemas as used in data warehousing and the correspondences include one correspondence relating the roots of the two schemas  then each correspondence can be unambiguously interpreted as a mapping constraint that is the equality of two join expressions: one over the source and one over the target. see figure 1  taken from  . thus  the data architect only needs to specify correspondences and does not need to translate them into mapping constraints. this simplifies the process of designing mapping constraints  but there s a cost: the set of expressible mappings is quite constrained. it would be useful to find more expressive graphical representations that are relatively simple  like correspondences  and have a precise interpretation as constraints.  
bohannon et al.  show how to generate an xml mapping from correspondences that map one dtd to another. 
1 modelgen 
modelgen is a model management operation that automatically translates a source schema expressed in one metamodel into an equivalent target schema expressed in a different metamodel  along with mapping constraints between the two schemas.  
the first generic  i.e.  metamodel-independent  approach we know of is that of atzeni and torlone . they introduced the idea of using a repertoire of rules over schemas expressed in a 

1. ¦Ðeid name empl  = ¦Ðsid name staff  
1. ¦Ðeid city empl  addr  = ¦Ðsid city staff  
   figure 1: interpreting correspondences as constraints universal metamodel  where each rule replaces one construct by others. the universal metamodel contains modeling constructs of all metamodels. a sequence of rules is applied to the source schema to eliminate all modeling constructs that are absent from the target metamodel. their rules are expressed in c++ with abstract signatures that help them determine the correct rule sequence for a given source and target metamodel. they did not generate instance-level mapping constraints. 
two recent projects have extended atzeni and torlone s work to  among other things  generate instance translations via three datacopy steps :  1  copy the source data into the universal metamodel s format;  1  reshape the data using instance-level rules that mimic the schema transformation rules; and  1  copy the reshaped data into the target system. this approach represents considerable progress  but it has two weaknesses: it is rather inefficient for data exchange. and it still falls short of the need for modelgen to return declarative mapping constraints between the source and target schema. 
an approach to modelgen that generates declarative mapping constraints is described briefly in . it also describes a flexible mapping of inheritance hierarchies to tables  which is needed for complex enterprise applications. although there is some claim of genericity in   we do not know of a published comprehensive demonstration that mapping constraints can be generated when modelgen is applied to rich schema languages  e.g.  going from sql to xsd or from xsd to odmg .  
mcbrien and poulovassilis describe equivalence-preserving translations of schema constructs in . their goal is data integration rather than schema translation per se  but their translation rules may also be applicable to modelgen.   
1. transformation generation 
in most of today s tools where engineered mappings play a central role  data architects must design transformations manually  possibly with automated support for generating correspondences using a schema matching algorithm. if we follow the three-step approach described at the beginning of section 1  then data architects would design mapping constraints  as explained in section 1.1   which the mapping design tool translates into executable transformations. we encapsulate this translation activity in an operation called transgen  which produces a transformation that is consistent with the mapping constraints it takes as input.  
the type of transformations that are generated depends on the usage scenario. for data exchange  the transformation copies the source database into the target database. for wrapper generation  report writers  and many other scenarios  view definitions are needed to support queries on the target database. for wrapper generation in support of data access applications  the views must also enable updates on the target  i.e.  wrapper  schema to be translated into updates on the source. 
in some approaches the mapping constraints are not functions from source to target. for example  they may be glav constraints. thus  given a database state that conforms to the source schema  there may be many states of the target database schema that satisfy the constraints. usually  the desired transformation is a function that creates a target database from a source database. so one of the many target database states that satisfy the constraints must be selected. the approach taken in the clio project  is to pick one that has the semantics of certain answers : a query over the target should return only those tuples that are in the output of the query for every target database that satisfies the constraints. in some cases  the desired database state  called a universal instance  contains labeled null values that are needed to compute the answers to queries but are not allowed to be returned as part of the answer. 
another approach is to generate the transformation directly  as in microsoft s next release of ado.net . in ado.net  the target is an extended entity-relationship  er  schema  called the entity data model . the source is a relational database. users write queries and updates against the target er schema  which are translated into queries and updates on the relational source database. each constraint is expressed as an equality condition between two algebraic expressions: one over the target and one over the source  as in figure 1. these constraints allow inheritance mappings  projections  and selections  but currently do not allow nesting  as in xml  or joins. the paper describes an algorithm that translates the mapping into two view definitions: a query view that expresses the target as a function of the source  which is used to support queries on the target er schema; and an update view that expresses the source as a function of the target  which is used to translate updates on the er schema into updates on the relational source database. the views must be lossless. in mms terms  this says that the composition of the update view with the query view must equal the identity on the target. it is called roundtripping since data that passes through the update view and back through the query view is unchanged. 
as soon as one moves beyond flat relational mappings  it becomes more difficult to interpret them as transformations. the clio project has papers explaining how to interpret mappings over xml schemas   and in  how to generate xslt transformations. in ado.net the need for a sophisticated algorithm for generating transformations is in part due to the richness of inheritance mappings.  
a lot more work is needed on generating transformations. constraints need to be enriched to handle more complex mappings. yet they must still be easy to understand to the data architects who design them. in addition  it must be possible to generate efficient transformations that implement them  which is likely to expose a wealth of optimization opportunities.  
1. mapping runtime 
most of the literature on problems related to engineered mappings  especially data integration and wrapper generation  assumes that the result of the mapping design is a query or view that relates one or more  source schemas s to a target schema t. so the runtime is simply a query processor. however  there are many scenarios that imply other runtime requirements  where actions on data in the context of t need to be interpreted in the context of s  or vice versa. the difficulty of this interpretation depends on the choice of language for expressing the mapping mapst between s and t. the amount of interpretation that must be done by the runtime depends on how much of it can be done statically by an mms. for example  consider the following issues: 
  update propagation  allow updates on schema t. these may be expressed in a data manipulation language. or they may be the result of object-at-a-time updates to cached objects which are later written through to the data sources. in either case  the updates on t need to be translated into updates on s via mapst. 
  peer-to-peer  there is a chain of mappings from the schema to be queried  t   to a source s1  which is mapped to a source 
s1  etc. the mapping design tool might optimize a query on t to collapse the chain into direct mappings  e.g.  from t to s1. in any case  the runtime needs to be able to process a query on t by propagating it through the chain . 
  provenance  after moving data from source to target  a user wants to know the source data that contributed to a particular target data item. this requires design-time analysis of the mapping plus runtime support to assemble a path of data instances that show how the target was derived. 
  errors  if a data access via t is translated into an access on s that generates an error  then the error needs to be passed back through mapst in a form that is understandable in the context of t. for example  in an object-to-relational mapping  an object access may cause an erroneous access to a table that the user of t doesn t recognize. 
  debugging  like any program  a mapping needs to be debugged. this could be done with breakpoints and singlestepping  which are set in the context of t but may need to be executed in the context of s. debugging can also benefit from provenance information that shows how the mapping generated target data  as in    and from intelligent mapping of errors from s to t. 
  access control  access control constraints on the target might be enforced by a combination of constraints enforced on the server and those enforced by the client runtime. this may affect the constraint preprocessing required by the design tools to distribute the access control work between the two layers.  
  integrity constraints  integrity constraints exhibit the same design choices as for access control constraints above. there are both efficiency and feasibility issues when distributing constraint checking between the two layers. that is  due to differences in s s and t s metamodels  some constraints on t may not be expressible on s. for example  the disjointness of two sets of instances of two classes in t with a common superclass is not expressible as relational integrity constraints on s if s is relational and the classes are mapped to distinct tables. 
  indexing  it may be desirable to index data that is exposed via t to support keyword search. however  in a wrapper or query mediator scenario  the data physically resides in the 
data sources which have schemas s. for efficiency reasons  it is probably best to index the data sources and derive a mapping that enables the index to be accessed via t.  
  business logic  triggers and other business logic may be attached to data in the context of t. it may be more efficient to execute them in the context of s. this requires pushing the business logic through mapst  which should be done statically. 
  notifications  suppose data is materialized according to t  either fully  e.g.  for a data warehouse  or partially  e.g.  as a cache . then it may be valuable for certain actions on data in s to produce notifications of corresponding actions to data in t. for update actions  this is the problem of maintaining materialized views. 
  synchronization logic  data replication rules may be stated in terms of t  e.g.  that complex objects in schema t1 should be replicated to corresponding complex objects in t1. for efficiency  it may be better to translate the rules into equivalent rules on finer-grained  e.g.  relational  data in the corresponding sources s1 and s1 to be executed there. 
  batch loading  since most database systems have a high performance interface for batch loading  in many scenarios it would be more efficient to load data directly into s rather than through t. this requires transforming the data to be loaded via mapst into the format required by s s loader. 
  data exchange  suppose s and t are logical views of physical schemas sp and tp  with logical-to-physical mappings maps-sp and mapt-tp. to execute mapst on the physical databases  it may be more efficient to translate it into a transformation mapsp-tp from sp to tp.  if s is a data source and t is a data warehouse  then the mapping may have interesting characteristics  such as deduplication or other heuristic operators  staging of data in mini-batches  sorting or other blocking operators  and a variety of metamodels such as spreadsheets and pivot tables. 
solutions to many of the above problems are in hand when s and t are relational schemas and mappings are conjunctive queries. however  there are many open problems when richer data models and mapping languages are permitted. 
1. schema evolution 
when a schema changes  the objects that depend on it may no longer work properly. these dependent objects include views  queries  constraints  and programs that reference the changed schema and databases that are instances of the changed schema.  
many commercial tools to solve the engineered mapping problems of section 1 require the data architect to develop mappings. as mappings proliferate  the importance of schema evolution is likely to increase as will the need for tools to help.  
there are hardly any schema evolution tools today. this is rather surprising since there is a huge literature on schema evolution spanning more than two decades. why is this  is it because research solutions are impractical in some way  it would be valuable to have case studies that apply these research solutions to identify their strengths and weaknesses. 
many of the approaches to repairing dependent objects that are affected by schema changes require the manipulation of mappings. these manipulations can be abstracted as sequences of model management operations. we discuss some of these sequences in 

