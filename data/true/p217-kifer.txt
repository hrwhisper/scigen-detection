limiting disclosure in data publishing requires a careful balance between privacy and utility. information about individuals must not be revealed  but a dataset should still be useful for studying the characteristics of a population. privacy requirements such as k-anonymity and `-diversity are designed to thwart attacks that attempt to identify individuals in the data and to discover their sensitive information. on the other hand  the utility of such data has not been well-studied.
　in this paper we will discuss the shortcomings of current heuristic approaches to measuring utility and we will introduce a formal approach to measuring utility. armed with this utility metric  we will show how to inject additional information into k-anonymous and `-diverse tables. this information has an intuitive semantic meaning  it increases the utility beyond what is possible in the original k-anonymity and `-diversity frameworks  and it maintains the privacy guarantees of k-anonymity and `-diversity.
1. introduction
　mining data sets that contain information about individuals in a population is a great way of learning about properties of that population. applications include studying the effects of treatments on disease  tracking disease outbreaks  and building economic models  from census data . aside from this  good  information  such data sets also contain sensitive information: the disease of an individual  the salary  etc. because of this  the goal of privacy-preserving data publishing is to maximize the  good utility  while limiting the ability of an adversary to identify specific individuals and learn their sensitive information from the data set.
　in terms of privacy  k-anonymity  and `-diversity  provide strong guarantees on the confidentiality of individuals in the data. both concepts rely on generalizations to preserve privacy: attributes are replaced with less specific information  for example   state  may be replaced with  region  and  age  may be replaced with  age range  . however  the utility of these  anonymized  data sets has received much less study. many heuristics for measuring utility have been proposed  but to the best of our knowledge there are no formal measures of the utility of an anonymized dataset.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
　to make matters worse  the curse of dimensionality that haunts the statistics and machine learning communities  also has an adverse effect on anonymized data . experimental evidence  suggests that many attributes in the data need to be suppressed in order to guarantee privacy. this effect was also present in the experiments we conducted for this paper: while generating anonymized data from the adult dataset in the uci machine learning repository   attributes such as  ethnicity  had to be completely suppressed. clearly this is bad for utility no matter what measure is used.
　one way to ameliorate this curse of dimensionality is to publish additional information  such as a table containing just the ethnicities and their frequencies in the original table. clearly we can generalize this to publishing marginals  or  equivalently  duplicatepreserving projections  or views  of the original table. the marginals themselves can be anonymized  i.e.  generalization can be performed on these marginals  and the generalizations used on the marginals need not be the same. this is precisely the approach that we are proposing.
　consider figure 1: we begin with a base table  figure 1 a   and then use an anonymization algorithm to create the  1 -diverse table in figure 1 b   the precise definition of `-diversity will be given in section 1 . the current approach in the literature is to stop at this point and publish this table. note that there is additional information we can publish in terms of anonymized marginals: more detailed age information  figure 1 c    detailed zip-code/disease information  figure 1 d    more detailed joint information about age and zip code  figure 1 e    etc. given so many choices  which anonymized marginals should we publish  clearly we cannot publish them all  because if an attacker knows that a person who lives in 1 and is under 1 is in the original table  from figure 1 e  or from background knowledge   then the attacker can join figures 1 b  and 1 d  to deduce that the person has measles.
zip codeconditionagezip codeconditionage1
1
1
1
1
1
1
1
1
1
1
1
1measles
allergy
gout
cancer
flu
heart
flu
cancer
heart
flu
cancer
cancer
heart1
1
1
1
1
1
1
1
1
1
1
1
11*allergy
gout
measles
cancer＋ 1
＋ 1
＋ 1
＋ 11*heart flu
heart
flu
cancer  1
  1
  1
  1
  11*heart
flu
cancer
cancer＋ 1
＋ 1
＋ 1
＋ 1zip codeconditioncount1flu1heart1measles1cancer1heart1allergy1gout1flu1cancer1cancer1heart1zip codeagecount1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1agecount 1   1 1 1   1 1 1   1 1 1   1 1 1   1 1 1   1 1 1   1 1 c  age marginal
 e  zip/age marginal
 d  zip/condition marginal
	 a  original table	 b   1 -diverse table
figure 1: anonymized marginals　in order to answer the question of which anonymized marginals to publish  we first introduce in section 1 a new way of quantifying the amount of information  utility  contained in anonymized data  and we discuss how this information should be combined to approximate the original data. using these ideas  we will take accepted single-table privacy definitions and extend them so that they apply to collections of anonymized marginals in section 1. the technical challenges are that combining information from marginals and computing the utility require slow iterative algorithms  and checking for privacy is np-hard. we will deal with those issues in sections 1  1  and 1. in section 1 we will review the notion of decomposability from the graphical models literature. this will let us impose a structure on anonymized marginals that will simplify utility calculations and will let us develop tractable algorithms for checking for privacy. as a result  a data publisher will have the ability to examine many different collections of anonymized marginals before deciding which collection to publish. in section 1 we will briefly review the theory of log-linear models and show that publishing marginals can be viewed in a different way: it can be viewed as selecting the set of conditional independence relations that best describe the original table. the theory in section 1 is designed for ordinary marginals. in section 1 we will extend this theory to anonymized marginals and discuss how to traverse the search space of anonymized marginals. then we will describe our algorithms for checking for privacy in section 1. in section 1 we will present our experiments that show that the strategy of publishing a collection of anonymized marginals indeed provides a dramatic improvement in utility over the strategy of publishing a single anonymized table.
　in summary  our contributions include formalizing the notion of utility for k-anonymous and `-diverse tables  extending these definitions to anonymized marginals  extending results from graphical and log-linear models to anonymized marginals  including results on search-space traversal   and providing algorithms for checking for privacy.
1. preliminaries
　in this section we will introduce the notation and basic definitions that will be used later on. first we will introduce basic notation and definitions related to privacy  and then we will introduce notation for dealing with tabular data.
1 privacy basics
　let d = {t1 ... tm} be a database of tuples where each tuple has d = d1 + d1 attributes: ti = {ti.r1 ... ti.rd1 ti.s1  ...  ti.sd1}. we will slightly abuse notation and use ri to also refer to the domain of attribute t.ri and si to refer to the domain of attribute t.si. the attributes r1 ... rd1 are called the nonsensitive attributes. this is because they are either public knowledge or because they are available from some external data set. for example  date of birth  gender  and zip code are available from voter registration records and so are considered nonsensitive. the attributes s1 ... sd1 are the sensitive attributes. for example  disease  in a hospital data set  would be considered sensitive. since the dataset is given in tabular form  we will use the terms  dataset  and  table  interchangeably.
　the goal of privacy-preserving data publishing is to make it difficult for an attacker to determine that someone is in the dataset  and to make it difficult to determine the values of the sensitive attributes of individuals that are known to be in the table. one glaring example of these goals not being met is described in : a  supposedly anonymized  medical dataset of massachusetts state employees was joined to voter registration records using the attributes birth date  gender  and zip code. since the governor of massachusetts had a unique combination of those attributes  his medical records were easily identified. generally  a set of attributes  like the set {birth date  gender  zip code} in the previous example  that acts almost like a key and can be used to uniquely identify some individuals is known as a quasi-identifier :
　definition 1  quasi-identifier . a set of nonsensitive attributes {r1 ... rn} in a database d is called a quasi-identifier if this set can be used to identify at least one individual from a given population by linking those attributes to external data sets.
　without loss of generality  we assume that all the set of all nonsensitive attributes forms the quasi-identifier. to prevent linking attacks that use the quasi-identifier  it is common to use generalizations:
　definition 1  generalization . let v be the domain of an attribute t.v . a generalization w of v is a new domain formed by partitioning v into disjoint buckets and identifying all the points in a bucket with one value in w. a generalization map is a function φ : v ★ w such that φ v  corresponds to the bucket that contains v.
　as an example  consider the integer-valued attribute age. one generalization of age is the set of intervals a1 = { 1   1   1   1   1   1  ...}. the generalization map from age to a1 replaces each integer with an interval. a1 itself can also be generalized. one such generalization of a1 is a1 = { 1   1   1   1  ...}. note that a1 is also a generalization of age  generalizations are transitive . thus we can define a partial order on domains: a b if and only if b is a generalization of a  note that a a is always true . to generalize a table  we choose a generalization for each attribute and apply the appropriate generalization maps to the attributes of all tuples t. we can perform generalizations on the nonsensitive attributes  quasi-identifier  to make linking attacks difficult. this is the goal of k-anonymity .
　definition 1  k-anonymity . a table d satisfies k-anonymity if for every tuple t （ d there exist at least k 1 other tuples that have the same values as t for every quasi-identifier.
　given the quasi-identifier {zip code  age}  the table in figure 1 b  is 1-anonymous. generalizations partition the tuples into anonymized groups:
	definition 1	 anonymized group . an anonymized
group is a  set-wise  maximal set of tuples that have the same  generalized  value for each nonsensitive attribute.
　note that k-anonymity says nothing about the sensitive attributes. in particular  it does not prevent all tuples in an anonymized group from having the same value for some sensitive attribute  thus benefiting an attacker who knows some of the individuals that are in that anonymized group . the concept of `-diversity is designed to guard against this. machanavajjhala et al  give several alternative formulations of `-diversity. any of them can be used here  but for concreteness  we will use the following:
　definition 1   c `  diversity . let c   1 be a constant and let q be an anonymized group. let s be a sensitive attribute  let s1 ... sm be the values of s that appear in the group q and let r1 r1 ... rm be the their corresponding frequency counts in q. let r 1  r 1  ... r m  be those counts sorted in non-increasing order. we say that the anonymized group q satisfies  c ` -diversity
m
with respect to a sensitive attribute. the set
{s 1 } is the head and the set {s `  ... s m } is called the tail.
　intuitively  `-diversity means that an adversary needs ` 1 pieces of background knowledge to eliminate `   1 possible values of a sensitive attribute in order to breach privacy. like k-anonymity  `-diversity can be achieved through generalizations. throughout the paper we will assume there is only one sensitive attribute. this is only necessary for clarity. the extension to multiple sensitive attributes is straightforward and is done in the same way as in .
　note that there are other ways of sanitizing data: tuple suppression   adding random noise  1  1  1   and swapping attributes between tuples . we will restrict our attention to generalizations because they are the most faithful to the data: any fact in a generalized table is true of the original table. for example  if the value for the age attribute has been generalized to  1   1  then we know for certain that the age is within that interval. furthermore  we could determine exactly how many individuals in the original table were between 1 and 1 years old. faithfulness is an important concept because it may be used by future data mining algorithms to give quality guarantees on their results.
1 tabular data
　recall that our database d = {t1 ... tm} is a set of tuples where each tuple has d = d1 + d1 attributes: ti = {ti.r1  ...  ti.rd1 ti.s1  ...  ti.sd1}. the domain of d  domain d   is the crossproduct r1 〜 ，，， 〜 rd1 〜 s1 〜 ，，， 〜 sd1. the nonsensitive domain  nonsendomain d   is the domain of the nonsensitive attributes: r1〜，，，〜rd1; the sensitive domain  sendomain d   is the domain of the sensitive attributes: s1 〜 ，，， 〜 sd1. when all the attributes are discrete  or have been bucketized   it is convenient to think of the data set as a contingency table t d : for any t （ domain d   t d  t  is the number of times t appears in d. whenever it is unambiguous  we will drop the explicit notational dependency on d and refer to the corresponding contingency table as t. let c   domain d . t c  is the number of tuples in d that also belong to the subset of the domain represented by c  we will use lower-case letters to denote tuples and upper-case letters to denote sets . thus we can think of t as a function defined on the powerset of domain d .
　we will also be concerned with marginals of the contingency table t d . let a be a set of attributes. then ta d  is the marginal contingency table that we get by projecting out all attributes of d that are not in a  while preserving duplicates . for example  if
a = {r1 r1} and t =  r1 r1  then is the number of tuples in d whose value for r1 is r1 and whose value for r1 is r1. note that the original contingency table t is itself a marginal  and that the marginal t  can be thought of as the function that always returns |d|  the number of tuples in the database . we will use the term anonymized table to refer to a table that has been altered through the use of generalizations. in particular  when we apply generalizations to a marginal contingency table  the result is an anonymized marginal.
1. utility measures
　in this section we review current utility measures for anonymized datasets  section 1  and then discuss a more formal measure of utility  section 1  which has connections to maximum entropy and conditional independence  and which will be suitable for anonymized marginals.
1 current measures of utility
　one of the earliest utility metrics is generalization height . generalization height is the total number of generalization steps that have been performed on the original data set. the intuition behind it is that a generalization step represents a loss of information  so one should use as few generalization steps as possible. as noted in   the problem with this approach is that not all generalization steps are created equal: a generalization step on one attribute may put many more tuples into an anonymized group than a generalization step on another attribute.
　two similar metrics that take anonymized group size into account are the average size of anonymized groups  and discernibility . discernibility assigns a cost to each tuple based on how many other tuples are indistinguishable from it. if a tuple is suppressed  its cost is |d|  the number of tuples in the original data . if a tuple is not suppressed  its cost is the number of tuples in its anonymized group. thus the discernibility is the sum of the squares of the anonymized group sizes plus |d| times the number of suppressed tuples. while appealing  neither of these two metrics takes the data distribution into account. an anonymized group where the original attributes were uniformly distributed represents less information loss than an anonymized group whose original attributes were skewed. for example  suppose there are 1 people who have the same value for every attribute except age  and their ages are between 1 and 1. in a sense  our best guess  using maximum entropy  or the principle of indifference  is that each age is equally likely. intuitively  had the original ages been 1 ... 1  this would not have been as much an information loss as the case where 1 people were 1 years old and the other 1 were 1. iyengar  presents a related loss metric which considers how many elements in the original domain have been grouped together. since it also ignores the tuple distribution  it has the same shortcoming.
　two utility metrics that take distribution into account are the classification metric  and information-gain-privacy-loss ratio . the classification metric is appropriate when one wants to train a classifier over the anonymized data. thus one attribute is treated as a class label. the classification metric assigns a penalty of 1 to every tuple that is suppressed. if a tuple t is not suppressed  we look at the majority class label in its anonymized group. if the class label for t is different than the majority class label  t is assigned a penalty of 1. the classification metric is then the sum of all penalties. the classification metric is an appealing measure of utility because it considers a possible use for the data. however  it is not clear what we should do if we want to build classifiers for several different attributes. the information-gain-privacy-loss ratio is also designed for the purposes of classification. it is a local heuristic in the sense that it is used to determine the next generalization step  it is similar to the way information gain is used to choose the next split point in a decision tree . here as well it is not clear what we should do if we want to build classifiers for several different attributes. furthermore  because it is a local heuristic  it is difficult to compare the utilities of two different anonymized tables.
1 a formal utility measure
　one of the main goals of data mining and statistics is to make statements about the probability distribution that generated the data - this is certainly true of classification  parameter estimation  hypothesis testing  and regression. in this spirit  we view the data as an iid  identically and independently distributed  sample generated from some multidimensional distribution f. here we shall assume that all attributes are discrete. note that if we have a continuous domain  we can bucketize it and treat the collection of buckets as a discrete domain; other ways of dealing with continuous attributes are the subject of future work. with this simplification  the data follows a multinomial distribution. suppose the tuples in our dataset have discrete-valued attributes u1 ... un. then we can estimate f with the empirical distribution f . f  u1 ... un  is an estimate of the probability p t.u1 = u1 ... t.un = un  and is defined as the fraction of tuples in the database that satisfy this constraint  i.e.  t.u1 = u1 ... t.un = un .
　now that we have given a probabilistic interpretation to the original data  we will proceed to do the same for the anonymized data. suppose we are given a collection of anonymized marginals that were derived from the same table. we can view these marginals as constraints. figure 1 shows a set of anonymized marginals that correspond to the table in figure 1 a  that has 1 tuples. these anonymized marginals impose constraints such as: 1% of the tuples have age between 1 and 1  figure 1 c  ; 1% of the tuples are in zip code 1  figure 1 d  ; 1% of the tuples are in zip code 1*  have cancer  and have age at most 1  figure 1 b  . thus given anonymized marginals  we can compute the maximum entropy probability distribution that corresponds to these constraints. we will see in section 1 that this maximum entropy distribution is exactly the same as the maximum likelihood distribution for a multinomial model that satisfies certain intuitive conditional independence requirements.
　we now have a probability distribution f 1 associated with the original data  and a probability distribution f 1 associated with the released anonymized marginals. the next step is to compare them. let x1 ... xn be the elements of the multidimensional domain for our data. let p 1 i be the probability of xi according to f 1 and let be the probability according to f 1. the kullback-leibler divergence  kl-divergence  between f 1 and f 1 is defined as:
x
i
the kl-divergence is minimized only when f 1 = f 1. in section 1  we shall see that the kl-divergence is equal to the difference in log-likelihood when we estimate the  true  distribution f with the original data and when we estimate the  true  distribution f with the anonymized marginals. since our goal is to determine which anonymized marginals to release  we will be changing f 1 but not f 1. thus minimizing the kl-divergence will be mathematically equivalent to maximizing p which is  1 times the cross-entropy between f 1 and f 1. we will use the standard convention that 1log1 = 1 so that we only need to compute probabilities for cells that appear in the original table.
　note that this approach is similar in spirit to  and : we are trying to reconstruct the original distribution as accurately as possible given anonymized  but unperturbed  data while  and  try to reconstruct the original distribution given a dataset with additive noise.
1. extending privacy definitions
　the next step is to extend the privacy definitions for k-anonymity and `-diversity from single-table anonymized data to collections of anonymized marginals. we can extend k-anonymity in two ways  reflecting the motivation provided by sweeney . following   we can protect the anonymized marginals from being linked to external data by requiring every anonymized marginal to be kanonymous.
　definition 1  k-link anonymity . a collection of anonymized marginals m1 ... mr satisfies k-link anonymity if for all i = 1...r and for all t （ nonsendomain mi  either mi t  = 1 or mi t  − k.
　we must also be sure that an adversary cannot use combinatorial techniques to determine that a tuple with a certain value for its quasi-identifiers exists in the original table and that the number of such tuples is less than k.
　definition 1  k-combinatorial anonymity . let d be the domain of the nonsensitive attributes. a collection of anonymized marginals m1 ... mr satisfies k-combinatorial anonymity if for all t （ d one of the following holds:
1. for all tables t consistent with the marginals m1 ... mr  t t  = 1
1. there exists a table t consistent with the marginals m1 ...  mr such that t t  − k.
　our final privacy requirement is a straightforward generalization of `-diversity. using the anonymized marginals  the maximum entropy principle  and techniques we will discuss later  we can fill in  fractional  cell counts for the original table. thus we directly apply the definition of `-diversity  to the reconstructed table  i.e.  for each combination of nonsensitive attributes  the sensitive attributes must have ` well-represented values .
　definition 1  maxent `-diversity . the anonymized marginals m1 ... mr satisfy maxent `-diversity if the maximum entropy distribution that is consistent with m1 ... mr satisfies
`-diversity.
1. statistical models
　in this section we will discuss how to combine information from ordinary marginals to estimate the original data; in section 1 we will show how to apply this theory to anonymized marginals. the estimator can be viewed as a maximum entropy distribution as well as a maximum likelihood estimator for multinomial models. this will give marginals interpretations as constraints and as statements about conditional independence. in general  computing the maximum entropy distribution requires iterative algorithms  1  1  1 . however  with some additional restrictions on the allowable marginals  there is a closed-form solution . this lets the data publisher examine many different collections of marginals before deciding which ones to publish. we will discuss how to compute the maximum entropy distribution in section 1 and we will discuss the connection to maximum likelihood in section 1.
1 decomposable graphical models
　given a set of marginals m1 ... mr  build an interaction graph in the following way: the vertices of the graph are the attributes that appear in any marginal. for any two vertices a and b  draw an undirected edge between a and b if attributes a and b appear together in some marginal. figure 1 a  shows an interaction graph

	 a  interaction graph	 b  components under a de-
composition
figure 1: interaction graph and decomposition for marginals
abc  bcd  de
ab
cd
figure 1: smallest non-decomposable graph
for the three marginals whose attributes are abc  bcd  and de our first requirement is that the interaction graph must be triangulated:
　definition 1  triangulated graph . an undirected graph is triangulated if for every cycle of length 1 or more  there exists an edge not in the cycle that connects two vertices in the cycle.
　intuitively this means that every cycle that has more than 1 nodes has a  shortcut . undirected triangulated graphs are equivalent to undirected decomposable graphs . to explain this concept  we need the following definitions:
　definition 1  separator  . let g =  v e  be an undirected graph and let a b   v be disjoint sets of vertices. the set c   v  {a“b} separates  is a separator of  a and b if every path from a to b contains a node of c.
　definition 1  decomposition  . let g =  v e  be an undirected graph and let a b c   v be disjoint sets of vertices such that v = a“b“c. then a b c form a decomposition of g if c separates a and b  and c is complete  every two vertices in c have an edge between them .
a decomposition  a b c  splits a graph g =  v e  into two components. the first component is the subgraph of g that contains the vertices a“c and the second component contains the subgraph induced by the vertices b “ c. if both a and b are nonempty then the decomposition is proper  each component is strictly smaller that the original graph g .
　definition 1  decomposable . a graph is decomposable if it is complete or if it has a proper decomposition where each component is decomposable.
	theorem 1	 triangulated graphs  . an
undirected graph is decomposable if and only if it is triangulated.
　figure 1 b  shows the components of the graph in figure 1 a  under the decomposition {a} {e} {b c d}  where {b c d} is the separator . because of the equivalence between decomposable and triangulated graphs  it is easy to check whether a graph is decomposable. figure 1 is the smallest non-decomposable graph  if it had an edge a and b or c and d then it would be triangulated/decomposable .
　definition 1  graphical marginals . let m1  ...  mr be a collection of marginals and let g be the corresponding interaction graph. the collection m1 ... mr is graphical if the marginals contain all of the maximal cliques of the corresponding interaction graph g.
in other words  if ei is a maximal clique of the interaction graph  there is a marginal mj such that all of the attributes that correspond to vertices in ei are the attributes of mj.
　definition 1  decomposable marginals . a set of marginals m1 ... mr is decomposable if it is graphical and the corresponding interaction graph g is decomposable.
　the interaction graph in figure 1 a  is generated by the marginals with attributes abc  bcd  and de. it is also generated by abc  bd  cd  and de. while the interaction graph is decomposable  the maximal cliques correspond only to abc  bcd  and de. thus the set of marginals {abc bcd de} is decomposable while {abc bd cd de} is not.
　decomposability is important to us because the maximal cliques can be ordered in a perfect sequence  which can be used to compute the maximum entropy distribution:
　definition 1  perfect sequence  . let g be a graph and let e1 e1 ... ep be a sequence of complete subgraphs of g that includes all the maximal cliques of g. the sequence e1 e1 ...  ep is perfect if for i = 1...p  the set si = ei ”  e1 “
，，，“ei 1  is complete and there exists a j   i such that si   ej.
　returning to our example in figure 1 a   we see that {a b c}  {b c d}  {d e} is a perfect sequence. it is a fundamental fact that every decomposable graph has a perfect sequence . also  it is easy to see that si separates  e1 “，，，“ei 1  si and ei  si. the sets e1 ...ep and s1 ... sp  note the separators are numbered starting from 1  can be used to compute the maximum entropy  maxent  distribution. let e1 ... ep be sets of attributes that correspond to a decomposable graphical model and that have already been arranged in a perfect sequence. let s1 ... sp be the corresponding separators as in definition 1. for each i  let tei be the marginal of the base table t corresponding to attribute set ei and tsi be the marginal corresponding to the attribute set si. for each t （ domain t  let tei be the projection of t onto the attributes in ei and similarly for tsi. the maximum entropy probability associated with t  which is also the maximum likelihood estimate associated with log-linear models  which will be briefly discussed in section 1  is :
		 1 
and  given a table of size |t|  the expected count of tuples in the cell corresponding to t is therefore:
		 1 
　by definition  for any j  there is an i such that sj   ei so that tsj can be computed from tei and so the maximum entropy distribution can be computed from the marginals corresponding to e1 ... ep with little effort once they are ordered in a perfect sequence  an algorithm for such an ordering is provided in  .
1 log-linearmodels: theconnectiontomaximum likelihood
　an interaction graph  e.g.  figure 1 a   also has an interpretation in terms of conditional independence . let a  b  and c be disjoint sets of attributes  vertices  in the interaction graph and suppose that every path from a vertex in a to a vertex in b contains a vertex in c. intuitively  the effects of a and b on each other are blocked by the separator c and thus we have the interpretation that a and b are conditionally independent given c. using the notation that ta  tb  ta“b and tc are the projections of t onto the attributes in a  b  a “ b  c  respectively  we can write the conditional independence restriction mathematically. for a （ a b （ b c （ c:
 1 
to see the relation to equation 1  suppose we were given two marginals ta“c and tb“c. we can compute tc from either ta“c or tb“c. we can estimate p ta“c  using the count data  and similarly for tc and tb“c . by substituting these estimates in the right side of equation 1 we get

which is the same as the maximum entropy estimate  equation 1 .
　to formally relate maximum entropy and maximum likelihood in multinomial models  we must first discuss log-linear models - popular statistical tools for analyzing contingency tables .
　let t be a cell in a contingency table  and let q t  be the expected cell count under a multinomial model. the goal of statistical analysis of contingency tables is to learn about some dependencies between the q t ; in particular  the goal is to determine how q t  is affected by the various attribute dimensions of the contingency table1. in this analysis  one builds a linear model for predicting logq t   there are technical reasons for modeling logq t  rather than q t  . to see how this works  suppose our table has 1 attributes a and b. for a cell t （ a 〜 b  let q t  be the expected cell count for t. for this scenario  the saturated log-linear model is:
	logq a b  = u + ua a  + ub b  + uab a b 	 1 
where u represents a baseline cell occupancy based on no interactions  ua represents the effect of attribute a on cell occupancy  beyond the effects of the baseline   ub represents the effects of attribute b on cell occupancy  beyond the effects of the baseline  and uab represents the effects of the interactions between a and b  beyond the individual effects of a  b  and the baseline . in the general case  a saturated model has a term for each subset of the attributes. the saturated model is not very interesting because it overfits the data: the maximum likelihood estimator for q t  is just the number of times t appears in the data. thus the saturated model is also called the unrestricted model. because the unrestricted model is too powerful  i.e.  it overfits the data   a typical statistical analysis would only look at a subset of the possible interaction terms. for example  if our table had 1 attributes a b c  we could try to model it with
	logq a b c  = u + ua a  + ub b  + uc c 	 1 
since there is no interaction term  i.e.  uab  uac  ubc  uabc   this model seems to suggest that attributes a  b  and c are independent. this is not a coincidence: we will see that a  b  and c are indeed independent in the maximum likelihood distribution for that model. in general  the maximum likelihood estimates for a restricted model will not be the same as the cell counts  unlike the case for saturated models . thus saturated models will always have higher log-likelihoods. a common way of measuring how well a restricted model fits the data  is to look at the difference between the log-likelihood of the saturated model and the log-likelihood of the restricted model . since these models estimate parameters for a multinomial distribution  i.e.  the cell counts or  equivalently  the cell probabilities   the difference in log-likelihoods is exactly the kl-divergence between the two respective maximum-likelihood distributions.
　most log-linear models used in practice have the following property: if an interaction term is included in the model  then its lower order effects are also included. for example  if a log-linear model has a uabcd term  then there is a term ux for every x  {a  b  c  d}. log-linear models that have this property are called hierarchical  and we shall restrict our attention to these types of models.
　just as with marginals  we can build an interaction graph for hierarchical log-linear models: the vertices are the attributes and there is an edge between two vertices if both are contained in some interaction term  for example  if the model had a uabc term  there would be edges between a and b  b and c  and a and c . it is common to describe hierarchical log-linear models using only the highest order interaction terms : if a model has a parameter uabcd and ubc  we would omit ubc because it is implied by the uabcd term. it is also common to represent interaction terms as  abc  rather than uabc. thus the log-linear model in equation 1 can be compactly represented as  ab  and the log-linear model in equation 1 can be compactly represented as  a  b  c . this compact representation is sufficient for constructing the interaction graph.
　the log-linear model is graphical if the interaction terms are exactly the maximal cliques of the corresponding interaction graph  and it is decomposable if it is graphical and if the interaction graph is decomposable as well. figure 1 a  shows the interaction graph for the log-linear model  abc  bcd  de .
　the similarity between the interaction terms of a decomposable log-linear model and the marginals  of the base table  with the same attributes is not superficial. the model can be built using only the marginals whose attributes are specified by the interaction terms  and the maximum likelihood estimates of cell probabilities and expected values for a graphical decomposable log-linear model are given by equations 1 and 1  respectively . furthermore  any conditional independence relations that we can read off the interaction graph are also true of the maximum likelihood estimator :
　theorem 1. given a graphical  decomposable interaction graph for a log-linear model  if the sets of variables/nodes a and b are separated by c  then under the maximum likelihood distribution  p a b|c  = p a|c p b|c .
　thus we have two complementary goals in releasing marginals: to provide a set of constraints  marginals  that lead to a maximum entropy distribution that is as close as possible to the real data subject to privacy restrictions; and to determine a set of conditional independence relations that best approximates the data subject to privacy restrictions.

figure 1: age hierarchy
1. anonymized marginals
　in this section  we will provide a reduction from anonymized marginals to ordinary marginals  section 1  and we will use this reduction to extend the theory of graphical models to anonymized marginals  section 1 .
1 a reduction
　for each attribute ri  let gi be the set of possible generalizations of ri which are to be considered. for example  we may specify that we are only interested in three generalizations for the age attribute: one that partitions age into the intervals  1    1   ...; one that partitions age into intervals  1   1    1   1   ...; and one that partitions age into the intervals  1   1    1   1   .... for tractability  and for clarity  we will restrict our attention to the case where  for each ri  the generalizations in gi are totally ordered according to    section 1 . this restriction is common in the literature   1  1  . without loss of generality we can assume that the most general generalization simply suppresses the attribute. first  we require that a collection of anonymized marginals m1  m1  ...  mr only use the anonymizations we specify.
　definition 1  validity . let m1 m1 ... mr be a set of marginals and let r1 ... rd be the attributes that appear in those marginals. for i = 1...d  let gi be a set of generalizations for ri such that gi is totally ordered according to  . then m1 ... mr are valid with respect to g1 ... gd if for every marginal mj and every attribute rq that appears in mj  rq has been generalized according to one of the generalizations in gq.
note that attributes can be generalized differently in different marginals: if age appears in marginal m1 and marginal m1  it could have been generalized using the intervals { 1   1  ...} in m1 and generalized using the intervals { 1   1   1   1  ...} in m1. the reduction from anonymized to ordinary marginals relies on the fact that a totally ordered set of generalizations induces a natural hierarchy on the base domain. the reduction proceeds as follows. let rj be an attribute and gj be a set of possible generalizations for rj. let h be the number of generalizations in gj. the first step is to label the generalizations so that the most general generalization is labeled g1  the second most general generalization is labeled g1  etc. figure 1 shows  part  of a hierarchy over the age attribute. here we have four generalizations g1  g1  g1  g1  where g1 is equivalent to suppressing the entire attribute. each generalization represents one level of the hierarchy  and each node in the hierarchy tree has a bounded number of children  since we have finitely many data points . for each node  we order its children  arbitrarily  and number them according to that order. we will use this numbering to create a new set of attributes. for i = 1 ... h   1  let ci be the maximum number of

generalization g1count1111111111111111............1111............
generalization g1lage1lage1count111111.........11.........figure 1: induced attributes
children for any node in generalization gi  in our example  c1 = 1  c1 = 1  and c1 = 1 .
　we can now treat the attribute rj as an  h 1 -dimensional vector of induced attributes where the ithdimension has ci points. a point  x1 ... xh 1  in this space represents the path taken from the root g1 to a leaf node. it is easy to see that any generalization gi （ gj of rj corresponds to the subspace consisting of the first i   1 dimensions   . figure 1 shows the induced attributes for two of the generalizations in figure 1. by applying this reduction to every attribute in every anonymized marginal m1 ... mr  we get a new set of marginals
. the only restriction on these new marginals is that if attribute appears in some marginal mi1 then mi1 must also contain the attributes. thus we view the set of induced attributesat resolution level i. the higher the resolution  the more information there is about attribute rj.
1 extensions of the theory
　given the reduction in section 1  the notions of interaction graph and decomposability carry over directly to anonymized marginals. in this section  we will discuss how this affects the maximum entropy distribution and the statements of conditional independence. we will also discuss how to traverse the search space of anonymized marginals.
　for anonymized marginals  the probability computation  equation 1  needs to be extended to deal with the case where some attribute never appears at its highest level of resolution. recall that for any attribute b  the induced attributes represents a path from the root to an interior node of the generalization tree for b. let child t l ib   be the function that first projects t onto the induced attributes to get a node in the generalization tree and then returns the number of leaves in the subtree rooted at that node. for example  let us consider the left-most table in figure 1. if t is a tuple in the first row  then child t lage1   = 1 because t projected onto gives us the left-most child of the root  and it has 1 leaf descendants.
　theorem 1. let e1 ... ep be the maximal cliques of a decomposable graphical model g arranged in a perfect sequence  and let s1 ... sp be the separators  as defined in definition 1 . let b1 ... bq be the original attributes. for each original attributebe the induced attribute of bj that appears in one of the marginals  that correspond to the ei  such that does not appear. then the maximum entropy probability of a tuple t （ domain t  is:
		1
			 1 
 bj  child t lmax
this says that the probability for a tuple t  that is not at the highest level of resolution  is spread uniformly across all possible completions of t.
　the interpretation of conditional independence can also be extended to anonymized marginals. let duced attributes for original attribute a. recall that
represent a at a lower level of resolution   rep-
resents a at an even lower level of resolution  etc  note if appears in a marginal then we have the requirement that appears in the same marginal for all j   i . thus generalizing a marginal involves suppressing  marginalizing  the induced attribute li a  with the largest index  i.e.  the marginal does not contain an induced attribute with j   i . equivalently  generalizing a marginal can be seen as reducing the level resolution for  original  attribute a. the conditional independence interpretation relies on the following theorem:
　theorem 1. let x  y   and c be disjoint sets of vertices of an interaction graph g for graphical decomposable anonymized marginals m1 ... mr. if x and y are complete subgraphs of g and c is a set-wise minimal  i.e.  no subset of c has this property  separator of x and y then the following is true
1. c is complete  and therefore the vertices corresponding to c correspond to attributes that are contained in some marginal
mj 
	 then for all	.
 andthen i   j  i.e.  c has lower resolution information about original attribute a than does x .
　theorem 1 says that if x is a set of attributes that appear in some anonymized marginal  and y is a set of attributes that appear in some anonymized marginal  the minimal separator c between x and y is also a set of attributes in some anonymized marginal. furthermore  x and y do not contain any of the same original attributes  even at different levels of resolution  for it does not make sense to talk about independence between the age ranges { 1   1   1   1  1 ± } and { 1   1  1   1 ± }. additionally  if x and c  or y and c  have information about the same original attributes  then the information in x is an incremental gain in resolution over c. thus in addition to statements about conditional independence of attributes  we also have statements about independence of resolution:  given some the level of resolution in c  the extra precision in x and y is independent.  for example  suppose we have a table of flu patients categorized by geographical region and age range   1   1   1   1   etc . given a marginal that consists of age ranges   1   1   etc  and a marginal that consists of states  instead of just geographical regions   the maximum entropy distribution would be consistent with the assumption that given the first table  increased resolution in age is independent of the increased resolution in location. in other words  once we know that a flu patient is in the northeast and is between 1 and 1 years old  knowing the exact state of a region would not help narrow the age range  assuming the maximum entropy distribution is correct .
　the previous example raises an important issue - how correct is the maximum entropy distribution  intuitively  adding additional marginals or merging marginals together  releasing abcd instead of ab and cd  gives us additional information and should help us better approximate the original distribution. in fact  this is also true mathematically.
　theorem 1. let g and h be the interaction graphs of two decomposable graphical models. if the vertices of h are a subset of the vertices of g and the edges of h are a subset of the edges of g  then the maximum entropy distribution for g approximates the original table at least as well as the maximum entropy distribution for h  in terms of the kl-divergence .
note that the case where the vertices of g and the vertices of h are the same is proved in . since generalization may completely remove some induced attributes from all of the marginals  this removal will result in a model with less nodes as well as edges. thus we need the result that adding edges and vertices to h  when the vertices of h are a subset of the vertices of g  never hurts utility.
　the only case where adding marginals or merging them would not increase utility is when the tuple distribution of original table is exactly the maximum entropy distribution for that set of marginals. since that is unlikely in practice  even the following simple technique is almost certainly guaranteed to improve the utility of a single anonymized table t1 that was derived from a base table t: take the marginal m of t that has all attributes but the sensitive ones. create a k-anonymous version m1 of m. then releasing m1 and t1 gives more utility than releasing t1 alone  as is the standard practice in the literature . in section 1 we will discuss how to make sure that privacy guarantees still hold.
　we conclude this section with a discussion of how to select a set of anonymized marginals to publish. it is known that model selection for decomposable graphical models requires an exhaustive search  and that even finding an optimal k-anonymous table is np hard  1  1 . therefore a search algorithm such as a genetic algorithm or a random walk on the space of models is needed. we will briefly discuss how to extend results on stepwise edge/vertex selection  1  1  that will allow us to go from one graphical model to another.
　the following three conditions need to be simultaneously satisfied in order to remove an edge connecting and:
1.  and	 cannot appear together in 1 or more marginals
 equivalently  they do not both appear in a minimal separator between two nodes . this rule  due to wermuth  ensures that the resulting model is decomposable .
1. a 1= b.
1. there is no edge connecting and	 with i1   i  and similarly for j .
the last two conditions ensure that the induced attributes in every marginal describe a path from the root to an interior node of the generalization tree  instead of only a subset of a path  and therefore correspond to an actual generalization. a node l ia  can be removed if
   there is no node with j   i.
this also ensures that the induced attributes in every marginal describe a path from the root to an interior node of the generalization tree. note that removing a node from a decomposable graph results in a graph that is decomposable . a node can be added to a graph if one of the following is true
   i = 1  in which case the node is added with no edges  or  is already in the graph for every j ＋ i 1. in this case  l ia  is added with an edge to every lj a .
and edge between  and  can be added if the following conditions hold:
1. there exists a minimal separator s between and such that every node in s has an edge to both l ia  and l jb . this rule  due to deshpande et al  ensures that the resulting model is decomposable .
1. i = j = 1 or for all i1   i there is an edge from to
  and similarly for j 
1. algorithms
　in this section we discuss procedures for checking a set of anonymized marginals for privacy. the first criterion  from definition 1  is that an attempt to link any marginal to external data will give either 1 or at least k tuples. thus it is sufficient to check that each marginal satisfies k-anonymity by itself.
　the next requirement  k-combinatorial anonymity  is more stringent. an adversary should not be able to use combinatorial tools  such as the inclusion-exclusion principle  to determine that for all tables consistent with a set of given marginals  a particular cell must have between 1 and k   1 tuples  for then this cell can be linked back to external data . in general  checking for privacy by computing upper and lower bounds for a cell is np-hard . however  when the marginals correspond to a decomposable graphical model  exact bounds can be computed in closed form. dobra's bounds  extend to anonymized marginals: the cell count t t  is bounded by
	t t  ＋ min te1 te1  ... tep tep  	 1 
 note the similarity to equation 1  and this bound is tight in the sense that for each upper bound  there exists a table that achieves it. checking for k-combinatorial anonymity relies on equation 1 and a variant of the maxent `-diversity algorithm that is described below. details are omitted due to lack of space.
　checking maxent `-diversity for all points in nonsendomain t  is a harder task. first  there are several simplifications we can perform:
　proposition 1. let m1 ... mp be a set of anonymized marginals in a graphical model. let v be the set of induced attributes that appear in at least one of the mi. then checking for maxent `-diversity in domain t  is equivalent to checking for maxent `-diversity in domain tv  .
proposition 1 tells us that the we do not need to worry about any level of resolution that does not appear in the marginals. thus we can use equation 1 instead of equation 1 for our computations.
　proposition 1. let e1 ... ep be sets of  induced  attributes arranged in a perfect sequence  definition 1  and let s1 ... sp be the corresponding separators. if any of these two conditions hold  then the set of anonymized marginals {te1 ... tep} does not satisfy maxent `-diversity:
1. the marginal tei is not `-diverse  individually  for some i.
1. there exists a j such that the intermediate product
p corresponds to a probability distribu-
tion that is not `-diverse.
proposition 1 tells us that sometimes intermediate results  rather than the complete maximum entropy distribution  can be used to determine if a set of marginals does not satisfy entropy `-diversity.
　proposition 1. let c be the sensitive attribute. let te1  ...  tep be decomposable and graphical  anonymized  marginals  let e1 ... ep be sets of  induced  attributes arranged in a perfect sequence  definition 1   and let s1 ... sp be the corresponding separators. let be the subsequence of the ei consisting of precisely the ei that contain c. let be the subsequence of the si consisting of precisely the si that contain c. then the following is true:
1.  are graphical and decomposable marginals.
1.  is a perfect sequence and are the corresponding separators to the perfect sequence  and hence p1 = p1 .
1. checking-diversity in c is equivalent to checking te1 ... tep for maxent `-diversity in c.
　proposition 1 tells us two things. first  marginals that do not contain the sensitive attribute do not affect maxent `-diversity at all. thus if we publish a table that is both `-diverse and k-anonymous  we can approximate the original table better  while preserving privacy  just by releasing additional k-anonymous marginals that do not contain the sensitive attribute. this is already an improvement over the standard technique of just releasing one table. second  by ignoring marginals without sensitive attributes  we get a smaller decomposable graphical model to which we can apply propositions 1 and 1. this lets us cut down on the size of the domain that must be checked.
　it is clear that checking for maxent `-diversity can be done in time that is linear in the size of the join of all the marginals containing the sensitive attribute. in cases where the sensitive attribute has a small domain  we can use the decomposable property of the interaction graph to reduce the complexity even further: we will prune away tuples that do not need to be joined. we will also present a variant of this pruning algorithm for the case where |c| = `. in this case  the running time will be o |c|1p|j|  where p is the number of marginals  |j| is the size of the largest join between 1 marginals  not counting duplicates   c is the domain of the sensitive attribute and |c| is its size. for other cases  when the overall join size is too large and when |c| is large   we will present an algorithm that relaxes the `-diversity conditions.
　to discuss the algorithms  we need to introduce the following definition:
　definition 1  junction tree . let v = {v1 ... vp} be a collection of sets. a junction tree is a graph  v e  that is a tree with the following property: for any vi vj （ v and for any v 1 （ v in the path between vi and vj  we have vi ” vj   v 1.
figure 1 shows a junction tree for the interaction graph in figure 1 a . if we let the vi be the maximal cliques of a connected decomposable graph  then there always exists a junction tree that contains all of the vi . junction trees can be created from scratch in time that is quadratic in the number of cliques  or maintained incrementally as in . because of proposition 1  we can assume
{bcd}

	{abc}	{de}
figure 1: junction tree for figure 1 a 
algorithm 1 : diversity check node: v 
require: each node v of a junction tree is a set of attributes
1: iv ○ tv
1: for all x （ children v  do
1:	diversity check x 
1:	tmp○ iv ./ ix
1:	for all t （tmp do
1:	tmp t  ○ iv tv  ， ix tx /tv”x tv”x 
1:	end for
1:	prune tmp v 
1:	iv ○ tmp
1: end for
1: if v =root then
1:	check if iv satisfies  c ` -diversity
1: else
1:	prune tmp v ” parent v  
1:	iv ○ tmp
1: end if

that all marginals contain the sensitive attribute c  that the decomposable graph is therefore connected  and therefore that a junction tree exists. in our case  each node of the junction tree is a set of attributes and corresponds to a marginal. it is not hard to see that any topological sort of a junction tree results in a perfect sequence  definition 1   and that the intersection between a parent and child is the separator for the child in the perfect sequence. thus we can compute the expected cell counts  in equation 1  by multiplying the marginal counts tv tv  corresponding to each node v  and dividing
by the separators tv”parent v  tv”parent v  .
　we will perform all joins in the junction tree from the bottom up. for a node v  let av be the set of attributes that appear in the subtree rooted at v. note that attributes in av not involved in a join between v and parent v  will never be used later on because  by definition of the junction tree  those attributes are separated from the rest of the tree by the attributes that are involved in the join. for example  in figure 1  {abc} can be joined with its parent {bcd} using the attributes b and c. a is not involved in the join and so does not appear anywhere except in the subtree rooted at {abc}. the attributes of v that appear only in the subtree rooted at v will be denoted by irrel v   because they are irrelevant for the join between v and its parent and any other join that will be performed afterwards  and the rest of the nonsensitive attributes of v will be denoted by rel v   relevant . after each join  we will group tuples into relevant blocks where all tuples with the same values for the attributes in rel v  are in the same relevant block. within each relevant block we will do the pruning.
　to do pruning  first note that each relevant block is composed of anonymized groups  recall that an anonymized group consists of all tuples with the same values for the nonsensitive attributes; in this case they are the attributes in rel v  and irrel v  . for pruning  we will treat each anonymized group as a vector of length |c| where the ith component is the frequency of sensitive value si in the anonymized group. in the pruning step  we remove all anonymized groups that are not in the convex hull in their respective relevant blocks.
　the pruning algorithm runs from the bottom up. for each node v whose children d1 ... dj are all leaves  it sequentially joins the marginals corresponding to v and its children to get an intermediate result iv. for each tuple t （ iv  the pruning algorithm computes the expected count by multiplying the marginal counts and dividing by the separators: tv tv qi tdi tdi /tv”di tv”di  . we can think of iv as a new marginal where the count of each cell is the expected count that we computed. iv will be treated as the  new  marginal for v and so iv will itself be joined with the parent of v and v's siblings. after each join  a pruning step is performed. the pseudo-code is shown in algorithm 1. note that algorithm 1 calls a procedure called  prune  which takes two arguments. the first is a marginal and the second is the set of attributes rel v . in the basic pruning algorithm   prune  removes anonymized groups that are not part of the convex hull of their relevant blocks.
　theorem 1  correctness of pruning . if there exists any t （ nonsendomain t  that is not maxent `-diverse then at least one such t will belong to an unpruned anonymized group of iroot at the end of the algorithm.
　in the case where ` = |c|  we can efficiently check for  c ` diversity while avoiding the computation of convex hulls. to accomplish this  we only need to modify the  prune  procedure. let s1 ... s` be the sensitive values of |c|. within each relevant block bi we do the following. for each ordered pair  sj sj1  of sensitive values  we find and retain the anonymized group in bi where the ratio of the frequencies of sj to sj1 is maximal. the anonymized groups that are not retained are discarded. thus for each combination of values that will participate in a join between two marginals  we have at most |c|1 tuples. with this pruning procedure  once we get to the root  we look at the ratio of frequencies of sj to sj1  for all j j1  in each anonymized group. if all of the ratios are ＋ c then the marginals satisfy maxent  c ` -diversity.
　in the case where the size of the join of all marginals containing the sensitive value is large  the worst case occurs when all marginals contain the same sensitive attribute and one additional attribute   and when |c| is large  there are several ways we can speed up the checking for maxent `-diversity. the first approach is to reduce the join size by imposing additional restrictions on the structure of anonymized marginals. when searching through the space of collections of anonymized marginals  we can restrict our attention to collections where at most m  a user-defined parameter  anonymized marginals contain the same sensitive attribute. another approach is to take a base table and to first apply any of the existing algorithms that can be used to generate minimal `-diverse tables  see  1  1  1  ; an `-diverse table t1 is minimal if there is no `-diverse table that can be transformed into t1 by using generalizations. afterwards  when we search for collections of anonymized marginals  we only consider collections that include that particular `-diverse version of the base table as one of the marginals. this type of search is equivalent to starting out with an `-diverse table and searching for which additional anonymized marginals can be published as well  thus these marginals are injecting utility into the original anonymized table . the inclusion of such an `-diverse table naturally limits the join size  and our experiments indicate empirically that this approach also yields good utility.

	figure 1: anonymized tables	figure 1: anonymized marginals	figure 1: incremental utility　the other approach is to relax the `-diversity requirements. given a relaxation parameter   we can guarantee that for each t （ nonsendomain t   there are at least ` sensitive values that are at least  times as frequent as the most frequent sensitive value for t. first we have a preprocessing step where for each non-root node v and each tuple t （ tv  we set tv  t  = tv t /tv”parent v  t   i.e.  we perform the division by the separators in advance . when v is the root  we set. we then form the marginals iv as follows. for each  we examine the anonymized group to which t belongs. let tmax be the tuple in t's anonymized group such that tv  tmax  is maximized. if  then iv t  = 1 and otherwise iv t  = 1. these are now the marginals that would appear in line 1 of algorithm 1. line 1 is now replaced by tmp t  ○ iv tv    ix tx . each anonymized group can now be treated as a vector of length |c| where the ith component is 1 if and only if iv ti  = 1  where ti is the tuple in the anonymized group such that t.c = si . the the pruning step removes redundant anonymized groups in each relevant block. it also removes anonymized groups which dominate another anonymized group: if g~1 and g~1 are vectors corresponding to anonymized groups g1 and g1  respectively  then we say g1 dominates g1 if every component of g~1 is greater than or equal to every component of g~1. at the root  we say that there is maxent `diversity with  relaxation if each anonymized group  when treated as a vector  has at least ` components equal to 1.
1. experiments
　we performed our experiments on the adult dataset in the uci machine learning repository . we removed all tuples with missing values and were left with a table containing 1 tuples. we used the attributes race  gender  age  and marital status as the nonsensitive attributes  and occupation as the sensitive attribute. using the same generalization hierarchies as in  and   we generated three tables that were simultaneously 1-diverse and 1anonymous. these tables were minimal in the sense that any other 1-diverse  1-anonymous table can be generated from one of these three by using generalizations.
　we measured utility in terms of kl-divergence; the smaller the number  the better it approximates the original un-anonymized table. figure 1 shows the utilities of the three minimal 1-diverse  1-anonymous tables. the bar labeled 1 corresponds to the kldivergence to the table where all nonsensitive attributes were completely suppressed  i.e.  they were generalized to a single value . the bar labeled opt corresponds to the kl-divergence to the best set of anonymized marginals that satisfy 1-anonymity and maxent 1-diversity. the three 1-diverse  1-anonymous tables were labeled a  b  and c. as we can see  the anonymized tables do not approximate the original table particularly well.
　one way to speed up the search for a good collection of anonymized marginals  and to make checking for `-diversity more efficient  is to start with an anonymized table and to only consider collections of anonymized marginals such that the given anonymized table is one of them  i.e.  start with an anonymized table and inject utility by adding additional anonymized marginals . for each of the anonymized tables a  b  and c  we found the best sets of anonymized marginals that contained each table. the results are shown in in figure 1. here the bar labeled a+ is the kl-divergence to the best collection of marginals that contain the anonymized table a  similarly for b+  and c+ . in our experiments it turned out that the best collection of anonymized marginals containing b was also the overall best collection of anonymized marginals  whose utility is labeled opt in figures 1 and 1 .
　finally  in figure 1 we show that even a very simple search for anonymized marginals can yield dramatic results when compared to the utility of just a single anonymized table. to illustrate this effect  we used table c  although our results were qualitatively similar for tables a and b as well. we measured how the kl-divergence decreased as we added marginals that contained only one attribute each. the marginals were added in order of greatest improvement in utility. starting out with table c  we first added a marginal on race  bar labeled c1   to this we then added a marginal on marital status  bar c1   then gender  bar c1   and finally we also added marginal on age  bar c1 . the marginal on age was bucketized into ranges of size 1   1   1   1   1  ...  in order to meet the k-anonymity requirements. note that there is still a noticeable difference in utility between this collection of anonymized marginals and the best collection that contains table c  as well as the overall best collection of anonymized marginals ; however  this simple collection of marginals still created an enormous improvement in utility over a single anonymized table.
1. related work
　the utility of data that has been altered to preserve privacy has often been studied in contexts where the future use of the data is known. for example   studies how to reconstruct association rules after noise has been added;  and  study how to reconstruct the distribution of a continuous variable after noise with a known distribution has been added;  studies how to perturb the values of continuous numeric attributes so that data clusters can be reconstructed  note that  also proposes publishing perturbed data in addition to a histogram  but this method does not handle non-numeric attributes and the privacy guarantees use the assumption that the data is generated from a uniform distribution ; and  and  anonymize data while trying to maximize decision tree accuracy. there have also been some negative results for utility. in addition to the curse of dimensionality for k-anonymity   there is work showing that an ideal privacy criterion places extremely strong restrictions on the types of queries that can be answered   in particular  aggregate statistics cannot be computed . kanonymity  and `-diversity  are weaker privacy definitions  they do not protect against adversaries with arbitrary amounts of background knowledge  but they provide considerably more utility.
　there are several approaches to sanitizing a dataset to ensure privacy. these include generalizations   tuple suppression  1  1   adding noise  1  1  1  1   publishing marginals that satisfy a safety range   and data swapping  - a technique where attributes are swapped between tuples in such a way that certain marginal totals are preserved. queries can also be posed online and the answers audited  or perturbed .
　log-linear models  1  1  and logistic regression are popular techniques for analyzing tabular data  and graphical models  1  1  1  provide a compact and interpretable representation of highdimensional probability distributions.
　the maximum entropy distribution that satisfies given constraints has also been studied in the database literature. for example  this has been applied to the exploration of olap data cubes .
1. conclusions and future work
　anonymized marginals can be thought of as statements about the original data set that are guaranteed to be true. the maximum entropy distribution is then our best guess about the rest of the data. another way to think of this is that anonymized marginals are a compact representation of a statistical model  a density estimate of the original table . a promising direction of future work is releasing a set of models in addition to the data  studying the utility of such an ensemble  providing guarantees about the resulting privacy of information  and constructing data mining algorithms that use all of this information as the input.
