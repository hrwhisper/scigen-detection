recent years witnessed an increasing interest in researches in xml  partly due to the fact that xml has now become the de facto standard for data interchange over the internet. a large amount of work has been reported on xml storage models and query processing techniques. however  few works have addressed issues of xml query optimization. in this paper  we report our study on one of the challenges in xml query optimization: containment join size estimation. containment join is well accepted as an important operation in xml query processing. estimating the size of its results is no doubt essential to generate efficient xml query processing plans. we propose two models  the interval model and the position model  and a set of estimation methods based on these two models. comprehensive performance studies were conducted. the results not only demonstrate the advantages of our new algorithms over existing algorithms  but also provide valuable insights into the tradeoff among various parameters.
1. introduction
　xml  the extensible markup language  has now become the de facto standard for data interchange over the internet. unlike relational data  a unique feature for data encoded in xml format is that they are virtually semi-structured data. the structure of xml data is usually defined by xml schema  or previously dtd . several data models have been proposed for xml data  e.g.  document object model  dom   and they all assume a tree based model to represent xml data. consequently  many query languages proposed are designed to allow queries that have both content and structural constraints. for example  the xpath query //paper appendix/table  selects those papers that have a

 the project was partly supported by the research grant council of the hong kong sar  china  grants aoe/e1 and hkust1e  and national 1 fundamental research program of china  g1 .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
table in their appendix sections.
　containment join  proposed in   has been recognized as the core part for xml query processing. a containment join between a set of ancestor nodes  denoted as a  and a set of descendant nodes  denoted as d  is to find all pairs of  a d   a （ a and d （ d  such that a is an ancestor of d. region coding scheme  i.e.  each element is encoded with its  physical or logical  start and end positions  is a powerful technique widely used to efficiently evaluate containment join. when the region coding scheme is used  a
　containment join between a and d can be evaluated as the following complex relational θ-join: a ./θ d  where the θ is a.start   d.start … d.end   a.end.  1  1  1  have shown that containment joins not only can be used to answer arbitrary xpath queries  but often outperform other alternative methods as well.
　while there are many studies on efficient containment join algorithms  there has not been much research into the size estimation problem for containment join. an accurate estimate of the result size of a containment join is a prerequisite for cost-based xml query optimizers. for example  consider the same xpath query: //paper appendix/table . given that we have some mechanism to retrieve //paper  //appendix and //table efficiently  a query optimizer has to choose the containment join order. one plan is to containment join //paper and //appendix and then join the intermediate result with //table by another containment join. another plan is to containment join //appendix and //table first and then join the intermediate result with //paper. if the sizes of two intermediate results differ greatly  then the total costs of the two plans might differ greatly as well. accurate estimation of the intermediate result sizes  in this example  can help to choose a better query execution plan. size estimation can also be useful in internet or interactive applications  where the estimated result sizes can be presented to the user in order to decide whether it is necessary to really launch the query  as it might require substantial system resources.
　containment join on data coded in region codes is essentially a complex relational θ-join. in addition  xml data has unique features that make any estimation method nontrivial. although we are aware of existing works in the following areas  none of them is directly applicable to our containment join size estimation problem.
  relational techniques. estimation of join sizes has been widely studied in the context of relational equijoins. however  the proposed methods were either designed to optimize for the worst case  or hard to be generalized to handle complex inequality join conditions  1  1  1 .
  estimation methods for xml data. to the best of our knowledge   is the only work that addressed the same estimation problem for xml data. in particular  ph and coverage histograms were proposed for join size estimation. however  those methods rely on a few strong assumptions and their performance may degrade greatly if the data do not conform to the assumptions.
  spatial techniques. most existing spatial join size estimation methods  1  1  1  rely heavily on multidimensional uniform distribution assumption and cannot take into consideration the constraints of xml data distribution.
　in this paper  we propose two models  the interval model and the position model  and a set of estimation methods based on these two models. a unique feature of our models is that they can capture alternative information for an element set according to its role  ancestor or descendant set  in a containment join. the interval model treats every element in a node set as an interval  when the node set acts as the ancestor set in the join or a point  when the node set acts as the descendent set. in the position model  conceptually  two auxiliary tables are constructed for each element set. the former table records the coverage information when the element set acts as the ancestor set  while the latter captures the start position information of each element when the element set acts as the descendant set. these two models enable us to transform the original challenging estimation problem to other estimation problems which are tractable. based on the two models  two classes of estimation algorithms are proposed: histogram based and sampling based algorithms. we show that our histogram based method uses weaker and more reasonable assumptions than those used by ph histogram and coverage histogram . our sampling based methods leverage adaptive sampling techniques and have provable accuracy of the estimated results. extensive experiments were conducted. the results not only demonstrate the advantages of our new algorithms over existing ones  but also provide valuable insights into the tradeoff among various parameters.
　the major contributions of the paper can be summarized as follows.
1. we propose two models for the containment join size estimation problem. the challenge of the original problem is to estimate the result size for a complex  inequality join  which renders existing models and hence methods inapplicable. we show that  in our newly proposed models  the original problem can be converted to other estimation problems that are tractable.
1. we propose and study two classes of estimation methods based on the new models. for each method  we exhibit our efforts to exploit the unique features of xml data and containment joins. we also give theoretical bounds on the quality of estimated results  whenever applicable  as well as detailed discussions.
1. we report results of experiments performed for various datasets under different settings. our new algorithms are shown to outperform previous ones up to an order of magnitude and are more robust. the experiment results also provide valuable insights into the tradeoff between various parameters.
　the rest of the paper is organized as follows. section 1 presents related work. section 1 introduces and discusses two proposed models. histogram based approaches are discussed in section 1 while section 1 discusses approaches based on sampling. section 1 presents our detailed experiment results and analyses. section 1 concludes the paper.
1. related work
　we will give a brief review of existing works on statistical estimation. then we focus on ph histogram  and bifocal sampling   which are closely related to the work reported in this paper.
　although histograms are by far the most popular statistical data structure for estimating query sizes  there is little work on estimating join sizes using histograms.  studied the kind of histogram based method that is optimal in the worst case and pointed out that all histograms are equally good on average with respect to the metric defined.
　there is not much work on estimating join sizes in the spatial database context either.  studied several histogram schemes for spatial join estimation. they proposed the parametric histogram  ph  based on the previous work . another histogram  geometric histogram  gh   was also proposed  based on the key observation that the intersection of two mbrs results in four intersecting points. gh was demonstrated to outperform ph and other methods without using histograms. the gh method  however  requires a fine grid over the workspace and may result in inaccurate estimation when this condition fails.  proposed formulae for selectivity estimation of pairwise joins restricted by two selections. their focus is on adjusting the workspace given two original workspaces of the joining datasets. recently  a framework for spatial join size estimation was proposed in   based on the notion of euler diagram. different probabilistic data distribution models can be plugged into the framework which generates different formulae to estimate the join results of corresponding grids from the datasets. specifically  they integrated the formulae in  1  1  and showed the improved accuracy over previous approaches.
　there have been many studies of  adaptive  sampling based estimation algorithms since hou  ozsoyoglu： and taneja presented their initial work in this area  1  1 .  introduced a partial ordering that compares the variability of the estimators of different classes  such as t index  t cross  etc.  proposed a new bifocal sampling technique that overcomes some well-known problems in previous schemes. we will discuss it in more details later. recently   reexamined lipton and naughton's adaptive sampling algorithm  and haas and swami's adaptive algorithm mainly in terms of efficiency. it also studied the monte-carlo algorithm due to e. cohen .  proposed the systematic sampling technique  which was demonstrated to outperform the t cross sampling method with the same amount of sampling. the idea is to sample the first k = dnn e tuples of r and every k-th tuple thereafter in the sorted order of the join attribute. more recently   1  1  1  proposed to use sketch based methods to estimate self-join and join sizes in the stream data processing setting. the basic idea of those approaches is much similar to that of sampling approaches: in order to estimate a function of f x  over a dataset  we instead compute an unbiased estimator f  x  of f x  such that the variance of f  x  is small and bounded. the mathematical tool used is four-wise independent binary random variables  which can be constructed from the orthogonal array of strength 1. although sketch based methods have been successfully applied in many problems  it is not obvious to be applicable to the containment join size estimation problem. a different but related problem is estimation of the selectivity of path expressions for xml data  1  1  1  1 . most existing approaches tried to solve the problem by capturing the structure of the xml data tree or graph.  proposed two techniques  namely path trees and markov tables  to summarize the structures of xml data.  employed xml schema types and histograms as statistical summaries.  exploited localized graph stability in a graph-synopsis model to approximate path and branching distribution in an xml data graph. our problem differs from theirs in that we do not have any restriction on the two datasets involved in the join nor do we need to capture the global statistics for the whole xml document.
1 ph histogram and coverage histogram
　 is the only previous work that addressed the same containment join size estimation problem. their approach is to mapping xml data into 1d space and maintaining certain statistics for data fallen in each pre-defined grid over the workspace. xml specific characteristics  such as  a  strictly nested property and  b  nesting within each dataset  are taken into consideration  which results in the ph-histogram and coverage histogram. the underlying assumption is two-dimensional uniform distribution of both datasets. that is  as we will show later  a strong and static assumption. for example  the estimated result for two corresponding  off-diagonal grids  buckets  will always be na ， nd. in addition  their ph histogram based estimation methods will give a highly erroneous estimation when no two nodes in the ancestor set can be of ancestor-descendant relationship  i.e.  when the ancestor set has no-overlap property. coverage histogram and new estimation algorithms were hence proposed as a remedy. however  their new method still suffers a lot from the additional assumption that the global coverage statistics are the same as the local coverage statistics.
1 bifocal sampling
　bifocal sampling was introduced as a new adaptive sampling technique for estimating equijoin sizes in the relational context . it classifies tuples in each relation into two groups  sparse and dense  based on the number of tuples with the same join value. distinct estimation procedures were employed for various combination of joining tuples  namely  the densedense subjoins and sparseany subjoins. bifocal sampling was shown to be able to overcome some well-known problems in previous adaptive sampling schemes. however  it focuses on relational equijoin only and it is not obvious as to extend the method to the complex containment join here.
1. estimationmodelsforxmldata
　in this section  we first give the formal definition of the problem  followed by discussions of the unique features of xml data and containment joins. they render existing estimation methods in other areas not directly applicable. two novel models that address above-mentioned challenges are proposed. they underpin the three classes of estimation algorithms to be discussed in the later sections.
1 problem definition and challenges
　xml data are commonly modelled as trees  termed as data trees. given an xml data tree t   we can get a set of element nodes by certain predicate based on values and structures. for example  the predicate could be an xpath query like //appendix. a set of element nodes is defined when the above predicate is evaluated against a given xml data tree.
　a containment join between two element sets a and d returns all pairs of  a d   where a （ a and d （ d  such that a is an ancestor of d.  proposed to use region codes  i.e.   start end   for each element and the containment join between a and d can be evaluated as a complex θ-join: a ./θ d  where θ = a.start   d.start … d.end   a.end. note that due to the strictly nested property of xml data  the above join condition can be simplified to θ1 as a.start   d.start   a.end. in addition  without loss of generality  we assume the region codes are distinct  that is  no two elements in t have the same start or end value.
　given an xml data tree t labelled with region codes  we denote cmin and cmax as the minimum of the start codes of all elements in the tree and the maximum of the end codes of all elements in the tree respectively. formally  cmin = mine（t {e.start} and cmax = maxe（t {e.end}. the region  cmin cmax  is also termed workspace.
　we formally define our containment join size estimation problem as follows.
　definition 1. given two element sets a and d where every element in both sets is encoded in region code  i.e.   start end   correctly estimate the cardinality of the containment join result between a and d.
　there are several unique features of our estimation problem that impose serious challenges.
1. complex θ-join conditions. the join condition consists of two correlated inequality conditions on multiple attributes.
1. data distribution. there are two constraints that confine the distribution of the elements.
 a  end value should always be greater than start.
 b  due to the strictly nested nature  the regions of any two elements cannot partially overlap. in the two-dimensional description  this constraint results in the forbidden region observed in .
1. correlation pattern. correlation is recognized as the major difficulty in estimating the result size of equijoins or spatial joins. there are two observations about the correlation between the ancestor and descendant sets.
 a  the correlation is determined by the structures of the data tree t .
 b  the correlation effects of the two sets are asymmetric. specifically  one element in the ancestor set could join with all elements in the descendant set  while one element in the descendant set can only join with up to h elements in the ancestor set  where h is the height of the xml data tree t .
　now we will briefly review previous models and show that their methods are not well suited to capture the abovementioned features.
　for one-dimensional histogram based methods  they are based on the model of frequency distribution table  or data distribution   which contains the set of pairs t = { v1 f1   ...  vd fd }  where vi is the value of the attribute in question and fi is the number of tuples in the relation that have value vi in that attribute . however  such model cannot be directly extended to join predicates that involve multiple attributes. multidimensional histograms were also proposed  but their ability to capture correlation among dimensions is still questionable  let alone the unique correlation pattern in xml data. last  histogram based methods are best suited for answering range/point queries  but not joins. both sampling and sketch based methods are based on statistic theorems. there has been no reported work on using sampling techniques in the presence of inequality joins. sketch based methods  while performing well for joins and self-joins  cannot be easily extended to handle inequality joins either.
　there are only few works for spatial join size estimation. all the existing works make some assumption on the data distribution of two joining datasets. either power-law compliant assumption  or multidimensional uniform / independence assumptions are used. those assumptions are not suitable for xml data either.
　wavelet or fourier transformation based methods were successfully used in estimation problems in one-dimensional or multidimensional context. however  one unsolved difficulty is that it is still unknown how to find good transformation for data distribution with certain constraints. for example  a large portion of approximated data could be invalid due to their violation of the strictly nested property. we defer studies in this direction to our future research.
1 interval model
　the interval model is a one-dimensional model. conceptually  each element set corresponds to two sets: interval set and point set. an interval set contains intervals and a point set contains points. the interval set of element set s  denoted as ima s   is used when s acts as an ancestor set in a containment join. for each element e in s  it corresponds to an interval of  e.start e.end . the point set of element set s  denoted as imd s   is used when s acts as a descendant set in a containment join. for each element e in s  it corresponds to a point e.start.
　theorem 1 indicates that the original estimation problem can be converted into the problem of estimating overlapping interval-point pairs.
　theorem 1. under the interval model  the result size of the containment join between a and d is equal to the number of overlapping interval-point pairs between ima a  and imd d .

 a  an example xml data tree

 b  interval model
	pma a 	pmd a 	pma d 	pmd d 
	v f	v f	v f	v f

1111111111111111111111
11111111111111111111111111111111111111111111
1111111111111111111111 c  position model
figure 1: illustration of two models
　example 1. we show an example xml data tree in figure 1 a . we choose three nodes as the ancestor set a and four nodes as the descendant set d. their region codes are also shown respectively. for example  node a1 has region code  1  1 .
　the interval sets and the point sets of the ancestor set and the descendant set are shown in figure 1 b . node a1  for example  corresponds to an interval from 1 to 1 in ima a  and node d1 corresponds to a point at 1 in imd d .
1 position model
　the position model is also a one-dimensional model. conceptually  each element set corresponds to two tables: covering table and start table. both tables have the same schema:  v f   where dom v   =  cmin cmax  and dom f  = {x （ z|x − 1}. the covering table of element set s  denoted as pma s   is used when s acts as an ancestor set in a containment join. for every tuple t （ pma s   t.f = |{e|e.start ＋ t.v ＋ e.end}|. that is  the f field records the number of elements whose regions cover the corresponding v value. the start table of element set s  denoted as pmd s   is used when s acts as a descendant set in a containment join. for every tuple t （ pmd s   t.f = |{e|e.start = t.v }|. that is  the f field records the number of elements whose start values are equal to the v value. note that  since the codes are distinct  the value of f for any tuple in pmd s  can be either 1 or 1  as there could only be 1 or 1 element with a given start value.
　theorem 1 indicates that the original estimation problem can be converted into the estimation of the size of an equijoin.
　theorem 1. under the position model  the result size of the containment join between a and d is equal to the inner product of of pma a  and pmd d   i.e.  pcmin＋i＋cmax  pma a  i  ， pmd d  i  .
　example 1. we show the covering and start tables for both ancestor and descendant sets in figure 1 c . for example  position 1 is covered by two ancestor nodes  namely a1 and a1  therefore  its f value is 1. the inner product of pma a  and pmd d  is 1  which is equal to the result size of the containment join between a and d.
1. estimation via histogram
　based on the interval model  we propose a new histogram based estimation method. in our new method  each descendant element d will correspond to a point in imd d  and each ancestor element a will be an interval in ima a . we will use a to denote ima a  and d to denote imd d  throughout this section  as long as there is no ambiguity.
1 pl histogram
　the pl histogram  point-line histogram  is based on the interval model. the basic idea is to partition the whole workspace  cmin cmax  into buckets and estimate the overlapping  interval  point  pairs within each bucket according to some assumptions.
we choose to make the following two basic assumptions:
  data distribution of a and d are independent.
  d conforms to uniform distribution in each bucket.
note that
1. they are not unreasonable assumptions. independence assumption and uniform assumption are often made in the relational context and spatial context as well. they enable us to do the estimation without any a priori knowledge about the data distribution or correlation of the joining data.
1. one-dimensional uniform assumption can be made approximately valid if the data is regular and bucket boundaries are carefully selected. this assumption is weaker than the two-dimensional uniform assumption adopted in the early work  i.e.  the ph histogram in . it can be shown that if the data distribution is two-dimensional uniform  it is also one-dimensional uniform. in addition  we only require the assumption on d's distribution.
　we now introduce the pl histogram and its estimation algorithm. we first divide the whole workspace of the element set into b buckets. for each bucket  we keep the following statistics n  wss  wse  l. the definitions of the statistics are listed in table 1  where r could be either a or d.
table 1: statistics for pl histogram
symboldefinitionn r i number of intervals/points in the i-th bucketwss r i start position of the i-th bucketwse r i end position of the i-th bucketl r i average length of the intervals in the i-th bucket  not valid for d 　let us consider the case when the whole workspace is a bucket. for each a （ a  according to our assumptions  the expected number of d elements that lie inside the range of
   where l a  is the length of an element a （ a. therefore the total number of  a d  pairs can be estimated as

　we can generalize the above formula  for 1 bucket  to the case of b buckets if we use the same partitioning scheme for both a and d. we can then estimate the size of containment join as:

note that
1. our formula is an adaptive one. intuitively  if there is no overlap of and the result size is less than d. that is reasonable as each d （ d can join with at most one a （ a. on the other hand  if elements in a are heavily nested  the results could be larger than d. on the contrary  the formula in ph histogram takes the form of c，n a ，n d  for estimation within a bucket  grid   where c is a constant  such as
   .
1. in order to avoid multiple counting when some element lies across bucket boundaries  we use the following rules: each a across multiple buckets will be counted multiple times  while each d across multiple buckets will be only counted in the first bucket  that is  the bucket where d.start is located.
algorithm 1 pl-hist-est a  d 
input:
a and d are ancestor and descendant sets respectively and both are partitioned identically into b buckets.
output: return the estimated size of join results. description:
1: est = 1
1: 1:
　algorithm 1 describes the procedure to estimate the join size with pl histograms built on both datasets. the algorithm iterates over all the b buckets of a  and for each bucket of a  it calculates the estimated number of joining pairs according to equation 1  line 1 . we sum up the estimated result for each bucket of a to get the final estimated result for the containment join between a and d.
1 the mre measure
　although our formula is accurate in the continuous domain  it can fail for a certain class of cases in the discrete domain when d is relatively sparse. to illustrate this  let us look at the examples in figure 1. there are 1 uniformly distributed elements  points  from d and 1 element  interval  from a in both figure 1 a  and  b . let the interval move from the initial position  the solid line  to the new positions denoted in dashed lines. we calculate and draw in the figure the number of matches  i.e.  overlapping interval-point pairs  during this process. it is clear that the number of matches will vary. intuitively  this illustrates that relative offset of a and d can affect the accuracy of the estimation.

	 a 	 b 
figure 1: an example that shows the effect of the offset to the join size
　formally  in the case of discrete data  both a and d can only start/end at integer positions and d can never be redundant  i.e.  having multiple d's with the same start position   the estimation formula is correct in the average case  but the real number  even the assumptions are valid  depends much on the relative offset of a and d. let 
. intuitively cov measures how many d's one a can cover on the average. from figure 1  we can see that the join result will be n a  dcove with probability  cov bcovc  and be n a  bcovc with probability 1  cov bcovc . traditionally  unlike sampling based methods  estimation given by histogram methods does not give any  confidence  measurement. here  we define a measure named the maximum relative error to characterize the worst case estimation error as
		 1 
　intuitively  if the mre value is large  we have the risk of having high inaccuracy for the estimation result. the worst case error will be big if cov is small. specifically  if cov   1  the maximum relative error could be unbounded. even in terms of the absolute error  it could still be big when cov   1. we show the mre values for cov ranging from 1 to 1 in figure 1. we can see that mre values vary regularly with a period of 1 and the maximum value of mre within each period goes down consistently when cov increases. we do not plot the cases when cov   1 here  because the mre values are unbounded.

1 1 1 1 1
cov
figure 1: mre vs cov for cov   1  mre is unbounded when 1   cov   1 
　to address the above problem  we use the following two approaches simultaneously.
  we provide a  rough  confidence measure to the user about the estimated result: the mre measure given in equation 1. if cov   1  it can be shown that 1   mre   1  and the maximum of mre values over each period decreases when cov increases. intuitively  mre gives the bound of relative errors of the estimation when datasets conform well to the assumptions of our pl histogram. note that the actual relative error could be even higher if the joining datasets do not conform to our assumptions well enough.
  the root of the problem is the inability for histograms to capture correlations. therefore  we develop other estimation techniques  such as sampling based methods in the next section  which can provide accurate estimation with high confidence measures  although the two measures cannot be directly compared  even in the presence of correlation.
1. estimation via sampling
　in this section  we present a class of new sampling algorithms  designed to take into consideration unique features of xml data and the containment join.
　although the effectiveness and the practicality of sampling based estimation methods have been demonstrated by many previous research works  none of them considers the case of estimating the result size of complex inequality joins. here  we consider the estimation problem under both the interval model and the position model. it will be shown that with the help of the models  our new sampling based algorithms are capable of giving provably good estimation of the containment join size.
1 interval model based adaptive sampling
　in the interval based model  the analogy to the equality condition of two relations is the overlapping condition of intervals from ima a  table and points from imd d  table. our algorithm is inspired by the bifocal sampling algorithm  as well as the following observation: an interval in ima a  could join with many points in imd d   up to |d|   while a point in imd d  can only join with relatively much fewer intervals in ima a   up to h  where h is the height of the xml data tree . bifocal sampling will classify all subjoins between a and d into two classes  densedense subjoins and sparseany subjoins  and distinct adaptive sampling algorithms are used for each class due to their different characteristics. h is usually a small constant and thus we assume under this assumption  the original bifocal sampling algorithm can be greatly simplified. the idea is that we only need to sample from d and all the subjoins will be sparse. due to the fixed  small constant h  the variances among subjoin sizes are also well bounded  which provides good estimation accuracy and confidence.
　the im-da-est algorithm  simplified from the bifocal sampling algorithm according to our assumption  is shown in algorithm 1. the algorithm takes m random points from the imd d  tables  then probes ima a  to calculate each subjoin size. the sizes of subjoins are summed up and scaled to get an estimate of the size of the containment join between a and d. the im-da-est algorithm approximates the real size with high probability due to theorem 1.
algorithm 1 im-da-est a  d  m 
input:
a and d are ancestor and descendant sets respectively.
   m is the number of sampled points from imd d  table. output:
return the estimated size of containment join result between a and d.
description:
1: est = 1
1: s  = a random sample  of size m  from imd d . 1: for all point p in s  do
1:	x = the number of intervals in ima a  that join with
p.
1:	est = est + x
1: end for
1: return est ，  |d|/m 

　theorem 1. let the size of the containment join between a and d be x  and let the estimate given by the imda-est algorithm be x . let n = |d|. we have e x   = x  with high probability x  = Θ x  + o n .
　proof.  sketch  let anca d  denote the number of intervals a point d （ imd d  will stab  or the number of ancestors that will join with the element corresponding to d. let j d  be the contribution of d （ imd d  to the estimate x . it is obvious that x  = pd（imd d  j d .
  if d （ s 
    =
	1	  otherwise
where s  is the set of samples with size equal to m. the expected value of j d  is

therefore 
e x  ==x anca d  = xd（imd d 
moreover  j d  is a random variable taken from domain  1 z   where z = n/m ， h. by hoeffding bounds  x =
Θ e x   with probability e Θ e x/z  . let m = nα  where 1   α   1  then the above probability is high if x =
  n .	
　notice that our confidence of the estimated result is high even when the real result is o n . this is an improvement upon the previous requirement in   which is o nlogn . this improvement is due to our effort to exploit the unique feature of containment join for xml datasets.
1 position model based adaptive sampling
　in the position model  tables  pma s  and pmd s   are generated for each dataset describing its positional information within the workspace under different circumstances. we have shown that the size of the containment join between a and d is equal to the inner product of pma a  and pmd d . hence  we choose to use a simplified bifocal sampling method to do the estimation for the equijoin. the simplification also comes from the observation that the size of subjoins are bounded the constant h  the height of the xml data tree. the pm-est algorithm is shown in algorithm 1. it takes m random points from the workspace  then probes both pma a  and pmd d  to calculate each subjoin size. the sizes of subjoins are summed up and scaled to get an estimate of the size of containment join between a and d. the pm-est algorithm approximates the real size with high probability due to theorem 1.
　theorem 1. let the size of the containment join between a and d be x  and let the estimate given by the pmest algorithm be x . let n = cmax   cmin + 1. we have e x   = x  with high probability x  = Θ x  + o n .
　note that the n in the above theorem is the length of the workspace  which is no less than |a| + |d|. each subjoin size in pm-est method is also bounded by h. therefore  we expect the performance of this method will be inferior to im-da-est. this is further verified by our experiment result.
algorithm 1 pm-est a  d  m 
input:
   a and d are ancestor and descendant set respectively. m is the number of sampled points from the workspace  cmin cmax . w is the length of the workspace  i.e.  cmax   cmax + 1. output:
   return the estimated result size of the containment join between a and d. description:
1: est = 1
1: s  = a random sample  of size m  from the workspace
 cmin cmax .
1: for all value v in s  do
1: x = pma v  1: y = pmd v 
1: est = est + x ， y 1: end for 1: return est ，  w/m 

1 discussions
1.1 indexes
　in both im-da-est and pm-est methods  we need to calculate the subjoin size for every sample. we consider building auxiliary indexes for the datasets to accelerate such probing operations.
　for im-da-est algorithm  we need to answer the following type of queries: given a point  return the number of intervals that overlap the point. we consider two kinds of indexes. the first index is the xr-tree   which is a dynamic external index structure capable of answering such stabbing queries  as well as range queries  efficiently. efficient containment join algorithms have been developed based on the xr-tree. probing the xr-tree index might not be optimal for the stabbing count query here  as we do not actually need to return all intervals that the query point stabs. however  probing in the xr-tree will cost only several page accesses in the worst case  when none of the nodes is in the buffer  and  more importantly  such probing helps to load part of the xr-tree index into the buffer  which will benefit the later containment join processing based on xr-tree. the other kind of index is based on the observation that the pma s  x   if we build pma for the ancestor set s  is exactly the number of intervals that overlap point x. instead of recording the whole pma s   we only need to index every turning point k of pma s  as well as the associated pma s  k  value via a b+-tree. a turning point k is a point where pma s  k  1= pma s  k 1 . it can be shown that there are only o |s|  such turning points and pma s  values between two adjacent turning points are the same. we refer a b+-tree built on those turning points and their values as a t-tree. we can look up the value of pma s  q  by 1  find ki ＋ q   ki+1 in the b+-tree and 1  return the value associated with ki.
　figure 1 shows an example t-tree constructed for the ancestor dataset in figure 1 c . if the query point is 1  we can use the b+-tree to search for the largest key that is no larger than the query key. in this example  we find 1 and return the frequency value associated with 1  which is also 1.
for pm-est algorithm  we need to answer the following
pma a 
v f

1111111111111111111111figure 1: an example t-tree
type of query: given a point  return the value associated with the point in either pma s1  or pmd s1 . the above mentioned t-tree is capable of answering such queries efficiently. notice that for the probing query on pmd s1   any other index structures built on the start field of elements are capable to answer it  e.g. b+-tree  xr-tree or hash indexes etc. if the query key is found in the index  then its value is 1  otherwise  the value is 1.
1.1 boosting
　the probabilistic boosting method  can also be applied to our sampling methods. we can prepare s1 ， s1 independently generated random samples and get their estimated results. we then take the average of every s1 estimated results and take the median of the s1 averages as the final result.
1. experimental study
　in this section  we present the results of our experiment study of the newly proposed algorithms. we also give some suggestions on the choice of the estimation algorithms based on the analysis of our experiment results.
1 experiment setup
　we ran experiments on a pc with amd atholon 1 mhz cpu  1m ram and 1g hard disk. the operating system is microsoft windows 1. we have implemented pl-hist-est  pl   im-da-est  im  and pm-est algorithms  pm . we also implemented positional histogram/coverage histogram  ph   which is the previous work on the same problem proposed in .
　we used both synthetic and real datasets in our experiments. we used one real dataset  dblp and two benchmark datasets  xmark  and xmach . we will focus on our results using xmark and dblp datasets because the results on xmach datasets are very similar to those on xmark datasets. dblp dataset was chosen because  1  it is used in the experiment of ph algorithm  and  1  compared to xmark  the schema of dblp data is relatively simple and its data distribution is more regular. xmark is complex and was also chosen as the experiment dataset in  because the xml data tree features both large fanouts table 1: statistics for datasets
 a  statistics for xmark
predicate namenode countoverlap propertyitem1no overlapdesp1no overlapparlist1n/alistitem1n/atext1no overlapopen auction1no overlapkeyword1no overlapname1no overlapmailbox1no overlapreserve1no overlapbidder1no overlapincrease1no overlap b  statistics for dblp
predicate name	node countoverlap propertyinproceeding1no overlapauthor1no overlaptitle1no overlapcite1no overlapsup1no overlaplabel1no overlap c  statistics for xmach
predicate name	node countoverlap propertyhost1n/apath1n/adoc info1no overlapdoc id1no overlapchapter1no overlapsection1n/ahead1no overlapparagraph1no overlaplink1no overlapand deep nesting in the element structure. for each dataset  we selected a set of predicates  usually the element tag name and derived a set of containment joins accordingly. table 1 shows the selected predicates and detailed statistics for each dataset. notice that we also gathered the overlap property information for each element set. such information  especially the information that a dataset has no-overlap property  is key to effectively estimating results of many queries for the ph algorithm. table 1 shows the set of queries for each dataset.
　we use relative error as the metric to judge the quality of the estimated result. formally  it is defined as 1%  where x is the real value and x  is the estimated value. the performance of sampling based algorithms were obtained by averaging over multiple runs under the same setting.
1 overall performance
　we show the relative estimation error for all the queries on xmark and dblp datasets in figure 1 and figure 1 respectively. for each dataset  we repeated the experiment for three different settings of different space budgets: 1  table 1: queries for datasets
 a  queries for xmark
queryancestordescendantq1itemnameq1itemmailboxq1textkeywordq1despparlistq1desplistitemq1parlisttextq1listitemkeywordq1parlistlistitemq1open auctiontextq1open auctionreserveq1bidderincrease b  queries for dblp
queryancestordescendantq1inproceedingauthorq1inproceedingtitleq1inproceedingciteq1inproceedinglabelq1titlesupq1citelabel c  queries for xmach
queryancestordescendantq1hostpathq1pathdoc infq1doc infdoc idq1chaptersectionq1sectionheadq1sectionparagraphq1paragraphlink1 and 1 bytes. note that even 1 bytes is well below 1% of the size of joining element sets. these settings roughly correspond to using 1  1  1 buckets for ph histogram method  1  1  1 buckets for pl histogram method and 1  1  1 samples for the sampling methods.
　from the figures  we can see that under the same amount of space budget  im algorithm always achieves the best accuracy among the four algorithms. in fact  its relative errors are close to zero for most queries  and up to one order of magnitude smaller than the ph algorithm. another finding is that sampling based algorithms  im and pm   in general  achieve lower relative errors than the histogram based algorithms. in all queries in the two datasets  the relative errors of sampling algorithms are below 1%  usually below 1%   while ph algorithm gives very bad result for some queries  1% for q1 in xmark .
　in the following two subsections  we focus on relative performance of two alternative algorithms for different classes of algorithms  histogram based and sampling based .
1 histogram based algorithms
　figure 1 a  and 1 b  show the relative errors of ph and pl algorithms on the xmark dataset for different number of buckets respectively. the relative errors of ph algorithm for


   q1 q1 q1 q1 q1 q1 q1 q1 q1q1  b  space limit = 1 bytes   q1 q1 q1 q1 q1 q1 q1 q1 q1q1  c  space limit = 1 bytes   q1 q1 q1 q1 q1 q1 q1 q1 q1q1  a  space limit = 1 bytesq1 q1 q1 q1 q1 q1
 a  space limit = 1 bytes figure 1: overall performance on xmark

	q1 q1 q1 q1 q1 q1	q1 q1 q1 q1 q1 q1
 b  space limit = 1 bytes	 c  space limit = 1 bytes
figure 1: overall performance on dblp
　
q1  q1 and q1 are omitted in figure 1 a  because they are extremely large  1% to 1% . figure 1 c  reexamines their accuracy .
several observations can be made from the figures:
  neither histogram based algorithm is sensitive to the number of buckets. in particular  queries having large relative errors cannot be significantly improved by allocating more buckets. this is partly due to the fact that histogram based methods cannot capture correlation between joining datasets and thus cannot reduce the inaccuracy that is due to correlation.
  pl algorithm does not require overlap information of the ancestor set  and is more robust even if such information is available to ph algorithm. notice that ph algorithm will give extremely erroneous estimation if the no-overlap property is not known beforehand.
  as discussed in the previous sections  pl algorithm bears high risk when the descendant set is relatively sparse  in which case the cov value is small and thus mre value is high. we have listed the average cov values for all the queries on the dblp dataset in table 1. we can see that cov values for q1  q1 and q1 of the dblp dataset are extremely small    1  compared to the rest of the queries. therefore  we expect the real join sizes will be quite sensitive to the data correlation and our pl estimates might be instable. this conjecture is proved in figure 1  as the relative errors of pl algorithm for q1  q1 and q1 are notably higher than those for q1  q1 and q1. however  pl algorithm still outperforms ph algorithm in all but one query  q1 .
1 sampling based algorithms
　figure 1 a  and 1 b  show the relative errors of im and pm algorithms for different number of samples respectively.
figure 1 c  reexamines their accuracy on the xmark dataset.
table 1: average cov values for queries on the dblp dataset


several observations can be made from the figures:
  im algorithm steadily improves its accuracy with more sample points  while pm algorithm still shows some fluctuation for certain queries. for all the queries on xmark dataset  im algorithm has lower relative error rate than pm algorithm. these are mainly because pm algorithm requires more sample points than im algorithm to achieve a good level of accuracy with high probability. this observation coincides with our prediction on the inferior performance of pm algorithm to the im algorithm. nevertheless  the relative error rates of the pm algorithm are still much lower than either of the histogram based methods  pl or ph   see figure 1 .
  both algorithms give good estimation under a small space budget. for many queries  im algorithm can already give very accurate estimation on xmark dataset with 1 samples  i.e.  1 bytes . with 1 samples  i.e.  1 bytes   im algorithm can give estimated results within 1% of the real values for all queries and pm algorithm can still give estimated results within 1% of the real values.
1 summary

1 1 1 1 1
number of buckets
 a  tradeoff between accuracy and
space for ph algorithm  xmark
dataset 

	number of buckets	q1 q1 q1 q1 q1 q1 q1 q1 q1q1
 b  tradeoff between accuracy and  c  ph vs. pl in terms of accuracy space for pl algorithm  xmark  xmark dataset 
figure 1: performance of histogram based algorithms
1 1 1 1 1		
	number of samples	number of samples	q1 q1 q1 q1 q1 q1 q1 q1 q1q1
 a  tradeoff between accuracy and  b  tradeoff between accuracy and  c  im vs. pm in terms of accuracy space for im algorithm  xmark space for pm algorithm  xmark  xmark dataset 
dataset 	dataset 
figure 1: performance of sampling based algorithmsdataset 
　
　to sum up  im algorithm achieves the best performance with a small space requirement and sampling algorithms tend to behave better than histogram based algorithms  especially in the presence of strong correlation between the ancestor and descendant sets.
　therefore  we recommend that a system should use pl histograms  with few buckets only  rather than ph histograms mainly for their robustness against the worst case  if there is no stringent requirement on the accuracy. on the other hand  in case when highly accurate estimation is required  or when the cov value is small and mre value is high or unbounded  the interval model based sampling algorithm is the best choice.
1. conclusions
　in this paper  we have reported our comprehensive study of size estimation problem for containment joins on xml data. accurate estimation of the containment join size is a key part for xml query optimization. the unique features of xml data and containment joins impose great challenges on the estimation problem and have rendered previous approaches inapplicable. we proposed two models  the interval model and the position model  in which the original containment join estimation problem are made tractable. two classes of estimation algorithms  based on histogram and sampling techniques respectively were proposed. the new pl histogram based method makes minimal assumptions about the data distribution and correlation and is shown to be more robust than previous histogram based methods.
we also proposed sampling based estimation methods that have provable accuracy with high probability. various optimizations of the new algorithms are discussed as well.
　extensive experimental evaluation using both real and benchmark datasets has demonstrated the effectiveness of the proposed methods. our new algorithms usually outperform previous methods up to an order of magnitude and are more robust. we have also observed interesting tradeoffs among various factors for different algorithms.
　our future work includes investigating alternative methods for the containment join size estimation problem. we are interested in applying other existing techniques  such as wavelet approximation and sketch  to this problem.
