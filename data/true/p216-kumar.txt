we present a graph-theoretic approach to discover storylines from search results. storylines are windows that offer glimpses into interesting themes latent among the top search results for a query; they are different from  and complementary to  clusters obtained through traditional approaches. our framework is axiomatically developed and combinatorial in nature  based on generalizations of the maximum induced matching problem on bipartite graphs. the core algorithmic task involved is to mine for signature structures in a robust graph representation of the search results. we present a very fast algorithm for this task based on local search. experiments show that the collection of storylines extracted through our algorithm offers a concise organization of the wealth of information hidden beyond the first page of search results.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval; h.1  information storage and retrieval : online information services-web based services; g.1  discrete mathematics : graph theory-graph algorithms
general terms
algorithms  experimentation  measurement  human factors
keywords
link analysis  communities  clustering  storylines  search results
1. introduction
　as the richness of content on the web grows  so does the social and economic significance of the web. web searches are increasingly becoming the default starting point for consumer product purchases  term papers  vacation plans  curiosity-driven exploration of topics  etc. the role of the search engine as an entry point to the millions of interesting slices of the web is therefore more sharply
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
accentuated. a question that naturally arises is how best to utilize a browser's screen in summarizing the thousands of web pages that mention the handful of terms a user types in to a search engine.
　the present generation of search engines  beginning with altavista up to the currently most popular google  have taken the viewpoint of ranking the search results in a linear order  and presenting the top ten or so results on the first page  with pointers to the next ten  and so on. this straightforward approach has served us remarkably well  and it is a fair guess that more than half of all user queries are adequately handled by the top page in the search results. there are two main reasons for this: first  a good search engine is often capable of promoting to the top spot the best page relevant to the query  and secondly  most queries tend to have many highly relevant pages on the web  so just about any of them would serve well as the top result.
　an interesting phenomenon occurs when one studies the top 1 pages for a query. search engines routinely optimize the result set primarily for the top 1 positions; the pages listed in positions 1- 1  for example  share many interesting characteristics. for example  these pages may be viewed as good reflections of the quality of the  web presence  of the topic  as discovered by a good but mechanical ranking algorithm. secondly  these results are usually relevant to the query  often contain valuable pieces of information  but are not necessarily the best pages on the topic. finally  the relative merits of these pages are not always obvious; for example  for the query  tree sap car   to find out how to remove tree sap from automobiles   we find that the page ranked 1 isn't particularly superior to the one ranked 1. a possible reason for the latter two phenomena is that search engines like google employ global ranking mechanisms  eg.  pagerank   and the top 1 results are just the most important places on the web where the query terms are mentioned.
　to summarize  documents 1 in a typical search result are good sources of valuable pieces of information  usually from reliable websites; what  if any  are the viewpoints on the query topic latent in these pages  are they mere restatements of what is contained in the top 1 pages  or are they untapped sources of added value to the user 
　as an example  consider the query  jon kleinberg.  a university of wisconsin page describing a colloquium talk by kleinberg is perhaps not the most exciting result for this query  but it is a meaningful snippet of information from a snapshot of the web at some point in time. however  when one notices that the wisconsin page is one among many announcements of kleinberg's talks at various places  what emerges is the analogue of what the news media considers  an interesting storyline.  the collective weight of evidence offered by a handful of pages in the top 1 suggests that this is an angle from which to summarize the web presence of the topic  jon kleinberg .
　the analogy with a newspaper storyline is compelling. the collection of search results for a particular query may be thought of as the collection of facts  chronicles  thoughts and ideas that abound following a major news event. for example  after the superbowl1  newspapers identify several storylines: the main news story about the championship game  one about outstanding contributions by key players  stories of unlikely heroes  key revenges/comebacks  travel and tourism tips about the city where the game takes place  the latest collection of superbowl television commercials  half-time shows  etc. newspapers have mastered the art of presenting these stories  arranging them on the  sports  front page to pique the readers' interest  and directing them to the inner pages that contain most of the content.
　in this paper  we study the problem of how to robustly formulate what constitutes a good storyline within search results; we also address the question of how to mine the top 1  or so  results to uncover the various angles from which the results may be summarized.
　the starting point of our formulation is the observation that  much as in newspaper storylines  each storyline lurking within search results has its unique vocabulary. in the example  jon kleinberg  mentioned above  it is not hard to notice that most announcements of kleinberg's talks share the words 'abstract ' 'distinguished ' 'seminar ' 'sloan ' 'investigator ' 'almaden ' etc. this type of vocabulary is indeed rather unique to university talk announcements  where a fairly standard template is used  containing words like 'abstract'  'seminar'  etc.   and a brief biographical sketch of the speaker is included  hence the words 'sloan' and 'investigator' . these words are also quite uncommon among the other top 1 pages for this query. thus this rather small set of words serves as a signature that unify a collection of pages thematically.
　this example also highlights some important aspects of discovering and presenting collections of thematically unified pages from among search results.
　the first fact is that the notion of a storyline is based on local structure among a handful of documents; therefore  we do not expect traditional clustering and classification approaches to identify these small focused collections. we will say more about this in section 1. in contrast  more focused and combinatorial approaches that explicitly scour the results for  signature structures   such as a small set of terms that uniquely characterize a collection of documents  are likely better suited for the task of finding hidden storylines. the latter fact is an interesting twist in the development of ideas relevant to information organization and retrieval. kleinberg's hits algorithm and its successors  1  1  may be thought of as applying classification concepts  specifically  latent semantic indexing  to link analysis on the web; these ideas have had tremendous influence on web search technologies. our proposal to use signature structures as the one outlined above may be viewed as an application of ideas from link analysis-specifically the work of kumar et al. -within the domain of text analysis.
　the example above also raises an intriguing question about how to present a storyline discovered from the search results. several search engines suggest various possible  query refinements   in terms of additional terms to be included to the query terms  and allow the user to choose one of them. we feel that it is not a good idea to offer the query refinement viewpoint to the user when summarizing an interesting collection of documents. it is probably somewhat

puzzling to a typical web user to be shown terms like the 'abstract' and 'sloan' as possible ways to refine the query  jon kleinberg.  rather  a simple list of  titles of  pages that are considered to be a group might serve as a better way to indicate to the user what the collection is about. to do this  however  it is very important to have robust algorithms to discover the collection  so that the titles and snippets of the web pages automatically convey to the user how these pages are related.
　the idea of mining the top 1 or 1 results may also be viewed as a method to rerank the best documents for a given query. currently  several heuristics exist  homepage detection  hub/faq identification  etc.  that are aimed at improving the quality of the top few results. our work suggests to mine the characteristics and structure in the top 1 pages as a reranking mechanism that will produce a more complete summary of the slice of the web pertaining to the query topic.
technical contributions.
　we first outline a formulation of the problem of reorganizing search results with the intent of highlighting the important storylines. to do this  we develop a semi-axiomatic approach  where we list certain desiderata that we would like our algorithms to satisfy. the compilation of the desiderata is motivated by fairly natural requirements  and the goal of establishing these is to cast the mining problem as a combinatorial optimization problem. roughly speaking  given the term-document relation  we wish to find many pairs  d1 t1  ...   where the di's and ti's are fairly large subsets  respectively  of documents and terms  such that most terms in ti occur in most documents in di  and very few terms in ti occur in the documents in dj  for j = i. each  d t  pair in the collection will correspond to a storyline.
　we show that natural formulations of this problem are np-complete  via a reduction from the maximum induced bipartite matching problem introduced by stockmeyer and vazirani . nevertheless  we show that algorithms based on local search and dynamic programming yield excellent solutions in practice. specifically  we design two algorithms for this problem  one based on a generate-and-prune approach employing local search  and one based on dynamic programming ideas  growing a collection of partial storylines that are best with respect to some cost function. both algorithms take advantage of a pre-processing step  where we present a robust method to identify the collection of terms in a document that are relevant to a set of given query terms.
　finally  we present a number of experimental results highlighting the hidden storylines uncovered by our algorithm  and based on these  present some thoughts about how best to integrate the storylines discovered with the rank-ordering produced by the search engine.
1 related work
　in this section we describe some of previous work in areas that are most relevant to this paper.
link analysis.
　the first body of related work is in the area of search algorithms that try to exploit hyperlink information. link analysis algorithms in the context of web search  starting with the hits algorithm  and its subsequent enhancements  1  1   and the pagerank algorithm   have been the hallmark of many commercially successful search engines.  for a detailed account of link analysis in web search algorithms  see .  the google search engine  google.com  is based on the pagerank algorithm and ideas behind the teoma search engine  teoma.com  are inspired by the hits algorithm.
clustering search results.
　the next is in the area of clustering  especially applied to clustering web search results. one of the early works on clustering web search results was done by zamir and etzioni   see also  ; their technique was to extract phrases from the search result snippets and to identify phrases that are common to groups of documents. the concepts of result set clustering and post-retrieval document clustering have been studied in the information retrieval community  cf.  ; traditionally  they have been used to cluster documents in the result set based on the degree of relevance  to filter out irrelevant documents  and to support context-based relevance feedback. clustering result sets has also been studied in the information visualization community where the goal is to present the search results to the user in the best possible way. latent semantic indexing lsi  and other spectral methods  e.g.    are popular techniques to cluster especially large collection of documents. our work differs fundamentally from the body of work on clustering  since our framework attempts to simultaneously find groups of documents and terms that can be mutually characterized by each other. we will say more about the differences in section 1.
　commercial and experimental search engines have been interested in the problem of clustering web search results. the nowdefunct manjara and its current improved incarnation called eigencluster  www-math.mit.edu/cluster provides a clustering of search engine results; the implementation is based on a spectral clustering algorithm . vivisimo  vivisimo.com  offers a document clustering product that is an overlay to a search engine and can organize search results on the fly. teoma  teoma.com  organizes search results into communities and presents them to the user; the user has the option of refining his/her search using the keywords presented for each community. google has a clustering agent called google sets  labs.google.com/sets . wisenet  wisenut.com   alltheweb  www.alltheweb.com  and many other search engines offer clustering information on top of search results. for a comprehensive account of clustering search engine results  see the article by calishain .
trawling.
　as we mentioned earlier  our problem can be related to that of trawling . trawling is a process to automatically enumerate communities from a crawl of the web  where a community is defined to be a dense bipartite subgraph. one way to trawl the web is to look for bipartite cliques. an algorithm to accomplish this enumeration  in especially massive graphs  was described in . our formulation is inspired by the notion of communities that was defined in trawling. however  we cannot use the trawling algorithm per se in our case since our notion of cliques is more general  and the degree characteristics of our underlying graph make it hard to apply the combinatorial algorithm of  for our instances. on the other hand  our graphs are relatively smaller and fit entirely in main memory  so our set of tools are more extensive  and include local search and dynamic programming .
re-ranking clusters.
　the storylines uncovered by our algorithm leads to the following combinatorial re-ranking question. suppose there is a linear ordering of elements of a universe and the goal is to rank given subsets of the universe  where the ranking should satisfy some basic axioms. in our setting  the universe corresponds to the top 1 results  the subsets correspond to the collections of webpages found by our algorithm  and the question is how to re-rank the clusters  taking into account the ranking of the web pages themselves. this problem has been considered before; cai  shows that under reasonable axioms  ranking functions do not exist for this problem.
1. formulation
　in this section  we describe a mathematical formulation of the storyline extraction problem.
　let q denote a query  a set of terms . let d denote the set of documents returned by the search engine for the query q. for a document d  let t d  denote the multiset of terms in d  and let t denote the union  over all d （ d  of the sets t d . similarly  for a term t  let d t  denote the set of all documents that contain the term t. let g = d t e  denote the bipartite graph  where the lhs set d consists of one vertex per document and the rhs set t consists of one vertex per term  and the edge relation e   d 〜t consists of pairs  d t  where document d contains term t.
　informally  our notion of a storyline consists of a set d of documents and a set t of terms that more or less pinpoint each other  that is  a document d contains most of the terms in t if and only if it belongs to d  and similarly  a term occurs in most of the documents in d and rarely in the others if and only if it belongs to the set t.
　as an example  consider the query  indira gandhi 1; the terms 'jawaharlal ' 'assassination ' 'sterilization ' 'bahadur' 1 appear together in almost all  and only in  pages that contain her biography. thus if we knew that a document d from the result set contains all these terms  we can immediately conclude that it is one of the biographical pages  as opposed to the numerous other top 1  and top 1  pages about the various institutions named after her. in fact  if we simply looked at the documents in d that contain the term 'sterilization ' they all turn out to be biographical pages of indira gandhi; however  to robustly characterize a group of pages as thematically unified  and to do it with a degree of confidence  it helps to find a collection of terms all of which pick essentially the same set of 1 documents from the top 1 results. this also underscores our earlier observation that the set of documents that form a storyline tends to share a vocabulary that sets it apart from the rest of the documents. see figure 1 for an example of signature of storylines in a graph.

figure 1: storylines in a document-term graph.

maximum induced partition into storylines
　let k l be two positive integers. let α β be two constants such that 1 ＋ α   β ＋ 1. given a bipartite graph g = d t  e   find a sequence of pairs of sets  called storylines 
　 d1 t1   d1 t1  ...  ds ts   where di   d  ti   t .
　 1  large disjoint subsets: for each i  |di| − k  |ti| − l; for i =	j  di ”dj = 1/ and ti ”tj = 1/;
　 1a  every document in a storyline consists of most of the terms that define the storyline: for each i  for each d （ di  |t d ”ti| − β|ti|;
　 1b  every term in a storyline appears in most of the documents that make up the storyline: for each i  for each t （ ti  |d t ”di| −
β|di|;
 1a  no term is popular in documents in storylines other than the
one it defines: for i  for each t	;  1b  no document contains too many terms that define the other storylines: for each i  for each d	.
 1  many storylines: s is as large as possible.
　when k l α β are fixed  we refer to the resulting problem as the  k l α β -storyline problem.

   theorem 1. for any integers k l   1 and any α β such that 1 ＋α β＋ 1  the  k l α β -storyline problem is np-hard  and the corresponding decision problem of whether there are at least s pairs of sets is np-complete.
　proof. stockmeyer and vazirani  showed that the maximum induced bipartite matching  mibm  is np-hard. equivalently  they showed that the following problem is np-complete: given a bipartite graph h = u v f   where f  u 〜v  and an integer r  the question is whether there are subsetsu	 u andv such that  r  and the induced subgraph onu  that
is  the subgraph consisting of the edges is a perfect matching1.
　we reduce mibm to the  k l α β -storyline problem as follows. suppose we are given an instance h = u v f  of the maximum induced bipartite matching problem. define the bipartite graph g = d t  e  as follows: each vertex u （ u will define k vertices u1 ... uk in d; each vertex v （ v will define l vertices v1 ... vl in t. if  u v  （ f  then we connect u1 ... uk to v1 ... vb  where b f  then no edge exists between any of the uj's and any of the vj's.
　notation. we will refer to the uj's and the vj's as copies  respectively  of u and v; conversely  we will call u and v the parents  respectively  of the uj's and the vj's. finally  we will refer to two children of the same parent as siblings.
　we claim that h has an induced bipartite matching of size s if and only if g has at least s-many  k l α β -storylines. clearly  if there are subsets u v such that v	| − s and the induced graph on  is a perfect matching  then each pair yields a storyline  du tv   where du ={u1 ... uk}
and tv = {v1 ... vl}  satisfying the  k l α β  requirements.
　conversely  suppose we are given s-many  k l α β -storylines  d1  t1  ...  ds ts  in g. we will construct an induced perfect matching of size s in h. letu =i di andv =i ti. we will  process  each  di ti   and show that we can extract an edge  u v  in h that wasn't extracted while processing  d1 t1  ...  di 1 ti 1 .
　for each i  we claim that di consists only of vertices whose parents are different from the parents of all the vertices in d1 ... di 1. suppose not. then there is a vertex uj （ di that is a sibling of
some vertex ui. however  the neighbors of uj and uj	are identical  so if uj has β|ti|   α|ti| neighbors in the set ti  then so does   contradicting requirement  1b  in the definition of a
 k l α β -storyline. thus  the set

 is said to be a perfect matching in h =  u v f  if
|u	and for each u （ u	  there is exactly one v such
that  u v  （ f
{u （ u | uj （ di for some j}
consists of vertices that are not in
	{u （ u | uj	for some j and some .
let u denote one such vertex  and let uj （ di be the copy of u in di. let vh （ ti denote the vertex that has the most neighbors in di; since the average number of di-neighbors of vertices in ti is βk  it follows that the degree if vh in di is at least βk. let uj （ di be one of the neighbors of vh. clearly  since  uj vh  （ e  it must be that  u v  （ f. thus it remains to show that v has not already been matched by one of the edges we picked in rounds 1 ... i  1. suppose to the contrary that we had picked in round i	  i  via its copy vh	. from the fact that vh	was picked in round i	  it follows that the degree of vh	in di	is at least βk. by definition of the bipartite graph  we know that if vh and vh	both have non-zero degree  then their neighborhood is identical  therefore vh	must have degree β|di|   α|di| in di  which contradicts requirement  1a  in the definition of a  k l α β -storyline. 
1 comparison with  traditional  clustering methods
　in this section  we will briefly discuss why our formulation above is substantially different from what may be considered traditional methods in clustering. the field of clustering is a mature mathematical discipline  with a wide variety of well-known and wellunderstood methods that have been analyzed extensively from various viewpoints; therefore  it is hard to precisely name a handful of methods as the traditional ones. nevertheless  we will note that many of the methods that exist in the literature have some features in common. namely  in a large class of methods  the points to be clustered are points in an ambient metric space  so there is a natural notion of distance between points. more generally  there are graphbased clustering methods where one is given a number of points together with pairwise distances  in the metric case  the pairwise distances obey the triangle inequality . the clustering problem in these settings is to pick several groups of vertices that satisfy some specified criteria  minimum/maximum constraints on the distances between intra- and inter-cluster points  cluster separations  etc. .
　in another class of methods  especially ones based on eigenvector methods  instead of pairwise distances  one starts with pairwise similarities  often they are interchangeable  especially in inner product spaces  as is commonly the case in scenarios where eigenvector methods are employed .
　a more unifying viewpoint of distance- and similarity-based clustering methods is that they are based on an underlying binary function between the given set of points. in our formulation outlined in section 1  note that this is not the case. we do not attempt to cluster the documents based on term-induced binary distance or similarity measures; similarly we do not attempt to cluster the term space based on document-induced binary measures. rather  we attempt to identify as many document set/term set pairs with specified occurrence patterns among them. thus we simultaneously divide the document- and term-space into groups with desirable characteristics. the characteristics we employ are substantially more holistic than can be captured via any binary measure. this is especially clear  considering the fact that the documents we cluster are the search results for specific query terms; therefore they are likely to contain-at the macro level-much the same broad set of terms  and hence will be deemed  similar  or  close  under any fairly natural binary measure. it is  in fact  the common existence of a fairly large set of  signature  terms that sets a storyline apart from others. the reader may recall the example of the query  jon kleinberg  mentioned in the introduction. here  most of the documents recalled by a search engine have similar vocabulary at a global level; nevertheless  there are clear cues such as the set of terms { 'abstract ' 'seminar ' 'sloan ' 'refreshments' }  which pinpoints web pages on kleinberg's talks  the set { 'scientist ' 'invented ' 'analyzed ' 'hits ' 'google' }  which pinpoint articles in blogs and in the popular media about kleinberg's work  and so on. there is no similarity or distance that can be defined a priori that can capture instance-dependent clusters of this kind.
　there is some work in the clustering literature that calls for separate discussion. specifically  the concept of co-clustering is especially relevant in our context; applied to our setting  co-clustering treats the incidence matrix of the document-term relation as specifying a joint probability distribution  of two random variables d and t   and attempts to define r.v. d   resp. t   that indicates which cluster a document  resp. term  belongs to  such that the mutual information between d  and t  is as close to the mutual information between d and t as possible.  see  for an excellent overview  and related literature  especially the information bottleneck method of tishby et al.   a one-sided precursor to co-clustering.  while there are superficial similarites  specifically  simultaneous grouping of documents and terms   there are several differences between co-clustering and our method. our formulation is explicitly combinatorial  and we do not require that all documents or terms be placed into clusters  esp. the pairs that have a high probability mass on them  which are important in maximizing mutual information . we do not seek to  explain  the large scale characteristics of the document-term relation; rather  our goal is to identify explicit  and often  quite small  signature structures that point to some underlying semantic structure in the relation.
　finally  let us say a few words comparing our method to that of zamir and etzioni   who presented an efficient clustering algorithm for documents that works roughly as follows. first one  implicitly  creates a list of all  fairly short  trailing subsequences of sentences from the documents; next  one compiles  for each such suffix  the set of documents that contain the suffix; finally  several of these sets are collapsed based on overlap  via a single-
link clustering method . thus  implicitly  their algorithm clusters documents without expressly using any binary measure of similarity or distance among the documents. nevertheless  one can define a graph on the set of documents based on how many common term sentence-suffixes two documents have  and then define a binary measure of similarity based on the intersection of neighborhoods of two documents in this graph. the resulting clustering is roughly what one obtains by identifying dense neighborhoods in this graph of low diameter. another difference between our method and theirs is that  since their algorithm was based on the suffix tree data structure  they did not consider arbitrary subsets of terms  beyond suffixes of sentences . in our formulation  and certainly as the examples illustrate   the small set of terms that define a storyline often need not occur within the same sentence in the document; furthermore  algorithmically we face a more challenging problem  np-hard   hence our recourse to local search and dynamic programming like methods.
1. algorithms
1 pre-processing
　in this section  we outline the pre-processing steps that we employ in creating the document-term bipartite relation to which we will apply the algorithms of section 1 and 1. the main goal of the steps outlined here are to identify  given a collection d of documents and a  small  set q of query terms  the terms from each d （ d that are  in some sense  most relevant to the terms in q.
　given a document d  we define a graph gd whose vertex set is t d    excluding a standard list of common stopwords   and where an edge is present between terms t t	iff t and t	occur together within a sentence. for the purpose of this step  a sentence is any textual unit with a natural semantics  e.g.  in the context of html pages  anchortext  titles  etc.  qualify as sentences. once gd is constructed  we perform connectivity analysis on gd and discard the small connected components. this has the desirable effect of automatically eliminating all irrelevant noise present in web documents  such as text from templates  sidebars  advertisement links  etc.  leaving us with a very accurate semantic summary of the document.  we could enhance the robustness of this structure further  for example  by eliminating all edges between terms that co-occur only once. 
　once the significant connected components are identified in gd  we conduct a breadth-first traversal on these components  starting at the vertices that correspond to terms in q  and continue the traversal until we have collected some pre-determined number of terms  say 1 . if none of the query terms occurs in the document  we simply start the traversal from the vertex of maximum degree. thus  the terms we collect are at a short semantic distance from the query terms within the document  that is  they co-occur with one of the query terms  or co-occur with terms that co-occur with the query terms  etc.
　note that with this focused term collection method  the resulting document-term bipartite graph is kept rather sparse. this implies that with a reasonable bit of engineering  the scheme is fairly practical  since the amount of data per document is roughly the same amount of data that search engines routinely serve as  snippets  along with each search result  highlighting where in the document the query terms appear . in fact  our storyline extraction algorithms can even be run as a client-side computation  upon request by the user.
1 a local search algorithm
for subsets d   d t   t   let d and let t 
is a subset of documents and t	is a subset of terms  let denote the set of edges in the subgraph induced by the vertex sets d	and t	.
　we now describe a simple heuristic to iteratively find storylines in the processed graph. each step of the iteration consists of first identifying a dense bipartite subgraph of a specific size  next applying a local resize procedure that possibly alters the size of the subgraph  and finally applying a local swap procedure to improve the quality of the storyline. at the end of the step  the documents and terms corresponding to the storyline are removed from the graph completely and the iteration is repeated.
　 1  we now outline the method based on local search to identify dense bipartite subgraphs of a specified size in the graph g. recall that a similar goal was formulated in the context of web trawling . in trawling  the graph consists of hundreds of millions of nodes and identifying a dense bipartite subgraph is quite formidable  even in a heuristic sense. the problem was addressed by first identifying complete bipartite subgraphs  cores  of dense bipartite subgraphs  whose existence in many cases was guaranteed by a theorem in extremal graph theory  and then expanding the cores to dense bipartite subgraphs. in our case  however  the situation is different. our graphs only have a few thousand nodes and edges. given the medium size of our graph  looking only for cores in this graph is quite restrictive as not all dense bipartite subgraphs will contain cores. coupled with the fact that we can hold our entire graph in memory  we can aim for a heuristic algorithm that finds dense bipartite subgraphs directly.
　we now describe a simple local search heuristic for the densest bipartite subgraph of size k 〜. suppose is the current solution  how to get a starting solution will be explained shortly . we apply the following procedure which consists of several local swaps:

repeat until there are no more changes to d and t:

if  d （ d d （ d such that
	 	 	 	   
if  t （ t t（ t such that
	 	“{ } { } 	|  	 | 

　first of all  it is easy to see that the procedure is guaranteed to converge and find a local maximum  since all the quantities are finite. secondly  the size of the subgraph is preserved across each local swap.
　to arrive at an initial solution  d t   we adopt one of the following two strategies. the first is a greedy one: start with an empty graph and keep adding nodes till it is of size k〜; at each step  pick a node which will contribute the greatest to the density. the second is a random one: pick k documents as d and  terms as t.
　the output of this step is the densest subgraph that results from applying local swaps to greedy/random starting solutions.
　 1  we apply the following local resize step to allow the storylines to grow or shrink beyond the original size of k〜. suppose the current storyline is  d t . we apply the following procedure:

repeat until there are no more changes to d and t or too many changes have occurred:
	if	d	 	  then
= “{ }
	if  	  then
	if  t （	  then
if  t （ t	  then let t
　 1  in this step  we use local swap once again  but this is to improve the quality of the storylines as prescribed by our formulation  rather than just optimize the density of the induced subgraph. local swap can be used in conjunction with any or all of quality measures that are described below.
　suppose  d t  is a storyline. then  we define the following measures of quality:
	q1 d 	 t = 
	q		q
	d（d	|t|	d（d	|t|
　note that q1 captures how dense  on average  the storyline is; q1 is an extremal version of q1 where we focus on the minimum induced degree of the documents in d. thus  a storyline satisfying criteria  1a  and  1b  in our formulation would have high values for these quantities. likewise  q1 and q1 capture how well  on the average/in the worst case  the storyline satisfies criteria  1a  and  1b  in the formulation. a good storyline satisfying our criteria would have low values for these quantities.
　it is easy to see that the above algorithm is very simple and can be implemented in an efficient manner.
1 a version of dynamic programming
　our next algorithm is based on ideas underlying dynamic programming. let s denote some integer parameter  say 1. we begin with s arbitrary storylines  d1 t1  ...  ds ts   where each ti has exactly one term  chosen either greedily by min-degree  or randomly  and di consists of all documents that contain the unique term in ti. then we visit each term t （ t in turn  and extend each storyline  d t  by adding t to t  if it is not already present in t . this gives us up to 1s distinct storylines  even though the document sets could be identical for many of the storylines . we evaluate each storyline  d t  with respect to various measures  including |d| |t| |d||t| q1 d t  q1 d t . let c1 c1 c1 denote cost functions such that
1 ＋ c1 ，  c1 ，  c1 ，  ＋ 1.
for each storyline  d t   define the total cost by
c d t = c1 |d| |t| +c1 q1 d t  +c1 q1 d t  .
the cost function c1 is chosen so that it is lowest when |d| and |t| are roughly k and l  respectively  where k and l are from the problem formulation  in practice  k and l are approximately 1   and becomes close to 1 if either |d| or |t| is too small or too large  or if the product |d||t| is too small or too large. the cost function c1 is typically chosen to be some constant multiple of q1  and c1 is chosen to be some constant multiple of q1.
　once the costs are computed forall the  up to  1s storylines we have  we sort them based on the costs  and retain the s storylines of lowest cost  after eliminating some of the storylines whose underlying document sets are duplicates of the document sets of many other storylines . this done  we proceed to the next term in t . after we have processed all terms in t   we have up to s storylines  and we output the ones with total cost below some pre-specified threshold.
　note that having three cost functions allows us to vary the degree to which each of the quantities |d| |t|  q1 d t   and q1 d t  influences the total cost  and hence the quality  of the storylines discovered. for example  if we simply optimize on |d||t|  ignoring the influence of q1 and q1  we will end up with the entire collection of documents and terms; if we focus entirely on q1  any one edge is sufficient; if we focus on |d||t| and q1  we will find large  but possibly unbalanced  and dense subgraphs  e.g.  the most popular terms  or the most dense documents   etc; if we focus entirely on q1  together with |d| |t| |d||t|  but ignoring q1  we will obtain subgraphs that may not be dense but whose terms are rare outside the subgraph. by carefully balancing these parameters  we have the ability to produce storylines that are dense within and sparse without  satisfying our goals.
　notice that  similar to dynamic programming  this method allows us a compact implementation with an s〜|t | matrix  where in cell  i t   we store whether term t was included in the i-th best storyline  the cost of this storyline  and a pointer to the storyline that includes  besides the initial terms  terms up to the predecessor of t.
　once the best storylines are identified at the end of the pass through all the terms  we remove the corresponding terms  and repeat the process until no more new storylines are extracted.
1. experiments
　we now present some highlights of the storylines uncovered by our algorithm for various queries. our experiment consisted of 1 queries  that were extracted from the lycos  lycos.com  weekly top queries from 1 and from a locally available query
　
log . we retrieved the top 1 documents for each query using vivisimo  vivisimo.com . on average  the graph corresponding to each query had 1 document nodes  1 term nodes  and 1 document-term edges.
flu epidemic 1   1; terms = vaccines strains infection viruses ... 1. msnbc - the genetic genesis of a killer flu
http://www.msnbc.com/news/1.asp cp1
1. flu center http://www.bcm.tmc.edu/pa/flucenter.htm
1. flu shot did not stop the flu epidemic
http://suewidemark.netfirms.com/flushots.htm 1   1; terms = deaths spread reported united ...
1. sheboygan-press: sheboygan couldnt escape flu ...
http://www.wisinfo.com/sheboyganpress/news /archive/local 1.shtml
1. bird flu epidemic spreads to pakistan as death ...
http://www.guardian.co.uk/international /story/1 1.html 1. cdc says flu epidemic appears to be waning http://www.duluthsuperior.com/mld
/duluthtribune/1.htm 1   1; terms = bird department studies avian ...
1. poultry vaccine might worsen flu epidemic http://www.rense.com/general1/poul.htm
1. portrait of a probable killer: viral double act ...
http://www.nature.com/nsu/1
/1.html 1   1; terms = spanish medical 1 pandemic ...
1. the flu epidemic of 1 http://www.viahealth.org/archives /history1.html
1. the american experience - influence 1 http://www.pbs.org/wgbh/amex/influenza/　we ran the local search and dynamic programming based algorithms for each of the query. on average  the algorithms took under nine seconds  the implementation of our local search was fairly naive-one could use sophisticated data structures to considerably speed up the local search . the average number of storylines found was 1 for each query and an average storyline had around 1 documents and 1 terms. the average number of edges in the induced subgraph of the storylines is 1; these numbers indicate that the storylines extracted by our algorithms are highly dense subgraphs. the largest storyline for each query had  on average  1 documents  1 terms  and 1 edges. this indicates that the largest storyline for each query is not as dense as an average storyline  suggesting that our algorithm is not biased towards the size of the storylines.
　since it is very difficult to evaluate the  quality  of storylines as perceived by a human user  or to compare their structure with that of results from  one of numerous  clustering methods  we will present some statistics  and some case studies of our algorithms.
　we evaluate the four quality measures for the queries. the average values and variances of thes measures are μ q1 =1 σ q1 = 1;μ q1 =1 σ q1 =1;μ q1 =1 σ q1 =1; and μ q1  = 1 σ q1  = 1. based on the variance  the measures q1 and q1 are less closely concentrated around the mean than the other two-this is not surprising  since these are  worst case  measures  based on min/max rather than average . based on visual examination of many of the results  we notice that a low value of q1 and a large gap between q1 and q1 always seems to indicate a storyline of high quality. this also yields a natural way to rank the storylines  and also to threshold them to ensure very high quality.
1 case studies
　we present some case studies of the storylines uncovered by our algorithms for various queries. to avoid clutter  we will provide only a subset of the terms  documents  and titles  for some of the prominent storylines. for each document  we also provide the ranks of the documents in the original results.
　 1  the first query is  flu epidemic.  in table 1  we see that the first storyline contains information about flu  identified by terms like 'vaccines'  'strains'   the second contains seasonal news  identified by terms like 'deaths'  'reported'   the third is about bird flu  identified by terms like 'avian'  'bird'   and the fourth is about
spanish flu epidemic from 1  identified by terms like 'spanish' 
'1' .	table 1: sample storylines for  flu epidemic. 
　 1  the second query is  jon kleinberg.  in table 1  we see that the first storyline contains information about lectures on some aspect of his research  identified by terms like 'networked'  'world'   the second contains bibliographic information  identified by terms from titles of his publications   the third storyline is about kleinberg's ibm almaden connections.
　 1  the third query is  thailand tourism.  in table 1  we see that the first storyline contains information about travel information on thailand  keywords are 'skytrain'  'reservations'  and the second storyline has an economic flavor to it  keywords like 'industries' 
'potential' .
thailand tourism 1   1; terms = skytrain reservations...
1. amazing thailand tourism travel directory http://www.thailand-travelsearch.com/ 1. amazing thailand tourism travel forums http://www.thailandtravelforums.com/
1. thailand airlines  hotels resorts...
http://www.thailandtravelsearch.com/thailand/ tourism travel directory/index.shtml
1. thailand tourism: vision 1
http://www.info.tdri.or.th/library/quarterly/ text/j1.htm 1   1; terms = potential worldwide spread industries 1. asian market research news:...
http://www.asiamarketresearch.com/news /1.htm
1. thailand outlook.com : the offical gateway...
http://www.thailandoutlook.com/top menu /special/thailand tourism.asp 1. suwan site-learn more about thailand http://www.geocities.com/heartland /1/thailand.html
1. thailand tourism: vision 1 http://www.info.tdri.or.th/library
/quarterly/text/j1.htmjon kleinberg 1   1; terms = models world networked properties ...
1. webshop 1 abstract: jon kleinberg
http://www.webuse.umd.edu/abstracts1/ abstract 1 kleinberg.htm
1. jon kleinberg http://www.cs.rochester.edu/seminars/ sems1/kleinberg.html
1. events: cis distinguished lecture series: jon kleinberg
http://dp.seas.upenn.edu/news-1-
　　　1-cis-kleinberg.html 1. jon kleinberg http://robotics.stanford.edu/ cs1
/previous/winter1/abst-kleinberg.html 1   1; terms = approximation neighbor server flow ...1. jon kleinberg's homepage http://www.cs.cornell.edu/home/kleinber/
1. dblp: jon m. kleinberg http://www.informatik.uni-trier.de/ ley/db
/indices/a-tree/k/kleinberg:jon m=.html
1. jon kleinberg http://www.cs.cornell.edu/annual report/ 1/kleinberg.htm 1. disc - jon m. kleinberg http://www.sigmod.org/sigmod/
/disc/a jon m kleinberg.htm 1   1; terms = ibm center prabhakar almaden ...
1. kleinberg
http://www.cs.cornell.edu/faculty/ annual reports/kleinberg.htm
1. trawling emerging cyber-communities automatically
http://www1.org/w1-papers/1a-search-mining /trawling/trawling.html
1. yuntis: collaborative web resource categorization ...
http://www.ecsl.cs.sunysb.edu/yuntis/
1. mit eecs events  1
http://www.eecs.mit.edu/ay1/events/　 1  the fourth query is  french schools religious head ban.  in table 1  the first storyline consists news articles on the broad topic  referring to various religious groups  eg.  sikhs   the second storyline also has news articles  but ones that essentially report the table 1: sample storylines for  thailand tourism. 
passage of the bill in the parliament  and the third consists of viewpoints and analysis from more religious organizations  keywords like 'belief'  'faith' .
　 1  the fifth query is  colin powell.  in table 1  we see that the first storyline offers viewpoints questioning the war  and the second storyline is on his biography  'bronx'  'parents' .
　 1  the sixth query is  atkins diet.  table 1 shows two storylines- the first one on books  information  etc.  while the second storyline consists of more critical reviews of the diet.
1 observations
　we note that for each of the queries  the storylines we chose to present are not the only ones uncovered by the algorithm; rather  they were chosen to convey storylines with strikingly different viewpoints from the generic top 1 page for the queries. we make several observations about the results we obtain.
　 1  in many cases  storylines  even though they might be quite small  turn out to be very useful and distinctive.
　 1  if we examine a given storyline  the ranks of the pages present in the storyline span the entire spectrum 1 of the results. most
table 1: sample storylines for  jon kleinberg. 
often  it is the case that for many of the pages in a storyline  the rank is much more than 1. in some storylines  even the best rank is quite poor.
　 1  by giving a chance to pages ranked well below the top 1  our algorithm addresses the  tyranny of the majority -in other words  it gives a chance for the minority viewpoints on the query to be reflected adequately in the first page of search results.
　 1  the algorithm can uncover interesting communities especially when it is not obvious such communities exist. by becoming aware of the existence of several different discussion groups  the user searching for information about removing tree sap from cars can directly post her/his query and benefit directly. in this case  note also that all the ranks are beyond 1.
colin powell 1   1; terms = civilian missiles reagan ...
1. kurt nimmo: colin powell  exploiting the dead ... http://www.counterpunch.org/nimmo1.html
1. washingtonpost.com
http://www.washingtonpost.com/wp-srv/nation/ transcripts/powelltext 1.html 1. alternet: the unimportance of being colin powell http://www.alternet.org
/story.html storyid=1
1. colin powell
http://www.worldworks.org/politics/cpowell.htm 1   1; terms = bronx earned parents 1 ...
1. colin powell http://www.infoplease.com/cgi-bin/id/a1
1. amazon.com: books: my american journey http://www.amazon.com/exec/obidos/asin/ /1/inktomi-bkasin-1/ref=nosim
1. powell  colin l. http://www.state.gov/r/pa/ei/biog/1.htm
1. general colin l. powell - biography
http://teacher.scholastic.com/barrier/ powellchat/bio.htm 1. askmen.com - colin powell http://www.askmen.com/men/business politics/
1 colin powell.htmlfrench schools religious head ban 1   1; terms = stigmatize helsinki sikh law lead 1. cnn.com - chirac: ban headscarves in schools http://www.cnn.com/1/world/europe/1/ france.headscarves/
1. cnn.com - france backs school head scarf ban http://www.cnn.com/1/world/europe/1/ france.headscarves 1. worldwide religious news http://www.wwrn.org/parse.php idd=1&c=1
1. rights groups fear french cult bill would curb...
http://www.cesnur.org/testi/fr1k july1.htm 1   1; terms = overwhelmingly hanifa conspicuous...
1. ktvu.com - education - head scarves ban in french...
http://www.ktvu.com/education/1 /detail.html
1. french parliament votes to ban headscarves in schools
http://www.brunei-online.com/bb/thu /feb1.htm
1. france-head-scarves  1st writethru
http://www.cbc.ca/cp/world/1 /w1.html
1. ap wire - 1/1 - lawmakers ok french ...
http://www.sunherald.com/mld/sunherald
/1.htm 1   1; terms = beliefs  society  council  faith...
1. diocese of stamford http://www.stamforddio.org/
1. the observer - special reports - schools' bid ...
http://observer.guardian.co.uk/islam/story/ 1 1.html
1. foxnews.com - top stories -...
http://www.foxnews.com/story /1 1.html
1. khilafah.com - germany paves way for hijab ban http://www.khilafah.com/home/ category.php documentid=1
1. islam online - news section http://www.islam-online.net/english/news
/1/1/article1.shtmlatkins diet 1   1; terms = weight diets protein purposes...
1. dr atkins diet - plan  recipe  books and more http://www.dr-atkins-diet.org/
1. amazon.com: books: dr. atkins' new diet revolution http://www.amazon.com/exec/obidos/asin
　　　/1x/inktomi-bkasin-1/ref=nosim 1. amazon.com: books: dr. atkins' new diet revolution http://www.amazon.com/exec/obidos
/tg/detail/-/1 v=glance
1. atkins diet & low carbohydrate weight-loss support http://www.lowcarb.ca/ 1   1; terms = tissue starches effective...
1. atkins diet exposed: why it is complete bull http://www.supplecity.com/articles /diets/atkinsdiet.htm
1. atkins diet
http://www.dieting-review.com/atkins.htm
1. weight loss and the atkins diet http://weightloss-and-diet-facts.com /atkins-diet.htm
1. phil kaplan revisits the atkins diet http://www.philkaplan.com/thefitnesstruth
/atkinsrevisited.htmtable 1: sample storylines for  colin powell. 
table 1: sample storylines for  french schools religious head ban. 
table 1: sample storylines for  atkins diet. 
　
1. concluding remarks
　we have presented a combinatorial framework to data-mine web search results with the aim of uncovering storylines that may be buried in the highly-ranked pages. our framework leads to a family of simple  natural  and efficient algorithms based on detecting dense bipartite subgraphs in the term-document relation. experimental evidence suggests that there is much value to be attained by mining the search results beyond the first screenful  and we hope that these ideas will influence web search paradigms of the future.
　an interesting future avenue is to work with enhanced formulations of the term-document relation  perhaps derived from natural language understanding  another question is to find methods that can scale well to the task of mining the top 1 or 1 search results-those pages might offer interesting viewpoints on the query topic as well.
　a larger issue to consider is the mining of an archive of web pages to extract interesting storylines; this will be particularly relevant  say  1 years from now when queries of the form  computer science research during 1  would be conceivably quite interesting. a ranked list of 1 web sites is almost surely not the answer one would like to see.
