this paper describes an evaluation method based on term relevance sets  trels  that measures an ir system's quality by examining the content of the retrieved results rather than by looking for pre-specified relevant pages. trels consist of a list of terms believed to be relevant for a particular query as well as a list of irrelevant terms. the proposed method does not involve any document relevance judgments  and as such is not adversely affected by changes to the underlying collection. therefore  it can better scale to very large  dynamic collections such as the web. moreover  this method can evaluate a system's effectiveness on an updatable  live  collection  or on collections derived from different data sources. our experiments show that the proposed method is very highly correlated with official trec measures.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval
general terms
measurement  performance
keywords
ir-system evaluation  web search evaluation  trels
1. introduction
　the evaluation of information retrieval  ir  systems is the process of assessing how well a system meets the information needs of its users. ir research has a well established tradition of comparing the relative effectiveness of different retrieval approaches. the predominant paradigm for system evaluation  first introduced in the cranfield experiments   requires a test collection consisting of a fixed set of documents  a set of topics  and a set of relevance judgments  called qrels  created by human assessors who mark the
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
documents deemed relevant to each topic. each retrieval system to be evaluated produces a ranked list of documents for each topic in the test collection. the quality of the system for a single topic is computed as a function of the ranks of the marked documents within the ranked list produced by the system. the effectiveness of the system as a whole is then computed by averaging the scores across the entire set of topics.
　the main difficulty of the cranfield approach is the necessity of complete relevance judgments  i.e.  for each topic  all relevant documents in the collection must be discovered and marked. while this may be realistic for small collection  it is certainly infeasible for large ones. voorhees  estimates that nine person months are needed to judge one topic for a collection of 1 documents. in order to overcome this difficulty  a pooling technique can be used to create a subset of documents to be judged for a topic. each document in the pool is judged by a human assessor and documents not in the pool are assumed to be irrelevant. zobel  showed that pooling  based on the top 1 results of each participant  can provide a reliable evaluation methodology. many studies have demonstrated the reliability of evaluation based on qrels and pooling .
1 problems with qrels-based evaluation
　qrels-based evaluation of ir systems is widely used for research purposes by the text retrieval conference  trec  and by many others. however  several problems with this method make it impractical for very large and dynamic collections. indeed  the scalability of trec's evaluation methods has been recently addressed in the sigir workshop on evaluation methodologies for terabytes-scale text collection . moreover  qrels are in particular inappropriate for many tasks that are present when developing commercial ir systems for intranets and enterprises. such systems need to be tested on many different collections  derived from different data sources and content types  with perhaps multiple score flavors. once such systems are deployed  qrels are again problematic when needing to continuously monitor search quality on  live   updatable indices. many commercial search engines often use human testers to evaluate the performance of their systems. following are several examples of the problems with qrels-based evaluation:
web indices. web data is extremely dynamic - millions of pages are created and deleted every day  and the content of existing pages changes quite frequently. a recent study found that within a single week  the content in over 1% of web pages will undergo some non-trivial change . consequently  snapshots of the web taken several weeks apart may give rise to very different indices. thus  qrels may rapidly become stale  with new pages replacing old ones as the most appropriate online resources for queries. in other cases  even though the  old  expected results remain good resources  search engines will not retrieve them in response to queries. instead  the engines will return near-duplicate pages  that have equivalent content but different urls than the qrels  replicated content is prevalent on the web  .
search over mail and news collection. these are collections where ir systems may want to factor the dates associated with documents into their ranking formulae  biasing the results toward the more recent and fresh news articles or mail documents. qrels-based approaches inevitably fail when recency is factored into the scores  since documents that existed while the qrels were assembled will now be ranked less favorably  with fresher documents  that were not available when the qrels were assembled  getting the advantage. thus  by design  date-based ranking flavors will achieve low scores in qrels-based evaluations.
stand-alone evaluation. in a competitive market  where rival companies cannot share collections with each other but rather must perform quality evaluations in stand-alone mode with only their system at hand  pooling of results is not a viable option and so assembling a comprehensive set of qrels is a daunting task. this is particularly true over collections of non-hyperlinked documents  where one cannot follow links from mediocre documents to relevant ones.
indices covering only part of the document space. in many large-scale search systems  the index does not cover the entire document space  yet qrels are distilled from the entire collection  not necessarily from its searchable  indexed  subset. web-based qrels  for example  may contain pages that were not crawled by some search engines  as even the largest engines cover only a fraction of the web . in such cases  qrels-based quality evaluations cannot distinguish between imprecision  problems with the ranking formula as applied to the indexed content  and bad coverage  non-comprehensive set of documents in the index . in the web-ir domain  where crawl policies impact the quality of the resulting index  1  1   it is important to identify whether low quality stems from insufficient coverage or from an inappropriate score function.
1 our approach
　in order to overcome the deficiencies of qrels-based approaches we propose a method that measures an engine's quality by dynamically examining the content of the retrieved results rather than by looking for pre-specified relevant pages. instead of counting the number of relevant documents in the result set  we look for occurrences of a pre-specified list of terms relevant and irrelevant to these queries in the result documents.
　specifically  the input to our evaluation system is a set of queries; each query is associated with two term relevance sets  trels . the first set  ontopic  contains terms that are likely to appear in most relevant documents. the second  offtopic  contains terms that are unlikely to occur within relevant documents. given a set of ranked documents returned by an examined system for a given query  our evaluation method scores every result d by considering the appearances of ontopic and offtopic terms in d. the score of all documents is aggregated into a score for the returned result set as a whole. this aggregation may take into account both the rank of each document d and the score it was assigned by the search system. as with qrels-based evaluation  the trels-based scores for all topics are averaged to produce the overall evaluation of the ir system.
　the proposed method does not involve any document relevance judgments  and as such is much less sensitive to changes to the underlying collection. furthermore  the evaluation method is not bound to any specific document collection nor restricted to any point in time  and can thus be used over different collections at different times. we show that there is a strong correlation between an engine's  or equivalently  a ranking function's  quality as measured by our method  and its quality as measured by the official trec measures. in particular  we demonstrate that our evaluation scheme clearly separates the best  and worst  retrieval systems from the middle of the pack.
　the rest of the paper is organized as follows: section 1 reviews related work on ir evaluation methodologies. section 1 formally defines trels-based evaluation  and highlights its advantages. section 1 describes our experimental setup  and reports the results of our experiments comparing evaluation by trels to evaluation by qrels. conclusions and directions for future research are presented in section 1.
1. related work
　interestingly  an evaluation approach similar to ours was already proposed in 1 . o'connor devised a method to validate manual indexing of a collection of 1 documents from a pharmaceutical research center. he examined two queries:  toxicity  and  penicillin  by characterizing for each query positive and negative evidence in a way which very much resembles our on/off-topic terms. o'connor used those lists to test the correctness of the manual indexing system by looking for occurrences of the appropriate terms in the documents indexed under  toxicity  and  penicillin . his results were excellent - between 1% and 1%  depending on the sample  of his predictions regarding indexing accuracy were indeed correct.
　several recent studies have dealt with automatic evaluation without relevance judgments in order to bypass the difficulties related to manual assessment. soboroff et al.  suggested an evaluation methodology which does not require relevance judgments at all. their method avoids manual assessments by randomly selecting a subset of documents from the pool of each topic  indicating them as  pseudo  relevant to that topic. while the ranking of the ir systems based on this method correlated positively with official trec ranking  the performance of the best systems and the worst systems was not predicted well.
　in a somewhat similar approach  nuray and can  also avoid the need for relevance judgments by automatically considering a subset of each topic's pool as pseudo-relevant documents. however  instead of selecting that subset randomly  all documents in a pool are ranked according to their similarity to the query and the top ranked documents are considered as  pseudo  relevant. they also show a positive correlation of their ranking with the official trec ranking.
　the correlation is similar to the one achieved by the random selection method described in .
