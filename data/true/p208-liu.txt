many existing data mining techniques often produce a large number of rules  which make it very difficult for manual inspection of the rules to identify those interesting ones. this problem represents a major gap between the results of data mining and the understanding and use of the mining results. in this paper  we argue that the key problem is not with the large number of rules because if there are indeed many rules that exist in data  they should be discovered. the main problem is with our inability to organize  summarize and present the rules in such a way that they can be easily analyzed by the user. in this paper  we propose a technique to intuitively organize and summarize the discovered rules. with this organization  the discovered rules can be presented to the user in the way as we think and talk about knowledge in our daily lives. this organization also allows the user to view the discovered rules at different levels of details  and to focus his/her attention on those interesting aspects. this paper presents this technique and uses it to organize  summarize and present the knowledge embedded in a decision tree  and a set of association rules. experiment results and practical applications show that the technique is both intuitive and effective. 
1. introduction 
producing too many rules has been regarded as a major problem with many data mining techniques  1  1  1  1  1 . the large number of rules makes it very hard for manual inspection of the rules to gain insight of the domain. this paper argues that the key problem is not with too many rules  but with our inability to organize and to present the rules in such a way that is easily analyzed by the user. 
   let us use an analogy to explain. we equate the discovered rules from a database with the knowledge contained in a conventional scientific book. this is reasonable because discovered rules are a form of knowledge. although a book can contain a huge amount of knowledge in a few hundred pages  people seldom complain that this book is too thick and contains too much information  and i cannot read and use it . in most cases  a thick book with a comprehensive coverage is of great value. however  we do often hear people complain that a book or an article is badly written and badly organized. a good book is easy to read because the author has put a great deal of effort to organize and present the contents of the book in such a way that is able to constantly draw the reader s attention and enables him/her to follow and to understand the contents easily. 
   a common strategy for organizing the contents of a book is by hierarchy. in such an organization  the contents are divided into topics  and presented at different levels of details. the main advantage of the hierarchical organization is that it allows the reader to manage the complexity of knowledge  and to view it from general to specific  and to focus his/her attention on those interesting aspects. for a hierarchical organization to work effectively  summarization is crucial. in a book  chapter and section headings  and abstract and/or summaries are used to tell the reader what to expect from each chapter or each section. from the summaries  the reader can obtain an overall view of what the chapter or section is about and decide whether to read the details. 
   the question is can we organize and present the discovered rules like a well organized book so that they can be easily browsed and used by human users  this paper shows this is possible. till now  little research has been done in this direction. in this paper  we propose a novel technique  called general rules  summaries & exceptions  gse   to organize  summarize and present the discovered rules. 
   in the past few years  we had a number of applications in education  medical and other domains. in these domains  users always find it very hard to browse through the discovered rules  even when the set is not very large  to gain a good understanding of the domain. although there exist techniques to deal with the problem of too many rules  they are still not satisfactory. through extensive interactions with our users  we discovered that our users always talk about knowledge in a certain way  i.e.  in terms of general patterns and special cases. for example  when we tried to profile the students of an educational institution  our users would make the following comment: in general  our good students have certain profiles. however  in some situations  these may not be true. in the medical applications  the situation is similar. the 

 
 
permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers  or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd 1  boston  ma usa 
 c  acm 1-1-1/1 ...$1 
doctors always say that people with certain characteristics tend to have a particular disease. however  in some special situations  they may not develop the disease. we began to realize that the individual rule based representation of knowledge employed in current data mining techniques is not intuitive for human users. this inspired us to design the proposed technique  which use general rules to give the user a big picture of the domain and exceptions to point out the special cases. the summaries are used to summarize those non-essential rules  which will be clear later.   
 existing data mining systems typically generate a list of rules with no further organization. this is like writing a book or a paper by randomly listing out all the facts and formulas. the resulting book will be difficult for anyone to read because one cannot see any relationships among the facts and formulas. in data mining applications  a list of rules presents the following two problems: 
1. it obscures the essential relationships and the special cases or exceptions  and makes it hard for the user to piece the information in the rules together to obtain a good overall understanding of the underlying relationships in the domain.  
1. it represents the discovered knowledge only at a single level of detail. this flat representation is not suitable for human consumption because we are more used to hierarchical representation of knowledge. hierarchical representation allows us to manage the complexity of knowledge  to view the knowledge at different levels of details  and to focus our attention on the interesting aspects. 
let us use an example to illustrate. our example dataset has two attributes  a1 and a1   1 data tuples and two classes  뫶 and o. the domain of a1 is {a  b  c  d}  and the domain of a1 is {x  y}. assume a rule miner found the following rules  support and confidence have their usual meanings  :  
 	r1: 	a1 = x 뫸 뫶 	 sup = 1%  conf = 1%  
 	r1: 	a1 = a  a1 = x 뫸 뫶   	 sup = 1%  conf = 1%   	r1: 	a1 = c  a1 = x 뫸 뫶 	 sup = 1%  conf = 1%   	r1: 	a1 = d  a1 = x 뫸 뫶    sup = 1%  conf = 1%  
 	r1: 	a1 = b  a1 = y 뫸 뫶 	 sup = 1%  conf = 1%   	r1: 	a1 = y 뫸 o 	 sup = 1%  conf = 1%   	r1: 	a1 = b  a1 = x 뫸 o 	 sup = 1%  conf = 1%  
 	r1: 	a1 = a  a1 = y 뫸 o 	 sup = 1%  conf = 1%   	r1: 	a1 = c  a1 = y 뫸 o  	 sup = 1%  conf = 1%   	r1: 	a1 = d  a1 = y 뫸 o 	 sup = 1%  conf = 1%  
by simply looking at the 1 rules above  it is hard for us to obtain a good overall picture of the relationships in the data.   these rules are actually generated from the data space given in figure 1. the two numbers within the brackets next to each junction  or value combination  are the number of 뫶 class tuples  and the number of o class tuples respectively. only the majority class symbol is drawn for each value combination.  
 	 
 	a1 x 	 1  1  	 1  1  	 1  1  	 1  
 
 
 	y 
 
 
 	 a 	 b  	c  	d a1 
             figure 1. the example data space  now  let us change the 1 rules to the following two general rule  summary & exceptions  gse  patterns  the picture becomes clear and much easier to understand. let us ignore the summary component for the time being. 
gse-1: a1 = x 뫸 뫶 	 sup = 1%  conf = 1%  
 	 	except r1:  a1=b  a1=x 뫸 o   sup = 1%  conf = 1%   
gse-1: a1 = y 뫸 o 	 sup = 1%  conf = 1%   	except  r1: a1=b  a1=y 뫸 뫶  sup = 1%  conf = 1%  
the first part of a gse pattern is a general rule  and the second part  after except   is a set of exceptions to the general rule. from these two parts of the gse patterns  we see the essential relationships of the domain  the two general rules  and the special cases  the exceptions  r1 and r1 . this representation is much simpler than the 1 rules because r1-r1 and r1-r1 are not used. these rules  r1-r1 and r1-r1  fragment the knowledge in the data. they are non-essential  and worse still they make the discovered knowledge hard to interpret.  
 this  however  is not to say that the fragmented rules are completely useless. the fragmented rules may be interesting due to their higher confidences  e.g.  1% and 1% . we use the summary component to summarize these fragmented rules. here  we use the highest confidence of the rules as the summary  see section 1 . thus  the complete gse-1 and gse-1 are as follows: 
gse-1: a1 = x 뫸 뫶 	 sup = 1%  conf = 1%  
 	summary  	highest conf. of 뫶 class rules = 1% 
 	 	except r1:  a1=b  a1=x 뫸 o   sup = 1%  conf = 1%  
gse-1: a1 = y 뫸 o 	 sup = 1%  conf = 1%  
 summary  highest conf. of o class rules = 1%  except  r1: a1=b  a1=y 뫸 뫶   sup = 1%  conf = 1%    the gse representation of the discovered knowledge has two key advantages: 
simplification: it simplifies the discovered rules using general rules and their exceptions  and hides those fragmented rules that have the same classes as the general rules. the fragmented rules are non-essential. they are summarized with the summary field of the gse pattern. if one is interested in them  one can follow this link to view them.  
organization: it organizes the discovered knowledge in an intuitive way  which is easy for the user to follow because the gse representation is a common way of representing knowledge in our daily lives. what is also important is that this organization naturally draws the user s attention. in our applications  we found that exceptions often attract the user because they are unexpected with respect to the general rules. when the user sees some general rules  he/she is almost always eager to see the exceptions.  
in section 1  we will see that the gse representation also naturally introduces a hierarchical  multi-level  organization of the discovered rules.  
 this paper presents the gse representation  and uses it to organize  summarize and present the knowledge embedded in a decision tree and a set of association rules. since decision trees and association rules embed somewhat different types of knowledge  the proposed technique is applied differently depending on the context. experiment results and practical applications show that the proposed technique is very effective and efficient. 
1.  related work 
in the past few years  a number of techniques were proposed to deal with the problem of too many rules  1  1  1  1  1  1  1 . the main idea of these techniques is to use the user s knowledge or statistical measures to remove those uninteresting rules. our work is different. we aim to organize and summarize the rules so that the user can browse through them easily and effectively.  
    proposes a template-based approach for finding interesting rules. this approach first asks the user to specify what rules he/she wants. the system then finds those matching rules. the technique assumes that the user knows exactly what he/she is looking for. however  in many situations  the user does not know what to find exactly. his/her needs can only be met by incorporating browsing as well as template-based search in the system. it is well known that humans are better able to recognize something than to generate a description of it .  
     1  1  1  1  propose an expectation or belief based approach to find interesting rules. in this approach  the user is also required to input his/her existing knowledge about the domain  and the system then finds those conforming and unexpected rules. this approach is more advanced than the template-based approach because it not only finds those rules that satisfy the templates but also the unexpected rules. however  since it requires input knowledge from the user  it suffers from the problem of knowledge acquisition. that is  one may know a great deal  but it is very difficult for one to tell what he/she knows .  
    1  1  investigate using item constraints specified by the user in association rule mining to generate only the relevant rules. this approach also requires the user to know precisely what he/she wants or does not want.  
  presents a technique to summarize the discovered association rules. it selects a subset of the rules  called direction setting rules  ds rules   to summarize the discovered rules. ds rules represent the essential relationships of the domain. for example  we have the following discovered rules from a loan application domain:   
 r1: job = yes 뫸 loan = approved   sup = 1%  conf = 1%  
 r1:  own house = yes 뫸 loan = approved  
  	 	 sup = 1%  conf = 1%  
if these two rules are known  then the following association rule is not so surprising to us:  
 r1: job = yes  own house = yes 뫸 loan = approved 
 	 	 sup = 1%  conf = 1%  
because it intuitively follows r1 and r1. r1 and r1  the ds rules  are used to provide a summary of the three rules. r1 is nonessential. this technique is proven useful in our real-life applications. however  some shortcomings are also exposed. firstly  it takes at least two rules to summarize another rule. however  the user only sees one rule at a time. thus  when the user just sees a particular ds rule  it is not easy to imagine those rules that have been summarized because he/she may not have seen the other ds rules. secondly  comparing to gse representation  ds rules are less intuitive as ds rules do not directly reflect the way that we think about knowledge. finally  the technique cannot be applied to summarize the rules in a decision tree because it requires at least two rules to summarize another. however  one of the rules may not be in the tree as a decision tree typical only discovers a subset of the rules in data.   in   we proposed to use general rules & exceptions  ge   without the summary component  to represent decision trees. the ge representation is less effective than gse as the information on the fragmented rules is lost.  also does not study organization  summarization and presentation of association rules  which presents a different set of problems.  
    1  1  study the mining of exception rules from the data given some general rules. our work is different. we aim to organize and summarize the mining results. we do not report another technique to mine exceptions from the data.  
    proposes ripple-down rules  which are rules with exceptions  for knowledge acquisition in ai. this confirms that general rules and exceptions are intuitive to human experts.  
1. general rules  summaries and exceptions 
we now present the gse representation. in this paper  we are interested in rules generated from a relational table d  which consists of a set n of data tuples described by m distinctive attributes a. rule mining in such a dataset is typically targeted at a specific attribute because the user normally wants to know how the other attributes are related to this target attribute  which can have many values   1  1 . this target attribute is often called the class attribute  and its values  the classes  denoted by c . below  we define the gse pattern in this context.   
definition 1  gse patterns : a gse pattern consists of three components  a single general rule  an if-then rule   a summary and a set of exceptions. it is of the following form:  
 x 뫸 ci   sup  conf   summary  except e1    en  where:  
1. x 뫸 ci is the general rule. x is a set of conditions  and ci 뫍 c is a class. sup and conf are support and confidence of the rule with their usual meanings .  
1. the summary field gives some essential information of the rules below the general rule. a rule r is below another rule r if r only covers a subset of the data tuples that are covered by r. a rule covers a data tuple if the data tuple satisfies the rule s conditions. 
1. e = {e1    en} is the set of exceptions. e may be empty  e =   . each ej is a gse pattern of the form:  
	 	xj 뫸 ck   sup  conf  
	 	summary 
 	 	except   ej1  	  ejw 
where  xj 뫸 ck  is called an exception rule. xj is a set of conditions  and ck 뫍 c and ck 뫛 ci. data tuples that satisfy xj must satisfy x. ejl are the exceptions of  xj 뫸 ck . note that  xj 뫸 ck  now behaves as the general rule of ejl. 
we now discuss the intuitive meanings of the three components of the gse pattern: 
1. the general rule gives the user a general relationship or a big picture of the domain. 
1. summary highlights some key information of the rules covered by the general rule. based on the summary  the user can make choices to explore only those interesting subset of the rules. the summary field is not defined formally. it varies according to implementation needs.  
1. exception rules are unexpected rules with respect to the general rule. they allow the user to see those special cases that do not follow the general rule. an exception rule covers only a subset of the data tuples covered by its general rule  and the class of the exception rule is different from that of its general rule.  
since each exception is also a gse pattern  the gse pattern can naturally represent knowledge in a hierarchical fashion. that is  a general rule can have exceptions  and an exception can also have its own exceptions  and so on. however  the depth  or the number of levels  of the hierarchy should not be too large  otherwise it also becomes difficult to understand. in an actual implementation  the user can decide when the system should stop using gse patterns. this will be explained in section 1  which is also applicable to section 1.  
   we will not give a definition of a general rule  or an exception rule  here because for different data mining tasks it may be defined differently  see section 1 and 1 .  
1. representing decision trees using gse patterns 
decision tree construction  is a popular method for building classification models. in this section  we use gse patterns to organize and summarize the knowledge embedded in a decision tree. the gse representation can also be expressed as a tree  which we call a gse tree.  
   our main objective here is to simplify a decision tree by converting it to a gse tree. the resulting gse tree can still be used for classification to produce the same result as the original decision tree.  
a decision tree has two types of nodes  decision nodes 
 internal node  and leaf nodes. a decision node specifies some test to be carried out on an attribute value. a leaf node indicates a class. a decision tree basically represents a partitioning of the data space. a serial of tests  or cuts  from the root node to a leaf represents a hyper-rectangular region. the leaf node gives the class of the region. for example  the five regions in figure 1 a  are produced by the tree in figure 1 b   ignore the numbers within    for the time being . the tree essentially represents a set of rules. for example  the leaf node 1 in figure 1 b  can be 
expressed with the rule   
 	 	a1   1  a1 뫞 1  a1 뫞 1 뫸 o 
a1 	  	 뫞 1 a1 	  1 
	 	 1 1 a1  1   1.1   
	 	 1 a1  1  1    1 
1.1  	 	뫞 1 1 a1  1   1.1   
	 	1 	1 a1  1  1  
	1 	1 	1 	1  1  	a1 	 	1뫞 1  	1   1 	 
 	 a   	 b  
figure 1. a partitioning of the data space and its decision tree 
   we now discuss how to convert a decision tree to a gse tree. before presenting the detailed algorithm  we first define a general rule or an exception rule in the context of a decision tree.  
definition 1  general or exception rules : a general  or an exception  rule is a significant rule  and its class is the majority class of the data tuples covered by the rule.  the significance of a rule can be measured in various ways. in this work  we measure it using chi-square  뷌1  test/fisher's exact test 1 . the way that we measure the significance of a rule is the same as that in . we will not discuss it further in this paper.  
 we require that the class of a general rule be the majority class because a classification system always outputs the majority class in prediction. in the gse representation  we aim to keep the original spirit of the underlying data mining task.  
   we now describe the conceptual algorithm for converting a decision tree to a gse tree. the algorithm consists of two steps  summarization is done in both steps  see the algorithm later :  
1. find top-level general rules: we descend down the decision tree from the root node to find the nearest nodes whose majority classes can form significant rules. we call these rules the top-level general rules.  
1. find exceptions: we go down from each top-level general rule to recursively find exceptions  exceptions of the exceptions and so on. we determine whether a tree node should form an exception rule or not using the following criteria:  
  significance: an exception rule must be significant  and its class is the majority class and is different from that of its general rule  following definition 1 and 1 .  
  simplicity: if we use a tree node to form an exception rule  it should result in fewer rules in the final gse representation  gse tree . the complexity of a gse representation of a decision tree is measured by the sum of the number of top-level general rules and the number of exception rules  which is also the number of leaves in the gse tree. let the class of the rule r formed by the current node be cj  and the class of its general rule be ci  i 뫛 j . let the number of leaves below this node in the tree for class ci be ni  and for class cj be nj. r will be an exception rule if the following condition is satisfied:  
	 	nj  뫟  ni + 1 	 1  
if this inequality holds  it means that using r as an exception rule with class cj  1 in the formula represents this exception rule  cannot result in a more complex final description of the knowledge. the number of leaves l of the other classes is not included in  1  because l should appear on both sides of  1  and get canceled. ni+1+l  or nj+l  is the total number of exception rules if we use  or do not use  r as an exception rule at this stage.  
   we use the example in figure 1 to illustrate the idea. from figure 1 b   the following gse patterns can be formed  for simplicity  we omit the support and confidence information :  
 	gse-1:  a1 뫞 1 뫸 g 
gse-1: a1   1 뫸 o 
 summary highest conf. of o = 1%  except a1   1  a1 뫞 1 뫸 g   summary highest conf. of g = 1% 
 	 	except  a1   1  a1.1  a1 뫞 1 뫸 o 
   a1 뫞 1 뫸 g and a1   1 뫸 o are top-level general rules  which are formed by node 1 & 1 in figure 1 b  . an exception rule  with its exception  is formed at node 1  according to formula  1   we have 1 뫟 1 + 1 . intuitively  we can also see in figure 1 a  that the area within  a1   1  a1 뫞 1  is a general area for class g. this gives us a 1-level hierarchical representation of knowledge. figure 1 a  shows the corresponding gse tree  which has the complexity of 1 as it has 1 leaves. an internal node with a class  g or o  represents a  general or exception  rule. $ represents the summary at the node. for simplicity  we do not list the content of the summary in the gse tree. those fragmented tree leaves or rules are omitted. in our implementation  these leaves or rules can be accessed by clicking on the summary.  

 	 a . gse-1 and gse-1 	 b . gse-1 and gse-1 
figure 1. two alternative gse tree representations  
   as mentioned in section 1  when the depth of a hierarchy is too deep  the knowledge embedded in it becomes harder to understand. an alternative representation of gse-1 is gse-1   which stops using the gse representation after node 1.  
gse-1': a1   1 뫸 o 
  summary highest conf. of o = 1%  except a1   1  a1 뫞 1  a1 뫞 1 뫸 g 
 	a1   1  a1 뫞 1 뫸 g 
the hierarchy of gse-1 has only two levels. this may be easier to understand. the user can obtain the desired output by specifying the maximum depth of the hierarchy. after this depth  no further split of general rules and exceptions will be performed. figure 1 b  gives the gse tree for gse-1 and gse-1 .  
 the detailed algorithm  called findgse  is given in figure 1. notice that finding general rules and finding exceptions rules are essentially the same because exception rules are general rules for their own exceptions. thus  the two intuitive steps can be performed with a single procedure  i.e.  the findexcepts procedure in figure 1.  
 findgse calls two procedures  countleavesandsummarize  and findexcepts. as discussed above  to decide whether a node below a general rule should form an exception rule or not  we need the number of leaves of each class below the node  formula  1  . the countleavesandsummarize procedure performs this task and also summarizes the tree leaves  or rules  below each internal node  which in our implementation records the highest confidence of the leaves of each class below the node. this procedure is fairly straightforward and is not discussed here. in the case of figure 1 b   countleavesandsummarize produces the numbers of leaves of different classes below each internal node as shown within      in figure 1 b . the highest confidence of the rules of each class below each node is not shown. the first number within    is the number of g class leaves below the node  and the second number is the number of o class leaves.  
 the findexcepts procedure traverses down the tree from a general rule to find its exceptions. ci is the class of the general rule. note that when starting from the root node  the second input parameter is set to 1  which is a special value  not a class in the database  to indicate that there is no general rule at the root node. thus  the first exception found along each path of the tree is a toplevel general rule. in lines 1  if node is a leaf and its class is different from the general rule's class ci  it is reported as an exception. otherwise  the procedure goes down to its children to find exceptions  line 1 . if the conditions in line 1 are satisfied  it means that node child can form an exception rule. ni is the number of leaves of class ci below the node child. the procedure marks this node to form an exception rule and recursively goes down  line 1 and 1 . for example  in figure 1 b   node 1 forms an exception rule because the conditions in line 1 are met  assume the rule is significant . if the conditions are not met  the procedure goes down further  line 1 . for the tree in figure 1 b   we obtain the gse tree in figure 1 a   with two top-level general rules and two exception rules. 
  a gse tree can be constructed very efficiently because the algorithm traverses the decision tree at most twice  once by countleavesandsummarize  and once by findexcepts.  
 	algorithm findgse rootnode    
 	1 	countleavesandsummarize rootnode ;  	1 	findexcepts rootnode  1  
 	findexcepts node  ci   
1 if node is a leaf then  
1 if the class of the leaf is different from ci then  
1 mark it as an exception  
1 else delete the node  /* it is a fragmented leaf or rule */  	1 	else for each child node in node.children do 
1 cj = majority class of node child;  
1 if cj 뫛 ci  and  the rule formed from root node to child with the class cj is significant and     nj 뫟  ni + 1 then 	/* n1 = 1 for root node */ 
1 mark it as an exception rule; 
1 findexcepts child  cj  1 else  findexcepts child  ci  
 	1 	end 
figure 1: finding general rules  summaries and exceptions 
1. organizing association rules using gse patterns 
we now discuss how to represent association rules using gse patterns. since association rule mining is not aimed at producing a classification model but to find all significant rules  we need to apply the proposed technique differently. these will become clear later. the nature of association rule mining also presents some special problems that make it hard to find real exceptions because of the minimum support and rule overlapping  the same data tuple may be covered by many rules . in this section  we first present the basic framework of using gse patterns to organize association rules. we then present and deal with the special problems.  
1  association rule mining 
the association rule mining model is stated as follows : let i = 
{i1    ir} be a set of items  and d be a set of data cases  transactions . each data case consists of a subset of items in i. an association rule is an implication of the form x 뫸 y  where x   i  y   i  and x 뫌 y =  . the rule x 뫸 y holds in d with confidence c if c% of data cases in d that support x also support y. the rule has support s in d if s% of the data cases in d contains x 뫋 y. the problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum support and minimum confidence.   in this work  we focus on association rule mining from a relational table  which is described by a number of attributes. an item is an attribute value pair  i.e.   attribute = value   numeric attributes are discretized . we also have a target or class attribute  which can have a number of values c  or classes . with the class attribute  we only mine rules of the form:  
       x 뫸 ci where ci 뫍 c is a class  and x is a set of items from the rest of the attributes. we say a rule is large if it meets the minimum support. we will not discuss the algorithm for mining such rules as the existing algorithm in  can be easily modified for this purpose  see also  .  
rule pruning: it is well known that many discovered associations are redundant or minor variations of others. their existence may simply be due to chance rather than true correlation. those insignificant rules should be removed.  studies the pruning of association rules. its basic idea can be shown with the following two rules from a loan application data.  
r: job = yes 뫸 loan = approved  sup = 1%  conf = 1%  r: job = yes  credit history = good 뫸 loan = approved  
	 	 	  sup = 1%  conf = 1%  
if we know r  then r is of limited use because it gives little extra information. its slightly higher confidence is more likely due to chance than to true correlation.  
   we say r can be pruned with respect to r because within the subset of data tuples covered by r  r is not significant. the significance  or positive correlation  of a rule is measured by chisquare test  뷌1  for correlation from statistics . the pruning is done as follows:  
  given a rule r  we try to prune r using each ancestor rule r  which has the same consequent as r but fewer conditions  of r. in the case that r  e.g.  x 뫸 ci   has only one conditions  its ancestor rule is 뫸 ci. that is  we perform a 뷌1 test on r with respect to r. if the test shows a positive correlation  it is kept. otherwise  r is pruned. the reason for the pruning is because within the data covered by r  r is not significant. see  for more details about pruning. note that we do not use minimum confidence in our framework. minimum confidence  a user-specified threshold  does not reflect the underlying relationships of the domain  1  1 . instead we use statistical significance as the basis for finding rules that represent the fundamental relations of the domain.  
1  representing association rules with 
gse patterns 
since association rule mining aims to find all rules in data  it often generates a huge number of rules. even after pruning  the number of rules left can still be very large  in the hundred or thousands  see section 1 . clearly  such a large number of rules are very difficult  if not impossible  to be analyzed by a human user.  
   we now discuss how to use gse patterns to organize and present the discovered associations so that the user can browse through them and focus on the interesting subset of the rules. the key issue still is how to determine whether a rule should form an exception rule. in the case of a decision tree  we make the decision based on two criteria  significance  statistically significant and majority class  and simplicity. statistical significance still applies  after pruning every association rule is statistically significant . however  we do not require that an exception rule s class be the majority class. since the aim of association rule mining is to find all significant rules  thus we cannot require each rule to use the majority class.   the simplicity criterion does not apply here either because if we find a significant rule and we do not use it as an exception rule  then the rule is lost. this is undesirable. in the case of a decision tree  the original rules for classification are formed only at the leaf level. thus  if an internal node does not form an exception rule  we do not lose any information.  
   hence  for association rules  we only use statistical significance to determine whether a rule should form an exception  of course  its class must be different from its general rule . since every rule is significant after pruning  we do not need any more significance test.  
   to convert association rules to gse patterns  we also need to use a different technique. it is not time and space efficient to build the whole structure like the gse tree for association rules. the reason is that this structure could be too large due to duplicate rules appearing in different parts of the structure  i.e.  the same rule may appear in many places as exceptions.  
   for example  we have two general rules   a1 = a 뫸 c1   and  a1 = b 뫸 c1 . assume that we also have the rule   a1 = a  a1 = b 뫸 c1 . then  this rule will appear as an exception rule of a1 = a 뫸 c1 and as an exception rule of a1 = b 뫸 c1. in the case of a decision tree  no rule will be an exception rule for more than one general rule because rules in a decision tree are disjoint. this is not the case for association rules as the above example shows.  
   a question that one may ask is why cannot we consider each rule as an exception only once  we could not do this because the new technique aims to allow the user to browse and explore the rules effectively. if we have 1 rules  obviously the user is unlikely to see all of them. gse patterns organize the rules in an intuitive way so that the user can focus his/her attention on only a subset of interesting rules. however  we will not know where the user will choose to focus on beforehand. thus  we are unable to decide whether the rule  a1 = a  a1 = b 뫸 c1  should appear as an exception of  a1 = a 뫸 c1  or  a1 = b 뫸 c1 . 
   in this work  we adopt a lazy approach  i.e.  we only compute the exceptions of a general rule when the user requests it. this approach is reasonable because of two reasons:  
1. the computation can be done very efficiently without the user even noticing it  see the experiment results in section 1 ;  
1. even if we display all the information  the user will not be able to view all of it.   
 while finding the exception rules of a general rule  we also summarize all the rules below each exception rule. this summary highlights the main characteristics of the rules below the exception rule. from the summary  the user can decide whether to go down further to see more details. if he/she decides to do so  he/she can click and the system will compute the exceptions of this exception rule  which now behaves as a general rule .   the summary field contains the following information:  
  the coverage of each class: this shows how big a population that each class covers. if a class covers too few data tuples  it may not be interesting. 
  the highest confidence of the rules in each class: this gives the highest confidence of the rules below the exception rule. it indicates the quality of the subsequent rules.  
  the number of rules in each class: this allows the user to decide whether to explore further using gse representation or simply see the rules if the number of rules is small.    the set of characteristics can be further expanded when the need arises. however  in our applications  we found that these together with the exception rules are sufficient for the user to decide where to focus on.  
   before presenting the algorithms for converting association rules to gse patterns  we first show how the rules are stored in memory. in essence  they are represented as a tree  called a rule tree. a rule tree is different from a decision tree because every node in the rule tree can be attached with a class distribution list representing some rules  and the branches of the tree at each level do not partition the data. each tree node contains three pieces of information:  
1. an item  which is represented by an integer number. 
1. a support count  which is the number of data tuples that fall into this node.  
1. a class distribution list  which stores the number of data tuples falling into each class of this node. if the rule formed with the items from the root to the current node with a class is not a significant rule  we indicate it with a 1 .  
an example rule tree is given in figure 1  which uses 1 items  1  1  1  1 . the class distribution list and the support counts are not shown. note that the items  represented in the tree  are in an ascending order either horizontally or vertically. this is inherited from association rule mining. this ordering greatly facilitates our computation later. 

figure 1. an example rule tree  no rule formed with item 1  
   we now present the overall algorithm  findgse  figure 1   to find all the exception rules of a general rule. the general rule is represented by the root node rootnode of a rule tree and the class grclass of the general rule. we will see later that a tree is constructed dynamically for each general rule  which is used to find its exceptions. the tree for a general rule contains only those rules  called superset rules below  that are below the general rule. the top-level general rules can also be seen as exception rules that do not have a general rule above them.  
 	algorithm findgse rootnode  grclass   	1 exceptionr = {}; 
모1 finderules rootnode  grclass ; 
모1 for each er 뫍 exceptionr do 
모1 supersetr = {}; 
모1 findsupersetr er.cond  er.class  rootnode ; 
모1 summarize supersetr ;  
모1 buildtree cond er   supersetr   1 endfor 
figure 1. the overall findgse algorithm 
   line 1 initializes a variable exceptionr to store all the exception rules of a general rule. the finderules procedure  line 
1  finds all the exception rules starting from rootnode. from line 1  for each exception rule er  we find all the rules below it  i.e.  those rules whose conditions are a superset of the conditions of er  line 1 . we call them the superset rules of er. er.cond gives the set of conditions of er  and er.class gives the class of er. line 1 initializes a variable to store all the superset rules of er. line 1 summarizes the superset rules of each class. this procedure is simple to design  and thus will not be discussed further. in line 1  we build a new tree using all rules in supersetr with all the conditions in er removed from each rule. this tree will be used to find exceptions below er  er will behave as a general rule . that is why the algorithm findgse always starts from the root node of a tree. the buildtree procedure is omitted  as it is straightforward.  
   the finderules procedure  figure 1  finds all exception rules from a root node representing a general rule with its class grclass. for finding top-level general rules  the same procedure is used. in this case  grclass is a special value  we use 1  that is not a class in the data.  
   in line 1  finderules descends down the tree to find exception rules from each child node. if an exception rule is found from a child  line 1   it is recorded  see geter procedure in figure 1 . otherwise  the procedure traverses further down to find exception rules. the procedure  geter  for finding exception rules within a node is given in figure 1.  
 	finderules rootnode  grclass   	1 	if rootnode.children 뫛 nil then  
 	1 	for each child in rootnode.children do 1 	if not geter child  grclass   then  
	1 	finderules child  grclass ; 
figure 1.  finding exception  or general  rules 
   the geter procedure  figure 1  uses the class distribution list of the node  in node.classdistr  to see whether any rules are formed here. if c = 1  it means that this class does not form a rule. if c 뫛 1 and c is not the same as grclass  an exception rule is found  line 1 . the variable found indicates whether any exception rules are found.  
 	geter node  grclass  
1 found = false 
1 for each c in node.classdistr do 
1 if c 뫛 1 then  
1 if c 뫛 grclass then  /* we have found an exception rule */ 
1 insert the exception rule to exceptionr;  
1 found = true   
1 endif  1 endif 
1 endfor 
1 return found  
figure 1. finding exception or general rules from a node 
figure 1 gives the findsupersetr procedure that finds all superset rules of an exception rule. these rules will be summarized to the user. assume the set of conditions of an exception rule is s = {s1    sk}  which is a set of numbers sorted in the ascending order. findsupersetr starts off from the root node of the current tree and traverses down every possible path of the tree to find all superset rules. note that we cannot simply traverse down the tree from the exception rule to find its superset rules because some of its superset rules may appear in other parts of the tree as well. 
1 job = yes 뫸 loan = approved 	  sup = 1%; conf = 1%  
1 summary 	approved:  	coverage = 1%  highest confidence = 1%  no of rules = 1; 
1 not approved:  coverage = 1%  highest confidence = 1%  no of rules = 1; 
1 except ... 
1 own house = yes 뫸 loan =  approved  sup = 1%; confidence = 1%  1 summary  approved:  coverage = 1%  highest confidence = 1%  no. of rules = 1; 1 credit history = bad 뫸 loan = not approved    sup = 1%; confidence = 1%  1 summary  approved:  coverage = 1%  highest confidence = 1%  no. of rules = 1;  
1 not approved:  coverage = 1%  highest confidence = 1%  no. of rules = 1; 
1 except ... 
1 job = no  saving = no 뫸 loan = not approved 	 sup = 1%; confidence = 1%  
1 summary  approved:  	coverage = 1%  highest confidence = 1%  no. of rules = 1;  
1 not approved:  coverage = 1%  highest confidence = 1%  no. of rules = 1; 
1 except ... 
figure 1. an example gse representation of association rules    figure 1 gives an example organization of a set of rules  which shows how general rules  summaries and exceptions are organized and presented. the rules are generated from a loan application data. the classes in the data are approved and not approved  indicating whether a loan is approved or not. lines 1  1  1 and 1 show 1 general rules. lines 1  line 1  lines 1  and lines 1 summarize the rules below the 1 general rules respectively. except  indicates that there are exceptions  which are not shown. the user needs to click on it to see the exception rules  which calls the findgse procedure. the user can also click on the class approved or not approved in the summary field to see all rules of the class below each general rule. 
findsupersetr s  node  
if node.children 뫛 nil then 
 	for each child in node.children do  	findfirst hd s   tail s   child  	 
/* hd s  gives the first element of s */ 
	 	endfor 	/* tail s  gives the rest of the elements of s */ 
	 	endif 
 findfirst first  rest  node  
 	if first 뫞 node.id then   /* node.id is the item number */ 
 	if first = node.id then  	 	 	 
 	if rest =   then getrules node   	else findsupersetr rest  node   	else findsupersetr {first} 뫋 rest  node  
 	endif 
 	getrules node   	for each c 뫍 node.classdistr do  	if c 뫛 1 then insert the rule into supersetr endif  
/* a superset rule is found */ 
 	endfor  	godown node  
 	godown node   	if node.children 뫛 nil then   	for each child of node.children do  
getrules child    
endfor 
endif 
figure 1. finding all superset rules 
1 	some special problems 
we now discuss three problems in using the gse representation to organize and summarize association rules. the solution to these problems is also presented.  
1. due to the minimum support constraint some exceptions for a general rule may not be discovered by the mining algorithm. this causes problems in understanding the domain because it may mislead the user to believe that no exception exists.  
1. due to pruning in rule generation  some exceptions may be pruned but their existence is important to the understanding of 
the domain. for example  we have the following two rules:  
 	r1: a1=a  a1=b 뫸 c1  	 sup = 1%  conf = 1%  
 r1: a1=a  a1=b  a1=e 뫸 c1   sup = 1%  conf = 1%  if r1 exists  r1 is pruned because r1 is insignificant with respect to r1. this pruning is clearly reasonable. otherwise  we will end up with a huge number of insignificant rules. however  this pruning may cause problems for the user in understanding the domain  in particular  the aspect of the domain related to a1=a  a1=b  a1=e. for instance  if the user is inspecting the following rule: 
 	 r1: a1=a  a1=e 뫸 c1  	 sup = 1%  conf = 1%  
no exception to r1 is found by the system. this is misleading because r1 is an obvious exception. r1 is not reported as an exception because it has been pruned. this is undesirable. one may argue that r1 should indicate to the user that r1 has exceptions  but the user may not know r1 when he/she is drilling down to r1.  
1. due to overlapping of rules  it is harder to find good exceptions. overlapping of rules means that the same data tuples may be covered by many rules. the user typically likes to find disjoint rules that cover the exception tuples. note that rules in a decision tree are disjoint.  
we deal with these problems using two methods.  
lower down the minimum support: when the user is interested in a general rule and its direct exceptions  he/she can request the system to extract the data tuples that satisfy the conditions of the general rule  and to use this subset of data to run the association rule miner again with a lower minimum support. the output rules are organized and presented to the user in the same way as described above. this method is effective in finding good exceptions if the items involved is not too many so that a very low minimum support can be used  otherwise it can still cause combinatorial explosion. the method does not help to solve the third problem.   
integrate with a decision tree algorithm: extract the data tuples that satisfy the conditions of the general rule  and use this subset of data to run a decision tree system. the output rules  or the tree  are presented to the user in the same way as described in section 1. this method is not constrained by the minimum support. the output rules are also disjoint. the disadvantage is that it does not find all possible rules.   
1. 	empirical evaluation  
we now evaluate the effectiveness and efficiency of the proposed technique. we first discuss the experiment results with decision trees  and then the experiment results with association rules. finally  we discuss our application experiences.   
1  experiments with decision trees 
we applied the proposed technique to the decision trees produced by c1  using 1 datasets in the uci machine learning repository . our objective here is to see how much the gse representation is able to simplify the decision tree. simplification  however  is not the only objective of the proposed technique. another important aspect is the organization of the discovered knowledge. unfortunately  organization is hard to quantify. we use our application experiences to show that users like the proposed technique due to its intuitive representation.    
 table 1 shows the average results over the 1 datasets. see  for the detailed results. the first column gives the average number of leaves in the decision trees produced by c1  after pruning  for the 1 datasets. the second column gives the average number of leaves in the gse trees for the 1 datasets.  
 	 
 	table 1: experiment results with decision trees 
no. of decision tree leaves no. of gse tree leaves  	1 1  	 
 from table 1  we can see that the number of leaves  or rules  in the gse tree is substantially smaller. on average  over the 1 datasets  the number of leaves in the gse tree is only 1% of that of the original decision tree. this shows that gse trees significantly simplify decision trees. the execution times are not reported as the algorithm runs so fast that they cannot be logged.  in the experiments  chi-square test at the significance level of 1% is used to measure the significance of a rule  this is a commonly used level  . when an expected frequency is below 1 for chi-square test  fisher's exact test is employed instead.  
1  experiments with association rules 
for the experiments with association rules  it is not appropriate to use the percentage of reduction to assess the effectiveness because there are duplicate exception rules in many places and our implementation is based on a lazy approach. the system only finds exception rules of a general rule when requested by the user. in this situation  efficiency is important because we cannot let the user wait for too long to obtain the results. we show the execution time taken to compute the exceptions of a general rule  and to find and summarize all their superset rules. the datasets used here are the same as those used in the experiments with decision trees. the experiments are done on pentium-ii 1 with 1mb of memory. 
   we use the system in  to generate association rules and to prune those insignificant rules  without generating the ds rules . the minimum support is set to 1% as it is shown in  that for these datasets  rules with this support threshold are sufficiently predictive. pruning of association rules also uses the 뷌1 value at significance level of 1%. many datasets we use contain numeric attributes. since association rule mining does not handle numeric attributes. we discretize these attributes into intervals using the target  class  attribute. we use the entropy-based method in . the code is taken from mlc++ .  
table 1 shows the execution time taken to find all top-level general rules and their superset rules  including summarization  for each dataset. we only use the top-level general rules to give an indication of efficiency because the number of top-level general rules is normally larger than the number of exceptions of a general rule below them. they also tend to have more superset rules as the number of conditions in a top-level general rule is small.  
   in table 1  column 1 gives the name of each dataset. column 1 gives the number of rules found after pruning for each dataset. although the number of rules left can still be quite large  it does not present a big problem any more as we will see in section 1. column 1 shows the number of top-level general rules found for each dataset. column 1 gives the execution time. we can see that the execution is very fast. the user can hardly notice that the computation is done on the fly.  
table 1: execution time for finding top-level general rules  and summarizing their superset rules 
 
 	datasetno. of rules after pruningno.of top 
level general rulesexecution time  sec.  	1adult11 	1anneal11 	1austra11 	1auto11 	1breast11 	1chess11 	1cleve11 	1crx11 	1diabetes111german11 1glass11 1heart11 1iono11 1kdd11 1mushroom11 1pima11 1satimage11 1splice11 1tic-tac11 1waveform11average11.1.1 	applications 
we built two systems in this work  one for converting a decision tree to a gse tree  we call this system gsetree  and one for organizing  summarizing and presenting association rules  we call this system gserule . gsetree is also integrated into gserule  i.e.  gserule can call gsetree when needed  see section 1 . the two systems have been used in a number of applications for two educational institutions  and a medical center.  
 the gsetree system was used in a number of classification tasks. for example  in a medical application  we needed to build a classifier to predict whether a person has a particular kind of disease. we used the c1 system  which produced a decision tree with 1 leaves. our gse tree had only 1 leaves or rules. for example  a general rule says that if the patient's age is less than 1  the chance of having the disease is small with 1 exceptions. however  the corresponding sub-decision tree has 1 leaves  or rules . another general rule says that if the patient's age is greater than 1  the chance of having the disease is very high with only one exception. however  c1 produced 1 fragmented rules. the doctors said that they could not obtain an overall picture of the domain from the fragmented decision tree leaves  or rules . the gse tree gives them exactly the types of knowledge they want. in the education application  the situation is similar. our users agree that the gse representation is simple and more intuitive.  
 the gserule system was used in all our applications. the users find it more effective than our existing systems. before this work  we had two previous systems. the first system is based on user expectations or beliefs . in this system  the user first inputs some existing knowledge  and the system then finds those conforming and unexpected rules. the main problem with this system is that the users found it difficult to give existing domain knowledge. very quickly they gave up and asked us to do it for them. the second system is the ds rule system . ds rules are the essential rules in the domain. since the number of ds rules is not very large in these applications  less than one hundred  they can be browsed manually. however  it is still hard to gain a good overall view of the domain using the ds rule system since the ds rules are just a list of rules with no organization. the expectation system and the ds rule system are also hard to understand. it often takes many sessions for a student to explain to our users what these systems do. the gserule system  on the other hand  is much easier to understand and to use because it is very intuitive. in fact  our users had inspired us to design the new representation. with the gse representation and the gserule system  browsing through the rules is no longer so difficult and boring as the exception  unexpected  rules always draw the users attention.   the new systems also provide a convenient tool for us  as we need to identify interesting rules for our users in some applications. our users could only explain to us the types of rules that may be interesting to them. using the gserule system  we are able to identify interesting rules easily. we can also present the rules to our users in the way  using the gse representation  that they can easily understand and appreciate.  
 in all our applications  we typically have 1 association rules after pruning. the large number of rules does not present a major problem as the users always have some tasks in mind and thus are only interested in certain aspects of the discovered rules. the gse representation makes it convenient to focus on these interesting aspects and to obtain a good overall picture of them. it is also quite interesting to note that even those less interesting aspects are often explored because exceptions  unexpected rules  can be quite tempting  i.e.  people are always curious about unexpected things.  
1. 	conclusion 
this paper proposed a novel technique to organize  summarize and present the discovered rules. the technique is based on the representation and organization of knowledge using general rules  summaries and exceptions  i.e.  gse patterns. the gse pattern is very intuitive as it is closely related to the way that we think and talk about knowledge in our daily lives. it allows the user to see an overall picture of the domain first and then the special cases. it manages the complexity of the discovered rules in a hierarchical fashion  which enables the user to focus his/her attention on the interesting aspects of the rules. in this paper  we applied the representation to organize and summarize the knowledge embedded in a decision tree and a set of association rules. our practical applications show that the representation is simple and more intuitive than a list of individual rules.  
acknowledgement: we thank shuik-ming lee  shanta c 
emmanuel  paul goh  jonathan phang  hing-yan lee and kinghee ho  for providing us the data and giving us feedbacks. the project is funded by national science and technology board  and national university of singapore under rp1.  
