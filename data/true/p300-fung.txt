in this paper  an algorithm called time driven documents-partition  tdd  is proposed to construct an event hierarchy in a text corpus based on a given query. specifically  assume that a query contains only one feature - election. election is directly related to the events such as 1 us midterm elections campaign  1 us presidential election campaign and 1 taiwan presidential election campaign  where these events may further be divided into several smaller events  e.g. the 1 us midterm elections campaign can be broken down into events such as campaign for vote  election results and the resignation of donald h. rumsfeld . as such  an event hierarchy is resulted. our proposed algorithm  tdd  tackles the problem by three major steps:  1  identify the features that are related to the query according to both the timestamps and the contents of the documents. the features identified are regarded as bursty features;  1  extract the documents that are highly related to the bursty features based on time;  1  partition the extracted documents to form events and organize them in a hierarchical structure. to the best of our knowledge  there is little works targeting for constructing a feature-based event hierarchy for a text corpus. practically  event hierarchies can assist us to efficiently locate our target information in a text corpus easily. again  assume that election is used for a query. without an event hierarchy  it is very difficult to identify what are the major events related to it  when do these events happened  as well as the features and the news articles that are related to each of these events. we have archived two-year news articles to evaluate the feasibility of tdd. the encouraging results indicated that tdd is practically sound and highly effective.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-clustering; h.1  information interfaces and presentation : miscellaneous

 the work was supported by a grant of rgc  hong kong sar  china  no. 1 .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  a ugus t 1  1  san jos e  c al i f or ni a  u sa .
copyright 1 acm 1-1-1/1 ...$1.
general terms
algorithms  design  management
keywords
hierarchies  events  time  clustering  text  retrieval  presentation
1. introduction
　in this information overwhelming century  information becomes ever more pervasive and important. while there are some excellent search engines  like google  to assist us to retrieve our target information by simply providing some keywords  the problem of too much information surrounding us makes it harder and harder to locate the right piece of information efficiently. as an example  consider we want to identify what had happened when the virus sars  severe acute respiratory syndrome  broke out in hong kong. by specifying the word sars as the search query  we may obtain a result that is similar to the one as shown in figure 1 a  through some existing news search engines such as factiva1. in the figure  it shows the headlines  and the first few lines of the news articles if their contents contain the word sars.
　yet  the search result in figure 1 a  does not capture any of the  time-dependent information   meanwhile this information is critical in terms of assisting us to efficiently locate our target. for example  it is very difficult to identify from figure 1 a  that what are the major events related to the keyword sars  e.g. the closure of school  the travel warnings issued by who  the resignation of the hong kong secretary of health  etc   the periods when these events happened  as well as the keywords and the news articles that are related to each of these events. on the other hand  the time related information is often associated with the documents  such as the time when the web page is being updated  the time when a news article is being released or the time when a blog is being written. it is desirable to group the documents in a corpus according to both their contents and their timestamps.
　this urges us to think critically whether we can extend the capability of the existing search engines to incorporate more information  such as including the time dimension. specifically  we are curious of whether it is possible to have an algorithm that is able to group the retrieved documents into events according to the similarity of their contents and their timestamps  so as to construct an event hierarchy according to a particular user's query.
　let us continue with our previous example about sars. the questions that we have raised before can all be answered efficiently by constructing an event hierarchy which is similar to the one shown


headlines 1 - 1 of 1 |  next 1
1. lack of precautions in schools criticised. 
south china morning post  1 march 1  1 words   english 
educators and parents yesterday criticised the government for not providing schools with concrete precautionary guidelines to avoid a spread of the atypical pneumonia... more like this
1. outbreak of virus in hanoi traced to hk.
south china morning post  1 march 1  1 words   english 
a businessman who spread pneumonia to vietnam was on the same floor of the metropole hotel as the index patient. the 1-year-old american-chinese ...  more like this
1. the metropole connection. 
south china morning post  1 march 1  1 words   english 
1 people are in hospital with sars  1 with confirmed atypical pneumonia. there have been six deaths  including one reported yesterday... more like this
1. education chief says schools may close. 
south china morning post  1 march 1  1 words   english 
as a fifth student falls ill  arthur li considers drastic options if the pneumonia outbreak worsens schools may be forced to close if the outbreak of atypical pneumonia... more like this
1. prospect of school closures fuels fears. 
south china morning post  1 march 1  1 words   english 
parents and teachers are worried by the education chief's warning as the virus outbreak shows no signs of abating parents and teachers have reacted with... more like this
1. outbreak began in guangdong  who believes. 
south china morning post  1 march 1  1 words   english 
the central government is urged to provide more information about pneumonia infections on the mainland. the central government was yesterday urged to disclose... more like this
1. scientists link `tricky virus' to pneumonia outbreak. 
south china morning post  1 march 1  1 words   english 
scientists from hong kong university said last night they had identified a  tricky new virus  they believe is behind an outbreak of a mystery pneumonia which... more like this
.
.
.	 a 	 b 
figure 1: the result of searching. in  a   it shows the search results of a traditional search engine against the feature sars. in  b   itorganized the search results in  a  by using an event hierarchy.
in figure 1 b . in the figure  the upper block diagram represents when the events happened. the x-axis is the time. the blocks in the diagram represent events. all events in the diagram are related to the keyword sars. they are arranged in a hierarchical structure  where the links denote their relationships. for instance  event a and event b are at the same level  where event a and event b respectively contain event c and event d. there is no direct relationship between event c and event d as they are not connected with each other. event c is further broken down into event e and event f  whereas the later two events are connected. similar description applies for the other events. the lower half of figure 1 b  is similar to figure 1 a  except that it only shows the information of the news articles that are related to a particular event in the upper block diagram  event j for this figure . as such  we can mitigate the problem of information overloading by displaying the information related to a particular event only. it is worth to note that we have preserved all information as we can easily switch from one event to the others. last but not least  in figure 1 b   every event is associated with a set of keywords. for instance  event j is associated with the keywords such as school  college and university. practically  it is very useful to associate a set of keywords with each event because we can identify their relationships efficiently.
　let us present one more example to account for our motivation. for the keyword  election  it is related to the events such as the 1 us midterm elections campaign  the 1 us presidential election campaign and the 1 taiwan presidential election campaign. usually  an event may be further broken down into several sub-events. in this example  the 1 us midterm elections campaign may be broken down into events such as campaign for vote  election results and the resignation of donald h. rumsfeld  the former us secretary of defense. furthermore  an event is usually associated with more than one feature. for instance  the 1 us presidential election campaign is further associated with the features such as president  campaign  bush  senator and kerry. eventually  an event hierarchy for the feature election  such as the one in figure 1  could be formulated. as a result  we will have a much clearer picture about the relationships among the features.
　formally speaking  given a user query  we propose an algorithm to construct an event hierarchy by grouping the retrieved documents into events according to their timestamps and their contents. a search query is defined as a set of keywords  in which we call them features. an event is defined as an object which consists of the following three components:  1  a set of documents with similar contents;  1  a set of representative features extracted from the documents; and  1  two timestamps to denote for the event begins and ends.
　conceptually  given a query  our problem can be readily solved by the following straightforward framework:  1  identify all of the documents that are related to the query;  1  group the highly related documents together to form events;  1  arrange the events in an hierarchical structure;  1  for each event  extract a set of features that can represent it. intuitively  this framework works well  and can be solved by combining some of the existing techniques  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 . unfortunately  we claim that this framework is ineffective to solve our problem. the reason is because of the imprecise nature of the search results. the search results returned in step  1  of the straightforward framework are usually contain a sizable number of unrelated documents. these unrelated documents are hard to clean up  making it difficult to group the documents into events so as to construct an appropriate event hierarchy and identify a set of representative features for each event. we will discuss the details in section 1.
　in contrast to the above straightforward approach  we proposed an algorithm called time driven documents-partition  tdd  to solve our problem. given a query  tdd will:  1  identify the features that are related to the query according to the timestamps and contents of the documents in the corpus. these features are regarded as bursty features;  1  extract the documents that are highly related to the bursty features based on time;  1  partition the extracted documents and organize them to construct an event hierarchy. we will show that bursty features are less prune to noise. so our approach starts with discovery the bursty features. we will discuss the details in section 1.
　we have conducted extensive experiments to evaluate tdd's effectiveness by using two-year news articles. we chose news articles because they are indexed by timestamps and their contents have strong temporal structure. it is easy to evaluate whether tdd is feasible. nevertheless  tdd can be extended to other areas  such as grouping the retrieved web pages according to their latest modification dates. this will be left for our future work. the rest of the paper is organized as follows - section 1 presents tdd; section 1 evaluates the effectiveness of tdd; section 1 briefly discusses some of the major related work; section 1 summarizes and concludes this paper.
1. proposed work
　table 1 summarizes the notations that would be used throughout this paper. let d ={d1 d1 ...} be a text corpus  where di is a document with the timestamp ti. following the existing direction  1  1  1   d  is partitioned into l consecutive and non-overlapping windows according to the timestamps of the documents. let w be a set containing all the windows. let f be a set containing all different features in d. a feature  f （ f  is a word in the text corpus. |f| is the number of different features in d.
　the input of our algorithm is a query  q  where q   f. the output is an event hierarchy  which is similar to figure 1 . we will first describe how we formulate the problem and why it is formulated in such a way by presenting some real life examples in section 1  then we will present the implementation details in the sections thereafter.
1 an overview
given a user query  q  in order to construct an event hierar-
sym.	descriptiond	a text corpus
d a set of documents  d   d di a document  di （ d
ti the timestamp of di w a set of windows w a window  w （w
l	number of windows in the text corpus
f a set of all different features in the text corpus f a feature  f （ f
nfw	number of documents contains f in w
nw	the feature count of w
q	a user query  q   f b	a set of bursty periods for q b	a bursty period  b （ b
fb	a set of features associated to b  fb   f
db	a set of documents in b that are related to q  db   d e	a set of events e	an event  e	e（
table 1: notation used in this paper.
chy  the first step arguably is to identify all of the events that are related to q. a possible approach is that we first retrieve all of the documents that are related to q  and then cluster the documents into groups so as to formulate events. we argue that this approach is inappropriate. let us consider for the following situation. there was a news article released from the south china morning post  scmp 1 with the headline  two chiefs  two systems of getting the job done  contained the feature sars. inter-
estingly  this article has nothing to do with the virus sars  severe
　　　　　　　　　　　　　　　　　　　　　　　　． acute respiratory syndrome. in fact  this news article was release
．	．	．
on 1/1  a date well before the virus sars outbreak  and was related to the appraisals of the two chief executives in the two special administrative regions  hong kong and macau  in china. ．from our experiences  when we issue the query． ． ． q = {sars}  we are always targeting for retrieving the information that is related to the virus  severe acute respiratory syndrome  but not the place  special administrative regions . query the information for the later issue will usually involve some other features  such as hong  kong or macau. even though a document  d （ d  matches the user query  q  the user may not be interested in it. accordingly  if we follow the approach of retrieving the related documents first and

then identify the events by clustering  we may need to have some other heuristics for filtering the documents that match q but do not coincide with the users' interest. this is difficult. similarly  even though a document which does not match q  the user may sometimes be interested in it  we will explain this later . in this paper  we claim that we should first identify the events  e  that satisfy q  and then map the related documents back to the events. it is a reverse of the just mentioned ineffective approach. yet  if we follow this new direction  a question immediately ensues: how to identify the events that are related to q in the text corpus without retrieving any of the documents 
　from our observations  the emergence of an event is always associated with a burst of features  where some features suddenly appear frequently when the event emerges  and their frequencies drop when the event fades away. for instance  from 1/1 to 1/1  1 days   the feature tsunami only appeared 1 times in 1 news articles in scmp1. yet  from 1/1 to 1/1  1 days   tsunami appeared 1 times in 1 news articles. both the number of news articles and the feature frequency increased dramatically in only 1 days. history tells us that there was an undersea earthquake that occurred at 1:1 utc on 1/1 with an epicenter off the west coast of sumatra  indonesia. the earthquake triggered a series of devastating tsunami that spread throughout the indian ocean killing large numbers of people and inundating coastal communities across south and southeast asia.1this event is usually regarded as the asian tsunami. its emergence can be identified by observing the distribution of tsunami across the timeline. hence  by monitoring the feature distributions in d  we can identify whether there is any event occurred. if the feature  f （ q  suddenly appears frequently  we can then conclude that some events related to q emerge. here  another question immediately arises: how to define the phrase  suddenly appears frequently   we will provide the implementation details in section
1.
　without loss of generality  assume that we know all of the periods where q suddenly appears frequently. let b be a set of such periods and db be a set of documents which satisfies q and resides in b （ b. intuitively  the next process is to construct an event hierarchy  e.g. figure 1  by recursively dividing each db based on some partitioning algorithms. however  we claim that this process should not be conducted at this moment. let us consider for the following situation. assume that q = {iraq}. there was an article release from scmp1 on 1/1 titled  a letter to saddam: be more like me . obviously  even just judging from the title  this article is related to iraq. but  in the whole article  the feature iraq is not found. some documents may be highly related to q  but their contents do not match q. so  before formulating the hierarchical structure  we should extract all of the documents that are highly related to each db. our problem now is: given a set of documents  db  extract all of the documents that have similar contents.
　a simple but ineffective method is to compare the documents in b with db one by one: for each document in b  see whether it is similar to any of the document in db. unfortunately  since the feature distributions are sparse in the text domain  two documents with similar features may not necessarily belong to the same event  1  1 . as the result obtained by comparing two sets of documents will usually be more reliable than comparing in a document basis   we should select a proper subset of documents in b and compare it with db. now  another question arises: how to select a proper subset of documents in b 
at the first glimpse  this question can be answered by the tech-


	 a 	 b 
figure 1: two binomial distributions.
niques used in query refinement  1  1  1 : find all of the features that are highly associated with q  and then for those documents belong to these features  try to align them to db   b （ b. unfortunately  the existing techniques merely rely on the co-occurrence of features in the text domain  and cannot include the time dimension. yet  time is important in solving our problem. for instance  the feature virus is highly related to the feature bird  bird flu - h1 - a kind of virus  for some bursty periods. not all of the bursty periods of virus are coincident with bird. it is unlikely that two features have high association for all of their bursty periods.
　in this paper  we try to solve the above problem with the help of the bursty patterns of the other features in d. firstly  we identify the bursty patterns of all features  f  in d. then  for each f （f  we determine whether any of the bursty periods of f is similar to any of the b （ b. finally  we compare the similarity between db and the documents that belongs to f by using two novel coefficients: intra-document similarity and inter-document similarity. their formulation is motivated by a newly defined concept called  map-to . details will be discussed in section 1.
　thus  we can obtain a set of documents  db  that are related to q and are released within the period b （ b. as discussed previously  each db may further be divided into several events. in this paper  we identify these sub-events by implementing the bisecting k-means clustering algorithm . details are discussed in section 1.
1 identify the bursty periods of the features
　let f （ q. in order to determine which of the windows the feature f  suddenly appears frequently   we try to compute the  probability of bursty  for each window w （ w. let p w  f; pe  be the probability that the feature f is bursty in window w according to pe  where pe is the probability that f would appear in a time window given that it is not bursty. the intuition is that if the frequency of f appearing in w is significantly higher that the expected probability pe  then w is regarded as a bursty period. we compute p w  f  according to the cumulative distribution function of the binomial distribution :
nfw
	p w  f; pe  = ‘ p k;nw  pe  	 1 
k=1
	p k;nw  pe  =k 	 1 
where nw is the total count of the features appeared in w and nfw is frequency of f appearing in w. p w  f; pe  is the cumulative distribution function of the binomial distribution and p k;nw  pe  is the corresponding probability mass function.

	 a 	 b 	 c 	 d 
figure 1: the bursty pattern of the feature sars. in  a   it shows all of the bursty periods related to sars. some bursty periods are gradually removed. we only retain those periods with many related documents.
　figure 1  a  and  b  respectively show the probability mass function  p k;nw  pe  and the cumulative distribution function  p w  f   of the binomial distribution with pe = 1 and nw = 1 using a algorithm 1 computebaselineprobability n 

input: r = {r1 r1 ... rl}
output: the expected probability pe
1: repeat 1: pe = mean r ; 1: σ = standarddeviation r ;
1:	for each ri （ p do
1:	if ri   mσ， pe then
1:	remove ri from r;
1:	end if
1:	end for
 1: until ri ＋ mσ， pe  ri （ r 1: return pe;

normal approximation. the x-axis represents the number of times f appears in w. their shape depends on pe only. p k;nw  pe  would be maximum if the probability of f appearing in w equals to pe. let r = nfw/nw be the probability of f appearing in w. if r  pe  p w  f  would approach to 1. in contrast  if r  pe  p w  f  would approach to 1. in other words  if the probability of f appearing in w is well below the expect probability  pe  we will not consider f is bursty in w. on the other hand  if the probability is much higher than pe  we conclude that f exhibits some abnormal behavior and hence it is bursty in w.
　in eq.  1   in order to compute pe  a simple yet ineffective approach is to explicitly define a fixed threshold  x  such that if f appears more than x% in w  f is bursty in w. this approach is impractical as different features have different thresholds. in order to assign different thresholds to different features automatically  we may attempt to rely on the mean probability: if f is distributed evenly over all windows in the text corpus  then the probability of
may not be appropriate. if the features with many bursty periods f in any window is: pe =  1/l ‘w（w nfw/nw . unfortunately  it
such as sars or iraq  their mean probabilities will be heavily biased by their bursty periods  and these probabilities cannot be used to model the situations when the features occur by chance. in this paper  we ignore the features with a significantly high frequencies in the windows when computing pe.
　algorithm 1 shows how we compute pe. let r = {r1 r1 ... rl} be a sequence where each element  ri  denote for the probability of f appearing in wi  ri = nfwi /nwi    where wi is the ith window in w. in lines 1 - 1  we compute the mean of r  pe  and its standard deviation. then  we check whether all ri drops within m standard deviations of the mean of r. we reject those points in r that are out of this range until all ri are less than or equal to this range. in this paper  we set m = 1.

	 a  iraq	 b  tsumani
figure 1: the distributions of iraq and tsunami.
　finally  algorithm 1 may encounter this problem: pe may be equal to 1. let us consider for the feature tsunami. its distribution from 1/1 to 1/1  1 day  is shown in figure 1 b . it only appears in 1 days with a total of 1 times. after completing algorithm 1  pe would be 1. if pe = 1  then the result of eq  1  would be undefined. so  we redefine ri by using the laplacian smoothing . as a result  ri and eq. 1  will become:
ri = nfwi +1   p k;n  p  nw k.	 1  	w	e
in order to decide whether f is bursty in w  we check where nfw drops in figure 1 b . if nfw drops in region d  then f is bursty in w as its occurrence is significantly higher than the expected probability  pe. if nfw drops in region a  then f is not bursty in w as it appears in w is less than the expected probability  pe.
　finally  we claim that user contribution in the bursty periods identification is necessary. let us take the feature sars as an example. its bursty patterns are shown in figure 1. the x-axes denote the day in 1 and the y-axes denote the bursty state  1 or 1 . when the bursty state is 1  no important event related to sars happened. the situation reverses if the bursty state is 1  where some important events emerged. figure 1  a  shows all of the bursty periods. the number of bursty periods decreased gradually from figure 1  b  to figure 1  d . we only retain those periods with many related documents. if the number of bursty periods is reduced  the number of events identified must also be reduced. hence  we may locate our target information more easily if there exists a threshold to control the number of events to be retrieved. let θ （  1  be such a threshold. we re-scale region b into this range. for instance  if θ = 1 and region b is from 1 to 1  then from all w with p w  f; pe  − 1  they will be regarded as bursty periods. our proposed algorithm includes this kind of user contribution  which is not being reported elsewhere.

figure 1: four bursty features and their corresponding documents.
featurerelationshipbirdb1   h1  b1  f1  b1  c1fluf1  c1cold-h1h1  b1  h1 f1	 	 
table 1: the map-to relationships in figure 1
1 identify the associated features and the associated documents
　let us assume that the bursty periods related to q are identified. let b be a set of bursty periods and db be a set of documents that satisfy q and reside in b （ b. in this section  we describe how we identify the features and the documents that are highly related to every b. for the features that are highly related to b  we call them as associated feature  fb  of period b. similarly  for the documents that are highly related to b  we call them as associated documents  db  of period b.
　as stated in section 1  we identify fb based on the following steps:  1  for all f （ f  identify their bursty periods according to the procedure described in the previous section;  1  let d be a set of documents that resides in one of the bursty periods of f. we conclude that db can be enlarged by d if d map-to db  d   db :
　definition 1.  map-to    let db be a set of documents that satisfies q and resides in b （ b. let d be another set of documents. db and d may be overlapped. we say that d map-to db  d   db  if and only if d is also resides in b and is significantly similar to db  where db   db. note that d is significantly similar to db does not imply d is signifantly similar to db  i.e. d   db does not imply db   d.
　example 1  map-to  figure 1 shows four features: two bursts for bird and flu  and one burst for cold and h1. according to definition1  a list of map-to relationships are identified  and are listed in table1. for instance  b1 and h1 are highly overlapped  b1  h1. similarly  as h1 and b1 are also highly overlap  h1  b1. since b1 is a subset of f1  b1  f1. in contrast  f1 cannot map to b1 or h1  as it is a superset of both of them. 
in defintion 1  we have to carefully define the phrase  significantly similar to . given two sets of documents  db and d  we determine whether they are siginficiantly similar to each other by two components: intra-similarity  si  and mapping-similarity  sm . we first present how they are computed  then explain why they are computed in this way. for the intra-similarity  si :
	si	d and d 	 1 
d|
 d（
where d is the nearest neighbor of d （ d  and c measures the similarity between d and d  e.g. cosine coefficient  . eq.  1  computes the average similarity between every pair of the nearest neighbor document in d. this is why si is termed as intrasimilarity. for the mapping-similarity:
	sm	|	  d （ db and d 	 1 
d
                        d（d where d is the nearest document with respect to d （ d.
si ，  and sm ，  differs in the contents of their similarity functions 
c ， ， . si computes the similarity within the same set of documents  whereas sm computes the similarity between two different sets of documents. specifically  sm ，  finds the set of the most similar documents in d that map-to db. this is why sm is termed as mapping-similarity.
　intuitively  if sm d    si d   we would be in favor of grouping db and d together  and rejecting to group them otherwise. unfortunately  directly comparing sm ，  and si ，  is inappropriate. both sm ，  and si ，  are averaged values. an averaged value may easily be affected by outliers. thus  standard deviations of both component must be used when we have to conduct the comparison. furthermore  the overlapping areas between db and d may also affect the comparison. for instance  if two sets of documents are highly overlapped  they may group together even if sm ，  is a bit smaller than si ，   i.e. some relaxation should be allowed in this case. on the other hand  no relaxation should be given if db ” d = 1/. to capture these ideas  we use a one-tail t-test with h1 : sm − si and h1 : sm   si  with the test statistics:
	s	 s
	t 	 1 
b
where σ is the standard deviation within si d . h1 would be rejected  d should not be mapped to db  ift1  tα  where 1 α＋1 is the significance level. α cannot be greater than 1 since this problem is a one tailed t-test. α must be chosen carefully  as it controls the relaxation of the t-test. the higher the value of α  the tighter is the control.
　as we discussed above  α should be determined dynamically based on the overlapping area between db and d. intuitively  α should behave as the pattern shown in figure 1  where the y-axis is the degree of confidence  α  and the x-axis is the percentage of overlapping between db and d. befor we continue  let us define a variable  δ:
	.	 1 
figure 1 captures the idea that if the overlapping area is large  δ  1   the required degree of confidence would be small   1 . this would result in a more relax situation. if the overlapping area is small   1   a much higher degree of confidence   1  would be resulted. if the overlapping area is halved  δ = 1   the required degree of confidence would be ambiguous  1 . logically  the relationship between δ and α should not be linear because we should be more certain with the decisions toward both ends. eventually 

figure 1: distribution of the confidence interval  α .
we can model the situation described so far by using a cosine function. mathematically 
	.	 1 
since it does not make sense if the degree of confidence is smaller than 1 or larger than 1  the cosine function is re-scaled within 1 to 1. this is why the two 1 are added.
1 construct event hierarchies
　in this section  we describe how to construct an event hierarchy for a particular query  q. let e be all of the events that appear in the event hierarchy. an event  e （ e  is an object which consists of the following three components:  1  a period of two timestamps;  1  a set of representative features; and  1  a set of similar documents.
　after the previous two steps  we will obtain a set of documents  db  that is highly related to q  some documents d   db match q directly  and some of them do not match q but are very similar to the other documents in that match q in db  and resides in b （ b  where b is a set of periods when q suddenly appears frequently. as discussed previously  for each db  it may further be broken down into several events. accordingly  for each db  we use the bisecting k-means clustering algorithm  to partition it  such that the documents with similar contents in db would be grouped together to form events. bisecting k-means is particularly suitable for partitioning the text corpus and will generate a dendrogram automatically. details of the algorithm could be referred to . similar to most of the clustering problems  the only issue remained here is how to specify a stopping criterion for the partitioning process: how many events should be left  defining a fixed value is unreasonable  as it is impossible to predict how many events exists in db in advance.
　in this paper  the stopping criterion of the bisecting k-means depends on the timestamp of the documents in db. let ti be the timestamp of di. we recursively partition db until max{ti+1 ti}   m  where m is a predefined threshold. in this paper  we set m = 1. this is based on our observations from the dataset that we used. the details of the dataset are described in section 1. eventually  each resulting partition is regard as an event. since a dendrogram is generated automatically from the bisecting k-means  a hierarchical tree structure which is similar to figure 1 is obtained naturally.
