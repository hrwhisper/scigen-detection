document clustering has long been an important problem in information retrieval. in this paper  we present a new clustering algorithm asi1  which uses explicitly modeling of the subspace structure associated with each cluster. asi simultaneously performs data reduction and subspace identification via an iterative alternating optimization procedure. motivated from the optimization procedure  we then provide a novel method to determine the number of clusters. we also discuss the connections of asi with various existential clustering approaches. finally  extensive experimental results on real data sets show the effectiveness of asialgorithm.
categories and subject descriptors
h.1  information search and retrieval : clustering; i.1  artificial intelligence : learning; i.1  pattern recognition :
applications
general terms
algorithms  experimentation  measurement  performance  theory  verification
keywords
document clustering  adaptive subspace identification  alternating optimization  factor analysis
1. introduction
　as a fundamental and effective tool for efficient document organization  summarization  navigation and retrieval of large amount of documents  document clustering has been very active and enjoying a growing amount of attention with the ever-increasing growth of the on-line information. a document clustering problem can be intuitively described as the problem of finding  given a set w of some n data points in a multi-dimensional space  a partition of w into classes such that the points within each class are similar to

1asistands for adaptive subspace iteration.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
each other. good document clustering enables better information services by browsing and organizing documents into meaningful cluster hierarchies and provides a useful complement for traditional text search engines when key-word based search returns too many documents.
　the more general problem of clustering has been studied extensively in machine learning  1  1   information theory  1  1   databases  1  1   and statistics  1  1  with various approaches and focuses. unfortunately  many methods fail to produce satisfactory results because they do not validate or offer interpretation of the clusters produced  because they make some simple but critical assumptions on the data distributions  e.g.  that they are gaussian   or because the criterion/objective function used is based on a distance function between sample points. the last property is critical. many of the existing clustering algorithms do not work efficiently in high dimensional spaces  curse of dimensionality . for document clustering  the data are indeed of high dimensions. it has been shown that in a high dimensional space the distance between every pair of points is almost the same for a wide variety of data distributions and distance functions . this justifies the attempt of reducing dimension of the input data. many feature selection techniques have been applied in that regard. however  as demonstrated in   the correlations among the dimensions are often specific to data locality  in the sense that some data points are correlated with a given set of features and others are correlated with respect to different features. in other words  in high dimensional space  each cluster usually has its own subspace structure. as pointed out in   all methods that overcome the dimensionality problems use a metric for measuring neighborhoods  which is often implicit and/or adaptive.
　in this paper  we propose a new clustering algorithm  asi  which explicitly models the subspace structure in the feature space associated with each cluster 1. the clustering algorithm simultaneously performs two tasks: data reduction  assigning data points into clusters  and subspace identification  identifying subspace structure associated with each cluster . the tasks are carried out by an iterative optimization procedure that alternates between identification of the subspace structure from current cluster partitions and updating of the clusters based on the identified new subspace structures. by explicitly modeling the subspace structure  asiproduces interpretable descriptions of the resulting clusters as an added bonus. in addition  through the iterative subspace identification  asi performs implicit adaptive feature selection at each iteration and flexibly measures the distances between data points. thus  it works well for high-dimensional data. we have shown the convergence property of asialgorithm and conducted extensive experiments to show its effectiveness. the rest of the paper is organized as fol-

lows: section 1 introduces the clustering models of asi  describes its optimization procedure and provides the method for deciding the number of clusters; section 1 shows our experimental results; section 1 surveys the related work. finally  our conclusions are presented in section 1.
1. asi clustering
1 the cluster model
　due to space limitation  the notations used in the paper are introduced in table 1. note that dt d =    ，，，1 ，，， ，，，，，， ，，，     is
	1	nk
w = wij n〜m nthe data set
number of data pointsmnumber of featurespi = wi1 ，，，  wim  kthe i-th data points number of clustersn	nknumber of points for each clusterc 	c1 ，，，ck}
d = dij n〜k
f = fij m〜kthe clusters
a n〜k matrix specifying cluster partitions dij = 1 if pi （ cj and 1 otherwise
f specifies the subspace structure for each cluster  fij's are coefficients of each feature associated with each cluster.wf wit ft j n〜kthe projection of the data points into the subspaces defined by f.s = dt d  1dtwfthe projection of the centroids into the subspaces defined by f.table 1: notation
a diagonal matrix with the cluster size on the diagonal. the inverse of dt d serves as a weight matrix to compute the projection.
　asi clustering explicitly models the subspace structure associated with each cluster via linear combinations of the original features. as described in table 1  f specifies the subspace structure for each cluster  i.e.  the coefficients of the linear combination of the features and s = dt d  1dtwf shows the projection of the centroids into the subspaces. the clustering is then sought such that the points of a cluster are close to each other within the subspaces.
formally this can be described as
	arg min owf 	 1 
d f s
where	f is the frobenius norm of the matrix x  i.e. .
note thatwf is the projection of the data points into the subspaces  d is a binary matrix specifying the cluster partitions and ds gives the approximation of the projection by the centroids. the above criterion is equivalent to the within-class distance of the partition induced by d.
　let tr a  be the trace of a. noting that and tr ab = tr ba  we have:
o wf 

 wt ft fw  1tr wfst dt  +tr dsst d  
= tr wt ft fw  tr ftwt d dt d  1dtwf 
= tr wt ft fw  tr ftwt ds 
it is easy to see that tr wt ftfw  is total deviance of wf and tr ftwt d dt d  1dtwf  is the between-class distance of the partition induced by d.
1 optimization
　the objective function can be minimized  local minima  by alternatively optimize one of d or f  and s  while fixing the other.
　first  given f and s  we will minimize o d f s  with respect to d. by equation  1   for fixed f and s  minimizing o d f s  reduces to maximizing tr ftwt ds . the problem is solved for different rows of d independently. since d is a binary matrix  we have
dij =1	if  wf i.  s.j = min{ wf i.  s.r r = 1 ，，，  k}
	1	otherwise
 1  second  given d  update f by minimizing
owf 
wf  d dt d  1dtwf 
 dt d  1dtw 
the columns of f are the coefficients of the features associated with different clusters  and are usually orthogonal. so  the objective criterion is minimized by taking the smallest k eigenvectors of wt  ind dt d  1dt  w  or equivalently  the first k eigenvectors of wt  d dtd  1dt   in w  where in is the identity matrix  see  . after f is updated  we also obtain a new s =  dt d  1dtwf.
　these two steps are executed iteratively. after each iteration  we compute the value of the objective criterion o d f s . if it is decreased  we then repeat the process; otherwise  the process has converged to a local minima. since the asi procedure monotonically decreases the objective criterion  it converges to a local optima. the clustering procedure is described as algorithm 1.

algorithm 1 asi: clustering procedure

input:  data points: wn〜m  # of classes: k  output: d: cluster assignment; begin 1.	initialization:
1	initialize d and f;
1	compute s = dtd  1dtwf;
1	compute o1 = o d f s 
1. iteration: begin
1 update d given f and s
1 update f given d
1 compute s
1 compute the value of o1 = o d f s ;
1 if o1   o1
1.1 set o1 to o1
1.1 repeat from 1
1 else
1.1 break from the loop;  convergence  end
1. return d f;
end

　the initialization step set the initial values for d and f. since d is a binary matrix having at most one occurrence of 1 in each row  the local minimum identified is very sensitive to initialization. to overcome the sensitivity of initialization  a refining procedure  whose idea is to use mutual information to measure the similarity between a pair of clustering results  is employed. asiattempts to find a best clustering result having the largest average mutual information against all others. in our experiments  ten runs of clustering are performed for refining.
　a special case for asi clustering occurs when we require that each row of f has at most one entry is 1 and all the rest is 1. in this case  it is assumed that different clusters associated with disjoint sets of features and f actually generates an implicit feature assignments into clusters. in other words  f induces the partitions of features and asiimplements a form of co-clustering of both data and features. we have developed a mutually reinforcing optimization procedure to exploit the duality of the data and features : if a feature is shared by many points associated with a cluster  then feature has a high weight associated with the cluster. on the other hand  if a data point is shared by many features associated with a cluster  then the data point has a high weight associated with the cluster.
1 deciding the number of clusters
　asiexplicitly models the subspace structure in cluster analysis  and  as seen in section 1  at each iteration  the subspace structure f is determined by the first k eigenvector of wt  d dt d  1dt   in w. note that each column of f corresponds to a cluster subspace and k is the number of clusters. also observe that  d dt d  1dt   in  serves as a weight matrix since the diagonal entries of  dt d  1 are cluster sizes. hence it is natural to relate the number of clusters with the spectrum of wtw.
　let q = min m n   wwt and wtw share the first q eigenvalues. note that wwt is a n〜n matrix and the ij-th entry of wwt computes the inner product of the i-th row and the j-th row of w. if w is normalized  then each each entry of wwt shows the cosine similarity between corresponding data points. for simplicity  suppose that the points in w are ordered according to their cluster memberships  that is  the points in the first cluster appear first and the point in the kth cluster appears at the end. such permutation does not change the spectral properties.
　since data points inside each cluster are similar to each other while they are quite different from those in other clusters wwt can be regarded as the sum of two matrices: wwt = l+e where l =
n  xirni〜ni
，，，
and e （ rn〜n is a matrix with a small value in each entry  if w is normalized  the cosine measure is very similar to the euclidean distance . .
lemma 1. let x   i.e.  all entries in the ma-
trix x （ rn〜n are 1. then the only nonzero eigenvalue of x is n.
　proof. since the rank of matrix x is 1  then the dimension of the null space of x is n 1. hence there is only one nonzero eigenvalue of x. it is clear the summation of all the eigenvalues of x equals to the trace of the matrix x  which is n. hence the nonzero eigenvalue is n. 
	lemma 1. let l.	that is  l is
a block diagonal matrix  with each block matrix formed as in lemma 1. let ni be the size of the matrix xi  for i = 1 ，，，  k. then the only nonzero eigenvalue of l are n1 n1 ，，，  nk.
　proof. since l is block diagonal  its eigenvalues and eigenvectors are the union of the eigenvalues and eigenvectors of its blocks  the latter padded approximately with zeros . 
　lemma 1. let a and e be two symmetric matrices with the same dimensions. then
  for i = 1 ，，，  n
where λi a  denotes the i-th largest eigenvalue of the matrix a  similarly  λi a+e  for matrix  a+e .
　proof. the lemma follows from the standard results in matrix perturbation theory. see  1  theorem 1.1. on page 1 .	
　theorem 1. let m = l + e where l has the form as in lemma 1 and e is a matrix with a small value in each entry. then m has dominant k eigenvalues  which are close to n1 n1 ，，，  nk.
　proof. the proof directly follows from lemma 1 and lemma 1. 
　since wwt can be regarded as the sum of two matrices  l and e  theorem 1 shows that wwt will have k dominant eigenvalues that are distinguished from the rest. using these theoretical results  we then obtain a novel method to determine the number of clusters based on eigenvalues of wwt .
　we note here that the above conclusion can also be derived from matrix perturbation theory  1  1 . denote s =wwt   set:
s ε = s 1 +εs 1 +ε1s 1 +，，，  
where s 1  = l is the unperturbed part of s. it can then follows from perron cluster analysis that the spectrum of s ε  can be divided into two parts: k dominant eigenvalues and the remaining part of the spectrum bounded away from the dominant eigenvalues.
1. experiments
　in this section  we apply our asi algorithm to document clustering and compare its performance with other standard clustering algorithms. in our experiments  documents are represented using binary vector-space model where each document is a binary vector in the term space and each element of the vector indicates the presence/absence of the corresponding term.
1 evaluation measures
　there are many ways to measure how accurately asiperforms. one is the confusion matrix which is described in . entry  o i  of a confusion matrix is the number of data points assigned to output class o and generated from input class i. another is the purity   which measures the extent to which each cluster contained data points from primarily one class. the purity of a clustering solution is calculated as a weighted sum of individual cluster purities:
purity . here si is a particular cluster of size ni  k is the number of clusters  and n is the total number of points. p si   the individual cluster purity of cluster si  is defined to be p si = n1i maxj nij   where nij is the number of documents of the i-th input class that were assigned to the j-th cluster. a high purity value implies that the clusters are  pure  subsets of the input classes. in general  the larger the values of purity  the better the clustering solution is.
1 document datasets
　to measure the effectiveness of asion document clustering  we use standard labeled corpora widely used in the information retrieval literature. we view the labels of the dataset as the objective knowledge on the structure of the datasets. we use the purity as the performance measure.
　we use a variety of datasets  most of which are frequently used in the information retrieval research. the number of classes ranges from four to twenty  and the number of documents ranges from 1 to 1. this is varied enough to obtain good insights on how well asi performs. table 1 summarizes the characteristics of the datasets.
　cstr this is the dataset of the abstracts of technical reports published in the department of computer science at the university of rochester between 1 and 1. the trs are available at http://www.cs.rochester.edu/trs. it has been used in  for text categorization. the dataset contained 1 abstracts  which were divided into four research areas: natural language processing  nlp   robotics/vision  systems  and theory.
　webkb the webkb dataset contains webpages gathered from university computer science departments. there are about 1 documents and they are divided into seven categories: student  faculty  staff  course  project  department and other. the raw text is about 1mb. among these seven categories  student  faculty  course and project are four most populous entity-representing categories. the associated subset is typically called webkb1. in this paper  we did experiments on both seven- and four-category datasets.
　reuters the reuters-1 text categorization test collection contains documents collected from the reuters newswire in 1. it is a standard text categorization benchmark and contains 1 categories. in our experiments  we use a subsets of the data collection which include the ten frequent categories among the 1 topics and we call it reuters-top 1.
　k-dataset the k-dataset was from webace project  and it was used in  for document clustering. the k-dataset contains 1 documents consisting news articles from reuters new service via the web in october 1. these documents are divided into twenty classes.
　to pre-process the datasets  we remove the stop words using a standard stop list  all html tags are skipped and all header fields except subject and organization of the posted article are ignored. in all our experiments  we first select the top 1 words by mutual information with class labels. the feature selection is done with the rainbow package .
datasets# documents# classcstr1webkb1 1webkb11reuters-top 1 1k-dataset11table 1: document datasets descriptions.
1 experimental results
　a recent comparative study on document clustering  showed that the performance of clustering based on graph partitioning package  cluto  is very good. the cluto package is built on a sophisticated multi-level graph partitioning engine and it provides many different criterion functions that can be used to drive both partitional and agglomerative clustering algorithms. here  in our experiments  we compare the performance of asi on the datasets with the algorithms provided in the cluto package  1  1 . we choose several partitional criteria and several agglomerative clustering algorithms for comparison. we also provide the experiment results of traditional k-means algorithm.
　the comparisons are shown in table 1. each entry is the purity of the corresponding column algorithm on the row dataset. p1 is a multi-level partitioning method which tries to maximize the cosine similarity between each document and the cluster centroid. slink  clink and upgma columns are different hierarchical clustering algorithms using single-linkage  complete-linkage and upgma aggregating policies. single-linkage and complete-linkage use the maximum and the minimum distance between the two clusters  respectively  while upgma - unweighted pair-groups method average uses the distance of the cluster centers to define the similarity of two clusters for aggregating. in our experiments  we use the cosine function measure of the two document vectors as their similarity.
datasetsasik-meansp1slinkclinkupgmacstr111111webkb1.1.1.1.1.1.1webkb111n/an/an/areuters111111k-dataset111111table 1: comparison of performance of clustering algorithms. each entry is the purity of the corresponding column algorithm on the row dataset.
　from table 1  we observe that asi achieves the best performance on all datasets except webkb1. in addition  asi outperforms k-means and the hierarchical methods on all datasets. on webkb1  p1 has the best performance of 1 and asigives the result of 1. figure 1 shows the graphical comparison. in total  the performance of asi is always either the winner or very close to the winner. the comparison shows that  although there is no single winner on all the datasets  asi is a viable and competitive algorithm in document clustering domain.
　table 1 shows the confusion matrices built from the clustering results on cstr dataset. the columns of the confusion matrix are nlp  robotics/vision  systems and theory  respectively. the result shows that systems and theory are much different from each other and each different from nlp and from robotics/vision  that nlp and robotics/vision are similar to each other  and that nlp is more similar to systems than robotics/vision.
inputoutput11a11b11c11d11table 1: the confusion matrix of asion the cstr dataset.
we now try to visualize the cluster structure that might be dis-

figure 1: comparisons of the purity values.
covered by asi algorithm. we restrict that each row of the f has at most one entry is 1 and assigns each feature into clusters. figure 1 shows the original word-document matrix of cstr and the reordered matrix obtained by arranging rows and columns based on the cluster assignments. the figure reveals the hidden sparsity structure of both the document and word clusters. the four block diagonals in figure 1 b  correspond to the four clusters and the dense region at the bottom of the figure identifies the feature outliers  which are distributed uniformly across the technical reports . the rough block diagonal sub-structures observed indicate the cluster structure relations between documents and words. asitends to yield better clustering solutions by explicitly identifying the subspace structure  especially for high dimensional sparse datasets. the dense region  corresponding to feature outliers  also reflects the feature selection ability of asi.
　a nice property of asiis that the resulting classes can be easily described in terms of features  since the algorithm explicitly exploit the subspace structure in the feature space. in table 1  we show the four word clusters obtained when applying asi to the cstr dataset. we see that these words are meaningful and are often representatives of the associated document cluster. for example  shared and multiprocessor are representatives of the systems cluster 1. similarly  turing and reduction are strongly related to the research efforts in the theory group at rochester. an interesting and also important implication is the interpretability of the clustering results. the document clusters could be well explained using its associated feature  word  clusters.
1 number of clusters
　the ability of asi to estimate the number of clusters can be demonstrated using the cstr dataset. in this experiment  we normalize the entries of wwt so that that the sum of each row equals to 1. the normalization makes the largest eigenvalue equal to 1 and all the others are less than 1. as discussed in section 1  the number of clusters then corresponds to the largest eigenvalues of wwt . figure 1 shows the top eigenvalues of wwt .
　note that the n-th eigenvalue is less than 1 when n   1. this is a strong indicator on the number of clusters in the dataset.



 b  dataset after reordering
figure 1: visualization of the original document-data matrix and the reordered document-data matrix. the shaded region represents non-zero entries.

figure 1: the top eigenvalues of normalized xxt. the y-axis indicates the eigenvalues and the x-axis indicates the order of the eigenvalues. note that the largest eigenvalue is 1.
cluster 1cluster 1cluster 1cluster 1sharetraintrackturingmultiprocessorspokenfreedomreductioncachedialoguemovementnondeterministicsynchronizationdiscourseperceptioncollapselocalplancalibrationbooleanremotespeakertargetoracleloadutterancesensorboundlatencycorpuseyeprovecontentionconversationfilterdownwardlockparsercameracountscheduleactbehaviorcircuitblockinferencemanipulatorfewpmessagesemanticmotorpspacebutterflydisambiguationrobotrelativizedmigrationlinguisticarmstringpolicyreasonstagemembershippagelexiconreconstructsatbusyphraseindoorequivalentwaitcoverageacquireautomatamultiprogrammingdeductivegeometrypolynomialtable 1: the four word clusters obtained using asi on the cstr dataset. cluster 1  1  1  and 1 represent systems  natural language processing  robotics/vision  and theory  respectively. for each cluster  only top twenty words based on the associated degree in the final feature coefficients are included.
1. related work
　traditional clustering techniques focus on one-sided clustering and they can be classified into partitional  hierarchical  densitybased  and grid-based  1  1 . partitional clustering attempts to directly decompose the data set into k disjoint classes such that the data points in a class are nearer to one another than the data points in other classes. for example  the traditional k-means method tries to minimize the sum-of-squared-errors criterion function. hierarchical clustering proceeds successively by building a tree of clusters. density-based clustering is to group the neighboring points of a data set into classes based on density conditions. grid-based clustering quantizes the object space into a finite number of cells that form a grid-structure and then performs clustering on the grid structure. most of these algorithms use distance functions as objective criteria and are not effective in high dimensional spaces.
　in what follows  we review the work that are closely related to our approach  see figure 1 for a summary .
　first of all  the asiclustering can be regarded as an integration of k-means and eigenvector analysis . asishares the alternating optimization procedure common to k-means type algorithms and performs factor analysis to identify the subspace structures for the clusters.
　the simultaneous optimization in both directions of data and feature used in asi is similar to the optimization procedure in coclustering. the idea of co-clustering of data points and attributes dates back to  1  1  1 . co-clustering is simultaneous clustering of both points and their attributes by way of utilizing the canonical duality contained in the point-by-attribute data representation. govaert  studies simultaneous block clustering of the rows and columns of contingency tables. the idea of co-clustering has been also applied to the problem of clustering gene and tissue types based on gene expression . dhillon  presents a co-clustering algorithm for documents and words using bipartite graph formulation and a spectral heuristic. more recently  dhillon et al.  propose an information-theoretic co-clustering method for two-dimensional contingency table. by viewing a non-negative contingency table as a joint probability distribution between two discrete random variables  the optimal co-clustering is obtained by maximizing the mutual information between the clustered random variables. in fact  as demonstrated in section 1  if we put restrictions on f  the subspace structure determined by f induces clustering on feature space. in other words  with restrictions on f  asi enables an iterative co-clustering procedure for both the data and the feature assignments. however  unlike previously proposed coclustering approaches  asi performs explicit subspace optimization using least square minimization via matrix operations.
　also  asi can be thought of as an approximate iterative information bottleneck method. the information bottleneck  ib  framework is first introduced for one-sided clustering . the core idea of ib is as follows: given the empirical joint distribution of two variables  x y   one variable is compressed so that the mutual information about the other is preserved as much as possible. the ib algorithm in  tries to minimize the quantity i x;x   while maximizing i x ;y   where i is the mutual information and x  is the cluster representation of x. the overall criterion there is formulated as i x;x   βi x ;y   where β is a lagrange multiplier determining the trade-off between compression and precision. agglomerative versions of the ib method are used in  1  1  to cluster documents after clustering individual words. asi is similar to ib method in that the feature functions in both are restricted to clustering. the subspace structure induced by f captures most of the information about the original dataset and thus is a compact representation. the optimization procedure is then a procedure of searching for the concise representation.
　the cluster model in asi is similar to that the data and feature maps in . the data and feature maps of  are defined as two functions mapping from the data and from the feature set to the number of clusters. the clustering algorithm of  is based on maximum likelihood principle via a co-learning process between the data and feature maps. the maps can be viewed as an extremal case of the model in asiwhen f is restricted to a binary function.
　the optimization procedure for subspace structure identification in asiutilizes the dominant eigenvectors of wt  d dt d  1dt   in w. this carries the spirit of spectral clustering. spectral methods have been successfully in many applications  including computer vision  1  1   vlsi design  and graph partitioning . basically spectral methods use selected eigenvectors of the data affinity matrix to obtain a data representation that can be easily clustered or embedded in a low-dimensional space  1  1 . the wt  d dtd  1dt   in w used in asi can be thought of as an affinity matrix with some weighting schemes based on cluster size.
　by iteratively updating  asi performs an implicit adaptive feature selection at each iteration and has some common ideas with adaptive feature selection methods. ding et al.  propose an adaptive dimension reduction clustering algorithm. the basic idea is to adaptively update the initial feature selection based on intermediate results during the clustering process and the process is repeated until the best results are obtained. domeniconi et al.  use a chi-squared distance analysis to compute a flexible metric for producing neighborhoods that are highly adaptive to query locations. neighborhoods are elongated along less relevant feature dimensions and constricted along most influential ones.

figure 1: summary of related work. the arrows show connections.　since asiexplicitly models the subspace structure at each iteration  it is viewed as an adaptive subspace clustering. clique  is an automatic subspace clustering algorithm for high dimensional spaces. it uses equal-size cells and cell density to find dense regions in each subspace in a high dimensional space  where cell size and the density threshold are given as a part of the input. aggarwal et al.  introduce projected clustering and present algorithms for discovering interesting patterns in subspaces of high dimensional spaces. the core idea is a generalization of feature selection which enables selecting different sets of dimensions for different subsets of the data sets. asi adaptively computes the distance measures and the number of dimensions for each class. it also does not require all projected classes to have the same number of dimensions.
　asi also shares many properties with sufficient dimensionality reduction  and with non-negative matrix factorization  1  1 .
1. conclusions
　in this paper  we introduced a new clustering algorithm that allows explicit modeling of the subspace structure associated with each cluster. a key idea of the algorithm is to iteratively perform two alternating procedures: optimization of the subspace structure and updating of the clusters. this is somewhat reminiscent of em. experimental results suggested that asiis a viable and competitive clustering algorithm.
acknowledgments
the authors want to thank mr. jieping ye for providing useful insights on section 1. we are also grateful to the conference reviewers for their helpful comments and suggestions. the first and the third authors are supported in part by nsf grants eia-1 and eia-1 and in part by nih grant p1-ag1.
