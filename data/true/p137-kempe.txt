models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains  including the diffusion of medical and technological innovations  the sudden and widespread adoption of various strategies in game-theoretic settings  and the effects of  word of mouth  in the promotion of new products. recently  motivated by the design of viral marketing strategies  domingos and richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation  and the goal is to trigger a large cascade of further adoptions  which set of individuals should we target 
﹛we consider this problem in several of the most widely studied models in social network analysis. the optimization problem of selecting the most influential nodes is np-hard here  and we provide the first provable approximation guarantees for efficient algorithms. using an analysis framework based on submodular functions  we show that a natural greedy strategy obtains a solution that is provably within 1% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.
﹛we also provide computational experiments on large collaboration networks  showing that in addition to their provable guarantees  our approximation algorithms significantly out-perform nodeselection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.
categories and subject descriptors
f.1  analysis of algorithms and problem complexity : nonnumerical algorithms and problems

 supported by an intel graduate fellowship and an nsf graduate research fellowship.
 supported in part by a david and lucile packard foundation fellowship and nsf itr/im grant iis-1.
 supported in part by nsf itr grant ccr-1  and onr grant n1-1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigkdd '1 washington  dc  usa copyright 1 acm 1-1/1 ...$1.
keywords
approximation algorithms  social networks  viral marketing  diffusion of innovations
1. introduction
﹛a social network - the graph of relationships and interactions within a group of individuals - plays a fundamental role as a medium for the spread of information  ideas  and influence among its members. an idea or innovation will appear - for example  the use of cell phones among college students  the adoption of a new drug within the medical profession  or the rise of a political movement in an unstable society - and it can either die out quickly or make significant inroads into the population. if we want to understand the extent to which such ideas are adopted  it can be important to understand how the dynamics of adoption are likely to unfold within the underlying social network: the extent to which people are likely to be affected by decisions of their friends and colleagues  or the extent to which  word-of-mouth  effects will take hold. such network diffusion processes have a long history of study in the social sciences. some of the earliest systematic investigations focused on data pertaining to the adoption of medical and agricultural innovations in both developed and developing parts of the world  1  1  1 ; in other contexts  research has investigated diffusion processes for  word-of-mouth  and  viral marketing  effects in the success of new products  1  1  1  1  1  1  1   the sudden and widespread adoption of various strategies in gametheoretic settings  1  1  1  1  1   and the problem of cascading failures in power systems  1  1 .
﹛in recent work  motivated by applications to marketing  domingos and richardson posed a fundamental algorithmic problem for such systems  1  1 . suppose that we have data on a social network  with estimates for the extent to which individuals influence one another  and we would like to market a new product that we hope will be adopted by a large fraction of the network. the premise of viral marketing is that by initially targeting a few  influential  members of the network - say  giving them free samples of the product - we can trigger a cascade of influence by which friends will recommend the product to other friends  and many individuals will ultimately try it. but how should we choose the few key individuals to use for seeding this process  in  1  1   this question was considered in a probabilistic model of interaction; heuristics were given for choosing customers with a large overall effect on the network  and methods were also developed to infer the influence data necessary for posing these types of problems.
﹛in this paper  we consider the issue of choosing influential sets of individuals as a problem in discrete optimization. the optimal solution is np-hard for most models that have been studied  including the model of . the framework proposed in   on the other hand  is based on a simple linear model where the solution to the optimization problem can be obtained by solving a system of linear equations. here we focus on a collection of related  np-hard models that have been extensively studied in the social networks community  and obtain the first provable approximation guarantees for efficient algorithms in a number of general cases. the generality of the models we consider lies between that of the polynomial-time solvable model of  and the very general model of   where the optimization problem cannot even be approximated to within a non-trivial factor.
﹛we begin by departing somewhat from the domingos-richardson framework in the following sense: where their models are essentially descriptive  specifying a joint distribution over all nodes' behavior in a global sense  we focus on more operational models from mathematical sociology  1  1  and interacting particle systems  1  1  that explicitly represent the step-by-step dynamics of adoption. we show that approximation algorithms for maximizing the spread of influence in these models can be developed in a general framework based on submodular functions  1  1 . we also provide computational experiments on large collaboration networks  showing that in addition to their provable guarantees  our algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality  from the field of social network analysis.
two basic diffusion models. in considering operational models for the spread of an idea or innovation through a social network g  represented by a directed graph  we will speak of each individual node as being either active  an adopter of the innovation  or inactive. we will focus on settings  guided by the motivation discussed above  in which each node's tendency to become active increases monotonically as more of its neighbors become active. also  we will focus for now on the progressive case in which nodes can switch from being inactive to being active  but do not switch in the other direction; it turns out that this assumption can easily be lifted later. thus  the process will look roughly as follows from the perspective of an initially inactive node v: as time unfolds  more and more of v's neighbors become active; at some point  this may cause v to become active  and v's decision may in turn trigger further decisions by nodes to which v is connected.
﹛granovetter and schelling were among the first to propose models that capture such a process; their approach was based on the use of node-specific thresholds  1  1 . many models of this flavor have since been investigated  see e.g.  1  1  1  1  1  1  1  1  1  1  1  but the following linear threshold model lies at the core of most subsequent generalizations. in this model  a node v is influenced by each neighbor w according to a weight bv w such that x bv w ≒ 1. the dynamics of the process then proceed
w neighbor of v
as follows. each node v chooses a threshold 牟v uniformly at random from the interval  1 ; this represents the weighted fraction of v's neighbors that must become active in order for v to become active. given a random choice of thresholds  and an initial set of active nodes a1  with all other nodes inactive   the diffusion process unfolds deterministically in discrete steps: in step t  all nodes that were active in step t   1 remain active  and we activate any node v for which the total weight of its active neighbors is at least 牟v:
	x	bv w	≡	牟v.
w active neighbor of v
thus  the thresholds 牟v intuitively represent the different latent tendencies of nodes to adopt the innovation when their neighbors do; the fact that these are randomly selected is intended to model our lack of knowledge of their values - we are in effect averaging over possible threshold values for all the nodes.  another class of approaches hard-wires all thresholds at a known value like 1; see for example work by berger   morris   and peleg . 
﹛based on work in interacting particle systems  1  1  from probability theory  we can also consider dynamic cascade models for diffusion processes. the conceptually simplest model of this type is what one could call the independent cascade model  investigated recently in the context of marketing by goldenberg  libai  and muller  1  1 . we again start with an initial set of active nodes a1  and the process unfolds in discrete steps according to the following randomized rule. when node v first becomes active in step t  it is given a single chance to activate each currently inactive neighbor w; it succeeds with a probability pv w - a parameter of the system - independently of the history thus far.  if w has multiple newly activated neighbors  their attempts are sequenced in an arbitrary order.  if v succeeds  then w will become active in step t+1; but whether or not v succeeds  it cannot make any further attempts to activate w in subsequent rounds. again  the process runs until no more activations are possible.
﹛the linear threshold and independent cascade models are two of the most basic and widely-studied diffusion models  but of course many extensions can be considered. we will turn to this issue later in the paper  proposing a general framework that simultaneously includes both of these models as special cases. for the sake of concreteness in the introduction  we will discuss our results in terms of these two models in particular.
approximation algorithms for influencemaximization. we are now in a position to formally express the domingos-richardson style of optimization problem - choosing a good initial set of nodes to target - in the context of the above models. both the linear threshold and independent cascade models  as well as the generalizations to follow  involve an initial set of active nodes a1 that start the diffusion process. we define the influence of a set of nodes a  denoted 考 a   to be the expected number of active nodes at the end of the process  given that a is this initial active set a1. the influence maximization problem asks  for a parameter k  to find a k-node set of maximum influence.  when dealing with algorithms for this problem  we will say that the chosen set a of k initial active nodes has been targeted for activation by the algorithm.  for the models we consider  it is np-hard to determine the optimum for influence maximization  as we will show later.
﹛our first main result is that the optimal solution for influence maximization can be efficiently approximated to within a factor of  1   1/e   汍   in both the linear threshold and independent cascade models; here e is the base of the natural logarithm and 汍 is any positive real number.  thus  this is a performance guarantee slightly better than 1%.  the algorithm that achieves this performance guarantee is a natural greedy hill-climbing strategy related to the approach considered in   and so the main content of this result is the analysis framework needed for obtaining a provable performance guarantee  and the fairly surprising fact that hill-climbing is always within a factor of at least 1% of optimal for this problem. we prove this result in section 1 using techniques from the theory of submodular functions  1  1   which we describe in detail below  and which turn out to provide a natural context for reasoning about both models and algorithms for influence maximization.
﹛in fact  this analysis framework allows us to design and prove guarantees for approximation algorithms in much richer and more realistic models of the processes by which we market to nodes. the deterministic activation of individual nodes is a highly simplified model; an issue also considered in  1  1  is that we may in reality have a large number of different marketing actions available  each of which may influence nodes in different ways. the available budget can be divided arbitrarily between these actions. we show how to extend the analysis to this substantially more general framework. our main result here is that a generalization of the hill-climbing algorithm still provides approximation guarantees arbitrarily close to  1   1/e .
﹛it is worth briefly considering the general issue of performance guarantees for algorithms in these settings. for both the linear threshold and the independent cascade models  the influence maximization problem is np-complete  but it can be approximated well. in the linear model of richardson and domingos   on the other hand  both the propagation of influence as well as the effect of the initial targeting are linear. initial marketing decisions here are thus limited in their effect on node activations; each node's probability of activation is obtained as a linear combination of the effect of targeting and the effect of the neighbors. in this fully linear model  the influence can be maximized by solving a system of linear equations. in contrast  we can show that general models like that of domingos and richardson   and even simple models that build in a fixed threshold  like 1  at all nodes  1  1  1   lead to influence maximization problems that cannot be approximated to within any non-trivial factor  assuming p 1= np. our analysis of approximability thus suggests a way of tracing out a more delicate boundary of tractability through the set of possible models  by helping to distinguish among those for which simple heuristics provide strong performance guarantees and those for which they can be arbitrarily far from optimal. this in turn can suggest the development of both more powerful algorithms  and the design of accurate models that simultaneously allow for tractable optimization.
﹛following the approximation and np-hardness results  we describe in section 1 the results of computational experiments with both the linear threshold and independent cascade models  showing that the hill-climbing algorithm significantly out-performs strategies based on targeting high-degree or  central  nodes . in section 1 we then develop a general model of diffusion processes in social networks that simultaneously generalizes the linear threshold and independent cascade models  as well as a number of other natural cases  and we show how to obtain approximation guarantees for a large sub-class of these models. in sections 1 and 1  we also consider extensions of our approximation algorithms to models with more realistic scenarios in mind: more complex marketing actions as discussed above  and non-progressive processes  in which active nodes may become inactive in subsequent steps.
1. approximationguaranteesinthe independentcascadeandlinear threshold models
the overall approach. we begin by describing our strategy for proving approximation guarantees. consider an arbitrary function f ﹞  that maps subsets of a finite ground set u to non-negative real numbers.1 we say that f is submodular if it satisfies a natural  diminishing returns  property: the marginal gain from adding an element to a set s is at least as high as the marginal gain from adding

1
 note that the influence function 考 ﹞  defined above has this form; it maps each subset a of the nodes of the social network to a real number denoting the expected size of the activated set if a is targeted for initial activation.
the same element to a superset of s. formally  a submodular function satisfies
	f s ﹍ {v}    f s 	≡	f t ﹍ {v}    f t  
for all elements v and all pairs of sets s   t.
﹛submodular functions have a number of very nice tractability properties; the one that is relevant to us here is the following. suppose we have a function f that is submodular  takes only nonnegative values  and is monotone in the sense that adding an element to a set cannot cause f to decrease: f s ﹍ {v}  ≡ f s  for all elements v and sets s. we wish to find a k-element set s for which f s  is maximized. this is an np-hard optimization problem  it can be shown to contain the hitting set problem as a simple special case   but a result of nemhauser  wolsey  and fisher  1  1  shows that the following greedy hill-climbing algorithm approximates the optimum to within a factor of  1   1/e   where e is the base of the natural logarithm : start with the empty set  and repeatedly add an element that gives the maximum marginal gain.
﹛theorem 1.  1  1  for a non-negative  monotone submodular function f  let s be a set of size k obtained by selecting elements one at a time  each time choosing an element that provides the largest marginal increase in the function value. let s  be a set that maximizes the value of f over all k-element sets. then f s  ≡  1/e ﹞f s  ; in other words  s provides a  1/e approximation.
﹛due to its generality  this result has found applications in a number of areas of discrete optimization  see e.g.  ; the only direct use of it that we are aware of in the databases and data mining literature is in a context very different from ours  for the problem of selecting database views to materialize .
﹛our strategy will be to show that for the models we are considering  the resulting influence function 考 ﹞  is submodular. a subtle difficulty lies in the fact that the result of nemhauser et al. assumes that the greedy algorithm can evaluate the underlying function exactly  which may not be the case for the influence function 考 a . however  by simulating the diffusion process and sampling the resulting active sets  we are able to obtain arbitrarily close approximations to 考 a   with high probability. furthermore  one can extend the result of nemhauser et al. to show that for any 汍   1  there is a 污   1 such that by using  1 + 污 -approximate values for the function to be optimized  we obtain a  1/e 汍 -approximation.
﹛as mentioned in the introduction  we can extend this analysis to a general model with more complex marketing actions that can have a probabilistic effect on the initial activation of nodes. we show in section 1 how  with a more careful hill-climbing algorithm and a generalization of theorem 1  we can obtain comparable approximation guarantees in this setting.
﹛a further extension is to assume that each node v has an associated non-negative weight wv  capturing how important it is that v be activated in the final outcome.  for instance  if we are marketing textbooks to college teachers  then the weight could be the number of students in the teacher's class  resulting in a larger or smaller number of sales.  if we let b denote the  random  set activated by the process with initial activation a  then we can define the weighted influence function 考w a  to be the expected value over outcomes b of the quantity pv﹋b wv. the influence function studied above is the special case obtained by setting wv = 1 for all nodes v. the objective function with weights is submodular whenever the unweighted version is  so we can still use the greedy algorithm for obtaining a  1/e 汍 -approximation. note  however  that a sampling algorithm to approximately choose the next element may need time that depends on the sizes of the weights.
independent cascade
in view of the above discussion  an approximation guarantee for influence maximization in the independent cascade model will be a consequence of the following
﹛theorem 1. for an arbitrary instance of the independent cascade model  the resulting influence function 考 ﹞  is submodular.
﹛in order to establish this result  we need to look  implicitly or explicitly  at the expression 考 a ﹍ {v}    考 a   for arbitrary sets a and elements v. in other words  what increase do we get in the expected number of overall activations when we add v to the set a  this increase is very difficult to analyze directly  because it is hard to work with quantities of the form 考 a . for example  the independent cascade process is underspecified  since we have not prescribed the order in which newly activated nodes in a given step t will attempt to activate their neighbors. thus  it is not initially obvious that the process is even well-defined  in the sense that it yields the same distribution over outcomes regardless of how we schedule the attempted activations.
﹛our proof deals with these difficulties by formulating an equivalent view of the process  which makes it easier to see that there is an order-independent outcome  and which provides an alternate way to reason about the submodularity property.
﹛consider a point in the cascade process when node v has just become active  and it attempts to activate its neighbor w  succeeding with probability pv w. we can view the outcome of this random event as being determined by flipping a coin of bias pv w. from the point of view of the process  it clearly does not matter whether the coin was flipped at the moment that v became active  or whether it was flipped at the very beginning of the whole process and is only being revealed now. continuing this reasoning  we can in fact assume that for each pair of neighbors  v w  in the graph  a coin of bias pv w is flipped at the very beginning of the process  independently of the coins for all other pairs of neighbors   and the result is stored so that it can be later checked in the event that v is activated while w is still inactive.
﹛with all the coins flipped in advance  the process can be viewed as follows. the edges in g for which the coin flip indicated an activation will be successful are declared to be live; the remaining edges are declared to be blocked. if we fix the outcomes of the coin flips and then initially activate a set a  it is clear how to determine the full set of active nodes at the end of the cascade process:
﹛claim 1. a node x ends up active if and only if there is a path from some node in a to x consisting entirely of live edges.  we will call such a path a live-edge path. 
﹛consider the probability space in which each sample point specifies one possible set of outcomes for all the coin flips on the edges. let x denote one sample point in this space  and define 考x a  to be the total number of nodes activated by the process when a is the set initially targeted  and x is the set of outcomes of all coin flips on edges. because we have fixed a choice for x  考x a  is in fact a deterministic quantity  and there is a natural way to express its value  as follows. let r v x  denote the set of all nodes that can be reached from v on a path consisting entirely of live edges. by claim 1  考x a  is the number of nodes that can be reached on live-edge paths from any node in a  and so it is equal to the cardinality of the union ﹍v﹋ar v x .
proof of theorem 1. first  we claim that for each fixed outcome x  the function 考x ﹞  is submodular. to see this  let s and t be two sets of nodes such that s   t  and consider the quantity 考x s ﹍ {v}  考x s . this is the number of elements in r v x  that are not already in the union ﹍u﹋sr u x ; it is at least as large as the number of elements in r v x  that are not in the  bigger  union ﹍u﹋tr u x . it follows that 考x s ﹍ {v}    考x s  ≡ 考x t ﹍ {v}    考x t   which is the defining inequality for submodularity. finally  we have
	考 a 	=	x	prob x  ﹞ 考x a  
outcomes x
since the expected number of nodes activated is just the weighted average over all outcomes. but a non-negative linear combination of submodular functions is also submodular  and hence 考 ﹞  is submodular  which concludes the proof. 
next we show the hardness of influence maximization.
﹛theorem 1. the influence maximization problem is np-hard for the independent cascade model.
proof. consider an instance of the np-complete set cover problem  defined by a collection of subsets s1 s1 ... sm of a ground set u = {u1 u1 ... un}; we wish to know whether there exist k of the subsets whose union is equal to u.  we can assume that k   n   m.  we show that this can be viewed as a special case of the influence maximization problem.
﹛given an arbitrary instance of the set cover problem  we define a corresponding directed bipartite graph with n + m nodes: there is a node i corresponding to each set si  a node j corresponding to each element uj  and a directed edge  i j  with activation probability pi j = 1 whenever uj ﹋ si. the set cover problem is equivalent to deciding if there is a set a of k nodes in this graph with 考 a  ≡ n + k. note that for the instance we have defined  activation is a deterministic process  as all probabilities are 1 or 1. initially activating the k nodes corresponding to sets in a set cover solution results in activating all n nodes corresponding to the ground set u  and if any set a of k nodes has 考 a  ≡ n + k  then the set cover problem must be solvable. 
linear thresholds
we now prove an analogous result for the linear threshold model.
﹛theorem 1. for an arbitrary instance of the linear threshold model  the resulting influence function 考 ﹞  is submodular.
proof. the analysis is a bit more intricate than in the proof of theorem 1  but the overall argument has a similar structure. in the proof of theorem 1  we constructed an equivalent process by initially resolving the outcomes of some random choices  considering each outcome in isolation  and then averaging over all outcomes. for the linear threshold model  the simplest analogue would be to consider the behavior of the process after all node thresholds have been chosen. unfortunately  for a fixed choice of thresholds  the number of activated nodes is not in general a submodular function of the targeted set; this fact necessitates a more subtle analysis.
﹛recall that each node v has an influence weight bv w ≡ 1 from each of its neighbors w  subject to the constraint that pw bv w ≒ 1.  we can extend the notation by writing bv w = 1 when w is not a neighbor of v.  suppose that v picks at most one of its incoming edges at random  selecting the edge from w with probability bv w and selecting no edge with probability 1   pw bv w. the selected edge is declared to be  live   and all other edges are declared to be  blocked.   note the contrast with the proof of theorem 1: there  we determined whether an edge was live independently of the decision for each other edge; here  we negatively correlate the decisions so that at most one live edge enters each node. 
﹛the crux of the proof lies in establishing claim 1 below  which asserts that the linear threshold model is equivalent to reachability via live-edge paths as defined above. once that equivalence is established  submodularity follows exactly as in the proof of theorem 1. we can define r v x  as before to be the set of all nodes reachable from v on live-edge paths  subject to a choice x of live/blocked designations for all edges; it follows that 考x a  is the cardinality of the union ﹍v﹋ar v x   and hence a submodular function of a; finally  the function 考 ﹞  is a non-negative linear combination of the functions 考x ﹞  and hence also submodular. 
﹛claim 1. for a given targeted set a  the following two distributions over sets of nodes are the same:
 i  the distribution over active sets obtained by running the linear threshold process to completion starting from a; and
 ii  the distribution over sets reachable from a via live-edge paths  under the random selection of live edges defined above.
proof. we need to prove that reachability under our random choice of live and blocked edges defines a process equivalent to that of the linear threshold model. to obtain intuition about this equivalence  it is useful to first analyze the special case in which the underlying graph g is directed and acyclic. in this case  we can fix a topological ordering of the nodes v1 v1 ... vn  so that all edges go from earlier nodes to later nodes in the order   and build up the distribution of active sets by following this order. for each node vi  suppose we already have determined the distribution over active subsets of its neighbors. then under the linear threshold process  the probability that vi will become active  given that a subset si of its neighbors is active  is pw﹋si bvi w. this is precisely the probability that the live incoming edge selected by vi lies in si  and so inductively we see that the two processes define the same distribution over active sets.
﹛to prove the claim generally  consider a graph g that is not acyclic. it becomes trickier to show the equivalence  because there is no natural ordering of the nodes over which to perform induction. instead  we argue by induction over the iterations of the linear threshold process. we define at to be the set of active nodes at the end of iteration t  for t = 1 1 ...  note that a1 is the set initially targeted . if node v has not become active by the end of iteration t  then the probability that it becomes active in iteration t+1 is equal to the chance that the influence weights in at  at 1 push it over its threshold  given that its threshold was not exceeded
b
pu﹋at at 1 v u
already; this probability is .
﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛﹛1   pu﹋at 1 bv u on the other hand  we can run the live-edge process by revealing the identities of the live edges gradually as follows. we start with the targeted set a. for each node v with at least one edge from the set a  we determine whether v's live edge comes from a. if so  then v is reachable; but if not  we keep the source of v's live edge unknown  subject to the condition that it comes from outside a. having now exposed a new set of reachable nodes  in the first stage  we proceed to identify further reachable nodes by performing the same process on edges from a1  and in this way produce sets. if node v has not been determined to be reachable by the end of stage t  then the probability that it is determined to be reachable in stage t + 1 is equal to the chance that its live edge comes from at   at 1  given that its live edge has not come from
                           pu﹋at at 1 bv u any of the earlier sets. but this is   which is the
1   pu﹋at 1 bv u
same as in the linear threshold process of the previous paragraph. thus  by induction over these stages  we see that the live-edge process produces the same distribution over active sets as the linear threshold process.	
influence maximization is hard in this model as well.
﹛theorem 1. the influence maximization problem is np-hard for the linear threshold model.
proof. consider an instance of the np-complete vertex cover problem defined by an undirected n-node graph g =  v e  and an integer k; we want to know if there is a set s of k nodes in g so that every edge has at least one endpoint in s. we show that this can be viewed as a special case of the influence maximization problem.
﹛given an instance of the vertex cover problem involving a graph g  we define a corresponding instance of the influence maximization problem by directing all edges of g in both directions. if there is a vertex cover s of size k in g  then one can deterministically make 考 a  = n by targeting the nodes in the set a = s; conversely  this is the only way to get a set a with 考 a  = n. 
﹛in the proofs of both the approximation theorems in this section  we established submodularity by considering an equivalent process in which each node  hard-wired  certain of its incident edges as transmitting influence from neighbors. this turns out to be a proof technique that can be formulated in general terms  and directly applied to give approximability results for other models as well. we discuss this further in the context of the general framework presented in section 1.
1. experiments
﹛in addition to obtaining worst-case guarantees on the performance of our approximation algorithm  we are interested in understanding its behavior in practice  and comparing its performance to other heuristics for identifying influential individuals. we find that our greedy algorithm achieves significant performance gains over several widely-used structural measures of influence in social networks .
the network data. for evaluation  it is desirable to use a network dataset that exhibits many of the structural features of large-scale social networks. at the same time  we do not address the issue of inferring actual influence parameters from network observations  see e.g.  1  1  . thus  for our testbed  we employ a collaboration graph obtained from co-authorships in physics publications  with simple settings of the influence parameters. it has been argued extensively that co-authorship networks capture many of the key features of social networks more generally . the co-authorship data was compiled from the complete list of papers in the highenergy physics theory section of the e-print arxiv  www.arxiv.org .1
﹛the collaboration graph contains a node for each researcher who has at least one paper with co-author s  in the arxiv database. for each paper with two or more authors  we inserted an edge for each pair of authors  single-author papers were ignored . notice that this results in parallel edges when two researchers have co-authored multiple papers - we kept these parallel edges as they can be interpreted to indicate stronger social ties between the researchers involved. the resulting graph has 1 nodes  and edges between about 1 pairs of nodes.

1
 we also ran experiments on the co-authorship graphs induced by theoretical computer science papers. we do not report on the results here  as they are very similar to the ones for high-energy physics.
﹛while processing the data  we corrected many common types of mistakes automatically or manually. in order to deal with aliasing problems at least partially  we abbreviated first names  and unified spellings for foreign characters. we believe that the resulting graph is a good approximation to the actual collaboration graph  the sheer volume of data prohibits a complete manual cleaning pass .
the influence models. we compared the algorithms in three different models of influence. in the linear threshold model  we treated the multiplicity of edges as weights. if nodes u v have cu v parallel edges between them  and degrees du and dv  then the edge  u v  has weight cdu vv   and the edge  v u  has weight c.
﹛in the independent cascade model  we assigned a uniform probability of p to each edge of the graph  choosing p to be 1% and 1% in separate trials. if nodes u and v have cu v parallel edges  then we assume that for each of those cu v edges  u has a chance of p to activate v  i.e. u has a total probability of 1    1   p cu v of activating v once it becomes active.
﹛the independent cascade model with uniform probabilities p on the edges has the property that high-degree nodes not only have a chance to influence many other nodes  but also to be influenced by them. whether or not this is a desirable interpretation of the influence data is an application-specific issue. motivated by this  we chose to also consider an alternative interpretation  where edges into high-degree nodes are assigned smaller probabilities. we study a special case of the independent cascade model that we term  weighted cascade   in which each edge from node u to v is assigned probability 1/dv of activating v. the weighted cascade model resembles the linear threshold model in that the expected number of neighbors who would succeed in activating a node v is 1 in both models.
the algorithms and implementation. we compare our greedy algorithm with heuristics based on nodes' degrees and centrality within the network  as well as the crude baseline of choosing random nodes to target. the degree and centrality-based heuristics are commonly used in the sociology literature as estimates of a node's influence .
﹛the high-degree heuristic chooses nodes v in order of decreasing degrees dv. considering high-degree nodes as influential has long been a standard approach for social and other networks  1  1   and is known in the sociology literature as  degree centrality .
﹛ distance centrality  is another commonly used influence measure in sociology  building on the assumption that a node with short paths to other nodes in a network will have a higher chance of influencing them. hence  we select nodes in order of increasing average distance to other nodes in the network. as the arxiv collaboration graph is not connected  we assigned a distance of n - the number of nodes in the graph - for any pair of unconnected nodes. this value is significantly larger than any actual distance  and thus can be considered to play the role of an infinite distance. in particular  nodes in the largest connected component will have smallest average distance.
﹛finally  we consider  as a baseline  the result of choosing nodes uniformly at random. notice that because the optimization problem is np-hard  and the collaboration graph is prohibitively large  we cannot compute the optimum value to verify the actual quality of approximations.
﹛both in choosing the nodes to target with the greedy algorithm  and in evaluating the performance of the algorithms  we need to compute the value 考 a . it is an open question to compute this quantity exactly by an efficient method  but very good estimates can be obtained by simulating the random process. more specifically  we simulate the process 1 times for each targeted set  re-choosing thresholds or edge outcomes pseudo-randomly from  1  every time. previous runs indicate that the quality of approximation after 1 iterations is comparable to that after 1 or more iterations.
the results. figure 1 shows the performance of the algorithms in the linear threshold model. the greedy algorithm outperforms the high-degree node heuristic by about 1%  and the central node heuristic by over 1%.  as expected  choosing random nodes is not a good idea.  this shows that significantly better marketing results can be obtained by explicitly considering the dynamics of information in a network  rather than relying solely on structural properties of the graph.

figure 1: results for the linear threshold model
﹛when investigating the reason why the high-degree and centrality heuristics do not perform as well  one sees that they ignore such network effects. in particular  neither of the heuristics incorporates the fact that many of the most central  or highest-degree  nodes may be clustered  so that targeting all of them is unnecessary. in fact  the uneven nature of these curves suggests that the network influence of many nodes is not accurately reflected by their degree or centrality.
﹛figure 1 shows the results for the weighted cascade model. notice the striking similarity to the linear threshold model. the scale is slightly different  all values are about 1% smaller   but the behavior is qualitatively the same  even with respect to the exact nodes whose network influence is not reflected accurately by their degree or centrality. the reason is that in expectation  each node is influenced by the same number of other nodes in both models  see section 1   and the degrees are relatively concentrated around their expectation of 1.
﹛the graph for the independent cascade model with probability 1%  given in figure 1  seems very similar to the previous two at first glance. notice  however  the very different scale: on average  each targeted node only activates three additional nodes. hence  the network effects in the independent cascade model with very small probabilities are much weaker than in the other models. several nodes have degrees well exceeding 1  so the probabilities on their incoming edges are even smaller than 1% in the weighted cascade model. this suggests that the network effects observed for the linear threshold and weighted cascade models rely heavily on low-degree nodes as multipliers  even though targeting high-degree

figure 1: results for the weighted cascade model
nodes is a reasonable heuristic. also notice that in the independent cascade model  the heuristic of choosing random nodes performs significantly better than in the previous two models.

figure 1: independent cascade model with probability 1%
﹛the improvement in performance of the  random nodes  heuristic is even more pronounced for the independent cascade model with probabilities equal to 1%  depicted in figure 1. in that model  it starts to outperform both the high-degree and the central nodes heuristics when more than 1 nodes are targeted. it is initially surprising that random targeting for this model should lead to more activations than centrality-based targeting  but in fact there is a natural underlying reason that we explore now.
﹛the first targeted node  if chosen somewhat judiciously  will activate a large fraction of the network  in our case almost 1%. however  any additional nodes will only reach a small additional fraction of the network. in particular  other central or high-degree nodes are very likely to be activated by the initially chosen one  and thus have hardly any marginal gain. this explains the shapes of the curves for the high-degree and centrality heuristics  which leap up to about 1 activated nodes  but make virtually no progress afterwards. the greedy algorithm  on the other hand  takes the effect of the first chosen node into account  and targets nodes with smaller

figure 1: independent cascade model with probability 1%
marginal gain afterwards. hence  its active set keeps growing  although at a much smaller slope than in other models.
﹛the random heuristic does not do as well initially as the other heuristics  but with sufficiently many attempts  it eventually hits some highly influential nodes and becomes competitive with the centrality-based node choices. because it does not focus exclusively on central nodes  it eventually targets nodes with additional marginal gain  and surpasses the two centrality-based heuristics.
1. ageneralframeworkforinfluence maximization
general threshold and cascade models. we have thus far been considering two specific  widely studied models for the diffusion of influence. we now propose a broader framework that simultaneously generalizes these two models  and allows us to explore the limits of models in which strong approximation guarantees can be obtained. our general framework has equivalent formulations in terms of thresholds and cascades  thereby unifying these two views of diffusion through a social network.
  a general threshold model. we would like to be able to express the notion that a node v's decision to become active can be based on an arbitrary monotone function of the set of neighbors of v that are already active. thus  associated with v is a monotone threshold function fv that maps subsets of v's neighbor set to real numbers in  1   subject to the condition that fv    = 1. the diffusion process follows the general structure of the linear threshold model. each node v initially chooses 牟v uniformly at random from the interval  1 . now  however  v becomes active in step t if fv s  ≡ 牟v  where s is the set of neighbors of v that are active in step t   1. note that the linear threshold model is the special case in which each threshold function has the form fv s  = pu﹋s bv u for parameters bv u such that x bv u ≒ 1.
u neighbor of v
  a general cascade model. we generalize the cascade model to allow the probability that u succeeds in activating a neighbor v to depend on the set of v's neighbors that have already tried. thus  we define an incremental function pv u s  ﹋  1   where s and {u} are disjoint subsets of v's neighbor
set. a general cascade process works by analogy with the independent case: in the general case  when u attempts to activate v  it succeeds with probability pv u s   where s is the set of neighbors that have already tried  and failed  to activate v. the independent cascade model is the special case where pv u s  is a constant pu v  independent of s. we will only be interested in cascade models defined by incremental functions that are order-independent in the following sense: if neighbors u1 u1 ... u` try to activate v  then the probability that v is activated at the end of these ` attempts does not depend on the order in which the attempts are made.
﹛these two models are equivalent  and we give a method to convert between them. first  consider an instance of the general threshold model with threshold functions fv. to define an equivalent cascade model  we need to understand the probability that an additional neighbor u can activate v  given that the nodes in a set s have already tried and failed. if the nodes in s have failed  then node v's threshold 牟v must be in the range 牟v ﹋  fv s  1 . however  subject to this constraint  it is uniformly distributed.thus  the probability that a neighbor u ﹋/ s succeeds in activating v  given that the nodes in s have failed  is
.
it is not difficult to show that the cascade process with these functions is equivalent to the original threshold process.
﹛conversely  consider a node v in the cascade model  and a set s = {u1 ... uk} of its neighbors. assume that the nodes in s try to activate v in the order u1 ... uk  and let si = {u1 ... ui}. then the probability that v is not activated by this process is by definition qki=1   pv ui si 1  . recall that we assumed that the order in which the ui try to activate v does not affect their overall success probability. hence  this value depends on the set s only  and we can define fv s  = 1   qki=1   pv ui si 1  . analogously  one can show that this instance of the threshold model is equivalent to the original cascade process.
an inapproximabilityresult. the general model proposed above includes large families of instances for which the influence function 考 ﹞  is not submodular. indeed  it may become np-hard to approximate the optimization problem to within any non-trivial factor.
﹛theorem 1. in general  it is np-hard to approximate the influence maximization problem to within a factor of n1 汍  for any 汍   1.
proof. to prove this result  we reduce from the set cover problem. we start with the construction from the proof of theorem 1  letting u1 ... un denote the nodes corresponding to the n elements; i.e. ui becomes active when at least one of the nodes corresponding to sets containing ui is active. next  for an arbitrarily large constant c  we add n = nc more nodes x1 ... xn; each xj is connected to all of the nodes ui  and it becomes active only when all of the ui are.
﹛if there are at most k sets that cover all elements  then activating the nodes corresponding to these k sets will activate all of the nodes ui  and thus also all of the xj. in total  at least n + n + k nodes will be active. conversely  if there is no set cover of size k  then no targeted set will activate all of the ui  and hence none of the xj will become active  unless targeted . in particular  fewer than n+k nodes are active in the end. if an algorithm could approximate the problem within n1 汍 for any 汍  it could distinguish between the cases where n + n + k nodes are active in the end  and where fewer than n + k are. but this would solve the underlying instance of set cover  and therefore is impossible assuming p 1= np. 
﹛note that our inapproximability result holds in a very simple model  in which each node is  hard-wired  with a fixed threshold.
exploring the boundaries of approximability. thus  the general threshold and cascade models are too broad to allow for nontrivial approximation guarantees in their full generality. at the same time  we have seen that the greedy algorithm achieves strong guarantees for some of the main special cases in the social networks literature. how far can we extend these approximability results 
﹛we can generalize the proof technique used in theorems 1 and 1 to a model that is less general  and also less natural  than the general threshold and cascade models; however  it includes our special cases from section 1  and every instance of this model will have a submodular influence function. the model is as follows.
  the triggering model. each node v independently chooses a random  triggering set  tv according to some distribution over subsets of its neighbors. to start the process  we target a set a for initial activation. after this initial iteration  an inactive node v becomes active in step t if it has a neighbor in its chosen triggering set tv that is active at time t   1.  thus  v's threshold has been replaced by a latent subset of tv of neighbors whose behavior actually affects v. 
it is useful to think of the triggering sets in terms of  live  and  blocked  edges: if node u belongs to the triggering set tv of v  then we declare the edge  u v  to be live  and otherwise we declare it to be blocked. as in the proofs of theorems 1 and 1  a node v is activated in an instance of the triggering model if and only if there is a live-edge path from the initially targeted set a to v. following the arguments in these proofs  we obtain the following
﹛theorem 1. in every instance of the triggering model  the influence function 考 ﹞  is submodular.
﹛beyond the independent cascade and linear threshold  there are other natural special cases of the triggering model. one example is the  only-listen-once  model. here  each node v has a parameter pv so that the first neighbor of v to be activated causes v to become active with probability pv  and all subsequent attempts to activate v deterministically fail.  in other words  v only listens to the first neighbor that tries to activate it.  this process has an equivalent formulation in the triggering set model  with an edge distribution defined as follows: for any node v  the triggering set tv is either the entire neighbor set of v  with probability pv   or the empty set otherwise. as a result  the influence function in the only-listen-once model is also submodular  and we can obtain a  1   1/e   汍 -approximation here as well.
﹛however  we can show that there exist models with submodular influence functions that do not have equivalent formulations in terms of triggering sets  so it makes sense to seek further models in which submodularity holds.
﹛one tractable special case of the cascade model is based on the natural restriction that the probability of a node u influencing v is non-increasing as a function of the set of nodes that have previously tried to influence v. in terms of the cascade model  this means that pv u s  ≡ pv u t  whenever s   t. we say that a process satisfying these conditions is an instance of the decreasing cascade model. although there are natural decreasing cascade instances that have no equivalent formulation in terms of triggering sets  we can show by a more intricate analysis that every instance of the decreasing cascade model has a submodular influence function. we will include details of this proof in the full version of the paper.
a conjecture. finally  we state an appealing conjecture that would include all the approximability results above as special cases.
﹛conjecture 1. whenever the threshold functions fv at every node are monotone and submodular  the resulting influence function 考 ﹞  is monotone and submodular as well.
﹛it is not difficult to show that every instance of the triggering model has an equivalent formulation with submodular node thresholds. every instance of the decreasing cascade model has such an equivalent formulation as well; in fact  the decreasing cascade condition stands as a very natural special case of the conjecture  given that it too is based on a type of  diminishing returns.  when translated into the language of threshold functions  we find that the decreasing cascade condition corresponds to the following natural requirement:
 
whenever s   t and u ﹋/ t. this is in a sense a  normalized submodularity  property; it is stronger than submodularity  which would consist of the same inequality on just the numerators.  note that by monotonicity  the denominator on the left is larger. 
1. non-progressive processes
﹛we have thus far been concerned with the progressive case  in which nodes only go from inactivity to activity  but not vice versa. the non-progressive case  in which nodes can switch in both directions  can in fact be reduced to the progressive case.
﹛the non-progressive threshold process is analogous to the progressive model  except that at each step t  each node v chooses a new value 牟v t  uniformly at random from the interval  1 . node v will be active in step t if fv s  ≡ 牟v t   where s is the set of neighbors of v that are active in step t   1.
﹛from the perspective of influence maximization  we can ask the following question. suppose we have a non-progressive model that is going to run for 而 steps  and during this process  we are allowed to make up to k interventions: for a particular node v  at a particular time t ≒ 而  we can target v for activation at time t.  v itself may quickly de-activate  but we hope to create a large  ripple effect.   which k interventions should we perform  simple examples show that to maximize influence  one should not necessarily perform all k interventions at time 1; e.g.  g may not even have k nodes.
﹛let a be a set of k interventions. the influence of these k interventions 考 a  is the sum  over all nodes v  of the number of time steps that v is active. the influence maximization problem in the non-progressive threshold model is to find the k interventions with maximum influence.
﹛we can show that the non-progressive influence maximization problem reduces to the progressive case in a different graph. given a graph g =  v e  and a time limit 而  we build a layered graph g而 on 而 ﹞ |v | nodes: there is a copy vt for each node v in g  and each time-step t ≒ 而. we connect each node in this graph with its neighbors in g indexed by the previous time step.
﹛theorem 1. the non-progressive influence maximization problem on g over a time horizon 而 is equivalent to the progressive influence maximization problem on the layered graph g而. node v is active at time t in the non-progressive process if and only if vt is activated in the progressive process.
﹛thus  models where we have approximation algorithms for the progressive case carry over. theorem 1 also implies approximation results for certain non-progressive models used by asavathiratham et al. to model cascading failures in power grids  1  1 .
﹛note that the non-progressive model discussed here differs from the model of domingos and richardson  1  1  in two ways. we are concerned with the sum over all time steps t ≒ 而 of the expected number of active nodes at time t  for a given a time limit 而  while  1  1  study the limit of this process: the expected number of nodes active at time t as t goes to infinity. further  we consider interventions for a particular node v  at a particular time t ≒ 而  while the interventions considered by  1  1  permanently affect the activation probability function of the targeted nodes.
1. general marketing strategies
﹛in the formulation of the problem  we have so far assumed that for one unit of budget  we can deterministically target any node v for activation. this is clearly a highly simplified view. in a more realistic scenario  we may have a number m of different marketing actions mi available  each of which may affect some subset of nodes by increasing their probabilities of becoming active  without necessarily making them active deterministically. the more we spend on any one action the stronger its effect will be; however  different nodes may respond to marketing actions in different ways 
﹛in a general model  we choose investments xi into marketing actions mi  such that the total investments do not exceed the budget. a marketing strategy is then an m-dimensional vector x of investments. the probability that node v will become active is determined by the strategy  and denoted by hv x . we assume that this function is non-decreasing and satisfies the following  diminishing returns  property for all x ≡ y and a ≡ 1  where we write x ≡ y or a ≡ 1 to denote that the inequalities hold in all coordinates :
	hv x + a    hv x 	≒	hv y + a    hv y 	 1 
﹛intuitively  inequality  1  states that any marketing action is more effective when the targeted individual is less  marketing-saturated  at that point.
﹛we are trying to maximize the expected size of the final active set. as a function of the marketing strategy x  each node v becomes active independently with probability hv x   resulting in a  random  set of initial active nodes a. given the initial set a  the expected size of the final active set is 考 a . the expected revenue of the marketing strategy x is therefore g x  = pa v 考 a  ﹞ qu﹋a hu x  ﹞ qv/﹋a 1   hv x  .
﹛in order to  approximately  maximize g  we assume that we can evaluate the function at any point x approximately  and find a direction i with approximately maximal gradient. specifically  let ei denote the unit vector along the ith coordinate axis  and 汛 be some constant. we assume that there exists some 污 ≒ 1 such that we can find an i with g x + 汛 ﹞ ei  g x  ≡ 污﹞ g x + 汛 ﹞ ej  g x   for each j. we divide each unit of the total budget k into equal parts of size 汛. starting with an all-1 investment  we perform an approximate gradient ascent  by repeatedly  a total of k汛 times  adding 汛 units of budget to the investment in the action mi that approximately maximizes the gradient.
﹛the proof that this algorithm gives a good approximation consists of two steps. first  we show that the function g we are trying to optimize is non-negative  non-decreasing  and satisfies the  diminishing returns  condition  1 . second  we show that the hillclimbing algorithm gives a constant-factor approximation for any function g with these properties. the latter part is captured by the following theorem.
﹛theorem 1. when the hill-climbing algorithm finishes with strategy x  it guarantees that  where x  denotes the optimal solution subject to pi x i ≒ k.
﹛the proof of this theorem builds on the analysis used by nemhauser et al.   and we defer it to the full version of this paper.
﹛with theorem 1 in hand  it remains to show that g is nonnegative  monotone  and satisfies condition  1 . the first two are clear  so we only sketch the proof of the third. fix an arbitary ordering of vertices. we then use the fact that for any ai bi 
	yai   ybi	= x ai   bi  ﹞ yaj ﹞ ybj 	 1 
	i	i	i	j i	j i
and change the order of summation  to rewrite the difference

﹛to show that this difference is non-increasing  we consider y ≒ x. from the diminishing returns property of hu ﹞   we obtain that hu x + a  hu x  ≒ hu y + a  hu y . then  applying again equation  1   changing the order of summation  and performing some tedious calculations  writing   v x y  = hv x + a    hv y + a  if v   u  and   v x y  = hv x    hv y  if v   u 
we obtain that

﹛in this expression  all terms are non-negative  by monotonicity of the hv ﹞    with the exception of 考 a + {u v}    考 a + u    考 a + v +考 a   which is non-positive because 考 is submodular. hence  the above difference is always non-positive  so g satisfies the diminishing returns condition  1 .
