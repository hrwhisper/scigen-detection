gene ontology  go  is a controlled vocabulary. given a gene product  go enables scientists to clearly and unambiguously describe specific molecular functions of the gene product  specific biological processes in which it is involved  and specific cellular components to which it is localized. in this paper  we present our approach to identifying which papers have experimental evidence warranting annotation with go codes. the training data set contains 1 relevant full-text articles and 1 irrelevant ones  and the test data set contains 1 positive full-text articles and 1 negative ones. we regarded this problem as a binary classification problem  and employed support vector machines  svms  to distinguish positive articles from negative ones. title  abstract  figure/table captions  and three standard sections - results  discussion  and conclusion were the targets of feature extraction. without incorporating mesh  medical subject headings  terms as part of the features  our system achieved 1 in normalized utility measure. 
1 introduction 
gene ontology  go   is a system of keywords hierarchically organized as a directed acyclic graph with three main categories - biological process  cellular component  and molecular function. it provides a unified set of terms for the annotation of gene products in different organisms. the assignment of a go term requires supporting evidence. the main source of evidence comes from published biomedical articles which contain accurate and reliable experimental results. usually  curators have to read newly published papers to update their databases  and obviously they can hardly keep up with the rapidly growing number of new biomedical papers. 
   efforts have been made to automatically annotate proteins with go terms based on analysis of biomedical literature  1  1  1 . none of these works  however  exploited full-text articles  which have been shown to contain more information than abstracts . the biocreative workshop 1  initiated a go annotation task  and provided 1 full-text documents from the journal of biological chemistry. the evaluation was manually done by curators  and the overall results showed low performances which indicated a long way from practical application. therefore  the categorization task in trec 1 genomics track  was simplified and limited to assignment of the three main categories. also  full-text documents from three journals over two years were provided in this task. 
   the categorization task tried to mimic two of the classification activities carried out by human annotators in the mouse genome informatics  mgi   system: a triage task and two variants of mgi's annotation task. curators at mgi employ a three-step process to identify the papers most likely to describe gene function. first  articles from several hundred journals are searched for keywords mouse  mice  or murine. second  confronted with articles from the first step  curators determine which articles should be sent for curation. the goal for this triage process is to limit the number of articles for more exhaustive analysis. finally  curators identify genes for which there are experimental evidence supporting assignment of go terms. 
   because of limited resources and time constraints  we did only the triage subtask. the goal of this task is to correctly identify which papers have been deemed to have experimental evidence warranting annotation of certain go codes. since this task can readily be regarded as a binary classification problem  we employed support vector machines  svms    which are especially suitable for binary classification problems. 
   feature extraction is the key to successful classification in the machine learning approach  and is even more important than the underlying classification algorithm. when it comes to text categorization  the simple bag-of-words representation is often the first choice. however  the performance of cross validation on this simple approach was only about 1 in normalized utility  nu  measure . therefore  we tried some other representations and settled down to the one adopted in this paper. first  we obtained a list of go terms  annotated to mgi markers. then  a document was represented by the similarities to those go terms. the details of this approach are discussed in the section 1. 
   the rest of this paper is organized as follows. section 1 presents the overview of our architecture. the basic idea and the experimental methods in this study are introduced in section 1. section 1 shows the results and makes some discussions. finally  section 1 concludes the remarks and suggests some future direction. 

figure 1: system overview.
1 architecture overview 
figure 1 shows the overall architecture of our method for the triage task. first  we obtained a list of n go terms from the mgi website . with m training documents available  we extracted a list of keywords for each document. each document was then represented by the vector sum of similarity between each keyword and the list of n go terms  forming an n-element vector. detail of the conversion process is explained in section 1. a svm classifier was trained with the resulting m n-element vectors. given a test document  it was converted to an n-element vector through the same process performed on those training documents. afterwards  this test instance was sent to the trained svm classifier to decide whether it is relevant or not. 
 
1 methods 
1 document preprocessing 
before performing classification  two document preprocessing operations were performed to extract more information from the full-text documents. the two operations were  1  acronym expansion and  1  keyword extraction. acronym expansion 
once the combination of sections was decided  which is explained in section 1  an operation was performed to substitute the long forms for the tagged acronyms  each of which referred to a glossary entry in the document. the reason for this operation is that acronyms are sometimes ambiguous  and their long forms obviously carry more information. an example of this operation is shown in figure 1.  in figure 1  an abbreviation  ip1  will be replaced with  inositol 

trisphosphase  ip1  . 
 
it is presently unclear how these receptors could selectively mediate camp responses to sugars and  glosref 
rid= g1  ip inf 1 /inf  /glosref  responses to artificial sweeteners. 
it is presently unclear how these receptors could selectively mediate camp responses to sugars and inositol trisphosphate  ip inf 1 /inf   responses to artificial sweeteners. 

figure 1: an example of acronym operation. 
keyword extraction 
after the acronym operation  the remaining sgml tags were removed from the documents for later keyword extraction. with the 1k-entry inflection table found in umls knowledge sources   the keywords were normalized and extracted from each document.  in this step  only words that occur in both the inflection table and the document were extracted and normalized. the normalization here refers to the transformation of words to their root forms. for example  a verb in the past tense like  demonstrated  is normalized to its base form  demonstrate . moreover  a plural noun like  receptors  is normalized to its singular form  receptor . then  stop words were removed in the next stage. an example of keyword extraction is shown in figure 1. the upper left part of figure 1 contains the target document for keyword extraction. the lower left part illustrates the inflection table found in umls. the right part shows the extracted and normalized list of keywords. 
the study demonstrated a decreased level of glucocorticoid the receptors   gr   in peripheral blood lymphocytes from study 
hypercholesterolemic subjects   and an elevated level in patients 	demonstrate a
with acute myocardial infarction. 	decrease 
level of
demonstrated | verb | demonstrate 	glucocorticoid 
                                                     receptor decreased | verb | decrease 	gr glucocorticoid | noun | glucocorticoid 	in receptors | noun | receptor 	peripheral blood lymphocyte
peripheral blood lymphocytes | noun | peripheral blood lymphocyte from hypercholesterolemic | adj | hypercholesterolemic hypercholesterolemic 
elevated | verb | elevate 	subject 
myocardial infarction | noun | myocardial infarction 	...  	...	...
	...	acute 
myocardial infarction 
figure 1: an example of keyword extraction and normalization. 
1 feature extraction 
under the bag-of-words representation  the feature vector of an article was 1k long and the resulting dataset severely suffered from the data sparseness problem. therefore  we attempted to solve this problem by reducing the dimension of feature vector via a list of n go terms. given a keyword  the similarity vector vk for this keyword is the similarity between this keyword and the n go terms. the feature vector vd for a document is therefore the vector sum of the similarity vectors of all keywords it contains.  in our study  classic dice  cd  coefficient was adopted as the similarity measure  and stop words were ignored at this stage.  the formulas of computing the cd coefficient and similarity values are listed below. 
 
cd ab      =  1  z x y /  +    where a  b are two strings  x is the number of tokens in a  y is the number of tokens in b  and z is the number of tokens occurring in both a and b.
vk = sim wk  t  = cd w t  k   1  cd w t  k   1  ... cd w t  k   n   t   where wk is a keyword and t is the vector of n go terms  t1  t1 and tn is the first  second and nth go term  respectively. 
vd = vk   where d is a document. 
k d
1 exploitation of full text documents 
which sections of an article should be the targets of feature extraction is also an important issue.  
in other words  we have to find out where the experimental evidence warranting annotation with go codes resides in a document. the triage subtask is very much similar to the task 1 in kdd cup 1   part of whose goal is to retrieve papers meeting the flybase  gene-expression curation criteria. it was found in this competition that besides the title and abstract  much of the experimental evidence is contained in the figure captions. hence  we started from the combination of the title  abstract  figure captions and table captions as the base combination  and gradually included some other sections of the article to the base combination. two other combinations were therefore constructed. unfortunately  none of them outperformed the base combination under the aforementioned feature extraction method. these two combinations are briefly described below. 
   some types of documents do not have the abstract part  and hence in this case  the body of the article is added to the simple combination  forming the second combination.  in other words  if the abstract is absent  the body of the article is added to form the second combination. for the third one  if the abstract is present  the result  discussion  and conclusion sections are included.  while these sections are intuitively evidence-rich sections  the third combination did not stand out as expected. 
1 svm classification 
the software package libsvm  was employed to deal with svm-related operations.  radial basis function was adopted as the kernel function  and 1-fold cross validation was performed to select the model attaining the highest normalized utility  i.e.  the best-performing parameters - c and gamma  which are the penalty constant in optimization and the parameter for radial basis kernel  respectively. under our feature extraction method  the selected values for c and gamma are around 1 and 1-1. another issue worth addressing is the imbalance among the number of training and test examples. for the training data  there are 1 positive examples and 1 negative examples  the ratio of which is about 1 to 1. therefore  we tried to put more weights on the positive examples  i.e.  the positive examples received larger c in svm training. as expected  setting cpositive to 1cnegative achieved the best performance in our cross validation. 
1 normalization versus stemming 
due to time constrains  some of the methods simply followed our intuition without further verification or experience backup. the one in which we were interested the most is using normalization instead of stemming  which is a usual preprocessing operation in text categorization. unlike stemming  normalization is more precise because it converts words to their base forms without losing too much information. as expected  further experiment displayed a 1 nu drop in the stemmed version. 
1 results and discussions 
table 1 lists the results of our three official runs  the results of two other top-performing teams  and the results of the median-performing run and the worst-performing run. as mentioned in section 1  ntu1n1 used the base combination  ntu1n1 used the second combination  and ntu1n1 used the third combination. it seems that adding other sections besides the title  abstract  and captions introduced more noise and less useful information. the results may be explained by schuemie et al.'s finding  that the information density is higher in the abstract than in all the other four standard sections - introduction  methods  results and discussion. therefore  some filtering techniques should be applied to these four sections to remove non-informative and noisy contents. 
   the official run dimacstfl1d was produced by dayanik et al.'s system  which attained the best performance. besides the title and abstract of an article  they used the mesh terms attached to the article as the target of feature extraction. bayesian logistic regression was adopted to perform classification. they also performed an interesting experiment which depended only on the mesh term  mice  to make the decision  and found that using this term alone can outperform all the other systems. fujita's system  achieved slightly lower performance of 1 nu  pllsgen1 . they used terms from full text  gene entities and mesh terms as the targets of feature extraction  and used svm as the classifier. it is obvious that mesh terms played an important role in distinguishing positive documents from negative ones  especially the term  mice . since this task aimed to assist curators at mgi  it is reasonable that articles attached with the mesh term  mice  are very likely to be positive. therefore  we can ascribe the high performance of these two systems to the use of mesh terms. 
   as there are 1 positive examples and 1 negative examples in the training/test dataset  the curators at mgi will have to read about 1 papers to find a positive one if they do not get any hints in advance.  for the best official run  dimacstfl1d   curators will have to read roughly 1 papers to find one useful  and around 1 percent of the positive papers can be retrieved.  using our approach  curators will have to read roughly 1 papers to find one useful and only 1 percent of the positive papers can be retrieved.  to put it in another way  the best run reduced from 1 to 1 the number of papers that the curators have to read to get a positive one  losing 1 percent of the useful papers.  in our opinion  the best official run greatly alleviated the burden of curators  and our approach didn't seem to help a lot.  however  it is possible to combine our approach with others  making the filtering job even more effective. 
table 1: results of official runs in the triage task. 
 normalized utilityf-scorerecallprecisiondimacstfl1d 1 111  mice  run 1 111 pllsgen1 1 11 1 ntu1n1 1 111 ntu1n1 1 111 median 1 11 1 ntu1n1.1 111 worst 1 111 1 conclusion 
we demonstrate our approach based on a list of go terms in this paper. we tried three combinations of sections in an article as the target of feature extraction  and found the simplest one most useful. we hypothesize that filtering should be applied to sections other than abstract before they can be used for feature extraction. also  we found that normalization is a better preprocessing operation than stemming under our feature extraction approach. 
   without the use of mesh terms  our system performed slightly better than the median-performing system. with our approach  1 percent of positive papers were retained at the precision rate of 1 percent. although we didn't achieve the best performance  it is possible to incorporate other ideas into our method  and combine other types of features with the existing ones. 
