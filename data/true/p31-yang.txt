this paper examines a new approach to information distillation over temporally ordered documents  and proposes a novel evaluation scheme for such a framework. it combines the strengths of and extends beyond conventional adaptive filtering  novelty detection and non-redundant passage ranking with respect to long-lasting information needs  'tasks' with multiple queries . our approach supports fine-grained user feedback via highlighting of arbitrary spans of text  and leverages such information for utility optimization in adaptive settings. for our experiments  we defined hypothetical tasks based on news events in the tdt1 corpus  with multiple queries per task. answer keys  nuggets  were generated for each query and a semiautomatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. we also propose an extension of the ndcg metric for assessing the utility of ranked passages as a combination of relevance and novelty. our results show encouraging utility enhancements using the new approach  compared to the baseline systems without incremental learning or the novelty detection components.
categories and subject descriptors
h.1  information search and retrieval : information filtering  relevance feedback  retrieval models  selection process; i.1
general terms
design  measurement  performance  experimentation.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
keywords
utility-based distillation  unified framework  adaptive filtering  novelty detection  passage ranking  flexible user feedback  evaluation methodology.
1. introduction
　tracking new and relevant information from temporal data streams for users with long-lasting needs has been a challenging research topic in information retrieval. adaptive filtering  af  is one such task of online prediction of the relevance of each new document with respect to pre-defined topics. based on the initial query and a few positive examples  if available   an af system maintains a profile for each such topic of interest  and constantly updates it based on feedback from the user. the incremental learning nature of af systems makes them more powerful than standard search engines that support ad-hoc retrieval  e.g. google and yahoo  in terms of finding relevant information with respect to long-lasting topics of interest  and more attractive for users who are willing to provide feedback to adapt the system towards their specific information needs  without having to modify their queries manually.
　a variety of supervised learning algorithms  rocchio-style classifiers  exponential-gaussian models  local regression and logistic regression approaches  have been studied for adaptive settings  examined with explicit and implicit relevance feedback  and evaluated with respect to utility optimization on large benchmark data collections in trec  text retrieval conferences  and tdt  topic detection and tracking  forums  1  1  1  1  1  1  1  1 . regularized logistic regression  has been found representative for the state-of-the-art approaches  and highly efficient for frequent model adaptations over large document collections such as the trec-1 corpus  over 1 documents and 1 topics . despite substantial achievements in recent adaptive filtering research  significant problems remain unsolved regarding how to leverage user feedback effectively and efficiently. specifically  the following issues may seriously limit the true utility of af systems in real-world applications:
1. user has a rather 'passive' role in the conventional adaptive filtering setup - he or she reacts to the system only when the system makes a 'yes' decision on a document  by confirming or rejecting that decision. a more 'active' alternative would be to allow the user to issue multiple queries for a topic  review a ranked list of candidate documents  or passages  per query  and provide feedback on the ranked list  thus refining their information need and requesting updated ranked lists. the latter form of user interaction has been highly effective in standard retrieval for ad hoc queries. how to deploy such a strategy for long-lasting information needs in af settings is an open question for research.
1. the unit for receiving a relevance judgment  'yes' or
'no'  is restricted to the document level in conventional af. however  a real user may be willing to provide more informative  fine-grained feedback via highlighting some pieces of text in a retrieved document as relevant  instead of labeling the entire document as relevant. effectively leveraging such fine-grained feedback could substantially enhance the quality of an af system. for this  we need to enable supervised learning from labeled pieces of text of arbitrary span instead of just allowing labeled documents.
1. system-selected documents are often highly redundant. a major news event  for example  would be reported by multiple sources repeatedly for a while  making most of the information content in those articles redundant with each other. a conventional af system would select all these redundant news stories for user feedback  wasting the user's time while offering little gain. clearly  techniques for novelty detection can help in principle  1  1  1  for improving the utility of the af systems. however  the effectiveness of such techniques at passage level to detect novelty with respect to user's  fine-grained  feedback and to detect redundancy in ranked lists remains to be evaluated using a measure of utility that mimics the needs of a real user.
　to address the above limitations of current af systems  we propose and examine a new approach in this paper  combining the strengths of conventional af  incremental learning of topic models   multi-pass passage retrieval for long-lasting queries conditioned on topic  and novelty detection for removal of redundancy from user interactions with the system. we call the new process utility-based information distillation.
　note that conventional benchmark corpora for af evaluations  which have relevance judgments at the document level and do not define tasks with multiple queries  are insufficient for evaluating the new approach. therefore  we extended a benchmark corpus - the tdt1 collection of news stories and tv broadcasts - with task definitions  multiple queries per task  and answer keys per query. we have conducted our experiments on this extended tdt1 corpus and have made the additionally generated data publicly available for future comparative evaluations 1.
　to automatically evaluate the system-returned arbitrary spans of text using our answer keys  we further developed an evaluation scheme with semi-automatic procedure for acquiring rules that can match nuggets against system responses. moreover  we propose an extension of ndcg  normalized discounted cumulated gain   for assessing the utility of ranked passages as a function of both relevance and novelty.
　the rest of this paper is organized as follows. section 1 outlines the information distillation process with a concrete example. section 1 describes the technical cores of our system called caf＞e - cmu adaptive filtering engine. section 1 discusses issues with respect to evaluation methodology and proposes a new scheme. section 1 describes the extended tdt1 corpus. section 1 presents our experiments and results. section 1 concludes the study and gives future perspectives.
1. a sample task
　consider a news event - the escape of seven convicts from a texas prison in december 1 and their capture a month later. assuming a user were interested in this event since its early stage  the information need could be: 'find information about the escape of convicts from texas prison  and information related to their recapture'. the associated lower-level questions could be:
1. how many prisoners escaped 
1. where and when were they sighted 
1. who are their known contacts inside and outside the prison 
1. how are they armed 
1. do they have any vehicles 
1. what steps have been taken so far 
　we call such an information need a task  and the associated questions as the queries in this task. a distillation system is supposed to monitor the incoming documents  process them chunk by chunk in a temporal order  select potentially relevant and novel passages from each chunk with respect to each query  and present a ranked list of passages to the user. passage ranking here is based on how relevant a passage is with respect to the current query  how novel it is with respect to the current user history  of his or her interactions with the system   and how redundant it is compared to other passages with a higher rank in the list.
　when presented with a list of passages  the user may provide feedback by highlighting arbitrary spans of text that he or she found relevant. these spans of text are taken as positive examples in the adaptation of the query profile  and also added to the user's history. passages not marked by the user are taken as negative examples. as soon as the query profile is updated  the system re-issues a search and returns another ranked list of passages where the previously seen passages are either removed or ranked low  based on user preference. for example  if the user highlights '...officials have posted a $1 reward for their capture...' as relevant answer to the query  what steps have been taken so far    then the highlighted piece is used as an additional positive training example in the adaptation of the query profile. this piece of feedback is also added to the user history as a seen example  so that in future  the system will not place another passage mentioning '$1 reward' at the top of the ranked list. however  an article mentioning '...officials have doubled the reward money to $1...' might be ranked high since it is both relevant to the  updated  query profile and novel with respect to the  updated  user history. the user may modify the original queries or add a new query during the process; the query profiles will be changed accordingly. clearly  novelty detection is very important for the utility of such a system because of the iterative search. without novelty detection  the old relevant passages would be shown to the user repeatedly in each ranked list.
　through the above example  we can see the main properties of our new framework for utility-based information distillation over temporally ordered documents. our framework combines and extends the power of adaptive filtering  af   ad hoc retrieval  ir  and novelty detection  nd . compared to standard ir  our approach has the power of incrementally learning long-term information needs and modeling a sequence of queries within a task. compared to conventional af  it enables a more active role of the user in refining his or her information needs and requesting new results by allowing relevance and novelty feedback via highlighting of arbitrary spans of text in passages returned by the system.
　compared to past work  this is the first evaluation of novelty detection integrated with adaptive filtering for sequenced queries that allows flexible user feedback over ranked passages. the combination of af  ir and nd with the new extensions raises an important research question regarding evaluation methodology: how can we measure the utility of such an information distillation system  existing metrics in standard ir  af and nd are insufficient  and new solutions must be explored  as we will discuss in section 1  after describing the technical cores of our system in the next section.
1. technical cores
　the core components of caf＞e are - 1  af for incremental learning of query profiles  1  ir for estimating relevance of passages with respect to query profiles  1  nd for assessing novelty of passages with respect to user's history  and 1  anti-redundancy component to remove redundancy from ranked lists.
1 adaptive filtering component
　we use a state-of-the-art algorithm in the field - the regularized logistic regression method which had the best results on several benchmark evaluation corpora for af . logistic regression  lr  is a supervised learning algorithm for statistical classification. based on a training set of labeled instances  it learns a class model which can then by used to predict the labels of unseen instances. its performance as well as efficiency in terms of training time makes it a good candidate when frequent updates of the class model are required  as is the case in adaptive filtering  where the system must learn from each new feedback provided by the user.  see  and  for computational complexity and implementation issues .
　in adaptive filtering  each query is considered as a class and the probability of a passage belonging to this class corresponds to the degree of relevance of the passage with respect to the query. for training the model  we use the query itself as the initial positive training example of the class  and the user-highlighted pieces of text  marked as relevant or not-relevant  during feedback as additional training examples. to address the cold start issue in the early stage before any user feedback is obtained  the system uses a small sample from a retrospective corpus as the initial negative examples in the training set. the details of using logistic regression for adaptive filtering  assigning different weights to positive and negative training instances  and regularizing the objective function to prevent over-fitting on training data  are presented in .
　the class model w~  learned by logistic regression  or the query profile  is a vector whose dimensions are individual terms and whose elements are the regression coefficients  indicating how influential each term is in the query profile. the query profile is updated whenever a new piece of user feedback is received. a temporally decaying weight can be applied to each training example  as an option  to emphasize the most recent user feedback.
1 passage retrieval component
　we use standard ir techniques in this part of our system. incoming documents are processed in chunks  where each chunk can be defined as a fixed span of time or as a fixed number of documents  as preferred by the user. for each incoming document  corpus statistics like the idf  inverted document frequency  of each term are updated. we use a state-of-the-art named entity identifier and tracker  1  1  to identify person and location names  and merge them with co-referent named entities seen in the past. then the documents are segmented into passages  which can be a whole document  a paragraph  a sentence  or any other continuous span of text  as preferred. each passage is represented using a vector of tf-idf  term frequency- inverse document frequency  weights  where term can be a word or a named entity.
　given a query profile  i.e. the logistic regression solution w~  as described in section 1  the system computes the posterior probability of relevance for each passage ~x as
		 1 
　passages are ordered by their relevance scores  and the ones with scores above a threshold  tuned on a training set  comprise the relevance list that is passed on to the novelty detection step.
1 novelty detection component
　caf＞e maintains a user history h t   which contains all the spans of text hi that the user highlighted  as feedback  during his or her past interactions with the system  up to the current time t. denoting the history as
	 	 1 
the novelty score of a new candidate passage ~x is computed as:
		 1 
where both candidate passage x and highlighted spans of text hi are represented as tf-idf vectors.
　the novelty score of each passage is compared to a prespecified threshold  also tuned on a training set   and any passage with a score below this threshold is removed from the relevance list.
1 anti-redundant ranking component
　although the novelty detection component ensures that only novel  previously unseen  information remains in the relevance list  this list might still contain the same novel information at multiple positions in the ranked list. suppose  for example  that the user has already read about a $1 reward for information about the escaped convicts. a new piece of news that the award has been increased to $1 is novel since the user hasn't read about it yet. however  multiple news sources would report this news and we might end up showing  redundant  articles from all these sources in a ranked list. hence  a ranked list should also be made non-redundant with respect to its own contents. we use a simplified version of the maximal marginal relevance method   originally developed for combining relevance and novelty in text retrieval and summarization. our procedure starts with the current list of passages sorted by relevance  section 1   filtered by novelty detection component  section 1   and generates a new non-redundant list as follows:
1. take the top passage in the current list as the top one in the new list.
1. add the next passage ~x in the current list to the new list only if
far ~x    t
where

and lnew is the set of passages already selected in the new list.
1. repeat step 1 until all the passages in the current list have been examined.
after applying the above-mentioned algorithm  each passage in the new list is sufficiently dissimilar to others  thus favoring diversity rather than redundancy in the new ranked list. the anti-redundancy threshold t is tuned on a training set.
1. evaluation methodology
　the approach we proposed above for information distillation raises important issues regarding evaluation methodology. firstly  since our framework allows the output to be passages at different levels of granularity  e.g. k-sentence windows where k may vary  instead of a fixed length  it is not possible to have pre-annotated relevance judgments at all such granularity levels. secondly  since we wish to measure the utility of the system output as a combination of both relevance and novelty  traditional relevance-only based measures must be replaced by measures that penalize the repetition of the same information in the system output across time. thirdly  since the output of the system is ranked lists  we must reward those systems that present useful information  both relevant and previously unseen  using shorter ranked lists  and penalize those that present the same information using longer ranked lists. none of the existing measures in ad hoc retrieval  adaptive filtering  novelty detection or other related areas  text summarization and question answering  have desirable properties in all the three aspects. therefore  we must develop a new evaluation methodology.
1 answer keys
　to enable the evaluation of a system whose output consists of passages of arbitrary length  we borrow the concept of answer keys from the question answering  qa  community  where systems are allowed to return arbitrary spans of text as answers. answer keys define what should be present in a system response to receive credit  and are comprised of a collection of information nuggets  i.e. factoid units about which human assessors can make binary decisions of whether or not a system response contains them. defining answer keys and making the associated binary decisions are conceptual tasks that require semantic mapping   since system-returned passages can contain the same information expressed in many different ways. hence  qa evaluations have relied on human assessors for the mapping between various expressions  making the process costly  time consuming  and not scalable to large query and document collections  and extensive system evaluations with various parameter settings.
1.1 automating evaluation based on answer keys
　automatic evaluation methods would allow for faster system building and tuning  as well as provide an objective and affordable way of comparing various systems. recently  such methods have been proposed  more or less  based on the idea of n-gram co-occurrences. pourpre  assigns a fractional recall score to a system response based on its unigram overlap with a given nugget's description. for example  a system response 'a b c' has recall 1 with respect to a nugget with description 'a b c d'. however  such an approach is unfair to systems that present the same information but using words other than a  b  c  and d. another open issue is how to weight individual words in measuring the closeness of a match. for example  consider the question  how many prisoners escaped  . in the nugget 'seven prisoners escaped from a texas prison'  there is no indication that 'seven' is the keyword  and that it must be matched to get any relevance credit. using idf values does not help  since 'seven' will generally not have a higher idf than words like 'texas' and 'prison'. also  redefining the nugget as just 'seven' does not solve the problem since now it might spuriously match any mention of 'seven' out of context. nuggeteer  works on similar principles but makes binary decisions about whether a nugget is present in a given system response by tuning a threshold. however  it is also plagued by 'spurious relevance' since not all words contained in the nugget description  or known correct responses  are central to the nugget.
1.1 nugget-matching rules
　we propose a reliable automatic method for determining whether a snippet of text contains a given nugget  based on nugget-matching rules  which are generated using a semiautomatic procedure explained below. these rules are essentially boolean queries that will only match against snippets that contain the nugget. for instance  a candidate rule for matching answers to  how many prisoners escaped   is  texas and seven and escape and  convicts or prisoners    possibly with other synonyms and variants in the rule. for a corpus of news articles  which usually follow a typical formal prose  it is fairly easy to write such simple rules to match expected answers using a bootstrap approach  as described below.
　we propose a two-stage approach  inspired by autoslog   that combines the strength of humans in identifying semantically equivalent expressions and the strength of the system in gathering statistical evidence from a humanannotated corpus of documents. in the first stage  human subjects annotated  using a highlighting tool  portions of ontopic documents that contained answers to each nugget 1. in the second stage  subjects used our rule generation tool to create rules that would match the annotations for each nugget. the tool allows users to enter a boolean rule as a disjunction of conjunctions  e.g.   a and b  or  a and c and d  or  e   . given a candidate rule  our tool uses it as a boolean query over the entire set of on-topic documents and calculates its recall and precision with respect to the annotations that it is expected to match. hence  the subjects can start with a simple rule and iteratively refine it until they are satisfied with its recall and precision. we observed that it was very easy for humans to improve the precision of a rule by tweaking its existing conjunctions  adding more ands   and improving the recall by adding more conjunctions to the disjunction  adding more ors .
　as an example  let's try to create a rule for the nugget which says that seven prisoners escaped from the texas prison. we start with a simple rule -  seven . when we input this into the rule generation tool  we realize that this rule matches many spurious occurrences of seven  e.g. '...seven states...'  and thus gets a low precision score. we can further qualify our rule - texas and seven and convicts. next  by looking at the 'missed annotations'  we realize that some news articles mentioned  ...seven prisoners escaped... . we then replace convicts with the disjunction
 convicts or prisoners . we continue tweaking the rule in this manner until we achieve a sufficiently high recall and precision - i.e. the  small number of  misses and false alarms can be safely ignored.
　thus we can create nugget-matching rules that succinctly capture various ways of expressing a nugget  while avoiding matching incorrect  or out of context  responses. human involvement in the rule creation process ensures high quality generic rules which can then be used to evaluate arbitrary system responses reliably.
1 evaluating the utility of a sequence of ranked lists
　the utility of a retrieval system can be defined as the difference between how much the user gained in terms of useful information  and how much the user lost in terms of time and energy. we calculate this utility from the utilities of individual passages as follows. after reading each passage returned by the system  the user derives some gain depending on the presence of relevant and novel information  and incurs a loss in terms of the time and energy spent in going through the passage. however  the likelihood that the user would actually read a passage depends on its position in the ranked list. hence  for a query q  the expected utility of a passage pi at rank i can be defined as
       u pi q  = p i     gain pi q    loss pi q    1  where p i  is the probability that the user would go through a passage at rank i.
　the expected utility for an entire ranked list of length n can be calculated simply by adding the expected utility of each passage:
	  	 1 
note that if we ignore the loss term and define p i  as
	p i  『 1/logb b + i   1 	 1 
then we get the recently popularized metric called discounted cumulated gain  dcg    where gain pi q  is defined as the graded relevance of passage pi. however  without the loss term  dcg is a purely recall-oriented metric and not suitable for an adaptive filtering setting  where the system's utility depends in part on its ability to limit the number of items shown to the user.
　although p i  could be defined based on empirical studies of user behavior  for simplicity  we use p i  exactly as defined in equation 1.
　the gain g pi q  of passage pi with respect to the query q is a function of - 1  the number of relevant nuggets present in pi  and 1  the novelty of each of these nuggets. we combine these two factors as follows. for each nugget nj  we assign an initial weight wj  and also keep a count nj of the number of times this nugget has been seen by the user in the past. the gain derived from each subsequent occurrence of the same nugget is assumed to reduce by a dampening factor γ. thus  g pi q  is defined as
	g pi q  = x wj   γnj	 1 
nj（c pi q 
where c pi q  is the set of all nuggets that appear in passage pi and also belong to the answer key of query q. the initial weights wj are all set of be 1 in our experiments  but can also be set based on a pyramid approach . the choice of dampening factor γ determines the user's tolerance for redundancy. when γ = 1  a nugget will only receive credit for its first occurrence i.e. when nj is zero1. for 1   γ   1  a nugget receives smaller credit for each successive occurrence. when γ = 1  no dampening occurs and repeated occurrences of a nugget receive the same credit. note that the nugget occurrence counts are preserved between evaluation of successive ranked lists returned by the system  since the users are expected to remember what the system showed them in the past.
　we define the loss l pi q  as a constant cost c  we use 1  incurred when reading a system-returned passage. thus  our metric can be re-written as
	 	 1 
where l n  is the loss associated with a ranked list of length
	 1 
1
note that 1 = 1
due to the similarity with discounted cumulated gain  dcg   we call our metric discounted cumulated utility  dcu . the dcu score obtained by the system is converted to a normalized dcu  ndcu  score by dividing it by the dcu score of the ideal ranked list  which is created by ordering passages by their decreasing utility scores u pi q  and stopping when u pi q  ＋ 1 i.e. when the gain is less than or equal to the cost of reading the passage.
1. data
　tdt1 was the benchmark corpus used in tdt1 and tdt1 evaluations. the corpus consists of over 1 news articles from multiple sources  ap  nyt  cnn  abc  nbc  msnbc  xinhua  zaobao  voice of america  pri the world  etc.  published between october 1 and january 1  in the languages of arabic  english  and mandarin. speech-recognized and machine-translated versions of the non-english articles were provided as well.
　ldc  has annotated the corpus with 1 topics  that correspond to various news events in this time period. out of these  we selected a subset of 1 actionable events  and defined corresponding tasks for them1. for each task  we manually defined a profile consisting of an initial set of  1 to 1  queries  a free-text description of the user history  i.e.  what the user already knows about the event  and a list of known on-topic and off-topic documents  if available  as training examples.
　for each query  we generated answer keys and corresponding nugget matching rules using the procedure described in section 1.1  and produced a total of 1 queries  with an average of 1 nuggets per query.
1. experiments and results
1 baselines
　we used indri   a popular language-model based retrieval engine  as a baseline for comparison with caf＞e. indri supports standard search engine functionality  including pseudo-relevance feedback  prf   1  1   and is representative of a typical query-based retrieval system. indri does not support any kind of novelty detection.
　we compare indri with prf turned on and off  against caf＞e with user feedback  novelty detection and antiredundant ranking turned on and off.
1 experimental setup
　we divided the tdt1 corpus spanning 1 months into 1 chunks  each defined as a period of 1 consecutive days. at any given point of time in the distillation process  each system accessed the past data up to the current point  and returned a ranked list of up 1 passages per query.
　the 1 tasks defined on the corpus were divided into a training and test set with 1 tasks each. each system was allowed to use the training set to tune its parameters for optimizing ndcu  equation 1   including the relevance threshold for both indri and caf＞e  and the novelty and antiredundancy thresholds for caf＞e.
　the ndcu for each system run is calculated automatically. user feedback was also simulated - relevance judgments for each system-returned passage  as determined by the nugget matching rules described in section 1.1  were

figure 1: performance of indri across chunks

figure 1: performance of caf＞e across chunks
used as user feedback in the adaptation of query profiles and user histories.
1 results
　in table 1  we show the ndcu scores of the two systems under various settings. these scores are averaged over the six tasks in the test set  and are calculated with two dampening factors  see section 1 : γ = 1 and 1  to simulate no tolerance and small tolerance for redundancy  respectively.
　using γ = 1 creates a much more strict metric since it does not give any credit to a passage that contains relevant but redundant information. hence  the improvement obtained from enabling user feedback is smaller with γ = 1 than the improvement obtained from feedback with γ = 1. this reveals a shortcoming of contemporary retrieval systems - when the user gives positive feedback on a passage  the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms - and thus - usually the same information. however  the user does not benefit from seeing such redundant passages  and is usually interested in other passages containing related information. it is informative to evaluate retrieval systems using our utility measure  with γ = 1  which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback  rather than using traditional ir measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.
table 1: ndcu scores of indri and caf＞e for two dampening factors  γ   and various settings  f: feedback 
n: novelty detection  a: anti-redundant ranking 
indricaf＞eγbase+prfbase+f+f+n+f+a+f+n+a1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1　sometimes  however  users might indeed be interested in seeing the same information from multiple sources  as an indicator of its importance or reliability. in such a case  we can simply choose a higher value for γ which corresponds to a higher tolerance for redundancy  and hence let the system tune its parameters accordingly.
　since documents were processed chunk by chunk  it would be interesting to see how the performance of systems improves over time. figures 1 and 1 show the performance trends for both the systems across chunks. while the performance with and without feedback on the first few chunks is expected to be close  for subsequent chunks  the performance curve with feedback enabled rises above the one with the no-feedback setting. the performance trends are not consistent across all chunks because on-topic documents are not uniformly distributed over all the chunks  making some queries 'easier' than others in certain chunks. moreover  since indri uses pseudo-relevance feedback while caf＞e uses feedback based on actual relevance judgments  the improvement in case of indri is less dramatic than that of
caf＞e.
1. concluding remarks
　this paper presents the first investigation on utility-based information distillation with a system that learns the longlasting information needs from fine-grained user feedback over a sequence of ranked passages. our system  called caf＞e  combines adaptive filtering  novelty detection and antiredundant passage ranking in a unified framework for utility optimization. we developed a new scheme for automated evaluation and feedback based on a semi-automatic procedure for acquiring rules that allow automatically matching nuggets against system responses. we also proposed an extension of the ndcg metric for assessing the utility of ranked passages as a weighted combination of relevance and novelty. our experiments on the newly annotated tdt1 benchmark corpus show encouraging utility enhancement over indri  and also over our own system with incremental learning and novelty detection turned off.
1. acknowledgments
we would like to thank rosta farzan  jonathan grady 
jaewook ahn  yefei peng  and the qualitative data analysis program at the university of pittsburgh lead by dr. stuart shulman for their help with collecting and processing the extended tdt1 annotations used in our experiments. this work is supported in parts by the national science foundation  nsf  under grant iis1  and the defense advanced research project
agency  darpa  under contracts nbchd1 and w1. any opinions  findings  conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
1. additional authors
　jian zhang  jianzhan stat.purdue.edu    jaime carbonell  jgc cs.cmu.edu    peter brusilovsky
 peterb+ pitt.edu    daqing he dah1 pitt.edu  
