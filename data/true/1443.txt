we present and investigate a new method for the traveling salesman problem  tsp  that incorporates backbone information into the well known and widely applied lin-kernighan  lk  local search family of algorithms for the problem. we consider how heuristic backbone information can be obtained and develop methods to make biased local perturbations in the lk algorithm and its variants by exploiting heuristic backbone information to improve their efficacy. we present extensive experimental results  using large instances from the tsp challenge suite and real-world instances in tsplib  showing the significant improvementthat the new method can provide over the original algorithms.
1 introduction
given a set of cities and the distances between them  the traveling salesman problem  tsp  is to find a complete  minimalcost tour visiting each city once. the tsp is a well-known np-hard problem with many real-world applications  such as jobshop scheduling and vlsi routing . the tsp has often served as a touchstone for new problem-solving strategies and algorithms; and many well-known combinatorial algorithms were first developed for the tsp  including the lin-kernighan local search algorithm . in this paper  we consider the symmetric tsp  where the distance from a city to another is the same as the distance in the opposite direction.
　local search  ls  algorithms have been shown very effective for the tsp. in ls algorithms for the tsp  one defines a neighborhood structure of tours  in which tour is a neighbor of tour if can be changed into by some local perturbation  such as by exchanging a pair  1-opt  or triplet  1-opt  of edges between cities  1; 1 . starting from a complete tour  ls repeatedly improves the current tour until it is the best among its neighbors; such a tour is called a local minimum. this process can be applied multiple times using different initial tours. although ls algorithms do not guarantee the optimality of the best solution found  they have been routinely applied in practice  and are among the best methods for the tsp. the best known ls algorithm for the tsp is the lin-kernighan  lk  algorithm . since its inception three decades ago  this classic algorithm has inspired much research

the research was supported nsf grant eia-1.
on ls. today  the best ls algorithms for the tsp are its variants  which include chained and iterated lk algorithms  1; 1; 1 . these algorithms can provide high-quality  near optimal solutions for problems with several thousand cities . it is nontrivial to find solutions better than those from the linkernighan family. nonetheless  improving these algorithms is of practical importance; even small improvements can have substantial financial impacts for many applications  as in manufacturing  where selected tsp tours need to be routinely traversed.
　a major deviation of most modern ls variants from the lk algorithm is the use of starting tours that are closely related to the previous local minimum or the best tour found so far  rather than independently generated . such starting tours can be typically generated by perturbations to the final tours of previous runs using neighborhoodstructures different from the ones used by the main procedure. for instance  double-bridge 1-opt moves have been used extensively for this purpose .
　the intuition behind using chained starting tours in an iterated ls is that high-quality local minima tend to reside in a small vicinity of a neighborhood structure. therefore  it is more effective and efficient to search for a better local minimum from a known one. this intuition is supported by the observation that local minima of a good ls method usually share many common partial structures  1; 1; 1 . such observations have also led to the  big valley  hypothesis   which suggests that high-quality local minima tend to have many edges in common  forming a single cluster around the optimal tour s   and that a better local minimum tends to have more common edges shared with an optimal solution than a worse local minimum.
　besides the iterated ls methods  common structures of local minima have been exploited in at least two other ways. the first is reduction   which first collects a small set of local minima and then locks in the edges appearing in all of them in the subsequent runs. this method has two effects: it can speed up the search  as the problem becomes smaller  and provides a means of directing search among a set of otherwise indistinguishable tours . in   lin and kernighan reported experimental results on instances up to 1 cities using reduction. the second and more recent way to exploit common structures of local minima is called tour merging . given a set of local minima  this method constructs a graph containing the union of their edges. thanks to the large number of shared edges among the local minima  the resulting graph is sparse. an optimal tour within the sparse graph is then uncovered as an approximate solution  which is very often optimal. our recent algorithm on maximum satisfiability  max-sat  also explicitly exploited the information in a cluster of local minima and incorporated it in the walksat algorithm   resulting in a significant performance improvement on diverse large maxsat problems . the current research was inspired by these previous results.
　our research was also motivated by the recent advances in characterizing typical case features of combinatorial problems by their phase transitions and backbones  1; 1 . a problem of fundamental importance and practical interest is to utilize inherent problem information  such as phase transitions and backbones  in a search algorithm to cope with problem difficulty. the research along this line is limited. besides the published work of utilizing backbone information in local search for sat and max-sat   previously published results include exploiting phase transitions in tree search problems in an approximation algorithm  and applying heuristic backbone information in a systematic search for sat .
　our new method explicitly exploits the structure of the local minima of ls algorithms  namely the possible backbone information embedded in the local minima  to improve the performance of the algorithm. here  a backbone variable for a tsp refers to an edge between two cities that appears in all optimal tsp tours. unlike the reduction and tour merging methods  our new heuristic does not freeze the common edges in all local minima in subsequent searches; it rather applies estimated backbone information to guide a local search to the region of the search space that is more likely to have better approximate  and hopefully optimal  solutions. specifically  we treat local minima from a local search as if they were optimal solutions  and use edge appearance frequencies to estimate the probabilities of backbone variables. we then apply the estimated backbone probabilities to alter the perturbations made by the local search algorithm so that a variable having a higher backbone probability will be less likely to be swapped out of the current tour than a variable having a smaller backbone probability  and conversely  will be more likely to be swapped in.
　the paper is organized as follows: in section 1  we describe the general idea of backbone-guided local search for the tsp. we then in section 1 discuss the backbone-guided lk algorithms. we present the experimental results in section 1  discuss related work in section 1  and conclude in section 1.
1 backbone guided local search
if all backbone variables of a tsp were known  they could provide a useful clue to how edges between two cities should be swapped in or out during a local search. if an edge is a part of the backbone  i.e.  it appears in all optimal solutions  obviously the edge should be swapped in if it is not included in the current tour. moreover  we can extend the concept of backbone to backbone frequency of an edge  which is the percentage of optimal solutions that have the edge. this means that a backbone edge has a backbone frequency of one and an edge that does not appear in any optimal solutions has a backbone frequency of zero. therefore  the backbone frequency of an edge is an indicator of how often that edge should be swapped in  or out  if it is  or it is not  part of the current tour. this can be exploited as a heuristic for selecting edges in local search.
　unfortunately  exact backbone frequencies are hard to come by without solving the problem exactly. the idea to bypass this problem was inspired by the fact that many approximation algorithms for the tsp  the lk algorithm and its variants in particular  have superior performance. they can often reach local minima that are within a small percentage of a global optimum and have common structures shared with a global optimal solution  as discussed in  1; 1 . therefore  we can treat local minima as if they were optimal to compute pseudo-backbonefrequencies to approximate the true backbone frequencies. we call this general approach backbone guided local search or bgls.
　we define the pseudo-backbone frequency of an edge as the frequency of its appearances in the local minima sampled. thus  if is the set of local minima  and a given edge appears in a subset of   the pseudo-backbone frequency of is simply .
　which local minima to use will affect the quality of the pseudo-backbone frequencies. ideally  we want the local minima to be an unbiased sample of all high-quality approximate solutions. one leading factor in reaching such an ideal is the set of initial tours: the more distinct the starting tours are  the more different the final tours will generally be. therefore  even though local minima reached from greedily generated starting tours are superior to those reached from random starting tours  a pseudo-backbone constructed from the latter generally leads to better overall performance.
　pseudo-backbone information can be incorporated in ls algorithms to  bias  the search. in ls  moves are evaluated by the differencebetween the total cost of the edges to be removed from the tour and the total cost of the edges to be added. if this value is positive  the moveis taken. in backboneguidedsearch  we can make biased movesin two differentways; oneonly uses pseudo-backbone frequencies and the other combines pseudobackbone frequencies and the distances between cities. let be the set of backbone edges of a tsp and the set of local minima from which pseudo-backbone frequencies were computed. let and be the candidate set of edges to be removed and added  respectively  at a step of searching for a -opt move. we prefer to replace by if has a smaller possibility of having more backbone variables than . if we assume edges to be independent of each other  we prefer to replace by if
	  where	is the backbone frequency of edge
　  computed from the set of local minima	. this method has been shown effective on maximum satisfiability .
　unfortunately  local search based exclusively on local perturbations using merely pseudo-backbone frequencies is not very effective on the tsp. one possible factor contributing to this discrepancy between the tsp and max-sat is the different sizes of their search spaces. the tsp has
states in its search space  where is the number of edges for an city tsp. these states are embedded in a constraint structure  in which  for example  taking one edge preventing many other edges from being taken. in comparison  the search space of max-sat has only states for boolean variables. the estimated backbone frequencies could thus be less reliable on the tsp than on max-sat. the deficiency of local perturbations based purely on pseudo-backbone frequencies indicates that the actual intercity distances should not be ignored for the tsp. this constitutes one of the main differences between backbone-guided local search for the tsp and that for max-sat.
　in the lk algorithm  moves are evaluated by summing the costs of the edges to be removed from the current tour  and subtracting the sum of the costs of the edges to be added. the same evaluation principle can be applied in backbone guided ls  but with cost computed differently. instead of taking the cost of a given edge to be the distance between two cities    it is taken to be   where p is the pseudo-backbone frequency of that edge. thus  the cost of an edge will decrease linearly in proportion to its pseudo-backbone frequency. the extreme cases are edges not in the pseudo-backbone  which have their original costs      and edges in all local minima  which their costs set to zero    .
　when this method is used  the results can be improved even more by carrying out regular lk search using original distances from the local minima found by the biased search. this is effectivebecause the biased search actually searches in a new tsp instance that has been created from the original by applying a  pseudo-backbone transform  to its edge weights. local minima in this new instance will not generally correspond to local minima in the original  so continued search of the original graph will improve the results  even after biased search has reached a local minimum.
　furthermore  pseudo-backbone information can be employed to generate starting tours. it is known that the effectiveness and speed of local search can be improved by using greedy starting tours . the greedy tour construction begins by randomly picking a starting city  and adding the shortest edge exiting the city to the tour. then  edges are greedily added one-by-one until the tour is complete. we can modify this process to naturally utilize the pseudo-backbone by redefining the  best  edge in terms of the pseudo-backbonetransformed weights  rather than the original weights. this gives us a greedy pseudo-backbone tour generation heuristic.
1 backbone guided lk and ilk
applying the above ideas and considerations to lk  we have the backbone guided lk algorithm  bglk . similar to lk  bglk runs many cycles  each of which starts from a new starting tour and reaches a local minimum. different from lk  bglk has two phases. the first is a learning phase that runs a fixed number of iterations of original lk with random starting tours. the local minima from these runs are used to compute pseudo-backbone frequencies. the second is a backboneguided improvement phase where biased -opts are utilized. in addition  in the second phase  biased starting tours can be used as well to improve local minima and speed up the search. in our experiments  we foundthat setting aside 1% of the total runs for learning was generally effective.
　a few issues must be dealt with to combine pseudobackbone utilization with ilk  the iterated lk algorithm  in deriving iterative bglk  ibglk . first  pseudo-backbone must be constructed from  unbiased samples  of local minima. since each local minimum found via ilk typically differs in only a small number of edges from its progenitor  a sampling of local minima from such a chained process is biased  and should not be averaged into the backbone. to deal with this  we construct a backbone in our experiments from 1 independent rounds  i.e.  restarts  of ilk  each of which is allowed a smaller number of iterations  such as 1% of the total number of iterations. another issue is that we would like to follow each round of bglk by a round of regular search using the original distances. however  since bglk uses  weighted  costs  it is impossible to ignore potential -opts involving only unchanged cities  as when moving between successive rounds of ilk with only a single search method. our solution to this has been to alternate between bglk and regular lk at a higher level of granularity: one of these search mechanisms is employed until it  fails   at which point we begin employing the alternate mechanism until it too  fails   then switch back to the original  and the alternation repeats. in this context  we have defined failure at goingthrough rounds  restarts  of optimization without finding an improvedtour. for our experiments  we have set   where is the number of cities.
1 experimental evaluation
we have carried out two sets of experiments to study different ways of applying pseudo-backbone information in bgls. in our experiments we used random instances from the tsp challenge suite   which includes problemclasses of uniform euclidean  uniform   clustered euclidean  clustered   and distance matrix  matrix . we have also used the large instances from the tsplib .
　we compared bglk and ibglk against lk  ilk  the reduction method of   and the search space smoothing method of . the smoothing method is related because it modifies the distances of tsp instances in an attempt to make it easier for local search to eventually reach local minima of higher quality. it uses a fixed formula    to transform distance from to   where is the average distance over all original distances  and takes a series of decreasing values  typically to help slowly move from an initially  smooth  instance  in which all distances look similar to one another  to the actual problem. we implemented this method in the lk algorithm for the test.
　the particular version of lk-based algorithms that we used was implemented and provided by johnson and mcgeoch  described and analyzed in . we leave the details of the implementation to its original description  while simply pointing out that all of our tests were carried out with the default settings for this implementation  namely length-1 neighbor lists for all levels of the search  don't-look bits  and the 1-level tree tour representation. we incorporated the reduction method and the space smoothing method in lk to generate two variants. we have also followed johnson and mcgeoch  in configuring ilk  allowing it the most flips    where is the problem size. this is our baseline ilk algorithm  abbreviated as ilk-1run. in addition  we used two different configurations of ilk in our experiments: the best of five runs of iterations  i.e.  ilk with four random restarts   which is named as ilk-1-runs  and the best of ten runs of iterations  which is called ilk-1runs. we configured ibglk as follows. it first runs 1 rounds of ilk  each of which starts from a random tour and is allowed iterations. it then executes 1 rounds of ilk with biased moves and biased greedy initial tours  each of which is also allocated iterations. all algorithms used the same total number of flips to give a relatively fair comparison.
　our experiments were run on 1 and 1 ghz amd athelon machines with 1 gigabytes of memory. all runtimes are normalized for a 1 mhz alpha  as described in   so that our results and the previous results can be directly compared.
1 the tsp challenge suite
we experimentally validate our claim that bgls allows local search methods to reach better local minima  using problem
n=1n=1n=1kn=1kopti1-1-----lk1 1 1 1 1 1 1 1 uredu1 1 1 1 1 1 1 1 smoo1-1-1-1-bglk1 1 1 1 1 1 1 1 opti1-1-----lk1 1 1 1 1 1 1 1 credu1 1 1 1 1 1 1 1 smoo1-1-1-1-bglk1 1 1 1 1 1 1 1 opti1-1-1---lk1 1 1 1 1 1 --mredu1 1 1 1 1 1 --smoo1-1-1---bglk1 1 1 1 1 1 --table 1: comparison of lk  reduction  smoothing and bglk on problems from the challenge suite on uniform  u   clustered  c  and matrix  m  problem instances. the numbers are tour costs over held-karp bounds in %  and normalized cpu times in seconds  in parentheses .
instances from the challenge suite. we examine the results from four different perspectives.
　in the first test  we compared bglk against lk  lk with reduction  and lk with search space smoothing. the lk algorithm has 1 runs; the reduction method uses an initial probing stage of 1-run of lk  followed by 1-run of lk with reduction; the smoothingmethod has 1 runs total  each of which runs lk five times  for . these algorithms all used greedy initial tours and had a total of 1 runs. bglk executes an initial 1-run of lk  using random starting tours  to learn pseudo-backbone  and then 1 runs of lk with biased moves and biased initial tours. the sizes of problem instances are 1  1  1  and 1  increased by a fac-

tor of   following the scheme proposedand used in . for each problem class and size  1 random problem instances were used for each algorithm. table 1 shows the averaged results. for comparison  percent over the held-karp bounds for optimal tours is listed when known.
　as shown  while the reduction and smoothing methods provide modest improvements over lk in certain instance classes and sizes  they are respectively the worst among all algorithms tested on the euclidean and clustered euclidean classes. in contrast  bglk is consistently superior  outperforming all the other three methods across all instance classes and sizes. the improvementof bglk over these algorithms varies from 1% to 1% on the largest problem instances of these classes. note that the average runningtime of bglk was no more than twice the time by lk. interestingly  bglk actually ran faster than lk on some instances  presumably due to its faster focus on promising areas. note also that the reduction method's running times significantly less than that of il  while the smoothing method's runtimes are essentially identical to lk's.
　in the second experiment  we compared ibglk against ilk  ilk-1-run and its two variants  ilk-1-runs and ilk-1runs . we used the same problem instances as in the first experiment  and computed the results the same way as in that experiment. the results are shown in table 1  where we adopted the same reporting scheme as in table 1. as shown  the two ilk variations do not provide much improvement over the baseline ilk  while ibglk is able to push significantly closer to optimal than ilk for most cases where ilk does not perform very well. for uniform euclidean instances  as ilk does not reach the optimum for these instances  ibglk has a good chance
n=1n=1n=1kn=1koptimal1-1-----ilk-1-run1 1 1 1 1 1 1 1 uilk-1-run1-1-1-1-ilk-1-run1-1-1-1-ibglk1 1 1 1 1 1 1 1 optimal1-1-----ilk-1-run1 1 1 1 1 1 1 1 cilk-1-run1-1-1-1-ilk-1-run1-1-1-1-ibglk1 1 1 1 1 1 1 1 optimal1-1-1---ilk-1-run1 1 1 1 1 1 --milk-1-run1-1-1---ilk-1-run1-1-1---ibglk1 1 1 1 1 1 --table 1: comparison of ilk  ilk-1-run   its variations  ilk1-run and ilk-1-run   and ibglk on problems from the challenge suite. the legend and interpretation of the table are the same as those for table 1.
to improve tour quality. ibglk's performance appears to remain constant  i.e.  1%  relative to the held-karp bound as the problem size increases  while ilk's performancedegrades. although ibglk runs longer than ilk  due to the extra time needed to construct and apply the pseudo-backbone  the overhead never exceeds 1% of ilk's runtime  and it appears that ibglk's and ilk's runtimes are asymptotically close to each other. for clustered problem class  ibglk  ilk and its variations' performances are compatible with one another. interestingly  ibglk actually runs faster than ilk on some of the clustered instances. for this instance class  ibgls's lack of significant improvement over ilk can be attributed to the fact that ilk is already effective and close to the optimum; there is little room for improvement. on the distance matrix class  similar to bglk versus lk  ibglk is significantly superiorto ilk on each of the individual instances of different sizes  and reduces average tour costs by more than 1% on the largest     instances tested. in addition  among ilk and its variants  ilk-1-run has the fastest running times  and its two variants are 1%-1% slower.
　in the third experiment  we considered relatively small instances of 1 cities and compared the local minima from lk  ilk and ibglk against the optimal solutions. we used 1 instances for each of the problem classes in the challenge suite. for each instances and each algorithm  we averaged three runs with different random seeds. as the result shows  ibglk generally produces solutions of higher quality on average. interestingly  it seems that more powerful search methods produce not only better  but more diverse solutions  as quantified by the standard deviations of the hamming distances between the structures of local minima and the optimal solutions  data not shown . figure 1 graphically shows relative performances of ilk and ibglk on individual instances in the test  indicating that ibglk is favorable to ilk on most of the problem instances.

figure 1: comparison of local minima from ilk and ibglk for 1 problems with 1 cities each. points with indicate that ibglk's solutions are of higher quality  points with indicate that ibglk's solutions are closer to optimal. both axes　additional insight can be gained by an inspection of the anytime behavior of these algorithms. as shown in figure 1  bglk's anytime behavior is remarkably different from that of lk and its variants. to begin with  lk outpaces bglk  since lk is being run from greedy starting tours  while bglk must be run from random tours to construct a reliable pseudobackbone  see section 1 . but once bglk begins using the pseudo-backbone to guide the search  its rate of descent increases  and the tables are turned on their performance. qualare normalized.
itatively similar anytime performance results have been found on large problem sizes and when comparing ibglk with ilk. 1 instances from the tsplib
table 1 contains the results for ilk  its variants  and ibglk  on all the instances in the tsplib that have at least 1 cities. on many of the instances  unfortunately  ilk is already at or near the optimum  so there is very little room for improvement. again  in most places where further improvements are possible  ibglk produces them. on this set of instances  ibglk championed on 1 of them  the ones in bold and underlined   whereas ilk and its variants came first on 1 instances  with some overlaps . note that on the largest instances  the ones with more than 1 cities   ibglk produces significant gains over ilk. the runtimes were from our amd athelon machines and then normalized to a 1mhz alpha as suggested in .
1 related methods and discussions
the most closely related work is our previous work on backbone guided walksat for maximum satisfiability . the backbone guided local search developed here for the tsp follows the same principles developed there; and thus can be viewed as an innovative extension to the work in . this research was also inspired by and builds upon the previous results of the  big valley  hypothesis on the clustering of local minima from the family of the lin-kernighan algorithm and its variants  1; 1; 1 .
　two pieces of previous work resemble bglk in some way. the first is the reduction method by lin and kernighan . as discussed earlier  reduction  locks in  the common edges in all local minima. this has two effects: it can speed up the search  as the problem becomes smaller  and provides a means of directing search among a set of otherwise indistinguishable tours. the main limitation of this method is that it is brittle  depending on the quality of the  locked in  edges. if a  locked in  edge turns out not to be part of the backbone  no optimal tour will be found. moreover  information such as  edge appears in 1% of all local minima  cannot be utilized  thus it simply disregards potentially useful information of backbone frequencies. the results in   up to 1 cities  is too limited to provide a fair assessment of this method. our results in section 1 revealed that reduction is not competitive.
　bgls is more general than reduction by providinga remedy to the problem of brittleness and making use of information embedded in backbone frequencies. bgls is a random strategy  in which edges that appear in all local minima may still be
% over the held-karp boundruntime in secondsnameilk-1ilk-1ilk-1ibglkoptimalilk-1ibglkdsj1.1.1.1.1.11pr1.1.1.1.1.11si1.1.1.1.1.11u1.1.1.1.1.11vm1.1.1.1.1.11pcb1.1.1.1.1.11d1.1.1.1.1.11rl1.1.1.1.1.11rl1.1.1.1.1.11nrw1.1.1.1.1.11fl1.1.1.1.1.11u1.1.1.1.1.11fl1.1.1.1.1.11d1.1.1.1.1.11vm1.1.1.1.1.11u1.1.1.1.1.11rl1.1.1.1.1.11d1.1.1.1.1.11u1.1.1.1.1.11u1.1.1.1.1.11pr1.1.1.1.1.11pcb1.1.1.1.1.11fl1.1.1.1.1.11fnl1.1.1.1.1.11rl1.1.1.1.1.11rl1.1.1.1.1.11pla1.1.1.1.1.11rl1.1.1.1.1.11usa1.1.1.1.1.11brd1.1.1.1.1-1d1.1.1.1.1.11d1.1.1.1.1-1pla1.1.1.1.1-1pla1.1.1.1.1-1# of bests11table 1: comparison of all instances in tsplib with at least 1 cities. numbers are tour costs over held-karp bounds in % and normalized runtimes in seconds  for a 1 mhz alpha .
swapped out of the current tour during the search  albeit with a slim possibility. this allows more tours be explored while still maintaining a focused search.
　the second related work is search space smoothing   which was briefly described in section 1. gu and huang only experimented with this idea with a 1-opt algorithm and on small random euclidean problems with no more than 1 cities. they did not compare their method to other techniques. our results in section 1 showed that smoothing outperforms lk on uniform and matrix instances. however  it is less efficient than bglk and ilk.
	uniform	clustered	matrix
	1 1 1 1 1	1	1	1	1	1	1
	runtime in seconds	runtime in seconds	runtime in seconds
figure 1: anytime performance of lk  lk with reduction  lk with smoothing  and bglk  for random instance from the tsp　although both smoothing and backbone guided search all modify distances  they are fundamentallydifferent. first  modifying distances is just one of the end products of the backbone guided search for the tsp. in fact  when applied to maximum satisfiability  backbone guided local search did not change challenge suite with 1 cities.
costs at all . estimated backbone information can be used not only to modify distances  but also in different places of a local search to make biased searches. for instance  it can be employed to generate biased initial starting tours. secondly and more importantly  the backbone guided search does not treat every distance equally; some distances may be altered dramatically while others may remain intact to help exploit the intricate constraints among the cities.
1 conclusion
we have presented and investigated a new method of applying backbone information to force a local search to make biased local perturbations  in the context of the tsp. based on the  big valley  hypothesis  we developed a method for deriving heuristic backbone information by exploiting the local minima from the efficient lin-kernighan local search algorithm. we demonstrated its effectiveness with extensive experiments on various problem types and instances.
　our results on the tsp show that bglk and ibglk compare very favorably to lk and ilk  respectively  on the instances in the challenge suite  up to 1 cities  and the instances from tsplib. for the uniform euclidean class  bglk and ibglk outperform lk  and its variants  and ilk  respectively. on clustered euclidean instances  bglk outperforms lk by reducing tour costs by more than 1% on average on 1-city instances; and ibglk and ilk have comparable performance.  note that an improvement of a fraction of percent in solution quality is significant on large tsps .  bglk and ibglk dominate their counterparts on tour quality on all instances in the distance matrix class we tested; on 1city instances  bglk and ibglk find tours with costs at least 1% and 1% smaller than lk and ilk  respectively. on real instances from the tsplib  ibglk finds nearly twice as many best solutions as three versions of ilk  i.e.  1 versus 1 . finally  bglk and ibglk have comparable running times with their counterparts  no more than twice as long as the original algorithms  while sometimes being faster.
　in short  the main contribution of this work is an effective method of utilizing inherent problem features  backbones in particular  of the tsp in the lk algorithms and variants to improve their efficacy. a method of exploiting problem features in search is of fundamental interest and practical importance. the results on the tsp in this paper have indicated that the idea of backbone-guided local search is general and can lead to efficient heuristic algorithms for large  difficult problems. the source code of our algorithm will be available on the web.
