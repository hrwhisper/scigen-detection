     this paper describes an approach to object location based on matching model regions to surface image regions. these matches  together with object models  provide hypotheses about the 1d location of the objects containing the recognized surfaces. when a hypothesis is complete  the program describes the matched data and model surface boundaries in terms of the raw model. these descriptions are used used to verify the physical consistency of the hypotheses and to explain extra or hypothesize missing features caused by obscured components. 
1. introduction 
     the most important recent work in high-level vision  acronym   has demonstrated the importance of embedding object models and an understanding of the scene-to-image transformation in an intelligent vision program. with these components  a program can predict how an object will appear given its current beliefs about the scene  rather than having to rely on image models  or  more likely  a feature space representation of these views . concurrently  much low-level vision research has been attempting to provide a surface-based description of images. 
     this paper describes a program  imagine  which matches surface regions to object models to recognize and locate projections of three dimensional objects in two dimensional images. the approach is data driven  with three major stages. the first stage matches image regions to model surfaces  with the goal of estimating the 1d orientation parameters for the image region. this information is used to hypothesize specific object surfaces. the second stage relates the hypotheses according to the structural relationships embodied in the object models. the third stage verifies that the hypothesized objects are consistent with real world constraints  such as boundary adjacency and surface ordering. this process requires calculating descriptions of the predicted surfaces and associated data regions. recognition is considered successful if a set of data is found that adequately accounts for all features of a model. in particular  this requires the program to reason about object back surfaces  tangent surfaces  small features  obscured features and non-rigidly attached subcomponents. 
this work was supported by a post-graduate studentship from the university of edinburgh. thanks also go to j.a.m. howe and r.j. beattle. 
in short  the program has four goals: 
- to locate instances of specific 1d objects in 1d images  
- to locate images features corresponding to all features of the model  or explain why the image features are not present. 
- to verify that the features are consistent with the geometrical and topological predictions made by the model  and 
- to extract the parameters needed to fully characterize an object's scene position  including any associated with flexibly connected objects. 
     coping with obscured features of objects is an important point of this work. this has required work in two areas; first  in deciding what are relevant aspects of obscured objects  ie. what features disappear and what new ones appear   and second  in extracting information about those features. this second point has required work on extracting 1d positional information from surface shapes and descriptions of surface and image regions. 
     this paper discusses the model and the reasoning rules used to analyze the main example image that of a robot upper and lower arm assembly. 
1. what does 'recognize* mean in this context 
     the meaning of  recognition  is largely dependent on the goals of the recognizer  that is. what type of output is desired for what type of input. if the only brown objects in the environment are tables  then a program could find tables by looking for brown regions. to some extent  all recognition programs are just discrimination programs  of which the table recognizer is a minimal such program. on the other hand  even the most sophisticated programs  e.g. acronym   are fundamentally the same  they attempt to find a set of evidence that is consistent with their model of the object and for which there is no contradictory evidence. the distinction is largely one of sophistication - richer models  greater embedded knowledge about object structure relationships and more detailed matchings reduce the possibilities of false recognitions. 
     in general  recognition consists of four subactions: 
- location of interesting data. 
- selection of a model. 
- instantiation and verification of model  and 
- determination of object's location and orientation. 
1 r. fisher 
in particular  this program: 
- considers all data interesting. this is mainly because the input has been previously segmented into connected surface regions. 
- considers all models this is mainly because model invocation is a research problem not addressed in this work. 
- attempts to fully instantiate the model. a model has slots for a set of structurally related surfaces and subobjects the program either fills the slots or finds evidence to explain why they cannot be filled.  this is an important requirement for an intelligent vision program.  surface slots are matched to image regions and substructure slots are matched to previously recognized objects. fully instantiated hypotheses must pass a verification process. 
- locates and orients the objects in three dimensions. the parameters come from estimating the transformation that maps a given model surface to a given image region based on the shape of the image region  and then inverting the transformation from surface to object. 
1. surfaces 
     tne project has concentrated on using surface regions as the primary data primitive. though there are no programs yet that reliably and completely give a surface segmentation  it doesn't seem like a too distant possibility. pentland's surface shape algorithm  1. various stereo algorithms  eg grimson  . optical flow  range finaer systems and structured light systems all provide the information needed to construct a sur-
ace map. 	 surface depth  shape and orientation are 
all largely equivalent representations.  
     there are many reasons for using surfaces as the primitive input data element at this level of analysis. first  irrespective of what interpretation is made  what is seen is the surface of objects. second  as surface regions are matched to model surface regions  the semantics of the relationship is simple: in unobscured situations  a six parameter geometric transformation plus a projection onto the image plane. third  the segmentation of a surface image into regions of uniform surface properties may be simpler. to some extent  any region of homogeneous image properties is likely to correspond with some subset of a uniform surface.  the reverse is not always true  as a shadow across a flat surface would create two distinct regions.  there are many open problems on this topic  such as merging split regions  scale of surface descriptions  etc fourth  surface regions lead to more accurate transformation parameter estimates  mainly because their size gives more accurate measurements . point to model matching and transformation inversion algorithms  e.g. roberts    while elegant  are sensitive to image measurement errors. worse  the loss of information from using just points makes it difficult to determine the image to model correspondences correctly. lastly  once surfaces are chosen as the desired primitive. 
other image boundary information is more useful: 
- reflectance boundaries: surface detail 
- obscuration: object limits  surface depth order 
- shadow: surface location in three dimensions 
- shape: surface relative orientation 
     the input used by the program is that of a surface image segmented into regions of either planer or simply curved orientation. hence  the data is simplified in that we know all boundaries correspond to surface or shape discontinuities. further  the boundaries form closed regions. the program could have used the surface depth or orientation information as part of its inputs  but it was decided to base the work just on the region shape. 
1 . object models 
     this work uses structural three dimensional models of the objects it expects to find the reasons for this are discussed by binford  1: a capable vision system should know about objects  and how objects are seen in images  rather than what types of images an object is likely to produce. one difference between this work and that of binford and brooks  acronym   has been the choice of primitives used to construct the models they use solid object models  generalized cones  as the fundamental building block  and then deduce what image boundaries are produced. one problem with this is that what is seen is surfaces  hence it seems reasonable that the preferential representation for a vision system would make surface-based information explicit.  this argument is largely one of conceptual simplicity  in that it should be possible to automatically deduce a surface representation from a solid representation  though not necessarily easily.  another difficulty with the generalized cone is that it seems to be a conceptually simple representation for only some object classes  though any one representation is probably not sufficient for all tasks. 
     surface primitives are constructed by specifying the boundary of the region  with the assumption that the surface is either planer or has a single axis of curvature and fills the region inside the boundary. 
     objects are described in a subcomponent hierarchy  with objects being composed of either connected surfaces or recursively defined sub-objects. the connections are specified using a six parameter attachment  x.y.z translations  rotation  slant and tilt . these attachments can be rigid  all parameters specified in the model  or may be flexible. flexible attachments are handled by assigning the parameter a symbolic and which is bound value when the corresponding subcomponent is added to the object hypothesis. symbolic parameters are only used with attachments. 
     figure 1 shows some of the model used for the example in section 1. the full model consists of the upper and lower main arms of a unimatlon puma robot. the figure here just shows some of the model for the upper arm and external linkages. figure 1 shows the part models and the coordinate frame locations. the variable  jntn  is the symbolic parameter for the robot's nth joint angle. in the example  the face primitives define the surfaces by listing key boundary points  1  and whether they are joined by a line or simple oriented curve. the assembly blocks define named structures  in terms of either other named structures or surfaces. the  at   ... . ..   portion describes the xyz translation  rotation  slant  tilt afflxment relationship between the main and sub-
     

figure 1 - part of model used for the robot arm recognition example 
1. matching model surfaces to image regions 
     this process produces the main input used by the object recognition and construction process described in section 1. its inputs are the boundary of the image region and a description of the surface as given by the model. it has the goal of estimating the transformation parameters that relates the image to the model 
     the visual motivation for this process is: the shape of an image region gives strong clues to the orientation of the surface. in particular  the program compares the image region shape to the model surface shape  using the boundaries  and attempts to extract measurements that lead simply to the desired parameters. the parameters are rotation  slant  tilt axis orientation  distance and x-y translation. the choice of rotation  slant and tilt as the orientation parameters was motivated by what information was easily found in an image of a transformed surface. the initial parameter estimation process is based on the following observations: 
- rotation and xyz translation of a surface do not alter its relative cross sectional dimensions  and that slant does not alter them drastically  over the range of estimated slants  less than 1 radians . 
- the tilt axis lies approximately at the orientation of greatest slant distortion. 
- the maximum distortion is an indication of the slant relative to the line of sight. 
- the relative scaling of features between the model and image indicates the distance. 
     the heuristics used for the parameter estimation are: 
rotation - correlation of data and model cross 
r. fisher 1 
sections. the peaks are potential rotation offsets. figure 1 shows typical model and data shapes and their corresponding cross section versus angie functions. note that the data function is largely just the translated model function. 

figure 1 - model and data cross sections 
slant and tilt - given an estimated rotation  create a ratio of maximum data to maximum model cross section widths as a function of cross section path angle.  use of the maximum is a heuristic to relate approximately corresponding cross sections.  slant  tilt and distance estimates come from the orientation of the minimum ratio  the ratio itself and the maximum ratio  figure 1 . 

figure 1 - data to model cross section ratio 
distance - given rotation  slant and tilt  compute a 
ratio of data to model cross sections along the data region's major and minor axes. two distance estimates are made from the average and most common ratios. the final value is the average of these two and the previous estimates. the major motivation for the use of three estimators is various sensitivities to obscuration or parameter mis-estimation. 
x.y translation - align data and mode centers of mass transformed by the estimated parameters 
     an optimization phase is then entered. the evaluation function is the weighted distance of the predicted model boundary from the observed image boundary. because of the expected accuracy of the various parameter estimates  optimization preference is given to the worst estimate in order of tilt  distance  slant and rotation  with the x-y translation always adjusted to best fit at each stage. 
     note that the transformation relating a modeled surface to an image region is not always unique  especially when model symmetries  spatial quantization and inaccuracies of fitting numerical data are considered. consequently  the parameter estimation process may produce more than one hypothesis. 
     
1 r. fisher 
rule 1:  object hypothesis   -  surface hypothesis  
     
     these heuristics gave reasonable parameter estimates in 1 of 1 test cases recently. given the tolerances in the matching process  the estimates are acceptable in the successful cases. unfortunately calculating these parameters is largely numerical and computationally expensive - mainly because of image parameter measurement and model-to-data boundary comparisons during the optimization phase. 
1. some rules for recognizing objects 
	here  recognition is a construction process. 	at 
each stage  a hypothesized structure is under consideration. this hypothesis may have several empty substructure slots  in which case the program will attempt to fill the slots in various ways or. the slots may all be full  in which case the hypothesis will be subjected to a verification process. finally  the structure may be used to hypothesize another structure 
     currently  the processing of the hypotheses is exhaustive  and is controlled by a first-in. first-out queue. the selection of which procedure to apply to a hypothesis is determined by a set of rules compiled from the set of meta-rules discussed below. 
some of the rules have binary right-hand-sides. 
when the matcher considers applying such a rule to a hypothesis  a prediction routine is called. using the object models  this routine predicts where in the image the second hypothesis needed for the matching might be found  c.f. freuder's advice process  . then  all hypotheses of the appropriate type from this location is paired with the current hypothesis node and the analysis routine is invoked for each pairing. 
     the general processing philosophy is for the rules to produce all hypotheses consistent with their definition. with the assumption that incorrect hypotheses will not be capable of full consistent instantiation. 
     about two dozen rules exist at the current time. the point of having different rules  rather than a general mechanism  is that each can carry out the specific type of reasoning needed to solve a particular problem. the rules described below are some of those used in the example given in section 1: 
rule 1:  surface hypothesls   -  lmage reglon  
     this rule selects  among all model surfaces  those that meet various pre-screening conditions relative to the particular image region  mainly minimum size relationships . then  the parameter estimation process  section 1  is applied for each surface candidate. a surface hypothesis is generated for each suitable estimate. 
rule 	1: 	 oblect 	hypothesls  	 - 	 ob|ect 	hypothesls  
 surface hypothesis  
     this rule adds a new surface to an existing structure hypothesis. for the rule to apply  the global location of the new hypothesis predicted by the new surface has to be consistent with the previous hypothesis. the resulting hypothesis has all the previous information transferred. including variable bindings. 
     this rule produces a structure hypothesis for each modeled object that has the particular face as a rigidly connected subcomponent. the location of the hypothesis is calculated by taking the location of the face and inverting the attachment relationship to the main structure  figure 1 . 

figure 1 - surface hypothesis to object hypothesis rule 
rule 1:  object hypothesis   -   object hypothesis  
     this rule fills slots corresponding to surfaces invisible because they lie on the back sides of the object. this is done by using the object model to predict the surface orientation  given the hypothesis's location and orientation. 
rule 1:  object hypothesls   -  object hypothesis  
