increasingly large numbers of situational applications are being created by enterprise business users as a by-product of solving day-to-day problems. in efforts to address the demand for such applications  corporate it is moving toward web 1 architectures. in particular  the corporate intranet is evolving into a platform of readily accessible data and services where communities of business users can assemble and deploy situational applications. damia is a web style data integration platform being developed to address the data problem presented by such applications  which often access and combine data from a variety of sources. damia allows business users to quickly and easily create data mashups that combine data from desktop  web  and traditional it sources into feeds that can be consumed by ajax  and other types of web applications. this paper describes the key features and design of damia's data integration engine  which has been packaged with mashup hub  an enterprise feed server currently available for download on ibm alphaworks. mashup hub exposes damia's data integration capabilities in the form of a service that allows users to create hosted data mashups. 
categories and subject descriptors 
h.m  information systems : miscellaneous  
general terms 
design 
keywords 
information integration  xml  data feed  
1. introduction 
there are two important trends motivating the need for a new type of enterprise information integration architecture  aimed primarily at satisfying the information integration requirements of situational business applications . 
the first trend is happening inside the enterprise where there is an increasing demand by enterprise business leaders to be able to exploit information residing outside traditional it silos in efforts to react to situational business needs. the predominant share of enterprise business data resides on desktops  departmental files systems  and corporate intranets in the form of spreadsheets  presentations  email  web services  html pages  etc. there is a wealth of valuable information to be gleaned from such data; consequently  there is an increasing demand for applications that can consume it  combine it with data in corporate databases  content management systems  and other it managed repositories  and then to transform the combined data into timely information.  
consider  for example  a scenario where a prudent bank manager wants to be notified when a recent job applicant's credit score dips below 1  so that she might avoid a potentially costly hiring mistake by dropping an irresponsible applicant from consideration. data on recent applicants resides on her desktop  in a personal spreadsheet. access to credit scores is available via a corporate database.  she persuades a contract programmer in the accounting department to build her a web application that combines the data from these two sources on demand  producing an atom feed that she can view for changes via her feed reader. 
there are a large number of such situational applications being created by business users and departmental it staff as an offshoot of solving day-to-day problems. these applications typically target a small community of users  and a specialized business need. in contrast  typical enterprise applications are developed by corporate it staff for a large number of generic users  and a general purpose. situational applications represent the  long-tail  of enterprise application development; consequently  there is a significant opportunity for it researchers and professionals to create innovations that facilitate their development.  
the second trend is happening outside the enterprise where the web has evolved from primarily a publication platform to a participatory platform  spurred by web 1 paradigms and technologies that are fueling an explosion in collaboration  communities  and the creation of user-generated content. the main drivers propelling this advancement of the web as an extensible development platform is the plethora of valuable data and services being made available  along with the lightweight programming and deployment technologies those allow these  resources  to be mixed and published in innovative new ways. 
standard data interchange formats such as xml and json  as well as prevalent syndication formats such as rss and atom  allow resources to be published in formats readily consumed by web applications  while lightweight access protocols  such as 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod'1  june 1  1  vancouver  bc  canada. 
copyright 1 acm  1-1-1/1...$1. 
 
rest  simplify access to these resources. furthermore  weboriented programming technologies like ajax  php  and ruby on rails enable quick and easy creation of  mashups   which is a term that has been popularized to refer to composite web applications that use resources from multiple sources .   
the damia project aims to seize upon the aforementioned opportunities to aid situational application development by harnessing many of the web 1 paradigms and technologies that have spurred the innovation in assembly manifested by the mashup phenomenon.  we envision corporate it steadily moving toward web style architectures.  in particular  we envision the corporate intranet steadily evolving into a platform of readily consumable resources  and lightweight integration technologies  which can be exploited by business users to create  enterprise mashups  in response to situational business needs. the lines between the intranet and web will progressively blur as enterprise mashups reach outside the corporate firewall to exploit data and services on the web.  
enterprise mashups present a data problem  as they can access  filter  join  and aggregate data from multiple sources; however  these data machinations are often done in the application  mixed with business and presentation logic. in damia  we are developing an enterprise-oriented data mashup platform on which such applications can be built quickly and easily  by enabling a clean separation between the data machination logic and the business logic. in particular  the damia data mashup platform  1  enables secure access to data from a variety of desktop  departmental  and web sources both inside and outside the corporate firewall  1  provides the capability to filter  standardize  join  aggregate  and otherwise augment the structured and unstructured data retrieved from those sources  1  allows for the delivery of the transformed data to ajax  or other types of web applications on demand  1  exposes these capabilities via lightweight programmatic and administrative apis that allows users with minimal programming expertise to complete integration tasks. 
we are still in the early stages of damia's evolution; however  we have developed a rather sophisticated prototype of damia's integration capabilities that we deployed in the context of a feed server  which was made available as a service on ibm's corporate intranet. in addition to exposing damia's integration capabilities  which allowed users to create hosted data mashups  the service also provided a directory where the damia community can tag  rate  and share data mashups  as well as other information assets that might be consumed by data mashups. a browser based userinterface exposed these capabilities in a way that allowed users with minimal programming expertise to take advantage of them.  
the damia data integration technology  along with key aspects of the original damia feed server such as its user-interface and directory services design  have since been made available for download on ibm alphaworks  in the context of mashup hub  an enterprise feed server that the damia research team is jointly developing with ibm software group.  mashup hub is a key technology of ibm's information 1 initiative   which aims to extend the reach of the enterprise information fabric into the desktop  web and other new data sources  and to provide tooling that facilitates situational application development.  
1 paper organization 
in this paper  we will describe the key features and design of damia  focusing primarily on the data integration technology. the remainder of the paper is organized as follows. in section 1  we give an overview of the architecture of the damia feed server that was deployed on the ibm intranet1.  in section 1  we present the data model  data manipulation operators  the data mashup compiler  and other details of the damia integration engine. in section 1  we illustrate damia's capabilities with use cases. sections 1 and 1 discuss related and future work  respectively.   
1. damia feed server  
this section provides a general overview of the damia feed server architecture. the integration engine  which is the primary focus of the damia project  is discussed in greater detail in section 1.  
the main components of the damia feed server are depicted in figure 1. in addition to the damia integration engine  the server is further comprised of a directory services component  a storage services component  and a rich client browser interface. 

figure 1 - feed server architecture 
1 integration engine 
damia provides a powerful collection of set-oriented feed manipulation operators for importing  filtering  merging  grouping  and otherwise manipulating feeds. these operators manipulate a general model of a feed  which can accommodate prevalent feed formats such as rss and atom  as well as other xml formats. data mashups are comprised of a network  or flow  of feed manipulation operators. the browser-based client interface provides a gui for designing flows.   
a flow is presented to the integration engine in the form of an xml document that depicts its flow representation in a serialized format. the flow is compiled into a set of lower level primitives that can be executed. a given feed manipulation operator might compile into number of lower level primitives.  
a compiled data mashup has an associated url; hence  the result of the data mashup can be retrieved via a simple rest call.  
1 directory services 
the feed server allows for general information assets to be cataloged  uploaded  and stored. examples of information assets include data mashups  public spreadsheets  and urls to interesting external feeds that might be used in data mashups. the directory services component provides capabilities to search  tag  rate  execute  and otherwise manage these information assets.  in effect  it provides a web 1 style framework for communityoriented information management. it also manages user profiles  authentication  and access control for assets. the directory services component uses the storage services component to store assets and metadata.  
1 storage services  
the storage services component handles the storage and retrieval of data and metadata needed by other damia components. for example  it is used by the directory services component to store community information assets and associated metadata. the storage services component is also used by the metadata services component of the damia integration engine to execute data mashups effectively.        
1 client interface  
situational applications are typically created by departmental users with little programming knowledge; consequently providing an intuitive interface where data mashups can be composed visually was a critical design point for damia. toward this goal  we developed a browser-based user interface that provides facilities for composing  editing  and debugging data mashups graphically. figure 1 shows a snapshot of the data mashup editor.  
 
figure 1 -snapshot of the damia gui 
the gui allows users to drag and drop boxes  representing damia operators  onto a canvas and to connect them with edges representing the flow of data between those operators. users can use a preview feature to see the result of the data flow at any point in the process. this approach makes the development process more natural and less error prone. once the data mashup design is completed  the result is serialized as an xml document and delivered to the server for further processing. it communicates with the server through a set of rest api interfaces  as illustrated in figure 1. the client also provides an interface to directory services capabilities  thus allowing users to search for  and manage information assets from the same client interface that they use to compose data mashups. 
1 implementation 
the damia feed server was implemented with a lamp stack. in particular  the integration engine  directory services  and other components are implemented in php. there were a few basic libraries from the lamp ecosystem that we exploited. for example  we exploited the zend framework for caching. the storage services component makes use of a relational database to store resources and metadata. we currently support either db1  or mysql. the client interface is an ajax application implemented using the dojo toolkit . the features of php used by the data integration engine are described in section 1.  
1. damia integration engine 
the damia integration engine compiles and executes data mashups. figure 1 depicts the overall architecture and relevant apis. the integration engine is comprised of a flow compiler  metadata services  and an augmentation engine.   

figure 1: damia data integrate architecture 
the flow compiler receives an xml specification of a data mashup and translates it into an augmentation flow  which is the manifestation of the data mashup that is executed by the augmentation engine. the xml specification represents the data mashup in terms of the conceptual feed-oriented data model and feed manipulation operators presented to end users; hence  it is the flow compiler that effectively implements the feed abstraction.  
metadata services stores the augmentation flow and the original xml representation of the data mashup  associating it with a resource identifier that can be used in subsequent execute  edit  and delete operations. metadata services also manages metadata and functions needed by the flow compiler and augmentation engine in order to perform their tasks. for example  it manages ingestion functions  which are needed to map imported data resources to equivalent xml representations.   
the augmentation engine executes a data mashup. it receives the resource identifier of the corresponding augmentation flow  which it retrieves from metadata services and evaluates. typical result formats are atom  rss  or general xml. the data manipulation primitives comprising an augmentation flow are implemented in php; thus  at its most basic level  an augmentation flow is simply a php script that is interpreted by a php engine.  
the augmentation engine is conceptually divided into an ingestion layer  augmentation layer  and publication layer. the ingestion layer is responsible for importing data and for mapping imported data into an instance of the augmentation-level data model. it also contains a resource cache from which it can serve resources in order to avoid accessing data sources. the augmentation layer is responsible for manipulating instances of the augmentation-level data model in order to produce the data mashup result. it is comprised of augmentation operators that can evaluate xpath expressions; perform sorts  joins  grouping  construction  and low level manipulations. the publication layer is responsible for transforming an instance of the augmentation-level data model to a specific format like rss  atom  or json  which it then serializes for consumption by web applications.       
the following subsections describe the data models  and the various components and layers of the damia integration engine in greater detail.   
1 data model 
feeds formats like rss  and atom  are prevalent xml data interchange formats. feeds often represent a set of data objects such as stock quotes  real estate listings  or employee records  which have been serialized for transport across a network. damia provides a powerful collection of set-oriented feed manipulation operators for importing  filtering  merging  grouping  and otherwise manipulating feeds. the feed-oriented data model that forms the basis for such manipulation can easily represent standard feed formats like rss and atom  but is designed to handle more general xml data formats that have repeating fragments that can be mapped to the model.  
data mashups are built by end users and applications in terms of the feed-oriented data model and feed manipulation operators; however  these are logical constructs that have no direct physical implementation. feed manipulation operators are compiled into augmentation operators  which are lower level data manipulation primitives that can be executed by the integration engine. the augmentation-level data model  which forms the basis for data manipulation by augmentation operators  is a derivative of the xquery data model  xdm  . it is somewhat simpler than xdm  however. for example  it does not support node identity  or the full complement of data types.  
1.1 augmentation-level data model  
the augmentation-level data model  adm  is comprised of nodes  atomic values  items  sequences  and tuples. a node is the root node of a tree of nodes. a node can be an element node  attribute node  text node  or any other type of xdm node.  an atomic value corresponds to an instance of a simple data type like a string  integer  or date. an item is either a node or an atomic value. a sequence is a named list of zero or more items. finally  a tuple is a non-empty set of sequences.   
augmentation operators are closed under adm. they consume one or more sets of tuples  or tuple streams  and produce a tuple stream. one class of operator works at the sequence level  extending tuples with new sequences derived from other sequences. for example  a union operator creates a new sequence by combining the items of two or more sequences. another class of operator works at the tuple level. for example  the sort operator reorders the tuple stream based on the values of specified sequences that define the ordering key.  
an augmentation flow is a network of augmentation operators that is executed using a data flow paradigm. the expressive power of an augmentation flow is analogous to that of an xquery flwor expression. consider figure 1  which illustrates the relationship. it shows a simple xquery flwor expression that creates a feed whose entries contain information about hotels in vancouver joined with their reviews. hotel information is coming from an atom feed provided by www.hotels.xyz  while hotel reviews come from an rss feed provided by www.reviews.xyz.  

figure 1 augmentation flow 
figure 1 depicts an abstract representation of an augmentation flow that is equivalent to the xquery. the import  iterate  filter  and construct augmentation operators perform similar functions as the xquery doc function  for clause  where clause  and return clause  respectively. the extract operator evaluates xpath expressions. the graph shows sequence names flowing as tuples along the edges between augmentation operators. these sequences correspond to the sequence variables in the xquery  which flow as binding tuples between flwor operations  
the steps of retrieving the hotel feed  extracting feed entries  iterating each entry  and extracting the hotel name from an entry  as carried out in lines 1  1  1  and 1 of the xquery  is implemented by the similarly numbered import  extract  iterate  and extract operators. note that each of the operators projects sequences that are not needed in the data flow. for example  the extract operator drops sequence $a  which contains the entire atom feed retrieved from www.hotels.xyz  after it has extracted all of the feed entries to form sequence $b.   
the fuse operator joins the tuple streams generated from the series of operations applied to the individual feeds retrieved from the two sources. the group operator aggregates tuples into sequences  as is required in the example to turn the set of output tuples from the flwor expression into a sequence. in addition to implementing the xquery return clause  the construct operator is also used to construct the final result document from the result sequence. in general  a construct operator is used to construct new sequences by substituting specified input sequences into a supplied xml template.    
damia has a number of other augmentation operators besides those required to implement a simple xquery flwor expression. for example  the hsjoin operator can perform a symmetric join comparable to a relational hash join.  
our objective is not to develop an implementation of xquery in our augmentation layer  as we did not want to confine ourselves strictly to the operations and semantics that the standard defines; however  there were definitely benefits to patterning our augmentation-level data model and corresponding augmentation operators after xdm and xquery. most importantly  it provided us with a strong semantic foundation for the manipulation of xml data. further  it provided us with a guiding context in which to make extensions. for example  we came across a proposal for adding direct support for grouping to xquery which helped influence the design of our group operator . section 1.1 discusses the group operator and other damia augmentation operators in more detail.  
1.1 feed-oriented data model 
the feed-oriented data model is comprised of containers  payloads  feeds  and tuples. a container is a special root node of a tree of nodes. child nodes of a container node are restricted to element nodes; however  other nodes in the sub-tree rooted at the container node can be any xdm node.  the term payload refers to the list of zero or more child nodes of a container node.  a feed is a named list of zero or more containers. a tuple is a non-empty set of feeds.  
 the mapping of rss and atom formatted xml documents to this model is straightforward  and handled automatically by damia. one container node is created per rss item  or atom entry  element. the payload of each container node is comprised of the element nodes that are children of the corresponding item or entry nodes. damia feeds can be derived from other types of xml documents as well. all that is required is an xpath expression that identifies the payload. for example  the payload for a damia feed that represents a set of real estate listings might be extracted from an xml document via the xpath expression //listing. multiple instances of the feed-oriented data model can be extracted from the same document in this way. the process of mapping an xml document to a damia feed maintains the payload in the same relative order as per the original document.  the import-feed operator is responsible for mapping an xml document to an instance of the feed-oriented data model.  
feed manipulation operators are closed under the feed-oriented data model. each operator consumes one or more feeds and produces a feed.  for example  the filter-feed operator takes a feed and a predicate as input  producing a new feed that is essentially a copy of the input feed  sans containers whose payload did not satisfy the specified predicate. a merge-feeds operator  which is analogous to a symmetric relational join operation  concatenates payloads from two different input feeds that match according to a specified join predicate. 
a data mashup is represented as a data flow graph of feed manipulation operators. nodes in the graph represent operators  and edges represent the flow of feeds between operators. feeds are packaged into tuples  which represent the unit of flow between operators. we introduced the notion of tuples into the feed-oriented data model in order to allow multiple versions of feeds to flow between operators. this capability enables different result feeds to be created for different subscribers  all in the same data mashup.  
feed manipulation operators are implemented by augmentation flows. feeds manifest at the augmentation level as sequences whose items are rooted by a special container node. the child elements of the container node are its payload. in general  feed manipulation operators iterate over the container nodes of a sequence and perform filtering  joins  aggregation  and other set manipulations that involve the extraction and comparison of attribute and element values of the payload. the implementation of a data mashup is constructed by first expanding each individual feed manipulation operator into a subgraph of equivalent augmentation operators. a global optimization step is subsequently performed in order to transform the initial graph representation into one that can be executed more efficiently.  section 1 discusses feed manipulation operators in more detail.  section 1 details the process by which feed manipulation operators are compiled into augmentation flows.    
1 ingestion layer  
the damia ingestion layer is comprised of connectors  ingestion functions  an import operator  and a resource cache.  
a connector is essentially a wrapper that implements an http interface to a particular data source. for example  a custom connector might be created to wrap  the db1 database for department k1   or the  acme finance  website. connectors understand the access protocols and apis of the data source  they handle authentication  and take care of other detailed aspects of data source access.  a connector can return files with any of the mime types currently understood by the damia ingestion layer. an example of a built-in connector is a simple file upload connector that allows users to upload spreadsheets  and other desktop data  thereby making it http accessible for reference in data mashups. the set of built-in connectors was designed to be extended by the damia community.  
an ingestion function maps a file of a specific mime type to an xml representation. for example  damia currently supports a builtin ingestion function for mapping comma separated files of mime type text/csv to an atom feed representation. the set of built-in ingestion functions can also be extended by the damia community.  
the import operator maps a resource provided by a data source into an instance of the augmentation-level data model by  1  invoking connectors or other web services to retrieve resources;  1  applying the appropriate ingestion function to a resource based on the mime type of the resource;  1  parsing and mapping the xml representation of the resource into the data model.  although the import operator is conceptually part of the ingestion layer  it supports the same iterator protocol as other augmentation operators  so we defer a more detailed discussion to section 1.  
the resource cache provides basic caching of imported resources. it supports a simple put and get interface to insert and retrieve resources. the url of the resource serves as the resource identifier for those calls. caching is only in effect for the leaf nodes of a data mashup and the import operator is the object that interacts with the cache. a cache policy requirement can be provided with a cache get call. we currently support only a freshness cache policy  which sets a limit  in terms of number of seconds  as to how stale a returned resource can be. more sophisticated policies will certainly be needed as we consider new scenarios. resources are aged out of the cache in a cyclic fashion  with older resources evicted first.  
1 augmentation layer 
the damia augmentation layer is comprised of a set of augmentation operators that are closed under the augmentation-level data model  as was described in section 1. data mashups are compiled into augmentation flows  which are networks of augmentation operators that execute in a demand-driven data flow fashion. augmentation operators produce and consume tuples according to an iterator protocol . as such  each operator supports bind-in  open  close  and next interfaces  which are used by other operators to initialize and uninitialize the operator  and to iteratively consume the tuple stream that the operator produces.  
operators have arguments which define input operands  as well as the operator's relationship to other operators in an augmentation flow. augmentation operators execute within a bind-in context which provides values for variables that are referenced in arguments. for example  a url parameter that returns the price for a given stock would contain a variable reference to the stock in the url  which it would pull from the bind-in context. the bind-in context for all operators is initialized with the attribute-value pairs in the http context passed to the data mashup when it is executed; hence  variables can be passed to operators from outside the data mashup. nested loop operations can extend the bind-in context of inner loop operators with attribute-value pairs passed from the outer loop. an operator's bind-in method  which provides and operator with its bind-in context  must be executed before an operator is opened. 
1.1 implementation 
augmentation operators are implemented as php classes. data mashups are essentially compiled into php scripts that are interpreted by the php engine. the augmentation operators rely on a couple of basic php features such as php dom support for parsing xml documents and for executing xpath statements  as well as php curl libraries for importing resources via http.  
