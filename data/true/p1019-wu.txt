the increasing complexity of enterprise databases and the prevalent lack of documentation incur significant cost in both understanding and integrating the databases. existing solutions addressed mining for keys and foreign keys  but paid little attention to more high-level structures of databases. in this paper  we consider the problem of discovering topical structures of databases to support semantic browsing and large-scale data integration. we describe idisc  a novel discovery system based on a multi-strategy learning framework. idisc exploits varied evidence in database schema and instance values to construct multiple kinds of database representations. it employs a set of base clusterers to discover preliminary topical clusters of tables from database representations  and then aggregate them into final clusters via meta-clustering. to further improve the accuracy  we extend idisc with novel multiple-level aggregation and clusterer boosting techniques. we introduce a new measure on table importance and propose an approach to discovering cluster representatives to facilitate semantic browsing. an important feature of our framework is that it is highly extensible  where additional database representations and base clusterers may be easily incorporated into the framework. we have extensively evaluated idisc using large real-world databases and results show that it discovers topical structures with a high degree of accuracy.
categories and subject descriptors
h.1  database applications : data mining
general terms
algorithms  design  experimentation  performance
keywords
database structure  topical structure  discovery  clustering
1. introduction
　a large enterprise typically has a huge number of databases that are increasingly complex  1  1 . for example  the database for a single sap installation might now contain
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
hundreds or even thousands of tables  storing several terabytes of data . to make things worse  the documentation and metadata for these enterprise databases are often scattered throughout the it departments of an enterprise- they are incomplete  inaccurate  or simply missing . in fact  a recent study  1  1  indicates that up to 1% of a data architect's time is actually spent on discovering the metadata of databases. thus  the scale of the databases along with the prevalent lack of documentation  make it a daunting task for data architects and application developers to understand the databases and incur significant cost in integrating the databases  1  1 .
　to illustrate these challenges  consider a data architect who tries to understand and integrate two large humanresource databases hr1 and hr1  shown in the source and target panes of figure 1.a respectively. these databases are taken from a real-world scenario described in section 1. suppose that hr1 has 1 tables  hr1 has 1 tables  and on the average there are 1 attributes per table. both databases were designed by contractors and have been in service for several years. the designers left the company but did not leave any design documents. furthermore  the implementation of the databases might not be consistent with the design. for example  the referential relationships of tables often are not enforced in the databases  due to varied reasons including the cost of enforcing the constraints  1  1 . all these make it extremely difficult for the data architect to understand  reverse-engineer  and integrate the databases.

	 a  without the topical structures	 b  with the topical structures
figure 1: understanding & integrating large databases　a key step in integrating the databases is to identify the semantic correspondences or mappings among the attributes from different databases  1  1 . the scale of the databases again poses serious challenges to this schema matching task. existing matching solutions typically attempt to find mappings between every two attributes  1  1  1 . this all-to-all approach is based on the assumption that the databases are small and all attributes in one database are potentially relevant to all attributes in another database. this assumption might not hold for large databases . for example  tables in both hr1 and hr1 may be naturally divided into several subject areas or topics such as employee and claim  and the tables from different subject areas are likely not very relevant. as a result  the all-to-all approach is inefficient in that it requires 1m attribute-level comparisons  and inaccurate in that it may match many attributes from irrelevant tables. to illustrate  arrows in figure 1.a indicate tables whose attributes are matched by this approach.  to avoid the cluttering  not all arrows are shown.  for example  a false mapping is discovered between attribute empid of table employee in hr1 and labor claim id of lbr clm  labor claim  in hr1  which contain very similar data values.
　to address these challenges  we consider the problem of discovering the topical structure of a database that reveals how the tables in the database are organized based on their topics or subject areas. conceptually  each topic or subject area comprises a group of closely related entities  where each entity may be represented by multiple tables in the database  e.g.  due to normalization .
　the topical structure of a database provides an intuitive way of browsing the semantic content of the database  and helps users to quickly find relevant information in a large database. for example  figure 1.b shows the tables in hr1 and hr1 organized by their topics  where each topic is labeled with the name of the most representative table among all the tables on that topic  details are given in section 1 . for example  in hr1  the eight tables on the employee information are grouped under the topic employee.
　knowing topical structures of databases also enables a new approach to matching large database schemas that is both more scalable and more effective than the previous all-to-all approach. specifically  the matching of two large schemas can now be done in a top-down  divide-and-conquer fashion. first  we find similar topics in two databases  where each topic may be simply represented as a text document  e.g.  comprising tokens in table names  attribute names  and data values from the tables on that topic. suppose that hr1 has 1 topics and hr1 has 1 topics. this step involves only 1 comparisons  between text documents   a huge saving from the 1m attribute-level comparisons required by the previous approach. to illustrate  figure 1.b shows several similar topics discovered between hr1 and hr1  indicated by arrows. for example  topic employee in hr1 is similar to emp in hr1  and clm lbr in hr1 to clm wrkarea  claim work area  in hr1. next  we can focus the further matching effort on the attributes from the tables on similar topics. this would avoid producing many false mappings between attributes from irrelevant tables  e.g.  the mapping between employee.empid and lbr clm.labor claim id  discovered by the previous approach.
　the problem of mining structures of databases has been studied in the context of data cleansing and data integration  1  1  1  1 . however  the focus of previous research was mostly on the discovery of keys  1  1   foreign keys  1  1   and functional dependencies   while the problem of discovering topical structures has received little attention. a related problem is schema summarization  which produces an overview of a complex schema with important elements in the schema. it measures the importance of an element by the number of its relationship & attribute links and the cardinality of its data values. it is not concerned with the topics of the elements. for example  there may be multiple elements in the summary which are all on the same topic  or there may be no elements in the summary to represent the less dominant topics in the schema. in contrast  our goal is to categorize the elements by their topics and exploit the topical structures to not only facilitate semantic browsing but also address the scalability issue in existing schema matching solutions to support a large-scale integration. in addition  we introduce a new measure on table importance based on shortest paths  and propose an approach to discovering representative tables within each topic.
　in this paper  we describe idisc  a system that automatically discovers the topical structure of a database through clustering. developing idisc required several innovations.
modeling databases: first  how should we represent the database to the clustering process  there is much disparate evidence on the topical relationships among the tables  e.g.  table & attribute names  attribute values  and referential relationships. as a result  it may be very difficult to come up with a single best representation. to address this challenge  we propose a novel modeling approach which examines the database from several distinct perspectives and computes multiple representations of the database based on its schema information and instance values.
　we describe methods for constructing three very different kinds of representations: vector-based  graph-based  and similarity-based. in a vector-based representation  each table is represented as a text document and the database as a collection of documents; in a graph-based representation  the database is represented as a graph  where nodes are tables and edges indicate the linkages  e.g.  referential relationships  between the tables; and in a similarity-based representation  the database is represented as a similarity matrix with table similarities computed from attribute values.
combining evidence: second  which clustering algorithm s  should we employ to discover topical clusters from the database representations  every algorithm typically has its own strength and weakness  and some may be more suitable for certain representations than others. no single algorithm can perform well over all representations. to address this challenge  we propose a novel discovery framework based on the multi-strategy learning principle . in this framework  a multitude of base clusterers are employed  each takes a representation and invokes a suitable clustering algorithm to discover preliminary topical clusters of tables. the results from the base clusterers are then aggregated into final clusters via meta-clustering. we further propose several approaches to constructing generic similarity-based and linkage-based clustering algorithms and describe how to instantiate them into clusterers for different representations. the proposed framework is unique in that:  1  it is highly extensible  where additional database representations and base clusterers can be easily incorporated to the system  to further improve its performance.  1  it provides an intuitive and principled way of combining evidence through aggregating the votes from the clusterers  rather than directly combining the disparate evidence from the database via an adhoc function.
handling complex aggregations: a key component of the framework is meta-clusterer. the meta-clusterer must identify and remove errors in the input clusterers and combine the strength of different clusterers  in order to produce better clusters. a similar problem has been studied in the context of clustering aggregation . unfortunately  existing solutions suffer from several key limitations.
　first  flat aggregation: all base clusterers are aggregated at once by a single meta-clusterer. nevertheless  some clusterers are inherently more similar than others. for example  clusterers using the same kind of representations may be more similar or correlated since they look at the database from similar perspectives. in other words  they are like experts with similar  but not exactly the same  expertise. intuitively  it is easier to identify the errors made by an expert if we compare him to others with similar expertise. to address this limitation  we introduce the concept of similarity level for clusterers and propose an approach to organizing the clusterers into an aggregation tree to perform a multilevel aggregation. we show that the new aggregation approach significantly improves idisc's performance.
　second  equal combination: all the input clusterers are treated as being equally good by the meta-clusterer. nevertheless  the performance of the clusterers may often vary a lot  depending on the characteristics of a particular data set: the same clusterer may perform well on one data set but very poorly on another. it is thus desirable to be able to dynamically adjust the weights of the clusterers on-the-fly so that the votes from the better-performing clusterers are weighted more. to address this problem  we propose a novel clusterer boosting approach and shows that it can effectively identify and boost good clusterers based on their run-time performance.
in summary  this paper makes the following contributions:   we formally define the problem of discovering topical structures of databases and demonstrate how topical structures can support semantic browsing and largescale data integration.
  we propose a novel multi-strategy discovery frame-

figure 1: invdb: an invoice management database work and describe the idisc system which realizes this framework. the system is fully automatic & highly extensible to other representations and clusterers.   we propose novel clustering aggregation techniques to address limitations of existing solutions.   we propose a new approach to finding representative tables using a novel measure on table importance.
  we have extensively evaluated idisc over real-world databases  and results indicate that it discovers topical clusters of tables with a high degree of accuracy.
　the rest of the paper is organized as follows. section 1 defines the problem. sections 1 describe the idisc system. section 1 presents our results and discusses the current system's limitations. section 1 reviews related work. section 1 discusses future work and concludes the paper.
1. problem definition
　we first formally define the problem. we will use the invoice management database invdb  shown in figure 1  as the running example. note that key attributes are underlined and referential relationships between tables are indicated by directed lines from foreign keys to primary keys. note also that these keys and foreign keys may not be documented or enforced and may need to be discovered  see section 1.1 . we can observe that the tables in invdb actually fall into three categories or topics: invoice  the tables in figure 1.a   shipment  the tables in figure 1.b   and product  the tables in figure 1.c . the goal of idisc is then to automatically discover these topics and the tables on each topic. we start by defining topical relationship & structure.
topical relationship: consider a set of topics p. consider further a database d with a set of tables  where each table t is associated with a topic p （ p  denoted as topic t  = p. then  we say that there exists a topical relationship between two tables s and t  denoted as ρ s t   if topic s  = topic t . for example  consider the database invdb in figure 1. suppose p = {invoice  shipment  product}. an example topical relationship is ρ invoiceitem  invoiceterm   since topic invoiceitem  = topic invoiceterm  = invoice.
　note that ρ is transitive  i.e.  if ρ r s  and ρ s t   then ρ r t . clearly  ρ is both reflexive and symmetric. as a result  ρ defines an equivalence class. the topics in p are assumed to be mutually exclusive  so each table in d may be associated with only one topic in p.

figure 1: the idisc architecture
topical structure: the topical structure of a database describes how the tables in the database are grouped based on their topical relationship. more precisely  consider a set of topics p  a database d  and a topical relationship ρ between the tables in d with respect to p. the topical structure of d is given by a partition c = {c1 c1 ... ck} formed by ρ over the tables in d  such that the tables in the same group ci are on the same topic  while the tables from different groups are on different topics. for example  the topical structure of invdb with respect to the above p is: {c1 c1 c1}  where c1 = {invoicestatus  invoiceterm  invoice  invoiceitem}  c1 = {shipment  shipmentmethod}  and c1 = {product  productcategory  category}.
　based on the above definitions  we define our problem as follows. we discuss the extensions of the problem to multiple topics per table and hierarchical topical structure in section 1 and section 1.
problem definition: given a database d with a set of tables  discover:  1  a set of topics p  which the tables in d are about; and  1  the topical structure of d with respect to p  in the form of a partition c = {c1 c1 ... ck} over the tables in d  where k = |p|.
1. the idisc approach
　idisc takes as the input a database d with a set of tables  and returns the topical structure of d as the output. figure 1 depicts the architecture of idisc. it consists of four major modules: model builder  base clusterers  meta-clusterer  and representative finder.
　idisc proceeds as follows. first  the model builder examines d from a number of perspectives and obtains a variety of representations for d. next  each base clusterer takes a representation and discovers a preliminary topical clustering  i.e.  partition  of the tables in d. these results are then aggregated by the meta-clusterer into a final clustering. finally  the representative finder takes the final clustering and discovers representatives in each cluster.
　this section describes the model builder  the base clusterers  and the meta-clusterer. section 1 extends the metaclusterer to handle complex aggregations. then section 1 describes the representative finder.
1 the model builder
　the model builder constructs varied representations for the database from its schema information and instance data.
these representations fall into three categories: vector-based  graph-based  and similarity-based.
1.1 vector-based representations
　vector-based representations capture the topical structure of the database via the descriptive information on the tables. in a vector-based representation  each table is represented as a text document and the database as a collection of documents. note that in these representations  the structures of individual tables are ignored. there are many possible ways of constructing such documents for the tables in the database  each resulting in a different representation of the database. for example  the document for a table may contain tokens from the name of the table  the names of its attributes  and the content of its attribute values.
　to illustrate  consider constructing documents for the tables in invdb  figure 1   such that the document for each table contains tokens from both table and attribute names. then the document d for the table invoicestatus will comprise tokens: invoice  status  id  and code  where both invoice and status occur twice in d.
　suppose that the number of unique tokens among the documents for the tables in the database d is n. then  each document d may be represented as an n-dimensional vector  w1 w1 ... wn   where the i-th dimension corresponds to the i-th unique token in d  and wi is the weight of the token for the document d. many weighting functions may be employed  e.g.  the tf   idf weight   and different functions may produce very different representations.
1.1 graph-based representations
　graph-based representations capture the topical structure of the database via the linkage among the tables. specifically  the database is represented as a graph  where nodes are tables and edges indicate the linkage between the tables. an important linkage between two tables is their referential  i.e.  fk-pk  relationship  where some attributes in one table  i.e.  foreign keys  refer to some key attributes in the other table. for example  there is a referential relationship between table invoiceterm and table invoice in invdb  since the attribute invoiceid in invoiceterm refers to the key attribute invoiceid in invoice.
　however  the information on keys and foreign keys is often missing in the catalogs  for a variety of reasons including the cost of enforcing the constraints  1  1 . in fact  in the databases for our experiments  section 1   keys and referential constraints are neither documented nor enforced by the system. to address this challenge  idisc implements a method similar to  to discover primary keys and then proceeds as follows to discover foreign keys.
　consider a key-attribute a in table t and an attribute b in table is determined to be a foreign key referring to a in t  if the following conditions are met:  1  |b”a| = |b|  i.e.  b is a subset of a. this is a necessary condition for b to be a foreign key.  1  |b|   1  which is to avoid many false discoveries for boolean attributes  since a boolean attribute may be contained in any other boolean/integer attributes.  1  |b|   .1|a|  which is to ensure that the domain of b is sufficiently similar to that of a  not just contained . note that if |a| ＋ 1  then condition 1 might not be satisfied even when condition 1 is satisfied  e.g.  when |a| = |b| = 1.  1  namesim a b    .1  where namesim is a measure  with range  1   on the similarity of attribute names.
1.1 similarity-based representations
　similarity-based representations capture the topical structure of the database via the value-based similarity between the tables. the idea is that if two tables are about the same topic  they may have several attributes containing similar values  1  1 . for example  the values for the attribute invoicestatus.invoiceid in invdb  figure 1  should be similar to those for invoiceterm.invoiceid.  note that there are no referential relationships between these two tables. 
　in a similarity-based representation  the database d is represented with a |d| 〜 |d| matrix m  where |d| is the number of tables in d and the entry m i j  stores the similarity between the i-th and j-th tables in d. there are many different ways of evaluating the table similarity  each resulting in a different representation for the database. currently  idisc employs the following procedure to evaluate the similarity between tables t and t.
1. evaluate value similarity between attributes: for every two attributes x and y   one from each table  compute their similarity as the jaccard similarity between the sets of values in x and y   i.e.  j x y   = |x ” y |/|x “ y |.
1. discover matching attributes: this step then finds a set z of matching attributes based on a greedy-matching strategy :  1  let z =    u = all attributes in t  and v = all attributes in .  1  find u （ u and v （ v such that they have a maximum  positive  similarity among all pairs of attributes from u and v.  1  add attribute pair  u v   to z  remove u from u and v from v.  1  repeat steps 1 and 1 until no more such pairs can be found.
　for example  consider tables t = invoicestatus and t = invoiceterm. suppose that j t.invoiceid  t.invoiceid  = .1  j t.invoiceid  t.termtype  = .1  j t.statuscode  t.termtype  = .1  and no other attributes have similar values. then  the first iteration matches attribute t.invoiceid and attribute t.invoiceid  and the second  final  iteration matches t.statuscode and t.termtype.
1. evaluate table similarity: the similarity of t and t  denoted as sim    is then given by the average similarity of their matching attributes:   where |t| is the number of attributes in t. for example  sim invoicestatus  invoiceterm  =  .1 + .1  / 1 = .1.
　to summarize this section  we stress that idisc's goal is not to build best models  which typically do not exist   but to show that it can produce a better solution by building & combining many different  possibly imperfect  models.
1 the base clusterers
　as described  the job of a base clusterer is to take a database representation and discover a preliminary clustering over the tables in the database. rather than building the individual clusterers separately & repeatedly  idisc first implements several generic clustering algorithms and then instantiates them into clusterers. in this section  we first describe two generic algorithms employed by idisc: similaritybased and linkage-based. we then show how to instantiate the former into clusterers for the vector-based and similaritybased representations  and the latter into clusterers for the graph-based representations.
1.1 generic similarity-based algorithm
　figure 1 shows simclust  a generic similarity-based clustering algorithm. simclust takes as the input a set of tables
simclust t   m  clsrsim  q  ★ c
input: t   a set of table {t1 t1 ... t|t |}; m  a similarity matrix for the tables in t ;
clsrsim  a cluster similarity function;
　　q  a clustering quality metric output: c  a partition of tables in t
1. set up initial clusters:
1 let i = 1
1 let c1 = {{t } {t1} ... {t	}}
1. repeat until
1 evaluate the quality of ci via q
1 evaluate the similarities of clusters in ci via clsrsim
1 find cx cy （ ci with a maximum similarity
1 merge clusters cx and cy
1 i ○ i + 1
1. return ci with a maximum q valuefigure 1: the generic similarity-based algorithm
t = {t1 t1 ... t|t |}  a similarity matrix m whose entry m i j  is the similarity between tables ti and tj in t   a cluster similarity function clsrsim  and a clustering quality metric q. it outputs a partition c over the tables in t . essentially  simclust can be regarded as a highly customizable hierarchical agglomerative clustering algorithm with an automatic stopping rule .
　simclust starts by placing each table in t in a cluster by itself. this generates the first version of the clustering c1. it then evaluates the quality of c1  based on the clustering quality metric q. it also computes the similarities among the clusters in c1  based on the similarity matrix m and the cluster similarity function clsrsim. next  it chooses two clusters with a maximum similarity and merges them into a single cluster. this generates the next version of the clustering c1. it then repeats this process until all the tables are placed in a single cluster. finally  simclust returns as the output the clustering with a maximum q value  among all the |t | versions of clusterings.
　simclust provides two customization points: clsrsim and q. clsrsim is a cluster similarity function which takes the similarity matrix m and two clusters of tables  cx and cy  and computes a similarity value between cx and cy. there are many different ways of implementing clsrsim  such as single-link  complete-link  and average-link  where the cluster similarity is respectively taken to be the maximum  minimum  and average similarity between two tables  one from each cluster .
　q is a metric for evaluating the quality of clusterings. determining the number of clusters in a data set is a wellknown difficult problem . many methods have been proposed  such as elbow criterion  gap statistics  and crossvalidation  but there is no best solution. intuitively  a good clustering should be one such that objects within the same cluster are similar while objects from different clusters are dissimilar. based on this intuition  idisc implements a default q as follows:
 intrasim ci    intersim ci     1 
where n is the total number of tables in the database  and |ci| is the number of tables in cluster ci （ c. intrasim ci  is the average similarity of tables within the cluster ci  while intersim ci  is the maximum similarity of ci with any other cluster in c  where the cluster similarity is the average similarity of tables between clusters. this default q is intuitive  easy to implement  and has performed quite well over several
linkclust 
input: t   a set of tables;
g  a linkage graph for the tables in t ;
edgedel  a function that suggests edges to be removed; q  a clustering quality metric
output: c  a partition of tables in t
1. let i = 1
1. repeat until g has no edges
1 let ci = connected components in g
1 evaluate the quality of
1 let ec = edgedel g 
1 remove edges in ec from g i ○ i
with a maximum q value   figure 1: the generic linkage-based algorithm databases in our experiments. but note that q is customizable to other possible implementations  see section 1.1 .
1.1 generic linkage-based algorithm
　figure 1 shows linkclust  a generic linkage-based algorithm. unlike simclust  linkclust discovers groups of related tables based on their linkage information. for example  figure 1 shows the tables in invdb  figure 1  and their linkage information.  the details on how to obtain this graph will be given in section 1.1 .
　the main idea of linkclust is to formulate the problem as one of discovering community structure in an interconnected network  e.g.  a social network or the internet. a key observation is that often the links within a community are dense  while the links between communities are relatively sparse. by finding and removing those inter-community links  we may reveal the communities in the network. for example  dotted edges in figure 1 are the inter-community links in the graph.
　linkclust takes as the input:  1  a set of tables t ;  1  a undirected graph g whose vertices are tables in t and edges indicate the linkage between the tables;  1  a function edgedel that suggests edges to be removed from g; and  1  a metric q on the clustering quality. it returns a partition of tables in t as the output. linkclust is a divisive algorithm. it starts by finding connected components in g  which forms the first version of the clustering. it then removes edges suggested by the edgedel from g to produce the next version of the clustering. the process is repeated until no edges remain in g. finally  the version of the clustering with the highest q value is returned as the output.
　linkclust also has two customization points: edgedel and q. we first describe two possible implementations of edgedel: one based on shortest-path betweenness   and the other based on spectral graph theory .
 a  shortest-path betweenness  sp : the key idea is to first find the shortest paths between the vertices  and then measure the betweenness of an edge  i.e.  the possibility of the edge lying between two clusters  by the fraction of the shortest paths that contain the edge. for example  in the linkage graph shown in figure 1  the number of shortest paths that contain the edge  invoiceitem  product   an intercommunity link  is 1  shown on the edge   the maximum among all the edges  while the number for the edge  invoice  invoiceterm   a within-community link  is only 1.
　more precisely  the betweenness of an edge e （ e  denoted as β e   is given by:   where σst is the number of distinct shortest paths between vertices s and t  and σst e  is the number of distinct shortest paths between s and t that contain the edge e. edgedel g  then returns an edge with a maximum β value.
 b  spectral graph partitioning  spc : in this case  edgedel returns an edge-cut of g  which comprises a set of edges which are likely lying between two clusters. spectral graph theory provides an elegant way of finding a good edgecut. specifically  consider g's laplacian matrix lg = dg ag  where dg is a diagonal matrix whose entry d i i  is the degree of the i-th vertex in g  and ag is g's adjacency matrix. then it can be shown that finding a minimum edge-cut of g corresponds to finding the smallest positive eigenvalue λ1 of lg . further  the eigenvector for λ1  known as fiedler's vector  suggests a possible bi-partitioning of the vertices in g  where the vertices with positive values are placed in one cluster and the vertices with negative values in the other cluster. for example  figure 1 shows these values next to the vertices. accordingly  the three tables about product will be placed in one cluster  and the rest of the tables in another cluster. note that if g contains several connected components  edgedel finds edge-cuts for larger components  with more vertices  first.
metric q: similar to q in simclust  q measures the quality of clusterings in linkclust. q captures the intuition that a good partition of the network should be one such that nodes within the same community are well-connected  while there are only few edges connecting different communities. based on this intuition  idisc implements a default q as follows :
	 	 1 
where |e| is the total number of edges in the graph  |eii| is the number of edges connecting two vertices both in the cluster ci  and |ei| is the number of edges that are incident to at least one vertex in ci. note that |eii|/|e| is the observed probability that an edge falls into the cluster ci  while  |ei|/|e| 1 is the expected probability under the assumption that the connections between vertices are random  i.e.  without regard to the community structure. finally  we note that q is also customizable to other implementations.
1.1 generating base clusterers
　we now describe how idisc generates base clusterers by instantiating simclust or linkclust. we consider in turn the representations described in section 1.
vector-based representations: for vector-based representations  idisc generates base clusterers by instantiating simclust. specifically  consider a database d with a set of tables t = {t1 t1 ... t|t |}  and denote the token vector for table ti as t i. first  for every two tables ti tj （ t   we evaluate their similarity based on their token vectors. the similarity between two vectors may be evaluated in a variety of methods  e.g.  via the cosine function commonly employed in information retrieval   where cos t i t j  =
 . note that multiple base clusterers may be generated from the same representation by employing different methods for evaluating the vector similarities.
invoicestatus invoiceterm	shipment	invoicestatus1	invoiceterm	shipment
1
product	productcategory category	1
	figure 1: a linkage graph	table 1: a meta-clusterer	figure 1: vote-based similarities　next  a similarity matrix m is constructed such that its entry m i j  holds the similarity between tables ti and tj. finally  a base clusterer is created by instantiating simclust with t   m  and particular implementations of clsrsim and q. for example  if clsrsim = single-link and q = the default q  formula 1   then the generated base clusterer can be denoted as simclust t   m  single-link  defaultq .
graph-based representations: for graph-based representations  idisc generates base clusterers by instantiating linkclust. since linkclust expects a undirected graph as the input  if the representation is a directed graph  e.g.  a reference graph described in section 1  it firsts needs to be transformed into a undirected graph. this is done by ignoring the directions of the edges in the original graph. for example  figure 1 shows the linkage graph transformed from the original reference graph for invdb in figure 1. a base clusterer is then created by instantiating linkclust with particular t   g  edgedel  and q. for example  linkclust t   g  sp  default  denotes a base clusterer where edgedel is implemented using the sp method  and the default implementation of q  formula 1  is used.
similarity-based representations: similar to vectorbased representations  for similarity-based representations  idisc also generates base clusterers by instantiating simclust. the difference is that here the similarity matrix in the representation is directly used for the instantiation.
1 aggregating results via meta-clusterer
　given a set of m preliminary clusterings c = {c1 c1 ... cm} from the base clusterers  the goal of the meta-clusterer is to find a clustering c  such that c agrees with the clusterings in c as much as possible. more precisely  we say that c and ci （ c disagree on the placement of two tables  if one places them in the same cluster while the other places them in different clusters. denote the number of disagreements among c and ci as d c ci . then  the job of the meta-clusterer is to find c that minimizes . a similar problem has been studied in the context of clustering aggregation and ensemble clustering  e.g.  see  . but these research efforts focused mostly on combining different clustering algorithms  e.g.  single-link vs. complete-link   and did not consider how to effectively combine different representation models  see section 1 for a detailed discussion .
　for example  columns 1 of table 1 show the preliminary clusterings given by three base clusterers: base 1  b1   base 1  b1   and base 1  b1   on invdb. the value j in
the column for base i indicates that base i places the corresponding table in the j-th cluster of its clustering. for example  the second cluster in the clustering given by base 1 contains tables shipment and shipmentmethod. both base 1 & 1 take a vector-based representation  with tokens from table names  as the input and employ simclust with the default q. base 1 uses complete-link for clsrsim  while base 1 uses single-link. base 1 takes a linkage-based representation  the reference graph in figure 1  as the input and employs linkclust with the default.
it is interesting to note that base 1 finds four clusters while base 1 finds three: the cluster on product in base 1 is split into two clusters {product  productcategory}  {category} in base 1.  we will further discuss this in section 1.  furthermore  invoiceitem is placed in the shipment cluster by base 1. this is due to the fact that the betweenness score  1  for the edge  invoiceitem  invoice  is larger than the score  1  for the edge  invoiceitem  shipment . overall  we can observe that the performance of base clusterers may vary depending on particular database representations and clustering algorithms employed.
　the last column of table 1 shows the clustering c obtained by the meta-clusterer meta  m . note that there are two disagreements between meta and base 1: on category and product  and category and productcategory. it can be shown that the total number of disagreements among meta and the three base clusterers is seven  the minimum among all possible c's.
　unfortunately  the problem of finding the best aggregated clustering can be shown to be np-complete . several approximation algorithms have been developed   and most of them are based on a majority-voting scheme. the metaclustering algorithm in idisc is also based on a voting scheme  but has a key difference. unlike other solutions  e.g.  the agglomerative algorithm in   it does not assume an explicit clustering threshold  e.g.  1 of the votes . instead  the algorithm automatically determines an appropriate number of clusters in the aggregated clustering  based on the particular votes from the input clusterers.
meta-clustering:	the algorithm involves two phases.
  a  vote-based similarity evaluation: consider two tables  and a clustering ci （ c. a vote from ci on the topical relationship between t and t is given by a 1 function v   which takes on the value one if t and t are placed in the same cluster in the clustering ci; and zero otherwise. based on the votes from the base clusterers  the similarity between two tablesis computed as:    where m is the number of base clusterers. for example  figure 1 shows the similarities between the tables in invdb  based on the votes from b1  b1  and b1.
  b  re-clustering: next  a similarity matrix mv is constructed from the above similarities. idisc then generates the metaclusterer as simclust t   mv  single-link  defaultq . but note that other options for clsrsim and q may also be used.
1. handling complex aggregations
　in this section  we extend idisc to handle complex aggregations. we first describe how to exploit the prior knowledge on the inherent property of the clusterers to organize them into an aggregation tree  to perform a multi-level aggregation. we then describe how to adjust the weights of certain clusterers in the run-time based on their actual performance  to achieve a more effective aggregation.

	bv1 bv1 bv1 bs1 bs1 bs1 bg1 bg1	bv1 bv1 bv1 bs1 bs1 bs1 bg1 bg1
	 a  single-level aggregation	 b  multi-level aggregation
figure 1: examples of aggregation trees
1 multi-level aggregation
　a key difference between multi-level and flat aggregations is that in a flat or single-level aggregation  the clusterings from all base clusterers are aggregated at once by a single meta-clusterer  while in a multi-level aggregation  aggregation is performed by multiple meta-clusterers  with some meta-clusterers taking as the input the aggregated clusterings from the previous meta-clusterers.
aggregation tree: in general  we may represent the aggregation structure with an aggregation tree h. the leaf nodes of h correspond to base clusterers  while the internal nodes correspond to meta-clusterers  each aggregating the clusterings from its child clusterers. the level of the aggregation is the depth of the deepest internal node in h. for example  figure 1.a shows a single-level aggregation tree with eight base clusterers  b's ; figure 1.b shows a two-level aggregation tree with four meta-clusterers  m's  on the same base clusterers.
　given a set of base clusterers  the key problem is then how to form an effective aggregation tree. a key observation is that the clusterers were not created equally and some are inherently more similar than others. for example  consider again the base clusterers in figure 1. suppose that biv's  s	g
bj's and bk's are respectively based on vector  similarity  and graph representations. then   is more similar to than to  since  looks at the database from a very different angle and may find very different clusters. in other words  and are experts with similar expertise  while the expertise of  and  may be quite different. intuitively  if we are to correct the errors by b1v  then it may be more effective to compare it against  which has similar expertise  than b1g  which has different expertise.
tree construction: motivated by the above observation  we define the similarity level of clusterers as follows.  a  level 1  the most similar : clusterers which take the same representation  e.g.  a vector-based representation   but employ different clustering algorithms  e.g.  single-link vs. complete-link versions of the similarity-based algorithm ;  b  level 1: clusterers which take the same kind of representations  e.g.  a vector-based representation constructed from table names vs. a vector-based representation constructed from both table & attribute names ; and  c  level 1: clusterers which take different kinds of representations  e.g.  a vector-based vs. a graph-based representation . furthermore  if one of the clusterers is a meta-clusterer  their similarity level is given by the least similarity level among all the base clusterers.
　based on the above definition  the aggregation tree is then constructed from a set of base clusterers in a bottom-up  clustering-like fashion. it involves the following steps:  1  initialize a set w of current clusterers with all the base clusterers.  1  determine the maximum similarity level l among all the clusterers in w.  1  find a set s of all clusterers with the similarity level l.  1  aggregate the clusterers in s using a meta-clusterer m and remove them from w. add m into
table pairsb1b1b1mv invoicestatus  invoiceterm 11 invoicestatus  invoice 11 invoicestatus  invoiceitem 11 invoiceterm  invoice 11 invoiceterm  invoiceitem 11 invoiceitem  invoice 11 invoiceitem  shipment 11 invoiceitem  shipmentmethod 11 shipment  shipmentmethod 11 product  productcategory 11 product  category 11 productcategory  category 11table 1: an example on clusterer boosting
w.  1  repeat steps 1 until there is only one clusterer left in w  which is the root meta-clusterer.
　for example  given the eight base clusterers shown in figure 1.a  the algorithm produces the aggregation tree shown in figure 1.b  where  and are first aggregated by vs g l
   which is further aggregated with m and m by m . 1 clusterer boosting
　unlike the multi-level aggregation which utilizes the static property of the clusterers  boosting exploits their dynamic behavior. it first estimates the performance of a clusterer by comparing it to other clusterers and then assigns more weights to the clusterers which are likely to be more accurate. the results from the clusterers are then re-aggregated based on the new weights. specifically  consider a metaclusterer m aggregating clusterings from a set of clusterers c = {c1  ...  cn}. boosting involves the following steps:
1. determining a pseudo-solution: the pseudo-solution s consists of a set of table pairs    which the majority of the input clusterers place them in the same cluster. following the notation in section 1  we have s =
. for example  consider the
meta-clusterer shown in table 1. table 1 lists the table pairs  out of 1 for invdb  which are placed in the same cluster by at least one base clusterer. for each pair  it shows which base clusterers  columns 1  and whether the majority of the base clusterers  column mv  place them in the same cluster  indicated by 1 . then s comprises the 1 table pairs which have value 1 in the column mv.
1. ranking input clusterers: s is then utilized to evaluate the input clusterers ci's. for this  idisc employs a measure Ψ which is taken to be the percentage of table pairs in s that are found by   = 1 . Ψ is similar to the recall metric in section 1. for example  Ψ b1  = .1  Ψ b1  = 1  and Ψ b1  = .1. ci's are then ranked by their Ψ scores. for example  we have b1   b1   b1.
1. adjusting weights: first  set the initial weights for all clusterers in c to 1. consider top k clusterers in c  for a desired number k. select the clusterer with the best score  increase its weight to 1. repeatedly find a clusterer ci with next best score. if ci is not highly correlated with any previously selected clusterers  set its weight to 1; otherwise  move to the next best clusterer  intuitively  this is because a clusterer with similar expertise has already been boosted .
　the correlation between two clusterers ci and cj is given by their correlation coefficient: ρxci xcj = cov xci xcj / σx σx   where  is a random variable cor-
	ci	cj
responding to the clusterer ci  and the sample space is taken to be the set of all table pairs     where . based on a well-known rule-of-thumb from statistics  two cluster-
	invoicestatus	invoiceterm	shipment

	invoice	invoiceitem	shipmentmethod
	 a  c1's linkage graph	 b  c1's linkage graph

	product	productcategory	category
 c  c1's linkage graph
　figure 1: an example on finding representatives ers may be regarded to be highly correlated if their |ρ| − .1. for example  the sample space for invdb contains 1 table pairs  1 of them are shown in table 1 . it can be shown that ρ b1  b1  = .1  ρ b1  b1  = .1  and ρ b1  b1  = .1. so b1 will first be boosted; since b1 is highly correlated to both b1 and b1  no other clusterers will be boosted.
1. finding cluster representatives
　in a complex database  there may be a large number of tables on the same topic. as a result  it is often desirable to discover important tables within each cluster. these tables are cluster representatives. they serve as the entry points to the cluster and give users a general idea of what the cluster is about. in addition  the names of these representative tables may be used to label the cluster as figure 1.b illustrates.
　in this section  we describe idisc's representative finder component. the key issue in discovering representative tables is a measure on the importance of tables. a key observation is that if a table is important  then it should be at a focal point in the linkage graph for the cluster. motivated by this observation  idisc measures the importance of a table based on its centrality score on the linkage graph. specifically  given a linkage graph g v e   the centrality of a vertex v （ v   denoted as ξ v   is computed as follows:
	 	 1 
where σst is the number of distinct shortest paths between vertices s and t  while σst v  is the number of distinct shortest paths between s and t that pass through the vertex v. based on this definition  we now describe the representative discovery algorithm repdisc in detail.
representative discovery: repdisc takes as the input a clustering c = {c1 c1 ... ck} over the tables in the database d  a linkage graph g of d  and a desired number r. it returns as the output up to r representative tables for each cluster in c.
　consider a cluster ci （ c  repdisc proceeds as follows to find representatives for ci.  1  obtain the linkage graph gci for the tables in the cluster ci. gci is a subgraph of g induced by a set of tables in ci. for example  consider the clustering c from the meta-clusterer meta shown in table 1. c contains three clusters {c1 c1 c1}  e.g.  c1 = {invoicestatus  invoice  invoiceitem  invoiceitem}. figure 1 shows the linkage graphs for these clusters  induced from the complete linkage graph in figure 1.  1  evaluate centrality scores for the tables in ci using formula 1.  1  rank the tables by the descending order of their centrality scores  and return top r tables in the ranked list. for example  suppose r = 1  the discovered representative tables for the clusters in figure 1 are highlighted with their names bolded.
complexity of repdisc: for each of the k clusters in c  three steps are executed. consider cluster ci （ c and denote the induced graph for ci as g vr er   where vr is a set of tables in ci and er is a set of linkage edges between the tables in ci. in step 1  for every two tables in vr  we need to determine if there is an edge between them. suppose g is implemented with an adjacency matrix. this can be done in o |vr|1 . further  the time to create the graph is o |vr| + |er| . thus  the overall complexity of step 1 is o |vr|1   since |er| is o |vr|1  . step 1 can be implemented based on brandes   where the complexity can be shown to be o |vr|   |er| . the complexity of step 1 is o |vr| . so the overall complexity for steps 1 is o |vr|   |er|   with the dominant factor being the time for step 1. assume that each cluster contains about the same number of tables with roughly the same amount of linkage between the tables  the complexity of repdisc is o k   |vr|   |er|  = o k   |v |/k   |e|/k  = o |v |   |e|/k . in other words  it is about 1/k of the time for computing centrality scores for the entire graph g.
1. empirical evaluation
　we have evaluated idisc on several real-world databases. our goals were to evaluate the effectiveness of idisc and the contribution of different system components.
1 experiment setup
data set: we report the evaluation of idisc on three large human-resource  hr  databases: hr1  hr1 and hr1  whose characteristics are shown in table 1. these databases were obtained from the service department of a large enterprise and cover varied aspects of resource management: hr1 on engagement management  hr1 on skill development  and hr1 on invoice tracking. they are among a large number of hr databases in the service department which has an ongoing effort of understanding & integrating these databases. for each database  we asked a data architect who has been using the database to manually examine the database  and determine  1  a set of topics in the database  and  1  which topic each table in the database is about. these were then used as the  gold standard  for our experiments.
performance metrics: we measured the performance of idisc using three metrics: precision  p   recall  r   and fmeasure  f1  . precision is the percentage of table pairs determined by idisc to be on the same topic that are indeed on the same topic according to the gold standard. recall is the percentage of table pairs determined by the domain expert to be on the same topic that are discovered by idisc. f-measure incorporates both precision and recall. we use the f-measure where precision p and recall r are equally weighted  i.e.  f1 = 1pr/ r + p .
experiments: for each database we conducted three sets of experiments. first  we measured the utility of various database representations  section 1  and the accuracy of individual base clusterers  section 1 . second  we measured the aggregation accuracy of the baseline metaclustering algorithm  section 1 . third  we measured the impact of the proposed complex aggregation techniques  section 1 . for all the base clusterers and meta-clusterers  the default q and q  sections 1.1 & 1.1  were employed. vector-based representations were constructed from table & attribute names and the cosine function was employed for

table 1: the data set for our experiments
computing vector similarities. since the databases contain a huge number of rows  we first created a sample of 1k size for each attribute  and then use them for discovering foreign keys  section 1.1  and attribute matches  section 1.1 .
1 results & observations
database representations & base clusterers: figures 1.a  1.b & 1.c show the performance of base clusterers on hr1  hr1 & hr1  respectively. for each database  eight base clusterers were employed. these base clusterers are indicated by the types of database representations and clustering algorithms they use. for example  vec-sl represents a base clusterer which uses vector representation and implements single-link as its clsrsim function  section 1.1 ; graph-spc represents a base clusterer which uses graph representation and implements the edgedel function using the spectral method  section 1.1 . for each base clusterer  three bars are shown  representing its performance in precision  recall  and f1  from left to right . from these results  we can make several observations.
　first  base clusterers employing a complete-link algorithm  cl  tend to have higher precision and lower recall than ones based on a single-link algorithm  sl . this is not surprising  given that cl-based clusterers typically produce a large number of small clusters  while sl-based ones produce a small number of large clusters. second  the precision of base clusterers using graph-based representations is relatively low in hr1 & hr1. detailed analysis reveals that keys and foreign keys in many tables of these two databases are named using patterns such as  xxxid and xxx cd . this confused the foreign-key discovery algorithm and as a result  many false foreign keys were discovered. a possible solution to this is to first determine which tables are similar  e.g.  by using vector-based representations  then only discover foreign-keys among the similar tables. third  the base clusterers utilizing vector-based representations perform consistently well over all three databases. this is due to the fact that similar tables in these database tend to have many common words  e.g.  emp  empresume  and emp photo.
fourth  the base clusterers utilizing similarity-based representations had poor performance on hr1. the main reason is that a large number of tables in hr1 have several similar timestamp-like columns  e.g.  create dt and del dt for table creation and deletion datetimes. as a result  many tables are falsely determined to be similar to each other  reducing the effectiveness of similarity-based representations. in summary  the performance of different base clusterers may vary a lot  depending on the characteristics of particular databases  database representations  and clustering algorithms.
meta-clusterers: for each database  four meta-clusterers were constructed. meta-vec aggregates base clusterers vecsl  vec-cl  and vec-al; meta-sim aggregates sim-sl  simcl  and sim-al; and meta-g aggregates graph-sp and graphspc. note that all these three meta-clusterers aggregate base clusterers which use the same kind of database representations. the last meta-clusterer meta-all aggregates

	 a  all topics	 b  topics with at least two tables
　figure 1: # of topics: gold standard vs. idisc all eight base clusterers. the results are shown in the first four groups of bars in figures 1.a-1.c. we observe that with the aggregation  the effects of  bad  base clusterers can be cancelled out. for example  in hr1  the precision of meta-vec  1%  is much higher than that of vec-sl  1% ; furthermore  meta-all is far more accurate than vec-sl  graph-sp  and graph-spc  and its f1 is higher than that of all base clusterers. all these strongly indicate the effectiveness of meta-clustering.
multi-level aggregation: we then formed a two-level aggregation tree with a top-level meta-clusterer  meta-lvl  combining three lower-level meta-clusterers: meta-vec  meta-
sim  and meta-g. the structure of the tree is as shown in figure 1. the performance of meta-lvl is shown in the second to last group of bars in figures 1.a-1.c. by contrasting meta-lvl with meta-all  i.e.  one-level aggregation   we can observe that f1 values significantly improve over all three databases  ranging from 1 percentage points in hr1 to as high as 1 percentage points in hr1. furthermore  the recall increases consistently  with very significant improvement in both hr1  by 1 percentage points  and hr1  by 1 percentage points . all these indicate the effectiveness of multi-level aggregation  where clusterers using the same kind of representations are first aggregated to remove errors and increase the precision  and then a second level meta-clusterer is employed to combine clusterers with different kinds of representations  and thus with quite different  expertise   to increase the recall.
clusterer boosting: next  we applied the boosting technique  section 1  on meta-lvl  where k was set to + 1  i.e.  up to two input clusterers will be boosted . the boosted version of meta-lvl is called meta-wgt  whose performance is shown in the last group of bars of figures 1.a-1.c. by contrasting meta-wgt with meta-lvl  we can observe increase in f1 consistently over all three databases  with the largest increase  1  in hr1. these indicate the effectiveness of the proposed boosting technique.
　meta-wgt is also the best performer among all the clusterers on the data set. its precision ranges from 1 to as high as 1  recall from 1 to as high as 1  and f1 from 1 to as high as 1. these indicate idisc's effectiveness.
number of topics: finally  we compared the number of clusters  i.e.  topics  discovered by meta-wgt with that in the gold standard. figure 1.a plots the total numbers of topics versus the databases. figure 1.b plots the total numbers of topics with at least two tables versus the databases. we observe that the numbers of topics discovered by idisc are very close to the numbers given by the gold standard. these further indicate the effectiveness of idisc.
1 discussion

	 a  on hr1	 b  on hr1	 c  on hr1
figure 1: the utility of different kinds of database representations & the accuracy of various base clusterers

	 a  on hr1	 b  on hr1	 c  on hr1
figure 1: the performance of the baseline meta-clustering algorithm & the complex aggregation techniques　there are several reasons that prevent idisc from achieving perfect accuracy. first  idisc may disagree with the domain expert on the granularity of partitioning and the number of subject areas in a database. for example  tables on the employee information and tables on the department information may be placed in the same cluster by the domain expert  e.g.  a department cluster based on the view that employees may be organized around the departments they work for  while idisc may produce more refined clusters  e.g.  one focusing on the employee information and the other on the department information.
　second  idisc and the domain expert may also disagree on the assignment of the tables to the clusters  particularly for those  boundary  tables that connect several related entities. for example  a works-for table  which employees work for which departments  may be placed in the employee cluster by one and in the department cluster by the other. a possible solution to the above two cases is to construct the gold standard by employing multiple domain experts and accept a discovered table pair as a correct answer as long as it agrees with one of the domain experts. but there may be a need to first reconcile any conflicts among the domain experts. another possible solution is to first determine the table pairs which the majority of the domain experts agree on and then compare idisc's results only to these pairs.
　third  some databases in our experiments contain reference tables such as country  with attributes like name  region and iso code  and language  with attributes like name and code . these tables are often referred to from multiple subject areas. the domain expert may decide to form a subject area  e.g.  reference entities  to include all such reference tables or may place them in some referring subject areas. since idisc may make very different decisions  this situation largely affects its performance. a possible solution is to extend idisc so that it produces soft clusters  where each table may be assigned to multiple clusters  i.e.  with  soft  membership  and regard these assignments as correct answers if one of them  or one of the top k most confident assignments  agrees with that given by the domain expert.
1. related work
we discuss related work from several perspectives.
mining database structures: there has been a lot of research on mining database structure  1  1  1  1  1 . bellman  is a well-known system that discovers join relationships among the tables in a database. there exists a join relationship between two tables if they have joinable attributes  i.e.  attributes which are semantically similar. similar attributes are identified using several set resemblance functions  similar to the jaccard function we utilized to compute attribute similarities  section 1.1 . the structure of the database is then a set of tables connected via join relationships or join paths. as we have seen  two tables connected via a join relationship  in particular  a referential relationship  may not be on the same topic. so the goal of our work is to identify such inter-topic links and partition the tables accordingly. in fact  the problem of finding clusters of inter-related tables was also identified in bellman as an important direction for further research .
　there is also previous work on abstracting and classifying conceptual schemas such as er models  1  1  1 . as discussed earlier  conceptual schemas are rarely available as part of the database design documentation. furthermore  this research largely relied on manually specified semantic links among the elements  such as is-a and aggregation relationships. in contrast  our solution does not require this information.
　data modeling products  such as erwin  and rda   allow users to organize entities in a large logical model by subject areas during a top-down modeling process  to cope with the complexity and facilitate a modular development. our solution complements these functions by enabling users to reverse-engineer subject areas from a large-scale physical database during a bottom-up modeling process.
information integration & complexity issues: information integration is a key problem in enterprise data management  and has been extensively studied . recently  the complexity issue in data integration has received active attention  1  1   largely due to the increasing complexity of data sources.  proposes a fragment-oriented approach to matching large schemas in order to reduce the matching complexity. it first decomposes large schemas into several sub-schemas or fragments and then performs fragment-wise matching. a fragment considered in  is either an xml schema segment  a relational table  or manually specified. our work is complementary to this work by providing an automatic approach to partitioning a large schema into semantically meaningful fragments.  proposes an incremental approach to matching large schemas.
the complexity issue has been also studied in view integration   where user views are mapped to a common semantic data model to reduce the complexity of integration.
multi-strategy learning & clustering aggregation: the multi-strategy learning paradigm  has been employed in schema matching  and information extraction  tasks with great success. but we are not aware of any previous attempts to apply this paradigm for mining database structures.  describes lsd  a system that matches source schemas against a mediated schema for data integration. lsd employs a set of base learners such as name matcher  naive bayes learner  and country-name recognizer. the predictions from the base learners are then combined via a meta-learner. lsd needs training data to train both its base learners and meta-learner. in contrast  the base clusterers and meta-clusterers in idisc do not require training. in other words  the learning in idisc is unsupervised.
　delta  computes the textual similarity between attributes based on their definitions in data dictionary  when available  to discover attribute correspondence. semint  automatically learns a neural-network classifier based on schema information & data statistics and employs it to match attributes.  notes the complementary nature of these two tools and suggests to combine them to improve the matching accuracy.  employs a multi-strategy learning approach to extracting information from text documents  where the extraction segments are represented in various models such as term-space and relational models.
　a formal treatment to clustering aggregation can be found in . there is a key difference between multi-strategy learning  and clustering aggregation . a multi-strategy learning approach  typically starts with raw evidence and addresses problems such as how to construct multiple effective representations from the evidence and how to design suitable base clusterers for each representation. in contrast  clustering aggregation  does not concern with these problems. it starts directly from the results from base clusterers and seeks an effective way of combining them.
1. conclusions & future work
　we introduced the problem of discovering topical structures of databases and described an automatic discovery system idisc. idisc is unique in that  1  it examines the database from varied perspectives to construct multiple representations;  1  it employs a multi-strategy framework to effectively combine evidence through meta-clustering;  1  it employs novel multi-level aggregation and clusterer boosting techniques to handle complex aggregations; and  1  it employs novel measure on table importance to effectively discover cluster representatives. experiments over several large real-world databases indicate that idisc is highly effective  with an accuracy rate  f1  of up to 1%.
　besides further evaluation on additional data set  we are investigating two directions to extend idisc. first  we plan to develop soft clustering & meta-clustering techniques as discussed in section 1 and incorporate them into idisc to examine their impact on its performance. we intend to draw upon and extend recent research in faceted browsing and search. second  we plan to extend idisc to produce hierarchical topical structure  where each topic may be further divided into sub-topics. this would not only enable directory-style semantic browsing but also further support the divide-and-conquer approach to schema matching and reduce the complexity of large-scale integration.
