the stream processing core  spc  is distributed stream processing middleware designed to support applications that extract information from a large number of digital data streams. in this paper  we describe the spc programming model which  to the best of our knowledge  is the first to support stream-mining applications using a subscriptionlike model for specifying stream connections as well as to provide support for non-relational operators. this enables stream-mining applications to tap into  analyze and track an ever-changing array of data streams which may contain information relevant to the streaming-queries placed on it. we describe the design  implementation  and experimental evaluation of the spc distributed middleware  which deploys applications on to the running system in an incremental fashion  making stream connections as required. using micro-benchmarks and a representative large-scale synthetic stream-mining application  we evaluate the performance of the control and data paths of the spc middleware.
1. introduction
　the widespread deployment of digital systems has led to a large increase in the number of sources of streaming digital data such as text and transactional data  digital audio  video and image data  instant messages  network packet traces  and variety of sensor data. the stream processing core is a distributed stream-mining middleware  built to support applications composed from user-developed processing elements that ingest  filter  and most importantly  mine data streams. spc supports the stream-mining paradigm  where users submit continuous inquiries expressed as processing flow graphs that are evaluated over continuous streams of data. similarly  results of the evaluation are  streamed  continuously to the user as they become available.
　our contributions in this paper are two-fold. first  we discuss a simple  yet very flexible programming model supported by spc for developing stream-mining applications. second  we describe the distributed architecture of spc
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
dm-ssp'1 august 1  1  philadelphia  pa usa copyright 1 acm 1-1-x ...$1.
middleware and its relationship to the programming model and how it addresses the scalability needs of large-scale stream-mining applications.
　many stream processing systems in the literature  1  1  1  are designed for applications employing only relational operators  and their adaptations to continuous streams  without any support for user-defined operators  which is required in many application domains. the spc programming model supports both relational operators and user-defined operators alike. most other stream-processing systems also precompile the processing flow-graph before deploying it into the runtime. spc  on the other hand  allows for flow-graphs to specify the streams to process using a subscription-like model. this enables stream-mining across raw data streams and data streams generated by other applications concurrently using the middleware. this dynamic nature fits the stream-mining model where information sources are everchanging and long-running stream-mining inquiries are looking for  relevant  information in the continuously changing set of data streams. finally the spc architecture capitalizes on several programming model features to achieve scalability and performance efficiency. for example  spc uses a two-tiered data routing approach which may accord substantial savings over a traditional pub-sub model for data dissemination.
　the key design points addressed by the spc architecture are:  i  data processed by applications is streamed in nature   ii  applications are not restricted to using relational operators   iii  applications are inherently distributed due to their resource requirements   iv  support for discovery of new data streams  as they are created in the system  is essential. that is  applications may dynamically tap into relevant streams possibly created by other applications   vi  since applications share streams  processing elements belonging to different applications need to share a common vocabulary to annotate discovered data in streams  and finally   v  volume of incoming data is very large and hence the system must be highly scalable.
　the rest of this paper is organized as follows: section 1 describes and contrasts spc with other stream processing systems. section 1 describes the spc programming model  which is the cornerstone of the middleware and the gateway for application developers into the runtime environment. section 1 provides an overview of the spc architecture. section 1 provides a performance study of a few critical system metrics by employing micro-benchmarks and a large-scale synthetic application where the middleware is deployed and stress-tested on 1 multiprocessor nodes in a cluster environment. finally  section 1 concludes the paper and discusses ongoing work towards evolving spc's architecture.
1. related work
　recently a large number of projects aimed at supporting streamed data processing have been started. not surprisingly  many of these initiatives are more slanted towards extending current database technology  although stream data processing has been an active area of research in the programming languages as well as in the high performance computing communities.
　current projects that aim to process unstructured information  such as gate  general architecture for text engineering   and uima  unstructured information management architecture    focus on mining text documents for information. these systems are designed with a focus on flexibility in assembling text mining applications using generic algorithms and not necessarily with a focus on largescale distributed multi-operator mining over multi-modal data.
　on the other hand  the infosphere project  1  1  1   aims at defining system support for writing informationdriven applications  focusing explicitly on defining the desired qos-level between distributed components that interact through streams. unlike in spc  application composition is done at compile-time.
　on the high performance front  datacutter  is a middleware for decomposing applications into processing filters responsible for implementing subsetting and reduction operations over streams of buffers. the lack of support for dynamic topologies and the lack of support for multiple coexisting applications are the main differences between datacutter and spc.
　in the database community  projects such as aurora   telegraphcq  1  1   borealis  1  1   and stream  1  1  have been making progress in providing support for streamed data manipulation from a database-centric perspective. that is  data is manipulated as in relational databases  by specifying queries  sometimes with time windows  over regular relational data as well as streamed data.
　while spc shares a few directions with some of the projects we discussed  it distinguishes itself from these systems in three main ways:  i  spc provides a programming model to write processing elements whose semantics are beyond relational operators;  ii  spc supports dynamic application composition and stream discovery  where multiple applications can symbiotically interact;  iii  spc adopts a design that enables scaling to a very large number of computational nodes supporting multiple coexisting applications.
1. programming model
　the stream processing core includes a programming model and development environment that enables the implementation of distributed  dynamic  and scalable applications. spc's programming model includes apis for declaring and creating processing elements as well as tooling to assemble  test  visualize  debug  and deploy applications. unlike other stream-processing middleware  the spc programming model supports non-relational operators and relational operators alike. the programming model has been shown to be flexible and comprehensive. applications from various domains have already been implemented  including the linear road benchmark  and a semantic video filtering application  1  1 .
　spc has been built to answer inquiries  which are highlevel user requests for information. an inquiry is translated into one or more processing flow graphs  which we refer to as a job or an application. jobs comprise one or more processing element. the processing element  pe  is the fundamental building block for creating applications that use the services provided by the spc and implements an applicationdefined operator. pes communicate through ports and are able to read  process  and write streams of data. a pe receives data from a collection of streams through input ports  process and writes resulting data to output ports  thus creating new streams. each port is associated with a declared format  which is a set of attribute types provided for each pe port. the format for an input port defines the structure of an sdo the pe can ingest through that port. an input port subscribes to data it intends to consume using a flow specification expression. it includes properties of streams that the pe intends to receive as well as a predicate to filter sdos from the streams. the format for an output port defines the structure of sdos in the stream produced by the output port. each output port produces a stream  which is a contiguous group of stream data objects  sdos  conforming to the format for the output port. that is  sdos are structured messages consisting of annotations and an associated payload.
　the rest of the discussion in this section centers on the process of creating processing elements and assembling applications.
　since one of the key features of spc is the sharing of streams across applications  all sdo annotations are typed using types declared in a global type system. this enables applications to be composed from pes written by different developers and different applications to share and cooperate through streams.
1 global type system
　the global type system is a library of agreed-upon types. types are like c-programming language typedefs where each type definition has a type name and may contain one or more typed features. type definitions may also be hierarchical in that they inherit all the features of their parent type. the global type system in spc defines a few basic types such as string  int and float and many other complex types are derived or composed from these basic types. for example  the description for a type named com.ibm.systems.sourceipaddress is shown in figure 1. this type has a description and the declaration of a feature called value  which is of type string. this type is used by the stream definition associated with the demultiplexedemailtraffic port depicted in figure 1.
　in spc  the type system definitions are created by and maintained in a central repository. c++ and java classes corresponding to each type definition  with getters and setters for each feature within the type are automatically generated to facilitate programmatic access to instances of types carried by the sdos.
1 processing element description
　since a pe is a basic building block of an application  it is essential that it is described adequately to enable either a human or an automated tool to compose applications. the
 typedescription 
 name com.ibm.systems.sourceipaddress /name 
 description a source ip address /description 
 features 
 featuredescription 
 name value /name 
 description ip address as string /description 
 rangetypename string /rangetypename 
 /featuredescription 
 /features 
 /typedescription figure 1: excerpt of the definitions in the global type system. the definition for type com.ibm.systems.sourceipaddress
spc programming model requires a pe developer to create a static descriptor tied to the pe implementation. this describes the pe's ports as well as its deployment requirements such as its library dependencies  resource requirements  executable location  and command line arguments.
　the description of the ports is key to the dynamic application composition performed by spc. both input and output ports have a declared format  which consists of a set of type names constraining the properties of sdos that can be manipulated through those ports. an input port format specifies the types of the annotations that the pe implementation will look for in every incoming sdo on that port. an output port format declares to the system the types of annotations that will be present in all the outgoing sdos produced by that port.
　an excerpt of a descriptor is depicted in figure 1 for a pe that demultiplexes email traffic into different protocols  e.g.  smtp  pop  imap . in particular  the descriptor contains the properties for the streams this pe should consume through the emailtraffic input port and the information about the streams it produces through the output port demultiplexedemailtraffic. it also includes runtime configuration such as command line arguments  args  and executable  exec .
1.1 output port stream description
　the stream is another basic concept in the programming model. a stream consists of sdos that are produced by a single output port. it is defined by the format of the output port and has a name  assigned by the application composer. streams can be declared to be private or public. only public streams are available to all applications in the system and can be tapped into by specifying the stream name in the flow specification. this matching process is done by the spc middleware during runtime  additional information about this matching process is found in section 1  .
1.1 input port flow specification
　as described earlier  in order to enable sharing of streams among applications and allow for dynamically tapping into a changing set of streams and mining them  spc allows pes to use a subscription-mechanism for the input ports. the pes specify the streams they are interested in consuming using a flow specification which is associated with each input port. it is important to note that the pe-developer specifies the format which defines the types to be found in the sdo  whereas the flow specification is filled in during the application composition process to specify the streams and sdos that must be routed to the pe. the flow specification
 pe   executable   main  exec  /main 
 arguments  args  /arguments   /executable 
 input 
 port name= port1  
 flowspecification  stream emailtraffic   /flowspecification 
 format 
 attribute com.ibm.systems.sourceipaddress /attribute 
 attribute com.ibm.systems.destinationipaddress /attribute 
 attribute com.ibm.systems.sourceportnumber /attribute 
 attribute com.ibm.systems.destinationportnumber /attribute 
 attribute com.ibm.systems.datasource /attribute 
 attribute com.ibm.systems.iptransporttype /attribute   payload text /payload 
 /format 
 /port 
 /input 
 output 
 port name= port1  
 streamname= demultiplexedemailtraffic / 
 format 
 attribute com.ibm.systems.sourceipaddress /attribute 
 attribute com.ibm.systems.destinationipaddress /attribute 
 attribute com.ibm.systems.sourceportnumber /attribute 
 attribute com.ibm.systems.destinationportnumber /attribute 
 attribute com.ibm.systems.datasource /attribute 
 attribute com.ibm.systems.iptransporttype /attribute 
 attribute com.ibm.systems.emailprotocol /attribute   payload text /payload 
 /format 
 /port 
 /output 
 /pe figure 1: excerpt of a pe descriptor with an input port port1 and an output port port1 producing a stream named demultiplexedemailtraffic
# stream specification
stream iptraffic emailtraffic;
# sdo filters com.ibm.systems.iptransporttype:value*='chat' or com.ibm.systems.iptransporttype:value*='email' or com.ibm.systems.iptransporttype:value*='avt' or com.ibm.systems.iptransporttype:value*='voice'figure 1: a sample flow specification expression
expression has two optional components: a stream filter and a predicate to filter sdos within the matching streams. in the descriptor shown in figure 1  input port port1 has a flow specification subscribing to a stream named emailtraffic . any stream that has that name will be multiplexed into that port by the spc at runtime. hence  by defining a flow specification expression  a single input port can have several streams be routed to it. in addition to the streams to connect to  the flow specification can indicate that sdos must be filtered according to a particular predicate  using an extended java message service  jms  syntax . figure 1 demonstrates how an input port can subscribe to two specific streams and filter out sdos according to a disjunctive predicate.
　the example in figure 1 shows the precise description of the streams that should be routed to an input port  e.g.  iptraffic . the absence of named streams in the flow specification  or the specification of stream properties alone  implies a loose subscription. in other words  if a loose specification is provided  consumers and producers are matched solely based on port formats on an ongoing basis  i.e.  every time a new stream becomes available .
1 pe api
　pes are built as dynamically loadable modules that run in the context of a container. multiple pes can be managed by a single pe container  which effectively hooks it up to the middleware. pes are currently implemented in c++1by inheriting from a simple pe abstract class  where a init  process  and shutdown methods must be customized and event handlers may optionally be installed.
　the init method may be used by the pe-writer to initialize its processing. similarly  the shutdown method is automatically called by the pe container when the pe is requested to terminate and may be used for providing cleanup. the process method is where the actual data processing  i.e.  reading sdos from streams  their manipulation  the execution of data analysis operations  and the generation of output sdos takes place. the event handler method  if implemented  is invoked in order to asynchronously communicate external runtime events to the pes. one typical example is to inform a pe when no pe is subscribed to its stream. this allows the pe to scale down its resource utilization and go into a  quiet  mode.
　a pe will typically perform some  or all  of the following operations: read from streams through one or more input ports  process data  and create one or more streams. an input port is the object used for reading sdos from the stream  or streams  matching the port's flow specification. input ports can be opened and closed  using open and close methods   sdos can be synchronously read  using the readnext method   and input ports can be selected  employing the select method . the select operation is similar to the unix select system call  that is  once sdos are ready to be consumed through a port  the port is flagged as such. the programming interface for output ports is similar  with the exception that only sdo writing operations can be performed  using the writenext method .
　as previously seen  the sdos that are created or ingested consist of two parts  a set of structured attributes and an opaque payload. the structured attributes are manipulated through a set of convenience classes that implement getter and setter methods for each type used by the pe as described in section 1. the opaque payload is manipulated as a blob of a particular size.
1 application composition
　pes are the building blocks of applications. in some cases  applications have well-defined boundaries where a collection of pes will act in concert in order to analyze data and produce results for a user. in other cases  applications will leverage and co-exist symbiotically with other pre-existing  or yet to exist  applications by consuming streams generated by them or by producing streams useful to others. in spc  an application consists of a collection of pes and some additional configuration metadata. the application description may specify the stream connections that will exist among the pes  by configuring the actual flow specifications for each input port for every pe in that application. note that the actual stream connections are not completely known a priori because input ports flow specifications do not necessarily hard-wire consumer and producer pes. in other words  the flow specification for the input port establishes a predicate that will  at runtime  result in the streams and sdos that an input port can consume.
　for example  when an application that consists of a source pe that advertises a stream s1 and a sink pe that has a flow specification of s1 is submitted to spc  it will be placed in the runtime environment and a connection between them will eventually be made. likewise  other pes belonging to other applications may eventually tap into s1  when the middleware identifies that a newly created pe has a flow specification that matches s1.
1. architecture
　in this section  we describe the architecture of the spc system. many basic requirements drove the design of the spc architecture. as stated earlier  spc supports largescale distributed streaming applications and is hence designed to run on large-scale clusters. applications need to tap into streams based on stream characteristics  using a subscription-like model. this flexibility in the programming model requires the middleware to establish connections on an ongoing basis  providing dynamic information discovery. the harmonious coexistence of applications sharing common computational resources requires monitoring and enforcement of resource consumption limits and policies. spc's components that host the pes enforce the resource limits determined by a global resource manager. finally  applications running on behalf of users having different isolation requirements needs to be supported. for this  the spc architecture employs pe-execution containers that host  manage  monitor and control a pe's life cycle.
　figure 1 a  depicts the spc architecture where three nodes  nodes 1  1  and 1  are allocated exclusively for some of the middleware daemons and two nodes are allocated for the pes  node 1 and 1  as well as their managing components. some of the spc components such as the dataflow graph manager are standalone daemons. others such as the the pe container and the data fabric server are distributed across all the spc nodes. these components support a logical topology of pes and their stream interconnections  an example of which is shown in figure 1 b .
　each of the system components was developed as a multithreaded corba service in c++ and the current intercomponent communication infrastructure relies on remote procedure calls and asynchronous notifications. the spc has been under development at ibm research for over a year and our initial prototype is functional on an 1-node cluster. the current spc code base consists of around 1 kloc. the various spc components are described in the following subsections.
1 the dataflow graph manager
　the dataflow graph manager  dgm  manages the global dataflow graph in terms of pes and stream connections among them. in order to provide this service  the dgm keeps a directory of existing pes  streams  and the current graph topology. some of the key functions of the dgm are described below.
1.1 dynamic stream connections
　as described in section 1  spc allows application composers to declare their input port flow specifications using a subscription model which is evaluated at run time to determine stream connections. this is one of the main tasks of the dgm. it achieves this by matching stream descriptions of output ports to flow specifications of input ports

 a  spc distributed components. nodes 1 and 1 host compo-  b  dataflow graph - a sample logical topology nents that a typical processing node requires. nodes 1  1  and 1 each host an infrastructure control component
figure 1: the spc physical and logical architecture. the physical interconnect can be provided by a localarea network and a storage area network
whenever new pes start running or leave the system.
　operationally  when a new pe is instantiated  the dgm determines all the stream connections to its input and output ports as follows -  1  subscription lookup: the dgm inspects the flow specifications of input ports of every pe already in the system for matches against streams produced by the new pe. the matching process also considers port format compatibility  i.e.  an input port's format has to be a subset of any candidate output port format  to match.  1  stream lookup: conversely  the dgm inspects the streams produced by every pe already in the system for matches against the flow specifications of the input ports of the new pe. once matches are detected by either process above  the dgm asynchronously notifies the data fabric of the corresponding stream connections.
1.1 two-tiered routing
　as also seen in section 1  the flow specification expression has two components  stream and sdo-filter  which enables the system to employ a two-level routing structure where -  1  candidate streams are matched against the stream portion of an input port's flow specification expression  allowing logical circuits to be established between consumers and producers;  1  sdos from these matched streams are then filtered  at the communication substrate level  i.e.  the data fabric   based on the sdo-filter  before delivery to the pe. in other words  dgm establishes logical circuits between pes generating streams and pes interested in consuming them. the physical connections are made by the data fabric upon notifications from the dgm.
　unlike traditional pub-sub scheme where single-tiered routing is usually employed  our design choice was made based on two assumptions. first  we believe that there is a more balanced ratio of publishers and subscribers in streaming systems  which makes it possible to pre-establish connections among them and then performing the per-sdo matching of sdo-filter portion of the subscribers' flow specifications within the stream. second  stream data objects belonging to one stream are assumed to have many attributes in common as opposed to pub-sub systems  where message formats are assumed to be arbitrarily variable. indeed  the stream format is an invariant and is used to establish circuits between producers and consumers.
1.1 pe reuse
　as we previously stated  another key design point is the support for the dynamic composition of applications. because applications are built employing pe building blocks  it is possible to leverage code and expertise previously
　application writers have access to pe libraries  where fundamental operations such as relational operators  signal processing filters  translators  and others are available for mixing and matching by new applications. reuse can be exercised when applications are designed and written or composed and is put to use when the application is deployed. at application dispatch time  dgm identifies commonalities between applications already in the system and the new application in terms of the pes they employ. that is  the reuse mechanism is triggered every time a new application is overlaid on the global dataflow graph and its pes are instantiated. the dgm creates a pe signature based on the pe configuration. using the pe's signature the dgm attempts to find other candidate pes in the system for reuse. candidate pes are then directly compared with the new pe descriptor. if equivalence can be established  the job submission client is instructed to not deploy the new pe. the dgm
instead simply increments the reference count for the reused pe and associates it with yet another job.
1 the data fabric
the data fabric is the data transport substrate in the
spc and is implemented by a collection of distributed servers  dfs   one per node in the cluster. it is responsible for routing  filtering  and flow-smoothing. it provides pes with access to sdos via ports and transports sdos within streams.
　figure 1 depicts the data fabric internal architecture with multiple pes placed on two nodes.
1.1 data routing
　the dfs is responsible for determining the best routing strategy based on the topology of the network as well as the subscriptions to a stream. as seen in section 1  pes can have input ports through which they read sdos and output ports to which they write sdos. each input port is associated with a flow specification  which has a stream-filter part and an sdo-filter part. the data fabric is responsible for setting up the circuits based on stream connection notifications from the dgm. once connections are in place  it multiplexes data from various streams that match the streamfilter part into one input port. it then delivers sdos that match the sdo-filter part of the flow specification to pe's input ports. the df uses a subscription matcher module for this task.
　the filtering of sdos based on the subscriptions of downstream consumers may be done either at producer or at each of the consumers of the data. in the current system implementation  the matching is done at the producer-end. however  the location of the matching process is flexible to help the dfs to adapt to available bandwidth and computation resources. for example  if all the subscriptions are evaluated at the source  the network bandwidth consumption potentially decreases  but the cpu cost for matching could be quite high.
1.1 sdo transport
　the df chooses dynamically the most efficient transport for delivering sdos from the same stream to different pes  depending on factors such as topology  reliability levels  stream popularity  and number of consumers. figure 1 shows examples of three transport layers available in the current prototype -  i  pointer transport: sdos are transferred from the producing pe to the consumer pe by simply transmitting a memory pointer. this is suited for communication among pes within the same container  where pes share a common address space;  ii  shared memory transport: sdos are transported using shared memory segments when pes run in different execution containers co-located in the same node;  iii  network transport: sdos are transported from the producing pe on one node to the consuming pe on another node. this transport layer can be implemented on top of tcp  udp  and ip-multicast. the former is available in the current prototype while the latter two are being implemented. when a connection between a producer and consumer needs to be reliable  we rely on the storage subsystem if necessary.
1.1 traffic isolation
　as will be seen in section 1  pes run in execution containers  which provide process isolation. since pes run userwritten code  their interaction with the data fabric needs to be mediated. hence  the data fabric is split into two parts - the dflocalrouter and the dfremoterouter. the dflocalrouter is responsible for routing traffic among pes that reside within the same pe execution container. the dfremoterouter is a standalone process responsible for routing traffic among pes in different pe containers  for keeping the routing tables  and communicating with other system daemons such as the dataflow graph manager.
1.1 flow balancing
　fluctuations and mismatches in sdo producing/consuming rates may result in data loss that can be prevented by performing local optimizations in buffer and cpu allocations. this is accomplished by the nano-scheduler component of the data fabric. the reader is referred to  for a complete description.
1 pe execution container
   every pe runs within the confines of a container called a pec. the pec provides isolation of pes from each other and also monitors and manages the pe life cycle. the current level of containment is provided by a process address space. however  if necessary  a pe can be further isolated by running it  and its pec  within a different virtual machine. isolation may be required due to an explicit request since a pe may be running in debug mode for instance. if less containment is needed  for example  for pes that are part of the same job  they can share the same container and be managed by the same pec. once the pe starts  the pec monitors the pe cpu-usage and memory and reports them to the global resource manager  a complete description of resource management issues is outside the scope of this paper . in the current prototype  the pec loads the pe dynamically from a library and runs each pe in its own thread.
1. performance evaluation
　in this section  we demonstrate the spc architecture by employing a large-scale synthetic application using the spc prototype as well as a set of micro-benchmarks. our studies were conducted on a cluster of 1 dual-processor hyperthreaded intel xeon 1 ghz nodes with 1 gb of ram interconnected with switched gigabit ethernet as well as with a storage area network  san   which is available to a subset of the nodes. the cluster runs suse linux enterprise server version 1. the aim of this study is to highlight some of the critical performance aspects as they relate to the overall middleware architecture. for an application-oriented experimental study based on the linear road benchmark   we refer the interested reader to .
1 experimental methodology
　the experimental results are presented in two parts. in the first part  we were primarily concerned with extracting a performance profile of the control infrastructure between the system components that support the processing elements of a large scale application. the first objective was to characterize the costs associated with deploying and removing a large number of applications from the system. these operations involve primarily interactions between the submission client and the dataflow graph manager using two remote procedure calls: instantiate job and condemn job. there are

figure 1: the data fabric internal architecture.also remote calls from the data fabric servers on behalf of pes connected - lookup streams and lookup subscriptions. these calls are used for establishing the initial stream connections between a new pe and the existing topology  both as a producer as well as a consumer . secondly  we were interested in understanding how the process of setting up stream circuits is affected as new pes join and leave the dataflow topology. these are notifications from the dataflow graph manager to multiple data fabric servers using these rpcs - new streams to input ports  new stream notif   new subscribers to output ports  new subscriber notif   removal of a stream  remove stream notif   and removal of a subscription  remove subscription notif .
　all these interactions are affected by the number of pes in the system  the job inter-arrival and inter-departure rate  the number of service threads in all the daemons  as well as the network traffic generated by the streams. since the dgm is in the middle of these interactions  we chose to collect the following metrics:  1  average rpc response time - these are rpcs coming into the dgm from the job submission or dfs remote requests. the timings we report capture the amount of time spent inside the dgm including queuing  execution  and other overheads associated with the operation;  1  average processing time - these are rpcs made out from dgm to dfs daemons as part of the stream setup and tear-down. in this case the timings we report were collected inside dgm and reflect end-to-end communication costs between dgm and a dfs. when collecting the experimental data  we refrained from sending data through the application  since the goal was to obtain a performance profile for the middleware control infrastructure. the results we present are averaged across three runs  where two control knobs were employed:  1  the job inter-arrival and inter-departure time  characterized by an exponential distribution with a preset mean and  1  the number of dgm service threads available for asynchronous operations  typically for issuing and processing dfs rpcs . the experimental results were obtained while the cluster had only a modest amount of background workload.
　the second part of our experimental study consists of micro-benchmarks that are relevant to pe-writers as they affect the performance of individual pes. we measured  1  how much effective bandwidth is available to pes as function of the size of the payload an sdo carries   1  the latency an sdo incurs as it propagates through a chain of pes  and  1  how efficient the data fabric's sdo filtering capability is.
1 a synthetic case study application
　the first part of our experimental study employed a largescale synthetic application we refer to as the large scale demonstration  lsdemo  for short . lsdemo was conceived to demonstrate the various features of spc - the capability to host a large set of jobs resulting in a large number of pes and streams hosted on a large cluster of nodes. lsdemo also showcases the middleware ability to dynamically compose applications. that is  many of its jobs have connections among pes that are loosely specified  i.e.  the stream connections are made solely based on the compatibility of input and output port formats.
　the challenge was not only to design a realistic large-scale stream processing scenario  but also to demonstrate the ingest and processing of an aggregate bandwidth of 1 gb/s of raw data from various data sources. the lsdemo consists of over 1 pes grouped into around 1 jobs  running on 1 nodes. lsdemo ingests content generated by a synthetic workload generator  consisting of internet protocol packets  e.g.  email  instant messaging chat applications  voice and video traffic  among others   de-multiplexes the traffic into specific protocols  extracts features such as ip addresses  subject names  email addresses and  eventually  correlates data across protocols for finding information nuggets such as messages from a particular sender or detecting buzzwords by sifting through the communication traffic. additionally  some pes in lsdemo are tasked with identifying information tidbits that can be used to influence the behavior of upstream pes by  for example  restricting the data a particular pe ingests. although the application employs pes for all these operations  the operations themselves were not fully implemented. instead  the results  or ground-truth  were encoded in the incoming data traffic generated by the workload generator. the actual operations carried out by
the pes consisted merely of forwarding sdos. the data fabric performed the content-based routing and filtered and forwarded sdos according to rules encoded in the sdo-part of their flow specification expressions.
　figure 1 provides a bird's eye view of the complete topology  left-side   and two partial zoomed in views. the figure on the top right-side depicts a network with a source pe  responsible for acquiring data from a workload generator and pumping it into the data processing pes  and a collection of de-multiplexer pes responsible for separating categories of traffic  e.g.  email from chat  and another layer of pes that separates sdos on a per protocol basis  e.g.  smtp from imap  icq from aim  etc . the bottom left-side screenshot shows a closer view of pes  represented as circles  and their input and output ports  represented as squares . the connections between ports are the streams. the popup message in that figure shows status information for a particular pe.
1 large-scale demo results
　the results depicted in this section were obtained by submitting the jobs for the lsdemo application using a single job submission client session  simulating an artificial bulk load/unload operation. an inter-arrival/inter-departure delay drawn from an exponential distribution was imposed between jobs  referred to as jiad in the charts . after a period of 1 seconds  the application was torn down by terminating jobs with a random inter-departure time drawn from the same exponential distribution. this configuration was chosen because it strenuously exercises most of the control paths in the middleware.
　the results shown in figure 1 consist of a parameter sweep where we varied the job inter-arrival and inter-departure rate as well as the number of threads that the dgm allocates to process asynchronous work  e.g.  interactions with data fabric servers .
　figure 1 a  illustrates the service time for calls issued by the job submission client  instantiate job and condemn job  as well as calls issued by data fabric daemons as they initially setup stream connections between processing elements  lookup streams and lookup subscriptions . the general trend is that job inter-arrival/inter-departure time impacts the service time  demonstrating as expected that  the more the requests-per-unit-time the dgm has to handle  the slower it becomes. overall  the service time is below 1 ms  which translates in very fast bulk load/unload operations. indeed  the dispatch of the 1+ pes can be done in approximately 1 seconds and  during this time  the topology becomes stable  i.e.  all stream connections are in place .
　figure 1 b  depicts the response time for calls issued from dgm to the data fabric servers as new streams and subscribers become available  or leave the system  and new stream connections are established or removed between data fabric servers on behalf of pes. the results show that while the number of dgm service threads is a performance factor  for the lsdemo application no more than 1 threads are required  which means that at most 1 concurrent interactions happen between the dgm and several dfs daemons. in most cases  a slower pace of job submission/termination implies quicker response times from the data fabric as each daemon is facing fewer requests per unit of time. the fact that for removed stream notif and removed subscriber notif requests  the data fabric daemons take longer to service
inter-pe throughput

figure 1: effective bandwidth between two pes on different nodes
the notifications as the inter-arrival and inter-departure time increases is interesting. this counter-intuitive behavior is explained by the underlying corba rpc control communication implementation  which harvests connections that go unused for long periods of time  which means that  at teardown time  when these requests are issued   longer response times are observed. in other words  as the inter-arrival and inter-departure rate increases  the corba connection harvesting threshold is reached and the corba channels must eventually be restored in order to carry out the notifications. in general  as seen for the lsdemo  the service and response times observed are considerably small  varying from a few hundred microseconds to a few thousand microseconds  which considerably aid in providing quick bulk load and unload capabilities. during normal operational conditions the service time is typically much smaller  because the control infrastructure is under much less stress as  among other issues  there is less contention for data structure locks.
1 micro-benchmarks
　in the second part of our study  the aim was in showcasing elements of the system design that affect application writers and users from the standpoint of individual pe performance. one of the critical elements is the actual bandwidth pes can effectively use. figure 1 demonstrates that the bandwidth between two pes running on two of our cluster nodes is a function of the size of the payload size carried by an sdo. the data was obtained by running 1-second measurement sessions where sdos were sent as quickly as possible and the number of bytes per second for the serialized sdos was averaged on the receiver side. for larger payloads   1 kbytes   the maximum bandwidth is close to the rated maximum obtainable using a 1 gbps ethernet interconnection.
　application performance is also affected by the latency experienced by sdos transported from the producing pe to the consumers. the results shown in figure 1 a  indicate the variation in the latency incurred by an sdo as a factor of the number of pes in series that it traverses. this experiment was performed with synthetic pes running in three configurations  thereby evaluating the three different transport layers currently available in the data fabric. the first configuration uses pointer transport where the pes are run-

figure 1: large scale synthetic applicationning as threads in a single pe container and  hence  a simple pointer transfer is required to transmit the sdo. the second uses shared memory transport where pes run in independent pec processes on the same node and we use a shared memory-based protocol to transfer the sdos. note that in our implementation  we ensure that there is only one writer per shared segment to provide lock-free access to memory. this results in different shared segments for pes' input and output and consequently data copies from one to the other to transfer the sdo. in the third case  network transport - tcp   each of the pes runs in a different processing node. as expected  the latency increases with the above cases since the data transfer overheads increase. in the experiment  the transmission of 1 sdos was paced over 1 seconds such that it avoided the situation where sdos get bunched together in the dfs' outbound transmission queue. this allowed us to capture the latency of individual sdos due to the transmission overhead  but not due to queuing. the latency was determined by measuring the time needed by an sdo to reach the final pe in the series.
　data filtering is one of the building block operations for streaming data applications. the micro-benchmark whose results are depicted by figure 1 b  illustrates spc's performance when a receiving pe defines a flow specification for one of its input ports. we employed a configuration where the producing and receiving pes are located in the same node on different pecs  which required using the shared memory transport. in this experiment  we created sdos whose structure consisted of 1 to 1 instances of the same attribute type. a flow specification expression consisting of a conjunction of clauses  to avoid a short-circuited evaluation  over individual attribute instances in the sdo was evaluated against each incoming sdo  1 sdos sent over 1 seconds . when the number of clauses in the flow specification expression is more than the number of attributes  the flow specification is evaluated multiple times. the results show that expression evaluation is a linear function of the number of instances of an attribute an sdo carries as well as of the number of clauses in the expression.
　our study has highlighted the middleware performance in a controlled setting  depicting many of the important aspects that are responsible for providing an effective environment to applications. we are now in the process of exposing additional control knobs and performance counters that will enable much more in-depth understanding and automatic control of performance issues in the face of multiple appli-
dgm service time
 a  remote calls serviced by dgm
df response time  measured at dgm 
figure 1: inter-component communication
	latency vs number of df hops	flowspec evaluation cost

 a  per sdo latency overhead as sdos propagate through  b  per sdo flow specification evaluation costs a chain of pes
figure 1: data fabric overheads b  remote calls issued by dgm and serviced by df daemons
　
cations in large configurations. one such approach is the automatic adjustment of stream rates . we believe that such an approach is essential for providing an autonomic computing environment of thousands of nodes hosting tens of thousands of pes.
1. conclusions and directions
　in this paper we presented the stream processing core  middleware designed for supporting large-scale stream mining applications. creating and tapping the knowledge and actionable information available from the huge number of raw data sources coming from the increasingly interconnected digital world requires massive processing capabilities. in this context  spc was designed to support processing elements that will filter  analyze and  as a collective entity  mine these streams for information. we described the spc programming model and its unique features such as the flow specifications that allow stream mining applications to tap into an ever-changing array of data streams available at run time. we also described the spc architecture in detail and demonstrated that our still evolving prototype is able to efficiently support a non-trivial large scale application. using micro benchmarks  we provided experimental evidence showing that the system has been engineered for providing high performance  both in the control infrastructure and in the data dissemination layer.
　we are currently in the process of evaluating how architectural decisions such as stream-based routing as well as pe reuse impact real large-scale applications and aid in addressing the middleware scalability requirements. we are also implementing a replica-based  fault tolerant  dataflow graph manager for greater scalability. in order to support larger volumes of data efficiently  we are implementing multicastbased transport in the data fabric. finally  on the programming model side  the support of java processing elements  additional and improved tooling for the design and implementation of processing elements coupled with gui-based interfaces for application assembly are currently all underway.
acknowledgments
spc is part of a larger project being developed as a collaborative effort amongst many groups within ibm research. the authors wish to thank nagui halim  the principal investigator  and several other project team members for many formative discussions.
