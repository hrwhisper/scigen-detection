   hummingbird participated in 1 tracks of trec 1: the ad hoc task of the robust retrieval track  find at least one relevant document in the first 1 rows from 1gb of news and government data   the mixed navigational and distillation task of the web track  find the home or named page or key resource pages in 1 million pages  1gb  from the .gov domain   and the ad hoc task of the terabyte track  find all the relevant documents with high precision from 1 million pages  1gb  from the .gov domain . in the robustness task  searchserver found a relevant document in the first 1 rows for 1 of the 1 new short  title-only  topics. in the web task  searchserver returned a desired page in the first 1 rows for more than 1% of the 1 queries. in the terabyte task  searchserver found a relevant document in the first 1 rows for 1 of the 1 short topics.
1 introduction
hummingbird searchserver1 is a toolkit for developing enterprise search and retrieval applications. the searchserver kernel is also embedded in other hummingbird products for the enterprise.
   searchserver works in unicode internally  and supports most of the world's major character sets and languages. the major conferences in text retrieval experimentation  trec   clef  and ntcir   have provided opportunities to objectively investigate searchserver's support for more than a dozen languages.
   this paper looks at experimental work with searchserver  experimental 1 builds  for robust retrieval  robustness of ad hoc search across topics   mixed web navigation and distillation  find the one page the user wanted  i.e. a known-item search task  or find key resource pages for broad topics   and terabyte retrieval  ad hoc search on terabyte scales .
1 robust retrieval
for the trec 1 robust retrieval track  there were 1 new topics and 1 old topics. the collection to be searched was the same as last year: a subset of the news and government data of trec disks 1 and 1  fbis  federal register 1  financial times  la times . it consisted of 1 documents totaling 1 1bytes  1gb . the average document size was 1 bytes  though some documents were hundreds of kilobytes.
   while the general objective was to find all the relevant documents for the topic and return them at the top of the list  participants were asked to focus not just on mean average precision but on at least one other measure indicative of  robustness  across topics  such as the success 1 measure  percentage of topics for which at least one relevant was retrieved in the first 1 rows .
   each topic contained a  title   subject of the topic  e.g.  killer bee attacks     description   a onesentence specification of the information need  e.g.  identify instances of attacks on humans by africanized  killer  bees.   and  narrative   more detailed guidelines for what a relevant document should or should not contain  e.g.  relevant documents must cite a specific instance of a human attacked by killer bees. documents that note migration patterns or report attacks on other animals are not relevant unless they also cite an attack on a human.  .
   it turned out one of the new topics had no relevant documents  leaving 1 topics in total. for these  there were on average 1 relevant documents per topic  low 1  high 1  median 1 .
   for the submitted runs due in august 1  it was requested that at least one run just use the title field as the basis of the query and at least one run just use the description field.
more information on this task is expected to be in the track overview paper of the trec proceedings.
1 indexing
our indexing approach was mostly the same as last year . we used a searchserver index which supported both exact matching  after some unicode-based normalizations  such as decompositions and conversion to upper-case  and matching of inflections based on english lexical stemming  i.e. stemming based on a dictionary or lexicon for the language . for example  in english   baby    babied    babies    baby's  and  babying  all have  baby  as a stem. some stop words were excluded from indexing  e.g.  the    by  and  of  in english ; we used a smaller stopfile than last year.
1 searching
unlike previous years  this year we experimented with searchserver's contains predicate  instead of the is about predicate ; for short boolean-or queries  contains should produce the same ranking as is about. our test application specified searchsql to perform a boolean-or or boolean-and of the query words. for example  for topic 1 whose title was  killer bee attacks   a corresponding searchsql query with a boolean-or of the title words would be:
select relevance '1'  as rel  docno
from robust1
where ft text contains 'killer'|'bee'|'attacks'
order by rel desc;
for a boolean-and  the contains list would be changed from 'killer'|'bee'|'attacks' to 'killer'&'bee'&'attacks'  and only documents containing all of the specified words  or an inflection of each of them  would be retrieved.
   most aspects of searchserver's '1' relevance value calculation are the same as described last year . briefly  searchserver dampens the term frequency and adjusts for document length in a manner similar to okapi  and dampens the inverse document frequency using an approximation of the logarithm. when doing morphological searching  i.e. when set term generator 'word!ftelp/inflect' was previously specified   these calculations are based on the stems of the terms  roughly speaking .
   this year's experimental searchserver version contains an enhancement for handling multiple stemming interpretations  e.g. in english   axes  has both  axe  and  axis  as stems . for each document  only the interpretation that produces the highest score for the document is used in the relevance calculation  but all interpretations are still used for matching and search term highlighting . sometimes this enhancement causes the original query form of the word to get more weight than some of its inflections  and it never gets less weight . this enhancement typically makes little difference for english  an exception is topic 1  mainstreaming  discussed below . more details were in our clef paper this year .
1 relevance options compared
for ranking  searchserver has several options  including 1 different relevance method settings  '1'  '1'  '1'  '1'  '1'  and a relevance dlen imp setting for controlling document length normalization  scale 1 to 1 . there is also a term generator option to control whether or not inflections of query terms are matched.
   using the full set of 1 topics  we looked at whether the impacts of searchserver's ranking and matching options for english ad hoc search were the same for boolean-and queries as for boolean-or queries. for these tests  just the title field of the topic was used  typically 1 words .
   for each approach  boolean-or and boolean-and   there was a  baseline  run using the options expected to score highest based on past experience  set relevance method '1'  set relevance dlen imp 1  set term generator 'word!ftelp/inflect' . the other diagnostic runs differed from the baseline as follows:
   1 : the run used relevance method '1'   hits count   i.e. a simple count of all of the matches in a document  so repeated matches count multiple times . note that document length normalization is ignored by this method.
   1 : the run used relevance method '1'   terms count   i.e. a count of the number of query terms matched  so if the query contains 1 words  the maximum score for a document is 1 . since inflections were enabled  a query term would count if any inflection of it was matched  but different inflections of the same word would not count for more than 1 . document length normalization is ignored by this method.
   1 : this is the baseline run using relevance method '1'   terms ordered . a formula which incorporates term frequency  inverse document frequency and document length normalization is applied as described earlier  section 1 .
   1 with no stemming : inflections were disabled for this run  i.e. set term generator '' .
   1 with no dlen : document length normalization was disabled for this run  i.e. set relevance dlen imp 1 .
   1 : the run used relevance method '1'   critical terms ordered   which squares the importance of inverse document frequency compared to '1'  i.e. less common terms get even stronger weight .
   1 : the run used relevance method '1'   consistent terms ordered   which is an experimental new relevance method that is the same as '1' except that it does not include inverse document frequency  all the query terms are treated as being equally important .
