this work describes an automatic query refinement technique  which focuses on improving precision of the top ranked documents. the terms used for refinement are lexical affinities  las   pairs of closely related words which contain exactly one of the original query terms. adding these terms to the query is equivalent to re-ranking search results  thus  precision is improved while recall is preserved. we describe a novel method that selects the most  informative  las for refinement  namely  those las that best separate relevant documents from irrelevant documents in the set of results. the information gain of candidate las is determined using unsupervised estimation that is based on the scoring function of the search engine. this method is thus fully automatic and its quality depends on the quality of the scoring function. experiments we conducted with trec data clearly show a significant improvement in the precision of the top ranked documents.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval
general terms
algorithms  theory
keywords
automatic query refinement  pseudo relevance feedback
1. introduction
　the goal of information retrieval systems is to identify documents from a given collection that best match the user's information needs. existing search engines provide valuable assistance to users in locating relevant information  however  finding precise information is progressively becoming more difficult. this is especially true for large collections and for
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  tampere  finland.
copyright 1 acm 1-1/1 ...$1.
interactive systems where users tend to only look at the top k documents where k is small  e.g.  1 documents .
　automatic query refinement  aqr  has long been recognized as an effective technique to improve retrieval performance  1  1  1 . aqr techniques refine the user's query by adding terms which are considered to be related to the original query terms. the goal of the refinement is to cover additional aspects of the information need as specified by the query. the expansion terms can be selected from a thesaurus  a synonym table  or a semantic word network such as wordnet . alternatively  aqr can be performed using pseudo-relevance feedback techniques  which are based on the assumption that the top-ranked documents for a given query are indeed relevant. traditional relevance feedback methods such as rocchio refinement process  can then be used to effectively refine the query. in particular  a set of top-ranked documents is first retrieved using the original user query. the weight of the query terms is modified according to their frequency in this set. in addition  expansion terms are selected from this set  based on various selection criteria  and added to the query. the refined query is then submitted to the system  resulting in the final set of documents considered relevant to the original user query. using this method with n  the number of top-documents used for expansion  ranging from 1 to 1  has been found to increase retrieval performance  by 1 to 1% on trec corpora .
　recent studies have however shown that while aqr improves recall  it could harm precision  especially when applied to very large document collections . furthermore  traditional aqr techniques are not necessarily effective for improving the precision of the top results. recall is generally improved by aqr since the expansion terms broaden the scope of the query. specifically  all documents retrieved for the original query are also retrieved for the expanded query and new documents containing the expansion terms are added to the result set. precision  however  may deteriorate if the expansion affects the order of results returned by the search engine  causing non-relevant documents to precede relevant ones. this phenomenon has been reported by web track participants of the trec 1 conference. only one participant reported improvement in performance by automatic query expansion  while all other participants reported either a decline or no gain in terms of precision .
　there are several reasons why the overall precision may diminish using aqr techniques:
  expansion terms may cause a drift in the focus of the search topic. this can happen when terms are added
　to the query since they happen to meet the selection criteria but are not directly related to the information need specified by the user query. consequently  documents that are relevant to the expanded query are not necessarily relevant to the original query. for example  mitra et al.  analyze the trec query  what is the economic impact of recycling tires  . their aqr method expands the query with terms relevant to plastic recycling in general rather than specific to tire recycling. thus  for this query  query expansion actually results in a significant loss of precision.   aqr can additionally  punish  relevant documents that do not include the terms selected for expansion. as a result  a relevant document that does not contain some of the additional terms might be ranked lower than non-relevant documents which contain those terms. for example  consider the web track topic  jennifer aniston . our aqr system  using rocchio refinement process  added the terms   actress    movie    player   to the original query terms. this expansion results in a loss of precision compared to the original query. the reason for this is a decrease in the score assigned to documents that include the original query terms but do not include the expansion terms. additionally  documents dealing with movies and movie players  but not with jennifer aniston  were assigned a high score since they contain the expanded terms with high frequency. consequently these irrelevant documents preceded relevant documents in the final result set.
  the documents that are used as a basis for selecting additional query terms  usually the top-ranked documents for the original query   may not be relevant to the original query. by expanding the query with terms extracted from the top results the system improves the rank of documents  similar  to the top results. however  for  difficult  queries  for which most of the top results are not relevant  aqr might fail. it turns out that aqr works best for  easy  queries. for difficult queries with non-precise top results it may not improve  and even harm precision.
　in this paper we suggest a novel query refinement technique  which attempts to improve the precision rather than recall by overcoming the above obstacles. the main idea is that rather than expanding the query by adding additional terms  we should narrow the query by further qualifying query terms. in other words  the goal is to find expanded terms that sift up documents that were already deemed somewhat relevant to the original query but received a low score.
　the basis for the method we suggest is lexical affinities. a lexical affinity  la  represents the correlation of words co-occurring in a document  and is identified by looking at pairs of words found in close proximity to each other . we consider las which contain exactly one of the original query terms. we show that adding such las to the query is equivalent to re-ranking search results  thus  precision might improve while recall is preserved.
　the method used to select las for refinement is based on information theory. specifically  we look for the most informative las  i.e.  the ones with the maximal information gain as defined in . the las that best separate the relevant documents from the irrelevant ones in the set of results are selected for refinement. since relevance information is clearly not available at the retrieval stage  our refinement process uses an unsupervised method which is fully automatic and can be applied without any manually labeled data. it estimates the relevance probability of a set of documents based on the relevance scores given by the search engine.
　the remainder of the paper is organized as follows: in section 1 we review aqr techniques in general and focus especially on work related to ours. section 1 describes our refinement method which is based on information theory and provides the theoretical basis for this method. in section 1 we show experimentally  using trec data  the advantages of our aqr method. the experimental results clearly show that the aqr method proposed here significantly improves precision of the top results. concluding remarks are given in section 1.
1. related work
　query expansion and pseudo relevance feedback have been studied quite extensively. the continuing desire to improve retrieval effectiveness has resulted in significant research in this area for quite some time. a good overview of aqr techniques by gauch et al. appears in . they divide the various aqr techniques according to the three main sources for candidate expansion terms:
1. query specific terms identified by local analysis of topretrieved documents e.g.  1  1  1 .
1. corpus specific terms identified by global analysis ofthe whole corpus and the use of automatic thesaurus construction e.g.  1  1  1 .
1. language specific terms found in a generally availablethesaurus e.g. .
　these techniques are mostly focused on selecting terms for query refinement that will improve recall and precision. our work  on the other hand  is focused on improving the precision of the top ranked results while preserving recall.
　in   hearst identifies the need to ensure high precision in the top-ranked documents retrieved by an interactive information access system. in order to provide better precision among the top-ranked documents  hearst proposes to add manually formulated boolean filters along with proximity constraints to the query. the work presented in  is also concerned with ways to improve the precision of the topranked documents. they use a two-stage document ranking method where at the second level  retrieved documents are re-ordered using a mutual information measure between query terms and document terms.
　mitra et al.  investigate ways to improve aqr process by increasing the precision of the top-ranked documents used for pseudo-relevance feedback. they add constraints to the original query in order to get stronger indications for document relevance. the new query is used to re-rank the initially retrieved documents and the refined set of top documents is used for expansion.
　the use of phrases for query expansion appears in  1  1  1   all use the inquery information retrieval system. jing and croft  describe a system called phrasefinder that identifies associations between phrases and query terms so that associated phrases can be used for improving retrieval performance via query expansion. in our work  the phrases  lexical affinities  used for query refinement are only those for which one of their terms is taken from the query. the phrases considered in  are the complementary phrases  i.e.  phrases including only query terms and phrases including no query terms.
　the mutual information measure for re-ranking used by kang et al.  considers phrases similar to las  i.e.  phrases that occur in a small window of terms around the query terms within the document.
　in   carpineto et al. present a selection method of terms for aqr which is based on information theory. they measure the difference between the probability distribution of a term in the set of  pseudo  relevant documents and its probability distribution in the whole corpus. their method selects terms that maximize the divergence between the two probability distributions.
　our approach differs in two main aspects. first  our method uses unsupervised estimation of the relevance probability of a set of documents  based on document scores. hence we consider range of relevancy of documents rather than binary  manually or automated  labeling. second  in our method terms are judged according to their capabilities to differentiate between relevant and non-relevant documents in the result set. specifically  we look for the terms that best split the result set into two subsets  one contains mostly relevant documents and the second contains mostly non-relevant documents. in the following section we describe our selection method in more detail.
1. an aqr method based on information theory
　in this section we describe our aqr method which is based on the information gain obtained by adding lexical affinities to the query. lexical affinities  las  represent the correlation between words co-occurring in a document  and are identified by looking at pairs of words found in close proximity to each other. thus  an la is a pair of distinct words  ti tj . previous work  explains how las  when used as indexing units  improve search precision by disambiguating terms. the same reference describes how las can be extracted from text by finding the most common pairs of words found close to each other in a window of small size.
　let q = {q1 ...qk} be the original query. let dq be the result set for query q  all documents associated with a positive score . let l be a set of las  each one contains exactly one of the original query term 
l = { ti tj | ti （ q or tj （ q  but not both}.
the set l can be extracted from the set of all las found in the collection  from a set of las extracted from the topranked documents in dq  or from any other static thesaurus that provides las for a given term. // our aqr method chooses the  best  las from l and adds them to the query. the selection method will be described in the following subsection. it is important to emphasize that if ti belongs to the original query q and  ti tj  is added to q  the term tj
is not added to the query. thus  such an addition only affects the score of documents which contain the pair  ti tj  while the score of documents containing only one of the pair terms  or none of them  is not affected1.
　assume the original query contains the term ti. by adding the la  ti tj  to the query all documents in dq containing  ti tj  will get a higher score while the score of all other documents will not change. precision is expected to improve if the set of documents containing  ti tj  includes more relevant documents than its complement  since many relevant documents will get a higher score which will improve their ranks. it might be the case that the set of documents not containing the added la will include more relevant documents than its complement. in this case we should add the la to the query with negative sign. thus  by adding an  informative  la to the query  i.e.  an la that best differentiates relevant documents from non-relevant documents  precision is expected to improve. figure 1 shows an illustration of the splitting process.

figure 1: splitting the result set by refining the query with la. the '+' sign marks relevant documents while the '-' sign marks non-relevant documents.
　note that the refinement method does not change the score of any document not in dq  thus the result set for the refined query remains the same. consequently  recall is not affected by our refinement method  hence  the suggested aqr process is equivalent to re-ranking of the original result set according to the refined query. all disk accesses  required for the second retrieval stage performed by traditional two-stage aqr methods  are thus not necessary using our method.
　we next describe the method used to select the best las for refinement.
1 choosing the best las for refinement
　as already defined  let dq be the result set for query q  i.e.  the set of documents with positive score. we consider las that split the set dq into two subsets  one set includes all documents containing the la and one includes all document not containing it. an  informative  la will be the one that successfully differentiates between relevant and non-relevant documents. ideally  one subset will include all relevant documents while the second will include all the

1
 we assume an additive scoring function  i.e.  adding terms to the query increases the score of documents containing these terms and does not affect the score of other documents. non-relevant documents. thus we look for the most informative la  i.e.  the one with the maximal information gain as defined in .
　definition 1.: the relevance probability of a set of documents d to a given query q  p d   is the probability of a document chosen randomly from d to be relevant to q. let r   d be the subset of relevant documents to q in d. then p d  = |r|/|d|.
　definition 1.: the entropy of the set d  h d   is defined as
h d  =	  p d log p d      1   p d  log 1   p d  .
h d  commonly refers to the degree of disorder of the set.
　definition 1.: given a document set d and a term l. let d+ be the subset of documents of d containing l and d  its complement. the information gain of l  ig l   is defined by

the first element describes the entropy of the set before splitting  the second describes the weighted average entropy of both subsets after splitting. the difference expresses the information gain achieved by the splitting process  i.e.  how much the entropy is reduced  or in other words how much order has been added to the set. for ideal splitting where all relevant documents belong to one of the subsets and all nonrelevant documents belong to its complement  the entropy will be reduced to zero and ig is maximal.
　the selection method is based on computing the ig for every la in the set of candidate las and choosing the best las with maximal ig for query refinement. alternatively we can compute the ig for all subsets of las of a given size and choose the subset with maximal ig for expansion.
　the ig computation for a given la depends on estimation of the relevance probability of subsets of documents to a given query. the following subsection describes how the relevance probability of a set of documents can be estimated.
1 unsupervised estimation of the relevance probability of a set of documents to a given query
　in order to choose the best lexical affinities we need to evaluate the relevance probability of a set of documents. this subsection describes an unsupervised method that estimates this probability.
　the relevance probability of a set of documents can be estimated by supervised methods that are based on labeled training data. given a subset of documents  labeled according to their relevance to the query  the relevance probability of the whole set can be estimated by simple sampling techniques. the problem is that it is hard to create a set of labeled documents since labeling is an expensive manual task.
　our method does not require any labeled training data. rather  it uses the scoring function of the search engine used to rank the search results. the quality of our estimation depends on the quality of the scoring function. for a reasonable scoring function that correlates document scores with their relevance probability  our estimation can successfully replace labeled training data.
　given a set of n documents  {d1 ...dn}  and a query q. let pi be the probability that document di is relevant to query q. later we will show how pi can be induced from the document scores provided by the search engine. for each document di define an independent variable xi representing the relevance of di. that is  xi is set to 1 with probability pi and to 1 with probability  1   pi . xi is a binomial independent variable with expectation e xi  = pi and variance v ar xi
　letis a random variable that counts the number of relevant documents in the document set. the expectation of x is
	n	n	n
	e x  = e	xi	=	e xi  =	pi.
	i=1	i=1	i=1
e x  estimates x  the number of relevant documents in the set.
　since x1 ...xn are independent binomial variables  the distribution of x is normal and the variance of x is
.
given δ   1  the error of estimating x can be measured by applying the chebyshev inequality:
.
let be the relevance probability of the set d. a straight forward estimation of p d  is
.
clearly  from chebyshev inequality we can estimate the error of  p d .
.
as ig is a continuous function in p d   p d+  and p d    we can estimate the error of ig which depends on the errors of these estimations. this can result in an improved term selection method but we leave this research direction for future work.
　the above measure of p d  depends on our knowledge of the relevance probability of every document in the set to the query. these probabilities can be induced from the scoring function of the search engine. scoring functions are usually designed for ranking purpose and not for probability estimation. for such scoring functions we need a tool that calibrates scores to probability values. recently  some methods for modeling score distribution has been suggested  1  1  1  and can be applied for this purpose. in this work we use a simple straight forward approach.
　let smax be the maximum score possible for a given query. we assume that the relevance probability of a document associated with such a score is 1. let si be the score for document di. thus  a natural approximation of the relevance probability of document di is pi = si/smax. this approximation guarantees that the probability assigned to each document preserves the order among documents  if si  = sj then pi  = pj   assigns relevance probability of 1 to documents with maximum score  and a zero probability to documents with zero score.
　the maximum score for a given query  smax  can be determined by computing a score of a virtual document identical to the query. this is based on the assumption that a document identical to the query must be relevant to the query.
　the quality of our estimation mostly depends on the quality of the scoring function. for example  given a scoring function that associates a score of 1 to all relevant documents and 1 to all the rest  our estimation will succeed in measuring the true number of relevant documents in the set  with zero variance   while for a scoring function that associates a random score for all documents  our estimation is meaningless.
1. experimental results
　in this section we report results from experiments we conducted to evaluate the influence of the suggested aqr method on search precision. we used the trec collection  for our experiments and evaluated our methods on the ad hoc tasks for trec-1  with topics 1. we experimented both with short queries and long queries. we used the topic title for short query construction  and the title concatenated with the topic description for long query construction. original query terms extracted from the topic text include keywords and inner las  both terms belong to the query . we used precision at k  p k  for k =  1 1 1   and mean average precision  map  measures to measure search precision .
　the refinement process for a given query is carried out as follows:
1. retrieve the result set for the query  all documentswith positive score 1.
1. extract the candidate las from the top-n documents  n was set to 1 in our experiments . a candidate la is a pair of closely related terms which contains exactly one of the original query terms.
1. compute the information gain for each candidate laas defined by equation 1.
1. add the m las with maximal gains to the query.
1. re-rank the result set using the refined query.
　in the first experiment we tested how the number of terms added to the query in the refinement process affects the precision. table 1 reports p k and map for different values of m  the number of added terms. the first row  1 terms  shows the average result for the original 1 queries with no refinement. row m shows the average results for the same queries  each expanded with the best m las.
　figure 1 graphically illustrates the improvement in precision for the refined queries relative to the original queries. from these results it is apparent that in general there is a significant improvement in precision when adding a small number of las  up to 1 . specifically p 1 improves by

1
 in huge collections such as trec result sets may include hundred thousands documents. in practice  trec evaluation procedures consider only the top-1 documents as the result set. similarly  from practical reasons  our method considers only the top-1 documents as the result set for a query.
terms no.p 1p 1p 1p 1p 1map1.1.1.1.1.1.11111111.1.1.1.1.1.11111111.1.1.1.1.1.11111111.1.1.1.1.1.11111111.1.1.1.1.1.11111111.1.1.1.1.1.1table 1: precision as a function of the number of added terms.
more than 1% when adding 1 las. interestingly  adding more than 1 las to the original query decreases p k  while still increasing the overall precision as measured by map. the reason for this phenomenon is that our selection process tests the gain of each candidate la separately and does not consider the dependency between las. ideally  the gain of each subset of candidates should be computed and the subset with maximal gain should be selected for refinement. this however is not computationally feasible in most cases.
　table 1 presents some of the queries and the las used for query refinement. note that the refined las are shown in their stemmed form. it is interesting to notice that the term  subject  is part of several las selected for refinement. such an la is extracted due to the appearance of one of the query terms in the document title which is marked by the tag  subject . our refinement method automatically deduces that the appearance of query terms in the document title is a good sign for relevancy.
　the refined las for topic 1 demonstrate one of the weaknesses of aqr methods based on pseudo relevance feedback including the method presented here. specifically  aqr can fail when handling  difficult  queries for which the system's scoring function is ineffective. this topic deals with king hussein's contribution to the peace process in the middle east. however  the scoring function fails to place truly relevant documents high in the result list during the first phase of query processing. in fact it retrieves documents dealing with saddam hussein and consequently two of the three las selected as having the maximal gain are related to sadam hussein. hence aqr  in this case  sifts up even more results dealing with sadam hussein and precision declines.
　the second experiment tested how query length influences the effectiveness of our refinement method. we ran two tests  one using short queries  based on topic title only  and one using long queries  based on title plus description . in contrast to the previous experiment  the original queries include keywords only and do not include inner las extracted from the topic text.
　for each test we computed the precision measures with and without refinement. the number of added terms for refinement was set to 1 for both tests. table 1 reports the results for the original queries and the refined queries for both query types.
　figure 1 illustrates the change in precision for both query types. while refinement for short queries improves the p 1 measure by almost 1%  the improvement in precision for long queries is lower than 1%. it is clear that the refinement
query typep 1p 1p 1p 1p 1mapshortorig111111refined111111longorig111111refined111111
figure 1: change in precision as a function of number of added terms.
query no.queryp 1 orig p 1 refined added las1behavioral genetic11genetic*subject  genetic*research genetic*medic1parkinson's disease11parkinson*victim parkinson*symptom parkinson*patient1tropical storms11storm*subject  storm*type storm*weaken1airport security11airport*subject  airport*headline1drugs  golden triangle11drug*mule  golden*southeast1art  stolen  forged11art*theft  stolen*subject *stolen*work1estonia  economy11estonia*lithuania  estonia*republ estonia*latvia1tourism  increase11tourism*travel  tourism*unit tourism*year1child labor11labor*law  labor*violat child*violat1king hussein  peace11hussein*jordan  hussein*iraq hussein*saddamtable 1: some queries and las selected for refinement.table 1: precision as a function of query type.
process works much better for short queries. one of the reasons for this is that the results for the original long queries are more precise  p 1 = 1  than the results for the original short queries  p 1 = 1   hence improvement is much harder to achieve. note however  that recently users tend to submit short queries to modern ir systems  thus it is crucial to improve the precision for this type of queries.
　the third experiment compared our information theoretic la selection process with other selection methods. we ran four tests with four different selection methods:
  random - select m las randomly form the set of all candidate las.
  tfidf - the gain of each candidate is computed by a traditional tf 〜 idf formula. the gain is a multiplication of the term frequency  tf  of the candidate la in the top-1 documents and the inverse document frequency  idf  in the whole collection.   unsvit - associates candidates with the information gain as defined in section 1.
  svit - associates candidates with information gain based on supervised labeled data. the relevance probability required for gain estimation is computed using the set of relevant documents provided for each topic by the trec data. given a document set of size n with r ＋ n relevant documents  the relevance probability of the set is determined by p d  = r/n.
for this experiment the number of added terms was set to 1  and the number of top documents for candidate extraction was set to 1.
　figure 1 shows the change in precision of the refined queries compared to the precision of the original queries  using the

figure 1: change in precision for short and long queries.
four selection methods. the random selection completely fails and severely hurts precision. the unsupervised method improves precision by 1% and clearly outperforms the tf 〜 idf method. the supervised method dramatically improves precision by 1%.

figure 1: change in precision for different selection methods.
　note that random selection is equivalent to invocation of the unsupervised selection method with a random scoring function. the supervised method is on the other hand equivalent to invocation of the unsupervised method with an ideal scoring function that associates a score of 1 to relevant documents and 1 to all others. thus  these results can be treated as a lower bound and an upper bound on the achievable results using the unsupervised method. it clearly demonstrates the strong dependency of aqr on the quality of the scoring function.
1. conclusions
　in this paper we have described a query refinement technique  which focuses on improving precision of the top ranked documents. the candidate-terms used for the refinement are lexical affinities  las  which contain exactly one of the original query terms. adding these terms to the query is equivalent to re-ranking search results  thus  precision is improved while recall is preserved. we describe a novel method to select the most  informative  las. namely  those las that best separate relevant documents from the irrelevant documents in the set of results. the relevance probability of a set of documents to a specific query is determined using unsupervised estimation. this estimation is based on the scoring function of the given search engine rather than on labeled training data. thus  our method is fully automatic and its quality depends on the quality of the scoring function.
　the effectiveness of the suggested aqr method was evaluated using the trec collection with the ad hoc-tasks of trec-1. our evaluation clearly shows a significant improvement in the precision of the top ranked documents compared to traditional tf 〜 idf selection methods. when using a supervised relevance information  using trec manual judgments   our method performs dramatically better.
　although our method significantly improves the precision of top ranked documents  it does not totally overcome the problem of  difficult  queries  i.e.  queries for which most of the top results are not relevant. the problem is that aqr methods based on pseudo-relevance feedback mistakenly assume that the top ranked results for such queries are relevant. our method uses the top ranked documents only for extracting the candidate las  and then selects las for refinement by computing their information gain. since difficult queries mislead the scoring function of the search engine to associate high scores to irrelevant documents  our computation of relevance probability is also faulty in this case.
　in the future  we would like to find ways to overcome this problem and thus further improve top ranked precision of aqr based results. one possible direction is to improve our term selection method by considering the error estimation of the information gain. in addition  more sophisticated phrases should be considered for refinement. another possible direction for improvement is to compute the information gain for subsets of las  and use the best subset  of any size  for the refinement.
