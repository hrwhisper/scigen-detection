　　　this paper investigates the applicability to a shape-recognition problem of a concept learning algorithm which generates decision rules from examples. a comprehensive analysis of this algorithm applied to an industrial vision problem is described. this problem has no obvious 'best' solution and much effort has been devoted to performing a realistic appraisal of the algorithm by making a detailed set of comparisons with the performances of appropriate alternative classifiers. results presented show the algorithm to be comparable in performance with the alternative classifiers but superior in terms of both the cost of making a classification and also the intelligibility of the solution. 
l. 	introduction  
　　　a nonparametric  or  distribution-free   solution to a pattern recognition problem is one designed only from information contained in representative examples of the pattern classes. this approach is popular in practical problems where detailed statistical information is rarely available. a promising nonparametric solution to classification problems is to produce decision rules which can be expressed in the form of decision trees. some algorithms for generating such decision trees from examples have been suggested  eg henrichon & fu 1  selthi & sarvaraydu 1  but have concentrated on classification using only numerical measurements of the patterns. a similar limitation arises with 'statistical' classifiers  these are well suited to patterns containing noise  random variation  but poor at dealing with structural information. the algorithm described here has the ability to use both numerical and non-numerical variables. its origins can be summarised. 
　　　an investigation by hunt  1  into inductive reasoning produced an algorithm called cls  concept learning system  for generating decision trees from examples defined in terms of nonnumeric attributes. each attribute has a predefined set of outcomes which need not represent values along a physical dimension or have any ordering. 1  iterative dlchotomiser 1.quinlan 1  an algorithm based on cls has been extensively tested on the problem of classifying positions as won or lost in chess endgames  shapiro & niblett 1 . id1 has displayed many advantages over alternative solutions. it oan often be used in such a way as 
 this research is supported by a grant from the 
1erc for co-operative research with gec hirst research centre. wembley. principal investigator: professor donald michie. 
to produce solutions with a high degree of human intelligibility and has been extended at miru  blake. 1  to handle both numerical and nonnumerical attributes. this extended form is called acls  analogue concept learning system . the work described here seeks to evaluate the use of acls-generated decision trees in image classification problems. an industrial vision problem was selected as test domain. this problem has no obvious 'best' solution and much effort has been devoted to performing a realistic appraisal of the acls solution by making a comprehensive set of comparisons with the performances of alternative classifiers. 
1 the. vision problem domain-
　　　a sufficiently demanding vision problem has been posed by rowntree of york concerning the identification of black magic chocolates at their factory. in this problem the 1 different types of chocolate have only one stable position but can take any orientation in the horizontal plane. classification is to be done from a plan view image of individual chocolates digitised to an accuracy of 1 pixels. the problem is further defined by making the classification based on shape information  ie. silhouettes  only. 
1  the acls algorithm  
　　　acls inputs examples of the pattern classes. each example consists of an attribute vector paired with a class value. a decision tree is generated from a collection of examples  training set  by recursively sub-dividing this collection into smaller subsets according to the values of the attributes. the id1 entropy-based cost measure is applied to each subset to determine which attribute will create the 'best' further sub-division and a node representing the attribute used is added to the 
tree. the form of the node will depend on whether the attribute is numerical  called integer  or non-numerical  called logical . nodes for integer attributes are binary and test the attribute against a single threshold value. in contrast nodes testing logical attributes have an output branch associated with each value the attribute may take. sub-dividing the training set continues until each subset contains examples of one class only  and the corresponding terminal node  leaf  is labeled with that class value. if the tree generated is binary then all leaves will be labeled but only with class values for which training examples exist and hence it will discriminate only between these particular classes. the multi-branching trees produced using logical attributes may contain many unlabeled leaves  called null . these leaves may 

1 b. shepherd 
correspond to attribute vectors not belonging to any of the pattern classes or they may have arisen only because of the absence of an appropriate training example. 
1. 	objectives of the investigation. 
　　　the advantages of a decision rule in tree form can be summarised: 
 a  generating the tree performs an 'attribute selection' function allowing each class to be described by the most appropriate subset of attributes. thus the set of attributes supplied to the learning phase can be selected without regard to minimising its size: reduction of size can be left to 
the algorithm itself. 
 b  when making a classification only attributes occurring on the decision path need be computed.  c  decision trees lend themselves to humanly understandable solutions. 
 d  acls trees in particular can handle both integer and logical attributes  see notel . 
	in 	order 	to 	measure 	these 	advantages 	a 
detailed experimental comparison was made with two alternative classifiers: the k nearest neighbour decision rule  k-nn   fix & hodges. 1  and the minimum distance classifier  mdc   see for eg. devijver & kittler. 1 . a third alternative means of classification was also undertaken and consisted of showing the chocolate images to a 
series of human subjects. it was hoped that these latter results would provide a useful measurement to help judge the machine performances. notel: a full investigation into the advantages and scope of logical attributes is unrealistic with this domain. however a class of logical attributes which can be investigated is that generated by discretising the integer attributes. if these discrete values can be given conceptual names eg. small  large then the intelligibility of the tree may benefit. 
1. 	experimental prgcflflurg  
　　　over 1 boolean images were created  with approximately equal numbers from each chocolate class. for each image the nine numeric shape descriptors summarised in fig. 1 were computed. these attributes were chosen mainly because of their ease to compute. they were not tailored to the problem neither were they expected to totally solve it. the resulting attribute vectors were paired with the correct class values and used as examples. testing the classifiers was achieved by dividing the total set of examples into various subsets and using each in turn as training and test sets. in order to pursue the use of logical attributes the integer attributes were divided into labeled subranges  and the experiments using the acls classifier repeated. 

1 results and discussion  
1 classification success rates. 
　　　the results obtained using the integer attributes are summarised in fig. 1. in order to both train and test the classifiers the maximum training set size was limited to 1 examples  leaving 1 test examples . all performances improve as the training set size increases to this value. it seems likely that these performances will improve further  to some steady value  given larger training sets. no major differences among the best performances of the three algorithms are evident from these results which are substantially better than those obtained from human subjects trained on similar material  see later . results for the smaller training set sizes show the k-nn algorithm to have a higher performance. note that the k-nn method  k equal to one  and the mdc method are algorithmically identical when the training set contains only one example of each class. 
logical attributes were investigated in two ways:  a  expressing each integer attribute as a multivalued logical attribute. 
　　　this gave inferior results which worsened as n the number of values in each attribute was increased. a larger value of n produced an increase in the number of valid combinations of the attributes which consequently required a greater number of training examples in order to be labeled correctly. this is an illustration of a problem often encountered in pattern recognition whenever the dimensionality of the measurement' space is increased. in this problem. increasing n caused the fixed size training sets to become less representative resulting in an increasing number of unlabelied  null  leaves appearing in the trees generated. the actual decrease in success rate arose because many of these null decisions occurred during the testing phase. the potential advantage of the multi-branching trees to distinguish non-classes from classes is not exercised here since ail test classifications were made using only class examples. hence all the null classifications which occurred were erroneous. 
 b  expressing each integer attribute as a series of boolean logical attributes: 
　　　the idea here was to use logical attributes and also generate binary trees. each outcome of the multi-valued logical attributes was used as a separate boolean attribute taking the values true or false. for example if the attribute 'area' was given the outcomes 'small' and 'large' then the boolean attributes derived in this way would be: 'ls-areasmall ' and 'is-area-large '. trees generated in this way were were found to perform nearly as well as those using integer attributes. 


1 intelligibility of the solutions. 
　　　two factors which effect the intelligibility of a 
　　　decison tree are its size and the intelligibility of the attributes used. the smallest binary tree which can solve this problem must have 1 branch nodes + 1 leaves = 1 nodes  but this would require ideal attributes. those used in this investigation possess a high degree of variation  see next section  and so the measured average tree size  best performance  of 1 nodes is not excessive. however inspection of the trees shows them to be too large and scattered to be easily comprehended. replacing each integer attribute by one many-valued logical attribute can help to conceptually simplify the tree. at each node all possible outcomes of the tested attribute are explained and each attribute only occurs once on any decision path. this compares favourably with the binary trees where the same attribute can be scattered intermittently along 
any path. this option has its limitations: an excessive number of outcomes in each logical attribute may also result in large and complicated trees. the lower observed performance of the multi-branching trees would seem to suggest a trade-off' between performance and any gain in intelligibility they may provide. however the size of the training set must also be taken into account here since a larger training set may well raise the performance to an acceptable level  see e.l.cp. 
1 success with the chocolate domain  the major cause of classification error arose from a large wlthin-class variation in the images which can be split into manufacturing and image oapture variations an analysis of the attribute values shows a large component of the latter arises from lens distortion effects. experiments were performed cmowforth.1  to test human subjects on the same 
images as used in the above experiments. the average suooess rate measured was only 1% but increased to 1% when using images created using a planar lens  no lens distortion . this confirms the lens as a souroe of error and also reflects well 
b. shepherd 1 
on the machine success rates. a performance of 1% is probably unattainable using silhouette images alone. the experimental confusion matrices show a strong confusion between two of the shapes  both squares  and this is confirmed by inspection of the chocolates themselves. a grey level measurement may be necessary to discriminate these particular chocolates. 
1. conclusions and further wprk. 
in this domain the decision trees generated by 
acls have performed comparably with those of practical alternative classifiers and all compare favourably with the performance of humans. in addition acls trees have displayed advantages regarding both the cost of making a classification and the intelligibility of the solution. comparisons amongst the various trees have indicated that binary trees may perform better than the logical multi-branching trees  although the latter may have a certain advantage regarding intelligibility. current work includes increasing the conceptual power of the trees. one approach  shapiro & niblett 1  
makes large trees more understandable by structuring them into a hierarchy of sub-trees each representing an identifiable sub-problem. the experiments are also being re-run using images with no lens distortion and this together with a small extension to the attribute set is expected to achieve a practically acceptable success rate in this problem domain. 
　　　future work will apply this algorithm to a more structurally complex problem domain. 
