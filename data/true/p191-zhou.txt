as more and more query processing work can be done in main memory  memory access is becoming a significant cost component of database operations. recent database research has shown that most of the memory stalls are due to second-level cache data misses and first-level instruction cache misses. while a lot of research has focused on reducing the data cache misses  relatively little research has been done on improving the instruction cache performance of database systems.
we first answer the question  why does a database system incur so many instruction cache misses   we demonstrate that current demand-pull pipelined query execution engines suffer from significant instruction cache thrashing between different operators. we propose techniques to buffer database operations during query execution to avoid instruction cache thrashing. we implement a new light-weight  buffer  operator and study various factors which may affect the cache performance. we also introduce a plan refinement algorithm that considers the query plan and decides whether it is beneficial to add additional  buffer  operators and where to put them. the benefit is mainly from better instruction locality and better hardware branch prediction. our techniques can be easily integrated into current database systems without significant changes. our experiments in a memory-resident postgresql database system show that buffering techniques can reduce the number of instruction cache misses by up to 1% and improve query performance by up to 1%.
1.	introduction
as random access memory gets cheaper and new 1-bit cpus join the pc family  it becomes affordable and feasible to build computers with large main memories. more and more query processing work can be done in main memory. on the other hand  recent advances in the speed of commod-

 this research was supported by nsf grants iis-1  and eia-1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  paris  france.
copyright 1 acm 1-1/1 . . .$1.
ity cpus have far outpaced advances in memory latency. main memory access is becoming a significant - if not the major - cost component of database operations  1  1 . recent database research has demonstrated that the dominant memory stalls are due to the data cache misses on the second-level cache and the instruction cache misses on the first-level instruction cache  1  1  1 .
a flurry of recent research has focused on reducing the data cache misses in database systems  such as designing new cache-conscious algorithms  1  1  1   new cache-conscious index structures  1  1  1  1  1  and new data storage models . however  relatively little research has been done on improving the instruction cache performance.
as demonstrated in  1  1  1  1   database workloads exhibit instruction footprints that are much larger than the first-level instruction cache. a conventional demand-pull query execution engine generates a long pipeline of execution operators. in this model  an operator returns control to its parent operator immediately after generating one tuple. the instructions for this operator get evicted from the cache to empty cache memory space for the parent operator. however  the evicted instructions are required when generating the next tuple  and have to be loaded into the cache again. the resulting instruction cache thrashing may reduce the overall query performance significantly.
in this paper  we focus on improving the instruction cache performance for conventional demand-pull database query engines. we aim to achieve fast query execution throughput. at the same time  we do not want to make substantial modifications to the existing database implementations.
given a query plan we add a special  buffer  operator in certain places between a parent operator and a child operator. during query execution  each buffer operator stores a
large array of pointers to intermediate tuples generated by the child operator. rather than returning control to the parent operator after one tuple  the buffer operator fills its array with intermediate results by repeatedly calling the child operator. control returns to the parent operator once the buffer's array is filled  or when there are no more tuples . when the parent operator requests additional tuples  they are returned from the buffer's array without executing any code from the child operator.
the net effect of this approach is that parent and child operpcpcpcpcpcpcpcpcpcpcp. .. pcccccpppppcccccppppp. ..
	 a  original	 b  buffered
figure 1: operator execution sequence
ators are no longer interleaved. the child operator is called repeatedly  until the buffer is full  then the parent operator is called repeatedly  until the buffer is empty. this change is illustrated in figure 1 with a buffer size of 1 tuples. a p denotes an execution of the parent operator code  while a c denotes an execution of the child operator code.
the interleaving of the parent and child operators causes instruction-cache interference  and instruction cache thrashing when the combined size of the two operators exceeds the size of the smallest  fastest cache unit. by changing the execution pattern to that of figure 1 b   we can ensure that at least four out of every five executions of an operator find the operator's code cache resident  assuming that the operator fits within the smallest  fastest cache unit. in the event that multiple consecutive operators in the pipeline can fit in the cache  we buffer once above the group of operators  rather than above each operator in the group.
while there is extra handling for the new buffer operators  we expect to benefit from improved instruction spatial and temporal locality  and thus to incur a smaller number of instruction cache misses. we demonstrate experimentally that in some situations there is a net benefit to buffering.
buffering query execution is not always beneficial. the decision is based on the interaction between consecutive operators and depends on the properties of related operators  such as cardinality estimates from the optimizer  etc. . we introduce a plan refinement algorithm that infers the instruction footprints and decides whether query execution needs to be buffered and where to buffer. we study various factors that affect cache performance and provide architecture-sensitive guidelines for choosing buffering parameters.
we built an experimental prototype on top of the postgresql database system . postgresql implements a large variety of database operations. during execution  the executor processes a tree of  plan nodes . the plan tree is essentially a demand-driven pipeline of tuple processing operations. each node  when called  will produce the next tuple in its output sequence  or null if no more tuples are available. if the node is not a primitive relation-scanning node  it will have child node s  that it calls in turn to obtain input tuples.
our results show that conventional demand-pull pipelined query execution has a high instruction cache miss rate. by simply adding additional buffer operators  we can reduce the number of instruction cache misses by up to 1%. as a side effect  buffering also improves the accuracy of hardware branch prediction and decreases the number of branch mispredictions. overall  our techniques improve the query performance by up to 1% over a memory-resident database.
the rest of this paper is organized as follows. we discuss related work in section 1. we briefly discuss hierarchical memory systems in section 1. we describe the conventional demand-driven pipelined query execution engine and demonstrate why it is suboptimal for instruction cache performance in section 1. in section 1  we present a new buffer operator and the detailed data structures. we derive guidelines for when and where to place buffer operators in section 1. in section 1  we present detailed experiments and validate our algorithms. we conclude in section 1.
1.	related work
compilers may contain schemes to improve instruction cache performance. a code layout algorithm is proposed in   which uses profile guided feedback information to contiguously layout the sequence of basic blocks that lie on the most commonly occurring control flow path. gloy et al. extend the algorithm to consider the temporal relationship between procedures in addition to the target cache information and the size of each procedure . a cache line coloring algorithm inspired by the register coloring technique is proposed in  to reduce the number of conflict misses in the instruction cache. similar code layout techniques are used for both decision support and oltp workloads in  1  1  to improve instruction fetch bandwidth.
several techniques for non-sequential instruction prefetching are proposed in  1  1  1 . prefetch instructions are inserted by the compiler to prefetch branch targets. previous work  presented a hardware mechanism to implement call graph prefetching. this scheme analyzes the call graph of a database system and prefetches instructions from the function that is deemed likely to be called next.
