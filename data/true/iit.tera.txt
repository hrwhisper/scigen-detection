for trec-1  we participated in the terabyte track. we focused on partitioning the data in the gov1 collection across a homogeneous cluster of machines and indexing and querying the collection in a distributed fashion using different standard retrieval models on a single system  such as the robertson bm1 probabilistic measure and a vector space measure. our partitioned indices were each independent of each other  with independent collection statistics and lexicons. we combined the results as if all indices were the same  however  not weighing any one result set more or less than another.
1. introduction
　for the trec 1 terabyte track  we focused on indexing and querying the data with our existing aire engine . we partitioned the 1gb gov1 data across fourteen machines and built independent indices for each segment. we then analyzed the statistics for each of these independent indices to make sure that the collections were roughly equal in size and characteristics. after satisfying ourselves that this was the case  we issued each query on each system and combined the results to get the required number of documents for each query.
1. experimental setup
1 indexing
　the hardware for our tests consisted of fourteen machines networked on a dedicated switch. each machine was equipped with 1 1 rpm 1 gb scsi drives  1 amd k1 athlon mp 1+ processors  1gb of ram  and redhat linux advanced server 1. the indexing of the system was done using the bea jrockit jvm  while the querying was done using the sun jdk 1.1. the system for this set of experiments was very much like last year's  with no changes introduced in order to be able to index or query the large amount of data needed for the terabyte track.
　the indexing engine was set up in as simple a way as possible to ensure maximum indexing speed and that it would run to completion. no stemming was done  nor was any link information or entity extraction mechanism used. phrasing was used  with a limit of two words per phrase and a minimum of 1 occurrences of a phrase to be entered in the lexicon.
　the data was partitioned out across each machine roughly equally  and left compressed in gzip format. no directory in the collection was split out across two separate machines  however. documents were chosen for each machine by picking directories of the gov1 collection sequentially as listed by ls and trying to fit them together so that each machine had roughly 1gb of data.
1 querying
　the query engine was set up to do no stemming nor use any link information. we ran two query runs: one with the iit vector-space similarity function   and one with the robertson bm1 probabilistic similarity function . only the query title was included in the query.
　since all queries were issued to each machine  we needed a way to combine the the independent results to come up with one master result set. since the sets of collection statistics were similar  we decided to treat each result set equally for each query and combine them by simply concatenating the results and sorting them based on non-normalized relevance score  as in the section on  perfect merge  in . not normalizing the score was important  since normalizing would have caused the top documents from each index to be treated equally by virtue of having the highest score for the index rather than the highest score overall.
1. results
1 indexing
　indexing the entire gov1 collection took 1 hours from the first machine to start until the last machine to end. each index was approximately 1gb in total. the lexicon took up an average of 1 mb  while the posting lists took an average of 1gb. the document data averaged 1 mb. each lexicon had between 1 and 1 million words  between 1 and 1 million phrases. standard deviation was 1% of the mean number of words for all lexica on all machines  and 1% for phrases. for document count and posting entry count  the deviation was a mere 1% of the mean number of documents. the average number of distinct terms per document in the collection  see table 1  was similar across the cluster  with a meager standard deviation of 1 versus a mean of 1 distinct terms per document. similarly  the average total number of terms per document varies only slightly across collections.
table 1: mean collection statistics across all indices
num docsnum entsmax nidf min nidf wordsphrasesmax1111min1111mean1111max-min1111std dev111table 1: mean term statistics across all indices
avg dist termsmin dist termsmax dist termsavg termsmin termsmax termsmax1111min1111mean11.1.11max-min1111std dev111the only statistics that really vary widely are the maximum terms per document and maximum number of distinct terms per document. most indexes have in the 1-1 range for maximum non-distinct terms  with two machines having a maximum of 1 1. the global maximum for number of non-distinct terms in a document was 1 terms. considering that the average number of terms in a document was only 1 terms  this would suggest that long documents were spread across the cluster fairly evenly.
1 querying
　querying the whole collection took one hour from the first machine to start until the last machine to end. query performance was hampered by the fact that a configuration error caused the entire lexicon to be loaded into memory. this caused our memory footprint for the query engine to jump to around 1gb - almost the full extent of ram. since this meant that garbage collection had to be done for almost every posting list  each query took between 1 seconds and 1 minutes to perform  when retrieving the top 1 documents. subsequent experiments with the lexicon in the proper format confirm that when our system is configured optimally  the memory footprint and query timings are much more reasonable  1mb heap size and around 1 seconds per query. however this mistake confirms that it is  if not practical  possible to load the entire lexicon into memory at once for such a large collection.
　most query result sets on the individual machines provided the maximum number of scored documents each  1. after the query engines on each machine finished  we copied the results to a single machine and used the unix sort utility to combine them  and then a short perl script to delete any results beyond the 1 highest ranked for each query. this process took 1 minutes for the entire result set. the simple vector-space and probability rankings with no relevance feedback returned enough results that only one query had few  less than 1  results.
1. conclusions
　in our experimental runs for this year's terabyte track  we distributed the collection across a homogenous cluster and treated each node in the cluster has having an independent index. we then ran probabilistic and vector space models and merged the independent results on each node as if they were one result set in order to create two master result sets: one for the probabilistic model and one for the vector space model. in the future  we will experiment with creating a consolidated collection statistics structure and lexicon with the intent of providing the same relevance rankings for any document regardless of which sub-index it is found in. we will also experiment with different document distributions to see if term statistics change noticeably for a round-robin or random distribution of documents across our cluster and if this changes our effectiveness score. we will also work to make sure that relevance feedback is available in our next year's track.
1. acknowledgements
　special thanks to tristan sloughter  iit information retrieval lab  for his work in distributing data and running tests tirelessly.
