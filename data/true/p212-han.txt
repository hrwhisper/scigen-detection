this paper proposes a probabilistic model for definitional question answering  qa  that reflects the characteristics of the definitional question. the intention of the definitional question is to request the definition about the question target. therefore  an answer for the definitional question should contain the content relevant to the topic of the target  and have a representation form of the definition style. modeling the problem of definitional qa from both the topic and definition viewpoints  the proposed probabilistic model converts the task of answering the definitional questions into that of estimating the three language models: topic language model  definition language model  and general language model. the proposed model systematically combines several evidences in a probabilistic framework. experimental results show that a definitional qa system based on the proposed probabilistic model is comparable to state-of-theart systems.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval
general terms
algorithms  experimentation
keywords
definitional question answering  probabilistic model  language model
1.	introduction
　definitional question answering  qa  is a task of answering definitional questions used for finding out conceptual facts or essential events about the question target. the definitional questions take the form of  what is x   or  who is x    such as  what is fractals   or  who is andrew
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
carnegie    and x is called the question target. there are several characteristics of the definitional qa that are different from those of the factoid qa handling a question such as what country is aswan high dam located in . definitional questions do not clearly imply an expected answer type but contain only the question target  e.g.  fractals and andrew carnegie in the above example questions   contrary to the factoid questions involving a narrow answer type  e.g.  country name for the above example question . thus  it is difficult to find which information is useful for the answer to a definitional question. another difference is that a short passage cannot answer the definitional questions because a definition needs several essential information about the target. therefore  the answer of definitional questions can consist of several component information called information nuggets. each answer nugget is naturally represented by a short noun phrase or a verb phrase. good definitional qa systems have to find out more answer nuggets with shorter length.
　related works have been actively carried out  mainly oriented by trec question answering track . most of the approaches rank answer candidates by combining various information heuristically without any formal model for definitional qa. the heuristic approaches are widely used for easy implementation. these approaches use external resources  such as a dictionary or an encyclopedia  and definition patterns for ranking answer candidates  1  1  1  1 . the definition patterns are also used for extracting shorter phrases than sentences  1  1  1 . however  since the heuristic approaches cannot explain the role of each information in a conceptual model  it is difficult to analyze why the system performance improves or deteriorates. therefore  it is not easy to make the systems based on the heuristic approaches perform better.
　prager et al.  proposed a definitional qa method which splits a definitional question into several auxiliary factoid questions and combines the factoid answers into a definitional answer. the auxiliary questions are generated by using patterns constructed manually according to the type  e.g.  person  of question target. however  it is difficult to set the auxiliary questions in advance  because the essential facts or events can be different between the question targets even if the targets have the same type.
　berger et al.  and soricut & brill  applied translation model to qa  considering the question and answer as a target and a source language  respectively. they collected the bulk of question-answer pairs like faq  frequently asked question  list  and trained the probability that each word in the answer is translated to each word in the question. the translation model is a formal model for qa  but it is difficult to use the model for definitional qa. the model requires a large amount of training corpus and complex word alignment between the question and the answer. the word translation probability does not seem to be well estimated for definitional qa  because the definition question is very short  i.e.  even the only question target  and the answer is very long. moreover  it is hard to use other information than the translation probability in the translation model.
　we propose a formal model for definitional qa  considering the characteristics of the definitional questions. a definitional question can be interpreted as the request for the definition about the question target. therefore  the answer for the question should be a definition-style representation form related to the question target. we model the definitional qa from the two points of view  topic and definition. the proposed model can naturally incorporate various information during the topic and definition modeling.
　we explain our intuition and the proposed model in detail in section 1 and describe a definitional qa system based on the proposed model in section 1. experimental results and discussions are presented in section 1. finally  we conclude our work in section 1.
1. probabilistic model for definitional question answering
1	topic and definition
　a definitional question such as  what is x   or  who is x   is a representation for user information need  find the definition about x . the definition1 about x consist of conceptual facts or principal events that are worth being registered in a dictionary or an encyclopedia for explaining x. provided that d is a set of definitions about all possible targets  the definition about x is a subset of d containing the related to x. since the topic of a question target is represented by facts or event related to the target  the definition about x can be considered as the definition that represents the topic of x.
　we can suppose that the following sentences are answer candidates for a question  what is nasa  .
s1: nasa is the agency responsible for the public space program of the usa.
s1: nasa was established in 1.
s1: the headquarters of nasa is located in washington  d.c.
s1: nasa announced the new annual budget.
s1: john who works for nasa gave a housewarming party yesterday.
s1: ji-sung park is a famous football player from south korea.
among the candidates  the sentences representing the topic of  nasa  are {s1 s1 s1 s1}  and those representing the definition are {s1 s1 s1 s1}. although the sentence s1
1
 the definition is interpreted in a broad sense according to the trec 1 qa track guideline.
contains  nasa   it is about john's housewarming party  and it is of no use as a definition about  nasa . the sentence s1 represents the topic of the target  but it cannot be considered as a definition sentence. on the other hand  the sentence s1 is a definition sentence  but it does not represent the topic of the target. therefore  it is reasonable to think of {s1 s1 s1} as the answer to the question. the example shows that it is necessary for a definitional qa system to see if each answer candidate represents both the topic related to the question target and the definition.
　given that t is a set of answer candidates describing the topic related to the question target and d is a set of answer candidates representing the definition  the answer a to a definitional question about the target is the intersection of
t and d.
	a = {a | a （ t  a （ d}	 1 
we design a probabilistic model for definitional qa from this point of view.
1	definitional question answering model
　given a definitional question q about the target x and a sentence s1  we have the following events:
  t: the sentence s includes the contents related to the topic of the target x.

  t: the sentence s includes no content related to the topic of the target x.
  d: the sentence s represents the definition.

  d: the sentence s does not represent the definition.
　as mentioned above  the answer to the definitional questions consists of the sentences which do not only describe the contents related to the topic of the target x  but also represent the definition. therefore  definitional qa can be considered as a task of finding the sentences which maximize a joint probability p t d|s  where p t d|s  is the probability that the sentence s does not only include the contents related to the topic of the target x  but also represent the definition. the probability is rewritten by using the chain rule as follows:
           p t d|s  = p t|s p d|t s   1  where p t|s  is the probability that the sentence s includes the contents related to the topic of the target  and p d|t s  is the probability that the sentence s including the contents related to the topic of the target represents the definition.
　we assume that we can identify whether a sentence represents the definition or not  regardless of whether the sentence is related to the target topic or not. for example  we can understand that a sentence  copland was born in brooklyn.  represents the definition without regard to the topic that the sentence describes. even if the target is not known in the sentence  i.e.   x was born in brooklyn.    the sentence can be considered as a definition sentence. therefore  assuming t and d are conditionally independent given s  we can simplify the probability p t d|s  as follows:


1
 we use a sentence for convenience  although any text segment can be valid.
equation 1 is expanded into equation 1 by using bayes' theorem.
　in order to answer a definitional question  it is necessary to select the sentences that deserve to be the answer by calculating the probability p t d|s . the task of classifying whether each sentence deserves to be the answer or not according to the probability p t d|s  can be regarded as the task of ranking the sentences according to the probability. the top ranked sentences are selected for the answer. it is not necessary to calculate the exact probability in order to rank the sentences. the equation can be simplified by order-preserving transformation instead. a scoring function used for ranking the sentences is defined as follows:
		 1 
as p t  and p d  in equation 1 do not have an effect on the rank under the condition that a score is assigned to each sentence for a given question  the equation can be transformed into equation 1. p s|t  is the probability that the sentence s is generated from the target topic  and p s|d  is the probability that s is generated from a set of definition representations. p s  is the prior probability of the sentence s.
　if the sentence s is a sequence of words w1 ，，，wn  the function is rewritten as follows:
	ds s 	=	p w1 n|t  〜 p w1 n|d  〜 p w1 n  1  1 
p w1 n|t  is the probability that the word sequence w1 n is generated from the target topic  and we call it topic language model. p w1 n|d  is the probability that w1 n is generated from the definition representations  and we call it definition language model. p w1 n  is the prior probability of w1 n  and we call it general language model. a score is assigned to each sentence by combining the probability of the three language models. different formulations of the model for definitional qa are possible according to the way each probability is estimated.
1	estimating language models
1.1	general language model
　the probability p w1 n  is rewritten by using the chain rule as follows:
p w1 n  = p w1 p w1|w1 p w1|w1 ，，，p wn|w1 n 1 
assuming the word occurrences are independent of one another  the probability is calculated by the following equation:
n
	puni w1 n  =	p wi 
i=1
the prior probability p wi  of a word wi is estimated by mle  maximum likelihood estimation  based on the whole collection where the answer is searched:

where the c wi  is the occurrence count of word wi in the whole collection. the probability of the general language model is also used for the smoothing of probabilities in other
language models.
1.1	topic language model
　the probability p w1 n|t  of the topic language model is rewritten by using the chain rule and the independence assumption of word occurrence as follows:
n
	puni w1 n|t  =	p wi|t 
i=1
if we had a set of ideal sentences describing the topic t about the question target x  we could model the target topic from the sentences. alternatively  we model the target topic using the following several evidences:
  top ranked documents r retrieved from the collection by the query x: pseudo relevance feedback techniques in information retrieval research consider top ranked documents as relevant ones and retrieve more relevant documents by modifying the original query based on them. this idea can be applied to topic modeling. top ranked documents retrieved by the query consisting of the question target are considered as the documents describing the target topic  and they are used for modeling the topic.
  definitions e for the target x from external resources: there may be definitions for the target x in the external resources such as online dictionary. the definitions for the target are very useful information for topic modeling  if there is any  because the definitions include only the content about the target x without any noise. however  since the definitions cannot cover all possible question targets  it is necessary to use other evidences for target modeling. the definitions from external resources are called external definitions from now on.
  top ranked web pages w retrieved from the www by the query x: the web pages are used to alleviate the sparseness of r and e. considered as the pages that describe the target topic  the top ranked web pages retrieved by the target are used for target modeling.
　the topic language model is estimated by linear interpolation of the above evidences as follows:

where p wi|r   p wi|e   and p wi|w  are the probability that word wi is generated from the top ranked documents r  external definitions e  and web pages w  respectively. the α  β  and γ are interpolation parameters  and they are empirically set satisfying α + β + γ = 1.
　each probability is estimated by dirichlet smoothing  which is known to be useful in information retrieval based on language model .
p wi|r 	=	cr wi  + μp wi  j cr wj  + μ
c
p wi|e 	=	e wi  + μp wi  j ce wj  + μ
	c	  + μp w
	p wi|w 	=	w wi	i 
j cw wj  + μ
where cr wi   ce wi   and cw wi  are the occurrence counts of wi in r  e  and w  respectively. p wi  is the prior probability of wi calculated in the general language model  and μ is a parameter for dirichlet smoothing. according to zhai & lafferty   the value of μ for a high performance in information retrieval is between 1 and 1  and the performance is robust with 1 of μ.
1.1	definition language model
　the probability p w1 n|d  of definition language model is rewritten by using the chain rule and independence assumption of word occurrence as follows:
n
	puni w1 n|d  =	p wi|d 
i=1
the definition corpus is necessary for definition modeling  and the definitions can be collected from the online dictionary or encyclopedia. we constructed the definition corpus by collecting the definitions for arbitrary definition targets from the online resources  and estimated the probability using the definition corpus.
　the word probability distribution can be different depending on the domain of the definition target. for example   president    scientist    born   and  died  will frequently occur in the definition for a person. on the other hand   established    member    headquarters   and  branch  will do in the definition for an organization. moreover  there may be some words that reflect the definition itself without regard to the definition domain. therefore  the definition language model is estimated by the linear interpolation of the definitions for the domain of question target and the definitions for all domains.

where p wi|dtx  is the probability that word wi is generated from the definition corpus whose domain for definition target is equal to the domain tx for question target  and p wi|dall  is the probability that wi is generated from the definition corpus for all domains. λ is a interpolation parameter whose value is empirically determined. we used three domains in this paper: person  organization  and term.1
　each probability is estimated by dirichlet smoothing as follows:
i| txj	dtx  wj  + μ c
cdall wi  + μp wi i| allj	dall wj  + μ ccdtx  wi  + μp wi 
	p w d	 	=	
	p w d	 	=	
where cdtx  wi  is the occurrence count of wi in the definition corpus whose domain is tx  and cdall wi  is that in the whole definition corpus. p wi  is the prior probability of wi calculated in the general language model  and μ is a parameter for dirichlet smoothing.
1	discussion
　our proposed probabilistic model handles the definitional qa in terms of topic and definition. as the definitional qa is separated into two points for finding the answer  topic

1
 targets in trec 1 were classified into the three domains.

figure 1: architecture of definitional question answering system
relevance and definition representation  the model has an advantage in the aspect of easy estimation of the probabilities.
　moreover  the model can be easily extended to other descriptive qa. if the question is  why ，，，     the qa for the question can be modeled in terms of topic and reason. given a binary random variable h which is 1 if a sentence represents the reason but 1 otherwise  the answer could be selected from the top ranked sentences according to the joint probability p t h|s .
　the proposed model transforms the definitional qa into the estimation tasks for the three language models: topic language model  definition language model  and general language model. the model is promising in the sense that the various techniques for the language modeling studied in speech recognition and natural language processing can be applied to it. as shown in equation 1  the general language model is used for normalizing the probability value. as a consequence  the proposed model prefers the sentences which are more likely to occur in the target topic and the definition than in the general text. during the probability estimation of each language model  the model is naturally served as a framework for combining various evidences sys-
tematically.
1.	definitional question answering system based on the probabilistic model
　in this section  we explain the definitional qa system based on the proposed probabilistic model described in the previous section. an overview of the system is shown in figure 1.
1	question analysis
　given a definitional question  the question target is extracted and its type is identified. since a definitional question is simple  it is easy to extract the question target. from a question such as  who is andrew carnegie    for example   andrew carnegie  is extracted by using simple rules. moreover  the type of the target is identified by a named entity tagger  bbn identifinder   and it is used for estimating the probabilities of definition language model. we classify the target into three types: person  organization  and term. if a target is not classified into person or organization by the named entity tagger  it is classified into term.
1	document retrieval
　since calculating the probabilities for all the documents costs too much  relevant documents to the question target are retrieved  and answer candidates are extracted from the retrieved documents. the document retrieval is carried out by using bm1 scoring function of okapi . the query consists of each word of the question target.
1	answer candidate extraction
　all sentences in the retrieved passages are usually used as answer candidates. however  a sentence may be too long that it is likely to contain information which is not related to the question target. thus  we try to extract target-related parts of sentences using syntactic structure of the sentences. if the parts are extracted  they are used as answer candidates. otherwise  the sentences are used as the candidates. we extract noun and verb phrases from the sentences using the syntactic definition patterns .
1	answer candidate ranking
　the answer candidates are ranked by using the proposed probabilistic model mentioned in the previous section. in order to keep the probability for each candidate from being too small and calculate it efficiently  we take the logarithm as follows:

1	answer selection
　the answer is selected from the ranked candidates  in order. the length of the final answer a is controlled by the score threshold δsel as follows:
	a = {aj|lds aj    δsel}	 1 
if the target length of the answer is set  the answer is truncated to the length.
　the redundant candidates are skipped during the answer selection process. the redundancy between two candidates ai and aj is calculated by the word overlap as follows:

where |ai … aj| is the number of common content words in the candidates  and |ai| is the number of content words in the candidate ai. the word overlap between an answer candidate and the selected answer is calculated as follows:

1who is the lead singer/musician in nirvana 1who are the band members 1when was the band formed 1what is their biggest hit 1what are their albums 1what style of music do they play 1otherfigure 1: question series for  the band nirvana 
if the word overlap exceeds the upper threshold  i.e.  1   the candidate is considered to be redundant. if the word overlap lies between the upper threshold and the lower threshold  i.e.  1   the semantic class of the candidate is checked. if there is any selected candidate that shares the synset in wordnet  the candidate is regarded as redundant one.
1.	experimental results
1	experiments setup
　we have experimented with 1 trec 1 topics and 1 trec 1 topics  and we found the answer from the aquaint corpus used for trec question answering track evaluation. the trec answer set for the definitional qa task consists of several definition nuggets for each target  and each nugget is a short string.
　in trec 1  as shown in figure 1  a topic consists of several factoid/list questions and one definitional question1 given at the end. the questions are associated with a question target and assumed to be processed in order. thus  the gold standard answer for definitional questions about a target does not include the answers for preceding factoid/list questions about the same target. however  the answer for those questions can be considered as the answer for definitional questions in order to evaluate definitional qa systems only. therefore  we expanded the trec 1 gold standard answer by adding the answer nuggets for factoid questions of each topic and used the expanded answer to evaluate for trec 1 topics.  when we compare our systems with other trec participant systems  we used the original gold standard answer.  we skimmed through the documents containing the answer string for each factoid question and composed a short phrase manually. for example  a nugget  formed in 1  is added to the answer for the definitional question about  the band nirvana   extracted from the document containing answer string  1  for factoid question  when was the band formed  .
　the evaluation of systems involves matching up the answer nuggets and the system output. because the manual evaluation such as trec evaluation requires a lot of cost  we evaluated our system using the automatic measure pourpre . the pourpre estimates the trec metric  nugget recall  precision  and f-measure  using term cooccurrences between answer nugget and system output. the pourpre better correlates with the trec official scores than another automatic measure rouge  does. there is no rank swap between official scores and pourpre scores except swaps attributed to error inherent in the evaluation

1
 definitional questions are called other questions in trec 1
table 1: performance according to topic modeling: trec 1
αβγrecprecf β = 1 11.1.1.1111111.1.1.11111111.1.1.1.1.1.1111111111111111111111111111111111table 1: performance according to topic modeling: trec 1
αβγrecprecf β = 1 11.1.1.1111111.1.1.11111111.1.1.1.1.1.1111111111111111111111111111111111process . f-measure is used for the official measure in trec evaluation  and β is set to three favoring the recall.
　for topic modeling  we collected external definitions from various online sites in query time: acronym finder  biography.com  columbia encyclopedia  wikipedia  foldoc  the american heritage dictionary of the english language  online medical dictionary  and google glossary. we also used ten web pages retrieved by the google search engine and five local documents retrieved from the aquaint corpus for topic modeling. for definition modeling  we constructed definition corpus from the above sites according to the target type: 1 persons  1 organizations  and 1 terms entries. we processed top 1 documents retrieved in all experiments.
1	topic modeling
　the topic modeling experiments are carried out to find out which information is useful for estimating the topic language model. table 1 and table 1 show the performance of our definitional qa system according to the various parameter settings for trec 1 and trec 1 questions  respectively. the α  β  γ are weight of top ranked documents r  external definitions e  and top ranked web pages w  respectively  in equation 1 for the topic modeling. λ and μ are set to 1 and 1  respectively. in order to evaluate the ranking performance  the score threshold δsel is not applied  and the target length is set to 1 byte1.

1
 the target length is measured by the number of non-whitespace characters. the answer can be shorter than the target length because of the insufficient candidates.
table 1: performance according to definition modeling: trec 1
λrecprecf β = 1 11111111111111111111111111111111111111111111table 1: performance according to definition modeling: trec 1
λrecprecf β = 1 11111111111111111111111111111111111111111111　as shown in table 1  the system using only the external definition outperforms the other systems. it is probably because there are external definitions for 1%  1  questions of trec 1 questions. on the other hand  there are external definitions for 1%  1  questions of trec 1 questions  and the system combining each information  as shown in table 1  outperforms the others. the experimental results for the topic modeling can be summarized as follows:
  the target topic is best represented by the external definitions. as the external definitions provide core information about the question target without noise  they contribute to the increase of the system performance in terms of recall and precision.
  top ranked documents and web pages complement insufficient coverage of the external definitions.   the confidence of each information is determined according to the degree of its noise. the experimental results show that the external definitions are the most confident  followed by the top ranked documents and web pages  in order. since the external definitions have almost no noise and the news articles are generally less noisy than web pages  it is conjectured that less noisy source supplies more confident information for the topic modeling.
1	definition modeling
　the definition modeling experiments are conducted to know how much the question target type affects the estitable 1: comparison with top 1 trec participant systems: trec 1
systemrecprecf β = 1 111111111top 1 systems111111at trec 1.1.1.1111111111111proposed 1 111proposed 1 111proposed 1 111mation of the definition language model. table 1 and table 1 show the performance according to the various parameter settings for trec 1 and trec 1 questions  respectively. λ is the degree to which the target type has an effect on the definition modeling. for a question such as  who is andrew carnegie    λ is a parameter for the definition corpus of person type. as shown in the tables  the systems reflecting the target type outperform the system which uses the definition corpus as a whole  not considering the target type  λ = 1 . the system considering the target type heavily  λ = 1 for trec 1 and λ = 1 for trec 1  performs best.
　in order to analyze the reason why the question target type have a such great effect on the system performance  we compared the term probability distribution of each definition corpus using jensen-shannon divergence  js divergence  . the value of js divergence lies between zero and two  and closer value to zero means that two input distributions are more similar to each other. table 1 shows the js divergences between various distributions where aq is aquaint corpus as a whole  and aq nyt and aq apw are nyt and ap part of the aquaint corpus  respectively. d all is the definition corpus for all domains  and d person  d org  and d term are person  organization  and term part of the definition corpus  respectively. the table shows that the term distribution of news text is very different from that of definition text. the divergence  1  between aq nyt and aq apw is small  but the divergence  1  between aq nyt and d all is much larger. on the other hand  the divergence between definition corpora is very large according to the definition domain  although they equivalently consist of definition text. for example  the divergence between d org and d person is 1  whereas the divergence between d org and aq apw is 1. the result implies that the term distribution of the definition corpus is very different between the target domains. the large difference in the term distribution explains the reason why the system heavily considering the definition type performs so well.
1	comparison with other systems
　we compared our proposed system with the previous trec participant systems  1  1 . for this experiments  the raw table 1: comparison with top 1 trec participant systems: trec 1
systemrecprecf β = 1 111111111top 1 systems111111at trec 1.1.1.1111111111111proposed 1 111proposed 1 111proposed 1 111run files of each system are offered by nist1. table 1 and table 1 show the pourpre evaluation result of top ten systems and our proposed systems proposed ，  ，  of which target length and score threshold is variously set. for example  proposed 1  is the system whose target length and score threshold is set to 1 byte and 1  respectively. because the responses of the trec participant systems are generated under the condition that the answers for definitional questions do not have to include the answer for other preceding questions  we evaluated the systems with the original trec 1 gold standard answer. our systems may be slightly underestimated for trec 1 questions because ours do not consider other types of questions. the tables show that our system is comparable to the state-of-the-art definitional qa systems.
1.	conclusions
　we proposed a probabilistic model for definitional qa  analyzing the problem into two main components  topic and definition. with the separation of the topic model and the definition model  the definition qa system can estimate each model effectively. the experimental results show that the external definition which has almost no noise is the most valuable information for topic modeling  and that the top ranked documents and web pages complement insufficient coverage of the external definition. the definition corpus is used for estimating the term probability of the definition language model. because of the large difference in term distribution between the definition domains  it is reasonable to estimate the probability by using the dynamically selected definition corpus based on the question target type.
　the proposed model can be easily extended to other descriptive qa by replacing the definition model with a new model customized to it. moreover  as the task of qa is transformed into that of estimating three language models  various techniques related to the language modeling can be applied to the model.
　for the future work  we will estimate the probabilities of the language models using more contexts. furthermore  we will extend our model to other question types and combine the models for each question type into a general model for descriptive qa.

1 http://trec.nist.gov/
table 1: jensen-shannon divergences between term probability distributions
