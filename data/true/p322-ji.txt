this paper presents a novel domain-independent text segmentation method  which identifies the boundaries of topic changes in long text documents and/or text streams. the method consists of three components: as a preprocessing step  we eliminate the document-dependent stop words as well as the generic stop words before the sentence similarity is computed. this step assists in the discrimination of the sentence semantic information. then the cohesion information of sentences in a document or a text stream is captured with a sentence-distance matrix with each entry corresponding to the similarity between a sentence pair. the distance matrix can be represented with a gray-scale image. thus  a text segmentation problem is converted into an image segmentation problem. we apply the anisotropic diffusion technique to the image representation of the distance matrix to enhance the semantic cohesion of sentence topical groups as well as sharpen topical boundaries. at last  the dynamic programming technique is adapted to find the optimal topical boundaries and provide a zoom-in and zoom-out mechanism for topics access by segmenting text in variable numbers of sentence topical groups. our approach involves no domain-specific training  and it can be applied to texts in a variety of domains. the experimental results show that our approach is effective in text segmentation and outperforms several state-of-the-art methods.
categories and subject descriptors
h.1  information systems : information search and retrieval; i.1  computing methodologies : natural language processing
general terms
algorithms  experimentation  theory
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
keywords
text segmentation  document-dependent stop words  anisotropic diffusion  dynamic programming
1.	introduction
　the task of text segmentation is to determine the boundaries between topics in long text documents or text streams and divide the texts into a set of text segments  each of which consists of a consecutive sequence of sentences or paragraphs sharing a coherent topic. several important applications in information retrieval and text mining make use of text segmentation as an important enabling technique  and we briefly list a few in the following: in traditional information access of text databases  we assume a notion of text documents  and then attempt to search and retrieve documents that satisfy users information needs. however  the increasing lengths of documents in full-text collection motivates smaller and more coherent passage level retrieval  which has been demonstrated to improve retrieval performance as well as reduce information overload in processing and transmission  1  1 . in addition  some text collections  such as newswire feeds and streams of automatic speech recognition transcripts  do not have an explicit notion of documents. these text streams usually contain many distinct topics with no explicitly marked boundaries between the topics. human can usually recognize the topical boundaries without much difficulty. however  with the increase in the sizes of text streams collection  it is infeasible to have human readers to manually mark the boundaries. thus  automatic text segmentation based on topics becomes a critical task for developing effective and efficient systems to access those collections of text streams. last but not least  the task of text segmentation is also utilized in some of the approaches for text summarization. since long text documents often discuss multiple topics  revealing the latent topical structures of textual documents enables summarizing subtopics of documents instead of the whole documents  1  1 . the ability of segmenting texts at various levels of granularity helps to generate summaries with different compression rates and coverage diversities  1  1 .
　text segmentation tasks in general fall into two categories: the first type is to identify the locations of topic changes for text streams. text streams  such as streams of transcripts from automatic speech recognition  newswire feeds  and television closed-caption transcripts  usually contain noticeable topic transitions. text segmentation for text streams targets those transition points in the text streams. the second type of text segmentation is to identify and isolate subtopics by splitting substantive long documents. the long documents usually address several closely related subtopics or several aspects of the same topic. because the topic transitions are much subtler than those in text streams  segmenting long documents  which is less well addressed in previous research work  tends to be a more challenging task than segmenting text streams.
　in this paper  we explore the idea of converting a text segmentation problem into an image segmentation problem. we employ the anisotropic diffusion technique  a technique that has been successfully used for deblurring noisy images in image processing   and develop a novel approach for both text streams with noticeable topic transitions and documents with subtle subtopic shifts. our approach  which is unsupervised and domain-independent  can be applied to texts in many application fields. instead of utilizing cue-words that indicate topic transition  we rely on lexical cohesion information of texts for topic boundaries detection which tends to alleviate the performance deterioration caused by possible errors in transcripts from speech recognition. the major contributions of this paper are as follows: we capture the sentence cohesion information of a document or a text stream with a square sentence dissimilarity matrix  each entry corresponding to the distance of a pair of sentences. the cohesion information can then be represented by a gray scale image  which enables converting the text segmentation problem to an image segmentation problem. we use the anisotropic diffusion method to simultaneously enhance the lexical cohesion of sentences within a topical group and sharpen the boundaries of the topic transitions. we also propose the concept of document-dependent stop words as an effective tool to reduce the correlation among the topical groups in long documents. lastly  the dynamic programming technique is adapted to segment texts in variable levels of topical granularity  which is much more flexible than those used in previous research.
　this paper is divided into five sections. we review previous work on various text segmentation methods in section 1. section 1 presents our proposed method  which contains the sentence similarity measure  distance matrix construction  document-dependent stop words computation  application of anisotropic diffusion method  and the customized dynamic programming technique. in section 1  we discuss details of our experiments. we conclude the paper in section 1 and discuss several possible future research topics.
1.	previous work
　there have been several research done on text segmentation especially those in connection with the topic detection and tracking  tdt  research  1  1 . most of previous work is targeted at topic tracking of broadcast speech data and newswire texts. neither their algorithms nor their experimental data sets deal with subtopic segmentation of substantive long documents. those existing approaches in general fall into two categories: supervised methods  1  1  1  1  1  1  1  and unsupervised methods  1  1  1  1  1 .
　those supervised learning approaches achieve good performance based on large amount of training data in some specific domains. however  the systems developed are not guaranteed to perform well when they are applied to a different text domain without adequate amount of training data.
on the contrary  the unsupervised methods tend to be more domain-independent. based on information used in segmentation  text segmentation methods can be put into two groups: some relying on lexical cohesion and others utilizing multi-source information. the essential assumption of lexical cohesion-based methods is that similar vocabulary tends to be in a part of a coherent topic segment. the lexical cohesion-based methods utilize word repetition  1  1  1   context vectors   and semantic similarity  1  1  1  to identify cohesion. multi-source based methods employ not only lexical cohesion but also indicators of topic transition in presentation format and regular cues . these cues can improve accuracy for topic segmentation tasks. however  usually there are high error rates in the transcripts from speech recognition and television closed-caption transcripts. these errors make it difficult for cue recognition  and thus deteriorate the performance of text segmentation algorithms based on cue recognition in real-world applications .
　dynamic programming technique  in particular  has been applied to text segmentation  1  1   where a length model is required in order to achieve a reasonable performance. however  this model is not applicable when the size and number of text segments vary significantly. we adapt the dynamic programming technique without requiring the text segmentation length model. the optimal number of segments and segmentation boundaries are automatically determined by our algorithm. in addition  users are able to set the number of segments for a document or a text stream  and then our method will generate the optimal segmentation with the number of segments defined by the user. by this means  users are able to access  sub topics of a text document at different granularity.
　several experimental studies have been reported for text segmentation. currently  the best performance is achieved by brants et.al.  and utiyama et.al.  based on the brown data sets. however  their experiments are limited to a set of artificially generated text documents concatenating texts with different contents or topics. the effectiveness of their algorithms in segmenting coherent documents for subtopics is unknown.
1.	methodology
　for our purpose  a text segment consists of a consecutive sequence of sentences. however  other text units such as paragraphs or text lines can also be used to substitute for sentences. the goal of text segmentation is to identify the boundaries of the text segments in a document or a text stream. first  sentences are tokenized into words  the words are stemmed  and generic stop words and documentdependent stop words  if necessary  are removed. after the preprocessing  sentences are represented by word-frequency vectors  based on which the pairwise distances are computed  and the sentence distance matrix are formed. in general  the distance matrix tends to capture the sentence cohesion information in a document or a text stream. the distance matrix is further processed by a technique - anisotropic diffusion - used in image processing to sharpen the segment boundaries. at last  we apply dynamic programming technique to the processed distance matrix to find the optimal segmentation boundaries.
1 sentence distance matrix a document with m sentences is modelled with a set of sentence vectors s = {s1 ... sm}  and each si corresponds to a sentence. the entries of the sentence vectors correspond to words occurring in the sentence and take values of the corresponding word frequencies. stop words and documentdependent stop words  which will be discussed in the next subsection  are removed before building the sentence vectors. the distance dij between the sentence pair si and sj is calculates as
 
where  ，  denotes the euclidean length of a vector. the sentence distance matrix of a document  d =  dij m〜m  is generated by computing all pairwise distances of the sentences in the document. obviously  the distance matrix is a symmetric square matrix because dij = dji. for example  the sentence distance matrix for a document with three subtopics is displayed as a gray-scale image in figure 1. in the image  sentences are arranged from left-to-right/top-todown in the same order as they are in the document. each pixel in the image corresponds to the distance between the corresponding pair of sentences. brighter pixel means larger distance between the pair of sentences. the pixels along the diagonal from top-left to bottom-right are black  which means a sentence has zero-distance from itself. the text segmentation task is to find a sequence of adjacent sub-matrices along the top-left to bottom-right diagonal satisfying certain criteria. each sub-matrix corresponds to a consecutive sequence of sentences in the original text.

figure 1: sentence distance matrix before document-dependent stop words removed
1	document-dependent stop words
　generic stop words consisting of function words such as conjunctions  propositions  pronouns  etc. are usually removed when constructing the sentence vectors. for text segmentation of a long documents  there is another type of words which we call document-dependent stop words that should also be removed. those are the kind of words that are useful in discriminating among several different documents but are rather harmful in detecting subtopics in a document. as an example  consider segmenting a long article about heart diseases which might include several subtopics such as anatomy and physiology  clinical trials  diagnosis and symptoms  and rehabilitation and treatment. the frequency of the word heart is certainly very high in this document  and it is a very useful word for identifying the document as one about heart diseases. however  the word heart also tends to appear in each of the subtopics with high frequencies and in general will not serve well to distinguish the subtopic boundaries. removing the word heart can break the correlation among the different subtopics and hence stress their differences. one thing we need to be careful about identifying words as document-dependent stop words is that relative high frequency alone is not sufficient for labling a word as document-dependent stop word. the word are also required to be uniformly distributed in the whole document. for example  the word surgery can also appear at very high frequency in the document  but we can not characterize it as a document-dependent stop word because it almost exclusively occurs in the subtopic about heart disease treatment. based on the above discussion  we propose the following steps to select document-dependent stop words.
1. count all words' frequencies in a document  and select those words with top k% highest frequencies as candidate document-dependent stop words;
1. divide a document with m sentences evenly into i segments for i = 1 ... l. for a candidate documentdependent stop word w  count w's frequencies in each of the i segments as cij  1 ＋ j ＋ i i = 1 ... l;
1. define. compute the variance of the word's frequency for each segmentation as
1 ;
1. compute the average variance across the l segmentations asis greater than a certain threshold   then the candidate stop word is a document-dependent stop word.
the parametersneed to be chosen in order to apply the above selection procedure. ideally they should be chosen using methods such as cross-validation to optimize certain performance criterion for text segmentation. we empirically choose them as k = 1  l = m/1  and e = 1.
　after removing the generic stop words and documentdependent stop words  we apply the porter stemming algorithm  to the remaining words to obtain word stems  and each sentence vectors are indexed by the resulted word stems. figure 1 illustrates the sentence distance matrix after the two types of stop words are deleted. there are fewer dark-square regions outside of the main diagonal area than those in the figure 1  which makes the dark-square regions along the main diagonal of the distance matrix more outstanding. however  the boundaries are still relatively blurry. in the next section  we will discuss a method to enhance the contrast among different topical groups represented as square submatrices along the main diagonal of the distance matrix.
1	anisotropic diffusion
　a common assumption in text segmentation research is that lexical information of sentences tend to be consistent with their semantic information. thus  researchers may rely on the lexical information  such as word repetition  textual

figure 1: sentence distance matrix after documentdependent stop words removed
substitution  co-reference  and conjunction  to recognize the coherent topic segments. because of the abundance and variation of vocabulary and complexity of thoughts expression  fully extracting and utilizing semantic information of text is a very challenging task through lexical information. it has been demonstrated that lexical repetition alone can be used to determine the degree of cohesion between pair of sentences in some circumstance . however  people sometimes fail to describe a theme with correct vocabulary or cannot arrange lexical information into a coherent semantic substance for some reasons. thus  it is not unusual that lexical information does not effectively convey intended semantic information. when lexical information is captured with distance matrix  ideally  there should be a sequence of darksquare regions along the main diagonal of the distance matrix  each corresponding to a sentence topical group. however  because of reasons listed above  certain amounts of noise do appear in these dark-square regions  and their existence tend to blur the boundaries of the square regions  just as is shown in figure 1. instead of involving more complex natural language processing techniques for deeper linguistic analysis  we resort to applying a technique from image processing - anisotropic diffusion - to the sentence distance matrix to deblur the noise inside each dark-square region while at the same time to sharpen the boundaries of darksquare regions.
　anisotropic diffusion was proposed by perona and malik  as a powerful tool for image enhancement. its goal is to reduce noise in homogeneous regions of an image  making homogeneous regions even more homogeneous  while at the same time also sharpen boundaries between homogeneous regions. thus  the anisotropic diffusion keeps a strong flow within homogeneous regions  and little flow is allowed across region boundaries. in order to achieve this property  the diffusion coefficient function is chosen to be inversely proportional to the size of the gradient so that the diffusion flow increases within homogeneous regions where the gradient tends to be small. the following two diffusion coefficient functions have been proposed in :
	 	 1 
and
	 	 1 
where   denote the gradient operator. let ii jt denote the intensity at pixel  i j   its evolution is then governed by the following equations:
 
where 1 ＋ λ ＋ 1  n s e w are the mnemonic subscripts for north  south  east  west  and  indicates nearestneighbor differences. for each pixel in the image  we can calculate in each direction with:

at the same time the cij in four directions can be calculated with:
 
where function g can be either  1  or  1 . the first function favors high-contrast edges over low-contrast edges  while the second function favors wide regions over smaller ones.
　we apply the anisotropic diffusion technique to sentence distance matrices. each value of the distance matrix corresponds to a pixel of the image. text segments correspond to homogeneous regions in the distance matrix  and there are boundaries separating these homogeneous regions. we use the equation  1  in our experiments. with diffusion over time  the homogeneity within each region increases as small grey level variations are smoothed out. at the same time  the boundaries between regions become more pronounced. we also quantize the distance matrix d after the anisotropic diffusion for better boundaries display with:
1 if 1 ＋ dij ＋ 1
ij =
1 otherwise
　the gray scale image in figure 1 illustrates the distancematrix after the anisotropic diffusion process and quantization. now  we can recognize that there are three sentence topical groups in the sample documents  which we know in advance.
1	segmentation by dynamic programming
　within the context of the sentence distance matrix  text segmentation amounts to partition the matrix into k blocks of sub-matrix along the diagonal. specifically  we denote
diffusion. we aim to partitiond  =  d ij m〜m as the distance matrix after applying anisotropicd  into  d ij ki j=1. each d ij
is a square sub-matrix and corresponds to a sentence topical group including sentence si si+1 ... sj . there is a e p m k   associated with the text segmentation  d ij ki j=1 with
 

figure 1: sentence distance matrix after documentdependent stop words removed and anisotropic diffusion
where diam d ij  is the sum of all elements of d ij. we apply dynamic programming to find the segmentation  d ij ki j=1 with the minimum of e p m k    the computational steps are listed below :
1. compute the diameter diam si sj   for all pair of sentences si  sj such that 1 ＋ si   sj ＋ s	s
;
1. compute the errors of the optimal partitions 
	1 ＋ s	p	diam
1  +
1. for each l 1 ＋ l ＋ m  calculate the errors of the optimal partitions e p si l   l ＋ si ＋
1. in order to decide the optimal number of textsegments  calculate the decrease of e p m i   with gi = e p m i     e p m i + 1    1 ＋ i ＋ m. the
k that max gk/gk+1  is the optimal number of text segments.
1. the optimal partition p m k  is discovered from the table of errors e p i l   1 ＋ l ＋ k 1 ＋ i ＋ m  by first finding j so that e p m k   = e p j   1 k   1   + diam j m .
1. based on above calculation  we choose the  j j + 1 ... m  as the last segment. then find j that e p j  1 k  1   = e p j  1 k  1  +
is the second-to-the last segment  and so on.
　with a fixed k  dynamic programming applied to the objective function e p m k   can be proved to achieve a global minimum. the is mainly because e p m k   can be written as a summation. the criteria in step 1 also provides a way to select the number of segments k to use. the segments boundaries that correspond to the given number of segments for the text can be retrieved in the same way. even the number of segments varies  the corresponding segmentations are guaranteed to be the optimal at the given number of segments. this provides the function of zoom-in and zoom-out to access subtopics with different granularity.
range of n1111number of samples11table 1: the first data set statistics  choi  1 
source of textsect1 ch1sect1 ch1sect1 ch1# of words11# of paragraphs11table 1: the second data set statistics
1.	experiments
　we carried out several experiments with various data sets and parameter settings. this section will discuss some of our experimental results and compare with those done in previous research. we also study the gains in performance of the anisotropic diffusion technique and the use of documentdependent stop words. the experimental results are evaluated with two well-established metrics. in addition  the parameters of anisotropic diffusion procedure vary in our experiments to study their impacts on the final results.
1	test data
　as we mentioned in the beginning of the paper  the tasks of text segmentation fall into two categories: text stream segmentation and coherent document segmentation. the  sub topics transition and changes are relatively obvious in text stream case  while they are usually subtler and more difficult to detect in coherent documents. in our experiments  we used two data sets.
　the first is a synthetic data set that was used in previous research by choi  and utiyama  to study the first type of segmentation task. the same data set has also been tested for other segmentation algorithms including c1   texttiling   dotplot   segmenter   and u1 . the data set contains 1 samples  each is a concatenation of ten text segments. a segment is the first n sentences of a randomly selected document from the brown corpus. table 1 lists the data statistics on n. since different documents are on different topics  the ten set of sentences in a concatenated text conveys different topics.
　the second data set are texts selected from mars written by percival lowell in 1. we present the results with section 1  as a star  of chapter 1  general characteristics   section 1  clouds  of chapter 1  atmosphere   and section 1  first appearances  of chapter 1  canals . table 1 lists the data statistics. heinonen also used some texts from the same book to evaluate his text segmentation method in .
1	evaluation
　we use the error metric proposed by beeferman et al.  to evaluate text segmentation methods. beeferman et al. extensively discussed other metrics including those in  1  1  1 . this criteria has been widely used in  1  1  1  1  to evaluate several text segmentation algorithms. it calculates pk ref hyp   which is the probability of a randomly chosen pair of words a distance of k words apart inconsistently classified. the probability contains the miss and false alarm probabilities :
p error|ref hyp k  = p miss|ref hyp diff k p diff|ref k + p falsealarm|ref hyp same k p same|ref k 
1111texttiling1%1%1%1segmenter1%1%1%1%dotplot1%1%1%1%c1%1%1%1%u1%1%1%1%topseg1.1%1%1%1%our method1%1%1%1%table 1: error rates on the first data set achieved by algorithms with the numbers of segments unknown in advance
1111c1'1%1%1%1%u1'1%1%1%1%our method1%1%1%1%table 1: error rates on the first data set achieved by algorithms with the numbers of segments known in advance
low error probability means high accuracy.
　recently pevzner and hearst pointed out that the above evaluation metric penalizes false negatives more heavily than false positives  over-penalizes near-misses. they proposed another evaluation metric :
.
　this evaluation metric is used in . we also present our result based on this metric for comparison as well as future research study.
1	experimental results
　table 1 shows results on the first data set when the number of segments are determined by the systems  instead of given in advance. they are the best error rates achieved by texttiling  segmenter  dotplot  c1  u1  1  1   topseg1  and our method. our approach outperforms other methods in segmenting all of the four subsets of data.
　table 1 shows the results on the first data set when the number of segments are given in advance. they are the best error rates achieved by c1  u1  and our method. both c1 and u1 reduce the average error rates a lot when the number of segments are given. our method generates almost the same error rates. the results indicate the adapted dynamic programming technique in our method is very effective and stable in finding the optimal number of segments. combining with the results in table 1  it is indicated that our method is more favorable for dealing with long text segments. the smallest error rate is achieved when the average segment size is the largest among all subsets of data.
　in order to demonstrate the impact of anisotropic diffusion technique in improving text cohesion in abstract data level  we compare the error rates of segmenting texts with anisotropic diffusion and without anisotropic diffusion employed. at the same time  the experiments are done with the number of segments known and unknown  respectively. figure 1 shows the error rates in different configuration with bars. there are four groups of bars along x-axis each containing four bars. each group of bars corresponds to one

figure 1: error rates on the first data set achieved with different configuration of our method

figure 1: error rates on the second data set achieved with different configuration of our method
subset of data from the first data set. on average  the error rates with anisotropic diffusion are reduced by as much as 1% over those without anisotropic diffusion.
　figure 1 illustrates the experimental results for the second data set. they are the best error rates achieved by our method. since no previous research extensively discuss the issue  we do not have other's experimental results for comparison. we change the conditions by adding document-stop words removal and anisotropic diffusion. all error rates are plotted in the figure for comparison. there are two groups of bars along x-axis each containing three bars. the first group of bars represents error rates when the segment numbers are not known in advance while the second group of bars represents error rates when the segment numbers are known in advance. the error rates with number of segments known in advance improve about 1% over those without the number of segments. the error rates with document-dependent stop words removed improve about 1% over those with anisotropic diffusion  and improve about 1% over those with dynamic programming only. from the results  we find that the content-dependent stop words are very important in discriminating semantic groups of sentences inside a doc-

figure 1: error rates when the number of iteration changes

figure 1: error rates when the lambda changes
ument. the anisotropic diffusion technique is effective in enhancing the  sub topic cohesion for text streams and documents.
　in the anisotropic diffusion algorithm  there are three parameters  the number of iteration  the λ  and the conduction coefficient  that affect the quality of distance matrix segmentation improvement in various aspects. we study their impacts on segmentation performance by varying those parameters over certain ranges. figure 1 shows the error rate changes when the number of iteration varies. figure 1 shows the error rate changes when the λ varies. our experiments show that the segmentation performance is in certain degree sensitive to the choices of those parameters. however the dependence on the conduction coefficient is very small and therefore we do not presented here. in all of our experiments we have used the set of parameter values in table 1.
　we have also computed the error rates for brown corpus data set based on the pevzner and hearst's evaluation metric. they are listed in table 1 for future research comparison. the average error rate of mars data set is 1% when the number of segments is known  and 1% when the number of segments is unknown.
# of iterationsconduction coefficientlambda11table 1: parameters setting for anisotropic diffusion
1111our method'1%1%1%1%table 1: error rate for the first data set calculated with second evaluation matric when the numbers of segments are known
1	computational complexity
　the time cost of our method mainly comes from pairwise sentence distance calculation  anisotropic diffusion process  and the dynamic programming. the pairwise sentence distance calculation takes o n1   and n is the number of sentences. the anisotropic diffusion process is o mn1   where m is the number of iteration in the anisotropic diffusion process. the dynamic programming takes o kn1   where k is the number of segments. in practice  m is a constant between five and ten in our experiments. the whole process takes about 1 seconds for 1-sentence documents topic segmentation on a 1mhz  1gbyte memory sun workstation with program implemented with perl language.
1.	conclusion and future work
　we found that dynamic programming technique performs relatively well by itself. however  directly applying it to the distance matrix did not generate the best segmentation results. the reason is that complexity in lexical information blurs the topical cohesion in sentence groups. thus  there are a significant amount of noise in the distance matrix. in order to reduce the noise as well as sharpen boundaries  we employ the anisotropic diffusion technique  which is the essential step in our method to reduce noise inside each sentence topical groups while keeping the topical cohesion of them well. in addition to generic stop words  we remove the document-dependent words as well. the experiment results demonstrate that the document-dependent words are very important to reduce the content correlation among sentences in long documents.
　some researchers developed text segmentation methods targeting at data sets from tdt corpus . these data sets are text streams with only two or three sentences in each segment. experimental results above indicate that our method needs to be improved in order to effectively segment this data set. we will investigate on this kind of data and improve our method for them. the document-dependent stop words will be through studies for potential impacts on different information retrieval tasks. the performance of our method will need to be optimized. we hope to speed up the current method with the current hardware configuration.
1.	acknowledgment
　the research was supported in part by nsf grant ccr1. the authors also want to thank yixin chen  ya zhang  and the referees for their constructive comments and suggestions.
