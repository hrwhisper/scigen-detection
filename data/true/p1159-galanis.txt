this paper presents oracle database replay  a novel approach to testing changes to the relational database management system component of an information system  software upgrades  hardware changes etc . database replay makes it possible to subject a test system to a real production system workload  which helps identify all potential problems before implementing the planned changes on the production system. any interesting workload period of a production database system can be captured with minimal overhead. the captured workload can be used to drive a test system while maintaining the concurrency and load characteristics of the real production workload. therefore  the test results using database replay can provide very high assurance in determining the impact of changes to a production system before applying these changes. this paper presents the architecture of database replay as well as experimental results that demonstrate its usefulness as testing methodology. 
categories and subject descriptors 
h.1.m  database management : miscellaneous 
general terms 
management  measurement  performance  verification. 
keywords 
capture  database  record  replay  testing. 
1. introduction 
large business-critical applications are complex and experience highly varying load and usage patterns. at the same time they are expected to provide certain service guarantees in terms of response time  throughput  uptime and availability. in an attempt to stay competitive and cope with increasing demand  operators of largescale information systems strive to keep their systems up-to-date by deploying the latest technology. constant advances in information technology mandate changes at an ever-increasing rate. 
making any change to a production system  such as upgrading the database or modifying configuration  necessitates extensive testing 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior
and validation before these changes can be applied. in order to be confident before implementing a change in the production system one needs to expose the test system to a workload that is very similar to the one it would experience in a production environment. with current technology  coming even close to real testing is virtually impossible. current testing methods often fail to predict problems that frequently plague production system changes. therefore in most information system environments  any change in production systems meets great reluctance.  
in practically all it environments the relational database management system  rdbms  holds a prominent role: it provides safe transactional storage and fast scalable retrieval for virtually all data in an enterprise. consequently  the ability to safely perform changes in the rdbms component is of utmost importance. as typical data sizes in an enterprise continuously scale up and as new needs define new requirements for the rdbms  production systems require frequent changes to their database systems. some of the possible changes are: 1  a software upgrade of the rdbms. 1  a hardware or operating system change  for example moving from a single host to a clustered system . 1  a change in the physical organization of the data  adding or removing indices . extensive testing very often cannot uncover many problems. thus  changes frequently lead to problems because of new bugs  undesired query plan changes and new resource contention points. 
problems associated with changes to the rdbms and the resulting reluctance of database administrators to implement these changes are consequences of the shortcomings of current testing procedures. one of the main reasons that current testing fails in many aspects  is that it is not possible to subject a test system to a realistic production workload. common approaches to testing rdbms changes are the following:  
  specially crafted test scripts that attempt to mimic production system scenarios:  these scripts are either manually written or generated by load generation tools   such as  . this approach fails in reproducing the concurrency characteristics of a production workload. 
                                                                                        specific permission and/or a fee. 
sigmod'1  june 1  1  vancouver  bc  canada. 	 work done while at oracle usa. currently veeve facebook.com. 
copyright 1 acm 1-1-1/1...$1.   real user testing: real users are asked to use the test system as if it were a production system in the hope to make potential problem surface during testing. this approach is random  time consuming and fails to reproduce the periodically varying load patterns of a production environment. 

  simulation: defining an adequate simulation model for a complex system like an rdbms is almost impossible. thus  this method is only useful for smaller scale systems. 
oracle database replay solves the problem of real database testing. it allows the recording of the production workload on the production system with minimal performance impact. the captured workload contains all requests made to the rdbms during the period of capture as well all concurrency and transactional information. one can then use the captured workload to drive any test system and test any change before implementing it in production. the major contribution of database replay is that using the captured workload  it can accurately reproduce the concurrency and load characteristics of the production workload on the test system. hence  after testing with a real workload  a database administrator can be confident that no surprises are in order when the change is implemented in the production rdbms. 
this paper is organized as follows:  section 1 presents an overview of the testing workflow and methodology of database replay. section 1 details the architecture of the capture infrastructure. section 1 presents the architecture of the replay infrastructure. section 1 presents a case study using database replay. section 1 outlines related work and section 1 concludes the presentation of database replay. 
1. real testing 
modern information systems usually contain two distinct environments: production systems and test systems. the production systems are the lifeline of the enterprise and are taxed with serving the enterprise's information technology needs. no change should be applied to any live production system without first having been extensively tested on the test systems. the test systems are setup to accurately reflect the production systems' architecture and setup so as to enable realistic testing. this section provides an overview of how database replay is used as a testing tool.  
1 testing support 
any change to the rdbms component of an information infrastructure can be tested using database replay. for example the following modifications on the rdbms can be tested: rdbms software upgrades and patches  schema changes  new indexing and data partitioning schemes  and configuration changes  from changing initialization parameters up to moving from single host setup to a clustered database . additionally  the following software or hardware changes below the rdbms layer can also be tested using database replay: operating system upgrade or operating system change  a move from windows to linux for example   hardware changes  e.g.  new cpu  more memory  and storage system changes. changes that cannot be tested are any changes above the rdbms such as changes to the middle tier or the application clients.  
1 capturing production workload 
the best possible workload for testing is the actual production workload. oracle 1g allows any running rdbms instance to start capturing the incoming workload. the user needs to pick an interesting period of regular business operations  find adequate disk space for the workload and start capturing the workload. the workload is stored in a user specified directory in operating system files. the impact on the production system is minimal  see section 1  and the rdbms continues to behave normally from the viewpoint of the applications. even in the case that capture errors out or runs out of disc space the production system is not affected. 
the capture can be fine-tuned with the definitions of workload filters. users can specify what part of the workload they want to exclude or include in the captured workload by setting filters on session attributes  user ids and other workload specific attributes. also the user can either specify for how long the workload should be captured or manually stop the capture. after the user finishes the capture they can move the captured workload to a test system  where real testing can begin. 
1 replaying production workload 
the replay of the production workload aims to stress the rdbms on the test system so as to determine whether the test system configuration would be appropriate for use in a production environment. in general  testing consists of 1 distinct phases: 1  setting up the test system  1  defining the test workload  1  running the workload and 1  analyzing the results. when using database replay step 1 is unnecessary because the workload is well defined:  it was captured it in the production system. hence  test developers do not need to understand the application and spend time writing testing code. this saves a usually very time consuming stage in rdbms testing. 
at the beginning of the test system setup the database state needs to be restored to a state that is logically equivalent to that at the beginning of the capture. several tools exist to accomplish this  since backup/restore is one of the most well understood rdbms technologies  therefore we will not detail this process in this paper. one final step before replay can start is the processing of the workload  see section 1.1 . this creates the necessary metadata required for replay and needs to be done only once. then the processed workload can be replayed as many times as necessary. 
the captured workload needs to be issued to the test rdbms. to this end we use one or more replay clients. the replay client is a special executable that reads the captured workload and submits it to the database  see 1.1  and is part of the oracle 1g rdbms software. the number of replay clients required depends on the maximum concurrency of the captured workload and can be estimated using a utility provided. the replay clients replace the original clients that were present during the capture  for example the application middle tier . 
after starting the replay clients the user can start the replay. the replay is controlled using an api defined by stored procedures. thus  to start the replay the user needs to connect to the rdbms and call the appropriate stored procedure. then the server sends a message to all connected clients so that they start issuing the workload. during replay  the replay clients read the captured workload and convert it to appropriate requests to the database. each client is assigned a part of the workload by the rdbms. the aggregate workload generated by all the replay clients mimics the production workload. for example  if during capture 1 users connected to the rdbms  during replay the same 1 users will connect following the same connection and request patterns. thus  the test rdbms is subjected to the same load and request rate as the production system during capture. additionally  the rdbms makes sure that the replayed requests perform meaningful work  by maintaining the data dependencies seen during capture  see section 1.1 . for example  if a request updated 1 rows during capture  during replay the rdbms makes sure that this request executes after a previous request that inserted these 1 rows. the result is that using capture and replay testing one can subject a test system to a production workload and perform real testing. 
while replay is going on the rdbms can still be accessed normally. this means that all available performance monitoring tools  such as addm and ash   can be used to monitor the rdbms during replay. furthermore  additional workload can be executed in parallel with the replay to further load the server. the replay can be stopped at any time or it can run to completion until the captured workload is consumed. after the replay is finished  the rdbms generates reports that help determine the quality of replay and compare key points of the capture and the replay. replay can be performed repeatedly using the same captured workload. obviously  the database state has to be restored appropriately before every replay. 

figure 1 database replay testing 
1 use cases 
change impact testing is one of the most useful test cases for database capture and replay testing. any conceivable change at or below the rdbms layer can be confidently tested before it is implemented. one very important use case is upgrade testing. one can replay an oracle 1g production workload against an oracle 1g test system in order to discover problems before upgrading the production servers. another testing application is when changing server hardware from a single instance system to a clustered system. the captured workload can be replayed at an increased request rate against a clustered database to figure out the performance gains. the increased request rate is achieved by issuing the captured requests at a rate higher than that observed during capture. the same method can be applied when moving from one operating system to another. cross os testing is possible because the captured workload is platform independent. deterministic debugging of server problems is another good use case of database replay testing. our experience has shown that the vast majority of issues discovered during testing using database replay are always reproducible.  
1. capture 
this section details how the rdbms workload is captured. the goal of capture is to record all necessary activity on the rdbms that is required to faithfully reproduce the same activity on a test system. moreover  this needs to be done with minimal overhead to the rdbms. 
1 point of capture 
capturing the rdbms workload either requires instrumentation of the software stack of the rdbms or the use of some external component that captures the network traffic between the rdbms and its clients. we chose to follow the former approach because it allows the captured data to be protocol independent and makes it possible to record rdbms internal data that is required for replay  see section 1.1 . furthermore  all capture probes are placed below the client-server protocol layer in the rdbms software stack. this ensures that all captured data is protocol and platform independent. 
1 workload sources 
the rdbms workload can be subdivided into 1 basic categories: 1  external clients requests  1  internal clients requests. external clients are applications or application server middle tiers. all the activity coming from such clients is captured. internal clients are considered background processes such as maintenance tasks or scheduler jobs. the workload from these clients is not captured because it is expected to be present during replay. for example  if a client issues a request to schedule a job  this request will be captured  but the execution of the scheduled job will not. later  during replay  the scheduling of the job will be replayed  which will cause the scheduled job to be executed in the replay system. if we had captured the scheduled job we would have 1 instances of the same job running on the replay system. 
1 capturing agents 
the instrumentation of the rdbms kernel makes up the capture infrastructure. it provides capture services to be used by the entities that record the workload  the capture agents. more specifically  the capture agents are the oracle server processes that service external client requests. since  the oracle server processes capture themselves  there are no additional capturing agents. when workload capture is enabled  the server processes use the capture services to record the workload into a plain operating system file  the capture file. all processes write into a single directory  the capture directory. for performance reasons  the captured workload is not stored in the database  since this would require generation of undo/redo information. 
the capture infrastructure is designed to also work with the oracle rdbms in a cluster configuration. coordination is only required during starting and stopping. in both cases cross-instance messaging is used to start and stop capture on all instances. one complication in the clustered environment stems from the fact that the capture directory may not be shared. in this case the contents of all directories from all instances have to be consolidated into a single directory for replay. 
1 workload contents 
capturing an rdbms workload for a period of time yields one file per rdbms server processes. each captured process file  capture file  contains workload from 1 or more database sessions. a database session is the set of interactions of a user between log on and logoff. multiple sessions can be multiplexed into 1 database server process. each session consists of consecutive service requests or database calls. a sql query  an update to a table  a commit  and a call to read from a large object are all examples of database calls. 
database calls are divided into two categories. each captured call is either a commit action or a non-commit action. a commit action is a call that commits the work that has been done by the captured session. examples of commit actions are: 1  a commit  1  an insert that is issued with auto-commit and 1  a create table statement  which always commits internally. a non-commit action is a call that does not commit any data. examples of non-commit actions are: 1  a select query  1  an update statement and 1  and insert statement. the distinction between commit and noncommit actions will become clearer in the context of replay  section 1.1 . the basic rationale for the distinction is that commit actions modify the database state and may affect the outcome of subsequent call. note that dml operations modify database state but the change is only visible to the session that issued the operation because of transactional isolation. so any non-committed operation that changes data does not affect any other session. 
each recorded call contains sufficient information that allows the accurate reproduction of the call during replay. this information can be divided into three categories: 
1. user data:  this is data that is sent from the client to the 
rdbms. 
1. server response data:  this is data that is sent from the server back to the user. 
1. system data:  this data is internal to the rdbms kernel  is not returned to the user but is crucial for replay. 
the majority of the captured data belongs to the user data category. more specifically  this data contains the full sql text of user requests  all the bind values  and all non-sql requests and their arguments  such as calls to manipulate large objects . the recorded user data does not contain system sql that is executed as part of a user request  for example catalog queries . similarly  we only record the full text of plsql    scripts  without recording each individual user sql that is executed as part of the script. 
the data that is sent from the server back to the client can potentially be very large  because it mainly contains query results. recording all the query results of a workload is not feasible since this would incur a high overhead. nevertheless  we need to capture sufficient data about the outcome of user requests so that we know how much data they processed and if they had any errors. therefore the captured server response data contains the number of rows affected/returned by the user request and error codes. additionally  for reasons that will become clear in section 1.1 system specific data that is part of query results is captured. such data are row identifiers  rowids that allow direct access to rows without the need of an index   large object locators and result set handles. 
system data is rdbms kernel internal data that is required only during replay. part of the system data is timing information. additionally  the captured system data contains system change numbers  scn  that characterize the database state in which the captured call executed and determine whether the call is a commit action or a non-commit action. the scn is a stamp that defines a committed version of a database at a specific point in time. oracle assigns every committed transaction a unique scn. these scns are used for ensuring isolation and read consistency within the database server. each call contains the scn that corresponds to the database state when the call started executing. this scn is called the wait-for scn. additionally each commit action contains the commit scn that is the scn that corresponds to the database state immediately after the commit action and before any subsequent commit action. the significance of both scns will become clear in section 1.1. finally  values returned by sequence generators  and other system functions that are omitted for simplicity  are part of the captured system data. these values are used during replay as shown in section 1.1. u ser   
	statem ent text: 	statem ent text: 
d ata 	select * from a w here 	begin update a 
	b = :1; 	set b=:1 w here b=:1;
	binds: bind 1: 'b' 	com m it; end; 
	 	binds: b ind 1: 'c'  bind 
	 	1: 'd' 
r esponse 	fetch: r ow s: 1 	exec: r ow s: 1 
d ata 	error code: 1 	error c ode: 1 
 	 
system  	call begin tim e: t1 	c all begin tim e: t1 
d ata w ait-for sc n : 1 w ait-for sc n : 1 execution: c ursor 1 c om m it scn : 1
	call end tim e:t1 	execution: c ursor 1 
c all end tim e:t1
	n on-com m it a ction 	c om m it a ction
 
figure 1 captured database call examples 
figure 1 depicts two typical captured database calls along with some of the key information captured. the first call is a non-commit action that corresponds to a select query with one bind. as part of this call we capture the call begin and end time  the type of the call  select...   the environment scn  wait-for scn  in which the query executed  the full text  the bind data  the internal id of this results set  cursor number 1  and the number of rows fetched by this call. the number of rows will be compared to that during replay to determine whether this call does the same work during replay. the second call is a commit action that corresponds to a plsql script that contains an update statement and a commit. again  full statement text  binds  call begin and end time are captured. in addition to the wait-for scn the call contains the commit scn  which defines the database state immediately after the statement executes. 
the capture files are plain binary files that follow a self-describing extensible format. the data in the capture files is platform independent so that one can capture the workload of an oracle database running on a 1bit linux os and then replay the workload on a 1bit windows system. furthermore  the format automatically allows backward compatibility with pre-existing captures regardless of how many new probes will be added in the future.  
1 overhead 
as expected the capture infrastructure adds some overhead to the running workload. the goal is to make the capture overhead as small as possible. to this end buffered i/o is used along with code optimizations. additionally  repeated data  such as sql text from repeated executions  is captured only once per process. the result is that for most workloads the overhead is reasonable enough so that it is possible to turn on capture on the entire production. moreover  critical production systems are usually over-provisioned so that the additional overhead of capture is acceptable. for example enabling capture on the tpc-c benchmark  only reduces transaction throughput by about 1%. 
the capture overhead is workload dependent. however  it is not proportional to the work that the rdbms performs. it is proportional to the data that needs to be captured in order to be able to replay the call. for example  if the workload consists mainly of calls that are very complex queries that each take very long to execute  the overhead is going to be minimal  since the data captured per time unit is small: only the text of the complex query once per session for each call that executes this query. on the contrary  if the workload is insert-intensive the overhead may be significant. the reason is that each captured insert will contain all the inserted data in addition to the insert statement. figure 1 shows how the overhead varies depending on the type of the workload.  
 
long running short insert large sql sql/dml intensive lobs  
figure 1 workload dependent capture overhead 
the space requirements of the capture are also workload dependent and so a reliable estimation is not possible. the space required is roughly equal to the data that is sent from the client to the server over the network. since performance overhead is a big concern  the workload data is not compressed during capture. to estimate the disk space needed one can turn on capture for a brief time when the system processes the workload of interest and then extrapolate for the potential duration of the capture. 
1 filters 
the capture infrastructure also provides the ability to filter out specific workload based on workload attributes  user  session id  cluster instance number  session state and others . the defined filters fully specify a part of the workload and are used in two modes:  inclusion mode and exclusion mode. if inclusion mode is specified  only the workload specified by the filters is captured. otherwise  exclusion mode  the workload that is specified by the filters is not captured. filters are useful in cases where an rdbms services multiple independent applications. in this case  it is possible to specify appropriate filters that will capture the workload coming from one specific application. then this workload can be tested in isolation. 
1. replay  
the high level goal of replay is to subject a test rdbms to a realistic workload. this means that replay will make the test system handle a load with near identical concurrency and transactional characteristics as a real workload. the capture infrastructure provides replay with all the necessary data to recreate the same workload on a test system. in this section we will briefly introduce basic replay concepts before we present the oracle 1g specific design and implementation of database replay. 
1 concepts 
1.1 replay testing success 
to determine whether replay can achieve its goal one needs a method to measure success. the goal of this method is to determine whether the replay of a captured workload is suitable for accurately testing prospective changes. there are three basic characteristics of a workload that we use to determine replay success: 1  the final database state after the workload  1  the concurrency and request rate characteristics and 1  the results returned to the clients. a replay is successful if it starts from the same database state as the captured workload  runs on an identical system and exhibits behavior identical to the captured workload with respect to these three workload characteristics. such a replay achieves end state consistency  result consistency and request rate consistency. note that there exists technology to create workloads that achieve each of the above characteristics separately. for example one can process the redo log of the database and create a workload that will achieve the same final state as the workload that generated the redo log. this workload  however  will not have any meaningful concurrency and will not execute any queries  only updates. furthermore  these updates are block level changes. thus  if the update contained a where clause  which could involve substantial processing  it will be lost. database replay takes on the challenge to create a workload that simultaneously preserves all three characteristics of a real workload in the context of a test system. in reality it is going to be almost impossible to achieve 1% consistency on all 1 characteristics with respect to the captured workload. nevertheless  a close approximation of each characteristic would still provide high confidence in testing. 
 

 
figure 1 transactional model example 
1.1 transactional model 
to achieve end state consistency and result consistency  we define the transactional model for replay that determines the replay order of captured calls. this model is based on the classification of the captured database calls into commit actions and non-commit actions. this distinction is based on the fact that a commit action changes the database state and makes the new state visible to all subsequent actions. since the result of any call depends on the database state  the replay has to ensure that each call c follows the appropriate commit action so as to satisfy result consistency and ultimately database end state consistency. in the current version of database replay the appropriate commit action is the most recent commit action in the entire captured workload with respect to c. additionally  non-commit actions do not need to be ordered with respect to other non-commit actions. this allows for concurrency between two subsequent commit actions. figure 1 shows how a replay would satisfy the transactional model. for example between calls 1 and 1  calls 1  1  1 and 1 can be executed in any order during replay but must come after 1. naturally  calls that belong to the same execution thread are issued in the order they were recorded during replay: 1 will always come after 1.  
the transactional model currently used for database replay assumes that every commit action affects every subsequent call. this is a coarse model and may consider two captured calls dependent even if they are not. for example in figure 1  consider calls 1 and 1. if 1 commits updates to table t1 and 1 selects from table t1  the model will consider 1 dependent on 1 even though they are not. it is possible to use a more refined transactional model for database replay that only considers true dependencies between captured calls. for the first version of database replay we chose to use the simpler model for several reasons. during capture the required information to define transactional dependencies is small which is beneficial for the capture overhead. furthermore  during replay enforcing the call order is simple and does not interfere significantly with the replayed workload. experiments with early prototypes have shown that the simple transactional model used currently is adequate to ensure database end state and result consistency while at the same maintaining the request rate observed during capture. 
1.1 time model 
to satisfy request rate consistency the transactional model is not sufficient since it only defines the order of replayed calls and not their timing. timing information is required in order for the replay to accurately recreate the request rate observed during capture. to this end we define two distinct time dimensions:   
1. connection time: this is the time between the start of the capture  replay  and the beginning of the first captured call of a captured process  replayed call of a replayed process .  
1. think time: this is the time between the end of call and the beginning of the next call in the same execution thread  captured or replayed server process .  
to achieve request rate consistency the replay has to maintain the connection time observed during capture for each replayed process and the think time observed during capture between any consecutive pair of calls within each replay process provided the call execution time does not change from capture to replay. in reality the call execution time will vary from capture to replay. section 1.1 describes how request rate is maintained in practice. 
1 database replay architecture 
the oracle 1g database replay feature is an implementation of the transactional model and the time model introduced in the previous section. using the captured workload on a properly set up test system  database replay achieves high degrees of end state consistency  request rate consistency and result consistency with the respect to the capture. this section explains how this is achieved.  
1.1 capture processing 
capture processing is the operation that in one pass reads all the captured workload and produces metadata required for the replay. this metadata consists of four parts:  
1. the data required to order commit actions during replay.  scn order table  
1. a collection of system-generated values used for the replay time emulation of system function.  sysid tables  
1. a collection of the connection descriptors used by the captured processes to connect the database. 
1. a summary of the workload used by the workload driver infrastructure.  connection queue  
the scn order table contains rows that correspond to commit actions in the workload. each row contains the following columns: the commit scn  section 1.1   the call id and the file id. the table is ordered by commit scn and contains all commit actions in the order they changed the database state. the reason we cannot use time to order commit actions is that the actual commit may happen anywhere within the database call. calls can overlap in all possible way  thus only the scn can determine which call committed its changes to the database first. this table is used during replay to implement the transactional model. 
the sysid tables contain values of system-generated ids that are used in  section 1.1 . one of these tables contains per call sequence values. during capture we record the return values of s.nextval and s.currval  where s is a sequence generator. these values are used during replay instead of the actual replay time return values of the s.nextval and s.currval. each row in the sequence table contains a reference to the call and the sequence it belongs and a range of values that this call consumed. one or more rows can correspond to a captured call. 
the collection of connection descriptors is used to initialize the connection map table before replay. the captured connection descriptors are invalid in the replay. so  before replay can start the user must map them to new valid connection descriptors.  
the connection queue is a single file that contains an entry for each captured process. the entries are sorted by connection time and contain information used by the workload driver infrastructure as explained in section 1.1. 
capture processing is a one-time operation that transforms the captured workload into the replay files. the replay files can be used for replay arbitrary many times. 
1.1 workload driver 
the workload driver is the infrastructure that reads the captured workload and issues it to the rdbms during replay. it is made up of one or more replay client processes  replay clients . each replay client process is a multithreaded application provided as part of the rdbms distribution. each replay client thread  replay thread  reads one captured process file and interprets its contents into appropriate database calls to the rdbms. the replay client is a regular oci client    and as such can connect remotely to the rdbms. this makes it possible to start the replay clients on multiple hosts. the workload driver infrastructure replaces any clients/middle tiers that were present during capture and is responsible for driving the workload to the server. 
protocol independence in the captured data complicates the choice of replay client protocols. conceivably the replay client can use multiple protocols during replay based on what was used during capture. however  this would lead to a multi-language code base  for example java for jdbc client code and c for oci client code  with significant code duplication that would be difficult to maintain. this technical difficulty combined with the fact the protocol specific code in the rdbms is not the main focus of testing  led to the adoption of functional replay. functional replay dictates that each replayed call be equivalent to the captured call in terms of the work it performs inside the rdbms regardless of the protocol used to send the call from the client to the server. thus  by design  the replay client only uses one protocol to talk to the server: the oracle call interface  oci . each captured call is converted to the appropriate sequence of oci interface calls. 
the captured workload can potentially consist of thousands of files and may require the replay of thousands of concurrent sessions connected to the rdbms. therefore the number of replay clients that can be used to drive the workload needs to scale appropriately. to achieve scalability the replay clients do not communicate with each other and are instead coordinated by the server. database replay contains a calibration utility that advises on the number of the replay clients that are needed based on the workload concurrency and the available hardware.  
starting the replay is a two-step process. first the rdbms is put in a special state  prepared state  in which it waits for replay clients to connect. then the user starts arbitrary many replay clients by pointing them to the prepared rdbms. after starting a client the user cannot interact with it anymore; the client is fully controlled by the rdbms server. the user can start the replay by interfacing with the rdbms through a server api. at replay start  the server determines how many clients have connected  sends each client the replay options  section 1.1  and distributes all the capture files among them in a round-robin fashion. at this point each client know exactly which capture files it will replay. then it sends the signal to each replay client to start issuing the captured workload. 
the workload driver infrastructure is tasked with implementing the replay time model  which ensures request rate consistency. connection time during replay is determined by the connection queue produced by workload processing. once replay starts each replay client scans the entire connection queue. for each entry in the queue the replay client checks whether the capture file this entry points to belongs to the client's workload assigned by the rdbms. if it does the client spawns a replay thread at the appropriate point in time. this thread is completely self-contained and does not communicate with other threads. the replay thread starts scanning the appropriate capture file and connects to the server only after satisfying the timing specified by the connection time in the capture. the connection endpoint to use is determined by the connection map table that the client reads from the server. 
the replay thread exclusively maintains the think time prescribed in the time model. between consecutive calls  the replay thread sleeps for the appropriate amount of time so as satisfy the captured request rate for the captured process it is replaying. thus  the aggregate request rate requirements are maintained globally as a result of each thread's effort to maintain the request rate locally. the time model think time requirement assumes that call execution time does not change from capture to replay. in reality this will almost never be true. therefore the replay thread maintains a cumulative time deficit that is subtracted from the think time in order to maintain the captured request rate. thus  if replay calls are on average slower the think time is shrunk in order to maintain the request rate  figure 1 . however  if calls are on average faster the think time is not expanded  even though this contradicts the request rate consistency. the reason is that it is more appropriate for testing purposes to let the replay run at a higher rate if the test rdbms can handle the workload. it is possible that the time deficit becomes so high that sleep time needs to be reduced to zero. at this point the replay cannot maintain the captured request rate and starts running slower.  

figure 1 synchronization example 
1.1 scn based synchronization 
scn based synchronization is the means with which database replay implements end state consistency and results consistency. this infrastructure resides in the rdbms server and is used by the replayed server processes. each captured call contains a wait-for scn and each commit action additionally contains a commit scn. conceptually  in order to achieve result consistency it is sufficient to ensure that every replayed call c is executed after the replay of the commit action a that is the most recent commit action that was captured before c. this is always true in the oracle rdbms because of the absence of  dirty reads . therefore the scn can provide the desired ordering.  
the captured commit scn values are used to maintain the replay clock during the replay. the replay clock is similar to simulation clocks in that it is advanced by specific events. in the case of replay these events are commit actions. every commit action sets the clock to the maximum of its commit scn and the current clock. the replay clock is observed by every replay call  both commit and noncommit actions . during replay at the beginning of each call the server process executing this call checks the value of the replay clock. if the wait-for scn of the replayed call is less than or equal to the replay clock  then the call is allowed to execute. otherwise the call is blocked waiting to be posted by a clock advance. this posting happens when a commit action a advances the clock to a new value. the values for the clock are calculated using the scn order table that is produced during capture processing  the appropriate portion of this table is cached in memory during replay . the new value of the clock is calculated by taking the commit scn of the commit action b immediately following a in the scn order table and subtracting one. then the server process that executed a posts all waiting processes that have pending calls with a wait-for scn less than or equal to the new value of the clock. note that the system scn during replay time is not used for replay purposes and so it does not have to match with the captured scn values. 
figure 1 shows an example of synchronizing 1 sessions during replay. the right side shows the corresponding calls with the recorded scn values. the scn values are monotonically increasing. during replay call a1 takes longer to complete than it did during capture. this causes call a1 to wait since the replay clock is still 1. after a1 completes it makes the replay clock 1 and posts a1. taking the commit scn of a1 and subtracting one produces 1  which is the new clock value. similarly a1 waits to be posted by a1. in reality the replay clock mechanism is more complicated than in this example. for illustration purposes the concept is simplified with respect to the actual implementation. 
the replay clock is maintained in global memory in the rdbms. in case of a clustered rdbms each instance maintains its own replay clock. these clocks are kept in sync by cross instance messaging. conceptually each commit action broadcasts the new replay clock to all cluster instances using some optimizations that are outside the scope of this paper. note that during replay  only sessions coming from replay clients use the scn ordering infrastructure. thus the database is still available for serving regular workload even if replay is ongoing.  
gating calls in the database occasionally may lead to replay induced deadlocks. the reason is that the server may block a call c that holds resources  locks or latches for example  needed by another commit action a to complete. if c is  transitively  waiting for the replay clock value that a is going to produce  then the replay will hang. to avoid such hangs during the replay a background process periodically checks for such deadlocks by following wait chains and resolves them by posting the process that executes that call that is blocking the commit action from proceeding. this type of deadlock resolution is different than the usual deadlock resolution mechanisms used in oracle in that the chosen deadlock detection victim is not actually forced to abort a transaction. it is rather allowed to continue without waiting for the replay clock until it has given up the resources  usually locks  it holds  i.e. until the next commit  so that the clock ticker waiting on these resources can make progress and advance the replay clock.  
1.1 replay-time data replacement 
