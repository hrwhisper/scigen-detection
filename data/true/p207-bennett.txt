the intuition that different text classifiers behave in qualitatively different ways has long motivated attempts to build a better metaclassifier via some combination of classifiers. we introduce a probabilistic method for combining classifiers that considers the contextsensitive reliabilities of contributing classifiers. the method harnesses reliability indicators-variables that provide a valuable signal about the performance of classifiers in different situations. we provide background  present procedures for building metaclassifiers that take into consideration both reliabilityindicators and classifier outputs  and review a set of comparative studies undertaken to evaluate the methodology.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval; i.1  artificial intelligence : learning; i.1  pattern recognition : models
general terms
algorithms  experimentation.
keywords
text classification  classifier combination  metaclassifiers  reliability indicators
1. introduction
　researchers have long pursued the promise of harnessing multiple text classifiers to synthesize a more accurate classification procedure via some combination of the outputs of the contributing classifiers. the pursuit of classifier combination has been motivated by the intuition that  because different classifiers work in related but qualitatively different ways  an appropriate overlaying of classifiers might leverage the distinct strengths of each method.
　classifiers can be combined in different ways. in one approach  a text classifier is composed from multiple distinct classifiers via
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  tampere  finland.
copyright 1 acm 1-1/1 ...$1.
a procedure that attempts to select the best classifier to use in different situations. for example  we may work to identify the most accurate classifier in some setting  seeking to learn about accuracy over output scores or some combination of output scores and features considered in the text analysis. other procedures for combining classifiers consider inputs generated by the contributing classifiers. for example  in a voting analysis  a combination function considers the final decisions made by each classifier as votes that influence an overall decision about document classification. in a finer-grained approach to combining multiple classifiers  the scores generated by the contributing classifiers are taken as inputs to the combination function. whichever approach to combination employed  the creation of enhanced metaclassifiers from a set of text classifiers relies on developing an understanding of how different classifiers perform in different informational contexts.
　we have pursued the development of probabilistic combination procedures that hinge on learning and harnessing the context-sensitive reliabilities of different classifiers. rather than rely solely on output scores or on the set of domain-level features employed in text-classification  we introduce the use of reliability-indicator variables-a set of features that provide a low-dimensional abstraction on context for learning about reliability. we borrow the reliability-indicator methodology from work initiallypresented by toyama and horvitz  to the automated vision community. they introduced the reliability-indicator learning and inference framework and showed how the approach could be applied in vision to integrate several distinct scene analyses into an overall higher-accuracy composite visual analysis. we have found that the reliability-indicator methodology is useful in the text classification realm for providing context-sensitive signals about accuracy that can be used to weave together multiple classifiers in a coherent probabilistic manner to boost overall accuracy.
　we will first review related work on the combination of textclassification procedures. then we introduce the use of reliability indicators in text classification  and show how we employ these variables to learn about the context-sensitive reliabilities of na： ve bayes  unigram  support vector machine  svm   and decision-tree classifiers. we describe how we integrate the indicator variables with base-level features and scores output by classifiers to build meta-classifiers that show enhanced performance. we highlight our methodology and results by reviewing several sets of experiments. finally  we summarize our contributions and discuss future directions.
1. related work
　the overlaying of multiple methodologies or representations has been employed in several areas of information retrieval. for example  past research in information retrieval has demonstrated that retrieval effectiveness can be improved by using multiple  distinct representations  1  1  1   or by using multiple queries or search strategies  1  1 . in the arena of text classification  several researchers have achieved improvements in classification accuracy via the combination of different classifiers  1  1  1  1 . others have reported that combined classifiers work well compared to some particular approach  but have not reported results that compare the accuracy of the classifier with the accuracies of the individual contributing classifiers. similarly  systems that seek to enhance classification performance by applying many instances of the same classifier  such as in boosting procedures  1  1   have compared the overall performance of the final methodology to other systems rather than to the weaker component learners.
　much of the previous work on combining text classifiers has used relatively simple policies for selecting the best classifier or for combining the output of multiple classifiers. as some examples  larkey and croft  used weighted linear combinations of system ranks or scores; hull et al.  used linear combinations of probabilities or log odds scores; yang et al.  used a linear combination of normalized scores; and li and jain  used voting and classifier selection techniques. lam and lai  use category-averaged features to pick a  possibly different  classifier to use for each category.
　as we shall highlight below  in contrast to prior research on classifier combination  our work centers on the use of a richer probabilistic combination of inputs  using combination functions learned with bayesian and svm learning methods. in this respect  our approach is similar to work by ting and witten  in stacked generalization  although they did not apply their approach to text problems. we also report baseline comparisons with voting and classifier-selection techniques.
　larkey and croft  used rank-based measures of performance because they were interested in interactive systems in which a rank list of codes for each document would be displayed to users. many other applications such as automatic routing or tagging require that binary class membership decisions be made for each document as it is processed. we focus on classifier combination to enhance such classification decisions. this goal appears to be more challenging than the use of classifiers for document ranking. as an example  hull et al.  found that while combination techniques were able to improve document ranking  they did considerably less well at estimating probabilities.
1. problem approach
　our work differs from earlier combination approaches for text classification by  1  the use of expressive probabilistic dependency models to combine lower-level classifiers  leveraging special signaling variables  we refer to as reliability indicators  and  1  a focus on measures of classification performance rather than the more common consideration of ranking.
1 reliability indicators
　previous approaches to classifier combination have typically limited the information considered at the metalevel to the output of the classifiers .
　we address the challenge of learning about the reliability of different classifiers in different neighborhoods of the classification domain at hand by introducing variables referred to as reliability indicators. a reliability indicator is an evidential distinction with states that are linked probabilistically to regions of a classification problem where a classifier performs relatively strongly or poorly.
the reliability-indicator methodology was introduced by toyama and horvitz  and applied initially to the task of combining several different machine-vision analyses in a system for identifying the head and pose of computer users. the researchers found that different visual processing modalities had distinct context-sensitive reliabilities that depended on dynamically changing details of lighting  color  and the overall configuration of the visual scene. the authors introduced reliability indicators to capture properties of the vision analyses  and of the scenes being analyzed  that appeared to provide probabilistic indications of the reliability of the output of each of the modalities. to learn probabilistic models for combining the multiple modalities  data was collected about ground truth  the observed states of indicator variables  and the outputs from the concurrent vision analyses. the data was used to construct a bayesian network model with the ability to appropriately integrate the outputs from each of the visual modalities in real time  providing an overall higher-accuracy composite visual analysis.
　the value of the indicator-variable methodology in machine vision stimulated us to explore the analogous application of the approach for representing and learning about reliability-dependent classifier contexts. for the task of combining classifiers  we formulate and include sets of variables that hold promise as being related to the performance of the underlying classifiers. we consider the states of reliability indicators and the scores of classifiers directly  and  thus  bypass the need to make ad hoc modifications to the base classifiers. this allows the metaclassifier to harness the reliability variables if they contain useful discriminatory information  and  if they do not  to fall back to using the output of the base classifiers.
　as an example  consider three types of documents where:  1  the words in the document are either uninformative or strongly associated with one class;  1  the words in the document are weakly associated with several disjoint classes; or  1  the words in the document are strongly associated with several disjoint classes. classifiers  e.g.  a unigram model  will sometimes demonstrate different patterns of error on these different document types. we have pursued the formulation of reliability indicators that capture different association patterns among words in documents and the structure of classes under consideration. we seek indicator variables that would allow us to learn context-sensitive reliabilities of classifiers  conditioned on the observed states of the variable in different settings.
　as a concrete example  figure 1 shows a portion of the type of combination function we can capture with the reliability-indicator methodology. the nodes on different branches of a decision tree include the values output by base classifiers as well as the values of reliability indicators for the document being classified. the decision tree provides a probabilistic context-sensitive combination rule indicated by the particular relevant branching of values of classifier scores and indicator variables. in this case  the portion of the tree displayed shows a classifier-combination function that considers thresholds on scores provided by a base-level linear svm  outputofsmox  classifier and a base-level unigram classifier  outputofunigram   and then uses the context established by reliability-indicator variables  unigramvariance and %favoringinclassafterfs  to make a final decision about a classification. the annotations in the figure show the threshold tests that are being performed  the number of examples in the training set that satisfied the test  and a graphical representation of the probability distribution at the leaves. the likelihood of class membership is indicated by the length of the rectangle at the leaves of the tree.
　the variable unigramvariance represents the variance of unigram weights for words present in the current document. the intuition is that the unigram classifier would be accurate when there is low variance in weights. the variable %favoringinclassafterfs is the percentage of words  after feature selection  that occur more often in documents within a target class than in other classes. classifiers that weight positive and negative evidence differently should be distinguished by this variable. for the strive-d  norm  classifier excerpt shown in figure 1 we have further normalized the metafeatures to have zero mean and unit standard deviation so most values fall between -1 and 1 as a result.
　the indicator variables used in our studies are an intuitive attempt at formulating states to represent influential contexts. we defined variables to represent a variety of contexts that showed promise as being predictive of accuracy-e.g.  the number of features present in a document before and after feature selection  the distribution of features across the positive vs. negative classes  and the mean and variance of classifier-specific weights.
　currently  our indicator variables roughly fall into of four types. different indicator variables measure  1  the amount of information present in the original document   1  the information lost or mismatch between the representation the classifier used and the original document   1  the sensitivity of decision to evidence shift  and  1  some basic voting statistics. documentlength is an example of type 1. the performance of classifiers is sometimes correlated with document length  because longer documents give more information to use in making a classification. documentlength can also be informative since some classifiers will perform poorly over longer documents because they do not model length  e.g. they double count evidence and longer documents are more likely to deviate from a correct determination . percentremoved is an example of type 1. this represents the percent of features removed by feature selection. if most of the document was not represented by the feature set used in a classifier  then some classifiers may be unreliable. others  e.g. decision trees that model missing attributes  may continue to be reliable. when the base classifiers are allowed to use different representations  then these features can play an even more important role. an example of type 1 is the unigramvariance variable. low variance means the decision of the classifier is unlikely to change with a small change in the document content; high variance increases the chances that the decision would change with only a small change in document content. finally  numvotingforclass or percentagreement are examples of the last type. these simple voting statistics improve the metaclassifier search space  since the metaclassifier is given the base classifier decisions as input as well . for a 1-class case the percentagreement variable may provide little extra information but for n-classes it can be used to determine if the base classifiers have fractured their votes among a small number of classes or across a wide array. we found all of these types to be useful in the final combination scheme  and the early analysis has not found that any one type dominates the combination models.
　reliability-indicator variables are qualitatively different than variables representing the output of classifiers because we do not assume that the reliability indicators have some threshold point that classifies the examples better than random  nor do we assume that classification confidence shows monotonicity trends as in classifiers. we currently do not exploit these underlying assumptions for the classifiers  but we hope to do so in the future. at a more practical level  the reliability indicators are usually much simpler than typical classification algorithms.
1 strive: metaclassifier with reliability indicators
　we refer to our classifier combination learning and inference framework as strive for stacked reliability indicator variable ensemble. we select this name because the approach can be viewed as essentially extending the stacking framework by introducing relia-

figure 1: the basic architecture of strive.
bility indicators at the metalevel. the strive architecture is depicted in figure 1.
　we map the original stacking learning problem to a new learning problem. originally  we have some document    with words  and class  . our classifiers    predict from the words. we denote their predictions by . then  our set of reliability indicators    use the words and the classifier outputs to generate their values  . we now have a new document 
	with class	which we will give to the
metaclassifier as the learning problem.
　we require the outputs of the classifiers to train the metaclassifier. thus  we perform cross-validation over the training data  and use the values obtained while an example serves as a validation item as the input to the metaclassifier.
1 bestselect classifier
　when classifier combination is formulated as the process of selecting on a per-example basis the best base classifier  we can introduce a natural upper bound on combination. such an upper bound can be useful as a benchmark in experiments with classifier combination procedures.
　to classify a given document  if any of the classifiers correctly predict that document  the best combination would select any of the correct classifiers. thus  such a classification combination errs only when all of the base classifiers are incorrect. we refer to this classifier as the bestselect classifier. if all of the base classifiers are better than random  the bestselect gives us a rough idea of the best we can do combining them in a selection framework.

figure 1: portion of decision tree  learned by strive-d  norm  for the travel & vacations class in the msn web directory corpus  representing a combination policy at the metalevel that considers scores output by classifiers  dark nodes  and values of indicator　we note that we are not using a pure selection approach  as our framework allows the possibility of choosing a class that none of the base classifiers predicted. in cases where the classifiers are not better than random  or are logically dependent   such an upper bound may be uninformative. if we simply seek an answer to the question   how much of what we could optimistically expect to gain with this set of base classifiers have we gained    then this loose bound provides a more pessimistic view of the results than is actually the case. therefore  we evaluate the scores of the bestselect classifier as a useful point of reference in our experiments. variables  lighter nodes .
1. experimental analysis
　we engaged in a large number of experiments to test the value of probabilistic classifier combination with reliability indicator variables. we shall describe the corpora  methodology  and results.
1 data
　we examined several corpora  including the msn web directory  reuters  and trec-ap.
1.1 msn web directory
　the msn web directory is a large collection of heterogeneous web pages  from a may 1 web snapshot  that have been hierarchically classified. weused the same train/test split of 1 documents as that reported in .
　the msn web hierarchy is a 1-level hierarchy; we used all 1 of the top-level categories. the class proportions in the training set vary from to . in the testing set  they range from to . the classes are general subject categories such as health&fitness and travel&vacation. human indexers assign the documents to zero or more categories.
　for the experiments below  we used only the top 1 words with highest mutual information for each class; approximately 1k words appear in at least 1 training documents.
1.1 reuters
　the reuters 1 corpus  contains reuters news articles from 1. for this data set  we used the modapte standard train/ test split of 1 documents  1 unused documents . the classes are economic subjects  e.g.   acq  for acquisitions   earn  for earnings  etc.  that human indexers decided applied to the document; a document may have multiple subjects. there are actually 1 classes in this domain  only 1 of which occur in the training and testing set ; however  we only examined the 1 most frequent classes since small numbers of training examples makes estimating some performance measures unreliable due to high variance. limiting to the 1 largest classes allows us to compare our results to previously published results  1  1  1  1 .
the class proportions in the training set vary from to . in the testing set  they range from to .
　for the experiments below we used only the top 1 words with highest mutual information for each class; approximately 1k words appear in at least 1 training documents.
1.1 trec-ap
　the trec-ap corpus is a collection of ap news stories from 1 to 1. we used the same train/test split of 1 documents that was used in . as described in   see also    the categories are defined by keywords in a keyword field.
the title and body fields are used in the experiments below. there are 1 categories total.
　the frequencies of the 1 classes are the same as those reported in . the class proportions in the training set vary from to . in the testing set  they range from to .
　for the experiments described below  we use only the top 1 words with the highest mutual information for each class; approximately 1k words appear in at least 1 training documents.
1 classifiers
　we employed several base-level classifiers and classifier combination methods in our comparative studies. we review the classifiers and combination methods here.
1.1 base classifiers
　we worked to keep the representations for the base classifiers analyzed in our experiments nearly identical so as to isolate the benefits gained solely from the probabilistic combination of classifiers with reliability indicators. we would expect that varying the representations  i.e.  using different feature selection methods or document representations  would only improve the performance as this would likely decorrelate the performance of the base classifiers.
　we selected four classifiers that have been used traditionally for text classification: decision trees  linear svms  na： ve bayes  and a unigram classifier.
　for the decision-tree implementation  we employed the winmine decision networks toolkit and refer to this as dnet below . dnet builds decision trees using a bayesian machine learning algorithm  1  1 . while this toolkit is targeted primarily at building models that provide probability estimates  we found that dnet models usually perform acceptably on error rate. however  we found that the performance of dnet with regard to other measures is sometimes poor.
　for linear svms  we use the smox toolkit which is based on platt's sequential minimal optimization algorithm. after experimenting with a binary and a continuous model  we used a continuous model as it seemed to perform at approximately the same level.
　the na： ve bayes classifier has also been referred to as a multivariate bernoulli model. in using this classifier  we smoothed word and class probabilities using a bayesian estimate  with the word prior  and a laplace m-estimate  respectively.
　the unigram classifier uses probability estimates from a unigram language model. this classifier has also been referred to as a multinomial na： ve bayes classifier. probability estimates are smoothed in a similar fashion to smoothing in na： ve bayes classifier.
1.1 basic combination methods
　we performed experiments to explore a variety of combination methods. we considered several different combination procedures. the first combination method is based on selecting one classifier for each binary class problem  based on the one that performed best for a validation set. we refer to this method as the best by class method.
　another combination method is based on taking a majority vote of the base classifiers. this approach is perhaps the most popular combination methodology. when performing a majority vote  ties can be broken in a variety of ways  e.g.  breaking ties by always voting for in class . we experimented with several variants of this method  but we only present results here for the method which relies on breaking ties by voting with the best by class classifier because it nearly always outperformed the other majority vote methods. we refer to this method as majority bbc.
1.1	hierarchical combination methods
stacking
finally  we investigate several variants of the hierarchical models described earlier. as mentioned above  omitting the reliabilityindicator variables is equivalent to stacking  1  1 . we refer to these classifiers below as stack-x where x is replaced by the first letter of the classifier that is performing the metaclassification. therefore  stack-d uses a decision tree as the metaclassifier  and stack-s uses a linear svm as the metaclassifier. it should be noted that stack-s is also a weighted linear combination method since it is based on a linear svm and uses only the classifier outputs.
　it can be problematic to learn the weights for an svm when the inputs have vastly different scales  in addition it may not be possible to pick good weights ; therefore  we normalize the inputs to the metaclassifiers to zero mean and unit standard deviation. in order to perform consistent comparisons  we perform the same alteration for the metaclassifiers using dnet. we also give for one of the dnet variants the results without performing normalization; as would be expected the impact of normalization for decision-tree learners is relatively minimal  and has both positive and negative influences .
strive
similar to the notation described above  we add a letter to strive to denote the metaclassifier being used. so  strive-d is the strive framework using dnet as a metaclassifier. for comparison to the stacking methods  we evaluate strive-d and strive-s. normalization is noted in the same way.
　the experiments reported here use a total of 1 reliability indicators  including those specific examples given in section 1 . these variables were simply our initial pass at representing appropriate information. in the future  we intend to publish an analysis of which variables are most useful  in addition to extending the set of variables currently employed.
1 performance measures
　to compare the performance of the classification methods we look at a set of standard performance measures.	the f1 measure  1  1  is the harmonic mean of precision and recall where  and	.
we can often assess a cost function in classification settings that can be described as where is the cost of a false positive classification and is the cost of a false negative classification. the most commonly used function in the literature is the error rate which is . however  the importance of varying cost functions has been recognized by many researchers because applications rarely have equal costs for different types of errors . in order to assess how sensitive performance is to the utility measure  we considered results for	and	.
　in addition  we computed and displayed a receiver operating characteristic  roc  curve  which represents the performance of a classifier under any linear utility function . we report results on the area under the roc curve as an attempt to summarize1 the linear utility space of functions.
　for each performance measure  we can either macro-average or micro-average. macro-averaging  which we present here  is computed separately for each class and then arithmetically averaged; this tends to weight rare classes more heavily. micro-averaged values are computed directly from the binary decisions over all classes; this places more weight on the common classes. we evaluated the systems with both macro and micro averaging  but because of space limitations  we simply note that our analysis is consistent with the micro results and omit presentation of the full analysis.
1 experimental methodology
　as the categories under consideration in the experiments are not mutually exclusive  the classification was done by training binary classifiers  where is the number of classes.
　decision thresholds for each classifier were set by optimizing them for each performance measure over the validation data; that is  a classifier could have different thresholds for each of the separate performance measures  and for each class . this ensures that the base classifiers are as competitive as possible across the various measures.
　to generate the data for training the metaclassifier   i.e.  reliability indicators  classifier outputs  and class labels   we used five-fold cross-validation on the training data from each of the corpora. the data set obtained through this process was then used to train the metaclassifiers. finally  the resulting metaclassifiers were applied to the separate testing data described above.
1 results
　tables 1  1  and 1  present the performance results over the three corpora. in terms of the various performance measures  better performance is indicated by larger f1 or roc area values or by smaller values. the best performance  ignoring bestselect 
in each column is given in bold.
　to determine statistical significance for the macro-averaged measures  a one-sided macro sign test and macro t-test was performed . differences with a p-level above 1 were not considered statistically significant.
　we do not explicitly report significance results for the t-test comparisons; instead  our analysis follows the macro sign test which yielded more conservative comparisons  i.e.  the t-test primarily just increased the number of differences found to be significant in the tables .
　the classifier combinations are annotated to indicate the results of a macro sign test. a indicates the method significantly outperforms  at the 1 level  the best base classifier. in addition  on the variants of stack and strive  a indicates the method outperforms the basic combination methods. results for the remaining sign test comparisons are omitted.
1 discussion
　first  we note that the base classifiers are competitive and consistent with the previously reported results over these corpora  1 

 we note that the area under the curve is not a precise summary of the linear utility space.
methodmacro f1errorc 1 c 1 roc areadnet11111smox11111na： ve bayes11111unigram11111best by class1111n/amajority bbc1111n/astack-d  norm 11111stack-s  norm 11111strive-d11111strive-d  norm 11111strive-s  norm 11111strive-d  norm  omit smox 11111bestselect1111n/atable 1: performance on msn web directory corpus
methodmacro f1errorc 1 c 1 rocdnet11111smox11111na： ve bayes11111unigram11111best by class1111n/amajority bbc1111n/astack-d  norm 11111stack-s  norm 11111strive-d11111strive-d  norm 11111strive-s  norm 11111strive-d  norm  omit smox 11111bestselect1111n/atable 1: performance on reuters corpus1  1  1  1  1 . the results reported for reuters are not directly comparable to those reported by yang & liu  as they report results over all 1 classes and do not give a breakdown for the 1 most frequent categories. furthermore  the fact that the linear svm smox tends to be the best base classifier is consistent with the literature  1  1  1 .
msn web directory
examining the results for the msn web directory corpus in table 1 reveals several points. first  the basic combiners have only one significant win  c 1  for the majority bbc approach. the results directly support the idea that a very good learner  smox  tends to be brought down when combined via a majority vote scheme with weak learners; in addition  the win most likely results from the fact that the base learners  other than smox  have a tendency to predict positive. when false negatives are weighed more heavily  the shift toward predicting positive helps reduce the number of false negatives.
　next  we see that stacking posts several significant wins and appears to have some advantages over the base classifiers. however  the stacking combination shows little significant improvement over the basic combination methods.
strive-d and strive-s  norm  show advantages that are robust across a variety of performance measures. each shows a small  about 1% error reduction  but consistent improvement across a variety of performance measures. when viewed in terms of the approximate ceiling as established by the bestselect model  the error reduction provided by the strive combination methods is an even greater portion of the total possible reduction.
　as can be inferred from the sign tests  these results are very consistent across classes. for example  on roc area  strive-d beats the base classifiers and basic combiners on 1 classes  and it beats the stacking methods on 1 classes. the notable exception is the performance of strive-s  norm  on roc area; graphical inspection of the roc curves suggests this result is likely based on too much weight being placed on the strong classifier for a curve.
　often  there is a crossover in the roc curve between two of the base classifiers further out on the false-positives axis. most utility measures in practice correspond to the early part of the curve  this depends on the particular features of the given curve . smox as a metaclassifier sometimes seems to lock onto the classifier that is strong in the early portion of the curve and loses out on the later part of the curve. since this portion of the curve rarely matters  one could consider using an abbreviated version of curve area to assess systems.
methodmacro f1errorc 1 c 1 rocdnet11111smox11111na： ve bayes11111unigram11111best by class1111n/amajority bbc1111n/astack-d  norm 11111stack-s  norm 11111strive-d11111strive-d  norm 11111strive-s  norm 11111strive-d  norm  omit smox 11111bestselect1111n/atable 1: performance on trec-ap corpus

figure 1: the roc curve for the health & fitness class in the msn web directory corpusin figure 1  we can see that the two strive variants dominate the four base classifiers. in fact  strive-d dominates  i.e.  its quality is greater than any other curve at every point  most of the msn web directory corpus. we also can see  note the truncated scale  the base classifiers catching up with strive-s  norm  on the right side of the curve. they  in fact  do surpass it. as a result  strive-d is usually a more appropriate choice if the utility function penalizes false negatives significantly more heavily than false positives.
　referring back to figure 1  we can understand why the decision tree is more appropriate for tracking crossovers in some cases. in this case  it is establishing a score region for smox and a score region for dnet where the reliability indicators give further information about how to classify an example.
　finally  we note that when we omit the base classifier smox in strive-d  norm  omit smox   the resulting combination improves by a large margin over the base methods; however  the resulting classifier generally still fails to beat smox. this suggests that there are not enough indicator variables tied to smox's behavior  or alternatively  that the other classifiers as a group behave like smox  rather than classify in a complementary fashion.
reuters and trec-ap
the results for reuters and trec-ap in tables 1 and 1 are consistent with the above analysis. we note that the level of improvement tends to be less pronounced for these corpora.
1. future work
　we are excited about the opportunities for probabilistic combination of multiple classifiers with reliability indicators. we are pursuing several research directions. foremost  we believe that a functional search that generates and tests a larger number of reliability indicators could provide valuable sets of informative reliability indicators.
　in the experiments above  the classifiers were specifically held constant in order to distill the effect of indicator variables. in future studies  we will allow representations to vary to induce more variety among the base classifiers.
　we are also interested in exploring the use of other classifiers as metaclassifiers. the metaclassifier should be a classifier that handles correlated input well  e.g.  use of maximum entropy  as classifiers performing better than random will be necessarily correlated.
1. summary and conclusions
　we reviewed a methodology for building a metaclassifier for text documents that centers on combining multiple distinct classifiers with probabilistic learning and inference that leverages reliabilityindicator variables. reliability indicators provide information about the context-sensitive nature of classifier reliability. we reviewed several popular text classification methods  and described several combination schemes. we introduced the strive methodology that uses reliability indicators in a hierarchical combination model and reviewed comparative studies comparing strive with other combination mechanisms. the empirical evaluations support the conclusion that a simple majority vote in situations where one of the classifiers performs strongly can  water down  that classifier's performance. the experiments also show that stacking and strive seem to provide robust combination schemes across a variety of performance measures.
acknowledgments
we thank max chickering and robert rounthwaite for their special support of the winmine toolkit  and john platt for advice and code support for the linear svm classifier.
