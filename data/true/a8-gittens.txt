timing-related defects are among the most difficult types of defects to catch while testing software. they are by definition difficult to reproduce and hence difficult to debug. not all components of a software system have timing-related defects. for example  either a parser can analyze an input or it cannot. however  systems that have concurrent threads such as database systems are prone to timing-related defects. as a result  software developers must tailor testing to exploit vulnerabilities that occur because of threading. this paper presents the focused iterative testing  fit  approach  which uses a repetitive and iterative approach to find timing-related defects and target product areas with multithreaded characteristics by executing system tests with a multi-user test suite.
categories and subject descriptors
h.1  information systems applications : systems- concurrency distributed databases; d.1  software engineering : testing and debugging-testing tools
general terms
reliability  verification
keywords
testing  database management systems  multi-threaded
1. introduction
　ibm r db1 r for linux r   unix r   and windows software  is an example of a complex distributed  multiprocess  and multi-threaded database management system  dbms . it consists of several million lines of source code. after an outage  when the operating system and dbms restarts  the dbms has to replay logs of the previous database  db  activity so that there are no partial transactions and so that atomicity  consistency  isolation  and durability
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
dbtest'1  june 1  vancouver  bc  canada copyright 1 acm 1-1-1/1 ...$1.
 acid  requirements are met to keep the database in a consistent state. however  in a multi-threaded  multi-process system  small timing holes 1 often exist and elusive pointin-time defects can occur. the point-in-time defects are elusive because when such an unexpected event occurs  the logs must capture concurrent events and interleave them in the manner in which they occurred so that states are repeated as they occurred previously and together. within this context  the db1 software quality assurance team varied the test approaches in several ways to trigger point-in-time  timingrelated  problems. these methods attempted to simulate the unexpected external issues common to databases and included:  1  varying the processor load by running an external program to consume most of the cpu cycles available to the dbms server;  1  instrumenting code to selectively slow down execution with logging overhead;  1  changing priorities of processes; and  1  iteratively executing commands or programs with a background workload.
　the main contribution of the work presented here is therefore a testing methodology with automation for complex multi-threaded dbms software that iteratively executes commands or programs with a background workload  since that was the most successful approach. the first three methods found defects by burdening the system resources with the various kinds of overhead mentioned in methods 1  1 and 1. these methods however  still executed limited sequences of events against the db  and though the additional overhead deprived the events of resources  the large set of execution possibilities  the state space  could not be representatively sampled. the iterative approach randomly samples every combination of the sequences of possible events  triggering timing-related defects  trds  that may occur in a production-level user environment. method 1 is therefore more representative of realistic usage than 1 and 1.
　the highly automated approach is applicable not only to databases  but to any multi-threaded  multi-process  multiuser server software system prone to timing holes. in particular  it can help to sample the complex state space of any systems with logging  locking  and/or latching. examples of other applications for the fit approach beyond databases are messaging systems such as ibm websphere r mq  journaled file systems such as jfs in the ibm aix r environment  and enterprise service bus systems such as
websphere message broker. however  we discuss the dbms

context here with db1 as an example.
　the paper continues as follows. section 1 reviews work related to testing trds in multi-threaded dbms applications. section 1 introduces the aspects of three facilities of db1 software that make monitoring  fast communication manager and crash recovery suitable for testing with this method. the paper then presents the methodology for fit in section 1. this section includes a description of the tool support. section 1 summarizes the results of using the approach. section 1 provides a summary of conclusions and potential directions for future work.
1. related work
　testing for the dbms is normally done under standard testing approaches varying from unit to system testing. all of these methods have been applied successfully to the dbms with several discussions and work in the more general applications. however  as databases grow larger and more distributed  the timing hole has been omitted by most of the more general methods. point-in-time interleaving of states on the stack causes transient problems  and to encounter these problems in the various combinations and permutations of states  testing with a statistical focus must be done. random testing has proven itself cost effective and more powerful than would be thought a priori. in his text also called random testing  hamlet discusses the perception of random testing as haphazard testing  done hastily and poorly. he consequently explains the correct meaning of random testing where test cases are chosen with no interrelationship. this gives statistical independence to the test points that endows statistical significance to the testing results and prediction of the expected quality of the software. the literature on random testing of dbms falls into the category of randomly generating inputs for the dbms tests  such as the evolutionary development of queries in recent work by bati et al.  where the researchers create new queries by mutating and synthesizing queries  and determining whether further queries can be generated. although a useful approach yielding new defects  the approach does not address the elusive timing hole.
　outside of the dbms domain other approaches to testing that handle multi-threading have existed for several years. these methods use model-checking approaches to handle the interleaving of concurrent processes and the unexpected interactions. however  at the time when the fit method was developed there was not adequate discussion of modelcheckers that could handle this state explosion  especially with the multiplicity of states possible in a dbms with tens of millions of lines of code with several interacting components. one available algorithm  proven on programs up to 1 lines of code  limits the state space to be searched to test a multi-threaded application. they use the context switches that occur when a thread temporarily stops execution and a different thread starts  and systematically or iteratively suspends or binds execution of the thread at some random or arbitrary point to allow other more interesting threads to continue. one of the benefits is that the total number of executions in a program is polynomial and it makes it theoretically feasible to scale systematic exploration to large programs without sacrificing the ability to go deep into the state space. however  that theory was unproven. after we developed fit and after the work by qadeer  we noted work by mutilin  which has been found to provide exhaustive exploration of the execution paths in javatm pathfinder  jpf  using localization of the search performed by jpf. this localization reduces the state space while automatically checking the correctness of the program. in our future work we will investigate the applicability of these methods to our context and its comparison to fit.
　sen  has very recently investigated  partial-order random testing approaches that choose thread schedules at random. this approach  though yielding more defects than a nonspecific random set of tests  and demonstrating that it is useful to detect the exceptions faster than the nonspecific set of tests  was only demonstrated for three small multithreaded programs from the java pathfinder distribution. these do not compare to complex interleaving of a large dbms such as db1. we cannot however say  that extrapolation and repetition of the runs would not simulate a similar execution to db1  but since this work is recent this question will have to be explored as future work. in addition  these methods such as the one by sen did not exist when we sought to meet the trd challenges. therefore  based on the existing work at the time  we found that random testing was most suitable to test trds in a complex multi-threaded dbms application. we therefore created the approach presented here.
1. the software under study and the components of interest
　fit works with how certain functionalities are implemented in a multi-threaded system such as a dbms. we explore the algorithms behind db1 monitoring  fast communication manager  and crash recovery components and why this iterative approach is particularly productive here.
1 monitoring
　the database monitor stores collected information in monitor elements. each monitor element stores information about one aspect of the state of the db. monitor elements collect data for one or more logical data groups. a logical data group is a collection of monitor elements that gathers db monitoring information for a specific scope of db activity. monitor elements are sorted in logical data groups based on the levels of information they provide.
　snapshot monitor is one way that db1 software presents element values. snapshots provide a point-in-time picture of the db state. during snapshot processing all relevant levels are read to complete the snapshot. as a result  for a db snapshot  the following events occur.  1  read element values from the application-level structure. this will contain values for all terminated applications since the dbms activated.  1  iterate through each application currently connected to the db. for each of these:  a  read the element values from the application level structure. this structure contains values for all agents that have disassociated from this application.  b  iterate through agents currently associated with the application. for each of these read the element values from the agent-level structure. in the snapshot output  the rowsread element will contain the total from all these levels. event monitors are a second facility with which db1 software makes element values available. event monitors provide a real-time trigger-based monitoring capability. the event monitor infrastructure buffers records  using two internal buffers  before writing them to disk.

figure 1: generating event records for a statement event monitor
　figure 1 illustrates how event records are generated for a statement event monitor. three applications are connected to the db  each having a single agent working on its behalf. as each application executes sql statements  their agents generate new statement event monitor records and insert them into the active buffer  which is buffer 1. when buffer1 has filled up  a message is sent to the event monitor writer instructing it to process all records in buffer 1. during this period  the  active  buffer is switched from buffer 1 to buffer 1 and the three applications will begin filling up buffer 1. the fast communication manager  fcm  event monitor writer  having received the message  processes buffer 1 and inserts the data into files  if the event monitor is a file event monitor; or a named pipe  if the event monitor is a pipe event monitor; or sql tables  if the event monitor is a table event monitor.
1.1 opportunities for fit
　one of the challenges facing monitor processing comes from the transient nature of the memory it needs to read. busy systems can find applications constantly starting up or terminating. this results in application memory being allocated and freed. moreover  during the processing of a snapshot  agents may be in the process of either associating with applications or disassociating from applications. at the start of snapshot processing  an agent may be working on behalf of one application but by the end of the snapshot processing  it may be working on some other application. to protect the monitor operation acid properties  and in particular the monitor action consistency and isolation  monitor processing locks resources. resources are locked before the resource can be accessed  and the lock protects the integrity of the monitor data and ensures that memory does not  disappear  while being read  resulting in crashes . locking of a resource involves acquiring a  latch   which is an internal mechanism for controlling concurrent events and the use of shared system resources. the protocol surrounding the locking of resources is strict  and resources must be locked and unlocked in a certain order and monitor processing must adhere to such protocols. if resource locking protocols are broken  the system can  hang   so extra care must be taken to ensure that protocols are followed.
this requirement for ordering of events and the strict requirements of the protocols means that timing problems are more probable. it also results in a requirement to test these protocols by concurrently to increase the probability of performing locking and unlocking events out of sequence. the fit method  which samples several event combinations and introduces resource constraints just as in a locking situation iterates through such probabilistic populations of events.
　multi-partition instances in the data partition feature  dpf  environment present monitor processing with other challenges. the activating or deactivating of event monitors requires the coordination of activity across all partitions. this involves sending messages to all partitions  waiting for all the partitions to perform the activation or deactivation and respond with success or failure  and coordinating the replies. in addition  global snapshot processing requires similar coordination. messages must be sent to all partitions  snapshots executed locally on each partition with output sent in replies to the messages  and then replies merged into a single snapshot output stream. this processing must prove resilient to partitions activating and deactivating and dropped messages. additionally  extra care must be taken to ensure that resources are not locked on one partition while messages are sent to other partitions. failure to ensure the coordination of the deactivation and activation  as well as locking and messaging transmission  may result in severe software defects. in a threaded environment  the iterative approach in fit creates a sample of situations where such interleaving and order can be disturbed.
1 fast communication manager  fcm 
　intra-query parallelism or smp  symmetric multi-processor configuration  facilitates parallel execution of activities by the dbms. smp configuration works by generating sql execution plans whereby portions of an sql statement are divided into individual sections  which can be executed concurrently and independently by multiple processes/threads. a second type of configuration is data partitioning feature  dpf . this configuration allows for the partitioning of data across multiple db1 nodes. each db1 node is responsible for managing one data partition. the architecture where each db1 node and its associated data partition are independent from other partitions is commonly referred to as  shared nothing . it permits function shipping whereby sql and non-sql operations are directed to those nodes where the target data is held for local operation. where multiple data partitions are involved  parallel processing occurs  with each db1 node only working with its subset of data . smp and dpf can also be combined within the same instance of db1 software. both these configurations require fast and efficient internal communication facilities. a user may configure a db1 instance with multiple nodes residing on the same host machine. such configurations are described as multiple logical nodes  mlns . in such configurations  communication between db1 nodes residing on the same host occurs through shared memory.
　the fast communication manager component includes fcm resources  fcm receiver and sender conduits  connection management and node failure support. fcm resources are allocated from a separate shared memory segment that is allocated at start-up time by db1. the two main fcm resources are buffers - which store communication data - and channels - which are the terminal points in communications. each node on a dpf instance will have at least one fcm receiver conduit for incoming messages and one fcm sender conduit for outgoing messages. connections are established on demand with connection management. the first indication of user activity on a node drives fcm to initiate communication with every other node configured in the instance. this ensures optimal performance and security of inter-node communication. node failure support involves interrupting applications with dependencies on nodes that have lost their connection to the system and cannot be contacted. with the fcm node failure-recovery facility  applications process node failures asynchronously.
1.1 opportunities for fit
　there are aspects of shared memory  failure recovery  connection management and conduit management that may create opportunities for challenges due to unusual circumstances such as frequent system interruptions  frequent reconnections and unusual resource deprivation. an example of this is monitor running with fcm. with fcm  the single shared resource pool is created on each host on the mlns to facilitate communication between logical nodes. in a multiprocess environment  the interleaving of events may compete for memory. fcm  with the conduits establishing their own connections and resulting connection management  is designed to handle this interleaving adequately. in the case of node failure  and other unexpected events however  the probabilities of unexpected and sometimes incorrect interleaving may be increased. in this case  fit can deprive resources and increase the samples of code failure events with memory management and connection management choices made by fcm. this will increase the probability of finding trds in fcm.
1 crash recovery
　the third db1 software feature suitable for testing with fit is the crash recovery feature. since units of work on a db can be interrupted unexpectedly  if an interruption occurs before all of the transactions in the unit of work are completed and committed  the db is left in an unusable state. crash recovery moves the db back to a consistent and usable state by rolling back incomplete transactions and completing committed transactions still in memory. transaction failures result from conditions that cause the db or the db manager to end abnormally. partially completed units of work that were flushed to disk at the time of failure leave the db inconsistent. following a transaction failure  the db must be recovered. conditions that may result in transaction failure include a power failure  causing the db manager and the db partitions to end abnormally; a hardware failure such as memory corruption  or disk  cpu  or network failure; or a serious operating system  os  error that causes the db1 application to end abnormally.
1.1 opportunities for fit
　the conditions mentioned above that lead to transaction failure can create vulnerabilities and timing issues that should be found in testing. order dependency is important because logs of the events that ran earlier must be replayed either to roll back partial transactions or complete uncommitted transactions in memory. the same issues arise because of parallelism and the need to replay logs in correct sequence. fit exploits the sequencing vulnerabilities by sampling from a large number of sequence possibilities. in addition  transactions are logged while they occur  whether or not the transactions are committed. transactions go from the log buffer to log files  transactional logging  before any data is written from the buffer pools to the db structures. challenges can occur again in a multi-threaded environment because of the interleaving of events and the need to separate a given sequence when a problem occurs. this is a standard protocol  but problems can only be revealed with mass repetition of such logging of parallel processes. fit tools can execute a large number of iterations of runtime and recovery scenarios  and increase the probability of finding defects that occur during a particular sequence of events.
1. methodology
　the fit approach hinges on repetition and resource deprivation  and as a result  automation is vital. the methodology presented is useful to those with large multi-process  multi-threaded software testing concerns  with vast combinations of possible executions and resource constraints that are likely to trigger problems. note that because of the nature of timing related defects  even if iteration n1 is executing exactly the same command as iteration n1  it is doing so with the db in a different state so even if iteration n1 was successful  iteration n1 may fail and run into a defect. the approach is run as a number of controlled iterations on any machine and os combination. the iterations proceed with the following steps.
step 1: run random concurrent dbms test suites in the background to stress the supporting hardware  os and dbms  while varying the configuration parameters of the db  the size of the db  the os  and the nature of the test suite being monitored  for example  with a workload that tests monitoring functionality such as snapshot  crash recovery functions  or the fast communication manager .
step 1: the essence of this step is to kill the dbms processes. this can directly trigger fcm and crash recovery type defects only  with monitor defects being revealed as a side effect. for crash recovery  deliberately crash the dbms server by issuing a kill signal to the os. the crash has a by-product effect that helps to reveal monitor defects since the sequence of the logs from the recovery of the crash may reveal incorrect interleaving. for fcm testing  kill only a subset of nodes. for example crash one or more of the logical or physical partitions during workload execution. for pure monitor testing we do not crash the dbms server. step 1: restart the dbms server and the db. for fcm testing only the subset of killed nodes need to be restarted. for pure monitor testing we do not need to restart the dbms server since we did not kill it in step 1. step 1: check for data integrity problems  dbms crashes  traps   and dbms hangs.
step 1: if any problem is found  then exit and notify the tester via electronic mail alert. else repeat from step 1.
　supporting tooling was created to run several parallel processes and vary parameters. these tools are available for both unix and windows platforms and allow the user to control the number of iterations. for monitor testing there are tools to execute the algorithm for the approach above and stress the monitoring function heavily by using the command line processor interface in one tool and the db1 application-programming interface  api  in another tool to invoke the snapshot and the event monitor functions. for crash recovery testing there is a tool to run the algorithm with several thousand crash recovery iterations by deliberately killing the db1 instance on all partitions  nodes  and then restarting the dbms. the tool for fcm testing is similar to the tool for crash recovery testing but it varies fcm parameters during each iteration and only kills the instance on a subset of the partitions  nodes . this allows one to test if the fcm code can reliably handle node failure and not bring the db1 instance down.

figure 1: defects for timing-dependent components found in system testing and by customers as an example of the defect discovery occurring between the introduction of fit in version b and beyond
1. results
　after applying the fit method  defect detection improved significantly  and increased tester productivity. automation was key to the approach since the fit tools were executed in scenarios with multiple databases with concurrent test suites running for extended periods  for example  overnight .
　figure 1 shows the general trend for the three components in the db1 product where fit was used. the areas indicated by the solid circles  versus the dotted circles  show the increase in severity 1 and severity 1 defects found in system test versus by the customer after fit in version b. severity 1 defects cause the system to become unavailable  and severity 1 defects cause problems that hinder work but may be worked around. in addition  the ratio of customer defects versus system test defects decreases. that is  after fit  testers find more defects and customers find fewer. figure 1 shows the ratio between defects found in testing in the components before  version a  and after  version b and on  fit; and the defects found by customers.
　the testing methods previously used included standard scenario-based testing and boundary value testing. they also included guided fault injection methods using instrumentation to stop execution at certain expected timing trigger points and insert certain inputs like bad data or known failure triggers to force the timing hole. this is similar in principle to the method presented in work by tai and carver   where a deterministic execution testing approach is proposed for concurrent programs by selecting a set of tests and forcing certain inputs if it is determined that those test inputs are feasible in a preset execution. if this deterministic execution is feasible and succeeds  then the output of the test set is checked to see if it has yielded a defect. these methods were not successful in our context largely because of the extreme volume of states to be considered in determinism. the insertion of the input is still on too infrequent and combinationally simple a scale  and was only effective in finding defects in unit and function testing  but not in testing at the system level.

figure 1: all defects for components most affected by timing issues as ratios between customer defects and test defects
　since the tool repeats the same steps nondeterministically with each iteration  the tool increases the probability of hitting the same defect over time. previously where trds occurred  the causes were difficult to record. this iterative method  using the same non-deterministic workload  increased the probability of hitting defects  therefore making it easier to reproduce the defects for debugging. additionally  the number of successful iterations before detecting a defect provided management with an objective quantitative measure of the quality of the software. the number of iterations is independent of cpu speed. as a result  the number of successful iterations differs from the execution time for a workload  which depends on the cpu speed. this led to the formal pre-shipping requirement of a minimum number of successful iterations.
　the approach was first applied to monitoring  and because of its success it was extended to crash recovery and fast communication manager. it is suitable to any areas of db1 software where timing-related concerns may exist. these are areas involving communications between different nodes  data transfer between nodes and monitoring of nodes.
1 fit overhead
　the mean number of iterations required to find a defect varies with time. the number is small at the start of the test cycle and grows towards the end. the number depends on the component under test. the ideal is infinity  where no defects are found and every iteration is successful. however  in reality  the principle of  good enough  reliability discussed in software reliability engineering is used  and test management decides on a particular target value for exiting the test cycle. the higher this target value  the higher the reliability of the component is deemed. there are two types of overhead for this technique:  a  extra tooling effort in automating the fit approach by making a fit tool and  b  extra computational effort of running a fit tool. for  a   the extra effort is mostly a one-time effort at the beginning of the test cycle. however  this tooling effort is small compared to the effort spent on the entire test cycle. for  b   the extra effort is negligible since most of the time of the fit tool is spent running the test and only a small amount is spent preparing for each new iteration.
1. conclusions
　the fit approach has many benefits. they included an increase in the number of timing-related defecttrds found in system testing as opposed to such discoveries by customers and a new objective measurement for the quality of the db1 timing-affected components that is independent of the platform on which the software runs. one of the side effects of the quantitative measure of quality is the ability to determine the mean number of iterations to failure for components tested with the fit approach. for crash recovery functionality  for example  this number has improved over tenfold since this method was employed and the measurement taken.
　the first major lesson is that the automation created to support fit is crucial because the large state space has meant that the ease in executing the algorithm has had many unexpected but welcome effects. for example  fit has been able to identify stability regression defects in real time during the development cycle  that is  whilst testing for build-to-build regressions. more specifically  during continuous crash recovery testing  when testing from one build to the next  there are sometimes regressions in both runtime and crash recovery testing. these are all rooted in recently integrated code that is intended to correct a defect or add new functionality. because of the existing automation and the ease of implementing the method  these build-to-build regressions were easily discovered.
　there was also a significant return on the initial investment to create the tools  since with the tools several workloads could be run simultaneously and easily  and left to sample the execution state space for crash recovery  monitoring  and fast communication manager. the tools would easily find new defects while running over many days. instead of requiring the previous tester time to explore the state space  the tools are left fishing for defects on their own. this is inexpensive. in addition  the fit approach revealed more defects when executed with very slow machines  on older hardware  for example   or on very fast machines  on newer hardware  for example . it is intuited that these environment characteristics deprive the system of resources and severely increase iterations possible.
　 build it and they will come  - this quote does not apply to testing tools. complicated tools  however useful  are left to gather dust. one of the other important points for fit beyond automation was the ease of use of its automation. if setup of tooling is complicated and running the tool is complicated  then it will likely be used sparsely in testing. the fit tools were carefully crafted to avoid such difficulty and have been intensely employed to validate the product. this has meant that many more defects have been found. the tooling has also been built so that one tester can easily set it running on many machines. moreover  the tool alerts the tester when a defect is found; hence the tester does not have to monitor the test systems continuously
　the underpinning factor has been the feasibility of this approach to test automation. this has resulted in returns that far exceed the investment. the future work with this approach will be to quantify its differences to new approaches with model-checkers and to extend it to additional areas of the db1 product and other multi-threaded programs.
