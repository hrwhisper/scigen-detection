in this paper  we describe the current techniques used in testing sql server analysis services  the challenges we face  and techniques we are currently looking into to improve the testing of sql server analysis services both in functionality as well as performance.   
categories and subject descriptors 
d.1  software engineering : software/program verification - correctness proofs  formal methods  validation.  
d.1  software engineering : testing and debugging - debugging aids  dumps  monitors  testing tools  tracing. 
general terms 
algorithms  measurement  performance  design  reliability  experimentation  languages  verification. 
keywords 
sql server analysis services  mdx  storage engine  formula engine  calculations. 
1. 	introduction 
microsoft sql server analysis services  ssas  is a multidimensional database and data mining platform. ssas 1 had a core set of multi-dimensional features  and with its ease of use became a market leader . the ssas 1 version was a significant leap from ssas 1 with architectural changes to handle broader scenarios as well as the ability to scale up better in middle tier scenarios. most customers re-designed their multidimensional databases to take advantage of ssas 1's new product features. the new ssas architecture  the addition of new features in ssas 1  and the combinations in which customer could use these features brought significant test challenges. 
testing is a key aspect of any software product development effort. this paper discusses the challenges we faced in verification and identifying functional and performance regressions while developing ssas 1 and ssas 1 along with proposed solutions we are currently considering/implementing to improve our testing and verification processes in the future.  
 
1.	architecture 
microsoft sql server analysis services  ssas  consists of a multi-dimentional database server and tools used to create and  manage databases. the multi-dimensional databases are stored on the server  referred to as the ssas engine. a multi-dimensional database consists of two major objects  dimensions  1  1  1  and cubes  1  1  1 . the ssas tools  integrated into the visual studio shell  help users model and create multi-dimensional databases that contain dimensions and cubes  1  1  1 . ssas client applications communicate with the ssas engine using xml for analysis  xmla    a standard application programming interface for online analytical processing  olap . xmla has two main soap methods  discover and execute. discover requests return metadata and data from databases. execute requests are used for creating  altering  deleting  querying  and processing multi-dimensional data  1  1  1  1  1 . statements used to create  alter  delete or process multi-dimensional databases are part of ssas's data definition language  ddl  . ssas also provides a set of object models that abstracts xmla  exposing an object model and making it easy for developers to build applications that can communicate to the ssas engine. 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
dbtest'1  june 1 vancouver  bc  canada. 
copyright 1 acm 1-1-1/1...$1. 
 
 
figure 1: ssas engine components 
1 the ssas engine components  
the focus of this paper will be testing the ssas engine. the ssas engine consists of five major components as shown in figure 1. they are:  
1.1 infrastructure 
　　the infrastructure component handles operations such as requests from clients  scheduling jobs  and memory management.  
1.1 data mining:  
　　the data mining component handles all data mining  1  1  requests such as data mining discover and dmx  1  1  queries.  
1.1 metadata manager 
　　the metadata manager handles the ddl statements that operate on the multi-dimensional database metadata  and is capable of handling concurrent requests.   
1.1 the storage engine 
　　the storage engine component  se  is used for populating multi-dimensional databases with data from relational databases and optimally storing them on disk. the se is also responsible for building indexes for the various objects to aid in fast and efficient retrieval of the data from disk at query time.  
1.1 formula engine 
　　the mdx query processor component  referred to as the formula engine component  fe  in this paper  determines an execution strategy for each mdx query  translates each query into a sequence of requests to specific se access methods and computes the results of the query based on any calculations defined for the multi-dimensional database.  
the se and fe are the core components of ssas's multidimensional database functionality. 
1 the ssas engine query execution 
 
figure 1: ssas query execution  
 
the ssas engine first parses client requests and routes them to the data mining  formula engine  or metadata manager component. figure 1 shows the query execution architecture for processing discover and mdx queries. an mdx query is first analyzed by the query parser and then passed on to the fe. the formula engine  fe  component evaluates the axes information first. the evaluation of the cell values is done by the calculation engine. if the query results are present in the fe's caches  they are retrieved returned back to the client. if the query results are not cached  fe sends appropriate internal queries to the se. the se has its own internal caches. the se evaluates if the query can be satisfied from its own caches and  if so  the se sends the cached results to the fe. if not  the se retrieves the results from partition data on disk  stores them in the se cache  and sends them to the fe. the fe does the final calculations  if needed  using the calculation engine and sends the results back to the client.  
1.	testing the ssas engine  
there are various aspects to testing software . database testing has unique challenges and several approaches have been proposed  1  1  1  1 .  testing the ssas multi-dimensional database also has various aspects such as functional verification  performance  stress  localization & globalization testing. testing the ssas multi-dimensional engine can be divided into two main areas. the first is the creation of the multi-dimensional database and loading it with data from the relational database  referred to as  cube processing  . this area primarily targets the metadata manager and the se components. the second area of focus is testing the various types of calculations supported by the ssas engine  specifically targeting the calculation engine in the fe component and verifying the results returned from ssas for mdx queries. in this paper we focus on the key challenges of functional and performance testing of the ssas engine. 
1 functionality verification 
to test the ssas engine  we use several in-house relational databases that include sample relational databases shipped with ssas 1 and ssas 1 products  as well as databases provided to us by our customers. we also utilize the multidimensional database sample shipped with ssas 1. these relational and multi-dimensional databases support a variety of multi-dimensional scenarios sufficient to validate all features supported by ssas. 
ssas clients communicate to the engine via xmla. hence  the tests used to validate the functionality of ssas engine are stored in xmla format. each ssas engine test case contains an xmla statement along with the expected result of the execution of that statement. a test author first validates the response for an xmla statement before storing it as the expected result in the test case. a test program  called the test executor  sends the xmla statement in each test case to the ssas engine for execution and compares the results returned from the engine with the expected results.  the xmla elements that are non-deterministic  such as timestamp values  are skipped when doing verification.  the test executor indicates if the test case passed or failed based on the expected results. the ssas engine testing process is shown in figure 1.  
1.1 database creation and processing  
the creation and processing of multi-dimensional databases involves sending ddl statements to the ssas engine.  testing this functionality involves exercising the various features against the different storage configurations of the multi-dimensional database: molap  rolap and holap. in the molap  multidimensional on-line analytical processing  configuration  both data and metadata are stored in the ssas database. in the rolap  relational on-line analytical processing  configuration  the meta-data is stored in the ssas database while the data is kept in the relational backend database. in the holap  hybrid on-line analytical processing  configuration  the metadata is stored in ssas database  the data is stored in the relational database  and pre-aggregated data is stored in the ssas database. the molap storage mode generally provides the best performance. the various supported features of the ssas engine are tested against these storage configurations. most of the test cases focusing on storage configurations are created with specific features and combinations that have been identified as being impacted by the storage configuration  having separate code execution paths based on the storage configuration used  and  most importantly  are parts of key customer scenarios. in order to verify that data is loaded correctly into the multi-dimensional database  a few simple mdx queries are executed and expected results are verified.  
 
 
figure 1: ssas test cycle 
1.1 mdx calculations and queries 
mdx calculations supported by the ssas engine and mdx query results are verified by sending appropriate mdx queries targeting appropriate features. calculations in ssas engine can be defined as part of ddl such as custom rollups  1  1  1   unary operators  1  1  1   or as part of mdx scripts  1  1  1 .  an mdx script can consist of calculations on dimensions such as named sets or calculated members on dimensions; also calculations on the cells of a cube . a cell in a cube is the intersection of a member in the measures dimension and member of each hierarchy in the each dimension . several calculations can be applied on a specific cell and hence ssas engine specifies a list of rules on how calculations will be applied to a cell. in order to verify the various mdx calculations  tests are created which focus on each specific calculation along with appropriate mdx queries to ensure that calculation rules expected from the ssas engine are being followed.  
 
once test cases are created  they are added to the test automation suites. the automated tests will be executed in many diverse contexts such as different operating systems  e.g.  windows xp  windows server 1   hardware architectures  x1  x1 and ia1   supported locales  e.g.  japanese  german  and product sku's  e.g.  sql server standard edition  sql server enterprise edition .  for most of these configurations  the expected test results from the ssas engine are the same. hence  the expected results verified by the automated tests are the same.  thousands of test cases are executed on every run  testing the different areas and features of the product.  we have observed that in most of the test runs there are at least a few test failures due to unexpected changes in responses or behavior of the product.  when this happens  the reason for the change in query results is investigated and the test is updated if needed.  if a test failure is ruled out  a product bug is reported. 
1 testing performance  
one way of evaluating performance of a product is to use standard benchmarks. for example  to evaluate the performance of the relational databases  benchmarks defined by the transaction processing council  such as tpc-c  tpc-h performance benchmarks are typically used. however for multi-dimensional database there  aren't any specific industry wide benchmarks. there are various aspects to testing the performance of the ssas engine. they are: cube processing  dimension processing  single user query performance and multi-user query performance testing. the ssas engine is also tested for processing scalability as well as query performance. for the purpose of this paper  we will focus on how we test single user query performance. 
the core benefit of a multi-dimensional  or olap  database is its ability to provide results fast in order to make business decisions . the ssas engine is known for its molap performance. most ssas customers use the molap configuration for this reason. ssas engine compresses the cube data in molap storage and hence it is critical to evaluate it's performance. when evaluating single user query performance of the ssas engine we use a combination of customer queries as well as mdx queries that have been created to focus on specific key customer scenarios.  when testing a specific version of ssas in development  we verify that new customer scenarios have desired performance  as well as making sure that existing customer scenarios do not exhibit performance degradation. we use customer queries to help us test customer scenarios and to identify any performance improvements or degradations early in the development cycle. this approach has interesting challenges which we will discuss in the next section.  
to test single user query performance we use a different test executor program than the one we use for functional testing. performance test results need to be consistent and repeatable and hence we use standardized machines. sets of related queries along with the corresponding multi-dimensional database data are combined to form a query group. for each query group the test executor configures the ssas engine  restores the database from a backup file  executes the set of queries twice  and records the measured query execution time in a database. we execute the query group a second time in order to evaluate the performance impact of caching on ssas engine. 
an olap cube has been built on top of the query performance results. an excel report from this cube  is used to compare the performance results of current product binaries with previous product milestones and releases  called baselines  and identify performance regressions. when generating baseline measurements  the queries are executed several times to identify the mean and standard deviation of the query's performance over multiple executions of the tests. this gives us an indication of the stability of our test bed. 
if the query execution time of the current version of the product is slower than its baseline  taking the variance into consideration  the query is considered to have regressed in performance. in addition to analyzing the performance of each individual query we also evaluate the performance of the query group. individual queries may have performance degradations but there might be an overall performance improvement in the query group as a whole.   depending on the magnitude of the degradations/improvements  this might be acceptable behavior.  
customer queries alone are not sufficient to test ssas performance since they will normally not cover the entire set of features which need to be tested.   in order to cover this deficiency  we add tests that specifically target product features to our performance test suites. 
 
1. 	challenges in testing ssas  
the ssas 1 product was a complete re-design of the ssas 1 product.  hence the ssas 1 product had unique testing challenges due to the new architecture and new features. in addition to testing various aspects of each feature  we worked closely with our customers to test the integration of features. we also re-designed our in-house ssas 1 databases to take into account the new ssas 1 architecture when evaluating the performance of ssas 1. ssas 1 added new features but the underlying architecture of the product itself was similar to that of ssas 1. in this section we highlight the key challenges faced when testing ssas 1 as well as ssas 1.  
testing ssas is different than testing a relational database. the multi-dimensional space in ssas and querying the multidimensional space using mdx where a typical cube has sparse data poses a unique set of challenges. a typical mdx query will request cell data sliced by multiple dimension axes members. the ssas engine uses a heuristic-based approach to pre-fetch data based on the selectivity ratio of dimension members as well as the complexity of mdx calculations used for dimension members and cell values. below are some of the key aspects of the ssas engine that makes functional and performance testing unique and challenging as compared to relational databases.  
 
1. ssas provides ease of modeling business requirements through the mdx query language. mdx queries typically result in highly correlated but deeply nested sub-queries that are not common in relational databases. typically executing such queries in relational databases can take a significant execution time as compared to ssas. 
1. built-in support for features such as recursive parentchild relationships complicates the database schema  but makes possible various mdx query execution strategies. each mdx query results in an execution plan being evaluated. small differences between queries can result in vastly different physical plans being chosen by the rule-driven optimizer.  
1. mdx calculations are implicitly recursive in nature unlike sql. as a result  the query plan space becomes prohibitively large and impractically expensive for exploration by a standard cost-based query optimizer. hence  a heuristic-based approach is used for determining the query execution path.  
1. implicit natural joins get applied to all mdx queries and expressions. many calculations may be applied to a specific cell. there are complex pattern matching rules that are applied when there are multiple calculations for a specific cell.   
1. the declarative calculation aspect of the mdx language associates expressions to large cartesian products of dimensions. this easily explodes the amount of work the engine needs to do for a query as the number of dimensions and attributes of a database increases.  
1 functional verification 
1.1 database creation and processing 
one challenge in functional testing of ssas ddl statements is that the various combinations of features of ssas engine that can be used together is quite large. we find that customers use combinations of features in ways that the features or combination of features were not intended to be used. this situation indicates that we have test coverage holes. 
1.1 mdx calculations and queries 
the challenges in testing mdx queries and calculations are similar to those of testing ddl statements. the combinations of features in ssas are quite large and generating corresponding calculations and mdx queries to test them is a large effort. hence  the current approach of generating test cases for specific features  and most important feature combinations doesn't give us complete coverage. for mdx queries and calculations  the ssas engine can take various internal code paths such as optimized or not optimized at several stages of internal query execution based on various conditions. this provides a unique testing challenge - a single mdx query can be evaluated in multiple ways depending on context. in the current approach only the default query execution path is tested.  
1 performance testing 
 the combination of various supported features in ssas and the various mdx queries that can be generated by clients is quite large. the current approach of ssas engine performance testing uses customer queries and queries for certain targeted features. the approach of gathering customer queries to measure performance can lead to test holes  redundant testing  and is not scalable. ssas has a very large customer base. all customers are not able to provide their databases and queries due to limitations of their corporate policy  and  even if they could  we wouldn't have the resources to test them all . hence  customer queries in the test system contain only a subset of the features  ssas customers use in their applications.  this approach also makes it time consuming to debug and fix issues when there are performance regressions. for most customer query performance regressions  code profiles need to be obtained to identify the root cause.  this is time consuming. in addition  customer queries can potentially contain multiple queries which exercise the same functionality.  
currently  we have very few performance queries that target specific features of the product. hence  test coverage on all ssas features and combinations is not available in the current suite of tests. even if there are queries targeting combinations of features which are added to the performance suite  caching effects between queries can potentially result in performance degradation which needs to be tested. even small changes to the product can result in changes in query results which pose a challenge in how performance queries are measured and evaluated. changes to cache can impact the queries that get executed afterwards.  
the current approach of performance testing  using customer queries  has helped in measuring performance for sql server 1  and in finding the root cause when performance degradations are found in test runs. however  due to the challenges in functional and performance testing discussed here  we need a better approach for testing future versions of sql server analysis services.  
 
1. 	new testing approaches 
as we considered our current testing techniques and the challenges we faced in testing ssas 1 and ssas 1  it became clear that we needed to consider additional approaches to testing the ssas engine. in this section we present the approaches we are planning to implement to enhance our testing of the ssas product.  
1 functional verification 
1.1 database creation and validation 
ssas ddl statements are being tested by tests that target each feature and the most important combinations of those features using existing relational databases or appropriately modifying those databases as per the needs of testing new ssas features. this approach can lead to test holes. to improve coverage in this area  a test tool will be created that will create combinations of various features supported by the ssas product. ssas provides an object model  analysis services management objects  or amo  which the tool can use to programmatically create various dimensions and cubes that are supported by ssas. one key benefit of creating the tool is that it will allow us to make sure all the data types supported by the ssas engine and appropriate conversions from the relational databases to ssas supported data types are covered. in addition  the tool will also be able to generate ddl tests that are invalid which the ssas engine should reject with appropriate error messages. the tool is also expected to create the various process ddl statements to test database processing functionality.  
once the ssas database has been processed the tool will send  discover and mdx queries to validate the results. the tool can generate simple mdx queries to retrieve members in each attribute hierarchy of each dimension and verify the results using appropriate discover statements which are slightly different code paths in the ssas engine. one way of to validate that the measure values are consistent is to verify the same measure value using the various storage configuration modes  molap  rolap and holap . the underlying data is retrieved from different storage locations but the results are expected to be the same in almost all scenarios.  
obviously  creating such a tool could lead to an explosion of the amount of tests that are created and this can make test execution prohibitively expensive  time and resource-wise. there are several ways to avoid the test case explosion. a simple way to identify the right set of test cases for regression testing is to first review the code coverage and arc coverage information from the tests executed and include the set of tests that give the maximum benefits with respect to code coverage. in addition to the tests identified by code coverage analysis  we plan to add all the tests that results in bugs in the product to this set of tests. this set of tests certainly can be quite large but will give us high confidence in the product quality.  
1.1 calculation engine and mdx query validation  
the validation of the correctness of calculation and mdx query results is by far the most critical aspect of testing the ssas engine and probably the most challenging one. this is primarily due to various combinations of features contributing to unique rules in query evaluation. specifying various calculations in mdx scripts offers a unique set of challenges in identifying the right cube space  applying the various types of calculations on the various cubes which will not be described in this paper.  
the ssas engine can take several query execution paths for a single mdx query. the ssas engine picks the most optimized path based on various factors considered while evaluating the mdx query. hence all the possible query execution paths are never explored in the context of a specific query. the ssas engine currently has a configuration flag to force the engine to evaluate an mdx query in block mode or cell by cell mode. in block mode  query results are evaluated only on the cells that are expected to return a value. however in the cell by cell mode every cell is evaluated. currently the ability to control the query execution path in the ssas engine is limited to these two paths. figure 1 provides a comparison of the two modes of query execution. we plan to extend this approach to explore other query execution paths.   

figure 1: block computation versus cell by cell computation 
 
in this new approach  we plan to implement functionality in the ssas engine which forces the engine to generate all the possible query execution paths for an mdx query. once the execution paths have been identified for a specific query  we will force the ssas engine to choose each path by providing the query execution path. adding this additional code within the ssas engine will improve the testability of the engine not only in evaluating the correctness of results from the ssas engine but also in identifying if the ssas engine indeed chooses the optimal execution path from the various possibilities. the cost of query execution for the various paths will be reported by the new code added to the engine.  if the path chosen was not optimal  we will have the ability to identify the root cause of why the expected path was not chosen and improve the decision algorithms.  
the mdx queries provided as input to the ssas engine will be generated using the mdx grammar. we have an existing tool that first analyzes the multi-dimensional database design and then generates mdx queries based on the grammar. however  verification of mdx query results is not simple as explained in this paper. hence  we have planned for two approaches to verify the results. the first approach is similar to the one explained in section 1 by changing the storage mode from molap to rolap or vice versa. in addition  we expect the query results for a specific mdx query to be the same for various query execution paths. adding this new approach should provide high confidence that the ssas engine is returning accurate results for the mdx queries.  
1 performance testing  
we have presented the various challenges in our current approach for single user query performance testing of the ssas engine in sections 1 and 1. while debugging the current query performance suites we have identified that most of the performance regressions can be identified by simple targeted queries instead of using large customer query sets. however identifying these queries is the critical element. we will be taking several steps to move in the direction of having the right set of single user performance queries.  
first  we will identify additional ssas engine features that need targeted performance testing based on observed performance regressions. these key performance tests should provide a good indication of the core performance of each feature.  the second step is to identify key components within the ssas engine that need targeted tests. in this aspect we have identified that customers typically have small cubes with a lot of complex calculations or very large cubes with very few calculations. we have planned to focus our efforts on these two categories of cubes along with feature combinations by getting appropriate cubes and corresponding queries from our customers. the third step is to add code in the ssas engine along with code described in section 1.1 that will log the factors influencing a decision for specific query execution path. having this information will help us to identify how the ssas engine decides between various query execution paths. once the key factors have been identified for existing query sets we should be able to divide our test queries into equivalence classes. this will help us trim down the number queries in the current performance suite. this code infrastructure will also be used to evaluate future query suites provided by our customers 
once we have the infrastructure in place to detect the various execution paths and factors influencing the query path decision  we can add appropriate code to detect performance gains/regressions based on analyzing a query work load. for example  if an optimization was added and the rule was changed  the execution path for a specific query will change.  we can easily detect the condition by the change in execution time.  this new infrastructure could also potentially be offered as a feature to customers during upgrade scenarios where customers provide a specific ssas workload. the new ssas engine should be able to evaluate if there would be overall performance gains due to the upgrade or make recommendations on any potential performance degradation and how to mitigate that degradation.  
 
1. 	conclusion  
in this paper we have presented our current approaches used to testing the ssas engine along with key challenges we face in doing that testing. we describe new testing approaches we plan to implement to enhance functional and performance testing of the ssas engine in future releases. also how these new approaches will help test additional areas of the ssas engine as well as help identify regressions faster. we also talked about a new feature that could potentially be added to the product which will help customers identify the impact to their query performance workloads of an ssas upgrade.  
. 
1. 	acknowledgments 
we thank lon fisher in providing some of the ideas presented in this paper. we thank ron pihlgren for reviewing and providing valuable feedback. our thanks to the ssas team members who helped in reviewing this paper. 
