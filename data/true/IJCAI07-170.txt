we explore an application to the game of go of a reinforcement learning approach based on a linear evaluation function and large numbers of binary features. this strategy has proved effective in game playing programs and other reinforcement learning applications. we apply this strategy to go by creating over a million features based on templates for small fragments of the board  and then use temporal differencelearning and self-play. this method identifies hundredsof low level shapes with recognisable significance to expert go players  and provides quantitive estimates of their values. we analyse the relative contributions to performance of templates of different types and sizes. our results show that small  translation-invariant templates are surprisingly effective. we assess the performance of our program by playing against the average liberty player and a variety of computer opponents on the 1¡Á1 computergo server. ourlinear evaluation function appears to outperform all other static evaluation functions that do not incorporate substantial domain knowledge.
1 introduction
a number of notable successes in artificial intelligence can be attributed to a straightforward strategy: linear evaluation of many simple features  trained by temporal difference learning  and combined with a suitable search algorithm. games provideinteresting case studies for this approach. in games as varied as chess  checkers  othello  backgammon and scrabble  computers have exceeded human levels of performance. despite the diversity of these domains  many of the best programs share this simple strategy.
¡¡first  positions are evaluated by a linear combination of many features. in each game  the position is broken down into small  local components: material  pawn structure and king safety in chess ; material and mobility terms in checkers ; configurations of discs in othello ; checker counts in backgammon ; single  duplicate and triplicate letter rack leaves in scrabble ; and one to four card combinations in hearts . in each case  with the notable exception of backgammon a linear evaluation function has provenmost effective. they are fast to compute; easy to interpret  modify and debug; and they have good convergence properties.
¡¡secondly  weights are trained by temporal differencelearning and self-play. the world champion checkers program chinook was hand-tuned by expert players over 1 years. when weights were trained instead by self-play using a temporal difference learning algorithm  the program equalled the performance of the original version . a similar approach attained master level play in chess . td-gammon achieved world class backgammon performance after training by td 1  and self-play . a program trained by td ¦Ë  and self-play outperformed an expert  hand-tuned version at the card game hearts . experience generated by self-play was also used to train the weights of the world champion othello and scrabble programs  using least squares regression and a domain specific solution respectively  1;
1 .
¡¡finally  a linear evaluation function is combined with a suitable search algorithm to produce a high-performance game playing program. minimax search variants are particularly effective in chess  checkers  othello and backgammon  1; 1; 1; 1   whereas monte-carlo simulation has proven most successful in scrabble  and hearts .
¡¡in contrast to these games  the ancient oriental game of go has proven to be particularly challenging. the strongest programs currently play at the level of human beginners  due to the difficulty in constructing a suitable evaluation function . it has often been speculated that go is uniquely difficult for computers because of its intuitive nature  and requires an altogether different approach to other games. accordingly  many new approaches have been tried  with limited success.
¡¡in this paper  we return to the strategy that has been so successful in other domains  and apply it to go. we develop a systematic approach for representing intuitive go knowledge using local shape features. we evaluate positions using a linear combination of these features  and learn weights by temporal difference learning and self-play. finally  we incorporate a simple alpha-beta search algorithm.
1 the game of go
the main rules of go are simple. black and white players take turns to place a single stone onto an intersection of the go board. stones cannot be moved once played  but may be captured. sets of adjacent  connected stones of one colour

	 a 	 b 	 c 
figure 1:  a  if black plays at a he captures two white stones on the right. playing at b is a common tesuji to capture the white stone at the top. the stones in the bottom-left form a common joseki from the marked stone.  b  black can play according to one of three proverbs: a is the one-point jump; b is the ponnuki; and c is a hane at the head of two stones.  c  the safety of the marked black stone depends on context: it is safe in the top-left; should be captured by white a in the top-right; but should be safe from white b in the bottom-left.
are known as blocks. the empty intersections adjacent to a block are called its liberties. if a block is reduced to zero liberties by the opponent  it is captured and removed from the board  figure 1a . at the end of the game  each player's score is equal to the number of stones they have captured plus the total number of empty intersections  known as territory  that they have surrounded.
1 shape knowledge in go
the concept of shape is extremely important in go. a good shape uses local stones efficiently to maximise tactical advantage. professional players analyse positions using a large vocabulary of shapes  such as joseki  corner patterns  and tesuji  tactical patterns . these may occur at a variety of different scales  and may be specific to one location on the board or equally applicable across the whole board  figure 1 . for example  the joseki at the bottom left of figure 1a is specific to the marked black stone on the 1 point  whereas the tesuji at the top could be used at any location. many go proverbsexist to describe shape knowledge  for example  ponnuki is worth 1 points    the one-point jump is never bad  and  hane at the head of two stones   figure 1b .
¡¡commercial computer go programs rely heavily on the use of pattern databases to represent shape knowledge . many years are devoted to hand-encoding professional expertise in the form of local pattern rules. each pattern recommends a move to be played whenever a specific configuration of stones is encountered on the board. the configuration can also include additional features  such as requirements on the liberties or strength of a particular stone. unfortunately  pattern databases suffer from the knowledge acquisition bottleneck: expert shape knowledge is hard to quantify and encode  and the interactions between different patterns may lead to unpredictable behaviour. if pattern databases were instead learned purely from experience  it could significantly boost the robustness and overall performance of the top programs.
¡¡prior work on learning shape knowledge has focussed on predicting expert moves by supervised learning of local shape  1; 1 . although success rates of around 1% have been achieved in predicting expert moves  this approach has not led to strong play in practice. this may be due to its focus on mimicking rather than evaluating and understanding the shapes encountered.
¡¡a second approach has been to train a multi-layer perceptron  using temporal difference learning by self-play  1; 1 . the networks implicitly contain some representation of local shape  and utilise weight sharing to exploit the natural symmetries of the go board. this approach has led to stronger go playing programs  such as enzenberger's neurogo iii   that are competitive with the top commercial programs on 1 ¡Á 1 boards. however  the capacity for shape knowledge is limited by the network architecture  and the knowledge learned cannot be directly interpreted or modified in the manner of pattern databases.
1 local shape representation
we represent local shape by a template of features on the go board. the shape type is defined by the template size  the features used in the template  and the weight sharing technique used.
¡¡a template is a configuration of features for a rectangular region of the board. a basic template specifies a colour  black  white or empty  for each intersection within the rectangle. the template is matched in a given position if the rectangle on the board contains exactly the same configuration as the template. a local shape feature simply returns a binary value indicating whether the template matches the current position.
¡¡we use weight sharing to exploit several symmetries of the go board . all rotationally and reflectionally symmetric shapes share the same weights. colour symmetry is represented by inverting the colour of all stones when evaluating a white move. these invariances define the class of location dependent shapes. a second class of location independent shapes also incorporates translation invariance. weights are shared between all local shape features that have the same template  regardless of its location on the board. figure 1 shows some examples of weight sharing for both classes of shape.
¡¡for each type of shape  all possible templates are exhaustively enumerated to give a shape set. for template sizes up to 1 ¡Á 1  weights can be stored in memory for all shapes in the set. for template sizes of 1 ¡Á 1 and larger  storage of all weights in memory becomes impractical. instead  we utilise hashed weight sharing. a unique zobrist hash  is computed for each location dependent or location independent shape  and h bins are created according to the available memory. weights are shared between those shapes falling into the same hash bin  resulting in pseudo-random weight sharing. because the distribution of shapes is highly skewed  we hope that updates to each hash bin will be dominated by the most frequently occurring shape. this idea is similar to the hashing methods used by tile coding .
¡¡there is a partial order between shape sets. we define the predicate gi j to be 1 if shape set si is more general than shape set sj  and 1 otherwise. shape sets with smaller templates are strictly more general than larger ones  and location

figure 1: examples of location dependent and location independent weight sharing  on a 1 ¡Á 1 board.
independent shape sets are strictly more general than location dependent. a more general shape set provides no additional information over a more specific shape set  but may provide a useful abstraction for rapid learning.
¡¡the frequency with which shapes occur varies by several orders of magnitude. for each of the m shape sets sj  the total number of weights in the set is denoted by nj  and the total numberof local shape features in the set that are matched in any position is a constant value nj. figure 1 shows the breakdown of total number and frequency of shape sets on a 1 ¡Á 1 board.
1 learning algorithm
in principle  a set of shape weights can be learned for any purpose  for example connecting  capturing  or surrounding territory. in our case  weights are learned that directly contribute to winning the game of 1 ¡Á 1 go. at the end of each game  the agent is given a reward of r = 1 for a win and r = 1 for a loss.
¡¡the value function v ¦Ð s  is defined to be the expected reward from board position s when following policy ¦Ð  or equivalently the probability of winning the game. we form an approximation v  s  to the value function by taking a lin-
templatelocationlocationsizeindependentdependentnininini1¡Á111¡Á111¡Á111¡Á111¡Á111¡Á1h1h1¡Á1h1h1¡Á1h1h1¡Á1h1hfigure 1: number of shapes in each set for 1 ¡Á 1 go.
ear combination of all local shape features ¦Õj k from all shape sets j  with their corresponding weights ¦Èj k. the sigmoid function ¦Ò squashes the output to the desired range  1 .
		 1 
.
¡¡all weights ¦È are initialised to zero. the agent selects moves by a -greedy single-ply search over the value function  breaking ties randomly. after the agent plays a move  the weights are updated by the td 1  algorithm . the step-size is the same for all local shape features within a set  and is defined to give each set an equivalent proportion of credit  by normalising by the number of parameters updated on each time-step.
	¦Ä = r + v  st+1    v  st 	 1 
		 1 
   because the local shape features are binary  only a subset of features need be evaluated and updated. this leads to an an efficient implementation rather than the  time that would be required to evaluate or up-
date all weights.
1 experiments with learning shape in 1¡Á1 go
we trained two go-playing agents by coadaptive self-play  each adapting its own set of weights so as to defeat the other. this approach offers some advantages over self-play with a single agent: it utilises two different gradients to avoid local minima; and the two learned policies provide a robust performance comparison. to prevent games from continuing for excessive numbers of moves  agents were not allowed to play within their own single-point eyes .
¡¡to test the performance of an agent  we used a simple tactical algorithm for the opponent: alp  the average liberty player. to evaluate a position  the average number of liberties of all opponent blocks is subtracted from the average over the player's blocks. any ties are broken randomly. for every 1 games of self-play  1 test games were played between each agent in the pair and alp.
¡¡we performed several experiments  each consisting of 1 separate runs of 1 training games. at the end of training  a final 1 test games were played between each agent and alp. all games were played on 1 ¡Á 1 boards. the learning rate in each experiment was measured by the average number of training games required for both agents to exceed a 1% win rate against alp  during ongoing testing. overall performance was estimated by the average percentage of wins of both agents during final testing. all experiments were run with ¦Á = 1 and an exploration rate of during training and during testing. for shape sets using hashed weight sharing  the number of bins was set to h = 1.
¡¡in the first set of experiments  agents were trained using just one set of shapes  with one experiment each for the 1¡Á1 location independent shape set  up to the 1 ¡Á 1 location dependent shape set. the remaining experiments measured the effect of combining shape sets of various degrees of generality. for each shape set si  an experiment was run using all shape sets as or more general  {sj : gj i}.
¡¡amongst individual shape sets  the 1 ¡Á 1 shapes perform best  figure 1   achieving a 1% win rate within just 1 games  and surpassing an 1% win rate after further training. smaller shapes lack sufficient representational power  and larger shapes are too numerous and specific to be learned effectively within the training time. location independent sets appear to outperform location dependent sets  and learn considerably faster when using small shapes. if training runs were longer  it seems likely that larger  location dependent shapes would become more effective.
¡¡when several shape sets are combined together  the performance surpasses a 1% win rate. location independent sets again learn faster and more effectively. however  a mixture of different shapes appears to be a robust and successful approach. learning time slows down approximately linearly with the number of shape sets  but may provide a small increase in performance for medium sized shapes.
1 board growing
the training time for an agent performing one-ply search is o k1  for k ¡Á k go  because both the number of moves and the length of the game increase quadratically with the board size. training on small boards can lead to significantly faster learning  if the knowledge learned can be transferred appropriately to larger boards.
¡¡of course  knowledge can only be transferred when equivalent features exist on different board sizes  and when the effect of those features remains similar. local shape features satisfy both of these requirements. a local shape feature on a small board can be aligned to an equivalent location on a larger board  relative to the corner position. the weights for each local shape feature are initialised to the values learned on the smaller board. some new local shape features are introduced in the centre of the larger board; these do not align with any shape in the smaller board and are initialised with zero weights.
¡¡using this procedure  we started learning on a 1¡Á1 board  and incremented the board size whenever a win rate of 1% against alp was achieved by both agents.
1 online cascade
the local shape features in our representation are linearly dependent; the inclusion of more general shapes in the partial order introduces much redundancy. there is no unique solution giving the definitive value of each shape; instead there is a large subspace of optimal weight vectors.
¡¡defining a canonical optimal solution is desirable for two reasons. firstly  we would like the goodness of each shape to be interpretable by humans. secondly  a canonical weight vector provides each weight with an independent meaning which is preserved between different board sizes  for example when using the board growing procedure.
¡¡to approximate a canonical solution  we introduce the online cascade algorithm. this calculates multiple approximations to the value function  each one based on a different subset of all features. for each shape set si  a td-error ¦Äi is calculated  based on the evaluation of all shape sets as or more general than si  and ignoring any less general shape sets. the corresponding value function approximation is then updated according to this error 
 1 
 1 
 1 
¡¡by calculating separate td-errors  the weights of the most general features will approximate the best representation possible from just those features. simultaneously  the specific features will learn any weights required to locally correct the abstract representation. this prevents the specific features from accumulating any knowledge that can be represented at a more general level  and leads to a canonical and easily interpreted weight vector.
1 generalised shape features
the local shape features used so far specify whether each intersection is empty  black or white. however  the templates can be extended to specify the value of additional features at each intersection. this provides a simple mechanism for incorporating global knowledge  and to increase the expressive power of the representation.
¡¡one natural extension is to use liberty templates  which incorporate an external liberty count at each stone  in addition to its colour. an external liberty is a liberty of a block that lies outside of the template. the corresponding count measures whether there is zero  one  or more than one external liberty for a particular stone. this provides a primitive measure of a stone's strength beyond the local shape. a local liberty feature returns a binary value indicating whether its liberty template matches the current position. there are a large number of local liberty features  and so we use hashed weight sharing for these shapes.
1 results
to conclude our study of shape knowledge  we evaluated the performance of our shape-based agents at 1 ¡Á 1 go against a variety of established computer go programs. we trained a final pair of agents by coadaptive self-play  using the online cascade technique and the board growing method described above. the agents used all shapes from 1 ¡Á 1 up to 1 ¡Á 1  including both location independent and location dependent shapes based on both local shape features and local liberty features  for a total of around 1 million weights.
¡¡to evaluate each agent's performance  we connected it to the computer go server1 and played around 1 games of

1 http://cgos.boardspace.net/1.html

figure 1:  top left  percentage test wins against alp after training with an individual shape set for 1 games.  top right  number of training games with an individual shape set required to achieve 1% test wins against alp.  bottom left  percentage test wins using all shape sets as or more general.  bottom right  training games required for 1% wins  using all shape sets asor more general.
1 ¡Á 1 go with a 1 minute time control. at the time of writing  this server includes over fifty widely varying computer go programs. each program is assigned an elo rating according to its performance on the server  currently ranging from around -1 for the random player up to +1 for the strongest current programs. programs based on a static evaluation function with little prior knowledgeinclude alp  +1  and the influence player  +1 . all of the stronger programs on the server incorporate sophisticated search algorithms or complex  hand-encoded go knowledge.
¡¡the agent trained with local shape features attains a rating of +1  significantly outperforming the other simple  static evaluators on the server. when local liberty features are used as well  the agent's rating increases to +1  figure 1 . finally  when the basic evaluation function is combined with a full-width  iterative-deepening alpha-beta search  the agent's performance increases to +1.
1 discussion
the shape knowledge learned by the agent  figure 1  represents a broad library of common-sense go intuitions. the 1 ¡Á 1 shapes encode the basic value of a stone  and the value of each intersection. the 1 ¡Á 1 shapes show that playing
cgos nameprogram descriptiongameselo ratinglinear-bshape features1+1linear-lshape and liberty features1+1linear-sshape features and search1+1figure 1: cgos ratings attained by trained agents.
stones in the corner is bad  but that surrounding the corner is good; and that connected stones are powerful. the 1 ¡Á 1 shapes show the value of cutting the opponentstones into separate groups  and the 1¡Á1 shapes demonstrate three different ways to form two eyes in the corner. each specialisation of shape adds more detail; for example playing one stone in the corner is bad  but playing two connected stones in the corner is twice as bad.

figure 1:  left  the 1 shapes in each set from 1 ¡Á 1 to 1 ¡Á 1  location independent and location dependent  with the greatest absolute weight after training on a 1 ¡Á 1 board.  top-right  a game between linear-b  white  and dingbat-1  rated +1 . linear-b plays a nice opening and develops a big lead. moves 1 and 1 make good eye shape locally  but for the wrong group. dingbat takes away the eyes from the group at the bottom with move 1 and goes on to win.  bottom-right  a game between linear-l  white  and the search based liberty-1  rated +1 . linear-l plays good attacking shape from moves 1. it then extends from the wrong group  but returns later to make two safe eyes with 1 and 1 and ensure the win.¡¡however  the whole is greater than the sum of its parts. weights are learned for over a million shapes  and the agent's play exhibits global behaviours beyond the scope of any single shape  such as territory building and control of the corners. its principle weakness is its local view of the board; the agent will frequently play moves that look beneficial locally but miss the overall direction of the game  for example adding stones to a group that has no hope of survival. our current work is focussed on using generalised shapes to overcome this problem  based on more complex features such as eyes  capture and connectivity.
