this paper explores topic aspect  i.e.  subtopic or facet  classification for english and chinese collections. the evaluation model assumes a bilingual user who has found documents on a topic and identified a few passages in each language on aspects of that topic. additional passages are then automatically labeled using a k-nearest-neighbor classifier and local  i.e.  result set  latent semantic analysis. experiments show that when few training examples are available in either language  classification using training examples from both languages can often achieve higher effectiveness than using training examples from just one language. when the total number of training examples is held constant  classification effectiveness correlates positively with the fraction of same-language training examples in the training set. these results suggest that supervised classification can benefit from hand-annotating a few same-language examples  and that when performing classification in bilingual collections it is useful to label some examples in each language.
categories and subject descriptors
i.1  pattern recognition : design methodology- classifier design and evaluation;
general terms
performance  experimentation
keywords
classification  subtopic  cross-language  test collection
1. introduction
　we are motivated by the problem of aspectual sentiment characterization: we wish to identify segments of individual documents that address some specified aspect of some specified topic and then characterize the aggregate sentiment expressed about that aspect of that topic in those segments.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  singapore.
copyright 1 acm 1-1-1/1 ...$1.
because we ultimately wish to do this for multilingual collections  bilingual topic aspect classification is a lesser included problem  and that is the problem on which we focus in this paper. specifically  we assume that the results of topical search are already available in two languages  and that sentiment analysis will be performed in a subsequent processing stage; our focus is therefore on labeling examples of aspects in the two languages and on training classifiers that can accurately identify additional document segments that address those same aspects in the two languages.
　cross-language text classification problems arise in two settings:  1  training examples have already been labeled in one language  and we wish to build a classifier for another language using that training data  or  1  no training data yet exists  and classifiers must be built that operate well in more than one language. our focus here is on the second of these settings  although our results offer some insight into the first as well . in particular  we are interested in the specific problem of topic aspect classification  where we mean 'aspect' in the same sense as was used in the trec interactive track  i.e.  a facet or specific subtopic of a topic1 .
　it is already known that training examples in one language can be used to build a classifier for another language  1  1  1  1   indeed  this is the key insight that motivates work on cross-language information retrieval . our new question in this paper is whether examples from two languages can productively be used together to improve classification effectiveness. although by no means obvious  since systematic translation errors could equally well have reduced classification accuracy   it turns out that the answer to that question is yes. these results suggest that balancing the investment in annotation of training examples across languages can be helpful when seeking to simultaneously optimize classification effectiveness for more than one language. the paper is organized as follows: section 1 introduces related work  section 1 describes our methods for aspect classification  section 1 addresses the design of the test collection  section 1 presents our results  and section 1 concludes the paper.
1. related work
　the goal of text classification is to classify the topic or theme of a document . automated text classification is a supervised learning task  defined as automatically assigning pre-defined category labels to documents . it is a well studied task  with many effective techniques. feature selection is known to be important. the purpose of feature

1
 in linguistics  'aspect' often denotes 'grammatical aspect;' we consistently mean 'topic aspect.'
selection is to reduce the dimensionality of the term space since high dimensionality may result in the overfitting of a classifier to the training data. yang and pedersen studied five feature selection methods for aggressive dimensionality reduction: term selection based on document frequency  df   information gain  ig   mutual information  a χ1 test  ciii   and term strength . using the knn and linear least squares fit mapping  llsf  techniques  they found ig and ciii most effective in aggressive term removal without losing categorization accuracy. they also found that df thresholding  the simplest method with the lowest cost in computation could reliably replace ig or ciii when the computations of those measure were expensive.
　popular techniques for text classification include probabilistic classifiers  e.g  naive bayes classifiers   decision tree classifiers  regression methods  e.g.  linear least-square fit   on-line  filtering  methods  e.g.  perceptron   the rocchio method  neural networks  example-based classifiers  e.g.  knn   support vector machines  bayesian inference networks  genetic algorithms  and maximum entropy modelling . yang and liu  conducted a controlled study of 1 well-known text classification methods: support vector machine  svm   k-nearest neighbor  knn   a neural network  nnet   linear least-square fit  llsf  mapping  and naive bayes  nb . their results show that svm  knn  and llsf significantly outperform nnet and nb when the number of positive training examples per category are small  fewer than 1 .
　in monolingual text classification  both training and test data are in the same language. cross-language text classification emerges when training data are in some other language. there have been only a few studies on this issue. in 1  topic detection and tracking  tdt  research was extended from english to chinese . in topic tracking  a system is given several  e.g.  1  initial seed documents and asked to monitor the incoming news stream for further documents on the same topic   the effectiveness of crosslanguage classifiers  trained on chinese data and tested on english  was worse than monolingual classifiers.
　bel et al.  studied an english-spanish bilingual classification task for the international labor organization  ilo  corpus  which had 1 categories. they tried two approaches- a poly-lingual approach in which both english and spanish training and test data were available  and cross-lingual approach in which training examples were available in one language. using the poly-lingual approach  in which a single classifier was built from a set of training documents in both languages  their winnow classifier  which  like svm  computes an optimal linear separator in the term space between positive and negative training examples  achieved f1 of 1  worse than their monolingual english classifier  with f1.1  but better than their monolingual spanish classifier  with f1.1 . for the cross-lingual approach  they used two translation methods-terminology translation and profile translation. when trained on english and tested on spanish translated into english  their classifier achieved f1 of 1 using terminology translation and 1 using profile translation; when trained on spanish and tested on pseudo-spanish  their classifier achieved f1 of 1; all worse than their corresponding monolingual classifiers.
　rigutini et al.  studied english and italian cross-language text classification in which training data were available in english and the documents to be classified were in italian. they used a naive bayes classifier to classify english and italian newsgroups messages of three categories: hardware  auto and sports. english training data  1 messages for each category  were translated into italian using office translator idiomax. their cross-language classifier was created using expectation maximation  em   with english training data  translated into italian  used to initialize the em iteration on the unlabeled italian documents. once the italian documents were labeled  these documents were used to train an italian classifier. the cross-language classifier performed slightly worse than monolingual classifier  probably due to the quality of their translated italian data. gliozzo and strapparava  investigated english and italian cross-language text classification by using comparable corpora and bilingual dictionaries  multiwordnet and the collins english-italian bilingual dictionary . the comparable corpus was used for latent semantic analysis which exploits the presence of common words among different languages in the term-by-document matrix to create a space in which documents in both languages were represented. their cross-language classifier  either trained on english and tested on italian  or trained on italian and tested on english  achieved an f1 of 1  worse than their monolingual classifier  with f1 =1 for english and 1 for italian .
　olsson et al.  classified czech documents using english training data. they translated czech document vectors into english document vectors using a probabilistic dictionary which contained conditional word-translation probabilities for 1 word translation pairs. their concept label knn classifier  k = 1  achieved precision of 1  which is 1% of the precision of a corresponding monolingual classifier.
　the main differences of our approach compared with earlier approaches include:  1  classifying document segments into aspects  rather than documents into topics;  1  using few training examples from both languages;  1  using statistical machine translation results to map segment vectors from one language into the other.
1. methods
　the goal of bilingual aspect classification is to classify english and chinese document segments that address the same broad topic based on relevance to specific aspects of that topic. here we define monolingual aspect classification as a task which uses training examples in one language only  and the test examples are in the same languages; we define bilingual aspect classification as a task which uses training data in two languages. in this section  we discuss general approaches to monolingual and bilingual aspect classification  classification methods  and evaluation metrics.
1 monolingual aspect classification
　our monolingual aspect classification system  which serves as a baseline  was built with 1 steps:
　 1  a user who can read and write both english and chinese retrieves a set of english document segments relevant to a topic from an english collection  and a set of chinese document segments relevant to the same topic from a chinese collection. the indri search engine1 was used to create the two information retrieval systems. the user then examines the two sets of retrieved document segments and  for each aspect  selects 1 document segments from each language.
　 1  local latent semantic analysis  lsa  is performed on

1 http://www.lemurproject.org/indri/ each set of retrieved document segments to reduce the dimensionality of the term space. in the vector space model  documents  and queries  are represented as term vectors in a t-dimensional space  t is the number of terms    which represents both  signal   i.e.  meaning  and  noise   from term usage variations . lsa reduces the dimensionality of the vector space with semantic information  hopefully  preserved but conflating similar terms towards a  conceptual representation. the dimensions that are kept are those that explain the most variance. the mathematical basis for lsa is a singular value decomposition  svd  of the highdimensional term-document matrix. the svd represents both terms and documents as vectors in a space of choosable dimensionality . this yields an optimal approximation to the original term-document matrix in the least squares  or l1 norm  sense. lsa for a large document collection is both computationally expensive and memory intensive  but local lsa is applied to smaller matrices and thus does not suffer from these computational problems. local lsa is defined as applying svd to a term-by-document matrix consisting only of the documents relevant to a topic . the svd decomposes a rectangular matrix of terms by documents  t 〜 d  into three matrices. for example  a t 〜 d matrix of terms and documents x can be decomposed into the product of three other matrices:  such that t1 and
d1 are orthonormal matrices of left and right singular vectors and s1 is the diagonal matrix of singular values. the diagonal elements of s1 are constructed to be non-negative and ordered in decreasing magnitude . by choosing the first k largest singular values in s1 and setting the remaining smaller ones to zero  and deleting the corresponding columns of t1 and d1   we get a matrix x  which is approximately equal to x  but with rank k. the chosen k for our experiments is introduced in section 1.
　 1  a classification algorithm takes the manually selected document segments as training examples and identifies which of the unlabeled document segments on that topic best match that aspect  in reduced term space .
　the keys to the classification algorithm are a segmentsegment similarity function and a threshold for making the classification decision. in the vector space model  a document segment is represented as a vector of term weights  and the similarity of two document segments can be computed as the cosine of the two vectors. a better index term weighting function can lead to a substantial improvement in information retrieval performance. okapi bm1 term weighting  1  1  has been shown to be robust and to achieve retrieval effectiveness that is on a par with any other known technologies  so we compute similarity on okapi bm1 term weights in local lsa space.
1 bilingual aspect classification
　since the user has selected training examples from two languages for a same aspect  the training examples in one language might be used as additional training examples for the other language  if we know how to map them correctly . the process of bilingual aspect classification involves the following steps  introduced using english as the language of the segments to be classified  chinese as the other language  and translating chinese into english; but it works for the other direction in a similar way :  1  once the chinese aspect training examples are provided by the user  they are translated into english.  1  fold in  or map  the translated training examples into the original english document segments' lsa space.  1  we suspect that systematic translation errors might put the translated training examples in the wrong place  so optionally  correct the mapping of the translated training examples by moving the centroid of these translated segments toward the centroid of the original english training examples.  1  classify the unlabeled english segments using the english and the translated chinese training examples in their english document segments' lsa space.
　 translation  here means mapping term statistics from one language to another  not simply replacing the terms themselves. if a translation probability matrix which estimates the probabilities that chinese words will be translated into english words is available  a chinese segment vector can be translated into an english segment vector by multiplying the segment vector by the translation probability matrix  and then folded into an english lsa space by multiplying the resulting english segment vector by the term-by-dimension matrix left singular vector t1. translation probabilities can be estimated from parallel corpora  from multilingual dictionaries  when presentation order encodes relative likelihood of general usage   or from the distribution of an attested translation in multiple sources of translation knowledge . in statistical machine translation  mt   translation probabilities are usually learned from parallel corpora. parallel corpora consist of pairs of documents in two languages that are translations of each other. sentence-aligned parallel corpora are required for statistical mt. with sentence-aligned parallel corpora  the freely available giza++ toolkit  can be used to train translation models. giza++ produces a representation of a sparse translation matrix. we re-used an existing translated model in our experiment  see section 1 . the document vectors extracted from the english index of a search engine  i.e. indri in our experiments  are english word stems and their term weights  and a word stem could have resulted from multiple word forms  so we conflated the probabilities of english words into probabilities for their corresponding stems in the probability tables.
1 classification methods
　previous studies show that k-nearest-neighbor  knn  and support vector machine  svm  technologies are among the best text classifiers . since a topic can have multiple aspects  our classification problem is an m-way multiple-class problem. knn is a natural choice for a multiple-class problem  so we selected knn for our experiments. here we introduce the classical knn approach and two variants. the classical knn algorithm is very simple: to classify a new object  i.e.  a document segment   consult the k training examples that are most similar  where k is an integer  k − 1. each of the k labeled neighbors  votes  for its category. then count the number of votes each category gets  and assign the category with the maximum number of votes to the new object . in our experiments  the similarity measure was the cosine similarity function with okapi weights. the best choice of k depends upon the data; generally  larger values of k reduce the effect of noise on the classification  but make boundaries between classes less distinct. a good k can be selected by using empirical techniques  e.g.  crossvalidation .
　in the classical knn algorithm  the degree of similarity between a test object and training examples is indirectly used  to pick the k nearest neighbors . to make better use of the similarity scores  we introduce two variants of the classical algorithm that directly use the similarity scores. franz's algorithm  sums up the weighted similarity scores as the contribution of the k nearest neighbors to their categories. that is  each category of the k nearest neighbors accumulates the similarity between the new object and the training examples of this category  the category with the maximum sum of similarity scores is assigned to the new object. a second variant of the classical knn algorithm was proposed by yang . here the conventional m-way classification knn is adapted to the 1-way classification problem. since we have multiple aspect classes to work with; when we are working with aspect x  we classify documents either into aspect x or non-aspect x. since we have different numbers of positive and negative training examples  we compute a relevance score for each test document as follows :

where ukp consists of the kp nearest neighbors of test document segment vector x among the positive training examples  and vkn consists of the kn nearest neighbors of x among the negative training examples; t is a threshold. in our experiments  we set t to 1. in classification  two types of decisions can be made: a soft classification decision in which the test object can be assigned to more than one class  and a hard classification decision in which the test object has to be assigned to only one class. in our experiments  we elected to make hard classification decisions to make the classifiers easier to build-the category with the maximum positive relevance value in yang's variant or the maximum sum of similarity scores in franz's variant is assigned. soft classification  however  can be useful when users define conceptually overlapped aspects or assign the same or overlapping segments to different aspects as training examples.
1. test collection design
　we adopted precision  recall  and the f1-measure  which we refer to generically as effectiveness . we also reported the standard deviation  stdev  of f1. we did not use accuracy because accuracy is often dominated by the count of correctly classifying a truly negative test instance into a negative category when the negative category is large. we are interested in how well we do on an aspect  so we used macroaveraged measures . to annotate aspects  we need topics  documents  and a definition of document segments. we have english and chinese news articles from the topic detection and tracking  tdt  collection  which includes pre-annotated topics: the tdt1 collection with 1 and 1 evaluation topics  and the tdt1 collection with 1 and 1 evaluation topics. tdt1 topics are described in both english and chinese languages  whereas tdt1 topics are described only in english.1 the tdt1 and tdt1 collections include news articles and automatically transcribed broadcast news from 1 news sources. tdt1 and tdt1 also include annotated relevant documents for the topics. most of the english relevant documents are from nyt  apw  and voa  whereas most of the chinese relevant documents are from xin  zbn  and vom  voa mandarin ; adding other

1 tdt topics: http://projects.ldc.upenn.edu/{tdt1|tdt1} sources does not significantly increase the number of relevant documents. so we selected the news documents from nyt  apw  voa  xin  zbn  and vom for our experiments. in our test collection  we have 1 chinese documents and 1 english documents from the selected sources.
　our first challenge was to estimate the number of aspects needed to preform statistically significant comparisons. aspects of a topic may be strongly related  so it is more reasonable to assume that it is the topics that are independent. assuming precision of aspect classification is normally distributed  according to cohen's general principle  for power = 1  α = 1  the effect size d = 1  and a two-tailed paired-sample t-test  total sample size required would be n = 1 topics . since we want at least 1 aspect categories for each topic  this suggests that at least 1 aspects must be annotated. beyond simply needing enough topics  we need topics with a sufficient number of relevant documents to yield an adequate number of aspect examples. we would have liked to have had at least 1 segments per aspect  so 1 could be used for training  1 for test . we therefore selected the 1 tdt1 and tdt1 topics for which at least 1 relevant documents were known.
　before annotation started  we had to choose the granularity  i.e.  the text unit  for annotation. to make the test collection more broadly useful  we divided each document into sentences and defined a document segment as a group of consecutive sentences. the annotators thereby were free to define the length of a segment in any reasonable manner  e.g.  depending on its context . two graduate students at the university of maryland were recruited to annotate the aspects. both were native speakers of chinese with good mastery of english. a two-step training session was provided before they started the annotation project. in the first step  the first author explained the concept of aspect. an aspect of a topic was defined as a subtopic or a facet of the topic  and an instance of the aspect was defined as a group of consecutive sentences that addressed the aspect. they were provided with an annotated topic as an example  which had been prepared by the first author. in the second step  the annotators were provided with a topic to do a test run to see whether they understood the process. they asked whether  when   where   and  who could be defined as aspects of a topic. the first author explained that these were not the aspects we intended because those were too general  and thus could apply to every news topic. they were told that what we intended were more specific and essential subtopics  such as reasons  consequences  people's reactions  and influences on our lives.
　the project was split into two phases. in the first phase  each person annotated 1 topics. they were provided with the topics and relevant documents in the two languages  and instructed to identify 1 aspects for each topic  and to try to finish annotating each topic within 1 hours. for examining inter-annotator agreement  in the second phase  each person annotated 1 topics that had already been annotated and were recommended by the other person. the two annotators annotated 1 bilingual aspects for the 1 topics  and 1 all others categories which held non-relevant segments from the same documents. the all others categories were incompletely annotated due to time constraints  so they were not used in our experiments. the annotations were automatically examined to remove non-existent sentence numbers  which the annotators occasionally recorded by mistake.
1. experiments
　we designed two experiments. experiment 1 was designed to test three knn algorithms and five ways of exploiting foreign-language training examples. experiment 1 then used the best configuration from experiment 1 to test the effect of varying the number of same- and foreign-language training examples on classification effectiveness.
　texttiling is a process for automatically subdividing a text document into multi-paragraph  passages  or subtopic segments that are topically coherent . before we used it to generate document segments  the documents were preprocessed to strip off xml tags. the original paragraph boundaries  marked with  p   were retained by replacing the  p  tags with two newline characters so that hearst's texttiling software recognized them. the texttiling software has a window size parameter w which defines the length of a text  block.  two text blocks are compared to identify topic shift. its default value is 1 words. the parameter was optimized to w = 1 by running the program on 1 relevant documents  for 1 topics  in the nyt/ape/xie news collection for the trec 1 high-accuracy retrieval of documents  hard  track  with the w parameter ranging from 1 to 1  generating text tiles  then evaluating them with ldc's passage markings. long tiles were further split if certain conditions were met. if a tile had at least 1 sentences and at least 1 words  or had at least 1 sentences and at least 1 words  it was split into segments with a maximum length of 1 words. these parameters were chosen by manually analyzing some of the long tiles and examining the resulting split tiles when different parameters were tried.
　english document segments were indexed using indri; the porter stemmer was applied and stopwords removed. word segmentation for chinese documents  and topics  was performed with the ldc segmenter1 because our translation resources used that segmenter. since hearst's texttiling software does not work with chinese documents in utf1 or gb encoding  chinese documents were converted to hexadecimal codes. hexadecimal codes for chinese punctuation were converted into corresponding ascii. the resulting chinese document segments were then indexed using indri; a chinese stopword list  was applied.
　tdt topics have 1 components: topic number  topic title  seminal event  what  who  when  where   topic explication  rule of interpretation  and examples. we manually prepared queries for each topic using topic titles  what and who specifications  and topic explications  including  on topic  statements but excluding off topic statements . tdt1 topics 1 already have chinese versions  other topics were manually translated into chinese by the first author. we used these queries to retrieve ranked lists of document segments for each language. next we experimented with how many segments we should use to construct the local lsa space. we had two concerns:  1  we needed enough segments so that most of the segments in the gold standard would contribute to the construction of their local lsa space  and  1  taking too many segments results in slower svd computation and less potential for dimensionality reduction. we experimented with taking the top 1  1  1  1  and 1 segments  and ultimately chose the top 1 chinese and 1 english segments to construct lsa spaces for those languages  because taking more seg-

1 http://projects.ldc.upenn.edu/chinese/ldc ch.htm ments would not have materially increased the number the segments in the gold standard .
　once the number of documents to be retrieved was decided  a term-by-document matrix was constructed  i.e.  extracted from the index  for each query. this was the input of the svd. the outputs we needed were a dimension-bydocument matrix d1  i.e.  the document vectors in the lsa space  and a term-by-dimension matrix t1. previous study of the relationship between the number of lsa dimensions retained and mean average precision for retrieval from the cranfield collection of 1 aerospace abstracts showed that retaining 1 dimensions yielded good results . both the number of abstracts and the length of the abstracts in that experiment were close to our case  so we decided to retain 1 dimensions.
　we obtained chinese-english and english-chinese translation probability tables prepared by wang . the chineseenglish bidirectional translation probability tables were generated using the foreign broadcast information service  fbis  parallel corpus.1 the word alignment models implemented by giza++ are sensitive to translation direction  so giza++ was run twice  one with english as the source language and the other with chinese as the source language . wang further improved the english to chinese translation probabilities by combining the translation probabilities in the two directions and applying statistical synonyms derived from the tables . we directly adopted the resulting translation probability matrices.
　we could translate a document segment vector in two ways. one was to directly translate okapi term weights. the other was to extract and translate the tf and df vectors separately  then to compute the okapi term weights. we expected that the second way would be better because pre-computed term weights represent the importance of the terms in the source language collection  and the tf*idf term weight function is not linear-it rewards rare terms. when a rare term was translated from its source language  e.g.  chinese  to the target language  e.g.  english   the resulting term weight could be over-estimated. estimating the importance of the translated terms in the target language  e.g.  english  by translating tf and df vectors separately before computing tf*idf can avoid this problem.
1 processing the test collection
　the annotation process resulted in a raw gold standard that could not be directly used for experiments because our systems used machine-generated segments  which were generally different from the hand-annotated segments. we mapped between machine-generated and hand-annotated segments based on sentence overlap. if at least one sentence in a machine-generated segment was marked as relevant to a certain aspect  this segment might be mapped onto that aspect. therefore  one machine-generated segment could possibly be mapped onto multiple candidate aspects; when this happened  a single mapping decision was made by a majority voting. ties were resolved arbitrarily by choosing the lowest numbered candidate. we also required that segments be assigned to at most one aspect of a topic. we used greedy selection to ensure that retained aspects were mutually exclusive: if the same sentence appeared in two or more aspects  the first aspect was kept and every other aspect annotated with that sentence was removed. if an aspect in

1
ldc catalog: ldc1.
runclassical  cl franzyangprf1  stdev prf1  stdev prf1  stdev base111  1 111  1 111  1 fold111  1 111  1 111  1 trtd111  1 111  1 111  1 foldc111  1 111  1 111  1 trtdc111  1 111  1 111  1 
table 1: english aspect classification; 1 e and 1 c training examples; mean over 1 aspects from 1 topics.
runsclassical  cl franzyangprf1  stdev prf1  stdev prf1  stdev base111  1 111  1 111  1 fold111  1 111  1 111  1 trtd111  1 111  1 111  1 foldc111  1 111  1 111  1 trtdc111  1 111  1 111  1 
table 1: chinese aspect classification; 1 c and 1 e training examples; mean over 1 aspects from 1 topics.one language was removed  its corresponding aspect in the other language was also removed. if all aspects of a topic had duplicate sentences  that topic was dropped.
　requiring at least 1 document segments per aspect disqualified too many topics and aspects. we therefore required each aspect to have at least 1 segments  so that 1 segments could be used for training and the others for testing. there were a total of 1 bilingual aspects from 1 topics that met this requirement  excluding the all others categories . to simplify our experiments  we dropped the document segments that were in the gold standard but were not in the ranked list of selected retrieved segments  although we could have kept them by folding them into the lsa spaces . ultimately we used 1 bilingual aspects from 1 topics  including 1 chinese aspects that could only be used as training data for english aspect classification because each of them had only 1 segments. this is the first version of our operational test collection.
　to examine the effect of varying the number of training examples on classification effectiveness  a second version of the test collection was created in which we required at least 1 segments for each aspect. this version has 1 aspects from 1 topics.
　the second phase of the annotation project was a semiopen annotation task because the documents for each aspect were provided  but segments could be freely defined. when validating the annotations for the inter-annotator agreement study  non-existent sentences were removed  but aspects with duplicate sentences were retained. we measured inter-annotator agreement using machine-generated segments that were mapped onto the annotated aspects. in this way we could directly examine the effect of disagreement on our experiment design. the average cohen's kappa was 1 for english and 1 for chinese1. the agreement on chinese aspects was higher

1
 the purpose of annotating recommended topics was to speed up the second phase because annotators often recommended the topics they were most confident in their annotations and they provided clear descriptions for the aspects of the topics. this has a potential bias of reporting a higher inter-annotator agreement.
than that on english aspects  probably because chinese was the annotators' native language.
1 experiment 1
　the first experiment was designed to to test whether adding foreign-language training examples would help to classify native-language segments by aspect. we needed as many aspects as possible  so the first version of the test collection was used. the document segments for each aspect were partitioned into training and test sets using cross-validation. we elected to run a maximum of 1 rounds of cross-validation. when foreign-language training segments were added together  the langauge with the greatest number of combinations determined the number of cross-validation rounds. the parameter k of knn was automatically selected topic by topic depending on the number of aspects for that topic. if a topic had m aspects  excluding the all others category   k was set to 1m + 1  an odd integer to minimize ties .
we have 1 ways of using foreign-language training:
base: baseline  using same-language training only.
　fold: translating foreign-language document segment vectors with pre-computed okapi term weights  then folding them into the same-language lsa space.
　foldc: translating the foreign-language segment vectors with pre-computed okapi term weights  folding them into same-language lsa space  then moving them toward the native-language training data so that their centroids meet.
　trtd: translating the foreign-language document segments' tf and df vectors separately  then computing their okapi weights  then folding the vectors with okapi weights into the same-language lsa space.
　trtdc: translating the foreign-language document segments' tf and df vectors separately  computing their okapi weights  folding the vectors with okapi weights into samelanguage lsa space  then moving them toward the nativelanguage training data so that their centroids meet.
　the three classification algorithms  classical knn  franz' variant  yang's variant  were applied to the five ways of using foreign language training data  so there were a total of 1 runs. experiment 1 tested the effect of adding 1 foreign-

figure 1: monolingual case; 1 aspects  1 topics.

figure 1: doubling training with foreign examples.
language training examples to 1 same-language training examples on classification performance. table 1 shows the comparisons of the 1 runs for classifying english aspects. table 1 shows the similar comparisons for classifying chinese aspects. both tables show that both foldc and trtdc consistently yield lower mean precision  recall  and f1 than fold and trtd respectively  so we focus on comparisons between base  fold  and trtd. fold and trtd consistently improve classification effectiveness over the baseline  indicating that foreign-language training examples are useful. trtd is better than fold  again confirming that translating tf and df vectors then computing okapi term weights is better than translating a vector of pre-computed term weights. the two knn variants are always better than the classical knn. there is no consistent difference between franz trtd and yang trtd. we therefore arbitrarily elected franz trtd for experiment 1.
1 experiment 1
the second experiment was designed to examine the effect
classifying english aspectsk total training exampleske
1e+ k-1 c
1e+ k-1 c
1e+ k-1 c
1e+ k-1 c
1e+ k-1 c
1e+ k-1 c
figure 1: varying number of foreign examples.
kcorrelationdfenglishchinese1.1.11-1111*1*111*111*11*11111
table 1: effect of same-language fraction on f   k: total examples; *: p   1; df: degrees of freedom .
of varying the number of training examples on classification performance. the second version of the test collection was used  in which each aspect has at least 1 segments in both languages. we used 1 document segments for training and the remainder for test  the plotted analytic baseline at zero training examples is based on randomly selecting an aspect for each topic . again  a maximum of 1 rounds of crossvalidations were performed. franz trtd was used for this experiment. since fewer than 1 document segments could be used as training examples  the parameter k of knn was set somewhat differently than in experiment 1. if an aspect had 1 or 1 training examples  k was the number of training examples; otherwise  k was 1m+1  where m was the number of aspects for the topic.
　figure 1 shows that when only same-language training examples are involved  more training examples generally yields better classification effectiveness. this meets our expectations. linear regression models for predicting f from the number of same-language training examples are significant

figure 1: holding total training examples constant.
at p   1  the correlation coefficient is 1 for english  and 1 for chinese . figure 1 reinforces the conclusion from experiment 1 that foreign-language training examples are useful. it shows that when equal numbers of foreign language and same-language training examples are used  classification effectiveness usually increases  1c and 1e being the exceptions . figure 1 shows the effect of supplementing varying numbers of same-language training examples with varying numbers of foreign-language ones. although a bit busy  it generally shows that a point of diminishing returns is reached beyond which fluctuations appear random. when a fixed number of same-language and foreign-language training examples  k=1  are available  classification effectiveness is generally positively correlated with the percentage of same-language training examples. table 1 shows that pearson's r correlation scores are positive  except for one outlier   and some are statistically significant at p   1. figure 1 illustrates this correlation graphically for k = 1. due to sparse data  the cases k   1 or k   1 are not included in table 1.
1. conclusion
　through our experiments  we were able to conclude that when only a few native-language training examples were available  a few additional foreign-language training examples would also be useful. but a point of diminishing returns occurred after a few foreign-langauge training examples were added. when a fixed number of training examples were used  the classification performance was mostly generally correlated with the percentage of native-language training examples. this implies that foreign-language training examples should generally be used as supplements to  rather than substitutes for  native-language training examples.
1. acknowledgments
　thanks to jianqiang wang for providing us with the bidirectional english-chinese translation probability tables  and gina-anne levow for proving us with the chinese stopword list. this work has been supported in part by darpa contract hr-1-1  gale  and nsf award dhb1.
