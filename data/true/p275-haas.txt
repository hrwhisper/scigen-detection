current database sampling methods give the user insufficient control when processing iso-style sampling queries. to address this problem  we provide a bi-level bernoulli sampling scheme that combines the row-level and page-level sampling methods currently used in most commercial systems. by adjusting the parameters of the method  the user can systematically trade off processing speed and statistical precision-the appropriate choice of parameter settings becomes a query optimization problem. we indicate the sql extensions needed to support bi-level sampling and determine the optimal parameter settings for an important class of sampling queries with explicit time or accuracy constraints. as might be expected  row-level sampling is preferable when data values on each page are homogeneous  whereas page-level sampling should be used when data values on a page vary widely. perhaps surprisingly  we show that in many cases the optimal sampling policy is of the  bang-bang  type: we identify a  pageheterogeneity index   phi  such that optimal sampling is as  row-like  as possible if the phi is less than 1 and as  page-like  as possible otherwise. the phi depends upon both the query and the data  and can be estimated by means of a pilot sample. because pilot sampling can be nontrivial to implement in commercial database systems  we also give a heuristic method for setting the sampling parameters; the method avoids pilot sampling by using a small number of summary statistics that are maintained in the system catalog. results from over 1 experiments on 1 real and synthetic data sets show that the heuristic method performs optimally about half of the time  and yields sampling errors within a factor of 1 of optimal about 1% of the time. the heuristic method is stable over a wide range of sampling rates and performs best in the most critical cases  where the data is highly clustered or skewed.
1. introduction
　the vast amount of data in modern warehouses and on the web poses a major challenge for analytical  mining  and statistical applications  and for data exploration in general. many data analysis algorithms simply do not scale to the hundreds of terabytes of data often found in modern repositories. at the same time  users are demanding that systems for data processing and analysis be increasingly fast  flexible  and responsive. this pressure by users arises both from the ever-increasing pace of e-business and from the development of applications that support real-time interaction with

 christian konig performed this work while at the ibm almaden research center.：
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  paris  france.
copyright 1 acm 1-1/1...$1.
data  such as spreadsheets and olap tools. although increases in cpu and disk speeds are helpful in dealing with massive data  hardware improvements alone do not suffice. indeed  there is evidence that computer systems are getting slower in that the volume of online data is growing at a rate faster than moore's law .
　in light of this situation  it has become increasingly apparent that sampling technology is a crucial component of a modern dbms. indeed  over the past few years all of the major commercial database vendors have been incorporating sampling capabilities into their products in response to user demands. when judiciously applied  these techniques permit the computation of approximate query results-which often suffice in practice-in a fraction of the time required to compute an exact answer; see  for a comprehensive survey of database sampling techniques. sampling complements other algorithmic techniques for taming massive data  such as data synopses and materialized views  and is particularly well suited for situations in which the queries are ad hoc or in which it is impractical or impossible to take even one pass through all of the data.
　recognizing the importance of sampling  the iso has been developing a standard that extends the sql language to encompass a useful class of sampling queries. this paper provides techniques that give the user more control over the processing of such queries than current systems provide.
　under the proposed iso sampling standard  each table reference in an sql query may be followed by a sampling clause. the general format of a sampling clause is
tablesample samplingmethod  samplingfraction  
where samplingfraction is interpreted as a percentage. some sampling methods may involve additional parameters beyond merely the sampling rate. the sampling methods currently supported by the standard are  row-level bernoulli sampling   bernoulli  and a vendor-defined sampling method  system . for row-level bernoulli sampling with sampling rate q （  1   each row is included in the sample with probability q and excluded with probability 1   q  independently of the other rows. this method is also called  binomial sampling  because the sample size is binomially distributed. in current commercial systems  the system sampling method is typically implemented as a page-level bernoulli sampling scheme. that is  for each page  all of the rows on the page are included in the sample with probability q and all of the rows are excluded with probability 1   q  independently of other pages. bernoulli schemes have proven popular in commercial database systems because bernoulli sampling is easy to parallelize and relatively easy to implement.
　this new database sampling functionality-while potentially quite effective for obtaining quick approximate answers to aggregation queries over massive datasets-is not always easy to exploit.
in particular  choosing the appropriate sampling method for a given query and dataset can be decidedly nontrivial. as discussed in detail below  row-level sampling yields accurate results but is often very expensive to execute  whereas page-level sampling is much faster  but yields results that are often much less precise. no current commercial dbms provides guidance on which sampling method to choose for a specified problem. if the user chooses row-level bernoulli sampling  then chances are that the query will take too long to complete. if  on the other hand  the user chooses page-level bernoulli sampling  then the query will execute much more quickly but the result may well be too inaccurate to be useful-in this case  the user will need to re-execute the query using a higher sampling rate  in a painful process of trial and error.
　in this paper  we present a bi-level sampling method that avoids the foregoing problems by combining row-level and page-level sampling schemes. the user can systematically trade off processing speed and statistical precision by adjusting the parameters of the method. we indicate the sql extensions needed to support bilevel sampling and provide techniques for automatically selecting optimal or near-optimal parameter values. this latter work can be viewed as a first step towards extending query-optimizer technology to the setting of sampling queries.
　we note that the sampling techniques described here complement online processing methods as in . sql sampling techniques do not provide the same level of real-time interactivity as those in   but they can be incorporated into existing database system architectures relatively easily  and so can be exploited now.
　the rest of this paper is organized as follows. after reviewing simple row-level and page-level bernoulli sampling schemes in section 1  we introduce in section 1 our bi-level bernoulli sampling scheme  which subsumes the simple schemes as special cases. we show how to use bi-level sampling to obtain quick approximate answers to an important class of sum  count  and avg aggregation queries  as well as how to estimate the precision of these answers. the required sql extensions to support such querying are described. we then derive  section 1  the optimal bi-level bernoulli parameter settings for the foregoing class of queries in the presence of explicit time or accuracy constraints. as might be expected  row-level sampling is preferable when data values on each page are homogeneous  whereas page-level sampling should be used when data values on a page vary widely. our results make this intuition precise. perhaps surprisingly  we show that in many cases the optimal sampling policy is of the  bang-bang  type: we identify a  page-heterogeneity index   phi  such that optimal sampling is as  row-like  as possible if the phi is less than 1 and as  page-like  as possible otherwise. the phi depends upon both the query and the data  and a dbms or application program can estimate phi by means of a pilot sample. in many commercial database systems  pilot sampling and other iterative or adaptive sampling schemes are difficult to implement natively  so in section 1 we give a heuristic method for setting the sampling parameters that avoids the need for a pilot sample. this method requires only that the database system store a small number of summary statistics in the system catalog; these statistics are easy to collect and maintain. in section 1 we experimentally evaluate the performance of the heuristic settings relative to the optimal settings. our results indicate that the use of our heuristic approach typically yields sampling performance that is reasonably close to optimal.
1. simple sampling schemes
we now review the strengths and weaknesses of the two basic
bernoulli sampling methods and briefly discuss some implementation issues.

figure 1: expected fraction of pages fetched in row-level bernoulli sampling.
1 row-level bernoulli sampling
　a mentioned above  row-level bernoulli sampling with sampling rate q （  1  includes each row in the sample with probability q and excludes the row with probability 1   q  independently of the other rows. thus to estimate total sales using a 1% bernoulli sample from the rows of a table of individual transactions  where there may be a predicate restricting the rows that we consider   we can use a query of the form1
select sum trans.amount  / 1
	from trans tablesample bernoulli 1 	 1 
where predicate
observe that for a sampling rate of 1q% we need to scale up the sum by a factor of 1/q to compensate for the fact that we have only observed a fraction of the rows. also note that the sample size is random  but is equal on average to nq  where n is the number of rows in the table. when n is large and q is not vanishingly small  it follows from a central-limit-theorem argument that the true sampling rate-that is the ratio of the actual sample size to the table size-is within ＼1  of the nominal rate q with probability close to 1%  where.
　for simplicity  we assume throughout that all sampling is performed on base tables-such tables are stored on disk and fetched one page at a time. in this setting  the main drawback of rowlevel bernoulli sampling is the high i/o cost. in the most naive implementation  the query is  in effect  rewritten to insert an extra predicate of the form
where rand     samplingfraction / 1
where the rand   function returns a pseudorandom number uniformly distributed on the interval  1 . for this implementation  each page is read into memory and each row is tested for inclusion in the sample. the resulting i/o cost is the same as that of a complete scan.
　the i/o cost of row-level bernoulli sampling can sometimes be reduced when the sampling rate is sufficiently low. suppose  for example  that there is a b-tree index on at least one column of the table to be sampled. then we can sample by reading the leaf pages of the index; a leaf page contains pointers to a subset of the rows in the table. for each leaf page read  the bernoulli inclusion test is performed for each row whose pointer is on the page; if a row is included in the sample  the page containing the row is fetched from disk. the i/o cost of this scheme is roughly proportional to the number of index pages stored on disk plus the number of pages that contain an included row - for low sampling rates this cost will be lower than the i/o cost of a table scan. an alternative approach that does not require an index  simulates  in advance the number of rows included from each page and fetches essentially only those pages that contain an included row. acceptance/rejection techniques are used to handle variable page sizes in a manner similar to  1  p. 1 .
　as the sampling rate increases  however  the i/o cost for even the best row-level scheme increases sharply. indeed  when there are m rows on each page and no page is fetched more than once  the expected fraction f of pages fetched for a sampling rate of q is given by f = 1    1   q m. figure 1 displays f as a function of q for m = 1 rows per page. as can be seen  essentially all of the pages must be fetched whenever the sampling rate is higher than about 1%. the i/o performance of the index-assisted sampling scheme degrades even further if pages that contain more than one included row are fetched multiple times; this is often the case  for example  when the index is  unclustered  in the sense that rows are not stored on disk in the same order that the corresponding row pointers appear in the index.
1 page-level bernoulli sampling
　page-level bernoulli sampling schemes avoid the high i/o costs associated with row-level sampling. for a sampling rate q  the i/o cost of obtaining a page-level bernoulli sample is roughly a fraction q of the cost of a full scan. a careful implementation of page-level sampling can fully exploit the prefetching capabilities of the dbms and minimize both i/o and cpu costs. for example  as suggested in   instead of  flipping a coin  for each page  by generating a pseudorandom number   one can directly simulate subsequences of coin flips by generating the random skips between the pages that are selected for inclusion. that is  assuming that the table consists of m pages  we can generate the random page numbers in our sample recursively by setting n1 = 1 and nk = nk 1 + sk for k − 1  where {sk : k − 1} is a sequence of independent and identically distributed  iid  random variables having a geometric distribution:
	p {sk = i} = q 1   q i 1 	i − 1
for each k. if s1   m  then the sample is empty; otherwise  the page numbers in the sample are given by n1 n1 ... nk  where k = max{k − 1: nk ＋ m }. once we generate this list of logical page numbers  we can convert the logical numbers to physical page numbers  sort the physical page numbers  and send fetch requests to the i/o subsystem. a well known method for generating the sequence {sk : k − 1} uses the transformation
  
where {uk : k − 1} is a sequence of iid pseudorandom numbers uniformly distributed on  1  and is the smallest integer greater than or equal to x; see  1  p. 1 .
　the downside of page-level sampling becomes apparent when the sample is used to obtain a quick approximate answer to an sql aggregation query. in this setting the statistical precision of the estimated query results-as measured  for example  by the standard error-is typically low relative to row-level bernoulli sampling. there are two reasons why page-level sampling leads to higher standard errors. the first reason stems from the randomness in the sample size under bernoulli sampling. suppose that we are using row-level bernoulli sampling with a sampling rate of q to estimate the sum of the values in a specified column of a table having n rows. also suppose that the number n of rows in our sample is unusually small  so that n/n   q. in this case we should scale up the answer to our sample query by a factor of n/n  but we will only scale up by 1/q    n/n   and so our estimate will probably be low. similarly  if our sample size is unusually large  our estimate will probably be too high. the same fluctuations occur under pagelevel sampling  but are magnified by a factor equal to the number of rows per page  leading to inflated standard errors. for example  consider a table with k rows per page  and suppose that the data is uniformly distributed and randomly assigned to pages. using the variance formula  1  in section 1 below  it can be shown that the standard error is magnified by roughly a factor of k1. values of k = 1 are not unusual  in which case the standard error is inflated by an order of magnitude. this inflation is exacerbated even further if the number of rows per page varies greatly among the pages. of course the foregoing problems can  in principle  be avoided by using n/n as our scaling factor rather than 1/q. a sufficiently accurate  up-to-date estimate of n is often unavailable in practice  however  in which case we are forced to use 1/q as our scaling factor.
　the second reason for the lower precision of page-level sampling with respect to row-level sampling is that page-level sampling is sensitive to  clustering.  we say that a set of attribute values is clustered if there is a strong dependence between the value of an attribute and the page on which the corresponding row is located. to understand the effect of clustering on precision  suppose again that we are estimating a sum and consider an extreme case of  perfect  clustering: for each page  the rows on the page have identical attribute values  but the attribute values on different pages vary widely. if we are dealing with nonnegative numbers and we sample a page where the attribute value equals 1 for every row  then our current estimate of the true sum will drop precipitously; if the attribute value equals 1  then our estimate will skyrocket. these massive swings result in large standard errors. row-level sampling  or page-level sampling in which the data is unclustered  avoids such extreme fluctuations. for example  consider the problem of estimating the answer to a sum query for a database with m pages and k rows per page  where the data consists of m k   1  values equal to 1 and m values equal to a   1. it can be shown that the standard error under page-level sampling is the same as that under row-level sampling when the data is unclustered but higher by a factor of roughly k1 when the data is clustered. thus  as with the random-sample-size effect  the clustering effect can easily inflate the standard error by an order of magnitude.
1. bi-level bernoulli sampling
　for a given sampling rate q  page-level sampling is faster than row-level sampling but often produces more variable estimates. to provide greater control over this tradeoff between speed and precision  we now consider a bi-level bernoulli sampling scheme that samples pages at rate p  − q  and then  for each sampled page  samples rows at rate r = q/p. when p = q  bi-level sampling reduces to page-level bernoulli sampling; when p = 1  bi-level sampling reduces to row-level bernoulli sampling. by varying p between q and 1  we obtain a spectrum of sampling schemes. bilevel bernoulli sampling does not appear to have been well studied in the statistical literature  probably because other schemes are better suited to traditional survey sampling  whereas bi-level bernoulli sampling is particularly well suited to database sampling tasks.
　the bi-level scheme can be implemented by  in effect  inserting a row-level sampling predicate into the query  as described in section 1  and  at the lowest level of query processing  replacing the usual table scan with a page-level sampling scan as described in section 1.
　in this paper we focus primarily on the problem of sampling from a single table t in order to obtain a quick approximate answer to an aggregation query of the form
select op expression  from t
 1 
where predicate
where op is one of sum  count  or avg  and expression is a numeric expression  such as t.a or sqrt t.a/t.b   that involves one or more columns of t.1 these latter aggregation functions are the most commonly used in practice. moreover  the case of sampling from a single table is a fundamental and important one  arising not only in queries that explicitly involve only one table  but also  for example  in queries on  star  schemas that consist of a massive  fact  table-which we sample-and many smaller  dimension  tables. in general  our results apply to queries that involve joins between multiple tables  provided that sampling is performed on exactly one of the tables and each join key for the sampled table is a foreign key. we also indicate how to extend our techniques to handle complex aggregates  that is  functions of simple aggregates such as sum and count.1 in later sections we consider more complicated situations in which the select list involves multiple aggregates.
　in general  we assume that both the number of rows and the number of pages in the table are unknown  since sufficiently accurate catalog statistics are often unavailable. we also assume that the number of rows on each page is unknown  even for sampled pages. this is often the case in practice  because usually a predicate is applied to the rows of the table in addition to sampling- the database component that performs statistical computations typically sees only the rows on a given page that satisfy the predicate and does not know how many rows from the page have been excluded.
1 approximate query results
　denote by α the true result of the query in  1  when op is sum. as with row-level bernoulli sampling-see  1 -a quick approximate answer to the query is obtained by executing the query on a bi-level bernoulli sample from t and then scaling up the answer by a factor of 1/q. denote the resulting estimate of α by α . note that the scaleup factor is independent of the bi-level sampling parameter p. count queries are treated similarly; indeed  a count query can be viewed as a special case of a sum query in which the value of expression equals 1. denote the true and estimated result of a count query by r and r   respectively. we show in the appendix that α  is unbiased for α and r  is unbiased for r  i.e.  e α   = α and e r   = r  where e denotes expectation. thus the estimates have the correct value on average over repeated executions of the sampling query.
　an avg query can be expressed as a sum divided by a count. it follows that the true query result μ can be written in the form μ = α/r  and a natural estimate of μ is μ  = α/  r .  for technical reasons  we define μ  = 1 in the unlikely event that α  = r  = 1.  this approach amounts to simply executing the query on a bi-level bernoulli sample from t and then not scaling up the answer at all  because the scaleup factors of 1/q in the numerator and denominator cancel.
1 precision of the approximate results
when using sampling to obtain quick approximate answers to
symbolmeaningtset of rows in tableuset of pages in tablepjset of rows on page j in tablev i value of expression for ith row in table  v i  = 1 if row i fails to satisfy predicate w i variable that equals 1 if row i satisfies predicate and equals 1 otherwise.αjsum of the v i  values on page j of the tableηjnumber of rows on page j of the table that satisfy predicate  i.e.  sum of w i  values on page jppage-level sampling raterrow-level sampling rateqoverall sampling rate  = pr t s set of rows in sampleu s set of pages in sampleset of those rows in pj that are in sampletable 1: notation for bi-level bernoulli sampling
aggregation queries  it is crucial to gauge the precision of these answers. a common measure of precision for an unbiased samplingbased estimate θ of an unknown quantity θ is the standard error of θ   which is defined as the square root of the variance of θ . theorem 1 below  which is proved in the appendix  gives formulas for the variance of the estimators α   r   and μ  for sum  count  and avg queries. the theorem also gives formulas for estimating these variances from the bi-level sample. standard errors and standard-error estimates are obtained by taking square roots. to our knowledge  these formulas have not appeared previously in the statistical literature.
　table 1 summarizes the notation used in theorem 1. note that v i  is the value of expression evaluated for the ith row if the ith row satisfies predicate; if predicate is not satisfied  then v i  = 1.
according to this notation  and
	.	 1 
similarly  . in the following   var  denotes variance and  cov  denotes covariance.
　theorem 1. define α  α   r  and r  as above  and set μ = α/r and μ  = α/  r . then


and

unbiased estimators of var  α  and var r   are given by

and

and an approximately unbiased estimator of var  μ  is given by
 
where
	 	and	
　we emphasize that the argument to the sum  count  or avg query can be an arbitrarily complex function of the columns in the table. the  taylor linearization  method that we use in the appendix to establish the variance formulas for avg can be used to handle many other complex aggregates  such as variance and standard deviation.
　the following sample sql query illustrates the potential usefulness of theorem 1. the query produces a quick approximate estimate of total sales  along with an estimated standard error of the approximate query result  based on a sample from a table of transactions called trans. the bi-level sampling rates q  p  and r appear as parameter markers :q  :p  and :r.
with
dt1 as  select sales  sample unit for trans as s u
from trans
　　　　　　tablesample bi-level-bernoulli 1*:q 1*:p    dt1 as  select sum sales  as s sales 
sum sales /:r as alpha hat 
sum sales*sales  as s v1
　　　　　from dt1 group by s u   dt1 as  select sum alpha hat*alpha hat  as s alpha hat1 
sum s sales  as tot s sales 
sum s v1  as tot s v1 from dt1 
select tot s sales/:q as estimated total sales 
sqrt  1/:p *  1/:p -1 *s alpha hat1
+  1/:q *  1/:r -1 *tot s v1  as std error
from dt1;
　note that two extensions of the current iso standard are required to formulate this query. syntax for the bi-level-bernoulli sampling method must be supported and the sample unit keyword must be added. the latter keyword denotes a system-defined identifier such that two rows have the same sample unit value if and only if they are always jointly included in or excluded from a sample. in the current setting  the sample unit of a row can be taken as the identifier of the page on which the row resides.1
1. optimal sampling
　theorem 1 specifies the standard error of sampling-based estimates for sum  count  and avg queries under the bi-level bernoulli sampling scheme as functions of the page-level sampling rate p and row-level sampling rate r. we now determine the optimal values of p and r for an important class of sampling queries  given some simple cost models for query processing along with user-specified constraints on cost or accuracy. a key result of this paper is that choosing the optimal plan often boils down to estimating the value of a page heterogeneity index  phi  that depends on the query and the data. a dbms or application program can  in principle  estimate the phi using a small pilot sample. the optimality results in this section are also important because they allow us to assess the quality of various heuristic schemes for choosing p and r  see section 1  that do not require a pilot sample and are therefore easier to natively implement in an existing commercial dbms.
1 problem formulation
　our first step is to formulate several cost models that relate query-processing performance to the values of the bi-level sampling parameters. for ease of exposition  we restrict ourselves to relatively simple models  but our approach can be extended to more complicated settings; we expect that the results will be qualitatively similar to those obtained here. one useful cost model is the i/o cost model  in which the query processing cost c is assumed to be dominated by the cost of fetching data from disk. formally  the model is c = c p   where c p  is a strictly increasing function of p. examples include specific models such as c p  = c|u|p  where c is a constant  and
.
where x is a real number. the first model assumes a fixed cost per page fetch-namely c-and c p  is the expected fetch cost. the second model also assumes a fixed cost per page fetch  and c p  is the probability that the overall fetch cost exceeds a specified upper limit x.
　an alternative simple cost model that can explicitly take into account cpu costs is the linear cost model given by
c p r  = c1|u|p + c1|t|pr.
the assumption here is that the system incurs a fetch cost of c1 per page sampled and a processing cost of c1 per row sampled; c p r  is the expected total cost. this model can be applied to situations in which each sample row requires data cleansing  transformation  or perhaps an expensive function call to evaluate the expression that appears in  1 .
we focus on four optimization problems:
  problem 1: for a specified overall sampling rate q = pr and specified maximum processing cost cmax  minimize the standard error.
  problem 1: for a specified overall sampling rate q = pr and specified maximum variance vmax  minimize the processing cost.
  problem 1: for a specified maximum processing cost cmax  minimize the standard error.
  problem 1: for a specified maximum variance vmax  minimize the processing cost.
　the first two problems correspond to the situation in which a user query contains an iso-style sampling clause of the form
tablesample system  q  
along with a time or accuracy requirement  where system sampling corresponds to bi-level bernoulli sampling. the latter two problems correspond to scenarios-not currently supported in the iso standard-in which the user requests sampling and specifies a time or accuracy constraint but lets the system determine the sampling rate. observe that minimizing the standard error of a sampling-based approximation is equivalent to minimizing the variance.
for convenience  we assume throughout that the cost function c

optimization	cost	optimal solution:	optimal solution:	feasibility
1
	p  = q  r  = 1	vmax −  q 1   1 min a b 
1 i/onone
1 i/onone
1 linear		p  = q  r  = 1	
1 linear		p  = q  r  = 1	vmax −  q 1   1 min a b 
1 linear	  	  	none
		r  = 1
1 linear		none

table 1: optimal sampling rates.is left continuous with c 1  = 1 and set1pmax = sup{p （  1 : c p  ＋ cmax }.
to avoid trivialities  we assume throughout that 1   q   1 when analyzing optimization problems 1 and 1.
1 optimality results
　we first consider optimal sampling schemes for obtaining quick approximate answers to queries of the form  1   where the select clause of the query contains a single aggregate. in order to state our main result  we first define the page-heterogeneity index  phi  corresponding to a count  sum  and avg query:
 
and
.
　roughly speaking  the phi measures the variability of values within a page relative to the variability of values between pages  with respect to the aggregate of interest. we sometimes need to reference the numerator and denominator of the phi separately  and so we write φα = bα/aα  φr = br/ar  and φμ = bμ/aμ.
　the following result  which is proved in the appendix  gives the optimal sampling rates for the various optimization problems and cost models.
　theorem 1. the bi-level bernoulli sampling rates p  and r  that solve the above optimization problems exist and are as given in table 1  provided that for each problem the feasibility condition in the rightmost column holds.
in table 1  the specific formula for the phi-denoted φ-depends upon the particular aggregate involved  as do the formulas for b and a  the numerator and denominator of φ.
　the results in table 1 for optimization problems 1 and 1 conform to intuition in that  if pages look very different from each other but the data values on each page are relatively homogeneous  then the optimal scheme examines a large number of pages  while looking at only a few rows on each page. conversely  if most pages are similar  containing a wide range of data values  then the optimal scheme examines relatively few pages  but looks at many rows on each page. our results make this intuition precise. what is striking and perhaps surprising is the  bang-bang  form of the optimal solution and the central role of the phi: if φ   1  then the sampling is as row-like as possible  whereas if φ − 1  then the sampling is as page-like as possible.
　the solutions to optimization problems 1 and 1 are trivial under the i/o model. since cpu costs are ignored and there is no overall constraint on the sampling rate  setting r  = 1 and looking at every row on a sampled page is clearly optimal. then p  is chosen so as to sample as many pages as possible subject to the cost constraint  problem 1  or to sample as few pages as possible while ensuring that the variance does not exceed vmax  problem 1 . the solution is more interesting under the linear cost model  and for the most part does not have the extremal character of the other solutions. the phi still plays a key role in determining r   however.
　observe that when φ   1  the optimal sampling scheme typically does not correspond to either pure row-level or pure pagelevel sampling  so that the full generality of the bi-level sampling mechanism is needed. results that are qualitatively similar to those in theorem 1 can be obtained for complex aggregates other than avg.
1 multiple aggregates
　the foregoing development applies to aggregation queries in which the select list contains a single aggregate. when there are k   1 aggregates  one simple approach is to reformulate the optimization problems so that the standard error now refers to the square root of the average variance over the aggregates in the select list. then the conclusion of theorem 1 holds  provided that the constants a and b are replaced by new constants  is replaced
by ．b/a．. here each  ak bk  pair corresponds to the denominator and numerator of the phi when the kth aggregate in the select list appears alone. a more sophisticated approach that minimizes the maximum standard error is discussed in the appendix.
1 use of pilot sampling
　in the foregoing development  the parameter values a and b are needed to compute the optimal sampling rates p and r. these parameter values are  however  usually unknown a priori. one solution to this problem is to obtain a small pilot sample s using initial  small  sampling rates  and then use this sample to estimate a and b  as well as φ = b/a.
for example  consider a sum query. in this case we have aα =
. set
 
where  as before  pj denotes the set of rows on page j in the table and denotes the set of those rows in pj that are also in the pilot sample. observe that  by the results in the appendix 

is unbiased for αj1  so that unbiased estimators for aα and bα are given by
	 and	 
where  and denote the sets of rows and pages in the pilot sample and is the overall pilot sampling rate. we then estimate φα as φ α =  bα/a α.
　count and avg queries are handled similarly. we expect that  in practice  r will typically be set equal to 1  so that the pilot sample will be a pure page-level bernoulli sample. in this case the preceding formulas simplify accordingly.
1. a heuristic sampling scheme
　because most commercial database systems do not currently provide support for pilot sampling  we give a  distinct-value  heuristic method for approximating the optimal sampling schemes given in the last section. the heuristic is motivated by our analytical results and avoids the need for a pilot sample. our focus is on what is currently the most important practical situation  in which the user has specified a sampling rate q as in optimization problems 1 and 1 of the previous section.
1 basic method
　we focus initially on the case in which there is a single aggregation function in the select list and the argument to this function is a single column name  e.g.  the query is of the form
select sum col1  / q
from t tablesample system 1*q  where predicate
the heuristic scheme assumes that we store in the system catalog three statistics for the column of interest:
  δ = avg δ1 ... δm   where δi is the number of distinct col1 values on the ith page;
  γ 1  = var a1 ... am   where ai is the average of the col1 values on the ith page; and
  γ 1  = avg v1 ... vm   where vi is the variance of the col1 values on the ith page.
the statistics δ  γ 1   and γ 1  can be quickly estimated from a small random sample of pages. alternatively  these statistics can be computed exactly and incrementally maintained using one-pass updating schemes as discussed  for example  in .
　we also assume that the dbms stores the number of pages |u|  as well as ρ  the average number of rows per page. most commercial systems store these latter two statistics. note that  for the purposes of determining the sampling rates p and r  it suffices for statistics such as ρ or |u| to be reasonably accurate; a very high degree of precision is not needed since these statistics are being used merely for query optimization and not for computing actual query results.
　if the data values are highly skewed-with a few rows significantly influencing the query result-then sampling will be ineffective in any case. we therefore seek a heuristic that works for reasonably well behaved data.
the intuition that underlies the heuristic is as follows.
1. if there are many distinct data values within a page  then thesampling scheme should resemble pure page-level sampling in order to observe these values; if there are few distinct values per page then the sampling scheme should more closely resemble row-level sampling.
1. the only situation in which the above intuition fails is whenthere are many distinct values on a page but the numerical variation between these values is small relative to the variation within the entire data set  so that row-level sampling is more appropriate.
the heuristic scheme therefore tries to observe a fraction f = f γ δ  of the distinct values on each sampled page  where γ = γ 1 /γ 1  and
.
when γ is close to 1  so that there is little variability between the data values on a page relative to the variation between pages  then f γ δ  「 1/δ  and we attempt to see roughly one distinct value per sampled page. when γ is very large  so that the variation within a page dominates the variation between pages  then f γ δ  「 1  and we try to observe all of the distinct values on a page.
　to set the value of r  suppose that the number of rows on each page is exactly equal to ρ  the number of distinct values is exactly equal to δ  and each distinct value has the same number of duplicates on a page  namely ρ/δ. fix j （ u and suppose that page j
　has been included in the set of sample pages. denote by dj the set of distinct values on page j. set id = 1 if distinct value d （ dj appears at least once in the row-level sample of values from page j  and set id = 1 otherwise. set  so that dj is the number of distinct values on page j that are included in the sample.
observe that
e  id  = p {at least one row with value d included}
= 1   p {no rows with value d included} = 1    1   r ρ/δ.
it follows that
 
and so the expected fraction of distinct values observed is given by e  dj /δ = 1    1   r ρ/δ. we obtain our heuristic sampling rates r  and p  by equating this expected fraction with our target fraction f γ δ . specifically  we set r  = max r1  q  and p  = q/r   where. as with optimal sampling  the heuristic scheme requires the full generality of the bi-level sampling mechanism.
1 constraints on processing cost
　the above approach can be modified to explicitly take into account constraints on the processing cost. for example  under the i/o cost model we have seen that a constraint of the form c p  ＋ cmax translates into an upper bound on p of the form p ＋ pmax. denoting by the value of r computed as in the previous subsection  we then take our final solution as and p  = q/r . the foregoing technique also applies under the linear cost model  with pmax =  cmax   c1|t|q / c1|u| ; see the discussion in the appendix.
1 multiple columns
　we next consider modifications of the basic heuristic when multiple columns are involved in the select list  because of multiple  possibly complex  aggregates each computed on a different column  a single aggregate computed on a function of multiple columns  or a combination of these scenarios. an example of such a query is:
select
sum t.col1 * t.col1 /1 
	avg t.col1  * avg t.col1 	 1 
from t tablesample system 1 
where predicate
　to handle a query of the form  1  involving columns c1  ... ck  a simple approach that has worked well in preliminary experiments is as follows. first compute heuristic sampling rates  p1 r1  ...  pk rk  individually for each column  as described previously. then select combined sampling rates pcomb and rcomb as the geometric means of the individual rates:
	pcomb =  p1 ，，，pk 1/k	and	rcomb =  r1 ，，，rk 1/k.
the motivation for the geometric mean is that if we want an overall sampling rate of q  as in problems 1 or 1   then we can choose each individual pi and ri so that piri = q  and it follows that pcombrcomb =  p1p1 ，，，pkrk 1/k =  qk 1/k = q
as desired. other approaches are possible  e.g.  based on other rules for combining the pk's and rk's or on the use of  combined  values of γ and δ.
1. experimental results
　in this section we describe an empirical study that tested the performance of the distinct-value heuristic with respect to the theoretically optimal sampling schemes derived in section 1. to save space  we focus primarily on sum queries  because such queries tend to be the most sensitive to sampling fluctuations and therefore stress the heuristic the most; experiments with count and avg queries  not reported here  verify this assertion. we consider both the situation in which the aggregate in the select clause involves only one column and the case in which the aggregate involves two columns. our experiments were performed using a large number of synthetic tables-1 for the one-column experiments and 1 for the two-column experiments-to test the robustness of the heuristics over a wide range of data characteristics. we also ran almost 1 experiments on two real-world databases  one containing 1 gb of automotive data and the other containing over 1 years of detailed baseball statistics .
　the parameters that determine the characteristics of the synthetic tables are as follows:
  rowcard - the target row cardinality of the table.
  pagecard - the number of rows per page.
  d - the target number of distinct values in the aggregation column;
  cluster - the cluster factor. if the cluster factor equals 1  then the data is  perfectly  clustered on the pages. in effect  the data values appear on pages in sorted order. if the cluster factor equals 1  then data is assigned to pages randomly. intermediate values of the cluster factor correspond to scenarios that fall between these extremes. more specifically  we completely cluster the data and then use a modified random shuffling algorithm: for m = rowcard rowcard   1 ... 1  generate pseudorandom numbers um and vm on  1   and then swap rows m and  if and only if um ＋ 1   cluster.
  α - a parameter that controls the range of the data values. specifically  the distinct values v1 v1 ... vd depend on α through the relationship vn = nα for 1 ＋ n ＋ d.
  θ - the data skew factor. the numerical values in the aggregation column are distributed according to a generalized zipf distribution with parameter θ. that is  distinct value vn occurs  times for n = 1 ...  where  is the greatest integer less than or equal to x  and β = rowcard. observe that the actual number of distinct values in the column might be less than the target value of d and the actual number of rows might be less than the target value of rowcard due to truncation effects. also observe that a value of θ = 1 corresponds to a perfectly uniform data distribution.
  mode - a parameter that determines the relationship between the value of the aggregation attribute and its frequency. when mode = 1  distinct value v1 is the most frequent  distinct value v1 is the next most frequent  and so forth. when mode = 1  distinct value vd is the most frequent  distinct value vd 1 is the next most frequent  and so forth. when mode = 1  distinct value v1 is the most frequent  vd is the second most frequent  v1 is the third most frequent  vd 1 is the fourth most frequent  and so forth. the case mode = 1 is the inverse of the case mode = 1: distinct value v1 is the least frequent  vd is the second least frequent  v1 is the third least frequent  vd 1 is the fourth least frequent  and so forth.
1 one-column experiments
　in these experiments  we considered the use of bi-level sampling for estimating the answer to the query select sum col 
parametervalue s parametervalue s rowcard1pagecard1d θ1  1  1 〜 1
1  1  1  1  1cluster mode1  1  1 1  1  1  1α1q1  1  1  1table 1: parameter values used in one-column experiments

figure 1: distribution of relative standard errors for the distinct-value heuristic  1 experiments on synthetic data .
from t. the goal was to choose the row-level sampling rate r and page-level sampling rate p so as to minimize the standard error of the estimated sum subject to the constraints p ＋ pmax and pr = q  where q is a user-specified overall sampling rate. that is  we were basically solving problem 1 of section 1 under the i/o cost model.1 table 1 displays the parameter values used to generate 1 synthetic tables  as well as the various sampling rates that we employed. we used 1 values of the sampling rate q for each table  which resulted in a total of 1 experiments. in each experiment  we set the maximum page sampling rate pmax equal to 1q; experiments using values other than 1q yielded very similar results. some of the parameters in table 1 have only one value; preliminary experiments indicated that varying these parameters did not substantially effect the experimental results. for each combination of parameter values and sampling rate  we computed the ratio of the standard error using the distinct-value heuristic to the minimum possible standard error under the optimal choice of p and r.
　figure 1 shows the distribution of these relative standard errors. use of the distinct-value heuristic results in the minimum possible standard error in 1 cases  1%   and the relative error is below 1 in 1% of the cases  with a median relative error of 1.
　figure 1 shows the effects of both clustering and data skew on the performance of the distinct-value heuristic. for a specified value of the cluster factor  the figure displays the relative standard error  with respect to optimal sampling   averaged over all experiments in which the cluster factor is equal to the specified value. similar averaging was performed for the zipf parameter. observe that the distinct-value heuristic performs optimally precisely in the most challenging cases  namely those tables having a high degree of clustering or skew.
　figure 1 also shows the effect of the overall sampling rate q on the performance of the distinct-value heuristic. as can be seen  this

figure 1: effect of clustering  skew  and sampling rate q on relative standard errors for distinct-value heuristic.

figure 1: distribution of relative standard errors for the distinct-value heuristic  1 experiments on real-world data .
performance is quite stable over a wide range of sampling rates. looked at another way  the relative performance of the heuristic at a given sample size remains stable as the size of the database increases.  indeed  this is why it suffices to report experimental results for a single database size.  observe that the relative standard error is actually somewhat better at the lower sampling rates that are typically encountered in practice. one possible reason for this behavior is that the optimal standard error is smaller at higher sampling rates  so absolute deviations from the optimal error are more noticeable. the number of distinct values d and the distribution shape  the mode parameter  appeared to have relatively little effect on the relative performance of the distinct-value heuristic.
　figure 1 displays the results of our experiments on our two realworld databases. we used 1 different numeric columns with sampling rates of 1% and 1%  for a total of 1 experiments. we actually used avg as our aggregation operator in order to obtain semantically meaningful queries. as can be seen  the performance of the distinct-value heuristic for the real-world data is similar to that for the synthetic data  with better worst-case performance. the standard error is within a factor of 1 of optimal in all cases  and coincides with the minimum possible error 1% of the time. though not reported here in detail  the sensitivity of the heuristic's performance to clustering  skew  and so forth for the real-world data is very similar to that for the synthetic data.
1 two-column experiments
　we next considered the use of bi-level sampling to estimate the answer to the query select sum col1*col1  from t. in applications  column col1 might contain the number of items sold
parametervalue s parametervalue s rowcard1pagecard1d1
cluster1  1 〜 1
1  1d1
cluster1  1 〜 1
1  1θ1.1  1  1θ1.1  1  1mode1α1q1  1table 1: parameter values used in two-column experiments

figure 1: distribution of relative standard errors for distinctvalue heuristic on two-column queries  1 experiments on synthetic data .
and column col1 might contain the price per item. as before  the goal was to choose sampling rates r and p so as to minimize the standard error of the estimated sum subject to the constraints p ＋ pmax and pr = q  where q is a specified overall sampling rate. to apply the distinct-value heuristic  we computed heuristic values of p and r for each column individually  and then obtained the combined values of p and r by taking geometric means as described in section 1. table 1 displays the parameters used to generate 1 synthetic data sets. in the table  a subscript i on a parameter refers to the ith column  i = 1 . parameters with only one experimental value are displayed without a subscript and apply to both columns. as before  we set the maximum page sampling rate pmax equal to 1q in each test case.
　the distribution of the relative standard errors for the distinctvalue heuristic is shown in figure 1. the performance of the heuristic for the two-column experiments is even better than that for the one-column case. in the two-column experiments  the median relative error is 1  the minimum possible standard error is attained in 1 cases  1%   and the relative standard error is below 1 in 1% of the cases. as can be seen from figure 1  the effects of clustering are similar to those in the one-column case.
　figure 1 displays the results of our experiments with two-column queries on real-world databases. we used 1 different pairs of numeric columns with sampling rates of 1% and 1%  for a total of 1 experiments. as before  we used avg as our aggregation operator. the performance of the distinct-value heuristic for the real-world data is similar to that for the synthetic data  again with better worstcase performance. the standard error is within a factor of 1 of optimal in all cases  and is indistinguishable from optimal about 1% of the time.
1. conclusions and future work
　the bi-level bernoulli sampling scheme presented here provides an improved means of processing iso-style sampling queries. our scheme permits a systematic tradeoff between processing speed and statistical precision. for an important class of sampling queries 

figure 1: effect of cluster factor on relative standard errors for the distinct-value heuristic.

figure 1: distribution of relative standard errors for the distinct-value heuristic on two-column queries  1 experiments on real-world data .
we have derived the optimal parameter settings for our sampling scheme  and have provided an effective practical heuristic method for approximating these optimal parameter settings while avoiding the need for a pilot sample. in future work  we plan to extend our results to queries that sample from multiple tables  as well as workload-driven sampling methods  thereby combining our results for bi-level bernoulli sampling with sampling techniques such as those in  1  1  1  1  1  1 .
　a number of further sql and dbms enhancements are needed to fully exploit bi-level bernoulli sampling. suppose  for example  that a dbms is modified to automatically compute optimal or near-optimal sampling parameters  e.g.  using the heuristic scheme. then the user must be able to specify to the dbms the maximum allowable processing time or standard error. conversely  the dbms must  at a minimum  tell the user its choice of bi-level sampling parameters  so that the user can compute standard errors manually  as in the example at the end of section 1. it appears extremely difficult  if not impossible  to try and be more ambitious  and modify a dbms to permit specification and subsequent automatic computation of standard errors for all possible aggregates. it may be reasonable  however  to provide this functionality for the most common simple aggregates that are likely to appear in sampling queries. the required extensions lead to nontrivial challenges in both system and sql language design.
acknowledgements
thanks to m. cilimdzic for running experiments on the automotive data and to p. brown for pointing us to the baseball database.
