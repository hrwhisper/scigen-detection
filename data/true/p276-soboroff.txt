existing methods for measuring the quality of search algorithms use a static collection of documents. a set of queries and a mapping from the queries to the relevant documents allow the experimenter to see how well different search engines or engine configurations retrieve the correct answers. this methodology assumes that the document set and thus the set of relevant documents are unchanging. in this paper  we abandon the static collection requirement. we begin with a recent trec collection created from a web crawl and analyze how the documents in that collection have changed over time. we determine how decay of the document collection affects trec systems  and present the results of an experiment using the decayed collection to measure a live web search system. we employ novel measures of search effectiveness that are robust despite incomplete relevance information. lastly  we propose a methodology of  collection maintenance  which supports measuring search performance both for a single system and between systems run at different points in time.
categories and subject descriptors: h.1  information storage and retrieval : systems and software-performance evaluation  h.1 online information services
general terms: experimentation  measurement keywords: retrieval test collections
1. introduction
　search is the most popular internet application after email. the proliferation of information available on the web makes search a critical application. the emergence of the web as the world's dominant information environment has created a surge of interest in search  and consequently important advances in search technology. however  it is difficult to measure the effectiveness of web search algorithms because our current methodologies assume that the document collection does not change.
　the dominant evaluation procedure is known as the cranfield or test collection methodology. a test collection consists of a set of documents  a set of information need descriptions  possibly including actual queries   and a mapping of needs to the documents that are relevant to them.
in response to each information need  a query is formulated
this paper is authored by an employee s  of the united states government and is in the public domain.
sigir'1  august 1  1  seattle  washington  usa.
acm 1-1/1.
and documents are retrieved from the collection using two  or more  search algorithms. the results of each search are examined to see which documents are relevant and which are not. if significant and noticeable differences in effectiveness are observed  and the differences are consistent across multiple test collections  we can conclude that one search algorithm is better than another.
　the cranfield methodology is so named after the first formalized measurements of search systems conducted by cleverdon at the college of aeronautics at cranfield . it was subsequently refined by many  most notably by sparck jones and van rijsbergen in 1  and in recent years in the scope of the text retrieval conferences  trec  . in trec parlance  the information needs are called  topics   and the mapping of topics to relevant documents is called the  relevance judgments  or  qrels . a  run  is the set of documents retrieved by some search algorithm for each of the topics in the test collection.
　the cranfield paradigm makes several assumptions in order to simplify and operationalize the measurement of search effectiveness. the assumption that we are chiefly concerned with in this paper is that the document collection is static with respect to the runs being measured. a second assumption that we address is that the relevance judgments are complete - all documents are judged with respect to all topics. the trec pooling process has shown that judgments need not be complete in order to accurately measure the relative performance of two or more systems  1  1 . since we are interested in the problem of collection decay  where our document collection and relevance judgments evolve out from under us  we will focus on measures which do not rely on the completeness assumption at all  such as bpref .
　the cranfield paradigm further assumes that the information needs  and thus the relevance judgments  are also static  so that for example if one wishes to measure the quality of a  find more like this  facility  it is assumed that the initial set of search results do not change the user's definition of what is relevant. other assumptions include the notion that the search process can be abstracted away from such vital system details as how queries are created and how results are presented to or indeed used by the end user. in this present work we retain these assumptions as part of the experimental design.
　the phenomenon of change and decay on the web has been well studied. cho and garcia-molina tracked 1 web pages daily over the course of four months in order to specify design choices for an incremental crawler . fetterly et al. expanded on that study  tracking more than 1 million web pages weekly over 1 weeks and also looking at content changes within pages . ntoulas et al. crawled 1 different complete sites weekly  and examined change in linkage  changes in page content  and new pages being created . with respect to information disappearing on the web  bar-yossef et al. looked closely at soft- and hard1 errors  and proposed models of web decay based on a markov chain model of dead link propagation inspired by pagerank . these studies all examine general web crawls in order to understand change on the web as a whole. brewington and cybenko looked at the change rates of pages specifically requested by users of a web clipping service . this last work is the closest to what we have done here  except that we are concerned with pages that are relevant to particular search topics. we are also particularly concerned with change specifically as it affects measurements of search quality.
1. problem definition
　there are many challenges to measuring search effectiveness on the web using the test collection methodology. we believe the key challenge is the requirement of a static document collection. holding the document collection fixed allows for straightforward reproducibility of results  but is a limiting requirement when we wish to measure search on the web. we propose allowing the document collection to change  while keeping the topics and the relevance judgments initially fixed. specifically  consider that we have a set of topics and relevance judgments that were constructed for a collection of web pages  and we wish to measure a set of live web search algorithms using them. we would rather avoid making any new relevance judgments if at all possible. if the documents are allowed to change as they do on the live web  we must account for several possible cases: judged documents will change over time  new pages will appear that are not in the collection  and the runs being measured may be collected at different points in time. first  documents in the collection which have already been judged are likely to have changed  or may no longer exist at all. a relevant document which no longer exists on the web is certainly no longer relevant. if it still exists but its content has changed  we might compare the new document's similarity to the judged document using standard ir similarity measures or a nearduplicate-document measure  1  1  1 . in this paper  we choose a simpler strategy which highlights the limits of our approach  and assume that a changed document is no longer relevant until we devote such resources to judge it anew. for documents judged not relevant  we assume that the page remains irrelevant to the topic even if it changes. we call a document valid if it has not changed since it was initially judged  or if we have re-examined the document and applied a new relevance judgment. a valid topic is one that has valid documents.
　needless to say  new web pages have come into existence since the initial relevance judgments were compiled  and some of these may be relevant. this can be more or less of a problem for evaluation depending on the timeliness of information desired by the searcher. rather than make any guess about the relevance of new unjudged documents  we monitor how they are retrieved and how they might affect our determination of the relative effectiveness of the search engines being measured.
lastly  it may be the case that the runs which we want to compare may have been been executed at different times or on different web crawls. this is likely to be the case when we want to compare live web search engines  but consider also that we may wish to examine several parameter settings of an engine or group of engines on a single large web snapshot  using existing relevance judgments which predate the snapshot. in this paper  we measure a single search engine and are careful to collect our runs within a short period of time  but in general one can use our methodology to compare runs done at different times or compare multiple engines. in these cases  to maximize fairness the set of relevance judgments should be constrained to the intersection of valid documents  so that runs are compared over documents which they all have equal opportunity to rank.
　in the next section  we examine the decay of relevance information in an existing test collection. we then illustrate the affect of collection decay over time on our retrieval effectiveness measures using a set of trec runs. we also present a small experiment measuring retrieval runs from a live web search engine using the decayed relevance judgments. the experiment motivates a maintenance regimen for test collections in order to measure search in dynamic collections.
1. data
　in this paper we use the gov1 collection from the trec terabyte track . the goal of the terabyte track is to scale information retrieval experiments beyond the gigabyte range it typically works in today  and to study how that scaling affects the experimental methodology. the gov1 collection is a fairly exhaustive crawl of us federal and state government web pages collected in the winter of 1  and contains about 1 million web pages or about 1gb of text.  there is an associated 1gb of image and binary data which are not typically searched in retrieval experiments  but which are used when making relevance judgments.  according to cho and garcia-molina   .gov pages tend to be much more static than those in other domains. fetterly et al.  confirmed this and also found that whereas generally longer pages change more often  this is not the case in .gov. thus  rates of change that we observe here are likely to be slower than on the web in general.
　in trec 1 and 1  two sets of fifty topics were created for the gov1 collection. these topics are general informational searches  such as might be done by someone compiling a research report. they consist of a short title  often used as a query   a sentence-length description  and a narrative paragraph which defines what the user expects the search system to return. there are 1 topics numbered 1; topic 1 was dropped from the 1 evaluation because no relevant documents were found for it. in those trec cycles  research teams submitted dozens of runs consisting of the top 1 ranked results for each topic according to their search engines. the top hits from two runs from each group were collected into a pool for each topic and judged by the nist assessor that created the topic. this process yielded 1 relevance judgments for these 1 topics. we use this combined topic set in order to maximize the number of usable topics after time is taken into account.
　to gather the history of each judged page since the crawl was done  we consulted the internet archive.1 using their

1
http://www.archive.org/
wayback machine service  we downloaded page revisions since february 1th  1  the end date of the gov1 crawl. only 1 of the judged pages were present in the wayback archives; we will presume for lack of better information that the others disappeared immediately after the gov1 crawl. we obtained a total of 1 page versions  an average of 1 revisions per page. of these  1 page versions were reported as present in the archives  but were not available due to system downtime. since in this study we work with the timestamps alone  we did not worry about the content of these missing versions.
　figure 1 illustrates the  lifetimes  of topics 1  the trec 1 topics   when we assume that a document becomes irrelevant the first time it changes. each topic's line shows the number of unchanged relevant documents remaining each day. the longest line  topic 1  extends for 1 days. that is in fact the longest lifetime of all 1 topics and represents the extent of historical information available from the internet archive at the time of writing. coverage is more complete for the first 1 days. some topics are more volatile  with their relevant documents disappearing quickly  while others exhibit a more gradual drop-off. at the end of the change history  there are on average 1 relevant documents remaining per topic.
　the distribution of times between changes for both relevant and irrelevant pages is shown in figure 1. the longest gap between changes that we observed was 1 days. 1% of gaps represent same-day changes; another 1% are 1-day gaps. the average gap between changes is 1 days and the median is 1. this supports previous findings that .gov pages change slowly. there are also peaks around 1 and 1 days which are due to default page revisit policies in the internet archive crawls. according to the first-change heuristic  the number of relevant documents decreases below 1% of the original after 1 days for the average topic.
1. measures for decayed collections
　the question of measures is critical when working with dynamic collections. in a sense  the relevance judgments are always incomplete  even less so than in a static test collection. traditional retrieval metrics such as mean average precision  map   precision at the top 1 documents retrieved  p 1   and mean reciprocal rank  mrr  of the first relevant document depend completely on the ranks of the relevant documents which have been retrieved by the system; unjudged retrieved documents are considered to be irrelevant. in our situation here  where so many of the documents are unjudged due to either being outside the collection or having changed since they were judged  such a measure would mostly indicate the sparsity of our relevance data rather than any comparative measure of the runs. instead  we use a relatively new measure  bpref  to compare the runs  and consider carefully which documents we should try to judge in order to improve the picture.
　the bpref measure  proposed by buckley and voorhees in 1   computes a preference relation of whether judged relevant documents are retrieved ahead of judged irrelevant documents. thus  it is based on the relative ranks of judged

figure 1: timelines of the number of valid relevant documents for topics 1 of the trec 1 terabyte test collection.

days between changes
figure 1:	histogram of time gaps between page changes.
documents only. the bpref measure is defined as1 bpref = 
where r is the number of judged relevant documents  n is the number of judged irrelevant documents  r is a relevant retrieved document  and n is a member of the first r irrelevant retrieved documents. bpref can be thought of as the inverse of the fraction of judged irrelevant documents that are retrieved before relevant ones. bpref and mean average precision are very highly correlated when used with complete judgments. but as judgments are degraded  in buckley and voorhees' study  by taking random samples of the judgments and the collection   rankings of systems by bpref still correlate highly to the original ranking  whereas rankings of systems by map do not .
　to better understand how bpref and map behave as the collection decays  we examined trec runs from the 1 terabyte track at one-week intervals through the collection change history. this is a different approach than buckley and voorhees took  in that we are observing the real-world  downsampling  of the collection over time. furthermore  we are able to compare bpref and map in the trec terabyte collections which were not available to them.
　figure 1 shows the map and bpref scores for each of the 1 runs from trec 1 when measured against the qrels that remain valid each week. the absolute value of the score is not important  but that shape of each curve is. as the collection decays  map decreases. bpref fluctuates somewhat  and actually increases as we lose more and more relevance information. this much is consistent with the findings of buckley and voorhees. at the end of the graphs  bpref drops

1
 this definition of bpref corrects a bug in  and follows the actual implementation in trec eval version 1; see the file bpref bug in the trec eval distribution for details.

figure 1: map and bpref scores for the trec 1 runs  scored according to the qrels remaining after each week. each subgraph shows a single run. past week 1  insufficient judged documents were retrieved to use even bpref.
sharply because we have only 1 judged documents total remaining to be retrieved  out of 1 in week 1. this is a smaller percentage than buckley and voorhees examined. even though bpref does show some fluctuations as relevance information decays  the relative ordering of systems according to bpref remains fairly close to the order in week 1. figure 1 shows the correlation of weekly system rankings to the original ranking using kendall's tau. whereas the correlation using map falls below 1 at 1 weeks  the lowest correlation for bpref during the entire period is 1 at week 1. thus  when the systems are compared using the bpref measure  we arrive at a consistent ordering despite severe decay in relevance data.
　note that using trec runs to illustrate the effect of collection decay is anachronistic because the runs were per-
formed on the gov1 collection as it was initially compiled  and the decay we observe happens after this point. in an operational setting  the runs always come from a document

week
figure 1: kendall's tau correlation of weekly system rankings to the week 1 ranking. bpref agrees more closely than map to the original ranking as the collection degrades.
collection that is more recent than the relevance judgments. in the next section we conduct just such an experiment.
1. measuring search engines
　we ran a small experiment to test the methodology for measuring  live web  search using a popular web search engine. this experiment tests if shorter or longer queries give better performance for the engine in question. we hypothesize that shorter queries will be more effective since most search engines combine terms in a noisy-and formula.
　as stated above  the 1 trec terabyte topics include a short title field and a sentence-length description field. for the short queries  we used the title field. for the long queries  we added description field words which were not present in the title. stop words were removed from both long and short queries. in some cases  the description text includes phrases in quotation marks; we retained these quoted phrases since the search engine allows this as an operator but removed most other punctuation. long queries were limited to nine terms  counting quoted phrases as a single term . all queries included a search restriction requiring that hits come from .gov sites.
　for each query  we attempted to retrieve the top 1 documents. this was a compromise between the limitations of the search engine api and the need for our rankings to go deep enough to find judged documents. for some queries the search engine returned less than 1 documents. for each search result  we checked to see if the url corresponded to a document in the gov1 collection  since these are the only documents for which we have judgments. if we were trying to measure multiple search engines or a single search
engine over time  we would need to restrict ourselves to the intersection of retrieved documents between the engines in order to ensure a fair collection.
run	retr.	in gov1	judged	rel

short-q	1  1% 	1	 1% 	1	 1%  in decayed qrels:	1	 1% 	1	 1% 

long-q 1  1%  1  1%  1  1%  in decayed qrels: 1  1%  1  1% 
table 1: number of retrieved documents present in the collection  judged  and relevant for the two runs.
　we also observed a somewhat amusing phenomenon  which is that if you search for trec topics after the trec conference cycle is completed  you tend to find the trec topic file high up in the ranking. fortunately  these pages are not in the collection and are thus ignored.
1 determining valid topics
　we first look at how many documents returned by the search engine occur in the collection  how many of those were judged  and how many were judged relevant. table 1 shows that less than a third of retrieved documents are in the collection  one-half to two-thirds of these were judged  and one-third to one-half of these last are relevant. the long-query run finds somewhat fewer gov1 documents and only half as many relevant documents as the short-query run  a clue that our hypothesis may turn out to be correct. these searches were conducted after the epoch of our historical data on the relevance judgments  but for simplicity we will assume that the revision data we have is current. recall that we assume pessimistically that the first change to a relevant page reverts the page to an unjudged status. we generate the set of relevance judgments that corresponds to pages that have remained unchanged. the lines labeled  decayed qrels  in table 1 indicate how many judged and relevant documents were retrieved. the short-query run retrieved no judged documents for topics 1  1  1  1  and 1. the long-query run retrieved no judged documents for 1  1  1  1  1  1  1  1  1  1  and 1. we are left with 1 valid topics to compare the two runs  a good-sized topic set.
1 per-topic results and analysis
　over the 1 topics  the short-query run has an average bpref of 1  and the long-query run scores 1  supporting our hypothesis that short queries perform better than long queries for this search engine.
　however  despite having 1 valid topics  we still have very little data with which to measure these two runs. the shortquery run has on average only 1 judged and 1 relevant documents per topic; the long-query run  1 judged and 1 relevant. furthermore  the short-query run finds no known relevant documents for 1 of these topics  and the long-query run finds no relevant for 1.
　even though bpref is designed for our degraded collection scenario  by using topics with only one or two judged documents we are forcing bpref to its corner case  and thus we should read it with care. in table 1  we focus on 1 topics for which both runs found at least one relevant document.
　the average within this subset still supports the hypothesis  but if we look closer at the per-topic results we should

1
 in fact  the search engine returned only one document for the long query for topic 1  and this hit was the trec topic file.
	short-q	long-q
	#rel	rel.ret	bpref	rel.ret	bpref

11.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.1111.11
	avg	1	1	1	1
table 1: per-topic measures for each run.  #rel  is the number of relevant documents in the decayed qrels.  rel.ret  is the number of relevant retrieved for that topic.
be cautious in accepting that conclusion. the number of retrieved documents judged to be relevant is between two and three on average  and so the bpref value is based on very few pairs of relevant and irrelevant documents. while the short-query run does find more relevant documents on average  and has most of the highest bpref scores per topic  the long-query run actually beats the short query run on 1 of the topics. the short-query run wins for 1 topics  but with a higher bpref difference. although the sample is small  we observe that a one-sided wilcoxon test is not significant  p = 1   but a one-sided paired t-test is  p = 1 .
　we conclude from this experiment  somewhat cautiously  that short queries do work better than long queries for this search engine; short queries tended to return more relevant documents  but it's hard to measure the quality of the ranking from so few documents. the situation would improve if there were 1 to 1 more judged relevant documents for each run in each topic  as we show in the next section.
1. maintaining test collections
　in their recent paper on building test collections incrementally  carterette and allan propose a method for choosing which documents to judge by giving priority to documents whose relevance will expose the greatest difference among the systems. using the map measure  they compute the potential difference in map if an unjudged document were to become relevant. to avoid bias  the process continues until adding more relevant documents stops affecting the relative performance of the systems . fundamentally  this approach builds on the results of zobel   who suspected that the trec relevance judgments were incomplete  but found that discovering more relevant documents did not affect the relative performance of systems very much. carterette and allan's approach takes a set of judgments which are too coarse to compare systems with  and moves them to the point observed by zobel in an optimal way.
　we do the same thing here with two changes. first  we would rather work with bpref than map  since bpref requires many fewer relevance judgments to achieve stability. choosing bpref also means that we don't need to optimize the document selection process as strongly as we would for map  since bpref only considers relative ranks of judged documents.
　second  we identify three types of unjudged documents. because we are maintaining an existing collection rather than building a new one  we have documents which are in the original collection and were unjudged. since our starting point here is a trec collection built by pooling a large number of system outputs  we should probably opt not to examine these documents.
　we also have unjudged documents that lie outside the original collection and should be candidates for examination. when selecting out-of-collection documents to judge  it is important to avoid bias in favor of one run or another. the trec pooling process  while not efficient  places a high priority on avoiding bias towards particular systems. we can maximize impact on the measure and avoid bias by selecting documents retrieved highly by both runs which have a high coefficient of variance in the rank retrieved.
　lastly  we have previously judged documents which have changed. we have chosen to invalidate their relevance judgments  but in particular previously-relevant documents would be a good place to start recovering relevance information. this would also follow if we had chosen to invalidate relevance judgments according to document similarity measures.
　in the case of previously-judged documents  we can choose documents to judge in an unbiased way by selecting them in order of most-recent change. in figure 1  we simulate the re-judging process assuming that documents regain their old relevance value  and show the effect on bpref scores. each point on the x-axis represents recovering one document for each run with the next latest change timestamp  assigning them their original relevance value  and recomputing the bpref score. we can see that for most topics  our conclusion is unchanged: either the runs are indistinguishable in effectiveness  or their initial effectiveness ranking is preserved. for some inconclusive topics we gain enough information to distinguish them after re-judging very few  expired  documents. it is also clear that we should first focus on those topics with the fewest retrieved relevant documents.
　once these topics have been stabilized  maintenance effort should be directed at reviving the 1 topics we were forced to discard because they retrieved no judged documents. priority should be given to topics with more than 1 retrieved documents which are unjudged due to page revisions  since in the 1 topic subset we see 1 judged irrelevant documents retrieved for every judged relevant one. further  we should choose topics where a large number of those revisions

figure 1: change in bpref in the 1-topic subset as changed documents are re-judged according to their original relevance value.
affected relevant documents  to ensure that we will recover relevant documents without needing to examine too many irrelevant ones.
1. conclusion
　we have shown that static test collections can be used to measure search in a changing document collection such as the live web by tracking changes in judged documents  applying heuristics to determine the decay of relevance information  and carefully re-examining the  old  relevant documents as well as the unjudged documents retrieved in each experiment. test collections with large sets of relevance judgments remain usable for a long time; the documents we use here were nearly two years old when these experiments were run. as judged documents change  measures such as bpref which work with incomplete information can be used with little or no additional relevance assessment.
　we propose the following approach for maintaining a test collection of topics and relevance judgments atop a changing web. first  one must consider how the initial test collection was created. the collection we address in this paper was built as part of a collaborative process which typically involves ten or twenty research teams using different systems with various tuned parameter settings and which may also include manually collected search results in addition to automatic system rankings. equivalent approaches include pooling the outputs of a smaller but highly diverse range of retrieval methods   or iterative search-and-judge procedures . test collections built using these methods avoid bias towards any particular search strategy by looking broadly and deeply into the collection for relevant documents.
　alternatively  the test collection might come out of an industry search environment  for example a search engine company  or an organization attempting to tune an intranet search engine. in this case there may be only one or two search algorithms contributing documents to judge  and one should be concerned about bias. if the relevance judgments are derived from a single retrieval strategy  then a new approach will retrieve many unjudged documents. the procedure of carterette and allan  may be useful here.
　in either case  as the document collection changes over time  there are several indicators to watch:
　the rate at which judged documents change as newer web crawls are done  combined with a heuristic for deciding when a page's relevance judgment no longer applies. our heuristic is based on the rate of change; alternatively it might be based on a similarity or fingerprinting comparison. the rate of change should be observed per topic  rather than per-document.
　the number of topics with only one or two retrieved documents that have valid relevance judgments. these topics will need some maintenance in the form of additional judgments in order to be useful. alternatively  we may decide that topics which fail to retrieve any judged documents can be retired or redone from scratch.
　the number of retrieved documents whose relevance values expired due to changes in the page. if we are still retrieving these pages they are good candidates for re-judging  particularly if they used to be relevant.
　the number of retrieved documents which lie outside the collection used to create the initial relevance judgments. these documents are unjudged and would not have been judged initially  and this number will only grow over time. if the researcher has several different retrieval algorithms at hand  these can be pooled and judged using the processes described in any of the above mentioned papers.
　the total number of valid topics. if fewer than 1 topics are usable due to relevance decay  then unusable topics should be patched. one can look to guidelines for topicset size such as  1  1   but keep in mind that the experimental conditions may necessitate more topics. more topics are always better. sanderson and zobel suggest that having many topics judged less is better than having fewer topics judged more completely .
　by following these indicators through frequent  repeated experiments  a test collection may be maintained over the live web or other dynamic collection and its usable lifetime extended considerably. as topics decay  one can re-examine past documents  spend resources to judge new documents  or retire topics in favor of developing new ones. maintenance is cheaper in terms of topic development and relevance assessment time  and permits the comparison of runs from different versions of the collection.
1. future work
　the results presented here are somewhat preliminary  and there are a number of improvements and future directions we would like to explore.
　we plan to conduct a fuller examination of the bpref measure  and system rankings in general in dynamic collections. we would also like to study explicit measures for detecting bias in selecting documents for relevance assessment. the notion of incremental and maintained test collections makes such measures critically important.
　lastly  we have not fully considered the ramifications of comparing runs done at different points in time on different versions of the collection. when a difference is discovered  is it due to algorithmic advantage or differences in the statistical distribution of features in the collection 
