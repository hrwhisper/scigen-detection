traditional query optimizers rely on the accuracy of estimated statistics to choose good execution plans. this design often leads to suboptimal plan choices for complex queries  since errors in estimates for intermediate subexpressions grow exponentially in the presence of skewed and correlated data distributions. reoptimization is a promising technique to cope with such mistakes. current re-optimizers first use a traditional optimizer to pick a plan  and then react to estimation errors and resulting suboptimalities detected in the plan during execution. the effectiveness of this approach is limited because traditional optimizers choose plans unaware of issues affecting reoptimization. we address this problem using proactive reoptimization  a new approach that incorporates three techniques:     i  the uncertainty in estimates of statistics is computed in the form of bounding boxes around these estimates  ii  these bounding boxes are used to pick plans that are robust to deviations of actual values from their estimates  and iii  accurate measurements of statistics are collected quickly and efficiently during query execution. we present an extensive evaluation of these techniques using a prototype proactive re-optimizer named rio. in our experiments rio outperforms current re-optimizers by up to a factor of three. 
1. introduction 
most query optimizers use a plan-first execute-next approach-the optimizer enumerates plans  computes the cost of each plan  and picks the plan with lowest cost . this approach relies heavily on the accuracy of estimated statistics of intermediate subexpressions to choose good plans. it is a well-known problem that errors in estimation propagate exponentially in the presence of skewed and correlated data distributions  1  1 . such errors  and the consequent suboptimal plan choices  were not a critical problem when databases were smaller  queries had few joins and simple predicates  and hardware resources were limited. in the last two decades  data sizes  query complexity  and the hardware resources to manage databases have grown dramatically.  query optimizers have not kept pace with the ability of database systems to execute complex queries over very large data sets. 
several techniques have been proposed to improve traditional query optimization. these techniques include better statistics   new algorithms for optimization  1  1  1   and adaptive architectures for execution . a very promising technique in this direction is reoptimization  where the optimization and the execution stages of processing a query are interleaved  possibly multiple times  over the running time of the query  1  1  1  1 . reference  shows that re-optimization can improve the performance of complex queries by an order of magnitude. 
current re-optimizers take a reactive approach to re-optimization: they first use a traditional optimizer to generate a plan  and then track statistics and respond to estimation errors and resulting suboptimalities detected in the plan during execution. reactive reoptimization is limited by its use of an optimizer that does not incorporate issues affecting re-optimization  and suffers from at least three shortcomings: 
  the optimizer may pick plans whose performance depends heavily on uncertain statistics  making re-optimization very likely. 
  the partial work done in a pipelined plan is lost when reoptimization is triggered and the plan is changed. 
  the ability to collect statistics quickly and accurately during query execution is limited. consequently  when reoptimization is triggered  the optimizer may make new mistakes  leading potentially to thrashing. 
in this paper we propose proactive re-optimization to address these shortcomings. we have implemented a prototype proactive reoptimizer called rio that incorporates three new techniques: 
  bounding boxes are computed around estimates of statistics to represent the uncertainty in these estimates. 
  the bounding boxes are used during optimization to generate robust and switchable plans that minimize the need for reoptimization and the loss of pipelined work. 
  random-sample processing is merged with regular query execution to collect statistics quickly  accurately  and efficiently at run-time. 
our experimental results demonstrate that proactive re-optimization can provide up to three times improvement over a strictly reactive re-optimizer. the rest of this paper is organized as follows. section 1 discusses related work. section 1 uses a series of examples to illustrate the problems with reactive re-optimization  and section 1 


  supported by nsf grants iis-1 and iis-1.   supported by nsf grant iis-1. 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
 
sigmod 1 june 1  1  baltimore  maryland  usa  
copyright 1 acm 1-1/1 $1 
shows how proactive re-optimization addresses these problems. section 1 describes the rio implementation and section 1 presents an experimental evaluation. we outline future work in section 1. 
1. related work 
reference  classifies adaptive query processing systems into three families: plan-based  routing-based  and continuous-query-based. in this paper we focus on plan-based systems  the more closely related to rio being reopt  and pop . other related projects include ginga   tukwila   query scrambling   and corrective query processing . reopt and pop use a traditional optimizer to pick plans based on single-point estimates of statistics. these reactive re-optimizers augment the chosen plan with checks that are verified at run-time. the query is re-optimized if a check is violated. 
the use of intervals instead of single-point estimates for statistics has been considered by least-expected-cost optimization  lec    error-aware optimization  eao    and parametric optimization  1  1  1 . lec treats statistics estimates as random variables to compute the expected cost of each plan. unlike lec  rio does not assume knowledge about the underlying distribution of statistics. instead  rio computes the uncertainty in these estimates based on how they were derived. like rio  eao considers intervals of estimates and proposes heuristics to identify robust plans. however  the techniques in eao assume a single uncertain statistic  memory size  and a single join. furthermore  lec and eao do not consider re-optimization or the collection of statistics during query execution. therefore  these techniques use execution plans that were picked before the uncertainty in statistics is resolved. parametric optimization identifies several execution plans during optimization  each of which is optimal for some range of values of run-time parameters. parametric optimization  along with the choose-plan operator   enables the optimizer to defer the choice of plan to run-time. switchable plans and switch operators in rio are similar. however  unlike choose-plan operators  switch operators may occur within pipelines. furthermore  parametric optimization does not consider uncertainty in estimates  collection of statistics during execution  robust plans  or re-optimization. 
rio combines the processing of random samples of tuples with regular query processing to obtain quick and accurate estimates of statistics during execution. this approach differs from previous uses of random samples  e.g.  providing continuously-refined answers in an online manner   computing approximate query results  1  1   or building base relation statistics from samples . robust cardinality estimation  rce  uses random samples for cardinality estimation  to deal with uncertainty  and to explore performancepredictability tradeoffs . however  rce does not consider reoptimization. furthermore  rce does not consider techniques such as merging random-sample processing with regular query execution  or propagating random samples through joins.  
1. problems with reactive reoptimization 
in this section we present a series of examples to highlight the problems with current approaches to query re-optimization. one known problem with traditional optimizers  e.g.   is that they rely frequently on outdated statistics or invalid assumptions such as independence among attributes. consequently  they may choose suboptimal query plans that degrade performance by orders of magnitude  1  1 . example 1 illustrates this problem. 

example 1: consider the query  select * from r  s where r.a=s.a and r.b k1 and r.c k1 . assume the database buffer-cache size is 1mb  |r|=1mb  |s|=1mb  and |考 r |=1mb  where 考 r  represents the result of the  r.b k1 and r.c k1  selection on r. however  because of skew and correlations in the data distributions of r.b and r.c  the optimizer underestimates |考 r | to be 1mb. with this incorrect estimate  the optimizer would pick plan p1a for this query  figure 1 . p1a is a hash join with 考 r  as the build input and s as the probe.  throughout this paper we use the convention that the left input of a hash join is the build and the right input is the probe.  however  since |考 r | is actually 1mb  plan p1a's hash join requires two passes over r and s. p1a is suboptimal because plan p1b  which builds on s  finishes in one pass over r and s.  

re-optimization can avoid problems similar to the one in example 1. current systems that use re-optimization first use a traditional optimizer to pick the best plan  and then add check operators to the chosen plan. the check operators detect suboptimality during execution  and trigger re-optimization if required. for example  the check-placement algorithm used by pop computes a validity range for each plan . let p be a left-deep plan. the root operator of p is a binary join operator with subtree d and base relation r as inputs. let |d| denote the result size of d. pop defines the validity range of p as the range of values of |d| for which p has the lowest cost among all plans p'  where p' is logically equivalent to p  p' is rooted at an operator with the same inputs d and r  and p' gives the same interesting orders as p. 
during execution  each check operator collects statistics on its inputs. if these statistics satisfy the validity ranges for the plan picked by the optimizer  then execution proceeds as usual. otherwise  re-optimization is invoked to choose the best plan based on the statistics collected. the reuse of intermediate results that were materialized completely in a previous execution step is considered during re-optimization. example 1 illustrates the overall technique. 

example 1: consider the scenario from example 1. a re-optimizer like pop will choose the same plan  p1a  as a traditional optimizer. additionally  pop will compute validity ranges for the chosen plan. for example  a validity range for p1a is 1kb≒|考 r |≒1mb. if |考 r | 1kb  then it is preferable to use an index nested-loops join with tuples in 考 r  probing a covering index on s. if |考 r | 1mb  then plan p1b is optimal. in this example  the check |考 r |≒1mb will fail during execution  invoking re-optimization. ← 

1 limitations of single-point estimates 
although re-optimization preempts the execution of the suboptimal plan p1a in example 1 when |考 r | 1mb  it incurs the overhead of calling the optimizer more than once and the cost of repeating work. for example  the  partial  scan of r in plan p1a until reoptimization is lost and must be repeated in p1b. the optimizer may be better off picking plan p1b from the start because p1b is a robust plan with respect to the uncertainty in |考 r |; see figure 1. 

figure 1 - cost of plans p1a and p1b as |考 r | varies 
when |考 r |≒memory  both plans finish in one pass and involve the same amount of io. however  when |考 r | memory  only p1b finishes in one pass. 
current re-optimizers do not account for robustness of plans since they consider single-point estimates for all statistics needed to cost plans.  to arrive at these single-point estimates  optimizers are often forced to make assumptions like uniformity and independence .  non-robust plans may lead to extra optimizer invocations and wasted work  as we will show in section 1. 
1 limited information for re-optimization 
current re-optimizers make limited effort to collect statistics quickly and accurately during execution. for instance  the validity check in example 1 will fail when |考 r |=1mb  and re-optimization will be invoked. however  the optimizer does not know |考 r | accurately at this point-it only knows that |考 r |≡1mb-which may cause it to chose a suboptimal plan again. example 1 illustrates an extreme instance of the thrashing that can result. 

example 1:  consider the query  select * from r  s  t where r.a=s.a and s.b=t.b and r.c k1 and r.d=k1 . assume that the sizes of the tables are known accurately to be |r|=1mb  |s|=1mb  and |t|=1mb. further assume that |考 r |=1mb  but that the optimizer underestimates it significantly as 1kb.1 based on these statistics  the optimizer chooses plan p1a. 

     figure 1 - thrashing with reactive re-optimization a reactive re-optimizer may compute validity ranges for plan p1a as shown by the gray boxes in this plan. for example  the validity range for the index nested-loops join between 考 r  and s in p1a is |考 r |≒1kb. this validity-range check will fail at run-time  triggering re-optimization. plan p1b will be picked next with a validity range as shown in figure 1. this check will fail and reoptimization will be triggered again  and so on until the optimal 
plan p1d is chosen finally. 	← 

1 losing partial work in a pipeline 
in addition to the multiple re-optimization steps as illustrated in example 1  current re-optimizers also lose the partial work done by a pipeline in execution when re-optimization is triggered. for example  plan p1c in figure 1 has a pipeline ppl1  enclosed with dotted lines  that scans r  probes s in hashjoin1  and builds joining tuples into hashjoin1. the validity-range check before hashjoin1 will fail before pipeline ppl1 finishes  and the partial work done by this pipeline will be lost. on the other hand  work done by completed pipelines  like ppl1-scanning and building s-can be reused. however  in this example  the build of s in plan p1c cannot be reused in plan p1d because the hash tables are built on different join attributes. 
1. proactive re-optimization 
this paper proposes proactive re-optimization  a new paradigm for query re-optimization. proactive re-optimization addresses the problems with current reactive approaches that were illustrated in section 1. a proactive re-optimizer incorporates three new techniques: 
1. computing bounding boxes-intervals around estimates-as a representation of the uncertainty in estimates of statistics. 
1. using bounding boxes during optimization to generate robust plans and switchable plans that avoid re-optimization and loss of pipelined work. 
1. using randomization to collect statistics quickly  accurately  and efficiently as part of query execution. 
figure 1 shows the architecture of a proactive re-optimizer. in section 1 we introduce rio  our specific implementation of a proactive re-optimizer. 

figure 1 - proactive re-optimization 
1 representing uncertainty in statistics 
current re-optimizers compute a single-point estimate for any statistic needed to cost plans. one way to account for possible errors in estimates is to consider intervals  or bounding boxes  around the estimates. if the optimizer is very certain of the quality of an estimate  then its bounding box should be narrow. if the optimizer is uncertain of the estimate's quality  then the bounding box should be wider. there are different ways of computing bounding boxes  e.g.  using strict upper and lower bounds  or by characterizing uncertainty in estimates using discrete buckets that depend on the way the estimate was derived . our implementation uses the latter approach as described in section 1. 

example 1: consider the scenario from example 1. the costs of plans p1a and p1b depend mainly on |考 r | and |s|. suppose a recent estimate of |s|=1mb is available in the catalog. however  in the absence of a multidimensional histogram on r  |考 r | must be estimated from the estimated selectivities of 
r.b k1 and r.c k1 and an assumption of independence between these predicates. this estimate of |考 r |=1mb is thus very uncertain. in this case  figure 1 shows an example bounding box around the single-point estimate  |考 r |=1mb  |s|=1mb . |s|  in mb  
1 using bounding boxes during optimization 
since current re-optimizers consider single-point estimates only  their plan choices may lead to extra re-optimization steps and to the loss of partial pipelined work if actual statistics differ from their estimates. bounding boxes can be used during optimization to address this problem. while there is always one plan that is optimal for a single-point estimate  one of the following four cases can occur with a bounding box b: 
 c.i  single optimal plan. a single plan is optimal at all points within b. 
 c.ii  single robust plan. there is a single plan whose cost is very close to optimal at all points within b. 
 c.iii  a switchable plan. intuitively  a switchable plan in b is a set s of plans with the following properties: a  at each point pt in b  there is a plan p in s whose cost at pt is close to that of the optimal plan at pt; b  the decision of which plan in s to use can be deferred until accurate estimates of uncertain statistics are available at query execution time; and c  if the actual statistics lie within b  an appropriate plan from s can be picked and run without losing any significant fraction of the execution work done so far. 
 c.iv  none of the above. different plans are optimal at different points in b  but no switchable plan is available. 
a proactive re-optimizer identifies which of the above four cases b falls into. note that a single optimal plan is also robust  and a robust plan is a singleton switchable plan. 
example 1 illustrates how a proactive re-optimizer can exploit robust plans and switchable plans. details of how to enumerate and choose robust and switchable plans are given in section 1. 

example 1: consider the scenario from example 1. figure 1 is the same as figure 1 except that it considers the bounding box b= 1mb  1mb  for |考 r |. 

potential min 	estimate	potential max	 figure 1 - robust and switchable plans 
as seen  plan p1a is optimal for the estimated |考 r |=1mb  but not in the entire bounding box. while plan p1b is not optimal for the estimated |考 r |  p1b is robust because its cost is very close to optimal at all points in b. therefore  picking plan p1b would be a safe option. however  as we will see in section 1  p1a and p1b  which are hybrid hash joins with build and probe reversed  are switchable. it is preferable to pick the switchable plan p={p1a  p1b} instead of the robust p1b because p is guaranteed to run the optimal plan as long as |考 r | lies within b.  ← 

1 accurate run-time statistics collection 
as seen in example 1  the lack of accurate run-time statistics collection can lead to thrashing during re-optimization. in general  accurate run-time estimates are needed to pick the right plan from a switchable set  to detect when to trigger re-optimization  and to pick a better plan in the next optimization step. 
for efficiency  we hide the cost of collecting accurate statistics by combining statistics collection with regular query execution. furthermore  for early detection of the need to re-optimize  the run-time estimates must be computed both quickly and accurately. we achieve these goals by using a new technique of merging the processing of random samples of tuples along with regular query execution. example 1 illustrates this approach. implementation details are given in section 1. 

example 1: consider example 1. assume that the optimizer had picked the suboptimal plan p1a which contains a pair of index nested-loops joins with 考 r  as the outer input. suppose tuples in r are physically laid out in random order on disk. then  once 1% of the r tuples have been scanned and processed  a fairly accurate estimate of the selectivity of 考 is available. thus  |考 r | can be estimated reliably. this estimate enables a proactive re-optimizer to detect quickly that p1d is the optimal plan  thereby avoiding the thrashing problem in reactive re-optimizers. ← 

1. proactive  re-optimization  with  rio 
section 1 presented an overview of proactive re-optimization without providing specifics about the implementation. we now describe our prototype proactive re-optimizer rio. 
1 building rio 
rio was built using the predator dbms  by extending it as follows: 
  equi-height and end-biased histograms were added . 
  predator has a traditional cost-based dynamic-programming optimizer  which we refer to as trad. we added: 
  a validity-ranges optimizer  vro   our implementation of the algorithms used by pop . 
  rio  our proactive re-optimizer. 
  uncertainty buckets and rules from  to generate and propagate uncertainty buckets during query optimization. 
  the following operators were added: 
  a hybrid hash join operator  that processes tuples from two input subtrees. at most one of the subtrees is a deep subtree and at least one is a subtree with one base relation. either subtree can be the build input of the hash join. thus  this operator enables us to consider arbitrary linear plan shapes  e.g.  right-deep join trees like plan p1c in figure 1. recall our convention that the left input to the hash join is the build and the right input is the probe. 
  a switch operator to implement switchable plans. 
  operators to read random samples from base relations and to generate random samples of joins as part of query execution. 
  buffer operators to buffer tuples and delay processing in a pipeline until the statistics necessary to choose among the set of plans in a switch operator have been collected. 
  operators to scan previously materialized expressions for reuse after re-optimization. materialized expressions that may be reused include completed builds of hash joins and the sorted temporary files created by a sort operator. 
  the original validity-ranges algorithm  uses checks on buffers to trigger re-optimization when the buffers overflow or underflow. in our vro implementation  validity ranges are checked by buffer operators placed appropriately in the plan which buffer and count incoming tuples. the buffer operators trigger re-optimization if any validity range is violated. 
  execution engine: 
  the ability to stop query execution midway  re-optimize  and restart execution. 
  an in-memory catalog to track statistics collected at runtime as well as expressions materialized as part of query execution. the optimizer consults this catalog during reoptimization. 
  an inter-operator communication mechanism based on punctuations  that  e.g.  allows an operator c to signal to its parent operator that c has generated a 1% random sample of its output. 
1 computing bounding boxes 
recall that a proactive re-optimizer uses bounding boxes instead of single-point estimates for statistics needed to cost plans. currently  rio restricts the computation of bounding boxes to size and selectivity estimates. for each such estimate e  a bounding box b is computed using a two-step process:   an uncertainty bucket u is assigned to the estimate e 
  	the bounding box is computed from the  e  u  pair 
to compute u  we adopted a technique from  that uses a set of rules to compute uncertainty.  we plan to try other techniques in the future  e.g.  stochastic intervals as in .  in the original approach   the value of u belongs to a three-valued domain {small  medium  large} that characterizes the uncertainty in the estimate e. the value of u is computed based on the way e is derived. for example  if an accurate value of e is available in the catalog  then u takes the value small that denotes low uncertainty. in rio  we augmented the domain of u to an integer domain with values from 1  no uncertainty  to 1  very high uncertainty . 
a bounding box b of an estimated value e is an interval  lo  hi  that contains e. the uncertainty value u is used to compute the values lo and hi as shown in figure 1. example 1 illustrates the computation of uncertainty buckets and bounding boxes for our running example. 

example 1: consider the scenario from example 1. the optimizer needs to cost plans p1a and p1b which depend on |考 r | and |s|. recall that 考 represents r.b k1 and r.c k1. the single-point estimates for |s| and |考 r | are es=1mb and er=1mb respectively. assume that es was obtained from the catalog. therefore  our rules adapted from  for derivation of uncertainty set us=1  low uncertainty in es . from figure 1  the bounding box for es is bs= 1  1 . on the other hand  assume that the estimate er was computed from the estimated selectivities of r.b k1 and r.c k1 based on the assumption that these predicates are independent  no multidimensional histogram was available . thus  the uncertainty in er is high. accordingly  our rules for derivation of uncertainty set ur=1. from figure 1  the bounding box for er is br= 1  1 . ← 

computeboundingbox inputs: estimate e  uncertainty u
outputs: lo  hi  {
 + = 1; // increment step
 - = 1; // decrement step hi = e *  1 +  + * u ; lo = e *  1 -  - * u ;
}

figure 1 - computing bounding boxes for an  e  u  pair 
1 optimizing with bounding boxes 
the trad optimizer enumerates and groups plans based on their join subset  js  and interesting orders  io  . for each distinct  js  io  pair enumerated  trad prunes away all plans except the plan with the lowest cost  denoted bestplan. the cost of each plan is computed based on estimated statistics. 
vro takes the same steps as trad initially  so vro will find the same optimal plan  bestplan  for each  js  io  pair. however  vro then adds validity ranges on the inputs to the join operators in bestplan . consider a join operator o with inputs rd and rb  where rd is the deep subtree input and rb is the base relation input. the validity range of o is the range of values of |rd| where operator o has the lowest cost among all join operators with the same inputs rd and rb  and giving the same set of interesting orders as o. the validity range of o is computed by varying |rd| up  and down  until the cost of o is higher than that of some other join operator with the same inputs rd and rb and giving the same set of interesting orders as o. the newton-raphson method can be applied to the join cost-functions to compute validity ranges more efficiently than linear search; see . 
unlike trad and vro  rio computes bounding boxes for all input sizes used to cost plans. then it tries to compute a switchable plan  which may also be a single robust plan or a single optimal plan  for each distinct  js  io  pair based on the bounding boxes on the inputs to the plan. if rio fails to find a switchable plan for a  js  io  pair  then it picks the optimal plan for  js  io  based on the single-point estimates of input sizes  bestplan   and adds validity ranges like vro. 
rio computes switchable plans in two steps. first  it finds three seed plans for each  js  io  pair. then  it creates the switchable plan from the seed plans as described next. 1.1 generating the seed plans 
in traditional enumeration  plan cost is computed using singlepoint estimates of statistics. in rio  the enumeration considers three different costs for each plan  clow  cest  and chigh. cost cest is computed using the single-point estimate of statistics exactly like in traditional enumeration. cost clow  chigh  is computed at the lower left corner  upper right corner  of a bounding box as illustrated in figure 1. 
rio augments the  js  io  pair used during traditional enumeration with an extra cost bucket cb that takes values low  estimated  or high. like the interesting order concept  the cost bucket defines which plans and costs are comparable during costbased pruning  e.g.  a plan p for  js  io  cb=low  is pruned if and only if there exists a plan p' for  js  io  cb=low  with a lower cost clow than p. for each distinct  js  io  pair  rio enumerates and prunes plans for the three triples  js  io  cb=low    js  io  cb=estimated   and  js  io  cb=high . the plans that remain after pruning are the three plans corresponding to the minimum clow  cest  and chigh for  js  io . 
	plan p 	bounding box for input 
sizes for j
	lod 	estd 	hid 
base relation 
	rb  	clow =  cost of p at  lod  lob  
cest   =  cost of p at  estd  estb  
                 chigh =  cost of p at  hid  hib    figure 1 - computing plan costs 
note that the best plan for  js  io  cb=estimated  is the same plan  bestplan  as computed by trad for  js  io . also  the addition of the extra cost bucket guarantees that the optimal plan for the estimated statistics will not prune away plans that are optimal at the upper right or lower left corners of the bounding boxes for input sizes. for each  js  io  pair  we end up with three seed plans from which a switchable plan will be created: 
  bestplanlow  the plan with minimum cost clow 
  bestplanest  the plan with minimum cost cest 
  bestplanhigh  the plan with minimum cost chigh 
1.1 generating the switchable plan 
given the seeds bestplanlow  bestplanest  and bestplanhigh  one of four cases arises: 
 c.i  	the seeds are all the same plan. 
 c.ii  the seeds are not all the same plan  but one of them is a robust plan. 
 c.iii  the seeds are not all the same plan  and none of them is robust  but a switchable plan can be created from the seeds. 
 c.iv  we cannot find a single optimal plan  a single robust plan  or a switchable plan from the seeds. 
in case  c.i   the single optimal plan is the switchable plan.  recall that an optimal plan is also robust and a robust plan is a singleton switchable plan.  in case  c.ii   the optimizer checks if any of the seeds is a robust plan. a necessary test to determine whether bestplanlow is robust is to check whether  i  cost cest of bestplanlow is close to  e.g.  within 1% of  cest of bestplanest  and  ii  cost chigh of bestplanlow is close to chigh of bestplanhigh. intuitively  we are testing whether bestplanlow has performance close to optimal at the estimated point and at the upper corner of the bounding box as well. while this test is not sufficient to guarantee robustness-because we do not check all points in the bounding box-rio currently labels a plan as robust if it passes this plan-robustness test. if one of the seeds passes this test  then rio uses that seed as a singleton switchable plan. 

example 1: consider the scenario from example 1. as seen in figure 1  bestplanlow = bestplanest = p1a and bestplanhigh = p1b. the cost of p1a is not within 1% of the cost of p1b at the upper corner of the bounding box  |考 r |=1mb . thus  p1a is not a robust plan within the bounding box. on the other hand  p1b is within 1% of the cost of p1a both at the estimated point  |考 r |=1mb  and at the lower corner of the bounding box  |考 r |=1mb . therefore  p1b passes the plan-robustness test. 
	cost of plans 	p1a
figure 1 - finding a robust plan in |考 r |'s bounding box  ←

if none of the seeds is a single optimal plan or a single robust plan  case  c.iii    then the optimizer tries to find a switchable plan. a switchable plan for a  js  io  pair is a set of plans s where: 
 i  all plans in s have a different join operator as the root operator.  hybrid hash joins with the build and probe reversed are treated as different operators.  
 ii  all plans in s have the same subplan for the deep subtree input to the root operator. 
 iii  all plans in s have the same base table  but not necessarily the same access path  as the other input to the root operator.  
figure 1 contains an example of a switchable plan with three member plans for  js={r s t}  io=  . any two members of a switchable plan are said to be switchable with each other. in section 1 we illustrate how the switchable plan chooses one of its members at execution time. 

figure 1 - possible members of a switchable plan 
if the seed plans for a  js  io  pair have the same subplan for the deep subtree  then the seeds themselves constitute a switchable plan. if these subplans are different  then rio picks one of the seed plans  say bestplanlow  and enumerates the set sw low of all plans that are switchable with bestplanlow based on conditions  i - iii  of switchable plans above. then  among the plans in sw low  rio finds the plan  planminest  with minimum cost at the estimated statistics point  and the plan  planminhigh  with minimum cost at the upper right corner of the bounding box. if cest of planminest is close to  e.g.  within 1%  cest of bestplanest  and chigh of planminhigh is close to chigh of bestplanhigh  then {bestplanlow  planminest  planminhigh} is a switchable plan. if not  rio tries the same procedure with the two other seed plans. 
example 1: suppose bestplanlow = plan p1a  bestplanest = plan p1b   figure 1   hashjoin1 and bestplanhigh = plan p1  figure 1  for r  s  t with no interesting orders. the hashjoin1 scan r subplan for the deep subtree of the outer join is different between p1a and p1  so they 
are not switchable. thus  rio enumerates scan s scan t sw low  which contains plan p1c. if chigh plan p1 of plan p1c is close to that of p1  then figure 1 {p1a  p1b  p1c} is a switchable plan. ← 
if these techniques fail to find a switchable plan  case  c.iv    then rio picks bestplanest-the optimal plan for the single-point estimates-and adds validity ranges  just like vro. 
1 extensions to the query execution engine 
a switchable plan s defers the choice of which member plan to use for a join until the uncertain input sizes can be estimated accurately at run-time. s ensures that no  partial  work done by the pipeline containing the join is lost whenever the actual input sizes lie within the corresponding bounding box. our implementation of switchable plans uses the following operators and communication framework: 
  a switch operator that corresponds to the chosen switchable plan. this operator decides which member plan to use based on run-time estimates of input sizes  and instantiates the appropriate join operator and base relation access path. 
  a buffer operator that buffers tuples until it can compute an input-size estimate needed by the switch operator. 
  randomization-aware operators that prefix their output with a random sample of their complete output. 
  an inter-operator communication mechanism based on punctuations  that allows operators to send size estimates and to demarcate random samples in their output stream. 
1.1 implementing switchable plans 
for a switchable plan chosen by rio during optimization  the execution-plan generator creates a switch operator and a buffer operator. figure 1 shows these two operators generated for the switchable plan in figure 1. note that the buffer operator is placed above the common subplan for r  s  marked in gray in both figures . the switch operator is placed above the buffer operator. 
during query execution  the buffer operator buffers tuples from the deep subplan until it gets an end-of-sample punctuation eos f .  generation of such punctuations is described in section 1.1.  punctuation eos f  signals that the set of tuples buffered so far is an f % random sample of the output of the deep subplan. based on the number of buffered tuples n  1n/f is a fairly accurate estimate of the final output cardinality of r  s. the switch operator uses this cardinality estimate to compute the total input 

figure 1-implementation of switchable plan from figure 1 
rio currently uses only the size of the deep subtree input rd to the join to choose the best member plan. in terms of figure 1  this limitation means that for a switchable plan p={plo  pest  phi}  where plo  pest  and phi were chosen for  lod  lob    estd  estb   and  hid  hib  respectively  recall section 1.1   rio has to choose among plo  pest  and phi based solely on the estimate of |rd|. plo is picked if | r | ﹋ lo   lo +est   1   pest is picked if 
| rd | ﹋  and phi is picked if 
| rd | ﹋  estd+ hid 1 hid . if |rd|   lod or |rd|   hid  then the switch operator triggers re-optimization after adding the collected estimate of |rd| to the catalog. 
1.1 random-sample processing during execution 
to generate eos f  punctuations required by buffer operators  we altered the regular processing of some of predator's operators so that  with minimal overhead  they can prefix their output with a random sample of their entire output. each such operator o first outputs an f % random sample of its entire output.  f is a userdefined parameter.  next  o generates an end-of-sample punctuation eos f  to signal the end of the sample. finally  o sends its remaining output tuples. as shown in figure 1  tuples output as part of the random sample are not generated again. 
normal output order without randomization a b c d e f g h i j 
output order with randomization 
	h 	a 	e 	b 	c 	d 	f 	g 	i 	j 
a random sample 	followed by the of the output 	rest of the output emits punctuation eos 1%  
 
figure 1 - random samples in the operator output 
reordering the output of an operator o is not an option if any of the operators above o in the plan depend on the order of o's output.  thus  random sample generation seems inapplicable to operators such as sorts and ordered scans from b-trees. however  there are ways around this problem. for example  the buffer operator above o can regenerate the order using a merge of the initial sample with the later output. furthermore  blocking operators1 like sorts provide simpler ways of estimating input sizes without requiring random samples or buffering. we plan to address these issues in detail in future work. 
next we describe how eos f  punctuations are generated by table scans and certain join operators. note that our techniques never transform a non-blocking operator into a blocking operator. 
1.1 randomization in table-scan operators 
we developed two techniques to enable a scan operator over a table t to first return a random sample of tuples from t: 
 i  if tuples in t are laid out in random order on disk  a sequential scan will produce the tuples in the desired order. whether t has a random layout pattern or not can be a physical property of the table  enforced when the table is created and updated. additionally  such a layout pattern can be detected using the kiefer-kolmogorov-smirnov test when runstats is invoked to collect statistics on t; see . this additional statistic can be maintained in the catalog. 
 ii  an f % random sample of t  denoted t sample  can be maintained explicitly as a separate table  e.g.  using the techniques from . each tuple in t contains an extra bit to denote whether the tuple is also present in t sample or not. at run-time the table scan first returns tuples from t sample  followed by an eos f . then it scans t  returning all tuples not contained in t sample. note that having tuples duplicated in t sample and t allows indexes over t to be built and used without any changes. the storage overhead is minimal. 1.1 randomization in join operators 
adding randomization to the nested-loops join operators-tuple  block  and index-was straightforward. these operators simply pass on the eos f  punctuations from their outer input  and ignore eos f  from their inner input. a join sample produced in this fashion is a true random sample of the join if the outer table's join column is a foreign key referencing the inner table . 
to producing a random sample first from a hybrid hash join  we made the following modifications to the standard algorithm: 
 i  first  tuples from the probe input are read into memory until an eos f  punctuation is received. these tuples represent an f % sample of the complete probe input. the join operator inserts these tuples into an in-memory hash table.  
 ii  next  the build input is read and partitioned completely. in addition  as these tuples are being processed  they are immediately joined with the in-memory sample of the probe input. joining tuples are sent in the join output. at the end of this phase  an eos f  punctuation  using the value of f received from the probe  is generated  and the in-memory sample is discarded. the tuples output so far correspond to taking an f % sample from the probe and joining it with the complete build. this sample is guaranteed to be a true join random sample if the probe input's join column is a foreign key referencing the build input .  
 iii  the scan of the probe input  which was paused after the eos f  in step  i   is resumed. the tuples are partitioned and joined with the memory-resident build partitions.  
 iv  the on-disk partitions are joined to complete the join. 
1. experiments 
in this section we describe an extensive experimental evaluation of the rio prototype. we compare rio with the traditional optimizer  termed trad in section 1  and with the validityranges re-optimizer  termed vro in section 1  under a variety of conditions. in our experiments we used a synthetic data generator provided by ibm. the generated dataset has four tables whose properties are shown in table 1. 
table 1 - summary of dataset used in the experiments 
table size  # of tuples sample correlated attrs accidents  a  1 mb  1 m accident with & damage  seat belt on & driver status cars  c  1 mb  1 mmake & model & color owner  o  1 mb  1 mcity & state & country demographics  d  1 mb  1 mage & salary & assets all experiments were done on a 1 ghz pentium machine with 1 mb l1 cache  1 mb memory  and a single 1 rpm disk. the buffer cache size is 1 mb. each hybrid hash join operator is allocated a fixed amount of memory which we vary in some of the experiments; the default value is 1 mb. buffer operators in rio and vro are allocated the same amount of memory as a hybrid hash join. the buffers spill to disk when they fill up. b-tree indexes were available on all primary-key attributes. equi-height and end-biased histograms were available on all integer attributes. the bounding box computation in rio happens as described in figure 1 with  +=1 and  -=1. the cost threshold for robustness tests is 1%  section 1.1 . the random-sample percentage for size estimation is 1%  section 1.1 . 
1 two-way join queries 
our first experiment studies the performance of trad  vro  and rio with respect to the error in estimates. we use a query joining accidents  a  with cars  c  on the car id attribute.  all joins we consider are foreign key to primary key joins.  there is a selection predicate on a  denoted 考 a   of the form a.accident year    year   where  year  is a parameter whose value is varied in this experiment. we removed the equi-height histogram on attribute a.accident year from the catalog to force the optimizer to use the default selectivity estimate of 1. thus  the optimizer always estimates |考 a |=1mb. by varying the value of  year   we vary the error between the estimate of |考 a | and its actual size. 
1.1 using robust plans 
the memory limit for a hybrid hash join was set to 1mb in this experiment. when |考 a | is less than the size of c  1mb   the optimal plan is a hybrid hash join with 考 a  as the build  denoted plan pac. when |考 a |   1mb  the optimal plan is a hybrid hash join with c as the build  denoted plan pca.  1mb corresponds to around 1 in figure 1.  although b-tree indexes are available on the join attributes  index-nested-loop joins never outperform hybrid hash joins in our setting.  
figure 1 shows query completion times  including both optimization and execution times   for trad  vro  and rio as we vary the error in the estimate of |考 a |. the error plotted on the x-axis is computed as |考 a |actual / |考 a |estimate - 1. a positive error indicates an underestimate and a negative indicates an overestimate. figure 1 also shows the performance of the optimal plan which we determined manually in each case. 
since the optimizer's estimate of |考 a | is 1mb  trad always picks plan pac which is optimal at |考 a |=1mb. as |考 a | is increased  and the estimation error increases   the cost of plan pac increases linearly at a small rate until |考 a |=1mb.  |考 a |=1mb corresponds to an error around 1 in figure 1.  when |考 a | 1mb  the hybrid hash join in plan pac starts spilling to disk. because of this extra io  the cost of plan pac increases at a steep rate when |考 a | 1mb  as shown by the plot for trad in figure 1. 
vro always starts with the same plan as trad  i.e.  plan pac. however  vro adds a validity range to the join and verifies this range before starting the join execution. the upper bound of the validity range for the hybrid hash join in plan pac is 1mb: if |考 a | 1mb  then plan pca performs better. therefore  as long as |考 a |≒1mb  the validity range is not violated and the performance of vro matches the performance of the optimal plot in figure 1. when |考 a | 1mb  the validity range is violated and vro is forced to re-optimize. plan pca is picked on reoptimization. vro cannot reuse the work done by the pipeline in execution in plan pac when re-optimization was invoked  namely the scan of a and evaluation of 考 a  up to that point. this loss of work results in the region in figure 1 where vro performs worse than trad. however  as the error increases  the reoptimization pays off quickly because when |考 a | 1mb  the join in plan pac spills to disk while pca scans a and c only once. 

rio first computes bounding boxes for |考 a | and |c|. since there are no selection predicates on c  the estimate of |c| available from the catalog is accurate. to illustrate robust plans  in this experiment alone we set  + and  - in figure 1 to very high values so that the bounding box on |考 a | is  1mb  1mb . rio identifies that plan pca is a robust plan within this bounding box.  rio identifies plan pca to be a robust plan even if the bounding box is smaller.  because the bounding box  1mb  1mb  covers the entire range considered in the experiment  rio runs plan pca at all points in figure 1. although plan pca is not optimal at all points in the bounding box  note that rio's performance is close to the optimal plot at all points in figure 1  showing the robustness of plan pca. since |c| is less than the memory available to the hash join  pca always finishes in one scan of a and c. 
for our default settings of  + and  -  the bounding box on |考 a | is  1mb  1mb . in this case rio used a combination of solutions  re-optimization  switchable plans  and robust plans  to provide near-optimal performance. this graph is omitted because section 1.1 shows rio's performance in a similar situation. 
1.1 using switchable plans 
our next experiment  reported in figure 1  considers the same query as in the previous section  but now hash joins are allocated only 1mb of memory for in-memory hash partitions. in this experiment  the behavior of optimal  trad  and vro regarding the choices of plans and re-optimization points are the same as in the previous section. however  rio behaves differently. rio computes the bounding box on |考 a | to be  1mb  1mb . the large width of the box corresponds to the high uncertainty in |考 a | since this estimate used a default value of selectivity. the bounding box on |c| has zero width since an accurate estimate of |c| is available from the catalog. rio finds that plan pac is optimal at  |考 a | |c| = 1mb  1mb   which is the lower corner of the bounding box  and also at the estimated point  |考 a | |c|  =  1mb  1mb . however  for  |考 a | |c|  =  1mb  1mb   which is the upper corner of the bounding box  plan pca is optimal. furthermore  neither pac nor pca is robust in this case. however  rio identifies that plans pac and pca are switchable plans  see section 1 . therefore  for this query  rio starts with a plan containing a switch operator with the two hybrid hash joins corresponding to pac and pca as member plans. rio estimates |考 a | during execution. based on this estimate  rio chooses one of the two joins or it re-optimizes. 
the accident year attribute in a is not correlated with the layout of a on disk  so a sequential scan of a produces tuples in random order to estimate the selectivity of 考 a   recall section 1.1 . rio gets a very accurate estimate of |考 a | from the default setting of 1% sampling. for example  when |考 a |=1mb in figure 1  which corresponds to an error of -1 and lies outside the bounding box  rio invokes re-optimization. since the optimizer now has accurate estimates of |考 a | and |c|  it correctly picks plan pac which is optimal at this point. note that rio's performance is very close to that of the optimal plan for |考 a | = 1mb  which shows that the overhead incurred by rio to sample 1% of a  obtain a run-time estimate of |考 a |  and to re-optimize the query is very small. 

when |考 a | lies within the bounding box computed by rio  reoptimization is avoided. in this case  the switch operator picks plan pac or plan pca appropriately  avoiding loss of work. for example  the switch operator picks plan pac when |考 a |=1mb  which corresponds to an error of -1 in figure 1. plan pca is picked when |考 a |=1mb  which corresponds to an error of 1 in figure 1. when |考 a | 1  which lies outside the 
table 1 - plans used by different optimizers at sample points a  b  c  and d in figure 1 
point |考1 a | trad vro rio optimal a 1 mb p1a inside validity range  runs plan p1a outside bounding box  re-optimize  picks plan p1a p1a b 1 mb p1a inside validity range  runs plan p1a inside bounding box  switch operator picks p1a p1a c 1 mb p1a outside validity range  re-optimize  picks p1d inside bounding box  switch operator picks p1d p1b d 1 mb p1a outside validity range: re-optimize  picks p1d outside bounding box  re-optimize  picks plan p1b p1b bounding box  rio will re-optimize with a fairly accurate value of |考 a | estimated via sampling. in this case  the optimal plan pca gets picked. therefore  rio's performance is always close to that of the optimal plan for this query. 
1 three-way join queries 
we now repeat the experiments in section 1 with a query joining a  c  and o. there are selection predicates on a.accident year  考1  and o.cars  考1 . we removed the equiheight histogram on a.accident year so that the optimizer uses a default estimate  and we vary the estimation error as in section 1. the results are shown in figure 1. the cardinality of 考1 o  is estimated accurately from an equi-height histogram. 
the optimal plan for this query for low values of |考1 a | is plan p1a shown in figure 1. for higher values of a  plan p1b in figure 1 becomes optimal. plan p1a is also the optimal plan for the single-point estimates of input sizes  hence trad always picks plan p1a. therefore  in the left part of figure 1  trad performs as well as the optimal plan  but its performance deviates more and more from the optimal as the error increases.  
rio starts with the plan p1c shown in figure 1. this plan has two switch operators corresponding to the two joins.  buffer operators are not shown in figure 1.  the two member plans in the first switch operator are  i  hybrid hash join with 考1 a  as build and c as probe  and  ii  hybrid hash join with c as build and 考1 a  as probe. the switch operator will choose between these plans based on a run-time estimate of |考1 a | computed from a 1% sample of a. the two member plans in the second switch operator are  i  hybrid hash join with 考1 a   c as build and 考1 o  as probe  and  ii  hybrid hash join with 考1 o  as build and 考1 a   c as probe. the choice between these two plans will be made based on an estimate of |考1 a   c| from a 1% sample of 考1 a  
 c obtained by sampling the join  recall section 1.1 . the bounding box on |考1 a | is the same as that in section 1. the bounding boxes on |c| and |考1 o | effectively have zero width since these estimates are known to be accurate. when |考1 a |=1mb  point a in figure 1 and in table 1   which corresponds to an error of -1 and lies outside the bounding box  rio invokes re-optimization and picks the optimal plan p1a. similarly  when |考1 a |=1mb  point c in figure 1 and in table 1   which corresponds to an error of 1 and is within the bounding box  both switch operators will pick the base relation input as the build  and execute plan p1d in figure 1. thereby  when |考1 a |=1mb  rio avoids re-optimization and the loss of pipelined work which results in the difference of around 1 seconds between rio and vro in this case. 
the performance of rio is always close to that of the optimal plan in figure 1 except for an intermediate range of estimation errors. in this region  rio picks plan p1d which turns out to be suboptimal compared to plan p1b. this region is a transition region where plan p1d stops being optimal with respect to plan p1b. because of an overestimate of the join selectivity of c  考1 o   rio continues to pick plan p1d as the optimal plan beyond the actual transition point. however  as the error in |考1 a | increases  rio converges to the optimal plan again around an error of 1 in figure 1. 
考1 a  c 
plan p1a c 考1 o  plan p1b 考1 a  c 
plan p1c hhj hhj hhj hhj 
考1 o  
	1	考1 a  	o 
c 考1 a  
plan p1d 考1 o  c 
plan p1e 考1 c  考1 a  
	plan p1f 	 	figure 1 - plans for a 	 c 	 o used in experiments 
vro starts with the same plan p1a as trad  but with validity ranges added. when |考1 a |≒1mb  none of the validity ranges are violated.  |考1 a |=1mb corresponds to around 1 in figure 1.  when |考1 a | 1mb  the validity range on 考1 a   c is violated and vro is forced to re-optimize. note that at this point  vro does not have an estimate of the actual size of |考1 a |. based on the amount of a it has seen so far  vro always picks plan p1d on re-optimization and adds validity ranges. in addition to the overhead of re-optimization and the loss of pipelined work  the choice of plan p1d illustrates one of the big problems with vro. vro gets stuck in a suboptimal plan as the validity ranges in plan p1d will never fail because of an underestimate of |考1 a |: there is no better plan to join c and 考1 a  for large |考1 a | than the hybrid hash join with 考1 a  as the probe  even though there is a better plan for the entire query. a similar situation arises for the second join since 考1 a  is part of the probe input here as well. hence  as illustrated by the results in figure 1  vro performs badly as the estimation error in |考1 a | increases. this experiment illustrates one of the pitfalls of reactive re-optimization where the execution plan is decided before the issues affecting re-optimization are considered. 
1 correlation-based mistakes 
so far the estimation errors we considered were due to selection predicates on an attribute on which there was no histogram. a more common case of estimation errors is the presence of correlated attributes  which we consider in this section. we use a three-way join query on a  c  and o with selection predicates 考1 a  and 考1 o . figure 1 shows the performance of three queries q1  q1  and q1 which have different sets of correlated predicates on a  causing the optimizer to underestimate |考1 a | in each case.  correlations usually lead to underestimates .  for example  query q1 contains predicates a.accident with =  car   a.driver status =  injured   and a.seat belt on =  on . |c| and |考1 o | are always estimated accurately. figure 1 indicates that the estimation errors caused by correlated attributes result in performance trends for trad  vro  and rio similar to those shown in sections 1 and 1. the reasons for these trends are also similar to those observed in sections 1 and 1. the optimal plan for each query is plan p1e in figure 1 which rio picks either because it is a robust plan  q1  or because rio discovers the estimation error and the actual estimate quickly because of randomization  q1 and q1 .  

figure 1 - errors due to correlated predicates 
1 thrashing 
so far we considered queries where the size of a single input is estimated incorrectly. in this section we consider the performance of vro and rio when the size of more than one input is estimated incorrectly. we use a three-way join query on a  c  and o with selection predicates 考1 a  and 考1 c . |考1 a | is underestimated significantly because 考 is on an attribute with no histograms  while |考1 c | is underestimated slightly because the histogram on the corresponding attribute was built from a small sample of c. for this query  vro thrashes and takes 1 seconds compared to 1 seconds for rio. vro starts with the optimal plan for the estimated statistics which is similar to plan p1a in figure 1. because |考1 c | is underestimated  vro computes an incorrect validity range for |考1 a |. this validity range is violated at run-time  and re-optimization picks plan p1f. since vro does not have correct estimates of |考1 a | or |考1 c | at this point  it computes incorrect validity ranges which fail again. this thrashing results in the factor two slowdown of vro compared to rio. rio invokes re-optimization once for this query when its run-time estimate of |考1 a | falls outside the bounding box. because rio estimates |考1 a | accurately at run-time using sampling  and also uses bounding boxes to allow for error in the estimate of |考1 c |  it finds the optimal plan in the first reoptimization step. 
1 increasing query complexity 
in this section we compare the relative performance of trad  vro  and rio as we increase the number of joins in the query. the results are shown in figure 1. 

	number of tables joined	 
figure 1 - increasing query complexity 
the dataset provided to us had four tables only  the actual dataset has around 1 tables  . for this experiment  we vertically partitioned each table into two and padded each partition with string fields to make it the same size as the original table. each query had correlated predicates on half of the joined tables. figure 1 shows the same trends observed in previous sections. the fraction of time spent by rio and vro in optimization steps was less than 1% in all cases in figure 1. roughly  the cost of each optimization phase in rio is three times the cost of the single optimization phase in trad.  
figure 1 also shows the relative performance of vro-r  which is the validity-ranges optimizer enhanced with our random-sample processing techniques from section 1.1. while randomization improved the overall performance of vro by reducing the time required to trigger re-optimization  the amount of wasted work  and the number of re-optimization steps  rio still outperforms vro-r by a significant amount. 
1. future work 
this paper proposes proactive re-optimization as a promising approach to deal with optimizer mistakes. we identified the core building blocks of proactive re-optimization: i  characterizing uncertainty in estimates of statistics using bounding boxes  ii  using the bounding boxes to pick robust plans and switchable plans  and iii  estimating statistics quickly and efficiently during execution. as a next step  we plan to evaluate our specific algorithms and implementation decisions against some alternative options: 
  uncertainty and bounding boxes. we used the uncertainty initialization and propagation rules from  to characterize the level of uncertainty in estimates and derive bounding boxes. an interesting alternative is to characterize uncertainty in terms of stochastic intervals . 
  plan robustness. currently we characterize a plan as robust if its cost is close to optimal at three points in the bounding box. 
both the location and the number of these points in the bounding box require further study. furthermore  alternative notions of plan robustness  e.g.  based on expected costs  or confidence thresholds   will be considered. 
  switchable plans. we considered a fairly restricted notion of switchable plans based on the complete reuse of execution work. more flexible definitions  e.g.  allowing re-ordering of operators in a pipeline  may give the optimizer more room to find switchable plans. 
  random-sample processing. our approach so far is to merge random-sample processing with query execution to reduce the overhead. random-sample processing could be used more aggressively to reduce the uncertainty in statistics even before starting query execution  introducing a new challenge in determining how much statistics collection to do in advance. a more general area of future work is to explore how randomization and ordered output can coexist best in rio  e.g.  in the context of top-k queries. 
1. acknowledgements 
we are extremely grateful to jennifer widom for helpful feedback and discussions. we would like to thank guy lohman and volker markl for providing us the dmv data and workload generator. 
