in recent years  the proliferation of voip data has created a number of applications in which it is desirable to perform quick online classification and recognition of massive voice streams. typically such applications are encountered in real time intelligence and surveillance. in many cases  the data streams can be in compressed format  and the rate of data processing can often run at the rate of gigabits per second. all known techniques for speaker voice analysis require the use of an offline training phase in which the system is trained with known segments of speech. the state-of-the-art method for text-independent speaker recognition is known as gaussian mixture modeling  gmm   and it requires an iterative expectation maximization procedure for training  which cannot be implemented in real time. in this paper  we discuss the details of such an online voice recognition system. for this purpose  we use our micro-clustering algorithms to design concise signatures of the target speakers. one of the surprising and insightful observations from our experiences with such a system is that while it was originally designed only for efficiency  we later discovered that it was also more accurate than the widely used gaussian mixture model  gmm . this was because of the conciseness of the micro-cluster model  which made it less prone to over training. this is evidence of the fact that it is often possible to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective.
categories and subject descriptors
h.1  database management : database applications
general terms
algorithms
keywords
speaker recognition
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  san jose  california  usa.
copyright 1 acm 1-1-1/1 ...$1.
1. introduction
¡¡the problem of speaker voice analysis and classification is useful in a number of applications such as real time monitoring  detection  and surveillance. in this paper  we are concentrating on the problem of text-independent speaker classification in which the actual textual content of the speech is not available for modeling purposes. a number of statistical and machine learning methods have been recently proposed for speaker classification. some examples of such techniques may be found in .
¡¡a well known method for speaker classification and identification is that of gaussian mixture modeling  gmm  . the first step is to extract multi-dimensional feature vectors in order to represent portions of sampled speech. in this method  it is assumed that each data point extracted from the speech segments from a number of known speakers are used to estimate the parameters of a gmm model. then the data points from an unknown speech segment are applied to each speaker model in order to estimate the maximum likelihood fit. the model with the highest fit is reported as the relevant class label. the gmm modeling method has been widely popular because of its intuitive appeal and effectiveness  and has therefore been used extensively.
¡¡in many applications  it is desirable to perform the speaker identification in real time. in such cases  we need to have a system which is adaptive enough to learn characteristics of new speakers in real time  and used these learned profiles in order to construct the final model for speaker classification. unfortunately  popularly used models such as gmm are not very appropriate for real time speaker modeling. this is because the first step of determining the parameters of the mixture model requires an iterative computationally intensive approach known as the em algorithm. the second stage of model fitting requires the evaluation of a log likelihood criterion on the data set for each model. these techniques can be computationally intensive in practice  and are often difficult to use effectively for real time modeling and classification. furthermore  we are also interested in other mining variations of the classification problem in which it is desirable to model or match individual segments of speech from unknown speakers in real time. this is not possible with an iterative approach such as the em algorithm. in addition  since we are making the stream assumption for all data processing  we assume that each point in the data can be scanned only once throughout the computation. this assumption is also violated by all other speaker identification systems  known to us. therefore  we need to construct
industrial and government track short paper
a system which can work with the constraints of a one-pass system and still provide accurate results for speaker identification. in addition to speaker classification  we would like to design a system which is capable of detecting quick changes in the pattern of the underlying data stream. such a system is useful in applications in which it is desirable to segment portions of speech into different speakers. we note that this segmentation may need to be done in an unsupervised way  since example segments of the different speakers may not be known in advance. in order to design a system which can work with such challenges  we construct a number of techniques which are designed towards stream based online processing of massive amounts of audio data. we adapt our earlier research results for stream micro-clustering  in order to create fast cluster based signatures of the data. these signatures are constructed and used in real time in order to perform the speaker identification.
¡¡this paper is organized as follows. in the next section  we will describe the speaker recognition system. in section 1  we will present the experimental results obtained from deploying these algorithms over a number of real data sets.
section 1 contains a number of conclusions and observations.
1. the speaker identification system
¡¡in our online voice recognition system  we will use a nonparametric density estimation approach. the idea is to determine the probability distribution of the data for each speaker and map it to the behavior of unknown test segments in order to perform the matching. the reason for using a non-parametric approach is that most parametric methods such as the gmm model require an iterative approach to estimate the densities in the underlying data. on the other hand  a non-parametric approach adapts very well to an online application such as that of clustering data streams. a well known non-parametric method of finding the data distribution is that of kernel density estimation. however  density estimation is often an inefficient method since it requires us to estimate the data behavior over all regions of the data. in high dimensionality  most of the regions in the data are sparse  and the estimation will need to be performed over a very large number of data points  which is exponentially increasing with dimensionality  in order to provide a comprehensive overview of the data behavior. this can rapidly become untenable for non-uniform data distributions of even modestly high dimensionality. in order to substitute for using density distributions directly  we will use very fine grained micro-clusters in the data. micro-clustering is a concept which is used to track very detailed level of statistics of clusters with high granularity. this is a desirable approach when the data sets are received in the form of massive data streams. the distribution of the data for different speakers over these fine grained clusters are used as a surrogate for the actual densities in the underlying data. we will see that this turns out to be a practical and scalable approach in most applications  and also simulates the underlying densities quite well. in order to design the voice recognition system  we will build upon some microclustering machinery developed in . the micro-clustering framework discusses some summary statistics which are used to maintain the statistical information about the clusters. we assume that the dimensionality of the data stream is d.
the definition of a micro-cluster is as follows:
¡¡definition 1. a micro-cluster for a set of d-dimensional points c = {xi1 ...xin} with time stamps ti1 ...tin is defined as the  wherein each correspond to a vector of d entries. the definition of each of these entries is as follows:
¡¡  for each dimension  the sum of the squares of the data values is maintained in . thus  contains d values. the p-th entry of  is equal to.
¡¡  for each dimension  the sum of the data values is main-
. the p-th entry of	 is equal to
j=1 ij
¡¡  the last update time  which is max{ti1 ...tin}  is main-
         t tained in cf  c .
¡¡  the number of data points is maintained in n c .
the design of micro-cluster statistics is chosen so as to to make some important predictions about the data points in them. we make the following two observations about the statistics  stored in the micro-clusters:
¡¡observation 1. the mean and variance of the data points in a cluster can be determined from the micro-cluster statistics.
in addition  the micro-clusters satisfy the additivity property.
we omit the details of the proofs since they can be found in
.
¡¡observation 1. the micro-cluster statistics satisfy the additivity property. the micro-cluster statistics for the union of two sets of data points is the sum of the micro-cluster statistics of the individual sets of points.
as discussed later  the above observations are used during the clustering process. the clustering process is used to create and store summary frequency information. this summary information is used in order to perform effective classification. a set of a clusters can be used to create a signature summary of the data. this signature summary is defined in terms of the frequencies of the different data points drawn from the segment s  in terms of their distribution over the clusters c1 ...ck. the signature summary is used as a surrogate for the probability density of the data points in the stream. as we will observe later  this is not an unreasonable assumption when a fine granularity of clustering is maintained. let us denote the data in the last segment of length s by d s . we define the signature summary of the data set d s  over the segment s with respect to the clusters c1 ...ck as follows:
¡¡definition 1. the signature summary ¦Â c1 ...ck s  for a set of k clusters c1 ...ck and segment of data points s is defined as the k-dimensional vector  f1 ...fk   where fi is defined as follows:
	fi = |d s  ¡É ci|/|d s |	 1 
it is important to note that the signature summary essentially defines the relative distribution of the data points across the different clusters. when the individual clusters have very small variance  they can be used as a surrogate for the true density distribution. in order to explain the relationship between the probability density and the frequencies of the data points across different clusters  we make the following observations.

¡¡let x ci  and h ci 1 be the centroid and variance of cluster ci. then  the density estimate ¦Á x  at any point x in the space can be constructed as follows:
		 1 
we note that equation 1 is a modification to the standard density function which is often used in kernel density estimation of individual data points. the primary difference is the cluster-specific value of the bandwidth h ci . here h¡ä is an additive bandwidth which is defined in the same way as standard kernel density estimation  except that we use the number of clusters rather than the number of points to define h¡ä. according to the silverman approximation rule  for a data distribution with n points  clusters  and variance ¦Ò of the data points  cluster centroids   the value of h¡ä is chosen to be 1¡¤¦Ò¡¤n 1. we note that limn¡ú¡Þh¡ä ¡ú 1. in effect  the density estimate is equal to the sum of gaussian kernels which are centered at different clusters  and have bandwidth which is defined by the variance of the data points in the cluster. for very fine grained clusters  such an approach provides a very good estimate to the process of kernel density estimation. this is because this limiting case defines a situation in which the value of h ci  tends to zero. on substituting the value of h ci  = 1 in equation 1  we obtain the same formula used for standard kernel density estimation. we summarize this observation as follows:
¡¡observation 1. in the limiting case  when each cluster contains only one data point  the value ¦Á x  is equal to the standard kernel density estimate.
¡¡the essential idea behind this exercise was to illustrate that the signature summary can encode very detailed information about the probability distribution of the data even for very fine grained clustering. we note that the difference in probability density between two distributions ¦Á1 ¡¤  and ¦Á1 ¡¤  may be defined as follows:
	diff ¦Á1 ¦Á1  = z | ¦Á1 x    ¦Á1 x |dx	 1 
x
this difference is essentially equal to the integral of the difference of the two distributions over the entire space. intuitively  we would like to use this difference in density distribution as a measure for the fit between two density distributions. note that even when probability densities can be estimated accurately  this difference can be difficult to compute in practice. therefore  as the surrogate  we compute the difference in signature summaries between the two data streams.
¡¡definition 1. the distance dist h1 ¡¤  h1 ¡¤   between two signature summaries h1 ¡¤  =  f1 ...fk  and h1 ¡¤  =  is defined as follows:
k
	dist h1 ¡¤  h1 ¡¤   = x |fi   fi¡ä|	 1 
i=1
this distance is thus defined in terms of the l1-norm. we note that this distance is essentially a discrete analogue of the difference in density distributions. in this case  we are algorithm constructspeechsignatures speech segment: s  current clusters: c1 ...ck  numberofclusters: k ; begin for each speaker label i
hi =  1 ...1 ;

for the next data point x in s do
begin

determine closest micro-cluster ci to x;

add x to ci and update
micro-cluster statistics for ci;
update signature counts for corresponding cluster in hj where j is the label of the current data point; end
end
figure 1: constructing summary signatures
summing the discrete differences in probabilities that a voice packet belongs to a particular micro-cluster.
¡¡the micro-clustering algorithm is used in order to construct profiles from the speaker data. these profiles are essentially the signatures in the data stream. the microclustering algorithm uses a nearest neighbor clustering algorithm in which each data point is assigned to the mean of the closest cluster. as discussed in observation 1  the mean of the cluster can be computed from the micro-cluster statistics. once the closest micro-cluster has been computed  we add the statistics for that data point to the micro-cluster statistics. this is possible to achieve because of the microcluster statistics discussed in observation 1. the overall process for signature construction is discussed in figure 1.
¡¡one problem which may arise in a real application is that of cluster centroid drift. cluster centroids can drift when the new data points which are added to the clusters have a different distribution from the original set of data points. such a drift may affect the significance of the signatures over time  since different voice streams may be received one after another  and may have slightly different cluster centroids. this situation is also quite possible in real applications since the data distribution may vary over time. this is an undesirable situation  since it is difficult to fit the test data accurately. we note that the key function of cluster centroids is to ensure that the most relevant  dense  regions in the data are represented by these anchors. the exact location of these clusters is not very important  as long as they remain fixed over time  and represent most of the dense regions. furthermore  we note that the only statistic which is used during the actual speaker identification process is the relative frequencies of the data points in the different clusters. therefore  it is acceptable to fix the cluster centroids once each micro-cluster is sufficiently populated. therefore  we pick a small threshold  say 1 voice packets  for each micro-cluster  and apply the micro-cluster update algorithm in a normal way till this limit. however  when the number of data points in the micro-cluster exceed this threshold  we do not add the assigned data point to the micro-cluster  but only update the count of the number of data points in it. the other statistics are updated by the corresponding multiplicative factor in order to reflect a larger number of data points. this ensures that the centroid of the micro-cluster remains fixed  but the frequency statistics are appropriately maintained.
during the training phase  we also maintain the signatures
industrial and government track short paper

figure 1: training signature for aaron brown
for the different speaker labels. these signatures are used for the speaker classification process. specifically  we assume that the k-dimensional vector hi is used in order to track the micro-cluster frequency behavior of the speaker i. whenever a new data point is received  we add to the frequency for the corresponding micro-cluster in the signature for speaker i.
¡¡we note that the great flexibility of the scheme is that the testing phase is essentially no different from the training phase  except that the micro-cluster statistics do not need to be updated. as in the training phase  we construct the signature for the test segment in order to create the summary signatures. this test segment is matched with the training segments using the relationship discussed in equation 1. the closest matching signature is returned as the speaker identification.
¡¡the method discussed in this paper can also be used for speech segmentation. the essential idea in speech segmentation is to partition the conversations between one and more speakers in an unsupervised way. we note that this is often required as a pre-processing step to speaker identification in a real time application in which one does not have the time to perform the segmentation manually after using the content of the speech. in order to extend the methodology to these cases  we perform the micro-clustering continuously as in the previous case  except that we save the signatures in each window at intervals which are equivalent to the window size. for each window  we determine the distance between the signatures for the current window and the window just before it. this leads to a continuous alarm function over the data stream. this alarm function can be used in order to track sudden changes in the trends of speaker behavior. this can create a clear segmentation between the different speakers in a conversation. more details and experimental tests of the approach will be described in an extended version of this paper.
1. experimental results
¡¡in this section  we will discuss some empirical results illustrating the effectiveness and efficiency of the method on a variety of voice data sets. for the purpose of comparison  we will use a standard gaussian mixture model  which was derived from a software called netlab . for each speaker  we first constructed a gmm on the training data set  and

figure 1: training signature for bill clinton

figure 1: testing signature for aaron brown  1 records 

figure 1: testing signature for aaron brown  1
records 

figure 1: accuracy of classification for test segments of different sizes 
calculated the log-likelihood of each speaker in the training data on the test data segments. the maximum likelihood speaker was reported as the speaker for the test data set. we note that the use of gmms is considered as state of the art for text independent speech recognition . while such models often cannot be used easily for online training and detection  the accuracy provides a idea of the desired accuracy of an online system.
¡¡the data set used is the hub-1 data set  which contains the voice signal of 1 different public personalities such as wolf blitzer  bill clinton  al gore  candy crowley etc. in each case  a training data set and test data set was constructed from the data. in most real time applications  the size of the test data can be quite small and cannot be controlled. furthermore  it is desirable to use as small a test segment as possible to perform the classification  since this affects the latency of speaker identification. on the other hand  very large sets of training data can often be made available in most applications for well known speakers by compiling data from multiple sources. therefore  we chose to divide the training data such that the training data size was twice the test data in each case.
¡¡first  we will discuss some examples of signatures which were obtained by using micro-clustering on the different data sets. in figures 1 and 1  we have illustrated some examples of the signatures obtained from the aaron brown  and bill clinton respectively. in each case  we have used 1 microclusters in order to construct the signatures. in each case  the cluster-id is illustrated on the x-axis  whereas the relative frequency of the corresponding cluster is illustrated on the y-axis. it is clear that in each case  the corresponding signatures are quite distinct. the distinct natures of these signatures are very useful in performing an exact classification process on different test segments. in figures 1 and 1  we have illustrated the signatures obtained on test segments of different sizes for the signatures corresponding to aaron brown. it is clear that the signature in figure 1 from the test segment of larger size is closer to the signature on the training data set. as a result  the accuracy of classification increases with increasing size of the test segment. in figure 1  we have illustrated the accuracy on test segments of different sizes. in each case  we trained for all 1 speakers simultaneously  and classified a test segment into one of these 1 speakers. this is significantly more difficult than a problem in which we try to distinguish a particular speaker from a control or mixed signal. this is because many speakers may have inherent similarity in their voice patterns  and it often becomes difficult to distinguish between a very large number of speakers using a single model. in figure 1  we have illustrated the number of records on the x-axis  and the classification accuracy over the 1 speakers on the y-axis. in the same figures  we have illustrated the accuracy using gaussian mixture models on test segments of different sizes. the accuracy of classification increases considerably with increasing test segment size because it is easier to build more accurate models on larger test segments. it is interesting to see that the accuracy of the micro-clustering process was significantly higher than the more computationally intensive  and offline  gmm approach. our initial goals in designing this system were only to construct a system which could work in a one-pass approach for an online data stream  and we did not expect the micro-clustering approach to outperform the gmm model in terms of accuracy. therefore  the  substantial  qualitative advantage of the micro-clustering approach over the gmm model was particularly surprising. we believe that the higher accuracy is because of the more compact representation of the micro-clustering approach.
1. conclusions and summary
¡¡in this paper  we discussed an online system for voice recognition in data streams. we discussed a micro-clustering approach which is not only efficient  but is significantly more accurate than the state-of-the-art gmm model. the greater accuracy of the micro-clustering approach is particularly surprising considering the fact that the gmm model is designed using the iterative em-approach which does not have the constraints of the one-pass stream based micro-clustering model. while the simplicity and compactness of the microclustering signature representation was originally designed only for speed  we found that it achieves the dual purpose of avoiding the overtraining of the parameter-heavy gmm model   a classic example of the  less is more  paradigm in machine learning.
