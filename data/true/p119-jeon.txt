libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. however  manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. here  we propose an automatic approach to annotating and retrieving images based on a training set of images. we assume that regions in an image can be described using a small vocabulary of blobs. blobs are generated from image features using clustering. given a training set of images with annotations  we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. this may be used to automatically annotate and retrieve images given a word as a query. we show that relevance models. allow us to derive these probabilities in a natural way. experiments show that the annotation performance of this cross-media relevance model is almost six times as good  in terms of mean precision  than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval.
categories and subject descriptors
h.1  information search and retrieval : retrieval models; i.1  image processing and computer vision : scene analysis-object recognition
general terms
algorithms  measurement  experimentation
keywords
image annotation  image retrieval  relevance models
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
1. introduction
　efficient access to multimedia information requires the ability to search and organize the information. while  the technology to search text has been available for some time - and in the form of web search engines is familiar to many people - the technology to search images and videos  is much more challenging. several researchers  see  for a review  have investigated techniques to retrieve images based on their content but many of these approaches require the user to query based on image concepts like color or texture which most people are not familiar with. in general  people would like to pose semantic queries using textual descriptions and find images relevant to those semantic queries. for example  one should be able to pose a query like  find me all images of tigers in grass . this is difficult if not impossible with many of these image retrieval systems and hence has not led to widespread adoption of these systems. the traditional solution to this problem  used by libraries and other organizations is to annotate such images manually and then search those annotations. although this allows semantic image retrieval manual annotations are expensive and do not always capture the content of images and videos well.
　one approach to automatically annotating images is to look at the probability of associating words with image regions. mori et al.  used a co-occurrence model in which they looked at the co-occurrence of words with image regions created using a regular grid. more recently  a few other researchers  1  1  have also examined the problem using machine learning approaches. in particular duygulu et al  proposed to describe images using a vocabulary of blobs. each image is generated by using a certain number of these blobs. their translation model - a substantial improvement on the co-occurence model - assumes that image annotation can be viewed as the task of translating from a vocabulary of blobs to a vocabulary of words. given a set of annotated training images  they show how one can use one of the classical machine translation models suggested by brown et al.  to annotate a test set of images.
　isolated pixels or even regions in an image are often hard to interpret. it is the context in which an image region is placed that gives it meaning. query expansion is a standard technique for reducing ambiguity in information retrieval. one approach to doing this is to perform an initial query and then expand queries using terms from the top relevant documents  often approximated by the top documents . this expanded query when used for retrieval increases the performance substantially. in the image context  tigers are more often associated with grass  water  trees or sky and less often with objects like cars or computers and we want to take advantage of this context.
　relevance-based language models  1  1  were introduced to allow query expansion to be performed in a more formal manner. these models have been successfully used for both ad-hoc retrieval and cross-language retrieval. here  we investigate the problem of automatically annotating images as well as the ranked retrieval of images using a modification of the relevance model. as in duygulu et al  we assume that every image may be described using a small vocabulary of blobs. using a training set of annotated images  we learn the joint distribution of blobs and words which we call a cross-media relevance model  cmrm  for images. there are two ways this model can be used. in the first case  which corresponds to document based expansion  the blobs corresponding to each test image are used to generate words and associated probabilities from the joint distribution of blobs and words. each test image can  therefore  be annotated with a vector of probabilities for all the words in the vocabulary. we call this the probabilistic annotation-based crossmedia relevance model  pacmrm . given a query word  this model can be used to rank the images using a language modeling approach  1  1  1  1 . while this model is useful for ranked retrieval  it is less useful for people to look at. fixed length annotations can be generated by using the top n  n = 1  1 or 1  words  without their probabilities  to annotate the images. this model is called the fixed annotation-based cross-media relevance model  facmrm . facmrm is not useful for ranked retrieval  since there are no probabilities associated with the annotations  but is easy for people to use when the number of annotations is small.
　in the second case  which corresponds to query expansion  the query word s  is used to generate a set of blob probabilities from the joint distribution of blobs and words. this vector of blob probabilities is compared with the vector of blobs for each test image using kullback-liebler  kl  divergence and the resulting kl distance is used to rank the images. we call this model the direct-retrieval cross-media relevance model  drcmrm .
　we should point out that cross-media relevance models are not translation models in the sense of translating words to blobs. instead  these models take advantage of the joint distribution of words and blobs - that is the fact that an image can be described both using image features  blobs  and text  words . as duygulu et al. point out the problem of object recognition can be viewed as one of assigning  names  or words to images or image regions. in our model  we assign words to entire images and not to specific blobs because the blob vocabulary can give rise to many errors.
　our annotation-based model performs much better than either the co-occurrence model or the translation model on the same dataset  same training and test images and the same features . specifically  for the top 1 annotations  we show that facmrm gives a mean precision of 1 compared to 1  obtained from published results   for the translation model. this is twice as good as the translation model. at the same time the facmrm also has a much higher recall than the translation model. both models perform substantially better than the co-occurrence model. pacmrm and drcmrm cannot be directly compared to the other systems since the translation model and the co-

figure 1: images automatically annotated as  sunset   facmrm  but not manually annotated as  sunset . the color of sunset may not show up clearly in black and white versions of this figure.
occurrence model have not been used for ranked retrieval.
　figure 1 illustrates the power of the relevance model. the figure shows three images  from the test set  which were annotated with  sunset  by facmrm. although the three are clearly pictures of sunset  the last picture shows both a sun and a sunset   the word  sunset  was missing from the manual annotations. in these cases  the model allows us to catch errors in manual annotation.
　this paper is organized as follows. we discuss related work in section 1. this is followed by a brief discussion of how the blob features are constructed. section 1 has a discussion of the cross-media language model and how it can be used for image annotation and retrieval. section 1 shows experimental results for the different models and compares them to those for the translation and co-occurrence model. the section also shows example results to illustrate the different aspects of the model. finally  the last section concludes with a discussion of future work in this area.
1. related work
　while there has been some work on statistical models for object recognition and image retrieval   there has been little work on automatically annotating images. we have already mentioned the co-occurrence  and translation models . the co-occurrence model tends to require large numbers of training samples to estimate the correct probability and also tends to map frequent words to every blob. duygulu et al  also try to use their translation model to label individual regions in the image. picard and minka  describe a tool for users to semi-automatically annotate image regions by selecting positive and negative examples manually and then using texture similarity to propagate annotations. barnard and forsyth extended hofmann's hierarchical aspect model for text and proposed a multimodal hierarchical aspect model for hierarchical clustering of images and words. the results of this model are not available in a form which can be directly compared to our present model. blei and jordan  extended the latent dirichlet allocation  lda  model and proposed a correlation lda model which relates words and images. this model assumes that a dirichlet distribution can be used to generate a mixture of latent factors. this mixture of latent factors is then used to generate words and regions. em is again used to estimate this model. blei and jordan show a few examples for labeling specific regions in an image. they also report recall-precision graphs for retrieval performance based on one word queries but the results are not directly comparable since the datasets are different.
1. discrete features in images
an important question is how can one obtain an image vocabulary. in other words  how does one represent every image in the collection using a subset of items from a finite set of items. an intuitive answer to this question is to segment the image into regions  cluster similar regions and then use the regions as a vocabulary. the hope is that this will produce semantic regions and hence a good vocabulary. in general  image segmentation is a very fragile and erroneous process and so the results are usually not very good.
　barnard and forsyth and duygulu et al.  used general purpose segmentation algorithms like blobworld and normalized-cuts to extract regions. these algorithms do not always produce good segmentations  see figure 1  but are useful for building and testing models. for each segmented region  features such as color  texture  position and shape information are computed. duygulu et al  used normalized-cuts to segment images and then extracted 1 features from the images. they ignored regions which were smaller than a threshold size. given a set of training images  a k-means clustering algorithm  k = 1  is applied to cluster the regions on the basis of these features. these 1 clusters which they call  blobs  compose the vocabulary for the set of images. each blob is assigned a unique integer to serve as its identifier  analogous to a word's ascii representation . all images in the training set can now be represented as a set of blobs from this vocabulary. figure 1 shows the segmentation and the clustering process for some training images. the resulting blobs produced by this approach still leave a lot to be desired  see for example section 1 . however  given the complexity of images  this is a good first start. given a new test image  it can be segmented into regions and region features can be computed. the blob which is closest to it in cluster space is assigned to it. our primary purpose in this paper is to show that relevance models are a powerful tool for solving the problem of image annotation and retrieval. in order to make a fair comparison with other models we choose to use their  data and feature sets.
1. cross-media relevance models
　suppose we are given a collection c of un-annotated images. each image i （ c is represented by a discrete set of blob numbers  generated as described in section 1: i = {b1 ... bm}. in this section we develop a formal model that allows us to answer the following questions:
 i  given an un-annotated image i （ c  how can we automatically assign meaningful keywords to that image 
 ii  given a text query w1 ... wk  how can we retrieve images i （ c that contain objects mentioned in the query 
　we assume there exists a training collection t   of annotated images  where each image j （ t has a dual representation in terms of both words and blobs:
j = {b1 ... bm; w1 ... wn}. here {b1 ... bm} represents the blobs corresponding to regions of the image and {w1 ... wn} represents the words in the image caption 1. the number of blobs and words in each image  m and n  may be different from image to image. in contrast to the translation model  we do not assume that there is an underlying one-to-one correspondence  alignment  between the blobs and the words in an image  we only assume that a set of keywords {w1 ... wn} is related to the set of objects represented by blobs {b1 ... bm}.
1 a model of image annotation
　suppose we are given an un-annotated image i （ c. we have the blob representation of that image i = {b1 ... bm}  and want to automatically select a set of words {w1 ... wn} that accurately reflects the content of the image.
　we adopt a generative language modeling approach  1  1  1 . assume that for each image i there exists some underlying probability distribution p ，|i . we refer to this distribution as the relevance model of i  see  1  1  . the relevance model can be thought of as an urn that contains all possible blobs that could appear in image i  as well as all words that could appear in the caption of i. we assume that the observed image representation {b1 ... bm} is the result of m random samples from p ，|i .
　a natural way to annotate an image i would be to sample n words w1 ... wn from its relevance model p ，|i . in order to do that  we need to know the probability of observing any given word w when sampling from p ，|i . that is  we need to estimate the probability p w|i  for every word w in the vocabulary. given that p ，|i  itself is unknown  the probability of drawing the word w is best approximated by the conditional probability of observing w given that we previously observed b1 ... bm as a random sample from the same distribution:
	p w|i  「 p w|b1 ... bm 	 1 
we cannot use the prevalent maximum-likelihood estimator for that probability because the image representation b1 ...bk does not contain any words. however  we can use the training set t of annotated images to estimate the joint probability of observing the word w and the blobs b1 ... bm in the same image  and then marginalizing the distribution with respect to w. the joint distribution can be computed as the expectation over the images j in the training set:

we assume that the events of observing w and b1  ...  bm are mutually independent once we pick the image j  and identically distributed according to the underlying distribution p ，|j . this assumption follows directly from our earlier decision to model each image as an urn containing both words and blobs. since the events are independent  we can rewrite equation  1  as follows:

the prior probabilities p j  can be kept uniform over all images in t . since the images j in the training set contain both words and blobs  we can use smoothed maximumlikelihood estimates for the probabilities in equation  1 . specifically  the probability of drawing the word w or a blob b from the model of image j is given by:
 1 
 1 

figure 1: image preprocessing: step 1 shows the segmentation results from a typical segmentation algorithm  blobworld  the clusters in step 1 are manually constructed to show the concept of blobs. both the segmentation and the clustering often produce semantically inconsistent segments  breaking up the tiger  and blobs seals and elephants in the same blob  .
here  # w j  denotes the actual number of times the word w occurs in the caption of image j  usually 1 or 1  since the same word is rarely used multiple times in a caption . # w t   is the total number of times w occurs in all captions in the training set t . similarly  # b j  reflects the actual number of times some region of the image j is labeled with blob b  and # b t   is the cumulative number of occurrences of blob b in the training set. |j| stands for the aggregate count of all words and blobs occurring in image j  and |t | denotes the total size of the training set. the smoothing parameters αj and βj determine the degree of interpolation between the maximum likelihood estimates and the background probabilities for the words and the blobs respectively. we use different smoothing parameters for words and blobs because they have very different occurrence patterns: words generally follow a zipfian distribution  whereas blobs are distributed much more uniformly  due in part to the nature of the clustering algorithm that generates them. the values of these parameters are selected by tuning system performance on the held-out portion of the training set.
1.1 using the model for image annotation
　equations  1  -  1  provide the machinery for approximating the probability distribution p w|i  underlying some given image i. we can produce automatic annotations for new images by first estimating the distribution p w|i  and then sampling from it repeatedly  until we produce a caption of desired length. or we could simply pick a desired number n of words that have the highest probability under p w|i  and use those words for the annotation.
1 two models of image retrieval
　the task of image retrieval is similar to the general ad-hoc retrieval problem. we are given a text query q = w1 ... wk and a collection c of images. the goal is to retrieve the images that contain objects described by the keywords w1 ... wk  or more generally rank the images i by the likelihood that they are relevant to the query. we cannot simply use a text retrieval systems because the images i （ c are assumed to have no captions. in the remainder of this section we develop two models of image retrieval. the first model makes extensive use of the annotation model developed in the previous section. the second model does not rely on annotations and instead  translates  the query into the language of blobs.
1.1 annotation-based retrieval model
　a simple approach to retrieving images is to annotate each image in c using the techniques proposed in section 1 with a small number of keywords. we could then index the annotations and perform text retrieval in the usual manner. this approach is very straightforward  and  as we will show in section 1  is quite effective for single-word queries. however  there are several disadvantages. first  the approach does not allow us to perform ranked retrieval  other than retrieval by coordination-level matching . this is due to the binary nature of word occurrence in automatic annotations: a word either is or is not assigned to the image  it is rarely assigned multiple times. in addition  all annotations are likely to contain the same number of words  so documentlength normalization will not differentiate between images. as a result  all images containing some fixed number of the query words are likely to receive the same score. the second problem with indexing annotations is that we must a-priori decide what annotation length is appropriate. the number of words in the annotation has a direct influence on the recall and precision of this system. in general  shorter annotations will lead to higher precision and lower recall  since fewer images will be annotated with any given word. short annotations are more appropriate for a casual user  who is interested in finding a few relevant images without looking at too much junk. on the other hand  a professional user may be interested in higher recall and thus may need longer annotations. consequently  it would be challenging to field the retrieval system in a way that would suit diverse users.
　an alternative to fixed-length annotation is to use probabilistic annotation. in section 1 we developed a technique that assigns a probability p w|i  to every word w in the vocabulary. rather than matching the query against the few top words  we could use the entire probability distribution p ，|i  to score images using a language-modeling approach  1  1  1  1 . in a language modeling approach we score the documents  images  by the probability that a query would be observed during i.i.d. random sampling from a document  image  language model. given the query q = w1 ... wk  and the image i = {b1 ... bm}  the probability of drawing q from the model of i is:
k
	 	 1 
where p wj|i  is computed according to equations  1  -  1 . this model of retrieval does not suffer from the drawbacks of fixed-length annotation and allows us to produce ranked lists of images that are more likely to satisfy diverse users.
1.1 direct retrieval model  drcmrm 
　the annotation-based model outlined in section 1.1 in effect converts the images in c from the blob-language to the language of words. it is equally reasonable to reverse the direction and convert the query into the language of blobs. then we can directly retrieve images from the collection c by measuring how similar they are to the blob-representation of the query. the approach we describe was originally proposed by  for the task of cross-language information retrieval. we start with a text query q = w1 ... wk.
we assume that there exists an underlying relevance model p ，|q   such that the query itself is a random sample from that model. we also assume that images relevant to q are random samples from p ，|q   hence the name relevance model . in the remainder of this section we describe:  i  how to estimate the parameters p b|q  of this underlying relevance model  and  ii  how we could rank the images with respect to this model.
　estimation of the unknown parameters of the query model is performed using the same techniques used in section 1. the probability of observing a given blob b from the query model can be expressed in terms of the joint probability of observing b from the same distribution as the query words
	 1 
the joint probability p b w1 ... wk  can be estimated as an expectation over the annotated images in the training set  by assuming independent sampling from each image j （ t :

