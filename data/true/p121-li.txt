in outsourced database  odb  systems the database owner publishes its data through a number of remote servers  with the goal of enabling clients at the edge of the network to access and query the data more efficiently. as servers might be untrusted or can be compromised  query authentication becomes an essential component of odb systems. existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. in this work  first we define a variety of essential and practical cost metrics associated with odb systems. then  we analytically evaluate a number of different approaches  in search for a solution that best leverages all metrics. most importantly  we look at solutions that can handle dynamic scenarios  where owners periodically update the data residing at the servers. finally  we discuss query freshness  a new dimension in data authentication that has not been explored before. a comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. our findings exhibit that the proposed solutions improve performance substantially over existing approaches  both for static and dynamic environments.
1. introduction
　　database outsourcing  is a new paradigm that has been proposed recently and received considerable attention. the basic idea is that data owners delegate their database needs and functionalities to a third-party that provides services to the users of the database. since the third party can be untrusted or can be compromised  security concerns must be addressed before this delegation.
there are three main entities in the outsourced database
 odb  model: the data owner  the database service provider  a.k.a. server  and the client. in general  many instances of each entity may exist. in practice  usually there is a single or a few data owners  a few servers  and many clients. the data owner first creates the database  along with the associated index and authentication structures and uploads it to the servers. it is assumed that the data owner may up-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
date the database periodically or occasionally  and that the data management and retrieval happens only at the servers. clients submit queries about the owner's data to the servers and get back results through the network.
　　it is much cheaper to maintain ordinary servers than to maintain truly secure ones  particularly in the distributed setting. to guard against malicious/compromised servers  the owner must give the clients the ability to authenticate the answers they receive without having to trust the servers. in that respect  query authentication has three important dimensions: correctness  completeness and freshness. correctness means that the client must be able to validate that the returned records do exist in the owner's database and have not been modified in any way. completeness means that no answers have been omitted from the result. finally  freshness means that the results are based on the most current version of the database  that incorporates the latest owner updates. it should be stressed here that query freshness is an important dimension of query authentication that has not been extensively explored in the past  since it is a requirement arising from updates to the odb systems  an aspect that has not been sufficiently studied yet.
　　there are a number of important costs relating to the construction  query  and update phases of the aforementioned model. in particular  in this work the following metrics are considered: 1. the computation overhead for the owner  1. the owner-server communication cost  1. the storage overhead for the server  1. the computation overhead for the server  1. the client-server communication cost  and 1. the computation cost for the client  for verification .
　　previous work has addressed the problem of query authentication mostly for static scenarios  where owners never issue data updates. in addition  existing solutions take into account only a subset of the metrics proposed here  and hence are optimized only for particular scenarios and not the general case. finally  previous work was mostly of theoretical nature  analyzing the performance of the proposed techniques using analytical cost formulas  and not taking into account the fact that certain cryptographic primitives do not feature idealistic characteristics in practice. for example  trying to minimize the i/o cost associated with the construction of an authenticated structure does not take into account the fact that generating signatures using popular public signature schemes is two times slower than a random disk page access on today's computers. to the best of our knowledge  no previous work ever conducted empirical evaluations on a working prototype of existing techniques.
our contributions. in this work  we: 1. conduct a methodical analysis of existing approaches over all six metrics 
	1
figure 1: example of a merkle hash tree.
1. propose a novel authenticated structure that best leverages all metrics  1. formulate detailed cost models for all techniques that take into account not only the usual structural maintenance overheads  but the cost of cryptographic operations as well  1. discuss the extensions of the proposed techniques for dynamic environments  where data is frequently updated   1. consider possible solutions for guaranteeing query freshness  1. implement a fully working prototype and perform a comprehensive experimental evaluation and comparison of all alternatives.
　　we would like to point out that there are other security issues in odb systems that are orthogonal to the problems considered here. examples include: privacy-preservation issues  1  1  1   secure query execution   security in conjunction with access control requirements  1  1  1  and query execution assurance . in particular  query execution assurance of  does not provide authentication: the server could pass the challenges and yet still return false query results.
　　the rest of the paper is organized as follows. section 1 presents background on essential cryptography tools  and a brief review of related work. section 1 discusses the authenticated index structures for static odb scenarios. section 1 extends the discussion to the dynamic case and section 1 addresses query freshness. finally  the empirical evaluation is presented in section 1. section 1 concludes the paper.
1. preliminaries
　　the basic idea of the existing solutions to the query authentication problem is the following. the owner creates a specialized data structure over the original database that is stored at the servers together with the database. the structure is used by a server to provide a verification object vo along with the answers  which the client can use for authenticating the results. verification usually occurs by means of using collision-resistant hash functions and digital signature schemes. note that in any solution  some information that is authentic to the owner must be made available to the client; else  from the client's point of view  the owner cannot be differentiated from a  potentially malicious  server. examples of such information include the owner's public signature verification key or a token that in some way authenticates the database. any successful scheme must make it computationally infeasible for a malicious server to find incorrect query results and verification object that will be accepted by a client who has the appropriate authentication information from the owner.
1 cryptography essentials
collision-resistant hash functions. for our purposes  a hash function h is an efficiently computable function that takes a variable-length input x to a fixed-length output y = h x . collision resistance states that it is computationally infeasible to find two inputs    such that h x1  = h x1 . collision-resistant hash functions can be built provably based on various cryptographic assumptions  such as hardness of discrete logarithms . however  in this work we concentrate on using heuristic hash functions  which have the advantage of being very fast to evaluate  and specifically focus on sha1   which takes variable-length inputs to 1-bit  1-byte  outputs. sha1 is currently considered collision-resistant in practice; we also note that any eventual replacement to sha1 developed by the cryptographic community can be used instead of sha1 in our solution.
public-key digital signature schemes. a public-key digital signature scheme  formally defined in   is a tool for authenticating the integrity and ownership of the signed message. in such a scheme  the signer generates a pair of keys  sk  pk   keeps the secret key sk secret  and publishes the public key pk associated with her identity. subsequently  for any message m that she sends  a signature sm is produced by: sm = s sk m . the recipient of sm and m can verify sm via v pk m sm  that outputs  valid  or  invalid.  a valid signature on a message assures the recipient that the owner of the secret key intended to authenticate the message  and that the message has not been changed. the most commonly used public digital signature scheme is rsa . existing solutions  1  1  1  1  for the query authentication problem chose to use this scheme  hence we adopt the common 1-bit  1-byte  rsa. its signing and verification cost is one hash computation and one modular exponentiation with 1-bit modulus and exponent.
aggregating several signatures. in the case when t signatures s1 ... st on t messages m1 ... mt signed by the same signer need to be verified all at once  certain signature schemes allow for more efficient communication and verification than t individual signatures. namely  for rsa it is possible to combine the t signatures into a single aggregated signature s1 t that has the same size as an individual signature and that can be verified  almost  as fast as an individual signature. this technique is called condensed-rsa . the combining operation can be done by anyone  as it does not require knowledge of sk; moreover  the security of the combined signature is the same as the security of individual signatures. in particular  aggregation of t rsa signatures can be done at the cost of t   1 modular multiplications  and verification can be performed at the cost of t 1 multiplications  t hashing operations  and one modular exponentiation  thus  the computational gain is that t   1 modular exponentiations are replaced by modular multiplications . note that aggregating signatures is possible only for some digital signature schemes.
the merkle hash tree. an improvement on the straightforward solution for authenticating a set of data values is the merkle hash tree  see figure 1   first proposed by . it solves the simplest form of the query authentication problem for point queries and datasets that can fit in main memory. the merkle hash tree is a binary tree  where each leaf contains the hash of a data value  and each internal node contains the hash of the concatenation of its two children. verification of data values is based on the fact that the hash value of the root of the tree is authentically published  authenticity can be established by a digital signature . to prove the authenticity of any data value  all the prover has to do is to provide the verifier  in addition to the data value itself  with the values stored in the siblings of the path that leads from the root of the tree to that value. the veritable 1: notation used.
symboldescriptionra database recordka b+-tree keypa b+-tree pointerha hash valuesa signaturesize of object x
total number of database recordsnrtotal number of query resultsppage sizefxnode fanout of structure xdxheight of structure xhl x  sl x  l x 
cxa hash operation on input x of length l
a signing operation on input x of length l
a verifying operation on input x of length l
cost of operation x
the verification objectvo
fier  by iteratively computing all the appropriate hashes up the tree  at the end can simply check if the hash she has computed for the root matches the authentically published value. the security of the merkle hash tree is based on the collision-resistance of the hash function used: it is computationally infeasible for a malicious prover to fake a data value  since this would require finding a hash collision somewhere in the tree  because the root remains the same and the leaf is different-hence  there must be a collision somewhere in between . thus  the authenticity of any one of n data values can be proven at the cost of providing and computing log1 n hash values  which is generally much cheaper than storing and verifying one digital signature per data value. furthermore  the relative position  leaf number  of any of the data values within the tree is authenticated along with the value itself.
cost models for sha1  rsa and condensed-rsa.
since all existing authenticated structures are based on sha1 and rsa  it is imperative to evaluate the relative cost of these operations in order to be able to draw conclusions about which is the best alternative in practice. based on experiments with two widely used cryptography libraries  crypto++  and openssl   we obtained results for hashing  signing  verifying and performing modulo multiplications. evidently  one hashing operation on our testbed computer takes approximately 1 to 1 μs. modular multiplication  signing and verifying are  respectively  approximately 1  1 and 1 times slower than hashing  verification is faster than signing due to the fact that the public verification exponent can be fixed to a small value .
　　thus  it is clear that multiplication  signing and verification operations are very expensive  and comparable to random disk page accesses. the cost of these operations needs to be taken into account in practice  for the proper design of authenticated structures. in addition  since the cost of hashing is orders of magnitude smaller than that of singing  it is essential to design structures that use as few signing operations as possible  and hashing instead.
1 previous work
　　there are several notable works that are related to our problem. a good survey is provided in ; our review here is brief. the first set of attempts to address query authentication problems in odb systems appeared in  1  1  1 . the focus of these works is on designing solutions for query correctness only  creating structures that are based on merkle trees. the work of  generalized the merkle hash tree ideas to work with any dag  directed acyclic graph  structure. with similar techniques  the work of  uses the merkle tree to authenticate xml documents in the odb model. the work of  further extended the idea and introduced the vb-tree which was suitable for structures stored on secondary storage. however  this approach is expensive and was later subsumed by . several proposals for signature-based approaches addressing both query correctness and completeness appear in  1  1  1 . we are not aware of work that specifically addresses the query freshness issue.
　　hardware support for secure data accesses is investigated in  1  1 . it offers a promising research direction for designing query authentication schemes with special hardware support. lastly  distributed content authentication has been addressed in   where a distributed version of the merkle hash tree is applied.
1. the static case
　　in this section we illustrate three approaches for query correctness and completeness: a signature-based approach similar to the ones described in  1  1   a merkle-tree-like approach based on the ideas presented in   and our novel embedded tree approach. we present them for the static scenario where no data updates occur between the owner and the servers on the outsourced database. we also present analytical cost models for all techniques  given a variety of performance metrics. as already mentioned  detailed analytical modeling was not considered in related literature and is an important contribution of this work. it gives the ability to the owners to decide which structure best satisfies their needs  using a variety of performance measures.
　　in the following  we derive models for the storage  construction  query  and authentication cost of each technique  taking into account the overhead of hashing  signing  verifying data  and performing expensive computations  like modular multiplications of large numbers . the analysis considers range queries on a specific database attribute a indexed by a b+-tree . the size of the structure is important first for quantifying the storage overhead on the servers  and second for possibly quantifying the owner/server communication cost. the construction cost is useful for quantifying the overhead incurred by the database owner for outsourcing the data. the query cost quantifies the incurred server cost for answering client queries  and hence the potential query throughput. the authentication cost quantifies the server/client communication cost and  in addition  the client side incurred cost for verifying the query results. the notation used is summarized in table 1. in the rest  for ease of exposition  it is assumed that all structures are bulk-loaded in a bottom-up fashion and that all index nodes are completely full. in addition  all divisions are assumed to have residual zero. our results can easily be extended to the general case.
1 aggregated signatures with b+-trees
　　the first authenticated data structure for static environments is a direct extension of aggregated signatures and ideas that appeared in  1  1 . to guarantee correctness and completeness the following technique can be used: first  the owner individually hashes and signs all consecutive pairs of tuples in the database  assuming some sorted order on a

figure 1: the signature-based approach.
given attribute a. for example  given two consecutive tuples ri rj the owner transmits to the servers the pair  ri si   where si = s ri|rj   '|' denotes some canonical pairing of strings that can be uniquely parsed back into its two components; e.g.  simple string concatenation if the lengths are fixed . the first and last tuples can be paired with special marker records. chaining tuples in this way will enable the clients to verify that no in-between tuples have been dropped from the results or modified in any way. an example of this scheme is shown in figure 1.
　　in order to speed up query execution on the server side a b+-tree is constructed on top of attribute a. to answer a query the server constructs a vo that contains one pair rq|sq per query result. in addition  one tuple to the left of the lower-bound of the query results and one to the right of the upper-bound is returned  in order for the client to be able to guarantee that no boundary results have been dropped. notice that since our completeness requirements are less stringent than those of   where they assume that database access permissions restrict which tuples the database can expose to the user   for fairness we have simplified the query algorithm substantially here.
　　there are two obvious and serious drawbacks associated with this approach. first  the extremely large vo size that contains a linear number of signatures w.r.t. nr  taking into account that signatures sizes are very large. second  the high verification cost for the clients. authentication requires nr verification operations which  as mentioned earlier  are very expensive. to solve this problem one can use the aggregated signature scheme discussed in section 1. instead of sending one signature per query result the server can send one combined signature sπ for all results  and the client can use an aggregate verification instead of individual verifications.
　　by using aggregated rsa signatures  the client can authenticate the results by hashing consecutive pairs of tuples in the result-set  and calculating the product mπ = q q hq
 mod n   where n is the rsa modulus from the public key of the owner . it is important to notice that both sπ and mπ require a linear number of modular multiplications  w.r.t. nr . the cost models of the aggregated signature scheme for the metrics considered are as follows:
node fanout: the node fanout of the b+-tree structure is:
	.	 1 
storage cost: the total size of the authenticated structure  excluding the database itself  is equal to the size of the b+tree plus the size of the signatures. for a total of nd tuples the height of the tree is equal to da = logfa nd  consisting of  nodes in total. hence:
	.	 1 
　　the storage cost also reflects the initial communication cost between the owner and servers. notice that the owner does not have to upload the b+-tree to the servers  since the latter can rebuild it by themselves  which will reduce the owner/server communication cost but increase the computation cost at the servers. nevertheless  the cost of sending the signatures cannot be avoided.
construction cost: the cost incurred by the owner for constructing the structure has three components: the signature computation cost  bulk-loading the b+-tree  and the i/o cost for storing the structure. since the signing operation is very expensive  it dominates the overall cost. bulkloading the b+-tree in main memory is much less expensive and its cost can be omitted. hence:
	.	 1 
vo construction cost: the cost of constructing the vo for a range query depends on the total disk i/o for traversing the b+-tree and retrieving all necessary record/signature pairs  as well as on the computation cost of sπ. assuming that the total number of leaf pages accessed is   the vo construction cost is:

where the last term is the modular multiplication cost for computing the aggregated signature  which is linear to nr. the i/o overhead for retrieving the signatures is also large.
authentication cost: the size of the vo is equal to the result-set size plus the size of one signature:
a
	|vo| = nr ， |r| + |s|.	 1 
the cost of verifying the query results is dominated by the hash function computations and modular multiplications at the client:
	 	 1 
where the modular multiplication cost for computing the aggregated hash value is linear to the result-set size nr  and the size of the final product has length in the order of |n|  the rsa modulus . the final term is the cost of verifying the product using sπ and the owner's public key.
　　it becomes obvious now that one advantage of the aggregated signature scheme is that it features small vo sizes and hence small client/server communication cost. on the other hand it has the following serious drawbacks: 1. large storage overhead on the servers  dominated by the large signature sizes  1. large communication overhead between the owners and the servers that cannot be reduced  1. a very high initial construction cost  dominated by the cost of computing the signatures  1. added i/o cost for retrieving signatures  linear to nr  1. an added modular multiplication cost  linear to the result-set size  for constructing the vo and authenticating the results. our experimental evaluation shows that this cost is significant compared to other operations  1. the requirement for a public key signature scheme that supports aggregated signatures. for the rest of the paper  this approach is denoted as aggregated signatures with b+-trees  asb-tree .
1 the merkle b-tree
　　motivated by the drawbacks of the asb-tree  we present a different approach for building authenticated structures

figure 1: an mb-tree node.

figure 1: a query traversal on an mb-tree. at every level the hashes of the residual entries on the left and right boundary nodes need to be returned.
that is based on the general ideas of   which utilize the merkle hash tree  applied in our case on a b+-tree structure. we term this structure the merkle b-tree  mb-tree .
　　as already explained in section 1  the merkle hash tree uses a hierarchical hashing scheme in the form of a binary tree to achieve query authentication. clearly  one can use a similar hashing scheme with trees of higher fanout and with different organization algorithms  like the b+-tree  to achieve the same goal. an mb-tree works like a b+-tree and also consists of ordinary b+-tree nodes that are extended with one hash value associated with every pointer entry. the hash values associated with entries on leaf nodes are computed on the database records themselves. the hash values associated with index node entries are computed on the concatenation of the hash values of their children. for example  an mb-tree is illustrated in figure 1. a leaf node entry is associated with a hash value h = h ri   while an index node entry with h = h h1|，，，|hfm   where h1 ... hfm are the hash values of the node's children  assuming fanout fm per node. after computing all hash values  the owner has to sign the hash of the root using the private key.
　　to answer a range query the server builds a vo by initiating two top-down b+-tree like traversals  one to find the left-most and one the right-most query result. at the leaf level  the data contained in the nodes between the two discovered boundary leaves are returned  as in the normal b+-tree. the server also needs to include in the vo the hash values of the entries contained in each index node that is visited by the lower and upper boundary traversals of the tree  except the hashes to the right  left  of the pointers that are traversed during the lower  upper  boundary traversals. at the leaf level  the server inserts only the answers to the query  along with the hash values of the residual entries to the left and to the right parts of the boundary leaves. the result is also increased with one tuple to the left and one to the right of the lower-bound and upper-bound of the query result respectively  for completeness verification. finally  the signed root of the tree is inserted as well. an example query traversal is shown in figure 1.
　　the client can iteratively compute all the hashes of the sub-tree corresponding to the query result  all the way up to the root using the vo. the hashes of the query results are computed first and grouped into their corresponding leaf nodes1  and the process continues iteratively  until all the hashes of the query sub-tree have been computed. after the hash value of the root has been computed  the client can verify the correctness of the computation using the owner's public key and the signed hash of the root. it is easy to see that since the client is forced to recompute the whole query sub-tree  both correctness and completeness is guaranteed. it is interesting to note here that one could avoid building the whole query sub-tree during verification by individually signing all database tuples as well as each node of the b+tree. this approach  called vb-tree  was proposed in  but it is subsumed by the signature based approach. the analytical cost models of the mb-tree are as follows:
node fanout: the node fanout in this case is:
	.	 1 
notice that the maximum node fanout of the mb-trees is considerably smaller than that of the asb-tree  since the nodes here are extended with one hash value per entry. this adversely affects the total height of the mb-tree.
storage cost: the total size is equal to:
	.	 1 
an important advantage of the mb-tree is that the storage cost does not necessarily reflect the owner/server communication cost. the owner  after computing the final signature of the root  does not have to transmit all hash values to the server  but only the database tuples. the server can recompute the hash values incrementally by recreating the mb-tree. since hash computations are cheap  for a small increase in the server's computation cost this technique will reduce the owner/sever communication cost drastically.
construction cost: the construction cost for building an mb-tree depends on the hash function computations and the total i/os. since the tree is bulk-loaded  building the leaf level requires nd hash computations of length input |r|. in addition  for every tree node one hash of input length fm，|h|
                                           d is computed. since there are a total ofnodes on average  given height dm = logfm nd   the total number of hash function computations  and hence the total cost for constructing the tree is given by:

vo construction cost: the vo construction cost is dominated by the total disk i/o. let the total number of leaf pages accessed be equal to and dq = logfm nr be the height of the mb-tree and the query sub-tree respectively. in the general case the index traversal cost is:

taking into account the fact that the query traversal at some point splits into two paths. it is assumed here that the query range spans at least two leaf nodes. the first term corresponds to the hashes inserted for the common path of the two traversal from the root of the tree to the root of

1 extra node boundary information can be inserted in the vo for this purpose with a very small overhead.
the query sub-tree. the second term corresponds to the two traversals after they split. the last two terms correspond to the leaf level of the tree and the database records.
authentication cost: assuming that ρ1 is the total number of query results contained in the left boundary leaf node of the query sub-tree  σ1 on the right boundary leaf node  and ρi σi the total number of entries of the left and right boundary nodes on level i 1 ＋ i ＋ dq  that point towards leaves that contain query results  see figure 1   the size of the vo is:
m
	|vo|	=
 1fm   ρ1   σ1 |h| + nr ， |r| + |s| +
 dm   dq  ，  fm   1 |h| +
dq 1
x  1fm   ρi   σi |h| +
i=1
	 fm   ρdq 1   σdq 1 |h|.	 1 
this cost does not include the extra boundary information needed by the client in order to group hashes correctly  but this overhead is very small  one byte per node in the vo  especially when compared with the hash value size. consequently  the verification cost on the client is:
dq 1
cvm = nr ， ch|r| + x fmi ， chfm|h| +
i=1
	 dm   dq  ， chfm|h| + cv|h|.	 1 
　　given that the computation cost of hashing versus signing is orders of magnitude smaller  the initial construction cost of the mb-tree is expected to be orders of magnitude less expensive than that of the asb-tree. given that the size of hash values is much smaller than that of signatures and that the fanout of the mb-tree will be smaller than that of the asb-tree  it is not easy to quantify the exact difference in the storage cost of these techniques  but it is expected that the structures will have comparable storage cost  with the mb-tree being smaller. the vo construction cost of the mb-tree will be much smaller than that of the asb-tree  since the asb-tree requires many i/os for retrieving signatures  and also some expensive modular multiplications. the mb-tree will have smaller verification cost as well since: 1. hashing operations are orders of magnitude cheaper than modular multiplications  1. the asb-tree requires nr modular multiplications for verification. the only drawback of the mb-tree is the large vo size  which increases the client/server communication cost. notice that the vo size of the mb-tree is bounded by fm ， logfm nd. since generally  the vo size is essentially determined by fm  resulting in large sizes.
1 the embedded merkle b-tree
　　in this section we propose a novel data structure  the embedded merkle b-tree  emb-tree   that provides a nice  adjustable trade-off between robust initial construction and storage cost versus improved vo construction and verification cost. the main idea is to have different fanouts for storage and authentication and yet combine them in the same data structure.
　　every emb-tree node consists of regular b+-tree entries  augmented with an embedded mb-tree. let fe be the fanout of the emb-tree. then each node stores up to fe triplets ki|pi|hi  and an embedded mb-tree with fanout

figure 1: an emb-tree node.
fk   fe. the leaf level of this embedded tree consists of the fe entries of the node. the hash value at the root level of this embedded tree is stored as an hi value in the parent of the node  thus authenticating this node to its parent. essentially  we are collapsing an mb-tree with height de ，dk = logfk nd into a tree with height de that stores smaller mbtrees of height dk within each node. here  de = logfe nd is the height of the emb-tree and dk = logfk fe is the height of each small embedded mb-tree. an example emb-tree node is shown in figure 1.
　　for ease of exposition  in the rest of this discussion it will be assumed that fe is a power of fk such that the embedded trees when bulk-loaded are always full. the technical details if this is not the case can be worked out easily. the exact relation between fe and fk will be discussed shortly. after choosing fk and fe  bulk-loading the emb-tree is straightforward: simply group the nd tuples in groups of size fe to form the leaves and build their embedded trees on the fly. continue iteratively in a bottom up fashion.
　　when querying the structure the server follows a path from the root to the leaves of the external tree as in the normal b+-tree. for every node visited  the algorithm scans all fe  1 triplets ki|pi|hi on the data level of the embedded tree to find the key that needs to be followed to the next level. when the right key is found the server also initiates a point query on the embedded tree of the node using this key. the point query will return all the needed hash values for computing the concatenated hash of the node  exactly like for the mb-tree. essentially  these hash values would be the equivalent of the fe   1 sibling hashes that would be returned per node if the embedded tree was not used. however  since now the hashes are arranged hierarchically in an fk-way tree  the total number of values inserted in the vo per node is reduced to  fk   1 dk.
　　to authenticate the query results the client uses the normal mb-tree authentication algorithm to construct the hash value of the root node of each embedded tree  assuming that proper boundary information has been included in the vo for separating groups of hash values into different nodes  and then follows the same algorithm once more for computing the final hash value of the root of the emb-tree.
　　the emb-tree structure uses extra space for storing the index levels of the embedded trees. hence  by construction it has increased height compared with the mb-tree due to smaller fanout fe. a first  simple optimization for improving the fanout of the emb-tree is to avoid storing the embedded trees altogether. instead  each embedded tree can be instantiated by computing fewer than fe/ fk   1  hashes on the fly  only when a node is accessed during the querying phase. we call this the emb -tree. the emb -tree is logically the same as the emb-tree  however its physical structure is equivalent to an mb-tree with the hash values computed differently. with this optimization the storage overhead is minimized and the height of the emb -tree becomes equal to the height of the equivalent mb-tree. the trade-off is an increased computation cost for constructing the vo. however  this cost is minimal as the number of embedded trees that need to be reconstructed is bounded by the height of the emb -tree.
　　as a second optimization  one can create a slightly more complicated embedded tree to reduce the total size of the index levels and increase fanout fe. we call this the emb tree. essentially  instead of using a b+-tree as the base structure for the embedded trees  one can use a multi-way search tree with fanout fk while keeping the structure of the external emb-tree intact. the embedded tree based on b+trees has a total of  nodes while  for example  a b-tree based embedded tree  recall that a b-tree is equivalent to a balanced multi-way search tree  would contain  nodes instead. a side effect of using multi-way search trees is that the cost for querying the embedded tree on average will decrease  since the search for a particular key might stop before reaching the leaf level. this will reduce the expected cost of vo construction substantially. the technical details associated with embedding the multi-way tree in the emb -tree nodes are included in the full version of the paper .
　　below we give the analytical cost models of the embtree. the cost models for the emb -tree and emb -tree are similar and for brevity their detailed analysis appears in the full version of the paper .
node fanout: for the emb-tree  the relationship between fe and fk is given by:
p −
fklogfk f e 1   1 fk |k| + |p| + |h|    |k|  + fk 1
	 fe |k| + |p| + |h|    |k| .	 1 
first  a suitable fk is chosen such that the requirements for authentication cost and storage overhead are met. then  the maximum value for fe satisfying  1  can be determined.
storage cost: the storage cost is equal to:
	.	 1 
construction cost: the total construction cost is the cost of constructing all the embedded trees plus the i/os to write the tree back to disk. given a total of nodes in the tree and nodes per embedded tree  the cost is:

it should be mentioned here that the cost for constructing the emb -tree is exactly the same  since in order to find the hash values for the index entries of the trees one needs to instantiate all embedded trees. the cost of the emb -tree is somewhat smaller than  1   due to the smaller number of nodes in the embedded trees.
vo construction cost: the vo construction cost is dominated by the total i/o for locating and reading all the nodes containing the query results. similarly to the mb-tree case:

where dq is the height of the query sub-tree and is the number of leaf pages to be accessed. since the embedded trees are loaded with each node  the querying computation cost associated with finding the needed hash values is expected to be dominated by the cost of loading the node in memory  and hence it is omitted. it should be restated here that for the emb -tree the expected vo construction cost will be smaller  since not all embedded tree searches will reach the leaf level of the structure.
authentication cost: the embedded trees work exactly like mb-trees for point queries. hence  each embedded tree returns  fk   1 dk hashes. similarly with the mb-tree the total size of the vo is:
e
|vo| = nr ， |r| + |s|+
	dq 1	dm 1
x 1|vo|	+ |vo|	+ x  fk   1 dk|h|   1  m	m
	1	dq
where |vo|m is the cost of a range query on the embedded trees of the boundary nodes contained in the query subtree given by equation  1   with query range that covers all pointers to children that cover the query result-set. the verification cost is:

where ck = ni ，chfk|h| is the cost for constructing the concatenated hash of each node using the embedded tree.
　　for fk = 1 the authentication cost becomes equal to a merkle hash tree  which has the minimal vo size but higher verification time. for fk − fe the embedded tree consists of only one node which can actually be discarded  hence the authentication cost becomes equal to that of an mb-tree  which has larger vo size but smaller verification cost. notice that  as fk becomes smaller  fe becomes smaller as well. this has an impact on vo construction cost and size  since with smaller fanout the height of the emb-tree increases. nevertheless  since there is only a logarithmic dependence on fe versus a linear dependence on fk  it is expected that with smaller fk the authentication related operations will become faster.
1 general query types
　　the proposed authenticated structures can support other query types as well. due to lack of space we briefly discuss here a possible extension of these techniques for join queries. other query types that can be supported are projections and multi-attribute queries.
　　assume that we would like to provide authenticated results for join queries such as   where ai （ r and aj （ s  r and s could be relations or result-sets from other queries   and authenticated structures for both ai in r and aj in s exist. the server can provide the proof for the join as follows: 1. select the relation with the smaller size  say r  1. construct the v o for r  if r is an entire relation then vo contains only the signature of the root node from the index of r   1. construct the vos for each of the following selection queries: for each record rk in r  qk = select * from s where r.aj = rk.ai . the client can easily verify the join results. first  it authenticates that the relation r is complete and correct. then  using the vo for each query qk  it makes sure that it is complete for every k  even when the result of qk is empty .
after this verification  the client can construct the results for the join query and be sure that they are complete and correct.
1. the dynamic case
　　surprisingly  previous work has not dealt with dynamic scenarios where data gets updated at regular time intervals. in this section we analyze the performance of all approaches given dynamic updates between the owner and the servers. in particular we assume that either insertions or deletions can occur to the database  for simplicity. the performance of updates in the worst case can be considered as the cost of a deletion followed by an insertion. there are two contributing factors for the update cost: computation cost such as creating new signatures and computing hashes  and i/o cost.
1 aggregated signatures with b+-trees
　　suppose that a single database record ri is inserted in or deleted from the database. assuming that in the sorted order of attribute a the left neighbor of ri is ri 1 and the right neighbor is ri+1  for an insertion the owner has to compute signatures  and s ri|ri+1   and for a deletion s ri 1|ri+1 . for consecutive updates in the best case a total of k +1 signature computations are required for insertions and 1 for deletions if the deleted tuples are consecutive. in the worst case a total of 1k signature computations are needed for insertions and k for deletions  if no two tuples are consecutive. given k updates  suppose the expected number of signatures to be computed is represented by e{k}. then the additional i/o incurred is equal to   excluding the i/os incurred for updating the b+-tree structure. since the cost of signature computations is larger than even the i/o cost of random disk accesses  a large number of updates is expected to have a very expensive updating cost. the experimental evaluation verifies this claim. the total update cost for the asb-tree is:
	.	 1 
1 the merkle b-tree
　　the mb-tree can support efficient updates since only hash values are stored for the records in the tree and  first  hashing is orders of magnitude faster then signing  second  for each tuple only the path from the affected leaf to the root need to be updated. hence  the cost of updating a single record is dominated by the cost of i/os. assuming that no reorganization to the tree occurs the cost of an insertion is m
cu = h|r| + dm hfm|h| + cio  + s|h|.
　　in realistic scenarios though one expects that a large number of updates will occur at the same time. in other cases the owner may decide to do a delayed batch processing of updates as soon as enough changes to the database have occurred. the naive approach for handling batch updates would be to do all updates to the mb-tree one by one and update the path from the leaves to the root once per update. nevertheless  in case that a large number of updates affect a similar set of nodes  e.g.  the same leaf  a per tuple updating policy performs an unnecessary number of hash function computations on the predecessor path. in such cases  the computation cost can be reduced significantly by recomputing the hashes of all affected nodes only once  after all the updates have been performed on the tree. a similar analysis holds for the incurred i/o as well.
   clearly  the total update cost for the per tuple update approach for k insertions is k ， cum which is linear to the number of affected nodes k ， dm. the expected cost of k updates using batch processing can be computed as follows. given k updates to the mb-tree  assuming that all tuples are updated uniformly at random and using a standard balls and bins argument  the probability that leaf node x has been affected at least once is and the expected number of leaf nodes that have been affected is fmdm 1 ， p x . using the same argument  the expected number of nodes at level i  where i = 1 is the leaf level and 1i ＋ dm  is fmdm i ， pi x   where pi x  =  1    1  
k
 . hence  for a batch of k updates the total expected number of nodes that will be affected is:
	.	 1 
hence  the expected mb-tree update cost for batch updates is
m
	cu = k ， h|r| + e{x} ，  hfm|h| + cio  + s|h|.	 1 
　　in order to understand better the relationship between the per-update approach and the batch-update  we can find the closed form for e{x} as follows:
pid=1m 1 fmi  1    fmifmi 1 k 
=
=
=
=	=	kdm   px=1 `x＞  1 	1  fm1  x 1
the second term quantifies the cost decrease afforded by the batch update operation  when compared to the per update cost.
　　for non-uniform updates to the database  the batch updating technique is expected to work well in practice given that in real settings updates exhibit a certain degree of locality. in such cases one can still derive a similar cost analysis by modeling the distribution of updates.
1 the embedded mb-tree
　　the analysis for the emb-tree is similar to the one for mb-trees. the update cost for per tuple updates is equal to k，cue  where cue = h|r|+de logfk fe， hfk|h|+cio +s|h|  once again assuming that no reorganizations to the tree occur. similarly to the mb-tree case the expected cost for batch updates is equal to:
e
cu = k ，h|r| +e{x}，logfk fe ， hfk|h| +cio  +s|h|.  1 
1 discussion
　　for the asb-tree  the communication cost for updates between owner and servers is bounded by e{k}|s|  and there is no possible way to reduce this cost as only the owner can compute signatures. however  for the hash based index structures  there are a number of options that can be used for transmitting the updates to the server. the first option is for the owner to transmit only a delta table with the updated nodes of the mb-tree  or emb-tree  plus the signed root. the second option is to transmit only the signed root
　　
and the updates themselves and let the servers redo the necessary computations on the tree. the first approach minimizes the computation cost on the servers but increases the communication cost  while the second approach has the opposite effect.
1. query freshness
　　the dynamic scenario reveals a third dimension of the query authentication problem that has not been sufficiently explored in previous work: namely  query result freshness. when the owner updates the database  a malicious or compromised server may still retain an older version. because the old version was authenticated by the owner  the client will still accept it as authentic  unless it receives some extra information to indicate that this is no longer the case. in fact  a malicious server may choose any of the previous versions  and in some scenarios even a combination of them. if the client wishes to be sure that the version is not only authentic  but is also the most recent version available  some additional work is necessary.
　　this issue is similar to the problem of ensuring the freshness of signed documents  which has been studied extensively  particularly in the context of certificate validation and revocation. there are many solutions which we do not review here. the simplest is to publish a list of revoked signatures; more sophisticated ones are: including the time interval of validity as part of the signed message and reissuing the signature after the interval expires  and using hash chains to confirm validity of signatures at frequent intervals .
　　the advantage of all merkle tree based solutions presented here is that any of these approaches can be applied directly to the single signature of the root of the tree  because this signature alone authenticates the whole database. thus  whatever solution to the signature freshness problem one uses  it needs to be used only for one signature and freshness will be assured. each update will require re-issuing one signature only.
　　this is in contrast to the asb-tree approach  which uses multiple signatures. it is unclear how to solve the freshness problem for the asb-tree without applying the freshness techniques to each signature individually  which will be prohibitively expensive because of the sheer number of signatures.
1. experimental evaluation
　　for our experimental evaluation we have implemented the aggregated signatures technique using a b+-tree  asbtree   the mb-tree  the emb-tree and its two variants  emb tree and emb -tree. to the best of our knowledge  this work is the first that performs simulations on a working prototype. previous work has used only analytical techniques that did not take into account important parameters. a well designed experimental evaluation can reveal many interesting issues that are hidden when only theoretical aspects are considered. in addition  empirical evaluations help verify the correctness of the developed cost models. the prototype can be downloaded from .
1 setup
　　we use a synthetic database that consists of one table with 1 tuples. each tuple contains multiple attributes  a primary key a  and is 1 bytes long. for simplicity  we assume that an authenticated index is build on a 

	 a  fanout	 b  height
figure 1: index parameters.
with page size equal to 1 kbyte. all experiments are performed on a linux machine with a 1ghz intel pentium 1 cpu.
　　the cost cio of one i/o operation on this machine using 1kbyte pages for the indexes is on average 1 ms for a sequential read and 1 ms for random access. the costs ch of hashing a message with length 1 bytes is approximately equal to 1 μs. in comparison  the cost cs of signing a message with any length is approximately equal to 1 ms. the cost of one modular multiplication with 1 byte modulus is close to 1 μs. to quantify these costs we used the publicly available crypto++  and openssl  libraries.
1 performance analysis
　　we run experiments to evaluate the proposed solutions under all metrics. first  we test the initial construction cost of each technique. then  we measure their query and verification performance. finally  we run simulations to analyze their performance for dynamic scenarios. for various embedded index structures  the fanout of their embedded trees is set to 1 by default  except if otherwise specified.
1.1 construction cost
　　first we evaluate the physical characteristics of each structure  namely the maximum and average fanout  and the height. the results are shown in figure 1. as expected  the asb-tree has the maximum fanout and hence the smallest height  while the opposite is true for the emb-tree. however  the maximum height difference  which is an important measure for the number of additional i/os per query when using deeper structures  is only 1. of course this depends on the database size. in general  the logarithmic relation between the fanout and the database size limits the difference in height of the different indices.
　　next  we measure the construction cost and the total size of each structure  which are useful indicators for the owner/server computation overhead  communication cost and storage demands. figure 1 a  clearly shows the overhead imposed on the asb-tree by the excessive signature computations. notice on the enclosed detailed graph that the overhead of other authenticated index structures in the worst case is twice as large as the cost of building the b+-tree of the asb-tree approach. figure 1 b  captures the total size of each structure. undoubtedly  the asb-tree has the biggest storage overhead. the emb-tree is storage demanding as well since the addition of the embedded trees decreases the index fanout substantially. the mb-tree has the least storage overhead and the emb -tree is a good compromise between the mb-tree and the emb-tree for this metric. notice that the proposed cost models capture very accurately the tree sizes.

	 a  time	 b  storage
figure 1: index construction cost.

	 a  hash computations	 b  time
figure 1: hash computation overhead.
　　the client/server communication cost using the simplest possible strategy is directly related to the size of the authenticated structures. it should be stressed however that for the hash based approaches this cost can be reduced significantly by rebuilding the trees at the server side. in contrast  the asb-tree is not equally flexible since all signatures have to be computed at the owner.
　　the construction cost of our new authenticated index structures has two components. the i/o cost for building the trees and the computational cost for computing hash values. figure 1 shows the total number of hash computations executed per structure  and the total time required. evidently  the emb-tree approaches increase the number of hashes that need to be computed. however  the additional computation time increases by a small margin as hashing is cheap  especially when compared with the total construction overhead  see figure 1 . thus  the dominating cost factor proves to be the i/o operations of the index.
1.1 query and verification cost
　　in this section we study the performance of the structures for range queries. we generate a number of synthetic query workloads with range queries of given selectivity. each workload contains 1 range queries generated uniformly at random over the domain of a. results reflect the average case  where the cost associated with accessing the actual records in the database has not been included. a 1 page lru buffer is used  unless otherwise specified. in the rest of the experiments we do not include the cost model analysis not to clutter the graphs.
　　the results are summarized in figure 1. there are two contributing cost factors associated with answering range queries. the first is the total number of i/os. the second is the computation cost for constructing the vo. the number of i/os can be further divided into query specific i/os  i.e.  index traversal i/os for answering the range query  and vo construction related i/os.
　　figure 1 a  shows the query specific i/os as a function of selectivity. straightforwardly  the number of page

	lru buffer.	out lru buffer.
figure 1: the effect of the lru buffer.
access is directly related to the fanout of each tree. notice that the majority of page access is sequential i/o at the leaf level of the trees. figure 1 b  shows the additional i/o needed by each structure for completing the vo. evidently  the asb-tree has to perform a very large number of sequential i/os for retrieving the signatures of the results. our authenticated index structures need to do only a few  upper bounded by the height of the index  extra random accesses for traversing the path that leads to the upper boundary tuple of the query result. figure 1 c  shows the total i/o incurred by the structures. it is clear that the asb-tree has the worst performance overall  even though its query specific performance is the best.
　　figure 1 d  shows the runtime cost of additional computations that need to be performed for modular multiplications and hashing operations. the asb-tree has an added multiplication cost for producing the aggregated signature. this cost is linear to the query result-set size and cannot be omitted when compared with the i/o cost. this observation is instructive since it shows that one cannot evaluate analytically or experimentally authenticated structures correctly only by examining i/o performance. due to expensive cryptographic computations  i/o operations are not always a dominating factor. the emb -tree has a minor computation overhead  depending only on the fanout of the conceptual embedded tree. the rest of the structures have no computation overhead at all.
　　interesting conclusions can be drawn by evaluating the effects of the main memory lru buffer. figure 1 shows the total query runtime of all structures with and without the lru buffer. we can deduce that the lru buffer reduces the query cost substantially for all techniques. we expect that when a buffer is available the computation cost is the dominant factor in query runtime  and the asb-tree obviously has much worse performance  while without the buffer the i/o cost should prevail. however  since overall the asb-tree has the largest i/o cost  the hash based structures still have better query performance.
　　finally  we measure the vo size and verification cost at the client side. the results are shown in figure 1. the asb-tree  as a result of using aggregated signatures always returns only one signature independent of the resultset size. the mb-tree has to return fm logfm nd number of hashes plus one signature. as the fanout is the dominating factor  and since the mb-tree has a relatively large fanout  the vo size is large. the emb-tree and its variants  logically work as an mb-tree with fanout fk and hence their vo sizes are significantly reduced  since
. notice that the emb -tree has the smallest vo among all embedded index structures  as the searches in its embedded multi-way search trees can stop at any level of the tree  reducing the total number of hashes.


	cific i/os.	tion cost.
figure 1: performance analysis for range queries.	 a  vo size.	 b  verification time.
figure 1: authentication cost.
　　the verification cost for the asb-tree is linear to the size of the query result-set due to the modular multiplication operations  resulting in the worst performance. for the other approaches the total cost is determined by the total hashes that need to be computed. interestingly  even though the mb-tree has the largest vo size  it has the fastest verification time. the reason is that for verification the number of hash computations is dominated by the height of the index  and the mb-tree has much smaller height compared to the other structures.
1.1 update cost
　　there are two types of update operations  insertions and deletions. to expose the behavior of each update type  we perform experiments on update workloads that contain either insertions or deletions. each update workload contains 1 batch update operations  where each batch operation consists of a number of insertions or deletions  ranging from a ratio σ = 1% to 1% of the total database size before the update occurs. each workload contains batch operations of equal ratio. we average the results on a per batch update operation basis. two distributions of update operations are tested. ones that are generated uniformly at random  and ones that exhibit a certain degree of locality. due to lack of space we present here results only for uniform insertions. deletions worked similarly. skewed distributions exhibit a somewhat improved performance and have similar effects on all approaches. finally  results shown here include only the cost of updating the structures and not the cost associated with updating the database.
　　figure 1 summarizes the results for insertion operations. the asb-tree requires computing between σnd + 1 and 1σnd signatures. essentially  every newly inserted tuple requires two signature computations  unless if two new tuples are consecutive in order in which case one computation can be avoided. since the update operations are uniformly distributed  only a few such pairs are expected on average. figure 1 a  verifies this claim. the rest of the structures require only one signature re-computation.
　　the total number of pages affected is shown in figure 1 b . the asb-tree needs to update both the pages containing the affected signatures and the b+-tree structure. clearly  the signature updates dominate the cost as they are linear to the number of update operations. other structures need to update only the nodes of the index. trees with smaller fanout result in larger number of affected pages. even though the emb -tree and mb-tree have smaller fanout than the asb-tree  they produce much smaller number of affected pages. the emb-tree and emb -tree produce the largest number of affected pages. part of the reason is because in our experiments all indexes are bulk-loaded with 1% utilization and the update workloads contain only insertions. this will quickly lead to many split operations  especially for indexes with small fanout  which creates a lot of new pages.
　　another contributing factor to the update cost is the computation overhead. as we can see from figure 1 c  the asb-tree obviously has the worst performance and its cost is order of magnitudes larger than all other indexes  as it has to perform linear number of signature computations  w.r.t the number of update operations . for other indexes  the computation cost is mainly due to the cost of hashing operations and index maintenance. finally  as figure 1 d  shows  the total update cost is simply the page i/o cost plus the computation cost. our proposed structures are the clear winners. finally the communication cost incurred by update operations is equal to the number of pages affected.
1.1 discussion
　　the experimental results clearly show that the authenticated structures proposed in this paper perform better than the state-of-the-art with respect to all metrics except the vo size. still  our optimizations reduced the size to four times the size of the vo of the asb-tree. overall  the emb -tree gives the best trade-off between all performance metrics  and it should be the preferred technique in the general case. by adjusting the fanout of the embedded trees  we obtain a nice trade-off between query  v o  size  verification time  construction  update  time and storage overhead.
1. conclusion

re-computations.	fected.
figure 1: performance analysis for insertions.　　we presented a comprehensive evaluation of authenticated index structures based on a variety of cost metrics and taking into account the cost of cryptographic operations  as well as that of index maintenance. we proposed a novel structure that leverages good performance based on all metrics. we extended the work to dynamic environments  which has not been explored in the past. we also formulated the problem of query freshness  a direct outcome of the dynamic case. finally  we presented a comprehensive experimental evaluation to verify our claims. for future work  we plan to extend our ideas for multidimensional structures  and explore more involved types of queries.
1. acknowledgments
　　this work was partially supported by nsf grants iis1  ccr-1 and ccf-1. the authors would like to thank the anonymous reviewers for their constructive comments.
