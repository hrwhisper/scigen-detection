oracle's objective in trec-1 was to study the behavior of oracle information retrieval in previously unexplored application areas. the software used was oracle1i text  oracle's full-text retrieval engine integrated with the oracle relational database management system  and the oracle pl/sql procedural programming language. runs were submitted in filtering and q/a tracks. for the filtering track we submitted three runs  in adaptive filtering  batch filtering and routing. by comparing the trec results  we found that the concepts  themes  extracted by oracle text can be used to aggregate document information content to simplify statistical processing. oracle's q/a system integrated information retrieval  ir  and information extraction  ie . the q/a system relied on a combination of document and sentence ranking in ir  named entity tagging in ie and shallow parsing based classification of  questions into pre-defined categories. 
1. filtering based on theme signature
as a first time filtering track participant  oracle submitted runs for adaptive filtering  batch filtering and routing this year. only linear-utility optimized runs were submitted for adaptive filtering and batch filtering. the filtering system is built based on the oracle 1i database with pl/sql - an oracle supported database access language. since the routing sub-task outputs the top 1 ranked documents per category  and the training process and similarity score calculation algorithm are the same for batch filtering and routing  we will focus our discussion on batch filtering and adaptive filtering. 
the filtering system can be divided into three parts based on functionality:
a. theme vector generation
b. training
c. classification
theme vector generation
theme vector generation generates a theme vector for each document. it is built-in functionality of  oracle text  the information retrieval component of the oracle database. a theme vector containing a list of themes  concepts  and associated weights carries all information of a document used in classification. themes are normalized words having meanings individually and extracted based on the oracle text knowledge base. the knowledge base is built in-house and contains about 1 thousand concepts  classified into 1 major categories. these categories are organized hierarchically under six top terms: business and economics  science and technology  geography  government and military  social environment  and abstract ideas and concepts. this knowledge base is built to support concept search and retrieval. for this trec work  the context knowledge base was employed in our filtering system to preprocess documents and generate concept terms. although the oracle text user extensible knowledge base functionality allows users to modify the built-in knowledge base using user specified thesaurus  we used the knowledge base without any modification. we believe augmenting the knowledge base using domain specific information could improve filtering performance. in the theme generation  known phrases are recognized using a greedy algorithm  unknown words and proper name phrases are recognized and treated as themes. words and phrases are normalized to their canonical forms. every normalized term is a potential theme for a document.  
theme weights are used to rank the semantic significance of themes to the aggregate document content. themes are assigned initial weights based on their lexical flags in the knowledge base. next  several factors derived from the structure of a document and the frequency of the theme in the document are employed to modify the initial weights of the themes. for example  the first few terms inside a sentence have higher weights than the terms at the end of sentences to account for  fronting  and sentence focus.  
generated theme vectors are normalized to have unity length before being sent to the training or classification process. this normalization can be written as : 
wj
wjn = ----------------
1
¡Æwj
where wjn and wj are the j-th component  j-th theme term  weight of theme vector w after and before unity normalization respectively.
our prior experience demonstrates that themes are superior to text tokens in representing text documents of medium to large size for classification purposes. oracle text first tokenizes documents and then processes these tokens using a greedy maximal match algorithm to generate themes. a brief  description of the process to generate themes from tokens may shed some lights on the reason why themes are superior to tokens in classification. after  finding a token  oracle text gets the part of speech information from the knowledge base or finds phrases based on the greedy algorithm and lexical knowledge base. if the token is a noun  a canonical form is used as a normalized form for this token  such as  tastefulness  with canonical form of  tasting  and  dull-headedness   with canonical form of  stupidity . if the token is a non-noun  a base form is found based on the knowledge base or morphology if the token does not exist in knowledge base. after that  a normalized noun form is used as the theme form for the non-noun base form. for example   steadied  has a base form of  steady  which corresponds to a normalized form of  steadiness . the following differences between themes and tokens may contribute to the different behaviors in classification:  
1. themes can handle phrases while tokens can not without a lexicon. 
1. themes are represented with normalized forms of concepts  while tokens are forms with orwithout stemming. word normalization is mostly based on lexical knowledge  while stemming of a token is mostly based on morphology. 
1. the weight of  a theme expresses the lexical information of a term  locations in a document and term frequency. the weight of a token typically only includes the information of term frequency. 
for the classification task no parent themes  broader terms  were used. whether or not the parent themes improve the learning quality is actually an open question. one side says a specific word should be more important for representing a document and a parent theme may act as a common word. on the other hand  one of the parent themes may tell exactly what a document is about. however  that might depend on the level of parent theme and depend on whether or not the hierarchy of the knowledge base represents the same knowledge hierarchy in the classification application. we intend to investigate this issue thoroughly in the future.
training 
the training process calculates the summation of all relevant theme vectors for each category. the  summation result serves as the original theme vector for one category.  because of accumulation  the number of themes in the category theme vector can be large. experiments show that reducing some common themes and less frequent themes for the category theme vector can improve  classification accuracy. theme reduction can also reduce the resource usage and improve classification performance. we adopt a two-step theme reduction. the first step is to choose the top 1 themes with highest theme weights in the category theme vector. as mentioned earlier  the theme weight obtained from oracle text combines information about the lexical significance  word position inside one sentence  and occurrence frequency inside the document. those top 1 themes in the category theme vector are the most frequently occurring and significant words to the category.  another rationale for choosing the theme by weights is that words with little meaning have lower weights and therefore can be removed. 
the first step of theme selection based on the theme weight may choose some themes which are common in lot of categories. these common themes are not specific to one category and may produce extra noise to the classification process. the second step of theme reduction is to choose themes which are more specific to one category. we use a chi-square test for theme selection . in specific  we choose a theme if the null hypothesis that this theme is independent of the considered category can be proved not true.  the themes will be chosen if:
         
	n nr 	t - ntr 1
------------------------------------------------   1
rnt n - r  n - nt 
where n is the total number of training documents
          r is the number of training documents in this category           nt is the number of training documents containing this word           rt is the number of training documents in this category and containing this word.           value 1 is chosen because the confidence of chi-square test is 1. 
by chi-square test  the average theme vector size can be reduced to 1. in the original category theme vector  the weight is the summation of each document's theme weights; those weights help us to choose the top 1 themes for the category. however  during the classification process  we use robertson-sparck jones weights  as term weights in category theme vectors. the weights are calculated based on the statistical characteristics of the training set and relevant category:  
      
 rt + 1  n - r - nt + rt + 1 
log--------------------------------------------------------------------------
 nt - rt + 1  r - rt + 1 
this formula is obtained from the bayesian statistical model. the robertson-sparck jones weight is the component weight for one term to estimate the log-odds of an given document belonging to the considered category in the assumption that terms are independent .  
classification
before classification  category theme vectors are normalized to have unity length. in classification   the similarity scores s between the incoming document and each category are calculated as a dot product between the document theme vector vd and category theme vector vc  that is s = vd.vc. the document is classified to the categories in which the similarity scores are larger than the corresponding category thresholds. the predefined thresholds are determined from the relevance information either from the training set in batch filtering or from feedback in adaptive filtering. 
threshold determination
batch filtering 
each category has its own threshold to determine if a document can be classified to it based on the similarity score. in order to determine the threshold for one category  we use the classification module to calculate the similarity score between all training documents and the considered category.  for any given threshold x  we can get the following contingency table as we know the actual categories of each training document. 
	                        relevant	                    not relevant
retrieved	                          r+	                           n+
not retrieved                              r-                                          n-
we can define a utility  goal  function of the about 1 numbers  say f r+ n+ r- n- x . x appears explicitly in the function because r+ r- n+ and n- are all functions of the threshold x. the threshold is chosen to maximize the function f. 
                 threshold = x: max f r+ n+ r- n- x 
                                  x
in trec-1  we submit the batch filtering run based on optimization function of linear-utility  which is f r+ n+ =t1u=1r+  -  n+.  
in implementation  one can generate a sorted array of training documents ordered by similarity scores to the given category with a decreasing sequence.  the relevance information of documents in the sorted array before any given document can determine r+  n+ at the threshold value equal to the similarity score of this document.  for each document in the sorted array  one then can calculate the t1u function value at the threshold value equal to the similarity score of this document based on calculated r+  n+. because the array is sorted such that the similarity scores are decreasing  one therefore can draw a curve of t1u vs threshold. as threshold decreases from the largest value  the t1u values first increase because more relevant documents are located at the positions having larger similarity scores  and decrease after reaching a peak. the peak position corresponds to a similarity score   whose value is the optimized threshold value to maximize t1u function. this calculation makes the assumption that the training set similarity score distribution and t1u quantity is similar to that of the test set.  
adaptive training
in adaptive filtering  we first built initial category theme vectors from training process of an initial training set  which contains two relevant documents per category. the training process is the same as we discussed above. the initial category threshold is set to be 1% of the minimum similarity score of the two relevant documents with the considered category. we then classify the test documents in a batch mode with each batch containing 1 documents coming from the test set stream. after classification of each batch  feedback information including the relevance judgments and the similarity scores is sent to adaptive training  see fig.1. 
adaptive training includes updating category theme vectors and category thresholds. in order to update the category theme vector  we have to maintain the original category theme vectors which are the theme vectors before any theme selections and has the theme weights from summation of oracle text theme weights. to keep the number of themes in the category theme vector from becoming too large  we limit the size of each original category theme vector to a maximum of 1. the extra feedback training document theme vectors are added to the original category theme vectors using widrow-hoff algorithm . 
n
wj = wj - 1z w   xi - yi xi j
where wj  wnj  are the weights for j-th component of the category theme vector before and after adaptive training  respectively. xi  is the theme vector of i-th feedback document  yi the relevance judgment of the i-th feedback document with the considered category with yi =1 denoting not relevant  yi=1 denoting relevant. w.xi denotes the dot product between the theme vector w and xi. z 1 is learning rate and is set to 1. 
the widrow-hoff algorithm generates a list of updated themes and weights. we maintain only the top 1 highest weight themes for each category. the weights here are calculated quantities from oracle text theme weights.  we apply theme selections and employ robertson-sparck jones weights as category theme vector weights for classification as discussed in the above training section. 

figure 1. adaptive filtering diagram 
thresholds can be calculated based on the relevance information and similarity scores of all previous feedback documents in the way we discussed in the threshold determination section. however  that calculation may take unacceptably long time. instead we adopt a simple method to adjust the existing thresholds only based solely on current feedback information. 
thresholds can be adjusted by calculating the optimal threshold for the extra feedback training set as discussed in threshold determination section. we denote the optimal threshold as optimal threshold extra training   then the updated threshold is :
      updated threshold = old threshold + c  optimal threshold extra training - old threshold 
where c is a learning parameter and is set to 1. we note that the feedback batch size and the learning parameter c are relevant parameters  if the feedback batch size is small  the optimal threshold for the extra feedback documents may vary a lot  one then choose a smaller c. c has to be chosen such that the updated thresholds change with the feedback process in a systematic and stable way. 
submission result and discussions
oracle submitted three runs. they are listed in the table 1  and table 1  with adaptive  batch runs in table 1 and routing in table1  respectively.  the numbers in the parenthesis are the median value of all participants. the median values are the  n/1 -th value in sorted decreaseing list if the number of participants n is even. except the precision for batch filter  all numbers in our submitted runs are above median. 
we note that the routing behaves better than batch filtering. the fact that batch filtering system has only one more component: thresholding  than routing implies that our threshold determination is not quite good for batch filtering. in batch filtering  the threshold can not be adjusted. once a threshold is determined  it is used to classify the whole test set without any adjustment. so the initial threshold determination is critical. however  it is interesting to note that the same simple method of determining threshold behaves quite well in adaptive filtering when comparing our adaptive filtering result with others.  
our training  classifying  and thresholding methods are all well-known methods  but our system behaves better than medians  especially in adaptive filtering. one explanation for this might be the linguistic suite in oracle text and knowledge base we used to process documents. the theme vector we get from oracle text contains more information than just text token and occurrence frequency in the document. theme vector have a list of normalized terms. this term normalization could reduce the size of collection thesaurus  and make it easier to match different terms with the same concept. the weight of the theme contains not only the occurrence frequency information  but lexical information. in conclusion  the combination of these linguistic functionalities and appropriately engineering some well-known learning methods are believed to make our system successful. 
table 1: adaptive and batch filtering result with t1u optimization. the numbers in the parathesis are the median value for all participants. 
run labelrun typeoptimization precision  median recall  median t1su 
 median f-beta  median oraau1adaptivet1u1 
 1 1 
 1 1 
 1 1 
 1 orabu1batcht1u1 
 1 1 
 1 1 
 1 1 
 1 table 1: routing result. the number in the parathesis is the median value for all participants. 
run label run typemean average precision 
 median oraro1routing1 
 1 1. question answering based on  information retrieval and information extraction
questions can be classified into pre-defined categories. typical categories are: person names  organization names  dates  locations  cities  countries  states  provinces  continents   numbers  times  meaning of acronyms and abbreviations  weights  lengths  temperatures  speed  manner  duration  products  reasons etc.
information extraction  ie  techniques allow us to extract lists of semantic categories from text automatically  such as person names  organization names  dates  locations  duration  etc.  which are subsets of the whole pre-defined question categories. if a question category is covered by ie  finding the locations of answer candidates becomes easier: the task remains is to rank the list of answer candidates extracted by ie. otherwise  a number of heuristics are employed to locate the answer candidates and rank them.
overview of oracle q/a system:
our q/a system consists of three major components shown in figure1:  1  question processor  1  sentence ranking  1  answer extraction.
question processor:
its role is to:  a  classify a question into a list of pre-defined semantic categories  b  extract content words from a question and send them to oracle to retrieve relevant documents.
to classify a question  the first step is to determine its question type. the following wh-words are used to determine the question types: who  why  where  whom  what  when  how much money  how much  how many  how  rich  long  big  tall  hot  far  fast  large  old  wide  etc. .
a list of heuristics will help to map the question types to the pre-defined semantic categories:
 1  who is  was   person name  =  occupation 
 1  other  who  types =  personal name 
 1  how rich  how much money  how much + vbd vbp  vbz  md  =  money expression 
 1  other  how much  types =  number 
 1  how hot  cold  =  temperature 
 1  how fast =  speed 
 1  how old =  age 
 1  how long =  period of time or length 
 1  how big =  length or square-measure or cubic-measure 
 1  how tall  wide  far  =  length
                                         

                           figure 1: architecture of the oracle q/a system
a complicated problem is to map the question type  what  to its semantic category. here  a part-of-speech  pos  tagger is used to assign the most appropriate part-of-speech for each word in a question based on the contextual information . the head noun of the first noun phrase in a question is used to decide its semantic category. for example   what costume designer decided that michael jackson should only wear one glove   the head noun of the first noun phrase is  designer . using wordnet's lexicon   one finds that  designer  is a person  so  the semantic category of this question is  person name . if the head noun of the first noun phrase in a question is a stop word  then  the head noun of the second noun phrase is used to decide the semantic category. for example   what was the name of the first russian astronaut to do a spacewalk   the head noun of the first noun phrase is  name   a stop word   so  the head noun of the second noun phrase  astronaut  is used to decide the semantic category. similarly  wordnet's api  can tell that its semantic category is  person name .
when extracting a list of keywords from a question  our principle is to extract all content words  but ignore all non-content words. the distinction between these two types of words is that content words should appear in the relevant documents  but non-content words should not appear in the relevant documents. at lease  stop words and stop phrase  such as: how much  what time  what country  belong to non-content words. furthermore  a list of heuristics is helpful to distinguish content words from non-content words. for example   what is the length of coastline of the state of alaska    and  what is the illinois state flower   word  state  is a non-content word in the first question  but a content word in the second question. removing non-content words as many as possible makes retrieved documents more focusing on the subject topic of the question and is very helpful for extracting right answers from retrieved documents.
sentence ranking:
after the query processor extracts a number of content words from a question  two queries are formulated: one uses proximity operator  near  with maximum span size 1 to connect these words  the other uses  accum  operator to connect them. near opearator find all query terms within specified span. documents are ranked based on the frequencies and proximity of query terms in the document. accum  accumulate  operator finds documents matching one or more query terms. documents are ranked based on the sum of weights of the terms matched and frequency of the terms in the document. the first query has higher priority than the second one  because  near  operator always retrieves more relevant documents  but usually  the number of documents retrieved by  near  is not big enough  so   accum  query is used to supplement it. oracle text retrieves a list of relevant documents  1 documents in trec1  based on the two queries. then  the relevant documents are broken into paragraphs  the paragraphs are segmented into sentences. according to our experiments  it is suitable to extract long answers  1 bytes  from ranked paragraphs  but to extract short answers  1 bytes   the paragraphs must be further segmented into sentences. 
ranking the segmented sentences is based on the following information:  1  the number of unique content words in a sentence  1  tf and idf of each content word  1  total number of content words in a query  1  the smallest window size which contains all the unique content words in the sentence.
our information extractor  ie  has two modules: one used for sentence filtering  the other used for answer extraction  ie-based answer extractor . if the semantic category of a question is covered by the ie  the ie is used for sentence filtering. only selected sentences which satisfy the ie  are the candidates of the sentence ranking. for example  if the semantic category of a question is  person name   only the sentences which include at least one person name will participate the sentence ranking  all the rest of sentences are filtered out from answer extraction  because they do not include answers of the question. the ie was also integrated with sentence segmentation algorithm. the standard sentence delimiters are   !.   followed by one or more spaces  then followed by a word whose first letter is a capital letter. there are many exceptional cases  such as mr. steve  st. louis. the ie could recognize these exceptional cases  and guarantee the success of the sentence segmentation.
answer extraction:
after the sentences are ranked  top five of them are used to extract the answers. from previous description  our ie only covers a subset of the whole semantic categories. if the answer type of a question belongs to the subset  it is easy to extract answers using the ie. otherwise  we concluded a number of heuristics  which help to extract answers. the sentence ranking algorithm can find the smallest window in a sentence  which contains all the content words in the sentence. this window divides the sentence into three parts:  1  the words in front of the window   1  the words after the window and  1  the words inside of the window. according to our observation  the priorities of the three parts are  1   1   1 . we further observed that in  1  and  1   the words closer to the windows have higher priority than others. based on these observations  we picked up certain percent of words from each part of the sentence according to their priorities to form the final answers.
other linguistic processing:
 1  acronyms and abbreviations: like other advanced search engines  our system also does limitedautomatic query expansion  mainly for queries with acronyms  abbreviations  etc. it expanded  a  acronyms of geographical terms  such as  u.s. = united states    n.c. =  north carolina   b  abbreviations of organization names  such as  ymca = young mens christian association    nbs = national bureau of standards 
 1  stemming: oracle's search engine does not use porter's stemmer. our stemmer is more conservative which obtains good precision  may hurt recall a little bit. to remedy this problem  extra stemming was added in rare situations. for example   when did hawaii become a state    the main verb was stemmed as  $become .
 1  information extractor  ie : an information extractor was created over the last few months to recognize a  person names  b  organization names  c  dates  d  number  e  locations  f  money expression  g  time  h  temperature  i  speed  j  weight  k  length  l  square measure  m  cubic measure  n  age  etc. 
performance evaluation:
a question answering system was created based on information retrieval and information extraction. our study shows that traditional ir technique are not only useful to rank documents  but also to rank paragraphs and sentences. finding the smallest window from a sentence which contains all the content words in it  is very helpful to extract answers when its semantic category is not covered by the ie  the window size is also an important factor to decide the sentence rank. 
the following table shows the evaluation result provided by nist for our system
                                                   strict             lenient
   nist score                              1               1
   % of correct answers                1%           1%
   % of correct first answers         1%           1%
the current  oracle 1i  knowledge base is designed for information retrieval; for q/a track  we found it necessary to expand the lexicon to cover wh-focus ontological facets.
1. web track 
as preparation  we investigated the trec-1 web task using trec-1 web track documents and queries. we also attempted to productize lessons learnt from our participation in trec1 adhoc manual task. a set of different collections including trec web and adhoc collections helped us in our effort to formulate generic techniques applicable across domain. due to resource constraints  we were unable to work on trec1 web track. here we summarize our findings based on older collections. 
our experiments in link analysis using oracle intranet data indicate that link analysis adds little value to intranet search. link analysis is a technique that helps bring order to an unorganized collection lacking central authority  such as web  by using popularity measure. a organized intranet will have clearly defined authorities for different subject matters. 
idf weighting used in tf-idf scoring is not very effective when the collection is pretty large  a couple of million documents  and number of terms in the queries is pretty high. if the queries are free-text queries  idf weighting fails to distinguish between important and unimportant terms. weighting techniques which weight terms inversely proportional to a factor of the frequency ratios  x times as rare terms get y times as much weight  seem to perform better in this situation. we saw significant improvement in r-precision by adopting this technique. 
as the number of documents increases  the number of distinct score values supported by a system becomes important. until recently oracle text used 1 distinct integers in the range of 1 to 1 for scoring. we found that allowing a million distinct values improves system ir quality computed in average precision by improving tie splitting. even though number of relevant documents retrieved did not increase very significantly  about 1%   average precision increased by 1%  for example  trec1 web track average precision improved from 1 to 1 . 
using trec1 and trec1 collections  we identified a few simple flaws in our system which have been removed. on average  recall has increased by about 1% and precision at 1 has improved by more than 1%. we ran this out-of-box automatic system against trec-1 adhoc task. oracle trec-1 manual task submission received an average precision score of 1. out of 1 benchmark queries  performance  number of relevant retrieved  is tied for 1 queries  1 won by manual and 1 won by automatic. 
