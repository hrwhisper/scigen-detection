 num  number: 1
 title  sugar tariff-rate quotas
 desc  description: describe the nature and history of sugar tariff-rate quotas in the united states.
 narr  narrative: documents describing the system  its history and how it works are relevant. proposed changes to the system or new agreements explaining how it works are relevant. listings of current allocations are not relevant.
figure 1: a sample trec 1 topic from the terabyte track test set.
environment that provides a means for researchers to explore the relative benefits of different retrieval strategies in a laboratory setting. test collections consist of three parts: a set of documents  a set of information needs  called topics in trec   and relevance judgments  an indication of which documents should be retrieved in response to which topics. we call the result of a retrieval system executing a task on a test collection a run.
1.1 documents
the document set of a test collection should be a sample of the kinds of texts that will be encountered in the operational setting of interest. it is important that the document set reflect the diversity of subject matter  word choice  literary styles  document formats  etc. of the operational setting for the retrieval results to be representative of the performance in the real task. frequently  this means the document set must be large. the initial trec test collections contain 1 to 1 gigabytes of text and 1 to 1 1 documents. the document sets used in various tracks have been smaller and larger depending on the needs of the track and the availability of data. the terabyte track was introduced in trec 1 to investigate both retrieval and evaluation issues associated with collections significantly larger than 1 gigabytes of text.
   while the initial trec document sets consisted mostly of newspaper or newswire articles  later document sets have included recordings of speech  web pages  scientific documents  blog posts  email messages  and so forth. in each case  high-level structures within each document are tagged using sgml or xml  and each document is assigned an unique identifier called the docno. in keeping of the spirit of realism  text is kept as close to the original as possible. no attempt is made to correct spelling errors  sentence fragments  strange formatting around tables or similar faults.
1.1 topics
trec distinguishes between a statement of information need  the topic  and the data structure that is actually given to a retrieval system  the query . the trec test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. the format of a topic statement has evolved since the earliest trecs  but it has been stable since trec-1  1  a topic statement generally consists of four sections-an identifier  a title  a description  and a narrative-though some tracks don't use topics at all  e.g.  spam  or use different formats to support the track  e.g.  legal . an example topic taken from this year's terabyte track is shown in figure 1.
   the different parts of the trec topics allow researchers to investigate the effect of different query lengths on retrieval performance. for topics 1 and later  the  title  field was specially designed to allow experiments with very short queries; these title fields consist of up to three words that best describe the topic. the description   desc   field is a one sentence description of the topic area. the narrative   narr   gives a concise description of what makes a document relevant.
   participants are free to use any method they wish to create queries from the topic statements. trec distinguishes among two major categories of query construction techniques  automatic methods and manual methods. an automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. the definition of manual query construction methods is very broad  ranging from simple tweaks to an automatically derived query  through manual construction of an initial query  to multiple query reformulations based on the document sets retrieved. since these methods require radically different amounts of  human  effort  care must be taken when comparing manual results to ensure that the runs are truly comparable.
   trec topic statements are created by the same person who performs the relevance assessments for that topic  the assessor . usually  each assessor comes to nist with ideas for topics based on his or her own interests  and searches the document collection using nist's prise system to estimate the likely number of relevant documents per candidate topic. the nist trec team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors.
1.1 relevance judgments
the relevance judgments are what turns a set of documents and topics into a test collection. given a set of relevance judgments  the ad hoc retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. trec usually uses binary relevance judgments-either a document is relevant to the topic or it is not. to define relevance for the assessors  the assessors are told to assume that they are writing a report on the subject of the topic statement. if they would use any information contained in the document in the report  then the  entire  document should be marked relevant  otherwise it should be marked irrelevant. the assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.
   relevance is inherently subjective. relevance judgments are known to differ across judges and for the same judge at different times . furthermore  a set of static  binary relevance judgments makes no provision for the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. despite the idiosyncratic nature of relevance  test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments .
   the relevance judgments in early retrieval test collections were complete. that is  a relevance decision was made for every document in the collection for every topic. the size of the trec document sets makes complete judgments utterly infeasible-with 1 documents  it would take over 1 hours to judge the entire document set for one topic  assuming each document could be judged in just 1 seconds. instead  trec uses a technique called pooling  to create a subset of the documents  the  pool   to judge for a topic. each document in the pool for a topic is judged for relevance by the topic author. documents that are not in the pool are assumed to be irrelevant to that topic. pooling is valid when enough relevant documents are found to make the resulting judgment set approximately complete and unbiased.
   the judgment pools are created as follows. when participants submit their retrieval runs to nist  they rank their runs in the order they prefer them to be judged. nist chooses a number of runs to be merged into the pools  and selects that many runs from each participant respecting the preferred ordering. for each selected run  the top x documents per topic are added to the topics' pools. since the retrieval results are ranked by decreasing similarity to the query  the top documents are the documents most likely to be relevant to the topic. many documents are retrieved in the top x for more than one run  so the pools are generally much smaller than the theoretical maximum of x¡Á the-number-of-selected-runs documents  usually about 1 the maximum size .
   the use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents.
   zobel demonstrated that the quality of the pools  the number and diversity of runs contributing to the pools and the depth to which those runs are judged  does affect the quality of the final collection . he also found that the trec collections were not biased against unjudged runs. in this test  he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. for the trec-1 ad hoc collection  he found that using the unique relevant documents increased a run's 1 point average precision score by an average of 1 %. the maximum increase for any run was 1 %. the average increase for the trec-1 ad hoc collection was somewhat higher at 1 %.
   a similar investigation of the trec-1 ad hoc collection showed that every automatic run that had a mean average precision score of at least 1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents . that investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs  an effect noted by the organizers of the ntcir  nacsis test collection for evaluation of information retrieval systems  workshop who performed their own manual runs to supplement their pools .
   the leave-out-uniques  lou  test can fail to indicate a problem with a collection if all the runs that contribute to the pool share a common bias-preventing such a common bias is why a diverse run set is needed for pool construction. while it is not possible to prove that no common bias exists for a collection  no common bias has been demonstrated for any of the trec collections until recently. when pools are shallow relative to the number of documents in the collection  the sheer number of documents of a certain type fill up the pools to the exclusion of other types of documents. in particular  otherwise diverse retrieval methodologies will all rank documents that have lots of topic title words before documents containing fewer topic title words since topic title words are specifically chosen to be good content indicators. to produce an unbiased  reusable collection  traditional pooling requires sufficient room in the pools to exhaust the spate of title-word documents and allow documents that are not title-word-heavy to enter the pool . but large document sets such as the one used in the terabyte track include so many documents containing topic title words that traditional pooling requires pools that are much far too large to be affordable to judge. one of the goals for the terabyte track is to investigate new pooling strategies to build reusable  fair collections at a reasonable cost despite collection size.
1 evaluation
retrieval runs on a test collection can be evaluated in a number of ways. in trec  ad hoc tasks are evaluated using the treceval package written by chris buckley of sabir research . this package reports about 1 different numbers for a run  including recall and precision at various cut-off levels plus singlevalued summary measures that are derived from recall and precision. precision is the proportion of retrieved documents that are relevant  number-retrieved-and-relevant/number-retrieved   while recall is the proportion of relevant documents that are retrieved  number-retrieved-and-relevant/number-relevant . a cut-off level is a rank that defines the retrieved set; for example  a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. the treceval program reports the scores as averages over the set of topics where each topic is equally weighted.  the alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important. 
   precision reaches its maximal value of 1 when only relevant documents are retrieved  and recall reaches its maximal value  also 1  when all the relevant documents are retrieved. note  however  that these theoretical maximum values are not obtainable as an average over a set of topics at a single cut-off level because different topics have different numbers of relevant documents. for example  a topic that has fewer than ten relevant documents will have a precision score at ten documents retrieved less than 1 regardless of how the documents are ranked. similarly  a topic with more than ten relevant documents must have a recall score at ten documents retrieved less than 1. at a single cut-off level  recall and precision reflect the same information  namely the number of relevant documents retrieved. at varying cut-off levels  recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.
   of all the numbers reported by treceval  the interpolated recall-precision curve and mean average precision  non-interpolated  are the most commonly used measures to describe trec retrieval results. a recall-precision curve plots precision as a function of recall. since the actual recall values obtained for a topic depend on the number of relevant documents  the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. the particular interpolation method used is given in appendix a  which also defines many of the other evaluation measures reported by treceval. recallprecision graphs show the behavior of a retrieval run over the entire recall spectrum.
   mean average precision  map  is the single-valued summary measure used when an entire graph is too cumbersome. the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . the mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. the average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents  and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. geometrically  average precision is the area underneath a non-interpolated recall-precision curve.
   as trec has expanded into tasks other than the traditional ad hoc retrieval task  existing evaluation measures have been adapted and new evaluation measures have been devised. the details of the evaluation methodology used in a particular track are described in the track's overview paper.
1 trec 1 tracks
trec's track structure was begun in trec-1  1 . the tracks serve several purposes. first  tracks act as incubators for new research areas: the first running of a track often defines what the problem really is  and a track creates the necessary infrastructure  test collections  evaluation methodology  etc.  to support research on its task. the tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. finally  the tracks make trec attractive to a broader community by providing tasks that match the research interests of more groups.
   table 1 lists the different tracks that were in each trec  the number of groups that submitted runs to that track  and the total number of groups that participated in each trec. the tasks within the tracks offered for a given trec have diverged as trec has progressed. this has helped fuel the growth in the number table 1: number of participants per track and total number of distinct participants in each trec
trectrack'1'1'1'1'1'1'1'1'1'1'1'1'1'1'1ad hoc1111routing111interactive11111spanish11confusion1merging1filtering1111chinese1nlp1speech11xlingual111high prec1vlc1query11qa1111web111video1novelty11genomics11hard11robust11terabyte11enterprise1spam1legal1blog1participants11111111of participants  but has also created a smaller common base of experience among participants since each participant tends to submit runs to a smaller percentage of the tracks.
   this section describes the tasks performed in the trec 1 tracks. see the track reports later in these proceedings for a more complete description of each track.
1 the blog track
the blog track is a new track in trec 1. its purpose is to explore information seeking behavior in the blogosphere  in particular to discover the similarities and differences between blog search and other types of search. the track contained two tasks  an open task and an opinion retrieval task. participants in the open task defined their own retrieval task and evaluation strategy using the blog corpus. these were pilot evaluations to inform the discussion of the track's future. the opinion retrieval task was a common task with topic development and relevance judgments performed at nist.
   the blog corpus was collected over a period of 1 weeks from december 1 through february 1. it consists of a set of uniquely-identified xml feeds and the corresponding blog posts in html. a  document  in the collection  for the purposes of the opinion task  is a single blog post plus all of its associated comments as identified by a permalink. the collection is a large sample of the blogosphere as it existed in early 1 that retains all of the gathered material including spam  potentially offensive content  and some non-blogs such as rss feeds.
   in the opinion task  systems were to locate blog posts that expressed an opinion about a given target. targets included people  organizations  locations  product brands  technology types  events  literary works  etc. for example  three of the test set topics asked for opinions regarding the macbook pro  jon stewart  and super bowl ads. targets were drawn from a log of queries submitted to blogpulse. the query from the log was used as the title field of the topic statement and the nist assessor created the remaining parts of the topic statement.
   while the systems' task was to retrieve posts expressing an opinion of the target without regard to the polarity of the opinion  the relevance assessments made for the track did differentiate among different types of posts to provide useful training data for future tasks. a post could remain unjudged if it was clear from the url or header that the post contains offensive content. if the content was judged  it was marked with exactly one of: irrelevant  not on-topic   relevant but not opinionated  on-topic but no opinion expressed   relevant with negative opinion  relevant with mixed opinion  or relevant with positive opinion.
   fourteen groups participated in the blog opinion task  and an additional two groups participated in the open task. the primary measure used in the track was map when treating a document as relevant if it was both on-topic and opinionated. runs were also evaluated using just on-topic as the definition of relevant. the correlation between the system rankings produced by the two definitions of relevant was high  a spearman's ¦Ñ of 1 and a kendall's ¦Ó of .1   suggesting that whether or not a document was on-topic dominated the retrieval results. a baseline run  created after relevance judging was complete  produced by the university of glasgow's terrier system with no opinion-specific processing was more effective than any of the submitted systems using either of the definitions of relevant. thus more work is required to be able to separate opinionated posts from on-topic posts.
1 the enterprise track
the enterprise track started in trec 1. the purpose of the track is to study enterprise search: satisfying a user who is searching the data of an organization to complete some task. enterprise data generally consists of diverse types such as published reports  intranet web sites  and email  and the goal is to have search systems deal seamlessly with the different data types.
   the document set used in both years of the track was the w1c test collection  see http:// research.microsoft.com/users/nickcr/w1c-summary.html . this collection  created by nick craswell  was created from a crawl of the world-wide web consortium web site and includes email discussion lists  web pages  and the extracted text from documents in various formats  such as pdf  postscript  word  powerpoint  etc. .
   the track contained two tasks  a discussion search task and a search-for-experts task. a total of twentyfive groups participated in the enterprise track.
   in the discussion search task the systems were to retrieve the set of messages in the email lists that provided pro/con arguments for a particular choice such as  html vs. xhtml . the task was specifically focused on finding arguments for or against a decision rather than simply finding information about the topic. the motivation for the task is to assist users in understanding why a particular decision has been made.
   the runs were evaluated both when relevance was defined simply as being on-topic as well as when relevance was defined as containing a pro/con argument. with a few exceptions including a manual run  the relative effectiveness of the runs was largely the same in both cases. indeed  a more detailed look at the document rankings  see the track overview paper for details  showed that most runs did not consistently retrieve documents containing an argument earlier than documents that were simply on-topic. thus  more work is needed to develop argument detectors.
   the motivation for the expert-finding task is being able to determine who to contact regarding a particular matter in a large organization. as operationalized in the track task  the expert search mines an organization's documents to create profiles of its people. systems returned a ranked list of person-ids and a set of supporting documents per person in response to a topic such as  ontology engineering . systems were given a mapping between names and person-ids of w1c members. the supporting documents were a set of up to 1 documents that the system believed demonstrated why the person was an expert on the topic. topic creation and relevance assessments were performed by the track participants.
   the better expert-finding runs had a mean reciprocal rank  mrr  score greater than 1 showing that those systems were generally able to return a true expert at rank one. corresponding p 1  scores were approximately 1 showing that the majority of candidate experts suggested by those runs were in fact experts.
1 the genomics track
the goal of genomics track is to provide a forum for evaluation of information access systems in the genomics domain. it is the first trec track devoted to retrieval within a specific domain  and thus a subgoal of the track is to explore how exploiting domain-specific information improves access. the trec 1 track consisted of a single passage retrieval task  though that task was evaluated in a number of different ways to explore a variety of facets. the task was motivated by the observation that the best response for a biomedical literature search is frequently a direct answer to the question  but with the answer placed in context and linking to original sources.
   the document set used in the track was a set of full-text articles from several biomedical journals which were made available to the track by highwire press. the documents retain the full formatting information  in html  and include tables  figure captions  and the like. the test set contains 1 documents from 1 journals and is about 1 gb of html. a passage is defined to be any contiguous span of text that does not include an html paragraph token   p  or   p  . systems returned a ranked list of passages in response to a topic where passages were specified by byte offsets from the beginning of the document.
   the topics were derived from the topics used in the trec 1 track. the form of the topic was a natural language question  though these were created using a set of  generic topic templates  such as find articles describing the role of a gene involved in a given disease. the test set contained 1 questions  seven

questions each from four templates.
   relevance judgments were made by 1 people with expertise in the domain. the judgment process involved several steps to enable system responses to be evaluated at different levels of granularity. passages from different runs were pooled  using the maximum extent of a passage as the unit for pooling.  the maximum extent of a passage is the contiguous span between paragraph tags that contains that passage  assuming a virtual paragraph tag at the beginning and end of each document.  judges decided whether a maximum span was relevant  contained an answer to the question   and  if so  marked the actual extent of the answer in the maximum span. in addition  the assessor assigned one or more mesh terms to that passage as the definition of the aspect that the passage pertained to. a maximum span could contain multiple answer passages; the same aspect could be covered by multiple answer passages and a single answer passage could pertain to multiple aspects.
   using these relevance judgments  runs were then evaluated at the document  passage  and aspect levels. a document is considered relevant if it contains a relevant passage  and it is considered retrieved if any of its passages are retrieved. the document level evaluation was a traditional ad hoc retrieval task  when all subsequent retrievals of a document after the first were ignored . passage- and aspect-level evaluation was based on the corresponding judgments. aspect-level evaluation is a measure of the diversity of the retrieved set in that it rewards systems that are able to find more different aspects. passage-level evaluation is a measure of how well systems are able to find the particular information within a document that answers the question.
   the genomics track had 1 participants. the passage-level task is apparently a difficult task as evaluation scores for this task were generally low. effectiveness for both the aspect and document levels was much better  suggesting that the difficulty for the passage level is in finding the appropriate extent of the required information.
1 the legal track
the legal track was a new track in 1. it focused on a specific aspect of retrieval in the legal domain  that of meeting the needs of lawyers to engage in effective discovery of digital documents. currently  it is common for the two sides involved in litigation to negotiate a boolean expression that defines the set of documents that are then examined by humans to determine which are responsive to a discovery request. the goal of the track is to evaluate the effectiveness of other search technologies in facilitating this process.
   from the retrieval perspective  the task in the track was an ad hoc search task using a set of hypothetical complaints and requests for the production of documents as topics. the document set used in the track was the iit complex document information processing collection  which consists of approximately seven million documents drawn from the legacy tobacco document library hosted by the university of california at san francisco. these documents were made public during various legal cases involving us tobacco companies and contain a wide variety of document genres typical of large enterprise environments. a document in the collection consists of the optical character recognition  ocr  output of a scanned original plus a metadata record.
   the production requests used as topics were developed for the track by lawyers and were designed to simulate the kinds of requests used in current practice. each production request includes a broad complaint that lays out the background for several requests and one specific request for production of documents. the topic statement also includes a negotiated boolean query for each specific request. systems could use the negotiated boolean query in any way they saw fit  including ignoring it completely  for the trec runs. stephen tomlinson of open text  hummingbird  ran the track's reference run  which consisted of running just the negotiated boolean query for each topic.
   the relevance assessments were made by legal professionals who followed their typical work practices. pools were created using traditional pooling for the trec submissions received from the six participating groups plus a stratified sample of the baseline boolean run. in addition  the track organizers arranged for a professional searcher familiar with the document collection to  manually  produce a set of approximately 1 documents for each topic that the searcher expected to be relevant to the topic and unlikely to be retrieved by the other methods. these documents were also added to the pools.
   to understand how ranked retrieval approaches can assist discovery  it is necessary to compare ranked retrieval results to the results obtained by the negotiated boolean queries. thus  one of the goals of the track was the development of an evaluation methodology that provides for the fair comparison of such runs on a large document set where only a sample of documents is judged. this is a very complicated issue that this first running of the track has just begun to address. in the interim  one measure used in the track was r-precision  a measure that probably favors ranked retrieval runs since the  first  r documents is not well-defined in a pure boolean run. however  each of the boolean runs submitted to the track including the reference run were ranked in some fashion after the boolean constraint was applied  so r-precision is defined for the track runs. using r-precision as the measure  the reference boolean run and several of the best ranked runs were equally effective.
   while the average r-precision for the better runs was approximately the same  different runs were relatively better for different topics and each run found relevant documents that the other systems did not retrieve. in particular  the collection contains many relevant documents that do not match the negotiated boolean queries. this is an important finding for current practice since legal discovery is a recall-oriented task.
1 the question answering  qa  track
the goal of the question answering track is to develop systems that return actual answers  as opposed to ranked lists of documents  in response to a question. the 1 track contained two tasks  the main task that was a series task similar to the task used in trecs 1 and 1  and a complex interactive qa  ciqa  task.
   the questions in the main task were organized into a set of series. a series consisted of a number of  factoid   questions with fact-based  short answers  and list questions that each related to a common  given target. the final question in a series was an explicit  other  question  which systems were to answer by retrieving information pertaining to the target that had not been covered by earlier questions in the series. answers were required to be supported by a document from the corpus used in the track  the aquaint corpus of english news text  ldc catalog number ldc1  see www.ldc.upenn.edu .
   in a change from previous years  time-dependent factoid questions were required to be answered with regard to a particular timeframe  as opposed to the timeframe of an arbitrary document containing an answer . for factoid questions phrased in the present tense  the implicit timeframe was the date of the latest aquaint document  i.e.  the system was required to answer with the most up-to-date information possible. for factoid questions phrased in the past tense  either the question specified the timeframe  what cruise line attempted to take over ncl in december 1   or the timeframe of the series that included the question was the implied timeframe  for a target of  france wins soccer's world cup   the question who was the coach of the french team  is to be interpreted as the coach at the time of the world cup .
   the score for a series was computed as a weighted average of the scores for the individual questions that comprised it  and the final score for a run was the mean of the series scores. in a second change from previous years  the weights given to factoid  list  and other questions in the average were equal. this change lessened the importance of factoid questions in the final score.
   in absolute terms  the series scores for participating systems have decreased since 1. this reflects the increasing difficulty-and realism-of the evaluation conditions. in particular  the new requirement for answers to be correct with respect to the date of the latest document in the collection is a significant departure from previous requirements.
   the ciqa task was a blend of the trec 1 relationship qa task and the trec 1 hard track. the goal of the task was to extend systems' abilities to answer more complex information needs than those covered in the main task and to provide a limited form of interaction with the user in a qa setting.
   the questions used in the task contained two parts  a specific question derived from templates of relationship question types  and a narrative that provided more explanation for the specific question. the system response to a question was a ranked list of information  nuggets  supported by aquaintdocuments  where each nugget provides evidence for the relationship in question.
the limited interaction with the user  using the assessor as the surrogate user  was accomplished through forms as in previous hard tracks. participants were allowed to create one html-based form per question per run. the form contained a task for the assessor to perform  and assessors were limited to no more than 1 minutes per form. the result of the interaction with a form were returned to the participant  who  presumably  incorporated the results into a new question answering run.
   six groups participated in the ciqa task. in addition  the university of maryland provided an initial baseline run constructed by retrieving sentences using the lucene search engine  and a corresponding final baseline run that eliminated those sentences that the assessor marked not relevant during the clarification form interaction. this baseline set was among the best of the runs  excluding a manual run set that was clearly more effective than all other submissions. this is yet another example in trec 1 where it has proved difficult to improve on the effectiveness of standard retrieval technology for more specialized tasks. thirty-one groups participated in the qa track.
1 the spam track
the spam track was first run in trec 1. the immediate goal of the track is to evaluate how well systems are able to separate spam and ham  non-spam  when given an email sequence. since the primary difficulty in performing such an evaluation is getting appropriate corpora  longer term goals of the track are to establish an architecture and common methodology for a network of evaluation corpora that would provide the foundation for additional email filtering and retrieval tasks. nine groups participated in the trec 1 spam track.
   the 1 track included an on-line filtering task as in the 1 track  plus an enhancement to that task and a new active learning task. for each task the track used a test jig developed for the track that takes an email stream  a set of ham/spam judgments  and a classifier  and runs the classifier on the stream reporting the evaluation results of that run based on the judgments. in the original on-line filtering task  the classifier receives the correct designation for a message as soon as it classifies the message  this represents ideal user feedback . in the delayed feedback extension to the task  the classifier eventually receives the correct designation for each message  but the designation for a given message m may come after some number of intervening messages that must be classified before the feedback for m is received. in the new active learning task  the classifier must determine the designations for the final 1% of an email stream based on learning the correct designations for exactly n messages of its own choosing from the first 1% of the stream  where n was much smaller than 1% of the collection size .
   the track used two private email streams and two public email streams. the private streams and one of the public streams were predominately english streams  some spam messages could be in other languages  while the second public stream was predominately chinese. participants ran their own filters on the public corpora using the jig and submitted the evaluation output to nist. for the private corpora  participants submitted their filters to nist. nist passed the filters onto the university of waterloo after stripping all identification of which filters came from which participant. the university of waterloo used the jig to run the filters on the private corpora and returned the evaluation results to nist  who then forwarded the evaluation results to the appropriate participant.
   the overall results were consistent across the four email streams. detecting spam is more difficult when given delayed feedback than when immediate feedback is available; the active learning task is even more difficult. nonetheless  filters are able to detect the vast majority of spam with high accuracy  and there is no indication that this year's  more recent  spam is any harder to detect than earlier spam.
1 the terabyte track
the goal of the terabyte track is to develop an evaluation methodology for terabyte-scale document collections. the track also provides an opportunity for participants to see how well their retrieval algorithms scale to much larger test sets than previous trec collections.
   the document collection used in the track was the same collection created for the initial running of the track in trec 1: the gov1 collection  a collection of web data crawled from web sites in the .gov domain during early 1. this collection contains a large proportion of the crawlable pages in .gov  including html and text  plus extracted text of pdf  word and postscript files. the collection contains approximately 1 million documents and is 1 gb. the collection is distributed by the university of glasgow  see http://ir.dcs.gla.ac.uk/testcollections/.
   the track contained three tasks  a classic ad hoc retrieval task  an efficiency task  and a named-pagefinding task. manual runs were strongly encouraged for the ad hoc task since manual runs frequently contribute unique relevant documents to the pools. as part of the inducement for manual runs  an  unspecified  prize was offered to the group that returned the greatest number of unique relevant documents. the efficiency and named page tasks required completely automatic processing only.
   fifty new information-seeking topics were created by nist assessors for the track. manual runs used only these 1 topics; automatic runs were required to use the set of 1 topics created for the track from trecs 1. systems returned the top 1 documents per topic. in an attempt to overcome the bias toward topic title word documents described in section 1.1  pools were created in multiple stages with only the initial stage using traditional pooling. see the terabyte track overview paper for more details.
   the more effective automatic ad hoc runs used a variety of retrieval models. most of these runs used features such as phrases or term proximity factors  and pseudo-relevance feedback was generally put to good use. none of the top eight runs made special use of anchor text  and only one used link analysis in producing the retrieved set.
   the efficiency task was designed as a way of comparing the efficiency and scalability of systems given participants all used their own  different  hardware. the  topic  set was a sample of 1 queries mined from web search engine logs. to be selected for the query set  the query was required to have a minimum number of hits in the gov1 collection. the title fields from the ad hoc and named-page tasks' topics were added to this set but were not distinguished in any way. the queries were distributed in four different sets to represent four query streams. queries in a given stream had to be processed in the order in which they appeared in the stream  but queries from different streams could be interleaved in any manner. participants ran their systems using the entire query set and returned the top 1 documents per query plus reported the average processing time per query and the total time for all queries. finally  participants were asked to submit one run using one of three open-source information retrieval systems whose efficiency characteristics are known as a way of normalizing for hardware differences. the queries corresponding to the ad hoc and named-page topics were used to measure the effectiveness of the efficiency runs.
   both effectiveness and efficiency varied greatly across participants. as to be expected  systems could realize effectiveness gains by being less efficient  i.e.  a system's most effective run differed from its most efficient run .
   since the document set used in the track is a crawl of a cohesive part of the web  it can support investigations into tasks other than information-seeking search. one of the tasks that had been performed in the web track in earlier years was a named-page finding task  in which the topic statement is a short description of a single page  or very small set of pages   and the goal is for the system to return that page at rank one. the terabyte named page task repeated this task using the gov1 collection and a set of target topics created by the participants.
   in contrast to the ad hoc task  the more effective named-page finding runs exploited some combination of link structure  anchor text and document structure  for example  giving greater weight to document title words . the most effective named-page run  indri1nsdpfrom the university of massachusetts that had a mean reciprocal rank score of 1  used all three factors. twenty-one groups participated in the terabyte track.
1 the future
initial plans for trec 1 were formulated during the trec 1 conference. all of the 1 tracks except the terabyte track will continue into 1; the terabyte track will pause while the feasibility of collecting and using an even larger document set than gov1 is explored.
   trec 1 will contain a new track optimistically called the  million query  track. while it is unlikely that a test collection with literally 1 1 queries will be constructed  the goal of the track is to test the hypothesis that a test collection built from very many  very incompletely judged queries  topics  is a better research tool than a traditional trec pooled test collection. both nist assessors and trec participants will judge on the order of 1 documents for a query. queries will be mined from web search engine logs with existing trec topics  title fields  included as part of the query set. the documents to be judged will be selected from participant submissions according to a particular sampling strategy such as those suggested by yilmaz and aslam  or carterette et al. .  particular strategies will be randomly assigned to queries.  the expectation is that this will allow different sampling strategies to be compared on both the validity of the resulting test collection and the expense of producing the collection.
acknowledgements
special thanks to the track coordinators who make the variety of different tasks addressed in trec possible. the track summaries in section 1 are based on the track overview papers authored by the coordinators.
