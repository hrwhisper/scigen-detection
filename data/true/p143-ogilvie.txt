this paper investigates the pre-conditions for successful combination of document representations formed from structural markup for the task of known-item search.  as this task is very similar to work in meta-search and data fusion  we adapt several hypotheses from those research areas and investigate them in this context.  to investigate these hypotheses  we present a mixturebased language model and also examine many of the current meta-search algorithms.   we find that compatible output from systems is important for successful combination of document representations.   we also demonstrate that combining low performing document representations can improve performance  but not consistently.  we find that the techniques best suited for this task are robust to the inclusion of poorly performing document representations.  we also explore the role of variance of results across systems and its impact on the performance of fusion  with the surprising result that the correct documents have higher variance across document representations than highly ranking incorrect documents.   
categories and subject descriptors 
h.1  information storage and retrieval : information search and retrieval - retrieval models 
general terms 
algorithms  experimentation 
keywords 
language models  known-item finding  meta-search algorithms  data fusion  
1. introduction 
known-item finding is an important information seeking activity that has recently gained some attention in the information retrieval community.  in this task the user knows of a particular document  but does not know where it is.  example document 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  july 1-august 1  1  toronto  canada. 
copyright 1 acm 1-1/1...$1. 
 types may be a web page  a report  or a normal text document.  very often  these documents have structural markup  such as html.  we believe that by forming a variety of document representations using this structural information and combining these representations during retrieval  systems can improve retrieval performance over using the single best document representation.  
one natural approach to combining document representations can be borrowed directly from the meta-search problem.  the goal in meta-search is to combine the results from different search engines to produce a single ranked list that is better than the results of any single search engine.  this is sometimes referred to as data fusion.  combining document representations is similar to creating a search engine for each of the document representations and performing meta-search to combine the result lists. 
an alternative to meta-search techniques is to use the document representations to modify term weights directly within a single search engine.  we present a technique for this using generative language modeling.  rather than using the language model estimated from a single document representation  this approach estimates a mixture language model based on a combination of language models created from the various document representations.   
there has been extensive work on studying the effectiveness of meta-search and the conditions for success .  aslam and montague  summarize this work by stating: 
 the systems being combined should  1  have compatible output  e.g.  on the same scale    1  each produce accurate estimates of relevance  and  1  be independent of each other.  
this paper investigates whether these hypotheses should also extend to the task of combining document representations for known-item finding.   
the first hypothesis can be directly applied to known-item finding.  we investigate this hypothesis of score compatibility using okapi and language modeling retrieval systems across different document representations.  croft  describes score compatibility as systems having  comparable output in that they are trying to make the same decision within the same framework .  within okapi and language modeling systems  croft's statements hold true when combining document representations.  however  it is difficult to recover the probability estimates from the ranking function used by okapi.  this may introduce some incompatibilities across the document representations.  to investigate this  we define a measure of compatibility that compares the shape of the normalized ranking functions across document representations. 
the second hypothesis does not directly apply  as the notion of relevance used in ad-hoc retrieval is different from the goal of known-item finding.  in known-item finding the goal is not to exhaustively find documents about a topic  but to find a single correct document.  a more appropriate hypothesis in this setting would be that the document representations tend to give higher weight to the correct document than to incorrect documents.  even so  we would like an approach that is robust to the inclusion of representations that only sometimes give high score to the correct document.  for example  a document representation of image alternate text in html may do very well sometimes at finding the correct document  but will tend to do rather poorly in general.  the ideal approach would be robust to errors in the poorly performing representations  but would also be able to leverage the representation when the correct answer is found.  we investigate the quality of representation hypothesis and also investigate whether any of the approaches can gain from poorly performing document representations.  
the third hypothesis states that the systems should be independent of each other.  that is  the systems should give high scores different sets of non-relevant documents than each other  but there should be a higher correlation among the relevant documents.  applied to known-item finding  an appropriate hypothesis is that the representations would give widely varying scores for the incorrect documents  but tend to score the correct document highly  with lower variance.   
in this paper  we explore these hypotheses using both meta-search algorithms and an approach motivated by language modeling.  section 1 describes related work and the techniques used in this paper.  section 1 describes our experimental methodology  evaluation  and system details.  in section 1 we present experimental results.  we conclude the paper in section 1. 
1. combining representations 
there are two ways to combine document representations in a retrieval system.  a meta-search approach would treat each document representation as a unique search engine possibly using different search algorithms  and then combine retrieval results from the separate engines to produce one ranked list.  a language modeling approach would combine the document representations into one mixture language model that estimates the query generation process  and then perform retrieval using the mixture language model.  merits of the meta-search approach include relaxed assumptions about the compatibility of the systems performing retrieval on the different document representations.  with the mixture-based language modeling approach we have guidance on how to combine the probability distributions.  the mixture-based language model also combines evidence at the query term level  rather than after performing retrieval. 
previous experiments in combining document representations include experiments with controlled vocabularies  passages  and phrases.  croft  provides an overview of these methods and experiments.  of particular interest here are experiments in the use of citations  which led to the use of link text and additional structural information on the web .  these experiments using link text all found that the link text was beneficial for homepage searching or site finding.  the work reported here investigates additional document representations for known-item searching on the web.  
1 meta-search/data fusion 
meta-search fusion algorithms were developed to address the combination of retrieval results from many retrieval systems.  in general  they combine the result lists containing the top n documents from each retrieval system.  this subsection describes the meta-search algorithms to which we will compare the language model approach  and discusses other related work where appropriate. 
there are two main classes of meta-search fusion algorithms: ones that use scores from systems and ones that do not.  the algorithms investigated in this paper are combmnz  combsum  condorcet fuse  borda fuse  and a reciprocal rank fusion strategy.  montague and aslam compare most of these approaches for metasearch in  and .  within these classes are variants that do and do not use training data.     
condorcet fuse  borda fuse  and the reciprocal rank fusion strategy do not use scores from the systems.  these approaches assume that the scores from the different retrieval systems are not directly comparable.  in a meta-search environment  this can be a good assumption  as the different search systems may use different ranking formulas or have different corpus statistics.  the corpus statistics may still be different in document representation fusion  but we can assume that the search engines use the same ranking function.  this may or may not lead to comparable scores  depending on the ranking function.   
condorcet fuse  and borda fuse  were originally developed to address elections and have been adapted to meta-search fusion.  we do not discuss the condorcet fuse algorithm in detail here  as it is beyond the scope of this paper.   borda fuse sums n - the rank of the document across all systems and sorts the documents in descending order  documents with higher scores are higher in the merged list .  system weights can be incorporated by multiplying the weight by the scores for the documents.  the reciprocal rank strategy  sums one over the rank the document across all search engines.  documents are sorted in descending order.  the reciprocal rank strategy gives much higher weight than borda fuse to documents that occur near the top of a list.  system weights can be incorporated by multiplying each document's inverse rank by the weight. 
both combmnz and combsum are variants of an algorithm developed by fox .  combsum ranks each document using the sum of the scores returned from the individual retrieval systems  and combmnz ranks by the sum multiplied by the number of systems that returned the document was in the top n results.  it is common to perform a linear normalization of the scores in the result lists for both algorithms so that the first document in each list has a score of one and the nth document has a score of zero.   
table 1: testbed characteristics 
task corpus number of topics number of documents size  document types homepage finding wt1g 1 1 1 1 gb html named-page finding .gov 1 1 1 1 gb html  doc  pdf  ps we introduce additional variant of these algorithms in which this the exponential function  escore  is applied to scores before the optional linear normalization.  this transformation is justified when the retrieval system returns the log of the query generation probability  because this places the scores back on the probability scale.  however  we will also consider this approach for as an adhoc normalization of okapi scores.  weights can be incorporated by multiplying the  normalized  document scores by the weight.   
weighted combsum is similar to using a linear combination  of the scores  but only uses scores in the top n results from each system.  other approaches not evaluated in this paper include a logistic regression model  and an approach that models score distributions . 
1 combining language models 
a unigram language model defines a multinomial probability distribution over all words in the vocabulary of the corpus.  these probabilities are interpreted as word generation probabilities  and documents are ranked by their probability of generating the query .  this generation probability is computed by taking the product over all query terms of the probability of the query term given the language model: 
p q붿d  =뫊q p qi 붿d   
모모모모모모모모i=1	 1  where qi is the ith query term of query q  |q| is the length of q  and 붿d is a language model estimated from document d.  in typical language modeling experiments for information retrieval  붿d is estimated using a linear interpolation of a language model estimated from the document text and one from the entire corpus:   
p w붿d  =뷂1pmle  wd +뷂1pmle  wc   1  where c is the entire collection.  these language models are estimated using the maximum likelihood estimate  mle  of the multinomial distribution.  in this case  it is the same as the empirical distribution.  the mle estimate for a document is defined as: 
pmle  wd = count w;d  
	d	 1  
the mle distribution for the collection is estimated similarly.  it is common to set the linear interpolation parameters 뷂1 and 뷂1 using guidance from dirichlet prior smoothing : 
뷂1 = dd+뷃  뷂1 = d뷃+뷃 	 1  where 뷃 is a parameter often set near the average document length in the collection. 
however  we wish to explore the combination of several language models estimated from different document representations.  one approach to take for combine language models created from different document representations is linear interpolation:   
p w붿d  =뫉k 뷂ip w붿d   i   
모모모모모i=1  where k is the number of language models  d i  is the document's ith representation  and 뷂i is the weight on the model 붿d i .  to ensure that this is a valid probability distribution  we must place these constraints on the lambdas: 
뫉k 뷂i =1	and for 1뫞 i 뫞 k  뷂i 뫟 1 
i=1	 1  
the form of linear interpolation presented in equation 1 is a special case where k=1.  the linear interpolation parameters can be trained or hand-tuned to the task.   
we estimate the probability distribution for each model 붿d i  by taking a linear interpolation of the mle of the text observed in the ith document representation and a collection language model estimated from all document representations of the same type in the collection.  we set the linear interpolation weights according to dirichlet prior smoothing.  this is the model we used in .   
1 related work  
the main difference between the language modeling approach presented here and the meta-search techniques is that it combines the document representations on the query term level  rather than as a post-retrieval score combination.  the approach presented here is not unique in that characteristic.  for example  much recent work in the initiative for evaluation of xml retrieval  combines document components for document retrieval.  some methods being investigated in this context combine vectors or probability distributions of document components.  while the document components are organized hierarchically  they could be easily adapted to the problem of combining document representations.   
there are other examples of research where document representations are combined within a model.  one related approach was proposed for language models in   but was not implemented.  myaeng et al  combine terms found in different document representations using bayesian inference networks.  the approach allows for different weights on terms found in different document representations  similar to what is done in the language model approach presented here. the authors had some success with this approach on limited ad-hoc retrieval experiments within a subset of the patent data in the trec document collection. 
1. experimental methods 
we performed experiments on two trec tasks and corpora.  table 1 summarizes the corpora and test topics.  our experiments were on the homepage finding task from trec 1 and the named-page finding task from trec 1.  the homepage finding task has 1 training topics  while the named-page finding task has no training data.  the homepage finding task uses the wt1g corpus  and the named-page finding task uses .gov corpus.  as there is no training data for the named-page finding task  we only present the results using equal weights on the document representations for this task.   
 homepage  named-page 	mrr 	# by 1 	mrr 	# by 1 full        1 1 	1 	1 in-link 1 1 	1 	1 title 1 1 	1 	1 alt 1 1 	1 	1 font 1 1 	1 	1 meta 1 1 	1 	1 table 1: performance of individual document  representations using okapi 
 homepage  named-page 	mrr 	# by 1 	mrr 	# by 1 full        1 1 	1 	1 in-link 1 1 	1 	1 title 1 1 	1 	1 alt 1 1 	1 	1 font 1 1 	1 	1 meta 1 1 	1 	1 table 1: performance of individual document representations using language models 
figures 1a-d: variants of combmnz for combining document 
representations.  applying the exponential function improved performance for all tasks and systems.   
 to evaluate the homepage finding task and the 
named-page finding task  we use mean-reciprocal rank  mrr  and number of topics where the correct page was found by rank 1 . 
our experiments are performed using the lemur toolkit .  a separate index is created for each document representation.  we use the porter stemmer and inquery's stopword list. each language model for a given document representation was estimated using a linear interpolation with the collection document representation model.  the linear interpolation weights for the document representation models were set using dirichlet priors as described in section 1  with the prior parameter set to approximately twice the average document representation length.  the parameters we used for okapi bm1 are k1.1 and b=1.    six different document representations were tested in our experiments:  i  the full document text   ii  in-link text   iii  title text   iv  image alternate text   v  large font text  including headings   and  vi  meta tag keywords and descriptions.   
this is a subset of the document representations we used in .  
tables 1 and 1 summarize their performance using okapi and dirichlet prior smoothed language models.  the best 1 document representations for both tasks and systems are the full text  in-link text  and the title text.    these tables illustrate that some of the document representations tend to be better than other representations  independent of the retrieval system used.   
1. experimental results 
this section presents experiments on combining document representations.  we combined representations in order of their individual performance  using the top 1 results from each result list for the meta-search algorithms. the parameters used with the weighted algorithms were according to the document representations' individual performance on the training data when measured by mrr.  as in previous work   we do not claim that these weights are optimal.  this same method was used for choosing the linear interpolation parameters for the weighted version of the language model approach. 
1 compatibility hypothesis 
we begin our examination of the document representation score compatibility hypothesis  by presenting experiments with variants of the combmnz algorithm.  in section 1 we mentioned two forms of score normalization: a linear scaling  which is standard  and transforming the original score using the exponential function.  figures 1a-d show the effects of these normalizations on meta-search using okapi and language models. 
for language models on the homepage finding task  figure 1b   we found that normalizing the scores with the exponential function helped significantly.  this is not surprising; lemur returns the log of the probability  and applying the exponential function returns the score to the probability scale.  normalizing these scores using a linear transformation hurt performance.  this may be because  in some sense  the generation probabilities are already normalized.  we found similar results for named-page finding  figure 1d . 
for combining okapi scores on both tasks  we found that using the exponential to normalize the scores also helped performance.  perhaps this normalization helped due to the log-like functions okapi uses in its ranking formula.  it does not place the scores on a probability scale  as with the language models  but there may be some similar effect.  we also tried the logistic regression transformation to the probability scale presented in   but it was not effective.  we believe this may be due to the small number of positive examples for the training topics.   
we hypothesize two representations are compatible  i.e. combine well  when they produce score distributions that have similar shapes.  in order to measure this  we computed the mean-squared error  mse  of document representation pairs for each query.   we then averaged the mse across queries and document representation pairs.  in order to get comparable mses across homepage finding 
	1 representations 	1 representations 
okapi  exp  not norm  	1 okapi  exp  not norm  	1 lm  exp  not norm  	1 lm  exp  not norm  	1 okapi  exp  	1 lm  exp  	1 lm  exp  	1 okapi  exp  	1 okapi 	1 okapi  not norm  	1 lm 	1 okapi  	1 okapi  not norm  	1 lm  	1 lm  not norm  	1 lm  not norm  	1 named-page finding 
	1 representations 	1 representations 
okapi  exp  not norm  1 okapi  exp  not norm  1 lm  exp  not norm  1 lm  exp  not norm  1 lm  exp  1 okapi  exp  1 okapi  exp  1 lm  exp  1 lm  1 okapi  1 okapi  1 lm  1 okapi  not norm  1 okapi  not norm  1 lm  not norm  	1 lm  not norm  	1 table 1:  scaled mean-squared error of the curves provided by the document representations averaged across topics and representation pairs. 
experiments  we normalized the scores to range from zero to one using a linear transformation. this transformation was specific to any given query  ranking algorithm  and normalization method.  this transformation was done after any normalization method applied to the rankings had been performed  for example  the linear transformation was done after the exponential normalization of scores .  the mse was computed over results at ranks one through thirty.  table 1 contains these numbers for both tasks when combining the best three and all six representations. 
within a retrieval algorithm the orderings of normalization techniques correspond exactly to the performance shown in figures 1a-d.  across retrieval methods  we found that the orders were not exact predictors of performance  although similar performance does correspond to similar mse.  we attribute the differences in part to the fact that the mse does not take into consideration the quality of the representations.   
from table 1 we can see that the mses from six representations are lower than the mses from three representations.  the lower mse scores found for all six representations do not mean that the results are more compatible.  as more representations are added  it is likely that some of the curves will be near others  which will reduce the mse.  the mse scores are only comparable where the same set of document representations are combined. 
we now extend the examination of the score compatibility hypothesis to additional algorithms.  figures 1 and 1 show the results of using the unweighted and weighted algorithms on homepage finding task.  figure 1 shows results on the namedpage finding task.  the unweighted algorithms only are used for named-page finding  as there is no training data.   by our definition of compatability  using system ranks instead of scores gives perfect compatibility.  however  the rank-based approaches do not perform as well as score-based aproaches.  we believe that this is because by disregarding scores and using ranks  important information encoded in the probability estimates returned by the ranking algorithms is lost. 
we omitted graphs showing the numbers of correct pages found by rank 1 in order to save space. the trends are largely the same as with mean-reciprocal rank  except that the relative performance of borda count fusing okapi results is slightly better for both the weighted and unweighted versions.  additionally  when fusing language models  the reciprocal rank fusion strategy's performance is better relative to the other systems for both the weighted and unweighted versions  and weighted condorcet fuse's performance under found by rank 1 was better than its performance when considering mean-reciprocal rank.  the relative system performances for the named-finding task when considering number of correct pages found by rank 1 was very similar to the mean-reciprocal rank measure.  these differences do not change the ordering of the systems. 
as in meta-search  we have demonstrated that compatibility is important for combining document representations.  however  combining document representations is distinct from meta-search in that we can choose the representations being combined and the ranking algorithms used on the representations.  with the added constraint that the ranking algorithms be the same for all representations  we can assume greater compatibility of the scores produced by the algorithms.  performing additional normalization of the scores can further improve the success of fusion. 
1 quality hypothesis 
this section investigates the hypothesis that the individual document representations must perform well in general to increase performance when fusing results.  the best performing algorithms did not gain from including the three poorly performing representations  figures 1 .  however  we sometimes find slight gains  and the best algorithms were robust to the addition of the other representations.   
to investigate this further  we combined the three worst performing language model representations: image alternate text  large font text  and meta tag contents.  combining these representations using weighted combmnz yielded a mrr of .1 on the homepage finding task and a mrr of .1 on the namedpage finding task.  these results provide very strong evidence that the combined performance is better than any of the three individual document representations.  using the signed-rank test with correction for multiple testing using the bonferroni method  the p-values were well under 1 for all comparisons to any of the original three representations.  it is not clear from these results what the conditions are for having a weak document representation improve performance significantly; it is probably dependent on the document representations being combined.   
in meta-search  much of the previous literature found improvements from combining search systems.  however  a recent study suggests that there is not always a consistent improvement when combining result lists .  the authors hypothesize that when combining results with already high quality results  there is not much improvement  but when combining lower performance search engines  there is a more consistent improvement in the quality of the result lists.  the findings in  are similar to ours. 


figure 1: results for homepage finding.   	figure 1: results for homepage finding.  	figure 1: results for named-page finding.  
	the fusion algorithms are unweighted.  	 the fusion algorithms are weighted. 	the fusion algorithms are unweighted. 
	 a  okapi  b  language models 	 a  okapi  b  language models 	 a  okapi  b  language models  
to summarize  we found that the best algorithms were robust to poorly performing document representations  even if they could not leverage the weak representations.  we also showed that there exist conditions under which poorly performing document representations can be combined to significantly improve performance.  these results are consistent with recent results in meta-search.  the quality hypothesis applies to both meta-search and combining document representations. 
1 variance hypothesis 
the third hypothesis was that in order to have successful fusion  the scores or ranks of the correct documents must vary less than those of the incorrect documents.  this is not as easy to measure as the independence hypothesis made in meta-search.  in metasearch it is appropriate to measure the overlap of the result lists for relevant documents and non-relevant documents.  in our task  there is only one correct document and the document representations vary widely  it is not likely that the correct document will be returned by all of the search systems.   
to investigate this hypothesis  we report the variance of the ranks/scores returned across all document representations of the correct document to the variance of results of the top 1 of any of the document representations.  for a given query  the variance we measured is the variance of the document's rank/score across all document representations being combined.  table 1 presents the percentage of documents where the variance of the top incorrect documents was lower than the variance of the correct document.   the variance of the scores and ranks of the correct document 

 
모table 1: percentage of times the variance of the correct document was higher than the variance of other high ranking 
documents.  the order of document representations combined is the same as in previous experiments.  hp = homepage finding  
np = named-page finding  okp = okapi  lm = language models. 
task alg type number representations 
1 	1 	1 	1 	1 hp okp rank 1 1 1 1 1 score 1 1 1 1 1 lm rank 1 1 1 1 1 score 1 1 1 1 1 np okp rank 1 1 1 1 1 score 1 1 1 1 1 lm rank 1 1 1 1 1 score 1 1 1 1 1 across the representations was higher than for other highly ranked documents.  this behavior was similar for the various ranking algorithms.  low variance of scores and ranks is not a factor in combining results lists formed from different representations.   
it may not be surprising that the variance is high for the correct documents.  in some document representations  the correct document may match the query very well  but not in all representations.  for incorrect documents  none of the representations may match the queries well  which may yield scores that do not vary as much.  this behavior distinguishes the problem of combining representations from the general meta-search problem.  in meta-search  larger agreement among the scores of relevant documents than among the scores of irrelevant documents has been considered important for successful fusion.  however  we find that low variance in scores of correct documents is not needed for successful for combination of representations.   
table 1: statistical significance tests for combining language models on the .gov named-page finding task 
 full text         ~ ~ ~ mixture lm  ~ ~          combmnz    ~ ~       combsum     ~       recip rank            borda fuse      ~ 1 direct comparisons/significance tests 
it is easy to identify trends from these figures 1.  the scorebased algorithms perform better than the ranked based algorithms.  the language models representations seem to do better than the okapi representations.  also  the mixture of language models performs best on all tasks.  it is not clear whether these differences are significant.   
table 1 reports significance tests for combining 1 language model representations on .gov named-page finding.  we used the wilcoxon signed-rank test and corrected for multiple testing using the bonferroni method.  a   indicates that the algorithm for the row outperformed  according to mrr  the algorithm in the column with a p-value of 1 or less  while a    indicates a pvalue of 1 or less.  a ~ indicates a p-value greater than 1.  the   and    signs indicate the column algorithm out-performed the row algorithm.     
there is very strong evidence the mixture-based language modeling approach is better than using the single best representation or using combining rank-based information.  additionally  there is very strong evidence that the score-based meta-search algorithms perform better than the borda and condorcet fusion algorithms.   
we would also like to answer is whether the results from combining language models using meta-search algorithms are significantly better than the results from combining okapi scores.  the signed-rank test was performed across all system pairs  using the unweighted algorithms on both tasks.  there was no statistical evidence of differences between language models or okapi for the score based fusion algorithms.   
however  we did find evidence that the mixture language model performed significantly better than the meta-search algorithms applied okapi results.  a p-value of 1 was obtained when comparing to combmnz  1 for combsum  and under 1 for other approaches. 
1 comparison to trec results 
for completeness  this section contains a brief comparison to results presented during and after the trec conference by research other groups.   for homepage finding  our results using the mixture language modeling system yielded an mrr of 1  failing to find only 1% in the top 1 returned results. for 1% of the topics  the correct document was in the top 1 results.  we did not enter a submission to the homepage finding task  but this performance is among the best three groups that participated  out of 1 .  all of the submissions with higher performance made use of the url text.  in particular  kraaij et al.  used a prior based on the depth of the url.  incorporating their prior into our results increases the performance of the mixture language model to a mrr of 1 and finds a correct document in the top 1 results for 1% of the topics.  this performance is comparable to that of kraaij et al.   mrr=1  found by rank 1.1% . 
our group did participate in the named-page finding task of trec 1.  our official submission used the same system  but included an additional language model created using the url text.  with this additional model  our system had a meanreciprocal rank of 1.  this was the second best official submission from a unique group.  without the url model  the language model approach gives a mean-reciprocal rank of 1.  zhang et al.  had a mean reciprocal rank of 1 for their best submission.  zhang et al. use similar structural information  along with creating separate indexes for html and pdf documents.  park et al.  also report comparable results.  their work uses a query-sentence similarity measures in addition to query-document similarity.  both groups used link anchor text in addition to other information present in the document. 
1. conclusions 
in this paper  we explored the use of meta-search algorithms and a language modeling approach to the combination of document representations.  the experiments were carried out on two knownitem finding tasks on html documents: homepage finding and named-page finding.   
we investigated three hypotheses adapted from the task of metasearch.  the first hypothesis was that the outputs from the search systems have compatible output.  we found this to be very important.  we proposed a measure of score compatibility using mean-squared error that accurately ordered the score normalization techniques according to their effectiveness.  we found that system scores could be leveraged effectively  and that rank-based fusion algorithms did not perform as well as the scorebased algorithms.  a mixture-based language model gave better results than applying meta-search techniques to okapi results.  we additionally found that normalizing okapi scores by applying the exponential function improved the compatibility of the scores. 
our second hypothesis was that for successful performance in combining document representations  each system should perform well.  we demonstrated that this hypothesis is not true; document representations that perform poorly can be combined with other representations to improve performance.  however  this gain in performance is not guaranteed  and adding representations to a high performing system may not improve performance.  we also demonstrated that the best algorithms for combining representations tend to be robust to the addition of representations.  that is  including new representations rarely hurt the performance.  when the new document representations did decrease performance  the degradation was not significant.   
the third hypothesis was that for successful combination of document representations  the scores or ranks of the correct documents across document representations would vary less than those of incorrect documents.  we found this to be false.  the scores and ranks of correct documents varied more than those of the incorrect documents  possibly because the correct documents would match the query very well in some of the document representations  but not well in others.  
the best meta-search algorithms did not perform quite as well as the mixture-based language model.  while these differences were not large or significant  this lends merit to the approach of directly combining the document representations within a retrieval model.  we found that the combmnz algorithm worked best among the meta-search algorithms. we demonstrated that the weighted versions of all algorithms could improve performance  even when using a non-optimal training strategy for selecting the weights. 
the problem of combining document representations is distinct from the meta-search problem.  when combining document representations  one can choose which ranking algorithms are used.  this added flexibility opens the doors for new methods of evidence combination and more appropriate normalization methods tailored to the ranking algorithms.  combining document representations is also different from meta-search in that the success of the combination methods is not dependent on the variance of scores or ranks of correct documents being lower than those of the incorrect documents.   
combining representations is an old idea within information retrieval  but the emergence of the web and xml give it new importance.  structured documents are becoming more common  not less  giving new urgency to developing retrieval models that manage them effectively.  the work reported here suggests that statistical language models are a sound foundation on which to construct such systems. 
1. acknowledgments 
this research was supported by the advanced research and development activity in information technology  arda  under its statistical language modeling for information retrieval research program  and by nsf grant eia-1.  any opinions  findings  conclusions  or recommendations expressed in this paper are the authors  and do not necessarily reflect those of the sponsors. 
