privacy-preserving data mining  ppdm  is an emergent research area that addresses the incorporation of privacy preserving concerns to data mining techniques. in this paper we propose a privacy-preserving  pp  cox model for survival analysis  and consider a real clinical setting where the data is horizontally distributed among different institutions. the proposed model is based on linearly projecting the data to a lower dimensional space through an optimal mapping obtained by solving a linear programming problem. our approach differs from the commonly used random projection approach since it instead finds a projection that is optimal at preserving the properties of the data that are important for the specific problem at hand. since our proposed approach produces an sparse mapping  it also generates a pp mapping that not only projects the data to a lower dimensional space but it also depends on a smaller subset of the original features  it provides explicit feature selection . real data from several european healthcare institutions are used to test our model for survival prediction of non-small-cell lung cancer patients. these results are also confirmed using publicly available benchmark datasets. our experimental results show that we are able to achieve a near-optimal performance without directly sharing the data across different data sources. this model makes it possible to conduct large-scale multi-centric survival analysis without violating privacy-preserving requirements.
categories and subject descriptors
i.1  artificial intelligence : miscellaneous
general terms
algorithms  theory  performance
keywords
privacy-preserving data mining  cox regression
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm 1-1-1/1 ...$1.
1. introduction
　privacy-preserving data mining  ppdm  is a research area that focuses on the incorporation of privacy preservation methods into data mining techniques  e.g.   . we are particularly interested in a scenario where the data is horizontally distributed among different entities or parties. in the medical domain this means that there exist several medical institutions  such as hospitals  clinics  etc.  and each one provides a database containing a complete  or almost complete  subset of item sets  patients . an efficient ppdm algorithm should be able to process the data from all the sources and learn data mining/machine learning models that take into account all the information available without sharing private information among the sources. the ultimate goal of a ppdm model is to perform similarly or identically to a model constructed by having access to all the original data  and in distributed scenarios  at the same time/location.
　there has been recent interest on the incorporation of electronic health records  ehr  in medical institutions worldwide. there is a general belief that the availability of ehr will have several significant benefits for health systems across the world  including improvements in the quality of care  e.g.  by tracking performance-based clinical measures   increases in the accuracy of insurance reimbursement systems  development of more advanced clinical  computer assisted diagnosis  cad  tools  etc.
　as a consequence  the number of hospitals storing large amounts of data has been increasing. this data can be used to build predictive models to assist doctors in the medical decision process for treatment  diagnosis  prognosis among others. ideally  the data from multiple institutions can be aggregated with the purpose of creating models with a higher statistical significance. however  sharing the data across institutions becomes a difficult and tedious process that also involves considerable legal and economic burden on the institutions sharing the medical data.
　in this paper we propose a novel privacy-preserving technique based on affine projections applied to learn survival predictive models. the model is based on the cox regression for survival analysis  and we apply it for non-small-cell lung cancer  nsclc  patients treated with  chemo  radiotherapy. the real data is collected from patients treated on three european institutions in two different countries  the netherlands and belgium . the framework we are describing in this paper allows to design/learn improved predictive models that perform better than the individual models obtained by using data from only one institution at a time.
next  we highlight the main contributions of this paper:
  it presents a new general criterion and formulation for building affine projections of the data for privacy preservation. this criterion differs from that used in the standard random projection method .
  it demonstrates how this criterion can be used to learn cox regression models for survival analysis.
  it applies the methodology to a real problem of interest in the clinical domain by learning survival models for lung cancer radiation therapy with data from multiple institutions addressing privacy.
  it provides an algorithm that allows to perform feature selection in the privacy-preserving setting.
　the criterion is based on our method for constructing sparse projection matrices . in this paper  this method is related and adapted to the privacy-preserving problem. one of its key advantages is that it builds a data representation that maintains the properties that are important for the problem at hand  while being privacy-preserving . we are not aware of any other privacy-preserving approach for learning cox regression or any survival analysis model. while our motivation is on radiation therapy  we also demonstrate our approach on publicly available datasets to ease future comparisons.
　the rest of the paper is organized as follows: in section 1 we present an overview of the related work. then in section 1 we present the overview of the cox model and the privacy-preserving cox model. section 1 develops the optimal projection method  and section 1 and 1 describe the experimental results using benchmark datasets and in a real clinical setting. finally we conclude the paper in section 1 with a brief discussion.
1. related work
　privacy-preserving data mining has received a lot of attention recently due to the increasing need to share and analyze data that have previously been stored in a passive state due to its private nature. an example is the healthcare setting  where it is becoming clearer that the market forces point in a direction that require making the full patient electronic health record  ehr  available to the patients themselves  to trusted parties  but also in a private manner to third party entities. the challenge is not just that of guaranteeing secure transmission of the data to trusted entities. a more critical challenge is instead to be able to analyze large amount of data available using several entities/locations that do not wish to share the actual data records with any of the other entities or even with a centralized entity. a clear example  which is the motivation for this paper  is the case when data from multiple healthcare institutions need to be used to build data mining or machine learning models without actually sharing the original data content.
　more generally  the focus of attention in the privacy preserving field has been: how to develop accurate models without access to the original data in individual records . an excellent overview of privacy-preserving data mining methods can be found in . privacy preservation is analyzed from various viewpoints based on:
  data distribution: referring to whether the data is to be centralized or distributed  including horizontal and vertical data partitioning .
  data modification: referring to how to change the data values so as to ensure high-privacy protection. various methods for data modification include data perturbation  additive and multiplicative   blocking  aggregation  etc.
  data management protocol: referring to how the data is exchanged to preserve privacy  including cryptography  reconstruction  and heuristic-based techniques 
　this paper is concerned primarily with the distributed scenario. we focus on horizontal data partition  the case when different entities hold the same input features for different groups of data points  individuals . for example  the horizontal case has been addressed recently in special scenarios  1  1  by privacy-preserving svms and induction tree classifiers. we concentrate on horizontal privacy-preserving data mining. in the vertical data distribution scenario  the entities have some subset of features for the same individuals. likewise  several techniques have been proposed to address this case in special setting including adding random perturbations to the data  1  1 
　the privacy-preserving methodology employed in this paper consists of a form of data modification based on aggregation that allows for it to be easily exchanged and still preserve the privacy of the original data. thus  avoiding the need for a more complex cryptographic protocol.
　this form of data aggregation is related to a data transformation referred to as random projections which was recently proposed for privacy-preserving data mining   including distributed . the basic idea consist of using an approximate random projection method to improve the level of privacy protection but still preserving some statistical characteristics of the data. this form of data modification was called randomized multiplicative data perturbation and basically consists of building a new representation of the data by projecting the data using a randomly build matrix as projection operator. the theoretical underpinnings of this form of transformation are based on the celebrated johnsonlindenstrauss lemma  which indicates that a set of n ddimensional points can be embedded into a k-dimensional space with k = o log n   such that the euclidean distance between any two points can be maintained within an arbitrarily small factor. thus  according to this the data can be made private and still preserve some statistics.
　the present approach also applies a multiplicative perturbation via a linear  and non-linear  projection. however  it is in sharp contrast with the randomized approach because it attempts to find a projection that is optimal for the problem at hand. the basic motivation for this is the question about why preserve the overall data structure when often by preserving only the relevant structure we can benefit both in terms of efficiency and accuracy. the efficiency gains are derived from the fact that by preserving only the relevant structure  it is possible to obtain a simpler and more compact representation. the accuracy gains are derived from the fact that for a fixed representation size  concentrating on the relevant structure for the problem at hand must improve its accuracy.
　we can summarize our basic idea as that of finding an optimal perturbation of the data that maintains  primarily  the relevant  important properties of the data and that at the same time promotes a compact representation.
　for completeness  we also note that various approaches are geared toward providing privacy to specific data mining techniques. for example  privacy preservation for clustering tasks . several other recently proposed privacypreserving classification techniques specific to the data mining model include cryptographically private svms   and wavelet-based distortion . we are not aware of any privacy-preserving approaches specific to cox regression or survival analysis. while our approach was designed with this setting in mind  we remark that it is more general and could be applied in a considerably general data mining scenario.
1. privacy-preserving cox model
　we begin with a brief introduction of the general cox regression model  and then present the privacy-preserving cox model. we discuss both the linear cox model and the non-linear cox model.
1 the general cox regression model
　in survival analysis  we are interested in the survival time t of each individual from a certain population . this can be characterized by the survival function s t  = pr t   t  for t   1  which is the probability that the individual is still alive at time t. a related function is the hazard function  which assesses the instantaneous risk of demise at time t  conditional on survival to that time:
 
with p t  being the density function of s t . it is easily seen that the hazard function fully determines the survival function as
　it is of practical interest to relate the hazard function not only to the time t  but also to a set of covariates  explanatory variables   xi （ rd  of each individual i. in clinical studies  the covariates typically include demographic variables such as age and gender  and diagnosis information like the tumor size. one of the first and the most popular survival models is the cox model  in which the hazard function takes  in its most general form  the following proportional-hazard form:
	λ t|xi  = λ1 t exp f xi   	 1 
where λ1 t  is the baseline hazard function  and f xi  is a function of the covariates xi . note that the cox hazard function depends on the covariates only via the timeindependent function f. it is commonly assumed that f is linear  i.e.  f xi  = w xi with some weight vector w （ rd  but we use the general form such that our privacy-preserving models can be applied to possibly non-linear f  cf. section 1 .
　in the original paper  cox also developed the partial likelihood for parameter estimation. for this we need to distinguish individuals who have the actual survival time observed  e.g.  observed death or cancer relapse after treatment   from individuals who are  right- censored  e.g.  still alive or cancer-free at the end of the study . let δi be an indicator variable which takes 1 if the individual i is from
the former group  and 1 otherwise. then the observed time ti is the survival time when δi = 1  and the censoring time when δi = 1. for a group of n individuals with outcome   the cox's partial likelihood l is defined as
exp f xi  
 
	j （	i
i fails
where ri = {j : tj − ti} is the risk set containing the individuals who are at risk  of failing  at time ti. the key idea here is to compare at each failure time  the risk for the failed individual to the risk for all the other individuals at risk at that time. this completely eliminated the baseline hazard λ1 t  from parameter estimation  indicating that the actual times of failure are not important. censoring times are not important as well  so long as we keep track of the risk sets.
　when f is linear  i.e.  f xi  = w xi  it is straightforward to maximize the partial likelihood w.r.t. vector w using  e.g.  gradient methods. when f is non-linear  one can formulate a regularization framework and optimize function f in the reproducing kernel hilbert space  rkhs  . see  for the details and some empirical studies.
1 horizontal privacy-preserving cox model
　in the case of horizontally partitioned data  suppose we have k different data sources  or parties  e.g.  hospitals or medical centers   and each party j has a subset of nj individuals  e.g.  patients . a same set of d predictors are shared among all parties  and survival outcomes are available within each party. the task is to develop a horizontal privacy-preserving cox model  hppcox  which is able to use all the data across different parties.
　we propose a hppcox model based on lower-dimensional projection methods such as random projection   or random kernel mapping in the general  non-linear  case . for simplicity we focus on the former which only applies for linear survival models  but the whole machinery also applies to non-linear models. the non-linear extension will be briefly discussed in section 1.
　our basic idea is based on the simple fact that lowerdimensional projections are in general not reversible  which means that with a fixed mapping matrix  we can certainly project a high-dimensional data point uniquely into a lowdimensional space  but it is not possible to uniquely recover the exact high-dimensional point with only its lowdimensional projection.1 our hppcox model thus has the following setup  assuming we use all the d features :
1. choose a lower dimensionality m   d;
1. locate a mapping matrix b of size d 〜 m;
1. project each individual xi （ rd into zi （ rm via mapping zi = b xi.
since there is an information loss when applying the mapping b  it is not possible to recover the exact xi given zi  even when b is known. therefore in the privacy-preserving setting  we can use this technique to  hide  the sensitive data xi  and only share with others the projected data zi along with the survival outcome {ti δi}. all the data from different parties are then combined  and a standard cox model can be learned using zi as the  reduced  predictors

1
 technically one must require that there does not exist a deterministic relationship between any two dimensions  but in general it is not difficult to verify.
for survival analysis. the whole algorithm is summarized in
algorithm 1.
　this hppcox model actually assumes the following hazard function  in the linear case :
　　　　　λhppcox t|xi  = λ1 t exp w b xi    1  where the weight vector w is only of length m  instead of d . when an optimal w  and baseline hazard λ 1 t  are found using the hppcox model  for a test individual x  the hazard function is calculated as λhppcox t|x   = λ 1 t exp w   b x  .
two important questions have been left out so far:
  which dimensionality m to choose 
  how to choose the mapping matrix b  and what if some original features are irrelevant  
a significant number of approaches choose the matrix b randomly  and refer to this as random projection privacypreserving data mining. apart from the simplicity and the nice properties as shown in   random projection in general yields inferior performance compared to the  non-privacypreserving  methods which share the data explicitly  also see section 1 for an empirical comparison . and so far there has been no approach addressing the feature selection problem in a privacy-preserving setting. in section 1  we address this problem by finding an optimal projection matrix
b （ rd1〜m  with d1 ＋ d  which is designed to have the following properties:
 i  relative distance preservation: by explicitly enforcing desired user-defined relations  in the form of linear constraints   e.g.  for cox regression it is desirable that the projected points preserve the explicit ordering imposed by the survival time in the projected space.
 ii  lower dimensionality in the projected space: by reducing the number of non-zero columns of b  data points are mapped into a lower dimensional space  which can be beneficial for model learning  specially in the presence of large datasets.
 iii  lower dimensionality in the input space  feature selection : by reducing the number of non-zero rows of b  from d to d1  irrelevant input features are not taken into account in the projection.
　to the best of our knowledge  there are no other methods that attempt to find a projection for privacy-preserving cox regression that is optimal in the sense described.
1 non-linear pp cox models
　the same lower-dimensional projection idea can be extended for kernel mapping   which makes it possible to derive non-linear privacy-preserving cox models. let φ x  be a mapping from the input space x （ rd into a rkhs space h. applying the lower-dimensional projection in h  we need to choose a matrix b and calculate b φ x   which contains the inner-product of φ x  with every column of b. let b be such that every column b : `  = φ b`  for some b` （ rd  we can calculate the inner-product as hφ x  b : ` i = hφ x  φ b` i = κ x b`  
algorithm 1 horizontal privacy-preserving cox model

require: k different parties  data sources   each holding a subset of individuals with survival outcomes. a same set of d predictive variables are shared among the parties.
1: choose m   d  and locate a matrix b of size d〜m. all the parties must agree on this matrix.
1: every party calculates zi = b xi for every individual xi  and shares a predictor profile {zi} and a survival outcome profile {ti δi} for its population.
1: all the data are combined  and a standard cox model is learned  specifically the weight vector w and the baseline hazard λ1 t   with zi's being the predictive variables.
ensure: the learned cox model is shared among all parties. survival prediction for a test individual x  is done by calculating z  = b x  and then applying the learned cox model.

with κ ， ，  being the reproducing kernel function. this reproducing property allows us to calculate the inner-product without an explicit form for φ ，   and motivates us to consider privacy-preserving methods for non-linear cox models. for instance in non-linear hppcox model  we need to take the following steps to get the projections:
1. specify a  non-linear  kernel function κ ， ， ;
1. choose a dimension m   n  the number of individuals;
1. locate m  fake individuals  {b`}  with each b` （ rd;
1. project each individual xi （ rd into zi （ rm via kernel function zi =  κ xi b1  ... κ xi bm   .
it is easily realized that when m   n  it is not possible to reconstruct xi from zi and κ ， ，   so privacy is preserved.
1. optimal projection
　as described in section 1  in order to preserve privacy we follow the standard methodology of applying a lossy transformation to the data. in this section we concentrate specifically on finding an optimal matrix that defines a linear transformation  projection . we represent the projection as a rank-deficient matrix b. instead of employing a random matrix b  we focus on identifying a lossy transformation that is optimal at maintaining certain properties of the data that  importantly  depend on the task at hand while still preserving data privacy.
　our approach for finding optimal projections is based on our approach for finding sparse matrices introduced in   where the optimal projection is found so that certain relationships among data points are preserved  while at the same time the dimensionality of the resulting data is reduced.
　in order to formally define what is meant by optimal projection  we require one additional ingredient. we define optimality in terms of how well the transformation preserves relationships among data points. the type of relationships we consider are quite general. they are of the type: data point i is more like data point j than data point k. thus  in order to measure the goodness of our projection  we use a set called t that is formed by t elements. each element is represented by a triplet  i j k  where i is an index for a data points  similarly for j and k   such that xi xj and xk satisfy the above relationship.
　the set t can be defined in multiple ways. a user can provide this as a special form of supervision or it can be given by an algorithm. note that no specific class label or distance measure is required  but could be used. as an example  an algorithm can simply use an attribute  dimension  of the data points and a simple partial order relation to define it. in the hppcox case t is naturally defined by the order suggested by the patient's survival time. more specifically  given three data points indexed i j k respectively  the goal is to preserve relationships of the form:
		 1 
for a set t of triplets for which this property must hold.
　the optimal projection matrix b can formally be defined as the solution to the following optimization problem:
d
rmaxd rm x1 bl = ~1 
b:	★
i=l

where 1 e  is the indicator function which returns the value 1 if the logical expression e evaluates to true and zero otherwise. the formulation is useful at formalizing the desired concept of an optimal projection. however  it is not practical since an efficient algorithm for finding b given the set t is not likely to exist.1 thus  we provide a different formulation that can be seen as an approximation based on a convex relaxation of the problem.
　let us define the d 〜 d matrix a = bb . the following formulation now focuses on finding an optimal a:
		 1 
note that in this problem we are attempting to make the norm of the columns/rows of a to be zero  thus making it a zero vector  through the use of an l1 norm regularization  and at the same time enforcing constraints that depend on the user/automatically obtained set of triplets. the parameter λ controls the balance between sparseness of a and inequality satisfaction. this can be obtained by tuning  depending on the problem at hand .
　the above formulation is convex  and can be solved via semi-definite programming  sdp . however  since our focus are data mining applications  we concentrate on large datasets  in terms both of the number of data points and their dimensionality  and thus: 1  the cardinality of t could be potentially large and similarly 1  the size of a increases quadratically with the input space dimensionality. it is wellknown that they sdp not scale well with the problem size. by contrast linear program  lp  solvers have much better scaling properties. in the following  we further modify the formulation to create an approximation that can be solved via lp.
　using the definitions x ij = vect xix j   and a = vect a   where vect   means  column-wise  alignment of all of the

1
 this would lead to a 1 mixed integer programming  mip  problem  known to be np-hard.
matrix elements in a column vector  it can be shown that the above optimization problem can be reformulated into:

		 1 
	a	=	a 
	  l c;l 1= c    slc	＋	alc ＋ slc
a
where the sdp constraint in formulation  1  was tighten into a diagonal dominance constraint using the auxiliary variables slc （ r  where s is a matrix of the same dimensionality as a. in this problem the last constraint is equivalent to diagonal dominance which implies positive semidefiniteness  cf. -theorem 1. . let us denote the data term in the first set of constraints as:
　　　　　cijk = x jj + x kk   1 x ij + x jk    1  for  i j k  （ t . note that for any  i j   in order to compute x ij we must know both xi and xj.
　this formulation does not take into account the distributed nature of the privacy-preserving problem in this paper since these constraints require all the data to be known and available in one location. for horizontal privacy preserving  we propose a protocol for computing the matrix b using data from all parties as follows. we assume that every party contributes with a set of relative relationships that must be preserved. similar to the general case  this set is denoted t  p  where p indexes the party.
　thus  each party p makes the following information available for  i j k  （ t  p :
	.	 1 
note that cijk p  does not reveal the original records xi xj xk because it combines data from these three records in a way that it is not possible to recover them back since the user does not know at any moment which three records are linearly combined.
　the resulting set of constrains provided by party p is incorporated into a large  combined  problem by any untrusted party given all the vectors cijk p  as follows:
	.	 1 
that is  the combined set of constraints in the final problem  formulation 1  is made of a combination of all sets t  p .
　note that while this allows imposing constraints for any triplet formed by records from the same party  it does not consider constraints that involve records across parties. this limitation is in general not critical since in most instances determining the relationship between records may require knowledge of the relevant records by the same entity  which is not the case for private information scenarios .
　it is important to note that formulation  1  provides a sparse solution  with zero columns/rows  for the symmetric d 〜 d matrix a. since our main interest is in b and a = bb   we can find the optimal d1 〜 m matrix b as follows. we first remove the zero rows/columns of a  which results in
a d1 〜d1 matrix a   i.e.  the rest d d1 features are irrelevant features . we then perform an eigenvalue decomposition for
table 1: summary of the benchmark datasets. n and d are the number of patients and predictive variables  respectively.
datasetndmissingcensoredsupport-11.1%1%support-11.1%1%support-11.1%1%support-11.1%1%melanoma11%1%a   i.e.  a  = vdv   with v an orthogonal matrix and d a diagonal matrix with non-negative diagonal entries sorted in a non-increasing order. the diagonal entries of d are all non-negative because a  is positive semidefinite. then we yield b = v d   where v  contains the first m columns of v  and d  contains the top-left m〜m submatrix of d. note that we should have m   d1 to ensure privacy is preserved  which means the feature selection step should obtain more than m features.
1. results on benchmark data
