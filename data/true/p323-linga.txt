new and emerging p1p applications require sophisticated range query capability and also have strict requirements on query correctness  system availability and item availability. while there has been recent work on developing new p1p range indices  none of these indices guarantee correctness and availability. in this paper  we develop new techniques that can provably guarantee the correctness and availability of p1p range indices. we develop our techniques in the context of a general p1p indexing framework that can be instantiated with most p1p index structures from the literature. as a specific instantiation  we implement p-ring  an existing p1p range index  and show how it can be extended to guarantee correctness and availability. we quantitatively evaluate our techniques using a real distributed implementation.
1.	introduction
　peer-to-peer  p1p  systems have emerged as a promising paradigm for structuring large-scale distributed systems. the main advantages of p1p systems are scalability  fault-tolerance  and ability to reorganize in the face of dynamic changes to the system. a key component of a p1p system is a p1p index. a p1p index allows applications to store  value  item  pairs  and to search for relevant items by specifying a predicate on the value. different applications have different requirements for a p1p index. we can characterize the index requirements of most p1p applications along the following three axes:
  expressiveness of predicates: whether simple equality predicates suffice in a p1p index  or whether more complex predicates such as range predicates are required.
  query correctness: whether it is crucial that the p1p index return all and only the data items that satisfy the predicate.
  system and item availability: whether it is crucial that the availability of the p1p index and the items stored in the index  are not reduced due to the reorganization of peers.
　for example  simple file sharing applications only require support for equality predicates  to lookup a file by name   and do not have strict correctness and availability requirements  it is not
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  baltimore  maryland  usa copyright 1 acm 1-1/1 ...$1.
catastrophic if a search occasionally misses a file  or if files are occasionally lost . internet storage applications require only simple equality predicates  but have strict requirements on correctness and availability  so that data is not missed or lost . digital library applications require complex search predicates such as range predicates  to search for articles within a date range   but do not have strict correctness and availability requirements. the most demanding applications are transaction processing and military applications  which require both complex range predicates  to search for objects within a region  and strong correctness/availability guarantees. as an example  consider the joint battlespace infosphere
 jbi   a military application that has high scalability and faulttolerance requirements. one of the potential uses of the jbi is to track information objects  which could include objects in the field such as enemy vehicles. a natural way to achieve the desired scalability and fault-tolerance is to store such objects as  value item  pairs in a p1p index  where the value could represent the geographic location of the object  in terms of its latitude and longitude   and the item could be a description of that object. clearly  the jbi requires support for range queries in order to find objects in a certain region. the jbi also requires strong correctness guarantees  so that objects are not missed by a query  and availability guarantees  so that stored objects are not lost .
　current p1p indices  however  do not satisfy the above application needs: while there has been some work on devising p1p indices that can handle expressive range predicates  1  1  1  1  1  1  1  1  1   there has been little or no work on guaranteeing correctness and availability in such indices. specifically  we are not aware of any p1p range index that guarantees that a query will not miss items relevant to a query. in fact  we shall later show scenarios whereby range indices  1  1  1  that are based on the chord ring   originally devised for equality queries  can miss query results for range queries  even when the index is operational. similarly  we are not aware of any range index that can provide provable guarantees on system and item availability.
　in this paper  we devise techniques that can provably guarantee query correctness  system availability and item availability in p1p range indices. at a high level  there are two approaches for guaranteeing correctness and availability. the first approach is to simply let the application handle the correctness and availability issues - this  for instance  is the approach taken by cfs  and past   which are applications built on top of the p1p equality indices chord  and pastry   respectively. however  this approach does not work in general for range indices because the application does not  and should not!  have control over various concurrent operations in a p1p range index  including index reorganization and peer failures. moreover  this approach exposes low-level concurrency details to applications and is also very error-prone due to subtle concurrent interactions between system components.
　we thus take the alternative approach of developing new correctness and availability primitives that can be directly implemented in a p1p index. specifically  we build upon the p1p indexing framework proposed by crainiceanu et al.   and embed novel techniques for ensuring correctness and availability directly into this framework. the benefits of this approach are that it abstracts away the dynamics of the underlying p1p system and provides applications with a consistent interface with provable correctness and availability guarantees. to the best of our knowledge  this is the first attempt to address these issues for both equality and range queries in a p1p index.
　one of the benefits of implementing our primitives in the context of a p1p indexing framework is that our techniques are not just applicable to one specific p1p index  but are applicable to all p1p indices that can be instantiated in the framework  including  1  1  1 . as a specific instantiation  we implement p-ring   a p1p index that supports both equality and range queries  and show how it can be extended to provide correctness and availability guarantees. we also quantitatively demonstrate the feasibility of our proposed techniques using a real distributed implementation of p-ring.
　the rest of the paper is organized as follows. in section 1  we present some background material  and in section 1  we outline our correctness and availability goals. in section 1 we present techniques for guaranteeing query correctness  and in section 1  we outline techniques for guaranteeing system and item availability. in section 1  we present our experimental results. in section 1  we discuss related work  and we conclude in section 1.
1.	background
　in this section  we first introduce our system model and the notion of a history of operations  which are used later in the paper. we then briefly review the indexing framework proposed by
crainiceanu et al.  and give an example instantiation of this framework for completeness. we use this instantiation in the rest of the paper to discuss problems with existing approaches and to illustrate our newly proposed techniques. we use the framework since it presents a clean way to abstract out different components of a p1p index  and it allows us to confine concurrency and consistency problems to individual components of the framework.
1	system model
　a peer is a processor with shared storage space and private storage space. the shared space is used to store the distributed data structure for speeding up the evaluation of user queries. we assume that each peer can be identified by a physical id  for example  its ip address . we also assume a fail-stop model for peer failures. a p1p system is a collection of peers. we assume there is some underlying network protocol that can be used to send messages reliably from one peer to another with known bounded delay. a peer can join a p1p system by contacting some peer that is already part of the system. a peer can leave the system at any time without contacting any other peer.
　we assume that each  data  item stored in a peer exposes a search key value from a totally ordered domain k that is indexed by the system. the search key value for an item i is denoted by i.skv. without loss of generality  we assume that search key values are unique  duplicate values can be made unique by appending the physical id of the peer where the value originates and a version number; this transformation is transparent to users . peers inserting items into the system can retain ownership of their items. in this case  the items are stored in the private storage partition of the peer  and only pointers to the items are inserted into the system.
p1p index	finditems predicate 
insertitem item  deleteitem item 

content router sendreceive msg  predicate 
replication managerdata storeinsertitems itemslist  deleteitems itemslist  getlocalitems  fault tolerant ringgetsucc   insertsucc peer  leave  figure 1. indexing framework
in the rest of the paper we make no distinction between items and pointers to items.
　the queries we consider are range queries of the form  lb ub    lb ub    lb ub  or  lb ub  where lb ub （ k. queries can be issued at any peer in the system.
　to specify and reason about the correctness and availability guarantees  we use the notion of a history of operations  1  1 .
definition 1  history h : history h is a pair  o ＋  where o is a set of operations and ＋ is a partial order defined on these operations.
　conceptually  the partial order ＋ defines a happened before relationship among operations. if op1 op1 （ o are two different operations in history h  and op1 ＋ op1  then intuitively  op1 finished before op1 started  i.e.  op1 happened before op1. if op1 and op1 are not related by the partial order  then op1 and op1 could have been executed in parallel.
　to present our results we also need the notion of a truncated history which is a history that only contains operations that happened before a certain operation.
definition 1  truncated history ho : given a history h =  oh ＋h  and an operation o （ oh  ho =  oho ＋ho  is a truncated history if oho = {o1 （ oh|o1 ＋h o} and  o1 o1 （ oho   o1 ＋h o1   o1 ＋ho o1  .
1	the p1p indexing framework from 
　a p1p index needs to reliably support the following operations: search  item insertion  item deletion  peers joining  and peers leaving the system. we now briefly survey the modularized indexing framework from   which is designed to capture most structured p1p indices. figure 1 shows the components of the framework  and their apis. the framework does not specify implementations for these components but only specifies functional requirements.
fault tolerant torus. the fault tolerant torus connects the peers in the system on a torus  and provides reliable connectivity among these peers even in the face of peer failures. for the purposes of this paper  we focus on a fault tolerant ring  a one-dimensional torus . on a ring  for a peer p  we can define the successor succ p   respectively  predecessor pred p   to be the peer adjacent to p in a clockwise  resp.  counter-clockwise  traversal of the ring. figure 1 shows an example of a fault tolerant ring. if peer p1 fails  then the ring will reorganize such that succ p1  = p1  so the peers remain connected. in addition to maintaining successors  each peer p in the ring is associated with a value  p.val  from a totally ordered domain pv. this value determines the position of a peer in the ring  and it increases clockwise around the ring  wrapping around in parenthesis. the value of a peer is introduced only for ease of exposition and is not required in the formal definition of a ring.
　figure 1 shows the fault tolerant ring api. when invoked on a peer p  p.getsucc returns the address of succ p .
p.insertsucc p1  makes p1 the successor of p. p.leave allows p to gracefully leave the ring  of course  p can leave the ring without making this call due to a failure . the ring also exposes events that can be caught at higher layers  such as successor changes  not shown in the figure . an api method need not return right away because of locks and other concurrency issues. each of the api methods is therefore associated with a start and an end operation. for example  initleave p  is the operation associated with the invocation of the api method p.leave   and leave p  is the operation used to signal the end of this api method. all the operations associated with the initiation and completion of the api methods  as well as the operations associated with the events raised by the ring form a history called an api ring history. the details can be found in .
data store. the data store is responsible for distributing and storing items at peers. the data store has a map m that maps the search key value i.skv of each item i to a value in the domain pv  the domain of peer values . an item i is stored in a peer p such that m i.skv  （  pred p .val p.val . in other words  each peer p is responsible for storing data items mapped to a value between pred p .val and p.val. we refer to the range  pred p .val p.val  as p.range. we denote the items stored at peer p as p.items.
　figure 1 shows an example data store that maps some search key values to peers on the ring. for example  peer p1 is responsible for search key values 1 and 1. one of the main responsibilities of the data store is to ensure that the data distribution is uniform so that each peer stores about the same number of items. different p1p indices have different implementations for the data store  e.g.  based on hashing   splitting  merging and/or redistributing  1  1   for achieving this storage balance. as shown in figure 1  the data store provides api methods to insert items into and delete items from the system. it also provides the api method p.getlocalitems   to get the items stored locally in peer p's data store.
　as with the api ring history  we can define the api data store history using the operations associated with the data store api methods. given an api data store history h and a peer p  we use rangeh p  to denote p.range in h and itemsh p  to denote p.items in h.
replication manager. the replication manager is responsible for reliably storing items in the system even in the presence of failures  until items are explicitly deleted. as an example  in figure 1  peer p1 stores items i1 and i1 such that m i1.skv  = 1 and m i1.skv  = 1. if p1 fails  these items would be lost even though the ring would reconnect after the failure. the goal of the replication manager is to handle such failures for example by replicating items so that they can be  revived  even if peers fail. content router. the content router is responsible for efficiently
	figure 1. chord ring	figure 1. p-ring data store
routing messages to relevant peers in the p1p system. as shown in the api  see figure 1   the relevant peers are specified by a contentbased predicate on search key values  and not by the physical peer ids. this abstracts away the details of storage and index reorganization from higher level applications.
p1p index. the p1p index is the index exposed to the end user. it supports search functionality by using the functionality of the content router  and supports item insertion and deletion by using the functionality of the data store. as with the api ring history and api data store history  we can define the api index history using the operations associated with the index api methods.
1	an example instantiation
　we now discuss the instantiation of the above framework using p-ring   an index structure designed for range queries in p1p systems. p-ring uses the fault tolerant ring of chord and the replication manager of cfs  and only devises a new data store and a content router for handling data skew. while the full details of p-ring are presented in   we concentrate only on features of p-ring that are common to many p1p range query index structures from the literature  1  1  1 : splitting  merging  and redistributing in order to balance the number of items at each peer. we would like to emphasize that while we use p-ring as a running example to illustrate query correctness  concurrency  and availability issues in subsequent sections  our discussion also applies to other p1p range indices proposed in the literature.
fault tolerant ring. p-ring uses the chord ring to maintain connectivity among peers . the chord ring achieves faulttolerance by storing a list of successors at each peer  instead of storing just a single successor. thus  even if the successor of a peer p fails  p can use its successor list to identify other peers to reconnect the ring and to maintain connectivity. figure 1 shows an example chord ring in which successor lists are of length 1  i.e.  each peer p stores succ p  and succ succ p   in its successor list . the successor lists are shown in the boxes next to the associated peers. chord also provides a way to maintain these successor lists in the presence of failures by periodically stabilizing a peer p with its first live successor in the successor list. p-ring also uses chord to maintain connectivity.
data store. ideally  we would like data items to be uniformly distributed among peers so that the storage load of each peer is about the same. most existing p1p indices achieve this goal by hashing the search key value of an item  and assigning the item to a peer based on this hashed value. such an assignment is  with high probability  very close to a uniform distribution of entries  1  1  1 . however  hashing destroys the value ordering among the search key values  and thus cannot be used to process range queries efficiently  for the same reason that hash indices cannot be used to handle range queries efficiently .
to solve this problem  range indices assign data items to peers

directly based on their search key value  i.e.  the map m is orderpreserving  in the simplest case it is the identity function . in this case  the ordering of peer values is the same as the ordering of search key values  and range queries can be answered by scanning along the ring. the problem is that now  even in a stable p1p system with no peers joining or leaving  some peers might become overloaded or underloaded due to skewed item insertions and/or deletions. there is a need for a way to dynamically reassign and maintain the ranges associated to the peers. range indices achieve this goal by splitting  merging and redistributing for handling item overflows and underflows in peers. let us give an example in the context of p-ring.
　the p-ring data store has two types of peers: live peers and free peers. live peers can be part of the ring and store data items  while free peers are maintained separately in the system and do not store any data items.1 the data store ensures that the number of items stored in each live peer is between sf and 1 ， sf  where sf is some storage factor  in order to balance storage between peers.
　whenever the number of items in a peer p's data store becomes larger than 1 ， sf  due to many insertions into p.range   it is said that an overflow occurred. in this case  p tries to split its assigned range  and implicitly its items  with a free peer  and to give a fraction of its items to the new peer. whenever the number of entries in p's data store becomes smaller than sf  due to deletions from p.range   it is said that an underflow occurred. in this case  p tries to merge with its successor in the ring to obtain more entries. in this case  the successor either redistributes its items with p  or gives up its entire range to p and becomes a free peer.
　as an illustration of a split  consider the data store shown in figure 1. assume that sf is 1  so each peer can have 1 or 1 entries. now  when an item i such that i.skv = 1 is inserted into the system  it will be stored in p1  leading to an overflow. thus  p1.range will be split with a free peer  and p1's items will be redistributed accordingly. figure 1 shows the data store after the split  where p1 split with the free peer p1  and p1 takes over part of the items p1 was originally responsible for  the successor pointers in the chord ring are also shown in the figure for completeness . as an illustration of merge  consider again figure 1 and assume that item i with t.skv = 1 is deleted from the system. in this case  there is an underflow at p1  and p1 merges with its successor  p1 and takes over all of p1's items; p1 in turn becomes a free peer. figure 1 shows the resulting system.
replication manager. p-ring uses cfs replication which works as follows. consider an item i stored in the data store at peer p. the replication manager replicates i to k successors of p. in this way  even if p fails  i can be recovered from one of the successors of p. larger values of k offer better fault-tolerance but have additional overhead. figure 1 shows a system in which items are replicated with a value of k = 1  the replicated values are shown in the top most box next to the peer .
content router. the p-ring content router is based on idea of constructing a hierarchy of rings that can index skewed data distributions. the details of the content router are not relevant here.
1.	goals
　we now turn to the main focus of this paper: guaranteeing correctness and availability in p1p range indices. at a high level  our techniques enforce the following design goals.
  query correctness: a query issued to the index should return all and only those items in the index that satisfy the query predicate.
  system availability: the availability of the index should not be reduced due to index maintenance operations  such as splits  merges  and redistributions .
  item availability: the availability of items in the index should not be reduced due to index maintenance operations  such as splits  merges  and redistributions .
　while the above requirements are simple and natural  it is surprisingly hard to satisfy them in a p1p system. thus  one approach is to simply leave these issues to higher level applications - this is the approach taken by cfs  and past   which are applications built on top of chord  and pastry   respectively  two index structures designed for equality queries. the downside of this approach is that it becomes quite complicated for application developers because they have to understand the details of how lower layers are implemented  such as how ring stabilization is done. further  this approach is also error-prone because complex concurrent interactions between the different layers  which we illustrate in section 1  make it difficult to devise a system that produces consistent query results. finally  even if application developers are willing to take responsibility for the above properties  there are no known techniques for ensuring the above requirements for p1p range indices.
　in contrast  the approach we take is to cleanly encapsulate the concurrency and consistency aspects in the different layers of the system. specifically  we embed consistency primitives in the fault tolerant ring and the data store  and provide handles to these primitives for the higher layers. with this encapsulation  higher layers and applications can simply use these apis without having to explicitly deal with low-level concurrency issues or knowing how lower layers are implemented  while still being guaranteed query consistency and availability for range queries.
　our proposed techniques differ from distributed database techniques  in terms of scale  hundreds to thousands of peers  as opposed to a few distributed database sites   failures  peers can fail at any time  which implies that blocking concurrency protocols cannot be used   and perhaps most importantly  dynamics  due to unpredictable peer insertions and deletions  the location of the items is not known a priori and can change during query processing .
　in the subsequent two sections  we describe our solutions to query correctness and system and item availability.
1.	query correctness
　we focus on query consistency for range queries  note that equality queries are a special case of range queries . we first formally define what we mean by query correctness in the context of the indexing framework. we then illustrate scenarios where query correctness can be violated if we directly use existing techniques. finally  we present our solutions to these problems. detailed definitions and proofs for all theorems stated in this section can be found in .
1	defining correct query results
　intuitively  a system returns a correct result for a query q if and only if the result contains all and only those items in the system that satisfy the query predicate. translating this intuition into a formal statement in a p1p system requires us to define which items are  in the system ; this is more complex than in a centralized system because peers can fail  can join  and items can move between peers during the duration of a query. we start by defining an index p as a set of peers p = {p1 ... pn}  where each peer is structured according to the framework described in section 1. to capture what it means for an item to be in the system  we now introduce the notion of a live item.
definition 1  live item : an item i is live in api data store history h  denoted by liveh i   iff  p （ p  i （ itemsh p  . in other words  an item i is live in api data store history h iff the peer with the appropriate range contains i in its data store. given the notion of a live item  we can define a correct query result as follows. we use satisfiesq i  to denote whether item i satisfies query q's query predicate.
definition 1  correct query result : given an api data store history h =  oh ＋h   a set r of items is a correct query result for a query q initiated with operation os and successfully completed with operation oe iff the following two conditions hold:
1.  i （ r   satisfiesq i  …  o （ oh   os ＋h o ＋h oe … liveho i     
1.  i   satisfiesq i  …  o （ oh os ＋h o ＋h oe … liveho i    i （ r    .
　the first condition states that only items that satisfy the query predicate and which were live at some time during the query evaluation should be in the query result. the second condition states that all items that satisfy the query predicate and which were live throughout the query execution must be in the query result.
1	incorrect query results: scenarios
　existing index structures for range queries evaluate a range query in two steps:  a  finding the peer responsible for left end of the query range  and  b  scanning along the ring to retrieve the items in the range. the first step is achieved using an appropriate content router  such as skipgraphs  or the p-ring  content router  and the related concurrency issues have been described and solved elsewhere in the literature  1  1 . we thus focus on the second step  scanning along the ring  and show how existing techniques can produce incorrect results.
　scanning along the ring can produce incorrect query results due to two reasons. first  the ring itself can be temporarily inconsistent  thereby skipping over some live items. second  even if the ring is consistent  concurrency issues in the data store can sometimes result in incorrect results. we now illustrate both of these cases using examples.
1.1	inconsistent ring
　consider the ring and data store shown in figure 1. assume that item i with m i.skv  = 1 is inserted into the system. since p1.range =  1   i will be stored in p1's data store. now assume that p1's data store overflows due to this insertion  and hence p1 splits with a new peer p and transfers some of its items to p. the new state of the ring and data store is shown in figure 1. at this point  p.range =  1  and p1.range =  1 . also  while p1's successor list is updated to reflect the presence of p  the successor list of p1 is not yet updated because the chord ring stabilization proceeds in rounds  and p1 will only find out about p when it next stabilizes with its successor  p1  in the ring.
　now assume that p1 fails. due to the replication manager  p takes over the range  1  and adds the data item i such that m i.skv  = 1 into its data store. the state of the system at this time is now shown in figure 1. now assume that a search q originates at p1 for the range  1 . since p1.val is the lower bound of the query range  p1 tries to forward the message to the first peer in its successor list  p1   and on detecting that it has failed  forwards it to the next peer in its successor list  p1 . p1 returns the items in the range  1   but the items in the range  1  are missed!  even though all items in this range are live - they are in p's data store.  this problem arises because the successor pointers for p1 are temporarily inconsistent during the insertion of p  they point to p1 instead of p . eventually  of course  the ring will stabilize and p1 will point to p as its successor  but before this ring stabilization  query results can be missed.
　at this point  the reader might be wondering whether a simple  fix  might address the above problem. specifically  what if p1 simply rejects the search request from p1  since p1 is not p1's predecessor  until the ring stabilizes  the problem with this approach is that p1 does not know whether p has also failed  in which case p1 is indeed p1's predecessor  and it should accept the message. again  the basic problem is that a peer does not have precise information about other peers in the system  due to the dynamics of the p1p system   and hence potential inconsistencies can occur. we note that the scenario outlined in figure 1 is just one example of inconsistencies that can occur in the ring - rings with longer successor lists can have other  more subtle  inconsistencies  for instance  when p is not the direct predecessor of p1 .
1.1	concurrency in the data store
　we now show how concurrency issues in the data store can produce incorrect query results  even if the ring is fully consistent. we illustrate the problem in the context of a data store redistribute operation; similar problems arise for data store splits and merges.
　consider again the system in figure 1 and assume that a query q with query range  1  is issued at p1. since the lower bound of p1.range is the same as the lower bound of the query range  the sequential scan for the query range starts at p1. the sequential scan operation first gets the data items in p1's data store  and then gets the successor of p1 in the ring  which is p1. now assume that the item i with m i.skv  = 1 is deleted from the index.
this causes p1 to become underfull  since it has no items left in its data store   and it hence redistributes with its successor p1. after the redistribution  p1 becomes responsible for the item i1 with m i1.skv  = 1  and p1 is no longer responsible for this item.
the current state of the index is shown in figure 1.
　now assume that the sequential scan of the query resumes  and the scan operation propagates the scan to p1  the successor of p1 . however  the scan operation will miss item i1 with m i1.skv  = 1  even though i1 satisfies the query range and was live throughout the execution of the query! this problem arises because of the concurrency issues in the data store - the range that p1's data store was responsible for changed while p1 was processing a query. consequently  some query results were missed.
p1p1	peer p
figure 1. peer p just inserted into the	figure 1. system after peer p1
search q originating at peer p1
	system	redistributes with peer p1
misses items in p1	ensuring correct query results
　we now present solutions that avoid the above scenarios and provably guarantee that the sequential scan along the ring for range queries will produce correct query results. the attractive feature of our solution is that these enhancements are confined to the ring and data store components of the architecture  and higher layers  both applications on top of the p1p system and other components of the p1p system itself  can be guaranteed correctness by accessing the components through the appropriate api. we first present a solution that addresses ring inconsistency  and then present a solution that addresses data store concurrency issues.
1.1	handling ring inconsistency
　as illustrated in section 1.1  query results can be incorrect if a peer's successor list pointers are temporarily inconsistent  we shall formally define the notion of consistency soon . perhaps the simplest way to solve this problem is to explicitly avoid this inconsistency by atomically updating the successor pointers of every relevant peer during each peer insertion. for instance  in the example in section 1.1  we could have avoided the inconsistency if p1's and p1's successor pointers had been atomically updated during p's insertion. unfortunately  this is not a viable solution in a p1p system because there is no easy way to determine the peers whose successor lists will be affected by an insertion since other peers can concurrently enter  leave or fail  and any cached information can become outdated.
　to address this problem  we introduce a new method for implementing insertsucc  figure 1  that ensures that successor pointers are always consistent even in the face of concurrent peer insertions and failures  peer deletions are considered in the next section . our technique works asynchronously and does not require any up-to-date cached information or global co-ordination among peers. the main idea is as follows. each peer in the ring can be in one of two states: joining or joined. when a peer is initially inserted into the system  it is in the joining state. pointers to peers in the joining state need not be consistent. however  each joining peer transitions to the joined state in some bounded time. we ensure that the successor pointers to/from joined peers are always consistent. the intuition behind our solution is that a peer p remains in the joining state until all relevant peers know about p - it then transitions to the joined state. higher layers  such as the data store  only store items in peers in the joined state  and hence avoid inconsistencies.
　we now formally define the notion of consistent successor pointers. we then present our distributed  asynchronous algorithm for insertsucc that satisfies this property for joined peers.
1.1 defining consistent successor pointers
　we first introduce some notation. let h be a given api ring history. this history induces a ring  denoted by rh. let ph be the set of live peers in joined state in the ring. p.succlisth is the successor list of peer p in h. p.succlisth.length is the length  number of pointers  of p.succlisth  and p.succlisth i   1 ＋ i   p.succlisth.length  refers to the i'th pointer in succlist. we define p.trimlisth as the trimmed copy of p.succlisth with only pointers corresponding to live peers in joined state in rh.
definition 1  consistent successor pointers : given an api ring history h  the ring rh induced by h has consistent successor pointers iff the following condition holds:
   p （ph    i   1 ＋ i   p.trimlisth.length  
succh p.trimlisth i   = p.trimlisth i + 1    … succh p  = p.trimlisth  .
　the above definition says that there are no peers in the ring between consecutive entries of p.trimlist i.e. p cannot have  missing  pointers to peers in the set ph. in our example in figure 1  the successor pointers are not consistent with respect to the set of all peers in the system because p1 has a pointer to p1 but not to p.
1.1 proposed algorithm
　we first present the intuition behind our insert algorithm. assume that a peer p1 is to be inserted as the successor of a peer p. initially  p1 will be in the joining state. eventually  we want p1 to transition to the joined state  without violating the consistency of successor pointers. according to the definition of consistent successor pointers  the only way in which converting p1 from the joining state to the joined state can violate consistency is if there exist joined peers px and py such that: px.succlist i  = p and px.succlist i + k  = py  for some k   1  and for all j 1   j   k  px.succlist i + j  1= p1. in other words  px has pointers to p and py but not to p1 whose value occurs between p.val and py.val.
　our algorithm avoids this case by ensuring that at the time p1 changes from the joining state to the joined state  if px has pointers to p and py  where py's pointer occurs after p's pointer   then it also has a pointer to p1. it ensures this property by propagating the pointer to p1 to all of p's predecessors until it reaches the predecessor whose last pointer in the successor list is p  which thus does not have a py that can violate the condition . at this point  it transitions p1 from the joining to the joined state. propagation of p1 pointer is piggybacked on the chord ring stabilization protocol  and hence does not introduce new messages.
 1: writelock succlist  statelist
1: send a message to p indicating it is now joined 1: statelist.update front joined 
1: releaselock statelist  succlist
1: statelist.push front joined 
1: succlist.pop back    statelist.pop back  
1: // handle joining peers
1: listlen = succlist.length
1: if statelist listlen   1  == joining then
1:	succlist.pop back  ; statelist.pop back  
1: else if statelist listlen   1  == joining then
1:	send an ack to succlist listlen   1 
1: end if
1: releaselock statelist  succlist

　algorithms 1 and 1 show the pseudocode for the insertsucc method and the modified ring stabilization protocol  respectively. in the algorithms  we assume that in addition to succlist  each peer has a list called statelist which stores the state  joining or joined  of the corresponding peer in succlist. we walk through the algorithms using an example.
　consider again the example in figure 1  where p is to be added as a successor of p1. the insertsucc method is invoked on p1 with a pointer to p as the parameter. the method first acquires a write lock on succlist and statelist  inserts p as the first pointer in p1.succlist  thereby increasing its length by one   and inserts a corresponding new entry into p1.statelist with value joining  lines 1   1 in algorithm 1 . the method then releases the locks on succlist and statelist  line 1  and blocks waiting for an acknowledgment from some predecessor peer indicating that it is safe to transition p from the joining state to the joined state  line 1 . the current state of the system is shown in figure 1  joining list entries are marked with a  *  .
　now assume that a ring stabilization occurs at p1. p1 will first acquire a read lock on its succlist and statelist  contact the first non-failed entry in its successor list  p1  to get p1's succlist and statelist  lines 1   1 in algorithm 1 . p1 then acquires a write lock on its succlist and statelist  and copies over the succlist and statelist it obtained from p1  lines 1   1 . p1 then inserts p1 as the first entry in succlist  increasing its length by 1  and also inserts the corresponding state in statelist  the state will always be joined because joining nodes do not respond to ring stabilization requests . p1 then removes the last entries in succlist and statelist  lines 1  to ensure that its lists are of the same length as p1's lists. the current state of the system is shown in figure 1.
 
	figure 1. after	figure 1. propagation and
	1
figure 1. completed figure 1. naive merge insertsucc leads to reduced reliability
　p1 then checks whether the state of the last entry is joining; in this case it simply deletes the entry  lines 1   1  because it is far enough from the joining node that it does not need to know about it  although this case does not arise in our current scenario for p1 . p1 then checks if the state of the penultimate peer  p  is joining - since this is the case in our scenario  p1 sends a acknowledgment to the peer preceding the penultimate peer in the successor list  p1  indicating that p can be transitioned from joining to joined since all relevant predecessors know about p  lines 1   1 . p1 then releases the locks on its lists  line 1 .
　the insertsucc method of p1  on receiving a message from p1  first send a message to p indicating that it is now in the joined state  line 1 . p1 then changes the state of its first list entry  p  to joined and removes the last entries from its lists in order to shorten them to the regular length  lines 1   1 . the final state after p is inserted into the ring and multiple ring stabilizations have occurred is shown in figure 1.
　one optimization we implement for the above method is to proactively contact the predecessor in the ring whenever insertsucc is in progress  to trigger ring stabilization. this expedites the operation since it is no longer limited by the frequency of the ring stabilization process.
　we can define a pepper ring history to capture our implementation of the ring api  including the operations in algorithms 1 and 1. we can prove the following theorem.
theorem 1  consistent successor pointers : given a pepper ring history ph  the ring rph induced by ph has consistent successor pointers.
1.1	handling data store concurrency
　recall from the discussion in section 1.1 that even if the ring is fully consistent  query results can be missed due to concurrency issues at the data store. essentially  the problem is that the range of a peer can change while a query is in progress  causing the query to miss some results. how do we shield the higher layers from the concurrency details of the data store while still ensuring correct query results 
　our solution to this problem is as follows. we introduce a new api method for the data store called scanrange. this method has the following signature: scanrange lb  ub  handlerid  param   where  1  lb is the lower bound of the range to be scanned   1  ub is the upper bound of the range to be scanned   1  handlerid is the id of the handler to be invoked on every peer p such that p.range intersects  lb ub   i.e.  p's range intersects the scan range   and  1  param is the parameter to be passed to the handlers. the scanrange method should be invoked on the data store of the peer p1 such that lb （ p1.range  i.e.  the first peer whose range intersects the scan range . the start and end operations associated with scanrange are initscanrangei p1 lb ub  and donescanrangei pn lb ub  for some i （ n. the index i is used to distinguish multiple invocations of the api method with the same signature. the scanrange method causes the appropriate handler to be invoked on every peer p such that p.range intersects  lb ub . scanrangei p p1 r  is the operation in the api
data store history that is associated with the invocation of the appropriate handler at peer p. here  r is the subset of p.range that intersects with  lb ub .
　scanrange handles all the concurrency issues associated with the data store. consequently  higher layers do not have to worry about changes to the data store while a scan is in progress. further  since scanrange allows applications to register their own handlers  higher layers can customize the scan to their needs  we shall soon show how we can collect range query results by registering appropriate handlers .
　we now introduce some notation before we define the notion of scanrange correctness. we use scanops i  to denote the set of scanrangei p p1 r  operations associated with the ith invocation of scanrange. we use rangeset i  = {r| p1 p1 scanrangei p1 p1 r  （ scanops i } to denote the set of ranges reached by scanrange. we use r1 r1 to denote that range r1 overlaps with range r1 and we use r1 “ r1 to denote the union of range r1 with range r1.
we can define scanrange correctness as follows:
definition 1  scanrange correctness : an api data store history h =  oh ＋h  is said to satisfy scanrange correctness iff  i （ n  lb ub  p1 （ p oe = donescanrangei p1 lb ub  （
oh  
1. os = initscanrangei p1 lb ub  ＋h oe
1.  o （ scanops i   p  r o = scanrangei p p1 r 
  os ＋h o ＋h oe … r   rangeho p 
1.  ol om （ scanops i  ol =1	om …  pl pm  rl rm ol = scanrangei pl p1 rl  … om = scanrangei pm p1 rm 
    ol 1 om 
1.  lb ub  = “r（rangeset i  r 
　condition 1 states that the initiate operation for scanrange should occur before the completion operation. condition 1 states that range r used to invoke the handler at peer p is a subset of p's range. condition 1 states that ranges rl and rm used to invoke the handlers at distinct peers pl and pm  respectively  are nonoverlapping. finally  condition 1 states that the union of all ranges used to invoke the handlers is  lb ub .
1.1 implementing scanrange
　we present now our implementation for the scanrange api method. algorithm 1 shows the pseudocode for the scanrange method executed at a peer p. the method first acquires a read lock on the data store range  to prevent it from changing  and then checks to make sure that lb （ p.range  i.e.  p is the first peer in the range to be scanned  lines 1 . if the check fails  scanrange is aborted  lines 1 . if the check succeeds  then the helper method processhandler is invoked.
algorithm 1 : p.scanrange lb ub handlerid param 
1: readlock range
1: if lb 1（ p.range then
1:	// abort scanrange
1:	releaselock range
1: else
1:	// p is the first peer in scan range
1:	p.processhandler r handlerid param 
1: end if
algorithm 1 : p.processhandler lb ub handlerid param 
1: // invoke appropriate handler with relevant range r
1: get handler with id handlerid 1: r =  lb ub  ” p.range
1: newparam = handler.handle r param 
1: // forward to successor if required
1: if ub 1（ p.range then
1:	psucc = p.ring.getsucc  
1:	psucc.processscan lb ub handlerid newparam 
 1: end if 1: releaselock range

processhandler  algorithm 1  first invokes the appropriate handler for the scan  lines 1   and then checks to see whether the scan has to be propagated to p's successor  line 1 . if so  it invokes the processscan method on p's successor.
　algorithm 1 shows the code that executes when psucc.processscan is invoked by p.processhandler. processscan asynchronously invokes the processhandler method on psucc  and returns. consequently  p holds on to a lock on its range only until psucc locks its range; once psucc locks its range  p can release its lock  thereby allowing for more concurrency. note that p can later split  merge  or redistribute  but this will not produce incorrect query results since the scan has already finished scanning the items in p.
　we now illustrate the working of these algorithms using an example. assume that scanrange 1 h1 param1  is invoked in p1 in figure 1. p1 locks its range in scanrange  to prevent p1's range from changing   invokes the handler corresponding to h1 in processhandler  and then invokes processscan on p1. p1 locks its range in processscan  asynchronously invokes processhandler and returns. since p1.processscan returns  p1 can now release its lock and participate in splits  merges  or redistributions. however  p1 holds onto a lock on its range until p1 handler is finished executing. thus  the algorithms ensure that a peer's range does not change during a scan  but releases locks as soon as the scan is propagated to the peer's successor  for maximum concurrency.
　we can define a pepper data store history to capture our implementation of the data store api augmented with the new operation scanrange . we can prove the following correctness theorem.
theorem 1  scanrange correctness : any pepper data store history satisfies the scanrange correctness property.
　using the scanrange method  we can easily ensure correct results for range queries by registering the appropriate handler. algorithm 1 shows the algorithm for evaluating range queries. lb and ub represent the lower and upper bounds of the range to be scanned  and pid represents the id of the peer to which the final result is to be sent. as shown  the algorithm simply invokes the scanrange method with parameters lb  ub  the id of the range query handler  and a parameter for that handler. the id of the peer pid that the result should be sent to is passed as a parameter to the range query algorithm 1 : p.processscan lb ub handlerid param  1: readlock range
 1: invoke p.processhandler lb ub handlerid param  asynhandler. the range query handler  algorithm 1  invoked with range r at a peer p works as follows. it first gets the items in p's data store that are in range r and hence satisfy the query result  lines 1 . then  it sends the items and the range r to the peer pid  line
1 .
　using the above implementation of a range query  the inconsistency described in section 1.1 cannot occur because p1's range cannot change  and hence redistribution cannot happen  when the search is still active in p1. we can prove the following correctness theorem:
theorem 1  search correctness : given a pepper data store history ph  all query results produced in ph are correct  as per the definition of correct query results in section 1 .
1.	system and item availability
　we now address system availability and item availability issues. intuitively  ensuring system availability means that the availability of the index should not be reduced due to routine index maintenance operations  such as splits  merges  and redistributions. similarly  ensuring item availability means that the availability of items should not be reduced due to maintenance operations. our discussion of these two issues is necessarily brief due to space constraints  and we only illustrate the main aspects and sketch our solutions.
1	system availability
　an index is said to be available if its fault tolerant ring is connected. the rationale for this definition is that an index can be operational  by scanning along the ring  so long as its peers are connected. the chord fault tolerant ring provides strong availability guarantees when the only operations on the ring are peer insertions  splits  and failures . these availability guarantees also carry over to our variant of the fault tolerant ring with the new implementation of insertsucc described earlier because it is a stronger version of the chord's corresponding primitive  it satisfies all the properties required for the chord proofs . thus  the only index maintenance operation that can reduce the availability of the system is the merge operation in the data store  which translates to the leave operation in the fault tolerant ring. note that the redistribute operation in the data store does not affect the ring connectivity.
　we show that a naive implementation of leave  which is simply removing the merged peer from the ring  reduces system availability. we then sketch an alternative implementation for the leave that provably does not reduce system reliability. using this new implementation  the data store can perform a merge operation without knowing the details of the ring stabilization  while being guaranteed that system availability is not compromised.
naive leave reduces system availability: consider the system in figure 1 in which the length of the successor list of each peer is 1. without a leave primitive  this system can tolerate one failure per peer stabilization round without disconnecting the ring  since at most one of a peer's two successor pointers can become invalid before the stabilization round . we now show that in the presence algorithm 1 : p.rangequeryhandler r pid 
1: // get results from p's data store
1: find items in p's data store in range r 1: send   items r   to peer pid
 	 
figure 1. final ack
figure 1. controlled leave received at peer p. peer p is
of peer p good to go.
of the naive leave  a single failure can disconnect the ring. thus  leave reduces the availability of the system. assume that leave is invoked on p  and p immediately leaves the ring. now assume that p1 fails  this is the single failure . the current state of the system is shown in figure 1  and as we can see  the ring is disconnected since none of p1's successor pointers point to peers in the ring.
solution sketch: the reason the naive implementation of leave reduced availability is that pointers to the peer p leaving the ring become invalid. hence  the successor lists of the peers pointing to p effectively decreases by one  thereby reducing availability. to avoid this problem  our solution is to increase the successor list lengths of all peers pointing to p by one. in this way  when p leaves  the availability of the system is not compromised. as in the insertsucc case  we piggyback the lengthening of the successor lists on the ring stabilization protocol. this is illustrated in the following example.
　consider figure 1 in which leave is invoked on p. during the next ring stabilization  the predecessor of p  which is p1  increases its successor list length by 1. the state of the system is shown in figure 1. during the next ring stabilization  the predecessor of p1  which is p1  increases its successor list length by 1. since p1 is the last predecessor that knows about p  p1 sends a message to p indicating that it is safe to leave the ring. the state of the system at this point is shown in figure 1. it is easy to see that if p leaves the ring at this point  a single failure cannot disconnect the ring  as in was the case in the previous example. we can formally prove that the new algorithm for leave does not reduce the availability of the system.
1	item availability
we first formalize the notion of item availability in a p1p index.
　we represent the successful insertion of an item i at peer p with operation insertitem i p  and deletion of an item i1 at peer p1 with operation deleteitem i1 p1 .
definition 1  item availability : given an api index history h 
an index p is said to preserve item availability iff
 i    p （ p   insertitem i p  （ oh   … 1  p1 （ p   deleteitem i p1  （ oh     liveh i   .
　in other words  if item i has been inserted but not deleted wrt to api index history h then i is a live item.
　the cfs replication manager  implemented on top of the chord ring provides strong guarantees  on item availability when the only operations on the ring are peer insertions and failures  and these carry over to our system too. thus  the only operation that

	causing loss of item 1	one additional hop.
could compromise item availability is the leave operation invoked on a merge. we now show that using the original cfs replication manager in the presence of merges does compromise item availability. we then describe a modification to the cfs replication manager and its interaction with the data store that ensures the original guarantees on item availability.
scenario that reduces item availability: consider the system in figure 1. the top box associated with each peer represents the items replicated at that peer  cfs replicates items along the ring . in this example  each item is replicated to one successor along the ring; hence  the system can tolerate one failure between replica refreshes. we now show how  in the presence of data store merges  a single failure can compromise item availability. assume that peer p1 wishes to merge with p1 in figure 1. p1 thus performs an leave operation  and once it is successful  it transfers its data store items to p1 and leaves the system. the state of the system at this time is shown in figure 1. if p1 fails at this time  this is the single failure   the item i such that m i.skv  = 1 is lost.
solution sketch: the reason item availability was compromised in the above example is because when p1 left the system  the replicas it stored were lost  thereby reducing the number of replicas for certain items in the system. our solution is to replicate the items stored in the merging peer p's replication manager for one additional hop before p leaves the system. this is illustrated in figure 1  where before p1 merges with p1  it creates one more replica for items in its data store and replication manager  at one additional peer. when p1 finally merges with p1 and leaves the system  the number of replicas is not reduced  thereby preserving item availability. we can prove that the above scheme preserves item availability even in the presence of concurrent splits  merges  and redistributions.
1.	experimental evaluation
　we had two main goals in our experimental evaluation:  1  to demonstrate the feasibility of our proposed query correctness and availability algorithms in a dynamic p1p system  and  1  to measure the overhead of our proposed techniques. towards this goal  we implemented the p-ring index  along with our proposed correctness and availability algorithms  in a real distributed environment with concurrently running peers. we used this implementation to measure the overhead of each of our proposed techniques as compared to the naive approach  which does not guarantee correctness or availability.
1	experimental setup
　we implemented the p-ring index as an instantiation of the indexing framework  section 1 . the code was written in c++ and all experiments were run on a cluster of workstations  each of which had 1ghz processor  1gb of main memory and at least 1gb of disk space. all experiments were performed with 1 peers running concurrently on 1 machines  with 1 peers per machine . the machines were connected by a local area network.
　we used the following default parameter values for our experiments. the length of the chord fault-tolerant ring successor list was 1  which means that the ring can tolerate up to 1 failures without being disconnected if the ring is fully consistent . the ring stabilization period was 1 seconds. we set the storage factor of the p-ring data store to be 1  which means that it can hold between 1 and 1 data items. the replication factor in the replication manager is 1  which means that each item is replicated 1 times. we vary these parameters too in some of the experiments.
　we ran experiments in two modes of the system. the first mode was the fail-free mode  where there were no peers failures  although peers are still dynamically added and splits  merges  and redistributes occur in this state . the second was the failure mode  where we introduced peer failures by killing peers. for both modes  we added peers at a rate of one peer every 1 seconds  and data items were added at the rate of 1 items per second. we also vary the rate of peer failures in the failure mode.
1	implemented approaches
　we implemented and evaluated all four techniques proposed in this paper. specifically  we evaluate  1  the insertsucc operation that guarantees ring consistency   1  the scanrange operation that guarantees correct query results   1  the leave operation that guarantees system availability  and  1  the replication to additional hop operation that guarantees item availability. for scanrange  we implemented a synchronous version where the processhandler is invoked synchronously at each peer  see algorithm 1 .
　one of our goals was to show that the proposed techniques actually work in a real distributed dynamic p1p system. the other goal was to compare each solution with a naive approach  that does not provide correctness or availability guarantees . specifically  for the insertsucc operation  we compare it with the naive insertsucc  where the joining peer simply contacts its successor and becomes part of the ring. for the scanrange operation  we compare it with the naive range query method whereby the application explicitly scans the ring without using the scanrange primitive. for the leave operation  we compare with the naive approach where the peer simply leaves the system without notifying other peers. finally  for the replication to additional hop operation  we compare with the naive approach without additional replication.
1	experimental results
　we now present our experimental results. we first present results in the fail-free mode  and then present results in the failure mode.
1.1	evaluating insertsucc
　in this section we quantify the overhead of our insertsucc when compared to the naive insertsucc. the performance metric used is the time to complete the operation; this time is averaged over all such operations in the system during the run of the experiment.
　we vary two parameters that affect the performance of the operations. the first parameter is the length of the ring successor list. the longer the list  the farther insertsucc has to propagate information before it can complete. the second is the ring stabilization period. the longer the stabilization period  the slower information about joining peers propagates due to stabilization.

　figure 1. overhead of insertsucc figure 1. overhead of insertsucc figure 1. overhead of scanrange the time for our insertsucc increases linearly with the successor list length  while the time for the naive insertsucc remains constant. this is to be expected because the naive insertsucc only contacts the successor  while our insertsucc propagates information to as many predecessors as the length of the successor list. second  perhaps surprisingly  the rate of increase of the time for our insertsucc operation is very small; this can be attributed to the optimization discussed in section 1.1  where we proactively contact predeces-　figure 1 shows the effect of varying the ring successor list length. there are several aspects to note about this figure. first  sors instead of only relying on the stabilization. finally  an encouraging result is that the cost of our insertsucc is of the same ball park as that of the naive insertsucc; this means that users do not pay too high a price for consistency.
　figure 1 shows the result of varying the ring stabilization frequency. the results are similar to varying the successor list length. varying the ring stabilization period also has less of an effect on our insertsucc because of our optimization of proactively contacting predecessors.
1.1	evaluating scanrange
　in this section  we investigate the overhead of using scanrange when compared to the naive approach of the application scanning the range by itself. since the number of messages needed to complete the operation is the same for both approaches  we used the elapsed time to complete the range search as the relevant performance metric. we varied the size of the range to investigate its effect on performance  and averaged the elapsed time over all the searches requiring the same number of hops along the ring. each peer generates searches for ranges of different sizes  and we measured the time needed to process the range search  once the first peer with items in the search range was found. this allows us to isolate the effects of scanning along the ring.
　figure 1 shows the performance results. as shown  there is practically no overhead to using scanrange as compared with the application level search; again  this indicates that the price of consistency is low. to our surprise  the time needed to complete the range search  for either approach  does not increase significantly with the increased number of hops. on further investigation  we determined that this was due to our experiments running on a cluster in the local area network. in a wide area network  we expect the time to complete a range search to increase significantly with the number of hops.
1.1	evaluating leave and replicate to additional hop
　in this section  we investigate the overhead of the proposed leave and replicate to additional hop operations as compared to the naive approach of simply leaving the ring without contacting any peer. for this experiment  we start with a system of 1 peers and delete items from the system that cause peers to merge and leave the ring.

1	1
	1	1	1	1	1	1	1
	successor list length	failure rate  failures per 1 sec 
figure 1. insertsucc in failure
figure 1. overhead of leave
mode
　we measure the time elapsed for three operations:  1  the leave operation in the ring  and  1  the merge operation in the data store  which includes the time for replicate to additional hop   and  1  the naive leave. figure 1 shows the variation of the three times with successor list length. note the log scale on y-axis. we observe that the leave and merge operations take approximately 1 msec  and do not constitute a big overhead. the naive version takes only 1 msec since it simply leaves the system.
1.1	evaluation in failure mode
　we have so far studied the overhead of our proposed techniques in a system without failures. we now look at how our system behaves in a system with failures. in particular  we measure the variation of the average time taken for an insertsucc operation with the failure rate of peers. the system setting is as follows: we insert one peer every three seconds into the system  and we insert two items every second. we use the default successor list length  1  and default ring stabilization period  1 sec .
　figure 1 shows the variation of average time taken for a insertsucc operation with the peer failure rate. we observe that even in the case when the failure rate is as high as 1 in every 1 seconds  the time taken for insertsucc is not prohibitive  about 1 seconds compared to 1 seconds in a stable system .
1.	related work
　there has been a flurry of recent activity on developing indices for structured p1p systems. some of these indices can efficiently support equality queries  e.g.   1  1  1    while others can support both equality and range queries  e.g.   1  1  1  1  1  1  1  1  1 . this paper addresses query correctness and availability issues for such indices  which have not been previously addressed for range queries. besides structured p1p indices  there are unstructured p1p indices such as  1  1 . unstructured indices are robust to failures  but do not provide guarantees on query correctness and item availability. since one of our main goals was to study correct-
ness and availability issues  we focus on structured p1p indices.
　there is a rich body of work on developing distributed index structures for databases  e.g.   1  1  1  1  1 . however  most of these techniques maintain consistency among the distributed replicas by using a primary copy  which creates both scalability and availability problems when dealing with thousands of peers. some index structures  however  do maintain replicas lazily  e.g.   1  1  1  . however  these schemes are not designed to work in the presence of peer failures  dynamic item replication and reorganization  which makes them inadequate in a p1p setting. in contrast  our techniques are designed to handle peer failures while still providing correctness and availability guarantees.
　besides indexing  there is also some recent work on other data management issues in p1p systems such as complex queries  1  1  1  1  1  1 . a correctness condition for processing aggregate queries in a dynamic network was proposed in . an interesting direction for future work is to extend our techniques for query correctness and system availability to work for other complex queries such as keyword searches and joins.
1.	conclusion
　we have introduced the first set of techniques that provably guarantee query correctness and system and item availability for range index structures in p1p systems. our techniques provide provable guarantees  and they allow applications to abstract away all possible concurrency and availability issues. we have implemented our techniques in a real distributed p1p system  and quantified their performance.
　as a next step  we would like to extend our approach to handle more complex queries such as joins and keyword searches.
1.	acknowledgements
this work was supported by nsf grants crcd-1  itr-
1  iis- 1  and by afosr muri grant f1-1. any opinions  findings  conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect the views of the sponsors.
