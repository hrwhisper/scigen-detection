we describe methods to solve partially observable markov decision processes  pomdps  with continuous or large discrete observation spaces. realistic problems often have rich observation spaces  posing significant problems for standard pomdp algorithms that require explicit enumeration of the observations. this problem is usually approached by imposing an a priori discretisation on the observation space  which can be sub-optimal for the decision making task. however  since only those observations that would change the policy need to be distinguished  the decision problem itself induces a lossless partitioning of the observation space. this paper demonstrates how to find this partition while computing a policy  and how the resulting discretisation of the observation space reveals the relevant features of the application domain. the algorithms are demonstrated on a toy example and on a realistic assisted living task.
1 introduction
partially observable markov decision processes  pomdps   provide a rich framework for planning under uncertainty. in particular  pomdps can be used to robustly optimize the course of action of complex systems despite incomplete state information due to poor or noisy sensors. for instance  in mobile robotics   spoken-dialog systems  1; 1  and vision-based systems   pomdps can be used to optimize controllers that rely on the partial and noisy information provided by various sensors such as sonars  laser-range finders  video cameras and microphones. unfortunately  to date  the use of pomdps in such real-world systems has been limited by the lack of scalable algorithms capable of processing rich and continuous sensor observations.
¡¡while model-free approaches such as neuro-dynamic programming   monte carlo sampling  and stochastic gradient descent  1; 1; 1  can tackle pomdps with arbitrary observation spaces  they tend to require a large amount of simulation or a priori knowledge to restrict the space of policies  which reduces the need for simulation . significant progress has also been made in developing approximate scalable algorithms for model-based pomdps with large state spaces  1; 1  and complex policy spaces  1; 1; 1   however model-based algorithms cannot tackle problems with continuous nor large discrete observation spaces.
¡¡in this paper we study and propose new algorithms for model-based pomdps with continuous or large discrete observation spaces. we first demonstrate that the complexity of observation spaces can often be significantly reduced without affecting decision quality. intuitively  observations provide information to the decision maker for choosing a future course of action. when the same course of action is chosen for two different observations  these observations are indistinguishable from a decision making point of view  and can therefore be aggregated. hence  when a policy is composed of a small set of conditional plans  conditional on the observations   it is possible to partition the observation space in a small number of regions corresponding to the relevant features of the observation space for decision making. many systems tackle the feature detection problem separately from the decision making problem  first building a set of features  then computing a policy based on those observation features. in this paper  we demonstrate how the decision problem can be used to automatically define a set of relevant features that are sufficient to find an optimal policy.
¡¡the paper first provides some background on pomdps in sect. 1  followed by a discussion of the partitioning of the observation space in sect. 1. sects. 1 and 1 discuss methods for the one-dimensional and multi-dimensional cases  respectively. sect. 1 reports experiments with an assisted living task.
1 partially observable mdps
formally  a pomdp is specified by a tuple hs a z t z r ¦Ã hi which consists of a set s of states s capturing the relevant features of the world  a set a of actions a corresponding to the possible control decisions  a set z of observations corresponding to sensor readings  a transition function t s a s1  = pr s1|s a  encoding the stochastic dynamics of the world  an observation function z a s1 z  = pr z|a s1  indicating how sensor readings relate to physical states of the world  a reward function r s a  encoding the objectives of the system  a discount factor ¦Ã  between 1 and 1  and a planning horizon h  assumed to be infinite in this paper . we assume that states and actions are discrete  but observations can be continuous or discrete.
while it is common for observations to be continuous because sensors often provide continuous readings  states are often abstract  unobservable quantities. in the case of user modeling problems and event recognition problems  sates are often discrete. for continuous observations  z a s1 z  = pdf z|a s1  is a probability density function. since states are not directly observable  the decision maker's belief about the current state is represented by a probability distribution b s  = pr s  called belief state. after executing a and observing z  the belief state b is revised according to
bayes' theorem:.
¡¡to illustrate the concepts we will present  we use the classic tiger problem   in which the decision maker is faced with two doors. a small reward lies behind one door  but a large cost  a tiger  lies behind the other. the decision maker can either open a door  or listen for the tiger. listening gives the decision maker information from which she can infer the location of the tiger. in the original  discrete  version of this problem  two possible observations result from listening  left or right   and these observations correspond with the true location of the tiger with probability 1. in the continuous version we will discuss here  the decision maker has access to the original microphone array measurement that  noisily  locates the tiger in the continuous horizontal dimension  from the far left to the far right .
¡¡at each time step  the decision maker picks an action to execute based on the information gathered in past actions and observations. we can represent the decision maker's possible strategies by a set cp of conditional plans cp which correspond to decision trees. fig. 1 a  shows the decision tree of a k-step conditional plan for the tiger pomdp with discrete observations. nodes are labeled by actions and edges are labeled by observations. the execution of a conditional plan starts at the root  performing the actions of the nodes traversed and following the edges labeled with the observations received from the environment. for example  the conditional plan in fig. 1 a   will lead to opening a door if two successive observations confirm the same tiger location. we can define recursively a k-step conditional plan cpk = ha osk 1i as a tuple consisting of an action a with an observation strategy osk 1 : z ¡ú cpk 1 that maps observations to  k   1 -step conditional plans. for pomdps with continuous observations  conditional plans cp are decision trees with infinitely many branches and observation strategies os are continuous functions. the value function ¦Ácp b  of a conditional plan cp is the expected sum of discounted rewards that will be earned when starting in belief state b. this value function is often called an ¦Á-vector since it is linear with respect to the belief space and therefore parameterized by a vector ¦Ácp s  that has one component per state  i.e.  ¦Ácp b  = ps b s ¦Ácp s  .
fig. 1 b  shows the 1 ¦Á-vectors corresponding to the conditional plans shown in fig. 1 a . the ¦Á-vectors corresponding to the conditional plans starting with an open door action  ¦Á1 and ¦Á1  have high value at one extreme of the belief space  when the certainty about the tiger location is high   but very low value at the other extreme.
¡¡a collection of conditional plans forms a policy ¦Ð. the value function v ¦Ð of a policy ¦Ð is the best value achieved by any of its conditional plans  i.e.  v ¦Ð b  = maxcp¡Ê¦Ð ¦Ácp b  .

	 a 	 b 
figure 1: a  tree representation of a three-step conditional plan for the simple tiger problem  starting with a uniform belief about the tiger location. b  corresponding value function of composed of 1 ¦Á-vectors
fig. 1 b  shows the value function of a policy for the tiger problem  composed of 1 conditional plans  each of which is highest for some part of the belief space. the goal is to find an optimal policy ¦Ð  with the highest value  i.e.  v ¦Ð  b  ¡Ý v ¦Ð b   ¦Ð . the optimal value function v   b  for an infinite planning horizon can be computed by value iteration which computes successive approximations v k b  by dynamic programming

 where	ra b 	=	ps b s r s a  	pr z|b a 	= 	is	the	revised
belief state after executing a and observing z. when v k is formed by the set of ¦Ácp  a new set of ¦Ácp1 for v k+1 can be constructed by point-based dynamic programming backups. a point-based backup computes the best conditional plan cp1  = ha  os i  and corresponding ¦Á-vector  from a set of conditional plans cp for a given belief state b:

s.t. os  z  = argmaxcp ¦Ácp baz    1  a  = argmaxa ra b  + ¦Ã pz pr z|b a ¦Áos  z  baz  1 
¡¡in practice  we cannot perform such point-based backups for every belief state since the belief space is continuous. however  as noted by smallwood and sondik   finite-horizon optimal value functions are composed by a finite number of ¦Á-vectors  which means that we only need to compute a finite number of point-based backups  one per ¦Á-vector to be constructed. exact algorithms such as linear support  and witness  perform this finite set of pointbased backups at carefully chosen belief points. alternatively  approximate algorithms such as point-based value iteration  1; 1; 1  heuristically sample a set of belief points at which they perform point-based backups.
1 policy-directed observation aggregation
in a decision process  observations provide information to the decision maker for deciding the future course of action. when the observation space is rich  i.e.  continuous observations or many discrete observations   the decision maker can devise rich policies with a different course of action for each possible observation. however  for many pomdps  there often exists good policies that are quite simple. these policies tend to select the same course of action for many different observations that share similar features. for pomdps with continuous observations  this will allow us to implicitly discretise the observation space without introducing any error. from the point of view of the application domain  this also gives us insights regarding the relevant features of the observation space. in this section we discuss how simple policies allow us to aggregate many observations  effectively reducing the complexity of the observation space.
¡¡recall that a conditional plan cp = ha osi is a tuple consisting of an action a and an observation strategy os. the observation strategy os z  = cp1 indicates for each observation z the conditional plan cp1 encoding the future course of action. intuitively  all the observations that select the same conditional plan are indistinguishable and can be aggregated. we can therefore view observation strategies as partitioning the observation space into regions mapped to the same conditional plan.1 in the continuous observation tiger problem  for example  as long as a sound is heard coming  from the left   the best choice of action may be to open the right door. although the precise location of the sound here is not important  the decision boundary is  e.g. how far right can the sound be heard before the decision maker would listen again .
¡¡in each point-based backup  we compute an observation strategy os which partitions the observation space into regions zcp that select the same conditional plan cp. let's examine how these regions arise. recall from eq. 1 that for each observation z  the conditional plan selected is the one that maximizes. hence  we define zcp  = {z|cp  =
 to be the set of observations for which cp  is the best conditional plan to execute in.
¡¡for each region zcp  we can compute the aggregate probability pr zcp|a s1  that any observation z ¡Ê zcp will be made if action a is taken and state s1 is reached by integrating pdf z|a s1  over region zcp  i.e.  pr zcp|a s1  = rz¡Êzcp pdf z|a s1 dz.1 the aggregate probabilities can be used to perform point-based dynamic programming backups ¦Á a  os i b  = ra  b  + ¦Ã xpr zcp|b a  ¦Ácp baz cp   1  h
cp
with pr zcp|b a   = ps s1 b s pr s1|s a  pr zcp|a  s1 
and. this is equivalent to eq. 1  except that we have replaced the sum over observations z with a sum over regions zcp  in each of which a particular conditional plan is dominant.
1 one-dimensional observation space
we now discuss how to find the implicit discretization of the observation space induced by a set of conditional plans  or ¦Ávectors  when the observation space is one-dimensional continuous. for this special case  the regions  zcp  over which observations can be aggregated are segments of a line corresponding to a range of observations for which the same conditional plan is optimal. segment boundaries are observations for which there are two  or more  conditional plans yielding the highest. to make clear that z is the only variable here  define ¦Âcpb a z  to be a function in z that corresponds to  where b and a are fixed  i.e. 
can find these boundaries by solving for every pair cpi  cpj of conditional plans.1 analytically solving this equation will be difficult in general  and is not possible for observation functions in the exponential family  e.g. gaussians . however  efficient numerical solutions can be used for many well-behaved functions. we used the mathematica function intervalroots  that finds all the roots of an arbitrary one-dimensional function in a given interval by interval bisection combined with gradient-based methods. once all potential regions are identified  the aggregate probabilities p zcp|b a  can be computed by exact integration  or monte carlo approximation  for each conditional plan cp.
¡¡consider again the continuous tiger problem introduced in sect. 1. we now illustrate how to find the observation regions induced by a set of conditional plans and how to use them in a point-based backup. suppose that the doors are located at z = 1 and z =  1  and the decision maker is at z = 1. due to a lack of accuracy  the binary microphone array reports the tiger's location corrupted by some zero-mean  normally distributed noise of standard deviation ¦Ò = 1. listening costs 1  while meeting the tiger costs 1  but opening the correct door is rewarded with 1. the discount is ¦Ã = 1.
¡¡suppose we have 1 conditional plans with corresponding ¦Á-vectors shown in fig. 1 b  and we would like to perform a point-based backup. when considering belief state b tiger location = left  = 1 and action a = listen  fig. 1 a  shows the ¦Â functions of each conditional plan. since the observation function is gaussian  the ¦Â-function is a linear combination of gaussian distributions. by finding the roots of  we obtain the boundaries z = 1 and z = 1 delimiting the observation regions zcp1  zcp1 and zcp1. we can thus form a discrete observation function pr zcp|s a  by integrating the original gaussian observation distributions over each region. we analytically integrate each gaussian over each region using the complementary error function erfc.1 fig. 1 c  shows the two gaussian observation distributions  and the aggregate observation probabilities for each region. using eq. 1  we can then compute the value of the conditional plan cp1 = ha ¦Òi where a = listen and ¦Ò z  = cpi if z ¡Ê zcpi.
   in contrast  the original discrete version of the tiger problem partitions the observation space in two halves at z = 1  resulting in a discrete  binary observation function with erfc 	. a
dynamic partition induced by the current set of conditional plans has the advantage that no information is lost. fig. 1 d  compares the value of the policies obtained with our dynamic discretization scheme and the naive binary discretization as we vary the variance ¦Ò1. the dynamic discretization outperforms the fixed binary discretization for all variances. the solutions are the same for almost perfectly observable cases  ¦Ò ¡Ü 1  and approach one another for almost unobservable cases  ¦Ò ¡ú ¡Þ .
figure 1: tiger example. a  left  = 1 and a = listen  showing regions over which each conditional plan will be selected. b  ¦Á-vectors of 1 conditional plans. c  observation function and aggregate probabilities of each observation region for each state. d  average discounted reward achieved over 1 trials of 1 simulated runs of 1 steps each  for different observation variances in the continuous 1d tiger problem.
1 multi-dimensional observation space
in many applications  observations are composed of several sensor readings. for example  a mobile robot may have a number of sonar  laser or tactile sensors  which together give rise to multi-dimensional observations. when the observation space is multi-dimensional  analytically finding the regions and computing their aggregate probability will be difficult for many observation functions.
¡¡we examine two approaches. the first  discussed in sect. 1  considers the special case of pomdps with observations composed of conditionally independent variables. for this special case  it is possible to define an equivalent pomdp where the observation variables are processed sequentially in isolation  essentially reducing the observation space to one dimension. for arbitrary multi-dimensional observations  we present a general sampling-based technique in sect. 1.
1 conditionally independent observations
in some applications  the random variables corresponding to the individual sensor measurements may be conditionally independent given the last action executed and the state of the world. in this case  it is possible to factor the observation function into a product of small observation functions  one for each random variable  i.e.  pr z1 z1|a s1  = pr z1|a s1 pr z1|a s1  . for example  consider a mobile robot with sonars pointing forwards and to the side. the readings from each sonar are conditionally independent given the location of the robot and a map of the robot's environment.
¡¡this factorization can be exploited to process the observations sequentially. for n conditionally independent observation variables  we divide each time step into n sub-steps such that only one observation variable is observed per sub-step. this can be easily accomplished by constructing a pomdp with an additional state variable  substep  that keeps track of the current sub-step. the observation function encodes the probabilities of a single  but different  observation variable at each sub-step. the transition function is the same as in the original pomdp when substep=1  but becomes the identity function otherwise. the rewards are gathered and the discount factor applied only when substep=1.
¡¡when all observation variables are conditionally independent  this effectively reduces the dimensionality of continuous observations to one  allowing the approach of sect. 1 to be used. for discrete observation variables  an exponential reduction is also achieved since the domain of a single variable is exponentially smaller than the cross-domain of several variables. note however that the complexity of the equivalent pomdp remains the same since the reduction is achieved by multiplying the horizon and the number of states by n.
1 sampling
for arbitrary multi-dimensional observations  an effective approximation technique for computing the aggregate probabilities consists of sampling. recall from sect. 1 that for each conditional plan cp  we can aggregate in one region zcp all the observations z for which cp yields the highest value  i.e.  . hence  for each ha s1i-pair  we sample k observations from pdf z|a s1  and set pr zcp|a s1  to the fraction of observations for which ¦Âcpb a z  is maximal  breaking ties by favoring the conditional plan with the lowest index.
¡¡this sampling technique allows us to build an approximate discrete observation function pr zcp|a s1  which can be used to perform a point-based backup for a set cp of conditional plans and a belief state b. the number of observations sampled is k|s||a| for each point-based backup. the quality of the approximation improves with k. in particular  using hoeffding's bound   we can guarantee that pr zcp|a s1  has an error of at most  with probability 1   ¦Ä when. interestingly  k doesn't depend on the dimensionality  nor any other complexity measure  of the observation space. it depends only on the number of regions  which is at most the number of conditional plans. while this is true in a single dp backup  the number of conditional plans may increase exponentially  in the worst case  with the number of observations at each dp backup . on the other hand  several algorithms  1; 1; 1; 1  can mitigate policy complexity by searching for good yet small policies represented by a bounded number of conditional plans or ¦Á-vectors. perseus   a randomized pointbased value iteration algorithm  is such an algorithm since the number of ¦Á-vectors is bounded by the number of belief points initially sampled. hence k depends on policy complexity  which generally depends on observation complexity  but can be made independent by restricting policies to have a bounded representation.
¡¡this sampling technique can also be used for pomdps with many discrete observations. in particular  when the observations are factored into several random variables  the number of observations is exponential in the number of variables  but as long as the number of conditional plans remains small  the number of samples will also be relatively small.
¡¡note also that we can often weaken the dependency between the number of samples and the size of the action and state spaces. instead of sampling k observations from each of the |a||s| densities pdf z|a s1   we can sample j observations from one proposal distribution p z . this sample of j observations can be used to approximate each pr zcp|a s1  as follows. first  we assign a weight pdf z|a s1 /p z  to each sampled observation z to obtain an unbiased sample of pdf z|a s1 . then  for each conditional plan cp  we find the subset of sampled observations z for which is maximal  and set pr zcp|a s1  to the  normalized  sum of the weights of the observations in that subset. the number of sampled observations j necessary to guarantee an error of at most  with probability 1   ¦Ä depends on how similar the proposal distribution p z  is with each density pdf z|a s1 . when the proposal is relatively similar to each of the densities then j tends to be close to k  significantly reducing the dependencies on |a| and |s|. however  as the differences between the proposal and each of the densities increase  j also increases and may become arbitrarily large. in sect. 1  we use pdf z|b a  as a proposal distribution.
1 experiments
this section presents experiments with a pomdp that assists people with cognitive difficulties to complete activities of daily living. focusing on the task of handwashing  we present a simplified pomdp for guiding patients with verbal prompts as they wash their hands. the goal of such a system is to minimize the human caregiver burden  and is part of an ongoing research initiative applying intelligent reasoning to assistive living . in this paper  we present results from simulations of our methods on a simplified pomdp model for the handwashing task. fig. 1 a  shows the graphical model of the handwashing pomdp. the pomdp's actions are the verbal prompts  corresponding to the canonical steps of handwashing: turn on water  wet hands  use soap  dry hands and turn off water  and a null action where the system waits. the states are defined by the variables handsstate  which can be {dirty  soapy  clean}  hand location  which can be {away  tap  water  soap  towel}  hands wet  which can be {wet  dry}  and water  which can be {on  off}. we assume the hands start dirty and dry  and the goal is to get them clean and dry  which can only happen if they become soapy and wet at some intermediate time. the water starts off and must be off for task completion. the cost of a prompt is 1  and a large reward

	 a 	 b 
figure 1: a  pomdp model of handwashing. b  example image showing the regions induced by the observation function alone  maxi p x y|handlocation = i 
 1  is given when the hands are dry and clean and the water is off. smaller rewards are given for progressions through the task: if the hands are clean  1   soapy  1  and wet  1   but other variables are not as in the final goal state. the discount was ¦Ã = 1.
¡¡we model the system as being equipped with an impeller in the water pipe  which returns 1 when there is no water flowing and 1 when the water is on full. the sensor's noise is zero-mean gaussian with standard deviation ¦Ò = 1  which gives it an 1% accuracy  when the most likely state of water is considered . the position of the hands in the horizontal plane is measured using a camera mounted in the ceiling above the sink  connected to a computer vision system that returns an estimate of the {x y} position of the patient's dominant hand in the image using skin color . fig. 1 b  shows an example image from the camera. the observation function that relates these measured hand positions to the actual handlocation was learned from a set of data taken from the computer vision system. an actor simulated the repeated handwashing trials for about 1 minutes. the vision system tracked and reported the {x y} position of the right  dominant  hand  while a researcher annotated the data with the actual handlocation.1 the functions p x y|hand location = i  were then learned by fitting a mixture of gaussians to the data annotated with the value hand location = i.1the mixture models were fit using a k-means initialization followed by the expectation-maximization algorithm. figure 1 b  shows the most likely handlocations for each {x y} position induced by the learned mixtures of gaussians. the water flow observation function was not learned from data.
¡¡the water flow and hand position readings yield a 1d observation space. although water flow and hand positions are conditionally independent  the {x y} coordinates of the measured hand positions are dependent. hence  we cannot process the observations sequentially as suggested in sect. 1 and must resort to the sampling technique of sect. 1. we extended perseus  with the sampling technique described

	 a 	 b 
figure 1: example regions during a simulated trial at three stages  from top to bottom  after 1  1 and 1 actions :  a  shows the best actions to take  the current belief state  and the current location of the hand  star ;  b  shows the regions zcp  each different shade corresponding to a different conditional plan that will be followed if an observation is made in the region.
in sect. 1 to compute an approximate discrete observation function at each point-based backup. more precisely  1 observations were sampled before each point-based backup to approximate the aggregate probabilities of each observation region induced by some conditional plan. we ran the algorithm for 1 iterations with 1 sampled belief states. for comparison purposes  we also ran the original perseus algorithm with a fixed discrete observation function obtained by partitioning the plane of hand positions according to the regions shown in fig. 1 b  and by partitioning water flow readings in two regions at 1. this is a natural discretization that arises from the observation function by considering the regions where each pdf x y|handlocation  and pdf impeller|waterflow  are highest. the computed policies were then simulated  and actions were selected by monitoring the belief state and computing the best conditional plan argmaxcp ps b s ¦Ácp  s . the discounted rewards  averaged over 1 trials of 1 simulated runs of 1 steps each  of the policies obtained by dynamic partitioning are 1 ¡À 1  while those for fixed partitioning are 1 ¡À 1  showing that our sample-based dynamic partitioning technique outperforms the fixed discretization technique. the final conditional plans were represented with 1 ¦Á vectors for our dynamic algorithm  and 1 ¦Á vectors for the fixed discretization.
¡¡figure 1 shows examples of the dynamic partitions found by our technique at three stages during a simulated trial. since we cannot show 1d partitions  we show the 1d partitions of the {x y} plane  ignoring the water flow sensor . at the beginning of the trial  top row  stage 1   the system will either prompt to use the soap or turn the water on  based on where it sees the hands. at this stage  the regions zcp mainly distinguish the areas surrounding the soap and the taps  since these are the usual first steps in handwashing. once the hands are believed to be soapy and the water on  stage 1  middle row  the system will prompt to rinse the hands  unless the patient has rinsed their hands or has turned the water off  in which case the prompt will be to turn the water off or on  respectively. we see in figure 1 b   middle row  that there are many regions now in the areas near the tap or under the water. this is the most uncertain area for this system  see figure 1 b    calling for many different conditional plans. finally  at stage 1  bottom row   the system believes the hands are clean  and will prompt the user to dry their hands or turn the water off. in this case  fewer possibilities remain  and so there are fewer regions in figure 1 b   bottom row .
1 conclusion
exploiting the fact that observations are useful only to the extent where they lead to different courses of actions  the paper describes how to dynamically partition observation spaces without any loss of information based on the current policy. for policies with a small number of conditional plans  observations can be aggregated in a small number of regions corresponding to the relevant observation features of the application domain. the region-based observation function can generally be constructed by numerical root-finding and integration algorithms for uni-dimensional observations or multidimensional observations composed of conditionally independent variables. for general multi-dimensional observations a general sampling technique was also described and demonstrated on a realistic assisted living task.
¡¡note that the dynamic partitioning technique proposed in this paper is tightly integrated with point-based backups. more precisely  a lossless dynamic partition of the observation space can be computed only with respect to a given belief state and a set of ¦Á-vectors. as a result  our technique cannot be integrated with algorithms that do not use point-based backups  e.g.  incremental pruning   bounded policy iteration  . furthermore  it cannot be integrated with the linear programs that find belief points prior to point-based backups in the witness algorithm . at the moment  full integration is only possible with linear support   pbvi   and perseus  since these algorithms make use of the observation function only in point-based backups. dynamic lossless observation partitioning for a broader range of algorithms is a possible direction for future research.
¡¡this paper tackles pomdps with continuous observations  but discrete states. as mentioned earlier  such pomdps are common in user modeling  event recognition and spokendialog systems  since the observations correspond to continuous sensor readings and the states are abstract discrete features. note also that our dynamic partitioning technique doesn't require the state space to be discrete. in fact  porta et. al  recently proposed an extension to perseus that can handle continuous state spaces. point-based backups are performed in a similar fashion  but given the continuous nature of the state space  ¦Á-functions are generated instead of ¦Ávectors. integrating our dynamic partitioning technique with such continuous point-based backups should be possible and is subject to future research.
¡¡our current work in using pomdps for assistive living tasks involves learning model structure and user behaviors from sequence data  rather than imposing our own structure on tasks. the pomdp models we learn have observation functions which are themselves dynamic bayesian networks  dbns  with video frame observations at each time step  leading to a hierarchical model. preliminary work along these lines is reported in . we wish to use the techniques we have described in this paper both to solve these pomdps and to learn models of human behaviors from video sequence data. another potential research direction includes the exploration of automated feature detection in application domains such as image processing and speech recognition by policydirected observation aggregation.
acknowledgements
the authors wish to thank alex mihailidis for help with the handwashing data  darius braziunas for pointing out some inconsistencies in some early experiments  nikos vlassis for helpful comments and jason williams for early discussions. the first author gratefully acknowledges the support of intel corporation and the american alzheimer association.
