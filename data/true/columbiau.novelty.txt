our system for the novelty track at trec 1 looks beyond sentence boundaries as well as within sentences to identify novel  nonduplicative passages. it tries to identify text spans of two or more sentences that encompass mini-segments of new information. at the same time  we avoid any pairwise comparison of sentences  but rely on the presence of previously unseen terms to provide evidence of novelty. the system is guided by a number of parameters  both weights and thresholds  that are learned automatically with a randomized hill-climbing algorithm. during learning  we varied the target function to produce configurations that emphasize either precision or recall. we also implemented a straightforward vector-space model as a comparison and to test a combined approach.
1. introduction
the novelty detection problem seeks an automatic way of identifying any new information in a document  or documents  on a given topic. it is a recent area of inquiry in the natural language processing and information retrieval communities and has been explored at the last three meetings of the text retrieval conference  trec   in the novelty track.
after the first novelty track in 1  the national institute of standards and technology  nist  separated the track into four tasks  two of which combined passage retrieval and novelty filtering and two of which concentrated on novelty filtering  giving participants the choice of whether to do the combined tasks  tasks 1 and/or 1  or whether to focus on the novelty detection alone  tasks 1 and/or 1 . in the combined tasks  participants have to first choose the sentences that are  relevant  to a given topic from a set of documents  and then make a second pass to remove duplicates. in the pure novelty task  participants are given an ordered set of relevant sentences and must filter them to choose all those with  new  information - that is the information that has not appeared previously in the set .
both the retrieval and filtering tasks are quite difficult in themselves  and it is problematic to join them and force the filtering systems to use the experimental output of the retrieval systems. the noisy input clouds what can be learned about determining novelty. we did only task 1 of the novelty track this year  since it is most closely related to our ongoing research into creating updates or bulletin summaries for an on-line news browsing system.
our submission for the novelty track  called sumseg  is based on observations of the data we collected for the development of our update summarizer. we saw that new information sometimes appears in passages that are two or more sentences long  and sometimes only in clauses embedded in a sentence.  task 1 is similar to task 1  but it allows the systems to see the novel sentences from the first five documents. time constraints prevented us from submitting runs for it that would have made use of additional input. 
in order to recognize novelty in both cases - segments of two or more sentences  and embedded clauses that are only part of a sentence - we avoid direct sentence similarity measures  and consider previously unseen words to be the main evidence of novelty. sumseg has a number of thresholds for deciding how much novelty is necessary to trigger a novel classification. we implemented a randomized hill climbing algorithm to learn thresholds for how many new words would trigger a novel classification. we also sought to learn different weights for different types of nouns  for example  persons  or locations or common nouns. in addition  we included a mechanism to allow sentences that had few strong content words to continue the classification of the previous sentence. the basic sumseg system is described in . finally  we used two statistics  derived from analysis of the full aquaint corpus  to eliminate low-content words.
for trec 1  we submitted a total five runs: the first two used learned parameters that aimed at high precision output  and the third at high recall. the fourth run was a straightforward vector-space model  with a cosine similarity metric  used as a baseline  and the fifth was a combination of the high recall run with the vector-space model. training was done on the 1 trec novelty data.
over all  we were most interested in trying to improve precision. it seemed from the experiences of the participants at trec and from our own work that precision was extremely difficult to increase much beyond a random selection of relevant sentences. in the 1 novelty track  the top precision was 1 although 1% of the relevant sentences were novel. the median precision among all 1 runs in 1 was 1  and the average 1. if we remove the five runs by one participant that were in the 1 range  possibly because of some misunderstanding  the median is still only 1 and the average 1. in our summarization work  we especially value conciseness and our long-term goal would be to find the minimal output of a novelty system.
the next section will review related work. section 1 will describe the system  and section 1 will discuss our experiments. finally section    will preview our performance in this year's novelty track.
1. related work
much of the work in this area has been done for the novelty track. a number of groups experimented with matrix-based methods. the group from the university of maryland and the center for computing sciences there used three techniques that operate on term-sentence matrices  qr decomposition  pivoted qr decomposition: qr algorithm  and singular value decomposition . the university of maryland  baltimore county  worked with clustering algorithms and singular value decomposition in sentence-sentence similarity matrices .
topic words were used to cluster candidate sentences by the information retrieval group at tsinghua university . the clusters then restrict the word overlap comparisons to reduce redundancy.
the institute of computing technology  the chinese academy of sciences  experimented with varying the number of novel sentences by the ordering of the source documents. they also tried maximal marginal relevance  and word overlap  and found that word overlap was the most effective .
meiji university embellished pairwise similarity calculations with co-occurrence data from a background corpus. it restricted the novelty comparisons to a time window for the publication dates and included an idf term in scoring sentences . the national university of taiwan also used term expansion to inform sentence similarity measures .
the university of iowa based its novelty decisions on a count of new named entities and noun phrases in a sentence .
an interesting approach at trec 1 was done by a group at cmu  which used wordnet to identify synonyms and a graph-matching algorithm to compute similar structure between sentences.
using the trec 1 data  allan  compared a number of sentence-based models ranging in complexity from a count of new words and cosine distance  to a variety of sophisticated models based on kl divergence with different smoothing strategies and a  core mixture model  that considers the distribution of the words in the sentence with the distributions in a topic model and a general english model.
our system is closest to the iowa system since it pays a large amount of attention to a count of new named entities and noun phrases  but we give different weights to different types of named entities. we also calculate the weights of common nouns with respect to their frequency in a large background corpus and in the document set for the current topic  as does allan's core mixture model.
1. system
this section will introduce the general outline of the system. the major components will be detailed in the subsections below.
our system was tailored to the problem posed in the task 1 of the trec novelty track. for each of the 1 topics  participants were given a set of sentences that have been judged relevant to the topic and were required to return a new list that contains no sentences that were covered by information seen earlier in the input. the relevant sentences were all drawn from a set of documents  at least 1 for each topic. some topics had additional documents  some not relevant to the topic  that were included to increase the difficulty of the tasks  but these would have no impact on task 1. the topics were evenly divided between opinion and events.
