distributed hash tables  dhts  provide a scalable solution for data sharing in p1p systems. to ensure high data availability  dhts typically rely on data replication  yet without data currency guarantees. supporting data currency in replicated dhts is difficult as it requires the ability to return a current replica despite peers leaving the network or concurrent updates. in this paper  we give a complete solution to this problem. we propose an update management service  ums  to deal with data availability and efficient retrieval of current replicas based on timestamping. for generating timestamps  we propose a key-based timestamping service  kts  which performs distributed timestamp generation using local counters. through probabilistic analysis  we compute the expected number of replicas which ums must retrieve for finding a current replica. except for the cases where the availability of current replicas is very low  the expected number of retrieved replicas is typically small  e.g. if at least 1% of available replicas are current then the expected number of retrieved replicas is less than 1. we validated our solution through implementation and experimentation over a 1-node cluster and evaluated its scalability through simulation up to 1 peers using simjava. the results show the effectiveness of our solution. they also show that our algorithm used in ums achieves major performance gains  in terms of response time and communication cost  compared with a baseline algorithm. 
categories and subject descriptors 
h.1  database management : systems - distributed databases  concurrency  query processing. 
general terms 
algorithms  performance  reliability. 
keywords 
peer-to-peer  distributed hash table  dht   data availability  data currency  data replication 
1. introduction  
peer-to-peer  p1p  systems adopt a completely decentralized approach to data sharing and thus can scale to very large amounts of data and users. popular examples of p1p systems such as gnutella  and kaaza  have millions of users sharing petabytes of data over the internet. initial research on p1p systems has focused on improving the performance of query routing in unstructured systems  such as gnutella and kaaza  which rely on flooding. this work led to structured solutions based on distributed hash tables  dht   e.g. can   chord   and pastry . while there are significant implementation differences between dhts  they all map a given key k onto a peer p using a hash function and can lookup p efficiently  usually in o log n  routing hops where n is the number of peers . dhts typically provide two basic operations : put k  data  stores a key k and its associated data in the dht using some hash function; get k  retrieves the data associated with k in the dht.  
one of the main characteristics of p1p systems is the dynamic behavior of peers which can join and leave the system frequently  at anytime. when a peer gets offline  its data becomes unavailable. to improve data availability  most dhts rely on data replication by storing  k  data  pairs at several peers  e.g. using several hash functions . if one peer is unavailable  its data can still be retrieved from the other peers that hold a replica. however  the mutual consistency of the replicas after updates can be compromised as a result of peers leaving the network or concurrent updates. let us illustrate the problem with a simple update scenario in a typical dht. let us assume that the operation put k  d1   issued by some peer  maps onto peers p1 and p1 which both get to store the data d1. now consider an update  from the same or another peer  with the operation put k  d1  which also maps onto peers p1 and p1. assuming that p1 cannot be reached  e.g. because it has left the network  then only p1 gets updated to store d1. when p1 rejoins the network later on  the replicas are not consistent: p1 holds the current state of the data associated with k while p1 holds a stale state. concurrent updates also cause inconsistency. consider now two updates put k  d1  and 

 
1
   work partially funded by ara  massive data  of the french ministry of research and the european strep grid1all project. 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod'1  june 1  1  beijing  china. 
copyright 1 acm  1-1-1/1...$1. 
put k  d1   issued by two different peers  which are sent to p1 and p1 in reverse order  so that p1's last state is d1 while p1's last state is d1. thus  a subsequent get k  operation will return either stale or current data depending on which peer is looked up  and there is no way to tell whether it is current or not. for some applications  e.g. agenda management  bulletin boards  cooperative auction management  reservation management  etc.  which could take advantage of a dht  the ability to get the current data is very important. 
many solutions have been proposed in the context of distributed database systems for managing replica consistency  but the high numbers and dynamic behavior of peers make them no longer applicable to p1p . supporting data currency in replicated dhts requires the ability to return a current replica despite peers leaving the network or concurrent updates. the problem is partially addressed in  using data versioning.  each replica has a version number which is increased after each update. to return a current replica  all replicas need to be retrieved in order to select the latest version. however  because of concurrent updates  it may happen that two different replicas have the same version number thus making it impossible to decide which one is the current replica. 
in this paper  we give a complete solution to data availability and data currency in replicated dhts. our main contributions are the following: 
  we propose a service called update management service  ums  which deals with improving data availability and efficient retrieval of current replicas based on timestamping. after retrieving a replica  ums detects whether it is current or not  i.e. without having to compare with the other replicas  and returns it as output. thus  in contrast to the solution in   ums does not need to retrieve all replicas to find a current one. in addition  concurrent updates raise no problem for ums. 
  we give a probabilistic analysis of ums's communication cost. we compute the expected number of replicas which ums must retrieve for finding a current replica. we prove that it is less than the inverse of the probability of currency and availability  i.e. the probability that a replica is current and available. thus  except for the cases where the availability of current replicas is very low  the expected number of replicas which ums must retrieve is typically small. 
  we propose a new key-based timestamping service  kts  which generates monotonically increasing timestamps  in a distributed fashion using local counters. kts does distributed timestamp generation in a way that is similar to data storage in the dht  i.e. using peers dynamically chosen by hash functions. to maintain timestamp monotonicity  we propose algorithms which take into account the cases where peers leave the system either normally or not  e.g. because they fail . to the best of our knowledge  this is the first paper that introduces the concept of key-based timestamping  and proposes efficient techniques for realizing this concept in dhts. furthermore  kts is useful to solve other dht problems which need a total order on operations performed on each data  e.g. read and write operations which are performed by concurrent transactions. 
  we provide a comprehensive performance evaluation based on the implementation of ums and kts over a 1-node cluster. we also evaluated the scalability of our solution through simulation up to 1 peers using simjava. the experimental and simulation results show the effectiveness of our solution. 
the rest of this paper is organized as follows. in section 1  we first propose a model for dhts which will be useful to present our solution  and then we state the problem. section 1 presents our update management service for dhts. in section 1  we propose a distributed timestamping service to support updates. section 1 describes a performance evaluation of our solution through implementation and simulation. in section 1  we discuss related work. section 1 concludes. 
1. dht model and problem statement 
in this section  we first present a model of dhts which is needed for describing our solution and proving its properties. then  we precisely state the problem. 
1 dht model 
a dht maps a key k to a peer p using a hash function h. we call p the responsible for k wrt h. a peer may be responsible for k wrt a hash function h1 but not responsible for k wrt another hash function h1. the responsible for k wrt h may be different at different times  i.e. because of peers' joins and leaves. we can model the mapping mechanism of dht as a function that determines at anytime the peer that is responsible for k wrt h; we call this function dht's mapping function. 
definition 1: dht's mapping function. let k be the set of all keys accepted by the dht  p the set of peers  h the set of all pairwise independent hash functions which can be used by the dht for mapping  and t the set of all numbers accepted as time. we define the dht's mapping function as m: k뫄h뫄t 뫸 p such that m k h t  determines the peer p뫍p which is responsible for k뫍k wrt h뫍h at time t뫍t. 
let us make precise the terminology involving peers' responsibility for a key. let k뫍k  h뫍h and p뫍p  and let  t1..t1  be a time interval such that t1 t1. we say that p is continuously responsible for k wrt h in  t1..t1  if it is responsible for k wrt h at anytime in  t1..t1 . in other words    t뫍t  t1뫞t t1       p=m k h t  . if p obtains and loses the responsibility for k wrt h respectively at t1 and t1  and is continuously responsible for k wrt h in  t1..t1   then we say that  t1..t1  is a p's period of responsibility for k wrt h. the peer that is responsible for k wrt h at current time is denoted by rsp k h . we also denote by prsp k h  the peer that was responsible for k wrt h just before rsp k h . the peer that will become responsible for k wrt h just after rsp k h  is denoted by nrsp k h . 
example 1. figure 1 shows the peers responsible for k뫍k wrt h뫍h since t1. the peer that is currently responsible for k wrt h is p1  thus p1=rsp k h  and p1=prsp k h . in the time interval  t1..t1   p1 is continuously responsible for k wrt h. it has obtained and lost its responsibility respectively at t1 and t1  thus  t1..t1  is p1's period of responsibility for k wrt h. also  t1..t1  and  t1..t1  are respectively p1's and p1's periods of responsibility for k wrt h. 
now 
... 
 
figure 1. example of peers' responsibilities 
in the dht  there is a lookup service that can locate rsp k h  efficiently. the lookup service can return the address of rsp k h  usually in o log  p   routing hops  where  p  is the number of peers in the system. 
1 problem statement 
to improve data availability we replicate the pairs  k  data  at several peers using several hash functions. we assume that there is an operation that stores a pair  k  data  at rsp k h  which we denote by puth k  data . this operation can be issued concurrently by several peers. there is another operation  denoted by geth k   that retrieves the data associated with k which is stored at rsp k h .  
over time  some of the replicas stored with k at some peers may get stale. our objective is to provide a mechanism which returns efficiently a current replica in response to a query requesting the data associated with a key. 
formally  the problem can be defined as follows. given a key k뫍k  let rk be the set of replicas such that for each r뫍rk  the pair  k  r  is stored at one of the peers of the dht. our goal is to return efficiently an r뫍rk which is current  i.e. reflects the latest update. 
1. update management service 
to deal with data currency in dhts  we propose an update management service  ums  which provides high data availability through replication and efficient retrieval of current replicas. ums only requires the dht's lookup service with puth and geth operations. to return current replicas  it uses timestamps attached to the pairs  k  data . in this section  we give an overview of our timestamping solution and present in more details ums' update operations. we also analyze ums's communication cost. 
1 timestamping 
to provide high data availability  we replicate the data in the dht using a set of pairwise independent hash functions hr h which we call replication hash functions. to be able to retrieve a current replica we  stamp  each pair  k  data  with a logical timestamp  and for each h뫍hr we replicate the pair  k  newdata  at rsp k h  where newdata={data  timestamp}  i.e. newdata is a data composed of the initial data and the timestamp. upon a request for the data associated with a key  we can thus return one of the replicas which are stamped with the latest timestamp. the number of replication hash functions  i.e.  hr   can be different for different dhts. for instance  if in a dht the availability of peers is low  for increasing data availability a high value of  hr   e.g. 1  is used. constructing hr  which is a set of pairwise independent hash functions  can be done easily  e.g. by using the methods presented in . 
to generate timestamps  we propose a distributed service called key-based timestamping service  kts . the main operation of kts is gen ts k  which given a key k generates a real number as a timestamp for k. the timestamps generated by kts have the monotonicity property  i.e. two timestamps generated for the same key are monotonically increasing. this property permits us to order the timestamps generated for the same key according to the time at which they have been generated. 
definition 1: timestamp monotonicity. for any two timestamps ts1 and ts1 generated for a key k respectively at times t1 and t1  if t1   t1 then we have ts1    ts1. 
at anytime  kts generates at most one timestamp for a key  see section 1 for the details . thus  regarding to the monotonicity property  there is a total order on the set of timestamps generated 
insert k  data  
begin 
   ts := kts.gen ts  k ;    for  each  h뫍hr  do       newdata := {data  ts};       dht.puth k  newdata ; end;  
retrieve k  
begin  	 
   ts1 := kts.last ts k ;    datamr := null;    tsmr := - カ    for  each  h뫍hr  do begin 
       newdata := dht.geth k ;  	data := newdata.data; 
 	ts := newdata.ts; 
모모모모if  ts1 = ts  then begin            return data; // one current  
                     // replica is found  
         exit;  
 	end 
모모모모else if   ts   tsmr  then  begin    datamr := data;//keep the most     tsmr := ts;//recent replica and              //its timestamp end;    end; return datamr  end; figure 1. ums update operations 
for the same key. however  there is no total order on the timestamps generated for different keys. 
kts has another operation denoted by last ts k  which given a key k returns the last timestamp generated for k by kts. 
1 update operations 
to describe ums  we use the kts.gen ts and kts.last ts operations discussed above. the implementation of these operations is detailed in section 1. ums provides insert and retrieve operations  see figure 1 . 
insert k  data : inserts a pair  k  data  in the dht as follows. first  it uses kts to generate a timestamp for k  e.g. ts. then  for each h뫍hr it sends the pair  k  {data  ts}  to the peer that is rsp k h . when a peer p  which is responsible for k wrt one of the hash functions involved in hr  receives the pair  k  {data  ts}   it compares ts with the timestamp  say ts1  of its data  if any  associated with k. if ts ts1  p overwrites its data and timestamp with the new ones. recall that  at anytime  kts.gen  ts  k  generates at most one timestamp for k  and different timestamps for k have the monotonicity property. thus  in the case of concurrent calls to insert k  data   i.e. from different peers  only the one that obtains the latest timestamp will succeed to store its data in the dht. 
retrieve k : retrieves the most recent replica associated with k in the dht as follows. first  it uses kts to determine the latest timestamp generated for k  e.g. ts1. then  for each hash function h뫍hr  it uses the dht operation geth k  to retrieve the pair {data  timestamp} stored along with k at rsp k h . if timestamp is equal to ts1  then the data is a current replica which is returned as output and the operation ends. otherwise the retrieval process continues 
while saving in datamr the most recent replica. if no replica with a timestamp equal to ts1 is found  i.e. no current replica is found  then the operation returns the most recent replica which is available  i.e. datamr.  
1 cost analysis 
in this section  we give a probabilistic analysis of the communication cost of ums in terms of number of messages to retrieve a data item. for a non replicated dht  this cost  which we denote by cret  is o log n  messages where n is the number of peers. the communication cost of retrieving a current replica by 
ums is cums = ckts + nums   cret  where ckts is the cost of returning the last generated timestamp by kts and nums is the number of replicas that ums retrieves  i.e. the number of times that the operation geth k  is called. as we will see in the next section  ckts is usually equal to cret  i.e. the cost of contacting the responsible of a key and getting the last timestamp from it. thus  we have cums =  1 + nums    cret.   
the the number of replicas which ums retrieves  i.e. nums  depends on the probability of currency and availability of replicas. the higher this probability  the lower nums is. let hr be the set of replication hash functions  t be the retrieval time  and pt be the probability that  at time t  a current replica is available at a peer that is responsible for k wrt some h뫍hr. in other words  pt is the ratio of current replicas  which are available at t over the peers responsible for k wrt replication hash functions  to the total number of replicas  i.e.  hr . we call pt the probability of currency and availability at retrieval time. we give a formula for computing the expected value of the number of replicas  which ums retrieves  in terms of pt and  hr . let x be a random variable which represents the number of replicas that ums retrieves. we have prob x=i  = pt    1- pt i-1  i.e. the probability of having x=i is equal to the probability that i-1 first retrieved replicas are not current and the ith replica is current. the expected value of x is computed as follows:  
hr
e x  =뫉i prob x = i                      
i=1
hr
e x   = pt   뫉i   1  pt  i 1                              1  
i=1
equation 1 expresses the expected value of the number of retrieved replicas in terms of pt and  hr . thus  we have the following upper bound for e x  which is solely in terms of pt: 
e                             1  
i=1
from the theory of series   we use the following equation for 1뫞 z  1: 

모모모모모i 1	 뫉i  z =	1 i=1	 1  z 
since 1 뫞  1- pt    1  we have:   
 i 1                          1  
뫉i  1  pt  	=  	1 i=1	 1  1  pt   
using equations 1 and 1  we obtain:  
1                                                          1   
e x   
pt
theorem 1: the expected value of the number of replicas which ums retrieves is less than the inverse of the probability of currency and availability at retrieval time. 
proof: implied by the above discussion. 
example. assume that at retrieval time 1% of replicas are current and available  i.e. pt=1. then the expected value of the number of replicas which ums retrieves is less than 1. 
intuitively  the number of retrieved replicas cannot be more than  hr . thus  for e x  we have: 
	1	                                         1  
e x  뫞 min 	  hr   pt
1. key-based timestamp service 
the main operation of kts is gen ts which generates monotonically increasing timestamps for keys. a centralized solution for generating timestamps is obviously not possible in a p1p system since the central peer would be a bottleneck and single point of failure. distributed solutions using synchronized clocks no longer apply in a p1p system. one popular method for distributed clock synchronization is network time protocol  ntp  which was originally intended to synchronize computers linked via internet networks . ntp and its extensions  e.g.  and   guarantee good synchronization precision only if computers have been linked together long enough and communicate frequently . however  in a p1p system in which peers can leave the system at any time  these solutions cannot provide good synchronization precision. 
in this section  we propose a distributed technique for generating timestamps in dhts. first  we present a technique based on local counters for generating the timestamps. then we present a direct algorithm and an indirect algorithm for initializing the counters  which is very important for guaranteeing the monotonicity of timestamps. we also apply the direct algorithm to can and chord. finally  we discuss a method for maintaining the validity of counters.  
1 timestamp generation 
our idea for timestamping in dhts is like the idea of data storage in these networks which is based on having a peer responsible for storing each data and determining the peer dynamically using a hash function. in kts  for each key we have a peer responsible for timestamping which is chosen dynamically using a hash function. below  we discuss the details of timestamp responsibility and timestamp generation. 
1.1 timestamping responsibility 
timestamp generation is performed by kts as follows. let k뫍k 
be a key  the responsible of timestamping for k is the peer that is responsible for k wrt hts  i.e. rsp k  hts   where hts is a hash function accepted by the dht  i.e. hts뫍h. each peer q that needs a timestamp for k  called timestamp requester  uses the dht's lookup service to obtain the address of rsp k  hts  to which it sends a timestamp request  tsr . when rsp k  hts  receives the request of q  generates a timestamp for k and returns it to q. figure 1 illustrates the generation of a timestamp for k initiated by peer q. 
gen-ts k  // timestamp generation by kts begin     p := dht.lookup k  hts ;    return gen-ts p  k ; end;  
gen-ts p  k  //generating a timestamp              // for a key k by peer p              // that is rsp k  hts  begin     cp k := search counter vcsp  k ;    if  cp k is not in vcsp  then     begin 
      new cp k ;//allocate memory for cp k 
      kts.counterinitialize k  cp k ;       vcsp := vcsp + {cp k};    end;    cp k.value := cp k.value + 1;    return cp k.value; end; if the peer that is rsp k  hts  leaves the system or fails  the dht detects the absence of that peer  e.g. by frequently sending  ping  messages from each peer to its neighbors   and another peer becomes responsible for k wrt hts. therefore  if the responsible of timestamping for k leaves the system or fails  another peer automatically becomes responsible of timestamping for k  i.e. the peer that becomes responsible for k wrt hts. thus  the dynamic behavior of peers causes no problem for timestamping responsibility. 
tsk 

figure 1. timestamp  generation 

      figure 1. example of timestamp generation 
1.1 guaranteeing monotonicity 
let us now discuss what a responsible of timestamping should do to maintain the monotonicity property. let k be a key  p the peer that is responsible of timestamping for k  and tsk a timestamp for k which is generated by p. to provide the monotonicity property  we must guarantee two constraints:  1  tsk is greater than all timestamps for k which have been previously generated by p itself;  1  tsk is greater than any timestamp for k generated by any other peer that was responsible of timestamping for k in the past. 
to enforce the first constraint  for generating timestamps for each key k  we use a local counter of k at p which we denote as cp k. when p receives a timestamp request for k  it increments the value of cp k by one and returns it as the timestamp for k to the timestamp requester.  
to enforce the second constraint  p should initialize cp k so that it is greater than or equal to any timestamp for k previously generated by other peers that were responsible of timestamping for k in the past. for this  p initializes cp k to the last value of cq k where q is the last peer that has generated a timestamp for k. in section 1  we discuss how p can acquire cq k. the following lemma shows that the initialization of cp k as above enforces the second constraint. 
lemma 1: if each peer p  during each of its periods of responsibility for k wrt hts  initializes cp k before generating the first timestamp for k  then each generated timestamp for k is greater than any previously generated one.  
proof: follows from the fact that initializing cp k makes it equal to the last timestamp generated for k  and the fact that timestamp generation is done by increasing the value of cp k by one and returning its value as output. 뫳 
after cp k has been initialized  it is a valid counter  i.e. p can use it for generating timestamps for k. if p loses the responsibility for k wrt hts  e.g. because of leaving the system  then cp k becomes invalid. the peer p keeps its valid counters in a valid counters set which we denote by vcsp. in other words  for each k뫍k  if cp k is valid then cp k is in vcsp. each peer p뫍p has its own vcsp and respects the following rules for it: 
1. when p joins the p1p system  it sets vcsp =  . 
1.  k뫍k  when p initializes cp k  it adds cp k to vcsp. 
1.  k뫍k  when  p loses the responsibility for k wrt hts  if cp k is in vcsp then p removes it from vcsp. 
when p receives a timestamp request for a key k  it checks for the existence of cp k in vcsp. if cp k is in vcsp then p generates the timestamp for k using cp k. otherwise p initializes cp k  appends it to vcsp and then generates the timestamp using cp k  see figure 
1 .  
the data structure used for vcsp is such that given a key k seeking cp k in vcsp can be done rapidly  e.g. a binary search tree. also  for minimizing the memory cost  when a counter gets out of vcsp  p releases the memory occupied by the counter  i.e. only the counters involved in vcsp occupy a memory location. to prevent the problem of overflow  we use a large integer  e.g. 1 bits  for the value of cp k. 
the following theorem shows that using vcsp and respecting its rules guarantees the monotonicity property. 
theorem 1: if the peer p  which is responsible for k wrt hts  for generating timestamps for k uses cp k that is in vcsp  then each generated timestamp for k is greater than any previously generated one. 
proof: let  t1  t1  be a p's period of responsibility for k wrt hts and let us assume that p generates a timestamp for k in  t1  t1 . rules 1 and 1 assure that at t1  cp k is not in vcsp. thus  for generating the first timestamp for k in  t1  t1   p should initialize cp k and insert it into vcsp  rule 1 . therefore  in each of its periods of responsibility for k wrt hts  p initializes cp k before generating the first timestamp for k. thus  each peer p  during each of its periods of responsibility for k wrt hts  initializes cp k before generating the first timestamp for k  so by lemma 1 the proof is complete. 뫳 
the other kts operation last ts k   which we used in section 1   can be implemented like gen ts except that last ts is simpler: it only returns the value of cp k and does not need to increase its value. 
1 counter initialization 
initializing the counters is very important for maintaining the monotonicity property. recall that for initializing cp k  the peer p  which is responsible of timestamping for k  assigns to cp k the value of cq k where q is the last peer that has generated a timestamp for k. but  the question is how p can acquire cq k. to answer this question  we propose two initialization algorithms: direct and indirect. the direct algorithm is based on transferring directly the counters from a responsible of timestamping to the next responsible. the indirect algorithm is based on retrieving the value of the last generated timestamp from the dht.  
1.1 direct algorithm for initializing counters 
with the direct algorithm  the initialization is done by directly transferring the counters from a responsible of timestamping to the next one at the end of its responsibility. this algorithm is used in situations where the responsible of timestamping loses its responsibility in a normal way  i.e. it does not fail. 
let q and p be two peers  and k' k be the set of keys for which q is the current responsible of timestamping and p is the next responsible. the direct algorithm proceeds as follows. once q reaches the end of its responsibility for the keys in k'  e.g. before leaving the system  it sends to p all its counters that have been initialized for the keys involved in k'. let c be an empty set  q performs the following instructions at the end of its responsibility: 
    for each cq k 뫍 vcsq do      if  k뫍k'  then 
        c := c + {cq k}; 
  send c to p;  
at the beginning of its responsibility for the keys in k'  p initializes its counters by performing the following instructions: 
   for each cq k 뫍 c do begin        new cp k ;     cp k.value := cq k.value;     vcsp := vcsp + {cp k};   end; 
1.1 application to can and chord 
the direct algorithm initializes the counters very efficiently  in o 1  messages  by sending the counters from the current responsible of timestamping to the next responsible at the end of its responsibility. but  how can the current responsible of timestamping find the address of the next responsible  the dht's lookup service does not help here because it can only lookup the current responsible for k  i.e. rsp k  hts   and cannot return the address of the next responsible for k. to answer the question  we observe that  in dhts  the next peer that obtains the responsibility for a key k is typically a neighbor of the current responsible for k  so the current responsible of timestamping has the address of the next one. we now illustrate this observation with can and chord  two popular dhts. 
let us assume that peer q is rsp k h  and peer p is nrsp k h  where k뫍k and h뫍h. in can and chord  there are only two ways by which p would obtain the responsibility for k wrt h. first  q leaves the p1p system or fails  so the responsibility of k wrt h is assigned to p. second  p joins the p1p system which assigns it the responsibility for k wrt h  so q loses the responsibility for k wrt h despite its presence in the p1p system. we show that in both cases  nrsp k h  is one of the neighbors of rsp k h . in other words  we show that both can and chord have the important property that nrsp k h  is one of the neighbors of rsp k h  at the time when rsp k h  loses the responsibility for k wrt h. 
can. we show this property by giving a brief explanation of can's protocol for joining and leaving the system . can maintains a virtual coordinate space partitioned among the peers. the partition which a peer owns is called its zone. according to can  a peer p is responsible for k wrt h if and only if h k   which is a point in the space  is in p's zone. when a new peer  say p  wants to join can  it chooses a point x and sends a join request to the peer whose zone involves x. the current owner of the zone  say q  splits its zone in half and the new peer occupies one half  then q becomes one of p's neighbors. thus  in the case of join  nrsp k h  is one of the neighbors of rsp k h . also  when a peer p leaves the system or fails  its zone will be occupied by one of its neighbors  i.e. the one that has the smallest zone. thus  in the case of leave or fail  nrsp k h  is one of the neighbors of rsp k h    and that neighbor is known for rsp k h . 
chord. in chord   each peer has an m-bit identifier  id . the peer ids are ordered in a circle and the neighbors of a peer are the peers whose distance from p clockwise in the circle is 1i for 1뫞 i뫞 m. the responsible for k wrt h is the first peer whose id is equal or follows h k . consider a new joining peer p with identifier idp. suppose that the position of p in the circle is just between two peers q1 and q1 with identifiers id1 and id1  respectively. without loss of generality  we assume that id1 id1  thus we have id1 idp id1. before the entrance of p  the peer q1 was responsible for k wrt h if and only if id1 h k 뫞id1. when p joins chord  it becomes responsible for k wrt h if and only if id1 h k 뫞idp. in other words  p becomes responsible for a part of the keys for which q1 was responsible. since the distance clockwise from p to q1 is 1  q1 is a neighbor of p. thus  in the case of join  nrsp k h  is one of the neighbors of rsp k h . when  a peer p leaves the system or fails  the next peer in the circle  say q1  becomes responsible for its keys. since the distance clockwise from p to q1 is 1  q1 is a neighbor of p.  
following the above discussion  when a peer q loses the responsibility for k wrt h in chord or can  one of its neighbors  say p  is the next responsible for all keys for which q was responsible. therefore  to apply the direct algorithm  it is sufficient that  before losing its responsibility  q sends to p its initialized counters  i.e. those involved in vcsq. 
1.1 indirect algorithm for initializing counters 
with the direct algorithm  the initialization of counters can be done very efficiently. however  in some situations the direct algorithm cannot be used  e.g. when a responsible of timestamping fails. in those situations  we use the indirect algorithm. for initializing the counter of a key k  the indirect algorithm retrieves the most recent timestamp which is stored in the dht along with the pairs  k  data . as described in section 1  peers store the timestamps  which are generated by kts  along with their data in the dht. 
the indirect algorithm for initializing the counters proceeds as follows  see figure 1 . let k be a key  p be the responsible of timestamping for k  and hr be the set of replication hash functions 
indirect initialization k  var cp k  begin  	 
   tsm := -1;    for each  h뫍hr  do begin 
      {data  ts} := dht.geth k ; if  tsm   ts  then           tsm := ts;     end; 
   cp k.value := tsm + 1; end;      figure 1. indirect algorithm for initializing counters which are used for replicating the data in the dht as described in section 1. to initialize cp k   for each h뫍hr  p retrieves the replica  and its associated timestamp  which is stored at rsp k  h . among the retrieved timestamps  p selects the most recent one  say tsm  and initializes cp k to tsm + 1. if no replica and timestamp is stored in the dht along with k  then p initializes cp k to 1. 
if p is at the beginning of its responsibility of timestamping for k  before using the indirect algorithm  it waits a while so that the possible timestamps  which are generated by the previous responsible of timestamping  be committed in the dht by the peers that have requested them. 
let cret be the number of messages which should be sent over the network for retrieving a data from the dht  the indirect algorithm is executed in o  hr  cret  messages. 
let us now compute the probability that the indirect algorithm retrieves successfully the latest version of the timestamp from the dht. we denote this probability as ps. let t be the time at which we execute the indirect algorithm  and pt be the probability of currency and availability at t  see section 1 for the definition of the probability of currency and availability . if at least one of the peers  which are responsible for k wrt replication hash functions  owns a current replica then the indirect algorithm works successfully.  thus  ps can be computed as follows: 
ps = 1 -  the probability that no current replica is available at peers which are responsible for k wrt replication hash functions  thus  we have: 
ps =1  1 pt   hr                                               
in this equation   hr  is the number of replication hash functions. by increasing the number of replication hash functions  we can obtain a good probability of success for the indirect algorithm. for instance  if the probability of currency and availability is about 1%  then by using 1 replication hash functions  ps is more than 1%.  
by adjusting the number of replication hash functions  the probability of success of the indirect algorithm is high but not 1%. thus  there may be some situations where it cannot retrieve the latest version of timestamp  in which case the counter of the key is not initialized correctly. to deal with these situations in a correct way  we propose the following strategies: 
  recovery. after restarting  the failed responsible of timestamping contacts the new responsible of timestamping  say p  and sends it all its counters. then  the new responsible of timestamping compares the received counters with those initialized by the indirect algorithm and corrects the counters which are initialized incorrectly  if any . in addition  if p has generated some timestamps with an incorrect counter  it retrieves the data which has been stored in the dht with the latest value of the incorrect counter and reinserts the data into the dht with the correct value of the counter. 
  periodic inspection. a responsible of timestamping which takes over a failed one  and which has not been contacted by it  periodically compares the value of its initialized counters with the timestamps which are stored in the dht. if a counter is lower than the highest timestamp found  the responsible of timestamping corrects the counter. furthermore  it reinserts the data which has been stored in the dht with the latest value of the incorrect counter  if any . 
1 validity of counters 
in section 1  the third rule for managing vcss states that if a peer p loses the responsibility for a key k wrt hts  then p should remove cp k from vcsp  if it is there . we now discuss what p should do in order to respect the third rule for vcsp. if the reason for losing responsibility is that p has left the p1p system or failed  then there is nothing to do  since when p rejoins the p1p system  it sets vcsp= . therefore  we assume that p is present in the p1p system and loses the responsibility for k wrt hts because some other peer joins the p1p system and becomes responsible for k. 
we can classify dht protocols in two categories: responsibility 
loss aware  rla  and responsibility loss unaware  rlu . in an rla dht  a peer that loses responsibility for some key k wrt h and is still present in the p1p system detects its loss of responsibility. a dht that is not rla is rlu. 
most dhts are rla  because usually when a new peer p becomes rsp k  h   it contacts prsp k h   say q  and asks q to return the pairs  k  data  which are stored at q. thus  q detects the loss of responsibility for k. furthermore  in most of dhts  p is a new neighbor of q  see section 1.1   so when p arrives q detects that it has lost the responsibility for some keys. for the dhts that are rla  the third rule of vcs can be enforced as follows. when a peer p detects that it has lost the responsibility for some keys wrt hts  it performs the following instructions:  
for each cp k뫍vcsp do 
     if p뫛rsp k hts  then         remove cp k from vcsp 	 
if the dht is rlu  then rule 1 can be violated. let us illustrate with the following scenario. let k be a key and p the peer that is rsp k hts  which generates some timestamp for k  i.e. cp k is in vcsp. suppose another peer q joins the p1p system  becomes rsp k  hts  and generates some timestamps for k. then q leaves the dht  and p becomes again rsp k hts . in this case  if p generates a timestamp for k using cp k 뫍vcsp  the generated timestamp may be equal or less than the last generated timestamp for k  thus violating the monotonicity property as a result of violating rule 1. to avoid such problems in a dht that is rlu  we impose that rsp k hts  assumes that after generating each timestamp for k  it loses its responsibility for k wrt hts. thus  after generating a timestamp for k  it removes cp k from vcsp. therefore  rule 1 is enforced. however  by this strategy  for generating each timestamp for k we need to initialize cp k  and this increases the cost of timestamp generation. 
1. performance evaluation 
in this section  we evaluate the performance of our update management service  ums  through implementation and simulation. the implementation over a 1-node cluster was useful to validate our algorithm and calibrate our simulator. the simulation allows us to study scale up to high numbers of peers  up to 1 peers .  
the rest of this section is organized as follows. in section 1  we describe our experimental and simulation setup  and the algorithms used for comparison. in section 1  we first report experimental results using the implementation of ums and kts on a 1-node cluster  and then we present simulation results on performance by increasing the number of peers up to 1. in sections 1  we evaluate the effect of the number of replicas  which we replicate for each data in the dht  on performance. in section 1  we study the effect of peers' failures on performance. in section 1  we study the effect of the frequency of updates on performance. 
1 experimental and simulation setup 
our implementation is based on chord  which is a simple and efficient dht. chord's lookup mechanism is provably robust in the face of frequent node fails  and it can answer queries even if the system is continuously changing. we implemented ums and kts as a service on top of chord which we also implemented. in our implementation  the keys do not depend on the data values  so changing the value of a data does not change its key. 
we tested our algorithms over a cluster of 1 nodes connected by a 1-gbps network. each node has 1 intel xeon 1 ghz processors  and runs the linux operating system. we make each node act as a peer in the dht. 
to study the scalability of our algorithms far beyond 1 peers  we implemented a simulator using simjava . to simulate a peer  we use a simjava entity that performs all tasks that must be done by a peer for executing the services kts and ums. we assign a delay to communication ports to simulate the delay for sending a message between two peers in a real p1p system. overall  the simulation and experimental results were qualitatively similar. thus  due to space limitations  for most of our tests  we only report simulation results. 
the simulation parameters are shown in table 1. we use parameter values which are typical of p1p systems . the latency between any two peers is a normally distributed random number with a mean of 1 ms. the bandwidth between peers is also a random number with normal distribution with a mean of 1  kbps . the simulator allows us to perform tests up to 1 peers  after which simulation data no longer fit in ram and makes our tests difficult. therefore  the number of peers is set to be 1  unless otherwise specified.  
in each experiment  peer departures are timed by a random poisson process  as in  . the average rate  i.e. 뷂  for events of the poisson process is 뷂=1/second. at each event  we select a peer to depart uniformly at random. each time a peer goes away  another joins  thus keeping the total number of peers constant  as in  .  
peer departures are of two types: normal leave or fail. let failure rate be a parameter that denotes the percentage of departures which are of fail type. when a departure event occurs  our simulator must decide on the type of this departure. for this  it generates a random number which is uniformly distributed in  1..1 ; if the number is greater than failure rate then the peer departure is considered as a normal leave  else as a fail. in our tests  the default setting for fail rate is 1%. 
in our experiments  each replicated data is updated by update operations which are timed by a random poisson process. the default average rate for events of this poisson process is 뷂=1/hour. 
in our tests  unless otherwise specified  the number of replicas of each data is 1  i.e.  hr =1. 
table 1. simulation parameters 
simulation parameter values bandwidth normally distributed random number  mean = 1 kbps  variance = 1 latency normally distributed random number  mean = 1 ms  variance = 1 number of peers  1 peers   hr  1  peers' 	joins 	and 
departures timed by a random poisson process with 뷂=1/second updates on each data timed by a random poisson process with 뷂=1/hour failure rate 1% of departures  
although it cannot provide the same functionality as ums  the closest prior work to ums is the bricks project . to assess the performance of ums  we compare our algorithm with the bricks algorithm  which we denote as brk. we tested two versions of ums. the first one  denoted by ums-direct  is a version of ums in which the kts service uses the direct algorithm for initializing the counters. the second version  denoted by ums-indirect  uses a kts service that initializes the counters by the indirect algorithm. 
in our tests  we compare the performance of ums-direct  umsindirect and brk in terms of response time and communication cost. by response time  we mean the time to return a current replica in response to a query q requesting the data associated with a key. the communication cost is the total number of messages needed to return a current replica in response to q. for each experiment  we perform 1 tests by issuing q at 1 different times which are uniformly distributed over the total experimental time  e.g. 1 hours  and we report the average of their results. 
1 scale up 
in this section  we investigate the scalability of ums. we use both our implementation and our simulator to study the response time and communication cost of ums while varying the number of peers. 
using our implementation over the cluster  we ran experiments to study how response time increases with the addition of peers. 
figure 1 shows the response time with the addition of peers until 
1. 	the 	response 	time 	of all three algorithms grows logarithmically with the number of peers. however  the response time of ums-direct and ums-indirect is significantly better than brk. the reason is that  by using kts and determining the last generated timestamp  ums can distinguish the currency of replicas and return the first current replica which it finds while brk needs to retrieve all available replicas  which hurts response time. the response time of ums-direct is better than ums-
indirect because  for determining the last timestamp  ums-direct uses a version of kts that initializes the counters by the direct algorithm which is more efficient than the indirect algorithm used by ums-indirect. note that the reported results are the average of the results of several tests done at uniformly random times. 

figure 1. response time vs. number of peers 
 

figure 1. communication cost vs. number of peers 
using simulation  figure 1 shows the response time of the three algorithms with the number of peers increasing up to 1 and the other simulation parameters set as in table 1. overall  the experimental results correspond qualitatively with the simulation results. however  we observed that the response time gained from our experiments over the cluster is slightly better than that of simulation for the same number of peers  simply because of faster communication in the cluster. 
we also tested the communication cost of ums. using the simulator  figure 1 depicts the total number of messages while increasing the number of peers up to 1 with the other simulation parameters set as in table 1. the communication cost increases logarithmically with the number of peers. 

figure 1. communication cost vs. number of replicas 
1 effect of the number of replicas 
in this section  we study the effect of the number of replicas  which we replicate for each data in the dht  on the performance of mus. 
using the simulator  figures 1 and 1 show how respectively response time and communication cost evolve while increasing the number of replicas  with the other simulation parameters set as in table 1. the number of replicas has a strong impact on the performance of brk  but no impact on ums-direct. it has a little impact on the performance of ums-indirect because  in the cases where the counter of a key is not initialized  ums-indirect must retrieve all replicas from the dht. 
1 effect of failures  
in this section  we investigate the effect of failures on the response time of ums. in the previous tests  the value of failure rate was 1%. in this section  we vary the value of fail rate and investigate its effect on response time. 
figure 1 shows how response time evolves when increasing the fail rate  with the other parameters set as in table 1. an increase in failure rate decreases the performance of chord's lookup service  so the response time of all three algorithms increases. for the cases where the failure rate is high  e.g. more than 1%  the response time of ums-direct is almost the same as ums-
indirect. the reason is that if a responsible of timestamping fails  both ums-direct and ums-indirect need to use the indirect algorithm for initializing the counters at the next responsible of timestamping  thus their response time is the same. 

figure 1. response time vs. failure rate 
 

figure 1. response time vs. frequency of updates 
1 effect of update frequency 
in this section  we study the effect of the frequency of updates on the performance of ums. in the previous experiments  updates on each data were timed by a poisson process with an average rate of 1/hour. in this section  we vary the average rate  i.e. frequency of updates  and investigate its effect on response time. 
using our simulator  figures 1 shows how response time evolves while increasing the frequency of updates with the other simulation parameters set as in table 1. the response time decreases by increasing the frequency of updates. the reason is that an increase in the frequency of updates decreases the distance between the time of the latest update and the retrieval time  and this increases the probability of currency and availability  so the number of replicas which ums retrieves for finding a current replica decreases. 
1. related work 
in the context of distributed systems  data replication has been widely studied to improve both performance and availability. many solutions have been proposed in the context of distributed database systems for managing replica consistency   in particular  using eager or lazy  multi-master  replication techniques. however  these techniques either do not scale up to large numbers of peers or raise open problems  such as replica reconciliation  to deal with the open and dynamic nature of p1p systems. 
data currency in replicated databases has also been widely studied  e.g.          and . however  the main objective is to trade currency and consistency for performance while controlling the level of currency or consistency desired by the user. our objective in this paper is different  i.e. return the current  most recent  replica as a result of a get request. 
most existing p1p systems support data replication  but without consistency guarantees. for instance  gnutella  and kazaa   two of the most popular p1p file sharing systems allow files to be replicated. however  a file update is not propagated to the other replicas. as a result  multiple inconsistent replicas under the same identifier  filename  may co-exist and it depends on the peer that a user contacts whether a current replica is accessed. 
pgrid is a structured p1p system that deals with the problem of updates based on a rumor-spreading algorithm . it provides a fully decentralized update scheme  which offers probabilistic guaranties rather than ensuring strict consistency. however  replicas may get inconsistent  e.g. as a result of concurrent updates  and it is up to the users to cope with the problem. 
the freenet p1p system  uses a heuristic strategy to route updates to replicas  but does not guarantee data consistency. in freenet  the query answers are replicated along the path between the peers owning the data and the query originator. in the case of an update  which can only be done by the data's owner   it is routed to the peers having a replica. however  there is no guarantee that all those peers receive the update  in particular those that are absent at update time. 
many of existing dht applications such as cfs    past  and oceanstore  exploit data replication for solving the problem of hot spots and also improving data availability. however  they generally avoid the consistency problem by restricting their focus on read-only  immutable  data. 
the bricks project  deals somehow with data currency by considering the currency of replicas in the query results. for replicating a data  bricks stores the data in the dht using multiple keys  which are correlated to the key k by which the user wants to store the data. there is a function that  given k  determines its correlated keys. to deal with the currency of replicas  bricks uses versioning. each replica has a version number which is increased after each update. however  because of concurrent updates  it may happen that two different replicas have the same version number thus making it impossible to decide which one is the current replica. in addition  to return a current replica  all replicas need be retrieved in order to select the latest version. in our solution  concurrent updates raise no problem  i.e. this is a consequence of the monotonicity property of timestamps which are generated by kts. in addition  our solution does not need to retrieve all replicas  and thus is much more efficient.  
1. conclusion 
to ensure high data availability  dhts typically rely on data replication  yet without currency guarantees for updateable data. in this paper  we proposed a complete solution to the problem of data availability and currency in replicated dhts.  our main contributions are the following. 
first  we proposed a new service called update management service  ums  which provides efficient retrieval of current replicas. for update operations  the algorithms of ums rely on timestamping. ums supports concurrent updates. furthermore  it has the ability to determine whether a replica is current or not without comparing it with other replicas. thus  unlike the solution in   our solution does not need to retrieve all replicas for finding a current replica  and is much more efficient. 
second  we gave a probabilistic analysis of ums's communication cost by computing the expected number of replicas which ums must retrieve. we proved that this number is less than the inverse of the probability of currency and availability. thus  except for the cases where the availability of current replicas is very low  the expected number of retrieved replicas is typically small  e.g. if at least 1% of replicas are current and available then this number is less than 1. 
third  we proposed a key-based timestamping service  kts  which generates monotonically increasing timestamps in a completely distributed fashion  using local counters. the dynamic behavior of peers causes no problem for kts. to preserve timestamp monotonicity  we proposed a direct and an indirect algorithm. the direct algorithm deals with the situations where peers leave the system normally  i.e. without failing. the indirect algorithm takes into account the situations where peers fail. although the indirect algorithm has high probability of success in general  there are rare situations where it may not be successful at finding the current replica. we proposed two strategies to deal with these situations. 
fourth  we validated our solution through implementation and experimentation over a 1-node cluster and evaluated its scalability through simulation over 1 peers using simjava. we compared the performance of ums and brk  from the brick project  which we used as baseline algorithm. the experimental and simulation results show that using kts  ums achieves major performance gains  in terms of response time and communication cost  compared with brk. the response time and communication cost of ums grow logarithmically with the number of peers of the dht. increasing the number of replicas  which we replicate for each data in the dht  increases very slightly the response time and communication cost of our algorithm. in addition  even with a high number of peer fails  ums still works well. in summary  this demonstrates that data currency  a very important requirement for many applications  can now be efficiently supported in replicated dhts. 
