a node role is a subjective characterization of the part it plays in a network structure. knowing the role of a node is important for many link mining applications. for example  in web search  nodes that are deemed to be authorities on a given topic are often found to be most relevant to the user's
techniques can be improved by knowledge of such roles. a network. knowledge of the node role  popularity  centrality e comf queries. there are a number of metrics that can be used to assign roles to individual nodes in a network  including degree  closeness  and betweenness. none of these metrics 
underlies the network. in this paper we define community- figure 1: groups within a network based roles that the nodes can assume  ambassadors  big fish  loners  and bridges  and show how existing link mining
the number of communities linked to a node. using this authority  is useful for many link mining applications such metric and a modification of degree  we show how to assign network community refers to groups of nodes that share benefits of knowing the community-based node roles in the similar properties. despite its importance  none of the metcontext of link-based classification and influence maximiza- community concept. knowing the role that a node assumes with respect to its related communities would be a new and 1.	and crime analysis  knowing that a person has contacts with
however  take into account the community structure thatnew community-based metric is introduced for estimatingcommunity-based roles to the nodes. we also illustrate thetion. introduction	as web search  threat detection  and co-citation analysis.rics that are used to define node roles explicitly use thevaluable tool for analysts. for example  in threat detectionin this paper  we define the various community-based rolesd	g unity 1ca	b	ki com	unity 1hj
	a network consists of nodes connected by directed or undi-	many groups could be valuable information.
rected links. it is used to represent complex  relational datasuch as web pages or social networks. the nodes can be a node can assume and show how existing link mining techassigned roles  which are subjective characterization of the niques may benefit from the knowledge of such roles. for part they play in the network. for example  within the web  example  a node whose role is defined as an ambassador has an authoritative page is one that is referred to by many links to many nodes from different communities while another pages whereas a hub page is one that has hyperlinks other node whose role is defined as a big fish has links only to many other pages. to other nodes in the same community. we offer two ex-
　there are a number of metrics that can be used to de- amples to illustrate the advantages of assigning communitytermine the roles of individual nodes in a network. among based roles to nodes  namely  influence maximization and those most widely used are degree  closeness  betweenness  link-based classification.
and rank. degree can be used to assess a node's popular- the problem of influence maximization can be thought of ity while closeness and betweenness can be used to assess as finding the best k people to target in order to maximize its centrality. rank  such as that used in the hits  or the number of people that will eventually be influenced  e.g.
pagerank  algorithms are measures of authority within a	weight between 1 and 1 representing the probability thatadopt an idea  buy a product  etc . links are assigned a
one node influences another when it is activated. several al-
permission to make digital or hard copies of all or part of this work for gorithms  1  1  have been developed in recent years to idenpersonal or classroom use is granted without fee provided that copies are tify the most promising set of nodes to activate. these alnot made or distributed for pro t or commercial advantage and that copies gorithms however focus only on maximizing the number of bear this notice and the full citation on the rst page. to copy otherwise  to activated nodes at the end of the influence diffusion process. republish  to post on servers or to redistribute to lists  requires prior speci c in some cases  it may be more useful to maximize the num-
joint 1th webkdd and 1st sna-kdd workshop '1   webkdd/sna-permission and/or a fee.kdd'1  august 1  1   san jose  california   usa	ber of communities that are influenced. as an example  amarketer might be interested in not only informing as many
copyright 1 acm 1-1-1 ...$1.	people as possible about their product but might also wish
　

figure 1: classifying related objects
to maximize their reach to different demographic groups.
　figure 1 shows a small network of eleven nodes from two communities-nodes a-g belong to community 1 and nodes h-k belong to community 1. suppose that we wish to find the one best node in this network to maximize the spread of influence. current algorithms would choose to activate node d. depending on the influence diffusion model and weights of the links  activating node d may not influence any of the nodes in community 1. choosing node b  on the other hand  would elevate the chances that nodes in both communities are influenced.
　link-based classification is the task of categorizing nodes using the node features and its link information. to illustrate this task  consider the classification problem of predicting the political leaning of a person  either conservative  c  or liberal  l   given the network of people shown in figure 1. the training examples are nodes labeled as c or l and the test examples are nodes labeled as  . links between the people represent friendship and the dashed ovals indicate the two communities. several studies  1  1  1  have shown that the performance of traditional classifiers can be improved by using link information-specifically the class of the neighboring nodes. in figure 1  one of the test examples is friends with only people in its community while the other has friends in both. it is likely that using the classes of neighbors would be more effective with the former test example than the latter. knowledge of community-based roles could be helpful to link-based classifiers in deciding when to use information about a node's neighbor.
　our proposed community-based node roles can be easily defined when the membership information of the communities is available. otherwise  a metric is needed to estimate the number of communities related to each node in the network. we have therefore introduced a new metric called rawcomm that gives a reasonably accurate measure of community belongingness  assuming that the links in the network strongly support the community structure. even when the assumption only weakly holds  our experimental results show that it still provides enough useful information to correctly assign community-based roles to nodes. finally  we have conducted extensive experiments to demonstrate the benefits of knowing the community-based node roles in the context of link-based classification and influence maximization.
1.	related work
　a network is an interconnected set of nodes related to each other by links that can have weights associated with them.
we will only be considering undirectional  unweighted links in this study. recent research has several categories of networks. a regular network is one where all of the nodes have a link to a fixed number of other nodes. a random network is one where the links between the nodes are completely random. small world networks  are somewhere between regular and random networks. they are characterized by many small groups of tightly connected nodes  like regular networks  with a few random links that connect the small groups  like random networks . because of this  small world networks have the property that every two nodes are connected by a relatively short path. scale-free networks  have the property that the degrees of the nodes must follow a power law function; the probability pr k  that a node will have k neighbors is proportional to k y where y is usually between 1 and 1.
1	node roles
　role is a concept that is used to describe the behavior of a node in relationship to its neighbors and to the network at large. the discipline of social network analysis contains several centrality measures used to determine the roles of nodes in a network. of these the most prominent are degree  closeness and betweenness . degree is the sum of the i is a 1 indicator function. p links attached to a node  cd ni  = j i  i j  （ e where
closeness is the reciprocal of the sum of all the geodesic
 shortest  distances from a given node to all others  cc ni  = i where d u v  is the geodesic distance
 1
from u to v. nodes with a small cc score are closer to the center of the network while those with higher scores are closer to the edge. betweenness  another metric that measures how centrally located a node is  is defined as cb ni  = where gjk is the number of geodesic paths from j to k and gjk ni  is the number of geodesic paths from j to k that go through i. a higher betweenness value for a node means that it is on more shortest-paths between nodes  which is an indication of the node's importance.
　all three of these metrics have values greater than zero with an upper bound based on the size of the network. all three can be normalized to a value between 1 and 1 by dividing it with its maximum value.
1	communities
　an assumption of networks as described above  is that there are communities of nodes which are not explicitly exposed but that the links infer. in social networks we think of friends  family  and colleagues as forming communities. data mining and link mining techniques have largely ignored utilizing this potentially helpful knowledge.
　even though there has not been an effort to exploit this knowledge there have been many clustering and community finding algorithms proposed for linked data  for a review see  . some are global while others find a community for a small set of nodes. they can also be overlapping or disjoint. all of them use the links to form the communities  that is  they implicitly assume that the links provide evidence for the communities.
　the number and diversity of the proposed community finding techniques suggests that there is no agreed-upon best technique which makes it difficult to explicitly use community knowledge.
1.	community-based node roles
　in this section we will define the community-based roles. we then introduce a novel approach to measuring the compatibility between communities and the link structure of a network. finally we present a new metric that we use to estimate the number of communities to which a node belongs.
1	community metric
　our measure of community assumes that a community is defined by a clique  maximal complete subgraph  in a network. so a group that forms a clique will be considered one community; another group that forms two non-overlapping cliques will be considered two communities. consider a clique of 1 nodes and remove one of the edges: it is no longer one community but we would not think of it as two either. we are looking for a metric that will assign a community value to such a group of slightly more than one. a group that forms two non-overlapping cliques except one connecting edge should have a score of slightly less than two.
　it should be noted that the clustering coefficient used by watts and strogatz  to identify small world networks does give a measure of a components' cliqueness but for a given node it measures the ratio of actual edges within its immediate neighborhood to the total possible edges. we are looking for a metric that approximates the number of communities that a nodes' neighbors form.
　our metric  which we call rawcomm is to be an approximate measure of the number of communities to which a node is attached. we assume that the communities are hidden but that links provide evidence of community. we define p as the probability that two linked nodes are in the same community and q as the probability that two non-linked nodes are in different communities. by using the probabilities p and q in our definition the metric becomes more flexible and is therefore useful for communities defined by means other than cliques.
　given a network g =  v e  without the community assignments the values of p and q will not be known but approximations can be calculated by sampling or finding a similar network that does have community assignments. in a previous work  we defined incomplete edges as edges  links  that connect two nodes in different clusters  communities  and impure edges as non-links that appear within a cluster  community . the approximation for p would then be the complete links divided by the total number of links and for q the number of pure links divided by the total number of non-links:
complete node pairs
	p	=
　　　total linked node pairs pure node pairs q	=
　　　　　　　　total non-linked node pairs our metric rawcomm is defined as:
where n u  is the neighborhood of - that is all of the nodes that are directly linked to u - and τu v  is
1
τu vi  = 1 +vj（n u  i v  v   ， p + i． vi vj  ，  1   q  i	j
between x and ycb and 1 otherwise.d a i．is 1 if there is not a linkeg f figure 1: calculating τ
i x y  is an indicator function that is 1 if there is a link
and 1 otherwise. it is convenient to think of τu v  as the contribution of node v to node u's rawcomm score.
　node v's contribution to u's rawcomm community score depends on how connected v is to the nodes in u's neighborhood. the denominator in the definition of τ is the expected number of other nodes in u's neighborhood are in a community with v . the 1 represents the node vi itself and the expressionj  ，  1   q  is the probability of vi and vj being in the same community. taking the reciprocal of the expectation then is the contribution of node vi to u's rawcomm score. example 1: refer to the network in figure 1 where we will assume that p = 1 and q = 1. node b is connected to a but not to any others so τa b  = 1 which essentially adds one community to a's rawcomm score. τa c  = τa d  = 1 so their sum also adds one community to a's rawcomm score. so far the calculation is straightforward.
　looking at node e we see that it is connected to only one other node with a so τa e  = 1. likewise τa g  = 1. since f is connected to two other nodes with a  τa f  = 1. so the sum of e  f and g's scores is 1. we interpret this to mean that those three nodes appear to be part of the same community but because e and g are not connected we are not sure - it could be more than one so their sum is slightly higher than one. note that if e and g were connected the four would form a clique and the sum of their scores would be exactly one.
　in general a group of nodes that form a clique will have τs that sum up to p. if they are connected but not completely  as the number of missing links grows the contribution of their scores also grows. the interpretation is that as groups of nodes become less densely connected the higher the probability that they form multiple communities which is the property discussed in the beginning of this section.
1	community-based roles
　we define the community-based role of a node according to the number of communities and links incident to it. figure 1 shows a community-degree chart that is divided into four quadrants for the four different roles. the vertical axis represents the degree while the horizontal axis represents the community metric.
　the community-based node role is identified based on which of the four quadrants a node falls into. nodes in the upper right quadrant are those with a high degree and a high community score. they act as ambassadors  providing connections to many different communities. the upper left quadrant contains what we call big fish from the cliche  big
　
method from shi and malik  can be much greater since
degree and low community score.however  depending on the distributions of degree and com-in the lower left are thebeyond the immediate neighborhoodmax1di al 1 lonbergs fcomislonersh unity metric-those with a low relativeambbridgaedsors 1 pable of adopting an idea  purchasing a product or some-our experiments we chose to use the independent cascadenetwork. we assume that the nodes in the network are ca-model. under this model  influence is spread from node toas described in the introduction  influence maximizationroles it involves finding the eigenvalues of the network.
	1.	applications of community-based
community-based roles can be useful in a number of ways. just by themselves these roles can provide useful informavtion to analysts in areas as such as anti-terrorism and law enforcement. in searching for potential terrorist threats  for example  analysts may find it useful to concentrate on suspects with certain roles. if they were looking for persons with few friends but having diverse contacts they could focus on bridges.
community-based roles could also be utilized in existing techniques. the area of link mining has a number of techniques that use the relationships between objects to rank
 figure 1: community-degree chart objects  select influential nodes  find communities as well other other tasks. many of them could potentially benefit from knowledge of the objects' community role. we will
fish in a small pond  meaning that they are very importanterergt	discuss the two techniques of influence maximization and
only within a community. this is due to their having a high link-based classification. degree but a relatively small community score. in the lower
right quadrant are those with a low degree but a high com- 1 in uence maximization munity score. these we call bridges because they serve as bridges between a small number of communities. finally  is concerned with finding the most influential nodes in a
　the metrics shown in the community-degree chart have thing similar. this process is referred to as activating. we munity metric  we subtracted the minimum and divided bybeen normalized to values between 1 and 1. for the com- also assume that nodes that are activated have the abilityto influence their immediate neighbors who themselves may the range between maximum and minimum. for degree  we choose to activate. the problem becomes choosing the best us a relative degree score between 1 and 1. in our experi-divided by the highest degree node in the network  giving nodes to initially activate in order to maximize the numberof activated nodes at the end of the process. ments  we chose a threshold of .1 to classify the node roles; in the paper by kempe  et al  several models are intromunity metric scores  other thresholds can be chosen. duced that describe the behavior of the node activation. in
1	node in discrete steps. a node i that becomes active in step
　in our definition of rawcomm we consider only the ef- t has one chance to make his inactive neighbors active in fect of the nodes in the immediate neighborhood of a node. step t + 1. the probability that node i will activate node j while using information beyond the immediate neighbor- in their paper will be called the edge weight.
hood might improve the accuracy of rawcomm it would be the work in this area is exclusively concerned with maxmuch more computationally expensive. for this paper we imizing only the raw number of nodes activated. however  used a normalized rawcomm primarily to define roles. it we propose extending the problem to focus on the number is important to have a relatively good estimate but abso- of communities covered. a community is covered if one of lute accuracy is not critical. if a particular node's score is the nodes in the community is activated. our approach will slightly off  it is still given the proper role designation then be to choose the initial set of nodes using the communityrawcomm has accomplished its task. the important aspect based node roles in order to maximize the groups covered. of commpct is that it is proportionally accurate - that is a the results of our experiments will show that using roles to node with a high probability of being attached to many com- maximize group coverage shows improvement over the other munities will have a correspondingly high commpct score. influence maximization methods.
1	analysis of algorithm	1	link-based classi cation
　the runtime complexity for rawcomm for a single node link-based classification uses both the attribute data from is o δ1  where δ is the maximum degree of the network. the objects as well as data acquired using the links. previso calculating it for all the nodes is o nδ1  where n is the ous studies have shown that ordinary classification can be number of nodes. the space requirements are o n1  which improved by using linked data. chakrabarti  et al. have is the size of the network. shown that using linked data can be helpful in some cir-
clust coefraw
commcomm
pctnodecdcccba1.1.1.1.1.1b1.1.1.1.1.1c1.1.1.1.1.1d1.1.1.1.1.1e1.1.1.1.1.1f1.1.1.1.1.1g1.1.1.1.1.1h1.1.1.1.1.1i1.1.1.1.1.1j1.1.1.1.1.1k1.1.1.1.1.1　this compares favorably to other methods of finding com- cumstances. in their paper  the authors were able to show munities. the complexity of determining the community that in some circumstances using the data from neighbors membership given the network structure is o nk . using a is not helpful but using the class from neighbors can be. a community finding algorithm such as the normalized cut study by yang  et al.  shows that data sets can contain dia regularity nodes tend to link to nodes of the same class.fia gure 1: sample networkb	c	de	f	g	hk	ij	figcoefficienture 1: comparison of rawcomm to clustering a 	 b 
different types of regularities. for example with encyclope-	table 1: comparison of metrics
they conclude that using the class of neighbors is helpful for some data sets while in others it is not - it depends upon the regularities present.
　since different data sets contain different regularities we contend that different regularities could exist in the same collection. it could be possible that some nodes can benefit from knowing the class of their neighbors while other nodes would not benefit. to improve the classifier under these conditions requires knowing which nodes should make use of their neighbor's class.
　we propose to use our community-based roles to decide which nodes would benefit from its neighbor's class. we hypothesized that big fish and loners that have a low community metric score are more likely to be conformists and therefore are more likely to have the same class as their neighbors.
conversely  bridges and ambassadors who are connected to to be connected to two communities  degree is 1 but rawmany groups are more likely to be independent. we pro- comm is 1. closeness and betweenness like degree corpose that using the neighbor's class for loners and big fish relate somewhat with rawcomm but by comparing nodes f and not for ambassadors and bridges will improve the accu- and g it is obvious that closeness does not capture the comracy of a link-based classifier. in the experiments section we munity knowledge. we can see that betweenness also fails show results that support this proposition. to capture this knowledge by comparing node c  1 communities  with node g  1 communities .
1.	experimental evaluation	what  negatively  related rawcomm will always give a bet-although clustering coefficient and rawcomm are some-
experiments which will demonstrate the distinctiveness andthe purpose of this section is to provide the results of ter estimate of the number of communities. this is shown infigure 1 where the center node in both  a  and  b  have the utility of rawcomm. specifically we show that: same clustering coefficient it appears that in  a  the node
  rawcomm provides community information about a belongs to two communities  whereas in  b  it appears to belong to three. this is borne out by the rawcomm values
	node that is not available from other metrics.	of 1 for  a  and 1 for  b . it should be clear by now that
  rawcomm is a proportionally accurate measure of the	commpct is a unique metric.
	number of communities to which a node belongs.	1	comparison to community- nding algorithm
  the accuracy of rawcomm is relative to the extent to in this section we will compare the rawcomm metric to acwhich the community structure aligns with the link tually finding the communities. normalized cut  ncut   structure of the network. is a graph segmenting algorithm from the family of spectral partitioning methods which has gained much attention re-
  the community-based role nodes follow a fairly pre- cently for its ability to globally optimize partitions. we will dictable distribution use this algorithm to find communities for a comparison to
1	distinctiveness of rawcomm	our rawcomm metric.since the algorithm requires that the number of partitions
　since rawcomm is a new metric it is instructive to il-  or communities  is given as a parameter we ran on every lustrate how it differs from the other metrics in terms of possible number of communities from 1 to n   1. we comcommunity information. looking at figure 1 we will show pared the algorithms on the facebook data as well as many that different nodes will have high scores for the different of the sets from uci-net. we show the results below from metrics. table 1 lists the values for degree  closeness  be- a typical set  karate  which represents the 1 relationships tweenness centrality as well as clustering coefficient  raw- between the 1 students of a karate studio. in each iteration comm  we used p = q = 1 for simplicity  and commpct. we used ncut to find k communities and then for each node
notice that the low rawcomm is 1 and the high is 1.	we determined the actual number of communities to which
　the degree metric appears to have many of the same val-	it was connected. we could then compare that to the numues as rawcomm but notice that for node g which appears	ber of communities predicted by rawcomm. we calculated rawcomm two ways: first by using p = q = 1 for a baseline in its own cluster. degree would be a perfect predictor of and second by calculating the p and q from the communities community adjacency. however  such small communities are that were found using ncut. notice that when the number often of little value. looking at the column under q it is of communities is small many of the edges will be within the obvious that the more natural community structures are in communities as will many of the non-edges  so p will be high the range of 1 to 1.
and q will be low. when the number of communities is large	one of the key findings can be seen from comparing columns
p will be low and q will high. 1 where we use p = q = 1 and 1 where we estimate p and q from the communities that are found from ncut. when p
	table 1: rawcomm vs. ncut communities	anddramatic. all of the sse values from column 1 are less thanq are known or accurately predicted the improvement is
nbr ofdegreep=q=1p q est.qgroupsssessessevalue1.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.1111-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11.1.1.1-11  the number of nodes which indicates that for any given node  on average  our error is less than 1  a most encouraging result.
　even though the scores for rawcomm are much better when a good estimate of p and q are given  when p = q = 1 is used the prediction is still not too bad. estimating p and q are domain specific tasks  beyond the scope of this paper but if an estimate can be made it will almost certainly improve the results.
	1	effect of p and q on accuracy
           we have seen how rawcomm can estimate community belongingness in the previous section  now we will show the accuracy of that prediction is based on the alignment between the community and link structures. we use p and q to measure this alignment: a high value of p means that most links are within communities and a high value of q means that the non-links are between communities - this represents a good alignment. lower values of p and q mean a worse alignment. to see how rawcomm is affected by different values of p and q refer to figure 1. in  a  the network has twenty nodes that are grouped into 1 hidden communities and having the links shown. in  b  the same nodes are grouped in the same communities but the links are different. in this example even though we know the communities we assume that our algorithm for calculating rawcomm does not. we calculate p = 1 and q = 1 in the top network and p = 1 and a sum squared error  sse  statistic by summing up  for allthe sse for the difference of the degree and the number ofgraph. so a score of zero means a poor community groupingbecomes a better predictor which is also not unexpected.posed by . it is the fraction of links within a communityq  is a modularity measure pro- estimation gets progressively worse. this is not unexpectedefficients are .1 for network  a  and .1 for network  b table 1 shows the actual community membership num-1ough correlation is a popular statistic we will use the1 1  1  1 1  b 
　in order to get a measure for how well rawcomm compared to the actual number of communities we calculated
nodes  the square of the difference between the actual num-
ber of communities and the raw comm. we also calculated	figure 1: effect of p on τ
communities.
	the last column  labeled	bers versus the rawcomm estimation. the correlation co-
minus the expected value of the same fraction for a random while the sum squared error  sse  is .1 for  a  and 1 for while better groupings have numbers farther from zero.  b  . thus  as p and q get progressively smaller rawcomm's looking at table 1 we can see that the sse for rawcomm  columns 1 and 1  is much lower than for degree which is not
unexpected. as the number of communities grows degree	sse for comparing rawcomm to actual communities because
it captures the absolute difference between the values rather
imagine partitioning the nodes into n clusters - each node	than just the relative difference
　
table 1: comparison of rawcomm to actual
network anetwork bnodeactualrawcommactualrawcomm1.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.111111.1.1.1.11111though  since as p and q get smaller the links do not really provide very good evidence of community belonging. one can easily inspect the networks to see that the nodes in network  a  appear to fit more naturally into the communities than those of network  b .
1	role distribution
　so far we have described the metrics and introduced the chart that separates the node types. in this section we plot the node roles for two real data sets so that we can get a clearer picture of how the nodes in a network will be distributed in our chart.
　generally we expect to see a corridor of nodes between the lower left corner where both degreepct and commpct are zero to the upper right corner where they are both 1. the rationale for this is that nodes with a high degree are more likely to have a high rawcomm. for any given node the minimum rawcomm is 1  if the node is part of a clique. the maximum rawcomm is the degree  if the node is the center of a star. in the extremely rare network where the largest degree nodes are in cliques  the smallest degree nodes are stars and the others are inbetween  the distribution would go from upper left to lower right. however  in our studies we have never experienced such a network where the degreepct and commpct are negatively correlated.
　the movie data set  uci1  contains 1 hollywood actors and the movies they starred in. from the data we built a network  shown in figure 1  of the actors with links between actors who co-starred together in at least one movie. we also assembled a file of students who belonged to the web site facebook. members of the site can list friends of theirs  also on facebook . the network shown in figure 1 was built using 1 students from a college in michigan with links between friends. looking at the charts for both sets one can see that they do in fact have the kind of shape we expected. both sets tend to have more loners than other

figure 1: face book	figure 1: movies
roles. we would expect this in a scale-free type of network where there are many more low degree nodes and than high. inspecting the movie data more closely we selected some actors to represent the different roles. as an ambassador  burt lancaster had a career which spanned many decades in which he accumulated many costars but also starred in many diverse films  and so belonged to many communities . a typical big fish is geraldine chaplin who had a relatively large number of costars but many of the costars were costars of each other - not surprising as nine of her films had the same director. there were no bridges  strictly speaking  but actors who were close to being bridges include kathleen turner  jamie lee curtis and martin landau  all of whom have taken roles with diverse sets of other actors.
1.	community metric applications
　analysts in many diverse fields currently use existing metrics like degree and betweenness centrality. we anticipate that many of them may find the roles that we describe in section 1 to be a useful addition to their toolbox. for example  in a database of terror suspects  it may be meaningful to know that a particular individual is  say an ambassador. although we cannot demonstrate the value that the roles may have by themselves we can show how some current techniques can be improved using the metrics we introduced.
1	in uence maximization
　as described above the problem of influence maximization is finding the most influential k nodes in a network. this is important in the new field of viral marketing where wordof-mouth advertising can be very effective but discounts or promotions can be costly so marketers need to be judicious about their choice of customers or nodes to activate.
　in our experiments we used the independent cascade model discussed in the paper by kempe  et al . under this model  influence is spread from node to node in discrete steps. a node i that becomes active in step t has one chance to make his inactive neighbors active in step t + 1. the probability that node i will activate node j in their paper is called pi j but to avoid confusion with our p value we will call this the edge weight.
group coveragealgorithmnodesmoviedirectorgenrerandom1111degree1111greedy1111comm1111ambass1111degpct1111　we evaluated six algorithms. we compared the total number of nodes that are activated  which is the original goal of this problem. but we also compared how many groups are reached which is our objective in this experiment. the baseline random approach selects k nodes randomly. degree selects the k nodes with highest degree. the algorithm proposed by kempe  et al.  labeled greedy  chooses one new node each iteration  selecting the node that will result in the greatest increase of active nodes according to the independent cascade model. the last three methods use the table 1: comparison of algorithms using movie data metrics we have proposed in this paper. the method comm selects the k nodes with the highest rawcomm score and amb selects the k nodes with the highest sum of commpct and degreepct  while degpct selects the k nodes with the highest degreepct.
1.1	in uence maximization using movie data
　the algorithms were compared using two groups of data  the actors from the movie data set described earlier and some synthetic data sets. there is a link between the actors who co-starred in at least one movie together. actors who appeared in only one movie were removed. the network had 1 nodes and 1 links. to find the activated nodes we used the independent cascade model. for the size of the target set  k  we used 1. all of the edge weights were .1. for each method we calculated the activated nodes 1 times and then averaged the results.
　we used the data in the original file to form the hidden groups  communities . for the first set of groups we used the movie that the actors appeared in. there are 1 movies and actors belong to all of the movies that they star in. the second set was based on the director of the movie of which there are 1. an actor belongs to a director's group if they starred in any of that director's movies. the third set is based on genre. every movie is associated with one of 1 genres. for all three of the group sets an actor can belong to more than one group.
　the results for the movie data are summarized in table 1. the column labeled nodes is the average number of nodes activated by the target 1 nodes. the columns under percent of groups indicates how many groups out of the total had at least one node activated. the greedy method  not surprisingly  was able to activate the greatest number of nodes. however  even though ambass activated fewer nodes  it was able to reach more groups. comm also was able to spread to a large number of groups even though it selected fewer nodes than degree  greedy or ambass. that is not too surprising given that nodes connected to many groups are not necessarily high degree nodes. the degpct performed almost as poorly as random.
　we also calculated the p and q values  described in section 1  for the movie data set. the p values were 1 for all three groups. the q values were .1  .1 and .1 for movie  director and genre groups respectively. this means that for all three groups if there is a link between two actors there is a 1% chance that the two actors will be in the same group. if there is not a link between them then the q tells us that for group one it is nearly certain that they will not be in the same group whereas with group three there is only a 1% chance that they will be in different groups.
1.1	in uence maximization using synthetic data
　we wanted to see how the algorithms would behave using different network types. so the next set of experiments use synthetic data sets using the methods described in section 1. our approach was to create the synthetic network  run the algorithms and then to assign the nodes to groups for evaluation. early investigations explored clustering algorithms  single-link  complete-link and group-average  as well as the normalized cut spectral graph partitioning algorithm.
　these community finding methods all resulted in undesirable p and q values. single-link had a very high p value but a q near zero. complete-link and group-average had the opposite problem - high q but a low p. the normalized cut had a higher p value but it was still around only 1%. since our original conjecture is that the links provide evidence of groups we decided to use cliques  maximal complete components  as our grouping algorithm. intuitively this is appealing because it allows a node to belong to multiple communities. the downside is that because it is np-complete we needed to use smaller  sparser data sets.
　recall that in the influence maximization experiment using the movie data set an edge weight of .1 was used for computing the number of nodes that become activated. this is the same edge weight that was used in the study by kempe et al. in their study  as in ours the accurate edge weights are not known so we choose values that make analysis possible. choosing the edge weight too small means that very few nodes will be activated and choosing an edge weight too large will result in all nodes being activated no matter what algorithm is used.
　since the synthetic sets are more sparse we used edge weights of .1 and .1 for comparison. for each network type  each algorithm selected 1 nodes to activate. then  using the independent cascade model  nodes were activated randomly 1 times and the results averaged. the selection process was repeated twenty times for each algorithm and again the results were averaged resulting in a stable distribution.
　the results  shown in table 1 illustrate how conditions affect the different algorithms. first note that the greedy algorithm always is able to activate the highest number of nodes - that is what it is tuned to do. activating more nodes does not necessarily translate to covering more groups. with an edge weight of .1 greedy covers fewer groups than any of the algorithms except for random and  in the small world and random networks  comm.
　with the edge weight of .1  the best performing algorithms are comm and degpct. looking back at figures 1 and 1  we can see that with a scale-free network the majority of nodes are loners which indicates many communities that are separated from each other. in such a case an algorithm the focuses on nodes that have a high rawcomm would be more likely to spread to more communities. the small world and random networks have many more big fish which indicates communities more likely to overlap each other. this is good for all the algorithms but particularly degpct since it selects nodes with a high relative degree it is more likely to select nodes that are initially not in the same community.
edge wgt=.1edge wgt=.1algorithmnodes% cov.nodes% cov.scale-free networkrandom1.11degree1.11greedy1.11comm1.11ambass1.11degpct1.11small world networkrandom1.11degree1.11greedy1.11comm1.11ambass1.11degpct1.11random networkrandom1.11degree1.11greedy1.11comm1.11ambass1.11degpct1.11　when the edge weights are changed to .1  greedy performs better. it is the best algorithm in the small world and random networks but under scale-free it still is beat by comm and degree. this is because as the edge weights increase table 1: comparison of algorithms using synthetic data greedy does a better job of spreading to other nodes and just by sheer numbers is able to cover more groups.
　it is also reassuring to note that the results from the movie experiment correlate best with the scale-free synthetic results. since the degree distribution for the movie data set follows a power law it would be considered a scale free network. so it is not surprising that it would agree with the results from the scale-free synthetic data set.
　when considering which algorithm to choose for maximizing community coverage it is important to know the structure of the network  the edge weights and the nature of the communities. in general  if the network is scale-free  using ambass or comm will most likely be result in the largest number of communities covered. also  although the greedy algorithm always spreads to more nodes than any of the competitors it is a very slow algorithm. if time is limited  using ambass or even degree are reasonable alternatives.
1	classi cation
　as a second example of how the new metrics can be used in existing applications we again use the facebook data set but for a different michigan university. previous studies have shown that ordinary classification can be improved by using linked data. we will show that by using the rawcomm metric we can better employ the linked data to further improve the results.
　chakrabarti  et al. and lu and getoor  have shown that using linked data can be helpful in some circumstances. in the former paper  the authors were able to show that in some circumstances using the data from neighbors is not helpful but using the class from neighbors can be. a study by yang  et al.  shows that data sets can contain different types of regularities - with some regularities using the class of neighbors is helpful while in others it is not - confirming chakrabarti's finding.
　users of the facebook website can elect to make their personal data visible to other users in the network  usually within a college or university . we were able to collect about 1% of the personal data for the 1 students in our data set. the personal data includes gender  birthdate  relationship status  single  etc.   personal relationship interests  friendship  dating  etc.   political view  i.e. conservative   home town  favorite books  favorite movies  leisure interests  skiing  shopping  etc.  and area of academic concentration. to build a set suitable for classification we discretized several of the features. for the features with lists such as favorite books we created five binary  y/n  features corresponding to the five most popular responses.
　we selected  as the class  the person's political view because it is a feature many organizations would be interested in if missing. there are 1 categories including  not available . using the data from just the webpage itself using a decision tree classifier the error was 1%.
　we suggest using our concept of node role to improve the performance of the classifiers that use the class of neighboring nodes. we conjecture that some node types  ambassadors for instance  may not be as influenced by their neighbors while other types  big fish  may be more influenced by them . before modifying the data for the classifiers we tested our hypothesis that nodes of different roles are influenced by their neighbors differently.
　the results are summarized in table 1. the second column represents the average  across roles  of the percentage of neighbors that have the same class as a node. since many of the classifiers discussed in the studies above used a majority vote algorithm we also calculated the percentage of nodes that had the same class as the majority of it's neighboring nodes  shown in column 1.
table 1: node class versus neighbor's by role
role% same% same as majorityloners11bridges11big fish11ambassadors11　next we modified the data for the classifiers so that for each instance another feature was added that was the class of the majority of its neighbors. using the neighbors class significantly improved the performance of the classifier as can be seen in table 1. the numbers in column two represent the number of correctly classified instances out of 1 instances for the decision tree classifier.
　the first line is the baseline classifier with no neighbor class information. the second line shows the results when we used the neighbor's class. finally we list the results when we used the neighbors's class only for loner and big fish nodes. while the improvement is not dramatic it is not unexpected given that over 1% of the nodes were loners. so the data was not changed significantly from the second test to the third. we expect that improvements will be more pronounced in data sets where there are different distributions of roles.
1.	conclusions and future work
　we have demonstrated in this paper the usefulness of a metric that measures the approximate number of communitable 1: classifying using role
descriptiontreewithout neighbors' class1with neighbors' class1selected neighbors' class1ties that a node in a network belongs to and the degree of a node relative to it's immediate neighbors. also introduced is the concept of community-based role which can reveal hidden characteristics of a node.
　we have shown how these new metrics can be used to improve the performance of classifiers and to expand the usefulness of algorithms that maximize the spread of influence. it is possible that there exist many more applications. additionally the assigning of roles to nodes could be useful itself to analysts.
