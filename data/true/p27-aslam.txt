we present a model  based on the maximum entropy method  for analyzing various measures of retrieval performance such as average precision  r-precision  and precision-at-cutoffs. our methodology treats the value of such a measure as a constraint on the distribution of relevant documents in an unknown list  and the maximum entropy distribution can be determined subject to these constraints. for good measures of overall performance  such as average precision   the resulting maximum entropy distributions are highly correlated with actual distributions of relevant documents in lists as demonstrated through trec data; for poor measures of overall performance  the correlation is weaker. as such  the maximum entropy method can be used to quantify the overall quality of a retrieval measure. furthermore  for good measures of overall performance  such as average precision   we show that the corresponding maximum entropy distributions can be used to accurately infer precision-recall curves and the values of other measures of performance  and we demonstrate that the quality of these inferences far exceeds that predicted by simple retrieval measure correlation  as demonstrated through trec data.
categories and subject descriptors
h.1  information storage and retrieval : systems and software - performance evaluation
general terms
theory  measurement  experimentation
keywords
evaluation  maximum entropy  average precision

 
 we gratefully acknowledge the support provided by nsf grant ccf-1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
1. introduction
　the efficacy of retrieval systems is evaluated by a number of performance measures such as average precision  rprecision  and precisions at standard cutoffs. broadly speaking  these measures can be classified as either system-oriented measures of overall performance  e.g.  average precision and r-precision  or user-oriented measures of specific performance  e.g.  precision-at-cutoff 1   1  1  1 . different measures evaluate different aspects of retrieval performance  and much thought and analysis has been devoted to analyzing the quality of various different performance measures  1  1 
1 .
　we consider the problem of analyzing the quality of various measures of retrieval performance and propose a model based on the maximum entropy method for evaluating the quality of a performance measure. while measures such as average precision at relevant documents  r-precision  and 1pt average precision are known to be good measures of overall performance  other measures such as precisions at specific cutoffs are not. our goal in this work is to develop a model within which one can numerically assess the overall quality of a given measure based on the reduction in uncertainty of a system's performance one gains by learning the value of the measure. as such  our evaluation model is primarily concerned with assessing the relative merits of system-oriented measures  but it can be applied to other classes of measures as well.
　we begin with the premise that the quality of a list of documents retrieved in response to a given query is strictly a function of the sequence of relevant and non-relevant documents retrieved within that list  as well as r  the total number of relevant documents for the given query . most standard measures of retrieval performance satisfy this premise. our thesis is then that given the assessed value of a  good  overall measure of performance  one's uncertainty about the sequence of relevant and non-relevant documents in an unknown list should be greatly reduced. suppose  for example  one were told that a list of 1 documents retrieved in response to a query with 1 total relevant documents contained 1 relevant documents. what could one reasonably infer about the sequence of relevant and non-relevant documents in the unknown list  from this information alone  one could only reasonably conclude that the likelihood of seeing a relevant document at any rank level is uniformly 1. now suppose that one were additionally told that the average precision of the list was 1  the maximum possible in this circumstance is 1 . now one could reasonably conclude that the likelihood of seeing relevant documents at low numerical ranks is much greater than the likelihood of seeing relevant documents at high numerical ranks. one's uncertainty about the sequence of relevant and non-relevant documents in the unknown list is greatly reduced as a consequence of the strong constraint that such an average precision places on lists in this situation. thus  average precision is highly informative. on the other hand  suppose that one were instead told that the precision of the documents in the rank range  1  was 1. one's uncertainty about the sequence of relevant and non-relevant documents in the unknown list is not appreciably reduced as a consequence of the relatively weak constraint that such a measurement places on lists. thus  precision in the range  1  is not a highly informative measure. in what follows  we develop a model within which one can quantify how informative a measure is.
　we consider two questions:  1  what can reasonably be inferred about an unknown list given the value of a measurement taken over this list   1  how accurately do these inferences reflect reality  we argue that the former question is properly answered by considering the maximum entropy distributions subject to the measured value as a constraint  and we demonstrate that such maximum entropy models corresponding to good overall measures of performance such as average precision yield accurate inferences about underlying lists seen in practice  as demonstrated through trec data .
　more specifically  we develop a framework based on the maximum entropy method which allows one to infer the most  reasonable  model for the sequence of relevant and non-relevant documents in a list given a measured constraint. from this model  we show how one can infer the most  reasonable  model for the unknown list's entire precision-recall curve. we demonstrate through the use of trec data that for  good  overall measures of performance  such as average precision   these inferred precision-recall curves are accurate approximations of actual precision-recall curves; however  for  poor  overall measures of performance  these inferred precision-recall curves do not accurately approximate actual precision-recall curves. thus  maximum entropy modeling can be used to quantify the quality of a measure of overall performance.
　we further demonstrate through the use of trec data that the maximum entropy models corresponding to  good  measures of overall performance can be used to make accurate predictions of other measurements. while it is well known that  good  overall measures such as average precision are well correlated with other measures of performance  and thus average precision could be used to reasonably predict other measures of performance  we demonstrate that the maximum entropy models corresponding to average precision yield inferences of other measures even more highly correlated with their actual values  thus validating both average precision and maximum entropy modeling.
　in the sections that follow  we first describe the maximum entropy method and discuss how maximum entropy modeling can be used to analyze measures of retrieval performance. we then describe the results of applying our methodology using trec data  and we conclude with a summary and future work.
1. the maximum entropy method
　the concept of entropy as a measure of information was first introduced by shannon   and the principle of maximum entropy was introduced by jaynes  1  1  1 . since its introduction  the maximum entropy method has been applied in many areas of science and technology  including natural language processing   ambiguity resolution   text classification   machine learning  1  1   and information retrieval  1  1   to name but a few examples. in what follows  we introduce the maximum entropy method through a classic example  and we then describe how the maximum entropy method can be used to evaluate measures of retrieval performance.
　suppose you are given an unknown and possibly biased six-sided die and were asked the probability of obtaining any particular die face in a given roll. what would your answer be  this problem is under-constrained and the most seemingly  reasonable  answer is a uniform distribution over all faces. suppose now you are also given the information that the average die roll is 1. the most seemingly  reasonable  answer is still a uniform distribution. what if you are told that the average die roll is 1  there are many distributions over the faces such that the average die roll is 1; how can you find the most seemingly  reasonable  distribution  finally  what would your answer be if you were told that the average die roll is 1  clearly  the belief in getting a 1 increases as the expected value of the die rolls increases. but there are many distributions satisfying this constraint; which distribution would you choose 
the  maximum entropy method   mem  dictates the
most  reasonable  distribution satisfying the given constraints. the  principle of maximal ignorance  forms the intuition behind the mem; it states that one should choose the distribution which is least predictable  most random  subject to the given constraints. jaynes and others have derived numerous entropy concentration theorems which show that the vast majority of all empirical frequency distributions  e.g.  those corresponding to sequences of die rolls  satisfying the given constraints have associated empirical probabilities and entropies very close to those probabilities satisfying the constraints whose associated entropy is maximal .
　thus  the mem dictates the most random distribution satisfying the given constraints  using the entropy of the probability distribution as a measure of randomness. the entropy of a probability distribution p~ = {p1 p1 ... pn} is a measure of the uncertainty  randomness  inherent in the distribution and is defined as follows
.
thus  maximum entropy distributions are probability distributions making no additional assumptions apart from the given constraints.
　in addition to its mathematical justification  the mem tends to produce solutions one often sees in nature. for example  it is known that given the temperature of a gas  the actual distribution of velocities in the gas is the maximum entropy distribution under the temperature constraint.

figure 1: maximum entropy die distributions with mean die rolls of 1  1  and 1  respectively.　we can apply the mem to our die problem as follows. let the probability distribution over the die faces be p~ = {p1 ... p1}. mathematically  finding the maximum entropy distribution over die faces such that the expected die roll is d corresponds to the following optimization problem:
maximize: h p~  subject to:
1
1. p pi = 1
i=1
1. p i ， pi = d
i=1
the first constraint ensures that the solution forms a distribution over the die faces  and the second constraint ensures that this distribution has the appropriate expectation. this is a constrained optimization problem which can be solved using the method of lagrange multipliers. figure 1 shows three different maximum entropy distributions over the die faces such that the expected die roll is 1  1  and 1  respectively.
1 application of the maximum entropy method to analyzing retrieval measures
　suppose that you were given a list of length n corresponding to the output of a retrieval system for a given query  and suppose that you were asked to predict the probability of seeing any one of the 1n possible patterns of relevant documents in that list. in the absence of any information about the query  any performance information for the system  or any a priori modeling of the behavior of retrieval systems  the most  reasonable  answer you could give would be that all lists of length n are equally likely. suppose now that you are also given the information that the expected number of relevant documents over all lists of length n is rret. your  reasonable  answer might then be a uniform distribution over all  different possible lists with rret relevant documents. but what if apart from the constraint on the number of relevant documents retrieved  you were also given the constraint that the expected value of average precision is ap  if the average precision value is high  then of all the `rnret＞ lists with rret relevant documents  the lists in which the relevant documents are retrieved at low numerical ranks should have higher probabilities. but how can you determine the most  reasonable  such distribution  the maximum entropy method essentially dictates the most reasonable distribution as a solution to the following constrained optimization problem.
　let p r1 ... rn  be a probability distribution over the relevances associated with document lists of length n  let rel r1 ... rn  be the number of relevant documents in a list  and let ap r1 ... rn  be the average precision of a list. then the maximum entropy method can be mathematically formulated as follows:
maximize: h p~  subject to:
1. p p r1 ... rn  = 1
r1 ... rn
1. p ap r1 ... rn  ， p r1 ... rn  = ap
r1 ... rn
1. p rel r1 ... rn  ， p r1 ... rn  = rret
r1 ... rn
note that the solution to this optimization problem is a distribution over possible lists  where this distribution effectively gives one's a posteriori belief in any list given the measured constraint.
　the previous problem can be formulated in a slightly different manner yielding another interpretation of the problem and a mathematical solution. suppose that you were given a list of length n corresponding to output of a retrieval system for a given a query  and suppose that you were asked to predict the probability of seeing a relevant document at some rank. since there are no constraints  all possible lists of length n are equally likely  and hence the probability of seeing a relevant document at any rank is 1. suppose now that you are also given the information that the expected number of relevant documents over all lists of length n is rret. the most natural answer would be a rret/n uniform probability for each rank. finally  suppose that you are given the additional constraint that the expected average precision is ap. under the assumption that our distribution over lists is a product distribution  this is effectively a fairly standard independence assumption   we may solve this problem as follows. let p r1 ... rn  = p r1  ， p r1 ，，，p rn 
where p ri  is the probability that the document at rank i is relevant. we can then solve the problem of calculating the probability of seeing a relevant document at any rank using the mem. for notational convenience  we will refer to this product distribution as the probability-at-rank distribution and the probability of seeing a relevant document at rank i  p ri   as pi.
　standard results from information theory  dictate that if p r1 ... rn  is a product distribution  then
n
h p r1 ... rn   = xh pi 
i=1
where h pi  is the binary entropy
h pi  =  pi lgpi    1   pi lg 1   pi .

setup for average precision.	setup for r-precision.	setup for precision-at-cutoff.furthermore  it can be shown that given a product distribution p r1 ... rn  over the relevances associated with document lists of length n  the expected value of average precision is
	 .	 1 
 the derivation of this formula is omitted due to space constraints.  furthermore  since pi is the probability of seeing a relevant document at rank i  the expected number of relevant documents retrieved until rank.
　now  if one were given some list of length n  one were told that the expected number of relevant documents is rret  one were further informed that the expected average precision is ap  and one were asked the probability of seeing a relevant document at any rank under the independence assumption stated  one could apply the mem as shown in figure 1. note that one now solves for the maximum entropy product distribution over lists  which is equivalent to a maximum entropy probability-at-rank distribution. applying the same ideas to r-precision and precision-at-cutoff k  one obtains analogous formulations as shown in figures 1 and 1  respectively.
　all of these formulations are constrained optimization problems  and the method of lagrange multipliers can be used to find an analytical solution  in principle. when analytical solutions cannot be determined  numerical optimization methods can be employed. the maximum entropy distributions for r-precision and precision-at-cutoff k can be obtained analytically using the method of lagrange multipliers. however  numerical optimization methods are required to determine the maximum entropy distribution for average precision. in figure 1  examples of maximum entropy probability-at-rank curves corresponding to the measures average precision  r-precision  and precision-at-cutoff 1 for a run in trec1 can be seen. note that the probabilityat-rank curves are step functions for the precision-at-cutoff and r-precision constraints; this is as expected since  for example  given a precision-at-cutoff 1 of 1  one can only reasonably conclude a uniform probability of 1 for seeing a relevant document at any of the first 1 ranks. note  however  that the probability-at-rank curve corresponding to average precision is smooth and strictly decreasing.
　using the maximum entropy probability-at-rank distribution of a list  we can infer the maximum entropy precisionrecall curve for the list. given a probability-at-rank distribution p~  the number of relevant documents retrieved until rank i is rel i  = pij=1 pj. therefore  the precision and recall at rank i are pc i  = rel i /i and rec i  = rel i /r. hence  using the maximum entropy probability-
trec1 system fub1a query 1 ap = 1

figure 1: probability-at-rank distributions.
at-rank distribution for each measure  we can generate the maximum entropy precision-recall curve of the list. if a measure provides a great deal of information about the underlying list  then the maximum entropy precision-recall curve should approximate the precision-recall curve of the actual list. however  if a measure is not particularly informative  then the maximum entropy precision-recall curve need not approximate the actual precision-recall curve. therefore  noting how closely the maximum entropy precisionrecall curve corresponding to a measure approximates the precision-recall curve of the actual list  we can calculate how much information a measure contains about the actual list  and hence how  informative  a measure is. thus  we have a methodology for evaluating the evaluation measures themselves.
　using the maximum entropy precision-recall curve of a measure  we can also predict the values of other measures. for example  using the maximum entropy precision-recall curve corresponding to average precision  we can predict the precision-at-cutoff 1. for highly informative measures  these predictions should be very close to reality. hence  we have a second way of evaluating evaluation measures.
1. experimental results
　we tested the performance of the evaluation measures average precision  r-precision  and precision-at-cutoffs 1  1  1  1  1  1  1  1 and 1 using data from trecs 1  1  1  1  1 and 1. for any trec and any query  we chose those systems whose number of relevant documents retrieved was at least 1 in order to have a sufficient number of points on the precision-recall curve. we then calculated the maximum entropy precision-recall curve subject to the given measured constraint  as described above. the maximum entropy precision-recall curve corresponding to an average precision constraint cannot be determined analytically; therefore  we used numerical optimization1 to find the maximum entropy distribution corresponding to average precision.
　we shall refer to the execution of a retrieval system on a particular query as a run. figure 1 shows examples of maximum entropy precision-recall curves corresponding to average precision  r-precision  and precision-at-cutoff 1 for three different runs  together with the actual precision-recall curves. we focused on these three measures since they are perhaps the most commonly cited measures in ir. we also provide results for precision-at-cutoff 1 in later plots and detailed results for all measures in a later table. as can be seen in figure 1  using average precision as a constraint  one can generate the actual precision-recall curve of a run with relatively high accuracy.
　in order to quantify how good an evaluation measure is in generating the precision-recall curve of an actual list  we consider two different error measures: the root mean squared error  rms  and the mean absolute error  mae . let {π1 π1 ... πrret} be the precisions at the recall levels {1/r 1/r ... rret/r} where rret is the number of relevant documents retrieved by a system and r is the number of documents relevant to the query  and let {m1 m1 ... mrret} be the estimated precisions at the corresponding recall levels for a maximum entropy distribution corresponding to a measure. then the mae and rms errors are calculated as follows.
	rms	
	mae	
the points after recall rret/r on the precision-recall curve are not considered in the evaluation of the mae and rms errors since  by trec convention  the precisions at these recall levels are assumed to be 1.
　in order to evaluate how good a measure is at inferring actual precision-recall curves  we calculated the mae and rms errors of the maximum entropy precision-recall curves corresponding to the measures in question  averaged over all runs for each trec. figure 1 shows how the mae and rms errors for average precision  r-precision  precision-at-cutoff 1  and precision-at-cutoff 1 compare with each other for each trec. the mae and rms errors follow the same pattern over all trecs. both errors are consistently and significantly lower for average precision than for the other measures in question  while the errors for r-precision are consistently lower than for precision-at-cutoffs 1 and 1.
　table 1 shows the actual values of the rms errors for all measures over all trecs. in our experiments  mae and rms errors follow a very similar pattern  and we therefore omit mae results due to space considerations. from this table  it can be seen that average precision has consistently lower rms errors when compared to the other measures. the penultimate column of the table shows the average rms errors per measure averaged over all trecs. on average  r-precision has the second lowest rms error after average precision  and precision-at-cutoff 1 is the third best measure in terms of rms error. the last column of the table shows the percent increase in the average rms error of a measure when compared to the rms error of average precision. as can be seen  the average rms errors for the other measures are substantially greater than the average rms error for average precision.
　we now consider a second method for evaluating how informative a measure is. a highly informative measure should properly reduce one's uncertainty about the distribution of relevant and non-relevant documents in a list; thus  in our maximum entropy formulation  the probability-at-rank distribution should closely correspond to the pattern of relevant and non-relevant documents present in the list. one should then be able to accurately predict the values of other measures from this probability-at-rank distribution.
　given a probability-at-rank distribution p1 p1 ... pn  we can predict average precision  r-precision and precision-atcutoff k values as follows:

  pc
the plots in the top row of figures 1 and 1 show how average precision is actually correlated with r-precision  precisionat-cutoff 1  and precision-at-cutoff 1 for trecs 1 and 1  respectively. each point in the plot corresponds to a system and the values of the measures are averaged over all queries. using these plots as a baseline for comparison  the plots in the bottom row of the figures show the correlation between the actual measures and the measures predicted using the average precision maximum entropy probabilityat-rank distribution. consider predicting precision-at-cutoff 1 values using the average precision maximum entropy distributions in trec 1. without applying the maximum entropy method  figure 1 shows that the two measures are correlated with a kendall's τ value of 1. however  the precision-at-cutoff 1 values inferred from the average precision maximum entropy distribution have a kendall's τ value of 1 when compared to actual precisions-at-cutoff 1. hence  the predicted precision-at-cutoff 1 and actual precision-at-cutoff 1 values are much more correlated than the actual average precision and actual precision-at-cutoff 1 values. using a similar approach for predicting r-precision and precision-at-cutoff 1  it can be seen in figures 1 and 1 that the measured values predicted by using average precision maximum entropy distributions are highly correlated with actual measured values.
　we conducted similar experiments using the maximum entropy distributions corresponding to other measures  but since these measures are less informative  we obtained much smaller increases  and sometimes even decreases  in inferred correlations.  these results are omitted due to space considerations.  table 1 summarizes the correlation improvements possible using the maximum entropy distribution corresponding to average precision. the row labeled τact gives the actual kendall's τ correlation between average precision and the measure in the corresponding column. the row labeled τinf gives the kendall's τ correlation between the
　

figure 1: inferred precision-recall curves and actual precision-recall curve for three runs in trec1.

figure 1: mae and rms errors for inferred precision-recall curves over all trecs.
trec1trec1trec1trec1trec1trec1average%incap1111111 rp11111111pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1pc-1.1.1.1.1.1.1.1.1table 1: rms error values for each trec.
trec1trec1trec1rppc-1pc-1rppc-1pc-1rppc-1pc-1τact111111111τinf111111111%inc111111111trec1trec1trec1rppc-1pc-1rppc-1pc-1rppc-1pc-1τact111111111τinf111111111%inc111111111table 1: kendall's τ correlations and percent improvements for all trecs.
	trec 1 actual rp vs actual ap	trec 1 actual pc 1 vs actual ap	trec 1 actual pc 1 vs actual ap
	1	1	
	1	1
1.1.1.1.1.1	1	1	1	1	1	1	1	1	1	1 actual rp	actual pc 1	actual pc 1
figure 1: correlation improvements  trec1.
measure inferred from the maximum entropy distribution uation . however  in different contexts  different measures corresponding to average precision and the measure in the may be more valuable and useful  such as precision-at-cutoff corresponding column. the row labeled %inc gives the per- 1 in web search  a user-oriented evaluation . r-precision cent increase in correlation due to maximum entropy mod- and average precision are system-oriented measures  whereas eling. as can be seen  maximum entropy modeling yields precision-at-cutoff k is typically a user-oriented measure. great improvements in the predictions of precision-at-cutoff another important conclusion of our work is that one can acvalues. the improvements in predicting r-precision are no- curately infer user-oriented measures from system-oriented ticeably smaller  though this is largely due to the fact that measures  but the opposite is not true.
average precision and r-precision are quite correlated to be- apart from evaluating the information captured by a singin with. gle measure  we could use the mem to evaluate the information contained in combinations of measures. how much does knowing the value of precision-at-cutoff 1 increase one's
1.	conclusions and future work	knowledge of a system's performance beyond simply know-
　we have described a methodology for analyzing measures ing the system's average precision  which is more inforof retrieval performance based on the maximum entropy mative: knowing r-precision and precision-at-cutoff 1  or method  and we have demonstrated that the maximum en- knowing average precision and precision-at-cutoff 1  such tropy models corresponding to  good  measures of overall questions can be answered  in principle  using the mem. performance such as average precision accurately reflect un- adding the values of one or more measures simply adds one derlying retrieval performance  as measured by precision- or more constraints to the maximum entropy model  and recall curves  and can be used to accurately predict the val- one can then assess the informativeness of the combination. ues of other measures of performance  well beyond the levels note that trec reports many different measures. using dictated by simple correlations. the mem  one might reasonably be able to conclude which the maximum entropy method can be used to analyze are the most informative combinations of measures.
other measures of retrieval performance  and we are presentlyconducting such studies. more interestingly  the maximum entropy method could perhaps be used to help develop and gain insight into potential new measures of retrieval performance. finally  the predictive quality of maximum entropy models corresponding to average precision suggest that if one were to estimate some measure of performance using an incomplete judgment set  that measure should be average precision-from the maximum entropy model corresponding to that measure alone  one could accurately infer other measures of performance.
