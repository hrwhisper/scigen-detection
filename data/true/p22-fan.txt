in this paper  we have proposed a novel framework to enable hierarchical image classification via statistical learning. by integrating the concept hierarchy for semantic image concept organization  a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination. thus  learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts. to effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy  we have proposed a novel adaptive em algorithm to achieve more effective model selection and parameter estimation. in addition  a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training. our experimental results in a specific image domain of outdoor photos are very attractive.
categories and subject descriptors
h.1  database management : database applications image databases.
general terms
algorithms  measurement  experimentation
keywords: image classification  hierarchical mixture model  adaptive em algorithm.
1. introduction
﹛as high-resolution digital cameras become more affordable and widespread  high-quality digital images have exploded on the internet. with the exponential growth on high-quality digital images  the need of mining image database on semantics is becoming increasely important to enable semantic image retrieval via keywords  1 . semantic image classification is a promising approach for mining large-scale image database on semantics and has attracted the interest of researchers from a variety of fields  1  1 .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
 kdd '1  august 1  1  chicago  il  usa
copyright 1 acm 1-1-x/1...$1.
lyon1.fr
﹛to enable image mining on semantics  it is very important to achieve a middle-level understanding of image semantics  i.e.  bridging the semantic gap between image semantics for human interpretation and low-level visual features extracted by computers for image content representation  . thus  the following inter-related problems should be addressed jointly:  1  what are the suitable image patterns that are able to achieve a middle-level understanding of the semantics of image contents   1  what are the underlying concept models for accurately interpreting the semantic image concepts of particular interest   1  what is the basic vocabulary of semantic image concepts of particular interest in a specific image domain   1  given the basic vocabulary of semantic image concepts  how can we learn the underlying concept models more accurately 
﹛to address the first issue  two approaches are widely used for image content representation and feature extraction:  a  image-based approaches that use the whole images for feature extraction.  b  region-based approaches that take the homogeneous image regions as the underlying image patterns for feature extraction. one common weakness of the region-based approaches is that the homogeneous image regions have little correspondence to the image semantics  thus they are not effective to support semantic image classification . without performing image segmentation  the image-based approaches may not work very well for the images that contain individual objects  because only the global visual features are used for image content representation . to enable more accurate interpretation of various semantic image concepts  we have recently developed a novel technique for semantic image classification and automatic annotation by using salient objects .
﹛to adress the second issue  gaussian mixture model  gmm  has been widely used for semantic image concept interpretation with a pre-defined model structure  1 . however  different semantic image concepts may relate to different numbers and types of various image blobs  and thus automatic techniques for model selection are strongly expected.
﹛unfortunately  there is no existing works in the literature to effectively address the third and fourth issues. most existing techniques for semantic image classification ignore the hierarchical relationships between the semantic image concepts at different semantic levels . they independently learn a set of flat classifiers for various semantic image concepts of particular interest. however  the semantic image concepts at the high level of the concept hierarchy may have larger hypothesis variance  and directly learning the flat classifiers for these high-level semantic image concepts may result in low prediction accuracy. some pioneer works have shown that the accuracy of text classifiers can be significantly improved by taking advantage of concept hierarchy such as wordnet and training the classifiers for multiple concepts hierarchically  1 . this observation also encourages us to integrate the concept hierarchy to enable more effective image classification.
﹛another major difficulty for most existing image classification techniques is that a large number of labeled images are required to learn the concept models accurately. unfortunately  labeling a large number of training images is very expensive and time-consuming. given this costly labeling problem  it is very attractive to design the semi-supervised classifier training techniques that can take advantage of unlabeled samples  1 . however  there are two basic assumptions for most existing semi-supervised classifier training techniques:  a  each unlabeled image originates from one of the known image context classes  i.e.  existing mixture components that have already been used for semantic image concept interpretation ;  b  all these relevant image context classes can be effectively learned from the available labeled images. when only a limited number of labeled images are available for classifier training  these two basic assumptions are not satisfied because of concept uncertainty  i.e.  presence of new concept  outliers  and unknown image context classes . considering that many image context classes and new concept have not even occurred in a limited number of available labeled images  using the outlying unlabeled images will corrupt the density estimation and lead to worse performance rather than improvement when the model structure is incorrect .
﹛based on these observations  we have proposed a novel framework to enable hierarchical image classifier training. this paper is organized as follows: section 1 presents a novel framework for hierarchical organization and modeling of semantic image concepts; section 1 proposes a novel algorithm for hierarchical image classifier training; section 1 introduces our technique for semantic image classification via partial matching; section 1 shows our work on algorithm evaluation; we conclude the paper in section 1.
1. hierarchical image concept organization and modeling
﹛the success of most existing techniques for semantic image classification is often limited and largely depends on the discrimination power of the low-level visual features for image content representation  i.e. attributes for image description  . on the other hand  the discrimination power of the low-level visual features also depends on the capability of the underlying image patterns to capture sufficient semantics of image contents.
﹛because the image blobs have the capability to characterize the relevant dominant image compounds  they are able to capture the middle-level image semantics and thus extracting the visual features from the image blobs can enhance the discrimination power of visual features and may result in more effective image classification .
﹛to enable more effective image classification  the concept hierarchy is used for semantic image concept organization and hierarchical classifier combination as shown in fig. 1 and fig. 1. the concept hierarchy defines the basic vocabulary of the semantic image concepts and their contextual

figure 1: the concept hierarchy for organizing outdoor photos in our experiments  where the first level represents the underlying atomic image concepts.
and logical relationships. the lower the level of a semantic image concept node  the narrower is its coverage of the subjects  and the semantic image concepts at a lower level of the concept hierarchy characterize more specific aspects of image semantics. thus  the hypothesis variances for the semantic image concepts at the lower level of the concept hierarchy may be smaller and can be interpreted effectively by using the low-level visual features. as a result  the concept nodes at the first level of the concept hierarchy are named as atomic image concepts as shown in fig. 1. one example of the concept hierarchy  that is used in our experiments for a specific image domain of outdoor photos  is given in fig.
1.
﹛as shown in fig 1  one certain atomic image concept cj at the first level of the concept hierarchy can be interpreted accurately by using a finite mixture model  fmm  to approximate the class distribution of the relevant image blobs:
百j
	p x cj 成cj  =	p x|cj sl 牟sl 肋sl	 1 
l=1
where  is the parameter set for model structure  weights and model parameters  p x|cj  sl  牟sl  is the lth mixture component to characterize the class distribution for the lth type of image blobs  百j is the model structure  i.e.  the optimal number of mixture components   肋cj = {肋s1  ﹞﹞﹞  肋s百j } is the weight set for 百j mixture components  肋sl is the relative weight for the lth mixture component to characterize the relative importance of the lth type of image blobs for accurately interpreting the given atomic image concept cj  牟cj = {牟s1  ﹞﹞﹞  牟s百j } is the set of model parameters  牟sl is the model parameters for the lth mixture component  x =  x1 ﹞﹞﹞  xn  is the n-dimensional visual features to characterize the visual properties for the relevant image blobs.
﹛at the second level of the concept hierarchy  the class distribution for a given semantic image concept ci can be interpreted by using a hierarchical mixture model to approximate the underlying class distributions for the relevant sibling atomic image concepts:
	百i	百j
p x ci 成ci  =	肋cj	p x|cj sl 牟sl 肋sl	 1  j=1	l=1
where  is the parameter set for model structure  weights and model parameters  and 肋ci is the set of weights for the gate networks that are used to define the relative importance of the 百i sibling atomic image concepts for accurately interpreting their parent node
ci .
﹛through a similar hierarchical approach  the class distribution for a given semantic image concept ck at the nth level of the concept hierarchy can be interpreted by using a hierarchical mixture model to approximate the class distributions of the  n   1 th sibling semantic image concepts:
	百n	百i	百j
p x ck 成ck  =	肋cm ﹞﹞﹞	肋cj	p x|cj sl 牟sl 肋sl
	m=1	j=1	l=1
﹛﹛﹛﹛﹛ 1  where  is the parameter set for model structure  weights  and model parameters  and 肋cm is the set of weights for the gate networks that are used to define the relative importance of the sibling semantic image concepts at the  n   1 th level of the concept hierarchy .
﹛our proposed framework for hierarchical classifier training has the following advantages:  a  by using the hierarchical mixture model for classifier combination  it is able to enable more effective learning of the classifiers for the high-level semantic image concepts. thus  the concept models for the high-level semantic image concepts can be adapted by the observations of the relevant base-level atomic image concepts. in addition  learning the high-level semantic image concepts hierarchically is able to reduce the size of covariance matrices being inverted.  b  it is able to enable a natural approach for achieving discriminative learning of finite mixture models by jointly learning the concept models for the sibling semantic image concepts under the same parent node  and thus the positive images for one certain semantic image concept can be treated as the negative images for its sibling semantic image concepts under the same parent node. by using the negative images to maximize the margins among the sibling semantic image concepts  our proposed hierarchical classifier training technique can achieve higher prediction accuracy.  c  by using the concept hierarchy for semantic image concept organization  our proposed framework is able to facilitate more effective image database indexing  searching and navigation. to support concept-oriented image database indexing  the finite mixture models for semantic image concept interpretation can be used to enable density-based database node representation . after all the images are classified into the semantic image concepts at different semantic levels  the underlying concept hierarchy can further be incorporated to construct the hierarchical image database indexing structure  where the concept nodes become the relevant database nodes at different semantic levels  upon which the root node of the image database can be constructed automatically.
1. hierarchical image classifier training
﹛to accurately learn the hierarchical mixture model for semantic image concept interpretation  the semantic labels for a set of training images are manually labeled for each atomic image concept. we use the one-against-all rule to organize the labeled images  cj = {xl cj sl |l = 1 ﹞﹞﹞  nl} into: positive images and negative images for a given atomic image concept cj  xl is the set of attributes  i.e. visual features  that are used to describe the training image sl. the unla-

beled images  cj = {xn sn|n = 1 ﹞﹞﹞  nu} can be used to improve the density estimation by reducing the variance of mixture density and discovering the unknown image context classes. for the given atomic image concept cj  we then de-

fine the mixture training image set as   =  cj  cj  1 . the labeled images for the sibling atomic image concepts are further combined as the joint training images for their parent node at the concept hierarchy.
﹛the visual features for image content representation include 1-dimensional coverage ratio  i.e.  density ratio  for a coarse shape representation  1-dimensional locations of image blobs  i.e.  1-dimensions for blob center and 1-dimensions to indicate the rectangular box for a coarse shape representation of image blob   1-dimensional luv dominant colors and color variances  1-dimensional tamura texture  and 1dimensional wavelet texture features.
﹛the atomic image concepts at the first level of the concept hierarchy have low hypothesis variances and thus they can be accurately interpreted by using the relevant image blobs. based on this understanding  we propose a bottom-up approach for hierarchical image classifier training.
1 base-level image classifier training
﹛to learn the model-based classifier for the given atomic image concept cj  maximum likelihood criterion can be used to determine the underlying model parameters. to avoid the overfitting problem   a penalty term is added to determine the underlying optimal model structure. the optimal parameters  i.e.  model structure  weights  and model parameters  成 cj =  百 j   肋cj  牟 cj  for the given atomic image concept cj are then determined by:
	成 cj =	arg max	l cj 成cj 	 1 
成cj
where l cj  成cj  =   xi﹋ cj logp xi cj 成cj  + logp 成cj  is the objective function    xi﹋ cj logp xi  cj  成cj  is the likelihood function  and log
is the minimum description
length  mdl  term to penalize the complex models with a large number of mixture components   n is the total number of training samples  and n is the dimensions of visual features x.
﹛the estimation of maximum likelihood described in eq.  1  can be achieved by using the em algorithm with a predefined model structure 百j  1 . however  pre-defining the model structure 百j is not acceptable for semantic image classification. thus  there is an urgent need to develop new techniques that are able to select the optimal model structure automatically.
﹛to automatically select the optimal model structure and estimate the accurate model parameters  we have proposed

inputs: training images  cj  百j = 百max
outputs: 成 cj
initialization is done by k-mean clustering; for each 百j do
jm i k 牟sik  = js cj 牟sik  +  js cj 牟si 牟sk 

for each image {xl sl} ﹋  cj do
e-step: m-step:
end for end for
an adaptive em algorithm for classifier training. to incorporate the negative images for discriminative learning of finite mixture models  we have taken advantage of the concept hierarchy for hierarchical classifier training  where the finite mixture models for interpreting the sibling atomic image concepts under the same parent node are learned jointly. thus  the positive images for the given atomic image concept cj can be used as the negative images for the sibling atomic image concepts under the same parent node. by learning these sibling atomic image concepts jointly  it is able to achieve discriminative learning of finite mixture models by incorporating the negative images to maximize the margins among different concept models.
﹛to achieve more accurate model selection and parameter estimation  our adaptive em algorithm performs automatic merging  splitting  and elimination to re-organize the distribution of mixture components and modify the optimal number of mixture components according to the class distribution of the available training images .
﹛to exploit the most suitable image context classes for accurately interpreting the atomic image concept cj  our adaptive em algorithm starts from a large value of 百j and takes the major steps as shwon in algorithm 1. with the given 百j  k-mean clustering technique is used to select the reasonable and robust initial values for the model parameters  i.e.  mean and covariance for each cluster .
﹛to determine the underlying optimal model structure  we use two criteria to perform automatic splitting  merging  and elimination of mixture components:  a  fitness between one specific mixture component and the distribution of the relevant training images;  b  overlapping between the mixture components from the same atomic image concept or the mixture components from the sibling atomic image concepts under the same parent node.
﹛our adaptive em algorithm uses symmetric jensen-shannon  js  divergence  i.e.  intra-concept js divergence  js cj  牟sl  牟sk  to measure the divergence between two mixture components p x|cj  sl  牟sl  and p x|cj  sk  牟sk  from the same concept model cj.

 where h p ﹞   =   p ﹞ logp ﹞  is the well-known shannon entropy  羽1 and 羽1 are the weights. in our experiments  we set.
﹛if the intra-concept js divergence js cj  牟sl  牟sk  is small  these two mixture components are strongly overlapped and may overpopulate the relevant training images; thus they are merged into a single mixture component p x|cj  slk  牟slk . in addition  the local js divergence js cj  牟slk  is used to measure the divergence between the merged mixture component p x|cj  slk  牟slk  and the local density of the training images p x  cj  牟 . the local density p x  cj  牟  is modified as the empirical distribution weighted by the posterior probability . our adaptive em algorithm tests  pairs of mixture components that could be merged and the pair with the minimum value of the local js divergence is selected as the best candidate for merging.
﹛two types of mixture components may be split:  a  the elongated mixture components which underpopulate the relevant training images  i.e.  characterized by the local js divergence ;  b  the tailed mixture components which overlap with the mixture components from the concept models for the sibling atomic image concepts  i.e.  characterized by the inter-concept js divergence . to select the mixture component for splitting  two criteria are combined:  1  the local js divergence js cj  si  牟si  to characterize the divergence between the ith mixture component p x|cj  si  牟si  and the local density of the training images p x cj  牟 ;  1  the inter-concept js divergence js cj  ch  牟si  牟sm  to characterize the overlapping between the mixture components p x|cj  si  牟si  and p x|ch  sm  牟sm  from the sibling atomic image concepts cj and ch.
﹛if one specific mixture component is only supported by few training images  it may be removed from the concept model for the cj. to determine the unrepresentative mixture component for elimination  our adaptive em algorithm uses the local js divergence js cj  牟si  to characterize the representation of the mixture component p x|cj  si  牟si  for the relevant training images. the mixture component with the maximum value of the local js divergence is selected as the candidate for elimination.
﹛to jointly optimize these three operations of merging  splitting and elimination  their probabilities are defined as:

 1 
where   is a normalized factor and it is determined by:
百j	百j	百j	百j	百h
	je i 牟si +	jm i k 牟sik +	js i m 牟si  = 1
i=1	i=1 k=i+1	i=1 m=i+1
                                                    1  the acceptance probability to prevent poor operation of merging  splitting or elimination is defined by:

where l cj 成1  and l cj 成1  are the objective functions for the models 成1 and 成1  i.e.  before and after performing the merging  splitting or elimination operation  as described in eq.  1   而 is a constant that is determined experimentally. in our current experiments  而 is set as 而 = 1.
﹛by optimizing these three operations jointly  our adaptive em algorithm is able to automatically select the optimal model structure to capture the essential structure of the image context classes. in addition  our adaptive em algorithm is able to escape the local extrema by re-organizing the distribution of mixture components and modifying the optimal number of mixture components according to the class distributions of the training images. by integrating the negative samples to maximize the margin among the concept models for the sibling atomic image concepts  our adaptive em algorithm is also able to enable discriminative learning of finite mixture models and result in high prediction accuracy.
1 training with unlabeled images
﹛when only a limited number of labeled images are available for classifier training  it is difficult to select the optimal model structure and estimate the accurate model parameters. in addition  incorporating the outlying unlabeled images for classifier training may lead to worse performance rather than improvement. thus  it is very important to develop new techniques able to eliminate the misleading effects of the outlying unlabeled images.
﹛the weak classifier for the given atomic image concept cj is first learned from a limited number of available training images  and the bayesian framework is used to achieve  soft  classification of the unlabeled images. for the given atomic image concept cj  the confidence score for an unlabeled image is defined as:
	肉 xl cj t  =	肉汐 xl cj t 肉汕 xl cj t 	 1 
where 肉汐 xl cj t  is the posterior probability for the unlabeled image {xl sl} with the given atomic image concept cj  肉汕 xl cj t  =  logp xl cj 成cj  is the log-likelihood value of the unlabeled image {xl sl} with the given atomic image concept ch. for the unlabeled image {xl sl}  its confidence score 肉 xl cj t  can be used as the criterion to indicate its possibility to be taken as an outlier of the cj.
﹛for the given atomic image concept cj  the unlabeled images are first categorized into two classes according to their confidence scores:  a  certain unlabeled images with high confidence scores may originate from the known image context classes  i.e.  mixture components  that have already been used for interpreting the given atomic image concept cj;  b  uncertain unlabeled images with low confidence scores may orginate from outliers and unknown image context classes that have not occurred in a limited number of available labeled images.
﹛the certain unlabeled images are first incorporated to improve the estimation of the mixture density  i.e. regular updating of model parameters without changing the model structure  incrementally by reducing the variance of the mixture density. by integrating the certain unlabeled images to update the statistical model  the confidence scores for some uncertain unlabeled images may be changed over time if they originate from the unknown image context classes that cannot be directly learned from the available labeled images. thus  the changing scale of the confidence score for the unlabeled image  xl sl  is defined as:
	yl = |肉 xl cj t + 1    肉 xl cj t |	 1 
where yl ≡ 1  肉 xl cj t  and 肉 xl cj t + 1  indicate its confidence scores with the atomic image concept cj before and after model updating.
﹛the informative unlabeled images with a large value of yl may originate from the unknown image context classes  and thus they should be incorporated for classifier training. on the other hand  the outlying unlabeled images with a yl value close to zero may originate from outliers and should be removed from the training set. in order to incorporate the informative unlabeled images for discovering the unknown image context classes  one or more new mixture components are added to the residing areas for the informative unlabeled images with a large value of yl  i.e. birth . p x cj 成cj  = 肋s百j+1p x|cj s百j+1 牟s百j+1 
百j
	+ 1   肋s百j+1 	p x|cj sl 牟sl 肋sl
l=1
where 肋s百j+1 is the weight for the  百j+1 th mixture component p x|cj  s百j+1  牟s百j+1  to characterize the appearance of unknown image context class.
﹛to eliminate the misleading effects of the outlying unlabeled images  a penalty term 污l is defined as:

where 1 ≒ 污l ≒ 1  污l = 1 if yl = 1. the penalty term 污l is able to select only the certain unlabeled images and informative unlabeled images for semi-supervised classifier training  thus the joint likelihood function is defined as:
百j
 	logp xl cj 成cj  竹	污n log	p xn|cj 牟sm 肋sm
xl﹋ 	xn﹋ 	m=1
where the discount factor 竹 = nnu is used to control the relative contribution of the unlabeled images for semi-supervised classifier training  n = nu + nl are the total number of training images  i.e.  unlabeled images nu and labeled images nl . using the joint likelihood function to replace the likelihood function in eq.  1   our adaptive em algorithm is performed on the mixture training image set  both originally and probabilistically labeled  to learn the image classifiers accurately.
1 higher-level image classifier training
﹛to learn the model-based classifier for the given secondlevel semantic image concept ci  we still use maximum likelihood criterion to determine the underlying model structure and model parameters. thus  the optimal parameter set  i.e. model structure  weights  and model parameters  成 ci
=  百 i   肋ci  牟 ci  is then determined by:	成 ci =	arg max	{l ci 成ci } 1 成ci

figure 1: our experimental results for statistical image modeling and semantic image concept modeling:  a  original images with  ocean view ;  b  individual statistical models for image blobs;  c  statistical model for semantic image concept  ocean v'iew'.
where the objective function l ci 成ci  for the second-level semantic image concept ci is defined as:
	n	百i	百j
l ci 成ci  =  log	肋cj	p xn cj|sl 牟sl 肋sl	+log 成cj  n=1j=1	l=1
	 	 1 
where l cj  成cj  is the objective function for the ci's children node cj that has been obtained by using eq.  1   n is the total number of the joint training images from all the sibling children nodes  i.e.  sibling atomic image concepts under the same parent node ci   肋cj is the weight parameter to define the relative importance of the atomic image concept cj for accurately interpreting the second-level semantic image concept ci. based on this understanding  we use the posterior probability to infer the logarithmic weight parameter.
﹛because the model structures and the model parameters for the sibling atomic image concepts have already obtained  a simple but effective solution is developed to determine the finite mixture model for accurately interpreting their parent node at the second level of the concept hierarchy. our classifier combination framework takes the following steps:  a  the mixture components from the 百i sibling atomic image concepts are combined to achieve a better approximation of the underlying class distribution of their parent node at the second level of the concept hierarchy  i.e.  the second-level semantic image concept ci . in addition  the training images for these sibling atomic image concepts are also combined as the joint training samples for their parent node ci.  b  based on the available finite mixture models for interpreting the 百i sibling atomic image concepts  our adaptive em algorithm is used to select the optimal model structure and estimate the model parameters for the given second-level semantic image concept ci by performing automatic merging  splitting  and elimination of mixture components.  c  the mixture components with less prediction power on the joint training images are eliminated. the over-

figure 1: our experimental results for statistical image modeling and atomic image concept modeling:  a  original images with  sunset ;  b  individual statistical models for image blobs;  c  statistical model for atomic image concept  sunset .
lapped mixture components from different sibling atomic image concepts are merged into a single mixture component. the elongated mixture components that underpopulate the joint training images are split into multiple representative mixture components.
﹛if one mixture component  p x|cj  sm  牟sm   is eliminated  the concept model for accurately interpreting the given second-level semantic image concept ci is then refined as:

where is the total number of the mixture components for the ci  百i is the number of the relevant sibling atomic image concepts  and 百j is the optimal model structure for its children node cj.
﹛if two mixture components p x|cj  sm  牟sm  and p x|ch  sl  牟sl  from two sibling atomic image concepts cj and ch are merged as a single mixture component p x|cj  sml  牟sml   the concept model for accurately interpreting the second-level semantic image concept ci is refined as:
百 1
p x ci 成ci  =	p x|cj sh 牟sh 肋sh+p x|cj ssml 牟sml 肋sml
h=1
 1 
where 肋sml is the weight parameter for the merged mixture
component p x|cj  sml  牟sml .
﹛if one mixture component  p x|cj sh 牟sh   is split into two new mixture components  p x|cj  sr  牟sr  and p x|cj  st  牟st   the concept model for accurately interpreting the second-level semantic image concept ci is refined as:

	+p x|cj st 牟st 肋st	 1 
﹛after the finite mixture models for the sibling second-level semantic image concepts are available  they are then integrated to obtain the finite mixture model for their parent

figure 1: the classification results for the secondlevel semantic image concept  garden  and the relevant first-level atomic image concepts.
node at the third level of the concept hierarchy. through a hierarchical approach  the hierarchical mixture models for accurately interpreting the higher-level semantic image concepts can be obtained effectively.
1. partial matching for semantic image classification
given a test image ii  its image blobs ii = {s1  ﹞﹞﹞  sl 
﹞﹞﹞  sn} and the relevant visual features x = {x1  ﹞﹞﹞  xl  ﹞﹞﹞  xn} are detected automatically by using statistical image modeling  where the class distribution for each dominant image compound  i.e.  image blobs  is charcterized by one or multiple mixture components. some experimental results on statistical image modeling are given in fig. 1 and fig.
1.
﹛the lth image blobs sl in the test image ii is first classified into the most relevant atomic image concept cj with the maximum value of posterior probablity:

﹛after the image blobs in the test image ii are classified into the most relevent atomic image concepts  the statistical models for interpreting these atomic image concepts are then integrated to characterize the semantics of the test image ii. by combining different number of these atomic image concepts and their statistical models to approximate the real class distribution of the test image ii  it is able to achieve multi-level representation of image semantics with different details. because the semantic similar images may consist of different numbers and types of atomic image concepts  our framework for multi-level image representation can provide a natural way for partial image matching. thus  semantic image classification is finally treated as a model matching problem  i.e.  macthing the statistical model for interpreting the semantics of the test image ii with the finite mixture models for interpreting the semantic image concepts of particular interest.
﹛to achieve partial image matching effectively  the bayesian approach is used to calculate the posterior probability be-

figure 1: the classification results for the second-level semantic image concepts  ocean view    praire  and the relevant first-level atomic image concepts.
tween the statistical model for interpreting the semantics of the test image ii and the finite mixture models for interpreting the semantic image concepts of particular interest  the test image ii is then classified into the most relevant semantic image concept ch with the maximum value of posterior probability.
﹛our current experiments focus on mining the image semantics on the first level and the second level of the concept hierarchy  such as  sunset    ocean view    beach    garden    mountain view    flower view    water way    sailing   and  skiing   which are widely distributed in a specific image domain of outdoor photos. some semantic image classification results are given in fig. 1  fig. 1  fig. 1 and fig. 1.  where the unknown atomic image concepts are shown in white color in the images.
﹛it is important to note that the text keywords for semantic image concept interpretation can be used to support multilevel image annotation effectively. the text keywords for interpreting the first-level atomic image concepts  i.e.  dominant image compounds  provide the annotations of the images at the content level. the text keywords for interpreting the relevant high-level semantic image concepts provide the annotations of the images at the concept level.
1. performance evaluation
﹛our experiments are conducted on two image databases: the image database from the google image search engine and the corel image database. the image database from google image search engine consists of 1 pictures. the corel image database includes more than 1 pictures with different image concepts. our works on algorithm evaluation focus on:  a  evaluating the performances of our adaptive em algorithm with different combinations of mergring  splitting and elimination;  b  evaluating the performance of our classifier training technique by using different sizes of unlabeled images;  c  comparing the performances of our hierarchical classifier with other flat classifiers under the same classification objective  i.e.  classifying the images into a set of semantic image concepts with or without using the concept hierarchy for classifier training .
table 1: the average performance of our classifiers for some atomic image concepts at the first level of concept hierarchy  precision 老 versus recall + .
conceptsbrown horsegrasspurple flower老1%1%1%+1%1%1%conceptsred flowerrocksand field老1%1%1%+1%1%1%conceptswaterhuman skinsky老1%1%1%+1%1%1%conceptssnowsunset/sunrisewaterfall老1%1%1%+1%1%1%conceptsyellow flowerforestsail cloth老1%1%1%+1%1%1%conceptselephantcatzebra老1%1%1%+1%1%1%﹛the benchmark metric for algorithm evaluation includes classification precision 老 and classification recall +. they are defined as:
		 1 
where   is the set of true positive images that are related to the given semantic image concept and are classified correctly  污 is the set of true negative images that are irrelevant to the given semantic image concept and are classified incorrectly  and 糸 is the set of false positive images that are related to the given semantic image concept but are misclassified.
﹛the average performance  precision vs. recall  of our classifiers for some atomic image concepts are given in table 1. the atomic image concepts at the first level of the concept hierarchy can be directly interpreted by using the relevant image blobs. the average performance of our classifiers for some second-level semantic image concepts are given in table 1. the semantic image concepts at the second level of the concept hierarchy can be interpreted by using the relevant atomic image concepts at the first level of the concept hierarchy. for each atomic image concept  we currently use 1 labeled images for weak classifier training.
﹛in our adaptive em algorithm  multiple operations  such as merging  splitting  and elimination  have been integrated to re-organize the distrbutions of mixture components  select the optimal number of mixture components and construct more flexible decision boundaries among sibling semantic image concepts according to the real class distributions of the training images. thus  our adaptive em algorithm is expected to have better performance than the traditional em algorithm and its recent variants.
﹛in order to evaluate the real benefits of the integration of these three operations  i.e. merging  splitting  and elimination   we have tested the performance differences of our adaptive em algorithm with different combinations of these three operations. as shown in fig. 1 and fig. 1  we have tested the performances of the classifiers under dif-

figure 1: the classification results for the secondlevel semantic image concepts  waterway    mountain view  and the relevant atomic image concepts.
ferent combinations of three operations: sm + neg represents combining three operations of merging  splitting and elimination of mixture components  sm indicates combining two operations of merging and splitting  split is for only operation of splitting  merge is for only one operation of mergring  borman is for the em algorithm developed by borman et al. .
﹛from fig. 1 and fig. 1  one can find that integrating negative images for discriminative learning of finite mixture models  i.e.  splitting by using negative images  can improve the classifiers' performance significantly.
﹛for the same purpose to classify the images into a set of pre-defined semantic image concepts  we have also compared the performance differences between our hierarchical classifier and the flat classifiers  i.e.  training the classifier for each semantic image concept independently . the test results are given in fig. 1. above the yellow line  our hierarchical classifier has better performance than the flat classifiers. below the yellow line  our hierarchical classifier has worse performance than the flat classifiers. by using the hierarchical mixture model for concept interpretation and classifier training  our hierarchical classifier has improved the performance significantly and the high-level semantic image concepts can be determined by detecting the presences of the relevant atomic image concepts.
﹛for some high-level semantic image concepts  our hierarchical image classification technique may have worse performance than the corresponding flat classifiers  under yellow line as shown in fig. 1 . the reason is that the classification errors for some low-level semantic image concepts may transmit to the relevant high-level semantic image concepts. shrinkage may be used to address this problem.
﹛given a limited number of labeled images  we have tested the performance of our classifiers by using different sizes of unlabeled images for classifier training  i.e. with different size ratios between the unlabeled images nu and the labeled images nl . the average performance differences for some semantic image concepts are given in fig.
1.
﹛one can find that the unlabeled images can improve the classifier's performance significantly when only a limited numtable 1: the average performance of our classifiers for some semantic image concepts at the second level of concept hierarchy.
conceptsmountain viewbeachgarden老1%1%1%+1%%1%1%conceptswater wayocean viewprairie老1%1%1%+1%1%1%conceptsailingskiingdesert老1%1%1%+1%1%1%conceptsflower viewweddingcity老1%1%1%+1%1%1%
figure 1: the classification results for the secondlevel semantic image concept  flower view    work ship  and the relevant atomic image concepts.
ber of labeled images are available for classifier training. the reasons are:  a  the certain unlabeled images  that originate from the existing image context classes for concept interpretation  are able to improve the density estimation by reducing the variances of mixture density.  b  the informative unlabeled images  that originate from the unknown image context classes  have the capability to provide additional image context knowledge to learn the concept models more accurately. by modifying the concept models to be more representative for the data resources  the concept models that are learned incrementally are able to obtain the accurate classifiers with higher prediction accuracy.  c  the outlying unlabeled images  that originate from outliers  can be predicted and their misleading effects on classifier training can be eliminated automatically by using a novel penalization framework.
1. conclusions and future works
﹛to support semantic image retrieval via keywords  we have proposed a novel framework for hierarchical image classification. by integrating the concept hierarchy and negative images for discriminative classifier training  our proposed framework has achieved very convincing results in a specific image domain of outdoor photos. in addition  an adaptive

figure 1: the relationship between the classifier performance  i.e.  precision 1老  and our multi-class em algorithm with different operations of merging  splitting and elimination for the semantic image concept  ocean view .

figure 1: the relationship between the classifier performance  i.e.  precision 1老  and our multi-class em algorithm with different operations of merging  splitting and elimination for the atomic image concept  rock .
em algorithm is proposed to select the optimal model structure and estimate the accurate model parameters  and thus the misleading effects of the outlying unlabeled images can be eliminated effectively. obviously  our proposed classifier training technique may be very attractive for other data domains. our future works will focus on addressing the error transmission problem for hierarchical image classifier training.
