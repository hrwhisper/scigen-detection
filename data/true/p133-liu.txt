we consider the problem of evaluating multiple overlapping queries defined on data streams  where each query is a conjunction of multiple filters and each filter may be shared across multiple queries. efficient support for overlapping queries is a critical issue in the emerging data stream systems  and this is particularly the case when filters are expensive in terms of their computational complexity and processing time. this problem generalizes other well-known problems such as pipelined filter ordering and set cover  and is not only np-hard but also hard to approximate within a factor of o logn  from the optimum  where n is the number of queries. in this paper  we present two near-optimal approximation algorithms with provably-good performance guarantees for the evaluation of overlapping queries. we present an edge-coverage based greedy algorithm which achieves an approximation ratio of  1 + log n  + log ¦Á    where n is the number of queries and ¦Á is the average number of filters in a query. we also present a randomized  fast and easily parallelizable harmonic algorithm which achieves an approximation ratio of 1¦Â  where ¦Â is the maximum number of filters in a query. we have implemented these algorithms in a prototype system  and evaluated their performance using extensive experiments in the context of multimedia stream analysis. the results show that our greedy algorithm consistently outperforms other known algorithms under various settings and scales well as the numbers of queries and filters increase.
categories and subject descriptors
f.1  analysis of algorithms and problem complexity : nonnumerical algorithms and problems-sequencing and scheduling; h.1  database management : systems- query processing
general terms
algorithms  performance  theory
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
keywords
shared filter ordering  query optimization  greedy algorithm  randomized algorithm
1. introduction
¡¡we consider the problem of evaluating multiple overlapping queries defined on data streams. each query is a conjunction of filters and each filter may be shared across multiple queries. the evaluation of a specific filter on a specific item in the data stream results in a boolean value of either true or false; in the former case  we say that the stream item satisfies the filter. if an item satisfies all the filters contained in a query  then the item is said to satisfy the query. for each item in the stream  we need to decide the subset of queries that are satisfied by this stream item. we focus on a setting where the filter evaluations are expensive  i.e.  filters incur large evaluation costs . our goal is to decide the subset of queries that are satisfied by each stream item  while minimizing the total cost incurred by the evaluation process. this problem generalizes the well-studied pipelined filter ordering problem  1  1  1  1  as well as the classical set-cover problem .
¡¡the key driver for our study is a real-time multimedia classification and matching application where users subscribe to multimedia streams that are generated continuously from different multimedia sources  e.g.  flickr  myspace  and facebook . in the context of our application  the stream items are images or videos  and the filters correspond to low-level semantic detectors that have been trained offline using manually tagged sample images. for instance  by applying the water filter on an image  we could detect if the image contains water; sky  people  blue and sand are further instances of filters. a query is a higher-level semantic concept and corresponds to an aggregate or conjunction of low-level filters. for instance  the query  does this image contain a day-time beach scene   can be resolved by applying the water  blue  sky  people and sand filters successively on the image.
¡¡the publicly available multimedia analysis and retrieval system  imars    which we utilized for the experiments in this work  is an example of a system that provides hundreds of low-level semantic multimedia filters. to understand its runtime performance  we measured the processing time of these multimedia filters on typical images  and the results are illustrated in figure 1. we can see that the evaluation of these filters is indeed very expensive. applying a single filter on a single image typically takes several hundreds of milliseconds or even longer  the most expensive one takes 1 seconds . as a result  filter evaluation is arguably

processing time of imars semantic filters  ms 
figure 1: cdf of processing time of imars semantic filters. the measurement was conducted on a xeon machine with 1ghz cpu and 1gb memory and the
filters were evaluated on 1¡Á1 jpeg color images. the median and mean processing times of a single filter are 1ms and 1ms respectively  which are orders of magnitude higher than the execution time of our filter ordering algorithm. for example  it takes about 1ms to execute the filter ordering algorithm  with 1 queries and 1 filters  more details in section 1.1 . these facts clearly underscore the need for optimizing the query and filter evaluation process.
a major bottleneck in real-time multimedia stream analysis  and a naive evaluation strategy would throttle the overall throughput achievable by the stream processing system.
¡¡motivated by the need for efficient stream processing  we tackle the central problem of selecting the order in which filters are evaluated on a stream item  so that we can minimize the total processing costs for identifying all the queries satisfied by the item. while multimedia stream applications are the key driver behind our work  we believe that the underlying problem of shared filter ordering has a general combinatorial structure that is applicable in a broad spectrum of query processing applications - hence efficient algorithms for shared filter ordering are of independent interest.
¡¡the optimal order of filter evaluation is jointly decided by many factors including: filter selectivities  i.e.  the probability that a stream item satisfies a given filter   filter popularities  i.e.  the number of queries that contain a given filter   and filter costs  i.e.  the time taken to evaluate a given filter . intuitively  it is beneficial to first evaluate filters with low selectivity  since it is likely that a stream item will not satisfy this filter  and we can quickly eliminate all the queries that contain this filter from further consideration. for a similar reason  it is also natural to evaluate highly popular filters first; if a popular filter is not satisfied  it decides many queries through a single evaluation. finally  all other factors being equal  filters with lower costs are preferable to filters with higher costs and need to be evaluated earlier. the complexity in our problem arises from the fact that we need to take the three competing factors mentioned above into account in a unified manner  while deciding the order of filter evaluations.
¡¡the shared filter ordering problem was introduced by munagala et al. . they showed that this problem is not only np-hard but also hard to approximate within a factor of o logn  from the optimum  where n is the number of queries. this essentially implies that efficient  polynomial time  algorithms for solving the shared filter ordering problem optimally are unlikely to exist and motivates the need for efficient approximation algorithms. munagala et al.  present a query-coverage based greedy algorithm for this problem with an approximation ratio of o log1 n  ¡¤ log    ; here n denotes the number of queries and   denotes the number of filters in the problem instance. for the special case of compact queries  where each query has at most ¦Â filters  they also present a hypergraph vertex-covering based o ¦Â1 -algorithm. prior to our work  these are the only two algorithms with non-trivial performance guarantees known for the shared filter ordering problem.
¡¡in this work  we present near-optimal and provably good approximation algorithms for the shared filter ordering problem which significantly improve upon the performance guarantees achievable through existing algorithms  and perform a rigorous experimental comparison of these algorithms in realistic scenarios. our specific contributions are as follows:
  we present a novel edge-coverage based greedy algorithm for shared filter ordering. the cost incurred by this algorithm is guaranteed to be at most a factor of  1 + log n  + log ¦Á   times the optimum  where n is the number of queries and ¦Á is the average number of filters in one query. this essentially matches the o log n  -hardness bound which is the best possible approximation guarantee achievable by any polynomial algorithm for the shared filter ordering problem. this significantly improves upon the approximation guarantee of o log1 n ¡¤log     with the previously known query-coverage based greedy algorithm .
  we present a randomized harmonic algorithm  which is fast and easily parallelizable  for shared filter ordering. when each query in the problem instance has at most ¦Â filters  the expected cost of the harmonic algorithm is guaranteed to be within a factor of 1¦Â from the optimum. this significantly improves upon the approximation guarantee of o ¦Â1  that is achievable through the previously known hypergraph vertexcovering based algorithm .
  we implement a prototype system based on the proposed algorithms and evaluate its performance using extensive experiments. the results confirm the superior performance and scalability of our edge-coverage based greedy algorithm over the other algorithms.
¡¡the near-optimal performance guarantees obtained in this work are brought through the following subtle algorithmic insight. the shared filter ordering problem may be viewed as the problem of covering the queries in the instance through a suitable choice of filters for each stream item; this is the view adopted in the work of munagala et al. . in this work  however  we view the filter ordering problem as one of covering the connections between queries and filters through a suitable choice of filters  rather than covering the queries themselves. specifically  consider the bipartite graph whose partitions are the set of filters and the set of queries respectively; an edge between a filter and a query indicates the fact that the filter is present in the corresponding query. for each stream item  evaluating a specific filter probabilistically removes  i.e.  covers  certain edges in this graph; the actual set of edges that are removed depends upon the structure of the query-filter graph just before the filter was evaluated  as well as whether the result of evaluation is true or false. for each stream item  our goal is to cover all the edges in the query-filter graph at the lowest cost of filter evaluation. this subtle transformation in the way we view the filter ordering problem has powerful consequences and results in our dramatically improved performance guarantees. we believe this insight could be of independent interest and applicable to other stochastic query evaluation problems as well.
¡¡the rest of the paper is organized as follows. we formulate the shared filter ordering problem in section 1. in section 1 and section 1  we present the design and performance analysis of our edge-coverage based greedy algorithm and our randomized harmonic algorithm respectively. in section 1  we report the experimental results obtained through a prototype multimedia stream system. we discuss the related work in section 1 and conclude the paper in section 1.
1. shared filter ordering problem
¡¡an instance of the shared filter ordering problem consists of a set of queries q  and a set of commutative filters f which process the input data stream. each filter fi ¡Ê f takes a stream item t as input and returns either true or false as output. if filter fi returns true for item t  we say that t satisfies fi. a query q ¡Ê q is a conjunction of a subset of filters f q    f; if a stream item t satisfies all the filters in f q   we say that t satisfies query q. for a stream item t  the evaluation plan consists of an adaptive ordering of filters f¦Ò 1  f¦Ò 1  ... f¦Ò r  such that the following properties hold:  p1 : if t satisfies query q  then all the filters in f q  appear in the adaptive ordering.  p1 : if t does not satisfy q  then at least one filter in f q  which is not satisfied by t appears in the adaptive ordering.
¡¡for each query q and stream item t  we can view the evaluation plan as providing a confirmation that the stream item t satisfies q or otherwise. the phrase adaptive ordering underscores the fact that the choice of the  i+1 st filter f¦Ò i+1  in the ordering depends on the outcome of the previous i filter evaluations. in general  a plan could evaluate only a  small  subset of filters in f in order to determine the queries that are satisfied by the stream item.
¡¡let s fi  denote the selectivity of filter fi: this is the  unconditional  probability that a stream item satisfies filter fi. let c fi  denote the processing time  or cost  incurred when filter fi is applied to a stream item.1 the cost of a plan o is the total time it takes to complete all of its filter evaluations  i.e. 
¡¡definition 1. the shared filter ordering problem is defined as the problem of computing an evaluation plan for each stream item which satisfies properties  p1  and  p1  such that the expected cost of the plan is minimized.
¡¡the expected cost of the plan is defined with respect to the joint distribution of filter satisfaction probabilities  which specifies the probability with which a stream item satisfies a given set of filters. as in munagala et al.   for the purpose of performance analysis alone  we will assume that the selectivity of filters are independent. as shown in   even with the independence assumption  no optimal polynomial time algorithm  or even a polynomial time approximation algorithm with an approximation ratio o logn  exists for the shared filter ordering problem  unless p = np. in the case of correlated filters  our algorithms can be generalized by incorporating conditional filter selectivities; we emphasize that the independence assumption is made only for the sake of analysis.
1 residual graph
¡¡we now formally introduce the notion of a residual graph which plays a key role in our algorithms. consider the current stream item t which is under processing. let b be the set of yet-to-be decided queries; b consists of queries b such that no filter in f b  has been evaluated to false until now 

and at least one filter in f b  is yet-to-be evaluated. let a denote the set of filters that will not be evaluated in the fu-

ture for the current stream item. a consists of all filters 'a' such that 'a' has already been evaluated  or none of the yet-

to-be decided queries  b  contain filter 'a'. let a = f   a be the set of yet-to-be evaluated filters.
¡¡definition 1. a residual graph g =  a b e  is a bipartite graph which consists of the yet-to-be evaluated filters and the yet-to-be decided queries; an edge {a b} ¡Ê e between a filter 'a' and a query b appears in this graph if filter a ¡Ê f b .
¡¡figure 1 illustrates the notion of a residual graph. whenever we refer to an edge {a b} in the residual graph  we will use the convention that 'a' denotes a filter  while b denotes the query.

figure 1: an example of a residual graph containing 1 undecided queries  qa  qb and qc  and 1 yet-to-be evaluated filters  f1  f1  f1 and f1 . although shown in the figure  filters f1 and f1 are not part of any yet-to-be evaluated queries and hence are not part of the residual graph; similarly  queries qd  qe  and qf have already been decided and are not part of the residual graph.
1. greedy algorithm
we now present our edge-coverage based greedy algorithm.
recall the notion of a residual graph introduced in section 1. the greedy algorithm evaluates filters one-by-one in a sequential order and terminates when all the queries have been decided. at each step  the choice of the next filter to be evaluated is determined by the current residual graph. specifically  let gi =  ai bi ei  denote the residual graph just before the greedy algorithm started the ith filter evaluation. for instance  at the beginning of the algorithm  no filters have been evaluated  and the residual graph g1 consists of all the filters and queries in the input instance. the ith filter to be evaluated is determined as follows: we computes the unit-price of each filter in the residual graph gi; the filter with the least unit-price  fmin  is evaluated next. after this evaluation  fmin is removed from the graph gi. in addition  any queries that have been decided due to this evaluation  and any other filters which are not part of a yetto-be decided query are also removed from the graph gi. this yields the updated residual graph gi+1. the algorithm terminates when all the queries have been decided.
¡¡to complete the description of the algorithm  we now state how the unit-price is calculated for a filter 'a' in the residual graph gi =  ai bi ei . we view the greedy algorithm as an incremental edge-covering algorithm. each filter evaluation results in a subset of edges in the residual graph being covered. the greedy algorithm terminates when all the edges in the original graph have been covered. based on this view  we define the unit-price of a filter such that it is inversely proportional to the expected number of edges it covers  but directly proportional to its cost. thus  the unit-price of a filter incorporates its selectivity  its popularity  as well as its cost.
let ¦Ä u  denote the degree of a node u in gi. for a filter
	.	.
'a'  define  true a  = ¦Ä a  and  false a  = b | {a b}¡Êe ¦Ä b . the former denotes the degree of filter 'a'  while the latter denotes the sum of the degrees of all the queries that contain filter 'a'. recall that c a  and s a  denote the cost and selectivity of filter 'a' respectively. the expected number of edges covered if 'a' is evaluated next is: s a  ¡¤  true a  +  1 s a  ¡¤ false a . specifically  this is the expected number of edges in gi that will not appear in gi+1 if 'a' is the ith filter to be evaluated. the unit-price of 'a' is the ratio of its cost and its expected coverage: i.e.  unitprice a  =
. this completes the descrip-
tion of the greedy algorithm.
¡¡a sample execution of the greedy algorithm is illustrated in figure 1. the greedy algorithm can be easily implemented by maintaining the current residual graph in an adjacency list data structure  and maintaining the current unitprices of filters in a  min  heap data structure. the runtime complexity of the greedy algorithm is characterized by theorem 1 below  while we present the performance analysis and establishes its approximation ratio later in section 1.
¡¡theorem 1. the worst-case time complexity of the greedy algorithm is o m¦Â log     where m is the number of edges in the initial residual graph  ¦Â is the maximum number of filters in any query  and   is the number of filters.
¡¡proof. we first compute the time complexity of the initialization phase of the greedy algorithm. since each filter and each query in the initial residual graph has at least one edge  the number of edges in the graph is at least half the number of vertices. hence  the adjacency list data-structure
for the residual graph can be created in time o n+ +n¦Á  = o n¦Á ; this follows from the fact that n¦Á is the number of edges in the residual graph. computing the unit-price of filters involve computing the  true and  false values of each filter. the  true value of a filter is simply its degree; hence  the degree of all nodes in the graph and the  true values can be computed in a single graph traversal. the  false value of a filter is simply the sum of the degree of its neighboring queries  and hence can be computed in a subsequent graph

figure 1: a sample execution of the greedy algorithm showing the changes to residual graph; in this step  the greedy algorithm has determined that filter f1 has the least unit-price and evaluates it.
traversal. both these traversals can be accomplished in time that is proportional to the size of the graph: o n¦Á . inserting the unit-price values of all the   filters into a min-heap takes o  log     time. hence  the time complexity of the initialization phase is o n¦Á +  log    .
¡¡step i of the greedy algorithm involves choosing the filter f¦Ò i  with the minimum unit-price from the heap. this can be achieved in o log   time. the evaluation of this filter might in general cover ¦Ä edges in the residual graph: depending upon the result of the evaluation being true or false  ¦Ä equals  true f¦Ò i   or  false f¦Ò i   respectively . deleting these edges from the residual graph takes o ¦Ä  time. each edge that is deleted would change the unit-price at most ¦Â filters  and hence would result in at most o ¦Â  heap updates. since m = n¦Á edges are totally deleted by the greedy algorithm and each heap update takes o log     time  the amortized time complexity of all the heap updates done during the greedy algorithm is o m¦Â log     which also dominates the runtime of the algorithm. this completes the proof of the theorem. 
1 performance analysis
¡¡we now analyze the greedy algorithm and derive an upper bound on the ratio between the expected cost incurred by the greedy algorithm and the expected cost incurred by an optimal algorithm.1 we show that the greedy algorithm has an approximation ratio of  1 + log n  + log ¦Á    where n is the total number of queries and ¦Á is the average number of filters per query.
we begin with the following definitions. suppose greedy algorithm evaluated a sequence of r filters f¦Ò 1  f¦Ò 1  ... f¦Ò r  before it finished processing a specific stream item t; further  let result f¦Ò i   denote the true/false result of the evaluation of the ith filter. in this case  we say that the greedy sequence ¦· = h f¦Ò 1  result f¦Ò 1    ...  f¦Ò r  result f¦Ò r   i occurred. we will also use ¦· interchangeably to denote the set of filters evaluated by the greedy algorithm without referring to their results; the meaning will be clear from the context. let opt denote the set of filters evaluated by the optimal algorithm when processing item x. let ¦´ denote a specific subset of ¦·. we will let ¦£ ¦· ¦´  denote the event that item t resulted in the greedy sequence ¦·  and an optimal sequence  opt  such that opt ¦· = ¦´. in other words  ¦´ is the set of common filters evaluated by both the optimal and greedy algorithms.
¡¡our performance analysis uses a novel revenue-collection mechanism and a cost sharing argument. each edge in the residual graph has to pay a price when it is covered in the greedy algorithm. each filter in the residual graph collects a revenue based on the number of edges it covers. we define a revenue collection mechanism for the greedy algorithm and for the optimal algorithm. these mechanisms ensure that in expectation  the total revenue collected by all the filters is equal to the total cost incurred in evaluating them. further  we define the price paid by the edges in the greedy algorithm in such a way that the total price they pay is equal to the total greedy revenue that is collected. finally  for any residual graph encountered by the greedy algorithm  we show that the total revenue collected by the optimal algorithm is at least the number of edges in the residual graph times the price paid by the first edge that is covered in the residual graph; these arguments directly lead to the logarithmic-factor performance guarantee for the greedy algorithm. while some of the ideas described above also feature in the analysis of the standard greedy algorithm for the deterministic set-cover problem   the probabilistic nature of our sets  filters could be true or false with some probability  makes our analysis significantly more sophisticated.
¡¡let gcost and ocost denote the cost incurred by the greedy algorithm and the optimal algorithm respectively for processing t. for each filter f¦Ò j  evaluated by the greedy algorithm  we define a value grev f¦Ò j   as follows. recall that gj represents the residual graph seen by the greedy algorithm just before it selected f¦Ò j  for evaluation. recall the definitions of unitprice ¡¤    true ¡¤   and  false ¡¤  from section 1. the greedy revenue  grev f¦Ò j    collected by filter f¦Ò j  in the greedy algorithm is the number of edges it covers times the unitprice of the filter. the value of
grev f¦Ò j   depends upon the result of the evaluation of
.
f¦Ò j . if result f¦Ò j   = true  then grev f¦Ò j   = unitprice f¦Ò j  ¡¤ true f¦Ò j  ; else  if result f¦Ò j   = false 
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡. then grev f¦Ò j   = unitprice f¦Ò j   ¡¤  false f¦Ò j  . if edge e is covered by the evaluation of filter f¦Ò j   then we say that price e  = unitprice f¦Ò j  . clearly  the total revenue collected by all the filters equals the total price paid by all the edges in the greedy algorithm.
¡¡for each filter f evaluated by the optimal algorithm  we define a value orev f  as follows. if f was also evaluated by the greedy algorithm  then orev f  = grev f ; else orev f  = c f   where c f  is the cost of evaluating f. we now prove the following useful relationship between total costs and revenues of greedy and optimal algorithms.
¡¡lemma 1. let e gcost   and e grev   denote the expected cost incurred by the greedy algorithm and the expected total revenue collected by the greedy algorithm for a randomly chosen stream item. let e ocost  and e orev   be the expected cost and revenue for the optimal algorithm  defined analogously. then  e gcost  = e grev    and e ocost  = e orev  .
¡¡proof. consider the execution of the greedy algorithm on a randomly chosen stream item. for a specific filter f and a specific residual graph h  let ¦® h f  denote the event that greedy algorithm encountered the residual graph h at some stage and evaluated filter f next. let grev f  | ¦® h f  be the random variable denoting the revenue collected by filter f given that event ¦® h f  occurred. let  true f   and  false f  be the  -values for f  and let unitprice f  denote the unit price of f just before it was evaluated. with probability s f   f evaluates to true and collects a revenue of unitprice f ¡¤ true f . with the remaining probability  the revenue is unitprice f  ¡¤  false f . hence:
e grev f  | ¦® h f   = s f  ¡¤ unitprice f  ¡¤  true f 
	+ 1   s f   ¡¤ unitprice f  ¡¤  false f  = c f 	 1 
¡¡eqn.  1  follows from the definition of unit price. let f ¡Ê ¦· denote the event that the greedy algorithm evaluated filter f at some stage. let e grev f  | f ¡Ê ¦·  denote the expected revenue collected by f given that it is evaluated by greedy. since eqn.  1  holds conditioned on any residual graph h  we have
	e grev f  | f ¡Ê ¦·  = c f 	 1 
	e grev   =	pr f ¡Ê ¦·  ¡¤ e grev f  | f ¡Ê ¦· 
f¡Êf
	=	pr f ¡Ê ¦·  ¡¤ c f  = e gcost 	 1 
f¡Êf
¡¡we now define the following events. events f ¡Ê opt  f ¡Ê ¦´  and f ¡Ê opt   ¦´ respectively denote the events that filter f was chosen by the optimal algorithm  f was chosen by both optimal and greedy algorithms  and f was chosen by optimal but not the greedy algorithm respectively. by definition  we have: orev f  | f ¡Ê opt   ¦´ = c f ; and  orev f  | f ¡Ê ¦´ = grev f  | f ¡Ê ¦´. since the probability of f being true is s f   using essentially the same sequence of arguments as in eqn.  1   we have:
	e orev f  | f ¡Ê ¦´  = e grev f  | f ¡Ê ¦´  = c f 	 1 
	e orev   =	pr f ¡Ê opt  ¡¤ e orev f  | f ¡Ê opt 
f¡Êf
	=	 pr f ¡Ê ¦´  ¡¤ e grev f  | f ¡Ê ¦´ 
f¡Êf
+pr f ¡Ê opt   ¦´  ¡¤ e grev f  | f ¡Ê opt   ¦´  
	=	 pr f ¡Ê ¦´  + pr f ¡Ê opt   ¦´   ¡¤ c f 
f¡Êf
	=	pr f ¡Ê opt  ¡¤ c f  = e ocost 	 1 
f¡Êf
	this completes the proof of the lemma.	
¡¡let price j  denote the price paid by the jth edge covered in the greedy algorithm. let m denote the total number of edges covered eventually by the greedy algorithm: i.e.  m is the number of edges in the initial residual graph g1.
let price j  | ¦£ ¦· ¦´  denote the price paid by the jth edge covered in the greedy algorithm  given that greedy sequence ¦· occurred and ¦´ is the set of common filters evaluated by both greedy and the optimal algorithm. recall that the random variable orev denotes the total revenue collected by the optimal solution. the following lemma holds.
lemma 1. for any  ¦· ¦´  and any j ¡Ê 1 ... m :
e.
¡¡proof. let the ith filter f¦Ò i  evaluated by the greedy algorithm be the one which covered the jth edge. let mlast denote the set of m   j + 1 edges that were the last ones to be covered in the greedy algorithm. in general  the residual graph gi contains all the edges in mlast and potentially some additional edges that were covered by f¦Ò i . let mcom   mlast denote the subset of edges in mlast that were covered in the greedy algorithm by the filters in ¦´. for each filter f ¡Ê ¦·  let cov f  denote the number of edges in mlast that were covered in the greedy algorithm by filter f. let f ¡Ê opt denote the event that filter f was evaluated by the optimal algorithm. let optcov f  be the random variable denoting the number of edges in the set mlast   mcom that are covered by filter f in the optimal algorithm. for any f ¡Ê f   ¦·  let grecov f  denote the expected number of edges in mlast   mcom that would have been covered by f had the greedy algorithm evaluated f after the first i   1 filters  i.e.  if f had replaced f¦Ò i  .
¡¡consider any f ¡Ê f   ¦·; suppose f covers ¦Ä edges in the set mlast   mcom in the optimal algorithm; by definition  all these edges could have been covered by f had it been chosen in place of f¦Ò i  in the greedy algorithm. hence  the expected number of edges in the set mlast mcom covered by f in the optimal algorithm is at most the expected number of edges in the set mlast mcom that could have been covered by f had it been chosen in place of f¦Ò i  in the greedy algorithm. thus  we have:
	e optcov f  |  f ¡Ê opt 	¦£ ¦· ¦´   ¡Ü
	e grecov f  | ¦£ ¦· ¦´  	 1 
since the optimal algorithm also needs to cover all the edges in the set mlast  we have: |mlast| = m   j + 1 = |mcom| +
|mlast  mcom| = f¡Ê¦´ cov f + f¡Êf ¦· optcov f . this equality follows from the fact that no filter in ¦´ covers any edge in mlast   mcom; further  by definition  the optimal algorithm does not evaluate any filter in the set ¦· ¦´; hence  the optimal algorithm cannot make use of any filter in ¦· to cover the edges in mlast   mcom. taking expectations on both sides of this eqn  we have:
m   j + 1 =	cov f  +	e optcov f  | ¦£ ¦· ¦´   =
	f¡Ê¦´	f¡Êf ¦·
	cov f  +	pr f ¡Ê opt | ¦£ ¦· ¦´  ¡Á
f¡Ê¦´	f¡Êf ¦·
e optcov f  |  f ¡Ê opt 	¦£ ¦· ¦´  . combining this with eqn.  1   we get:
	m   j + 1 ¡Ü	cov f  +	pr f ¡Ê opt | ¦£ ¦· ¦´   ¡Á
	f¡Ê¦´	f¡Êf ¦·
	e grecov f  | ¦£ ¦· ¦´  	 1 
¡¡if a filter f is present in ¦´  it is evaluated by both the optimal and greedy algorithms and collects a revenue of grev f  in the optimal algorithm; if a filter f is evaluated by the optimal algorithm but not by greedy  then it collects a revenue of c f  in the optimal algorithm. hence  the expected revenue collected by the optimal algorithm is:
	e orev | ¦£ ¦· ¦´   ¡Ý	f¡Ê¦´ grev f 
	+	f¡Êf ¦· pr f ¡Ê opt | ¦£ ¦· ¦´   ¡¤ c f 	 1 
therefore  e

eqn.  1  follows by the fact that for any positive values z1  z1  z1 and z +z ¡Ý min z1 z . we now claim that this value is at least price j  | ¦£ ¦· ¦´ . first  if f ¡Ê ¦´ is one of the first i   1 filters to be evaluated by greedy  then by definition  cov f  = 1; else  if f = f¦Ò i   then this value is at most the unitprice f   which is also price j . for any other f  if price j  exceeds this value  then f would have incurred a lower unitprice than f¦Ò i  and f would have been evaluated in place of f¦Ò i  in the greedy algorithm: this is a contradiction. this completes the proof of the lemma. 
¡¡theorem 1. the expected cost of the greedy algorithm is at most hn¦Á-factor away from the optimal expected cost 
where n is the total number of queries  and ¦Á is the average
.
number of filters per query. here  hm = mj=1j denotes the mth harmonic number; since hm   1 + logm for any m  our greedy algorithm has an approximation ratio of at most  1 + log n  + log ¦Á  .
proof. by lemma 1  for any  ¦· ¦´  and any j ¡Ê 1 ... m 
e	¦´ . hence  by deconditioning and taking expectations on both sides over all events
¦£ ¦× ¦´   we have:   . summing up over j = 1 ... m yields e orev   ¡¤ hm ¡Ý e grev  . combining this with lemma 1  and observing that m = n¦Á yields the theorem. 
1. harmonic algorithm
¡¡in this section  we present another randomized algorithm  called harmonic  for the shared filter ordering problem. this algorithm evaluates filters in the following sequence. let gi =  ai bi ei  be the residual graph just before the selection of the ith filter. we select an arbitrary edge {a b} ¡Ê ei. let n u  denote the set of neighbors for any node u in gi; for a query b  n b  denotes the set of filters connected to b in gi. for each filter f ¡Ê n b   we define a normalized weight function w ¡¤  as follows: if f = a  w a  = c a ; else if f =1 a 
. we choose a single random filter t in the set n b  with probability inversely proportional to its weight and evaluate t. in particular  the probability that a specific filter t ¡Ê n b  is chosen is. we also associate this filter evaluation with the edge that selected it  and say that edge {a b} is the owner of filter t. after the evaluation  we update the residual graph by removing this filter  the edges covered by this filter  and newly decided queries  and the filters who are no longer part of any yet-to-be decided queries; this yields the updated residual graph gi+1. the algorithm terminates when all the queries are decided. figure 1 illustrates a sample execution of the harmonic algorithm.

figure 1: a sample execution of the harmonic algorithm showing the process of picking an edge and then picking a filter. assuming f1 was picked  the figure shows possible changes to residual graph after evaluating f1.
¡¡the advantages of the harmonic algorithm stem from the fact that at any time it does not impose any condition on how the edge {a b}  and hence  the query b  must be chosen by the algorithm. we could choose multiple disjoint queries  and thus  multiple filters for evaluation in parallel   without compromising the performance guarantee of the algorithm. the harmonic algorithm only needs to maintain the current residual graph at any stage and can be implemented easily using an adjacency list data structure. due to lack of space  we present the runtime complexity of the harmonic algorithm in theorem 1 and defer its proof to the full version of this paper. we present the performance analysis of the algorithm which establishes its approximation ratio  along with complete proofs  in section 1.
¡¡theorem 1. the worst-case time complexity of the harmonic algorithm is o m + ¦Â   where m is the number of edges in the initial residual graph  ¦Â is the maximum number of filters in any query  and   is the number of filters.
1 performance analysis
¡¡we now analyze the harmonic algorithm and derive an upper bound on the ratio between the expected cost incurred by the harmonic algorithm and the expected cost incurred by an optimal algorithm. we show that the harmonic algorithm has an approximation ratio of 1¦Â  where ¦Â is the maximum number of filters in any query.
¡¡as in the analysis of greedy algorithm  our analysis of the harmonic algorithm also employs a novel revenue-collection mechanism and cost sharing argument. each edge in the residual graph pays price when it is covered by the harmonic algorithm; each filter evaluated by the harmonic algorithm or the optimal algorithm collects a revenue. we define the revenue for the filters in the optimal solution in such a way that their total expected revenue is at most twice the expected cost of their evaluation; the revenue for harmonic filters are defined such that their total expected revenue is equal to their expected cost of evaluation. for any stream item  we proceed to show that the total expected revenue collected by the harmonic algorithm is at most ¦Â times the total revenue collected by the optimal algorithm. this directly yields the 1¦Â-factor performance guarantee.
¡¡consider a stream item y. let ocost y  and hcost y  respectively denote the cost paid by the optimal algorithm and the harmonic algorithm for processing y. we define revenues for the optimal algorithm and the harmonic algorithm as follows. let orev f  denote the revenue collected by a filter f which is chosen by the optimal algorithm. if result f  = true  then f collects orev f  = c f ; else if result f  = false  then f collects .
the random variable orev  y  denotes the total revenue collected by all the filters chosen by the optimal algorithm. let hrev f  denote the revenue collected by a filter f which is chosen by the harmonic algorithm. the revenue structure in the harmonic algorithm is defined as follows. let {a b} be the edge that owns filter f  see the description of the harmonic algorithm for a definition of owner . if a = f  i.e.  the edge that owns the selected filter is incident on the filter   then hrev f  = c f . else  if a 1= f and result f  = false  then. otherwise  if a 1= f and result f  = true  then hrev f  = 1. let the random variable hrev  y  denote the total revenue collected by all the filters evaluated by the harmonic algorithm. the following lemma holds.
¡¡lemma 1. the expected revenue collected by the optimal algorithm is at most twice the expected cost of the optimal algorithm: i.e.  e orev  y   ¡Ü 1 ¡¤ e ocost y  . the expected revenue collected by the harmonic algorithm is equal the expected cost of the harmonic algorithm: i.e.  e hrev  y   = e hcost y  .
¡¡proof. let 'f ¡Ê opt' and 'f ¡Ê ¦·' denote the events that filter f was evaluated by the optimal algorithm and by the harmonic algorithm respectively. for any filter f  we have:
e orev f  | f ¡Ê opt 

hence  e orev  y   = f¡Êf pr f ¡Ê opt ¡¤e orev f  | f ¡Ê opt  ¡Ü f¡Êf 1pr f ¡Ê opt  ¡¤ c f  = 1 ¡¤ e ocost y  .
this proves the first claim in the lemma.
¡¡let '{a b} owns f' denote the event that filter f was evaluated by the harmonic algorithm and was owned by edge {a b}. if a = f  then e hrev f  | {a b} owns f  = c f . else  if a 1= f  then e hrev f  | {a b} owns f  = pr 
+pr result f  = true | {a b} owns f  ¡¤ 1 = c f . since the expected revenue collected by f is its cost regardless of the edge that owns it  it follows that e hrev f  | f ¡Ê ¦·  = c f . hence  e hrev  y   = f¡Êf pr f ¡Ê ¦·  ¡¤ e hrev f  | f ¡Ê ¦·  = f¡Êf pr f ¡Ê ¦·  ¡¤ c f  =
e hcost y  . this completes the proof the lemma.	
¡¡theorem 1. the expected cost incurred by the harmonic algorithm is at most 1¦Â times the optimal expected cost  where ¦Â is the maximum number of filters present in any query.
¡¡proof. consider a stream item y  and its result-vector  :   is the vector of |f| components  one for each filter in f and   i  = result fi  specifies if y satisfies filter fi or not. of course  we do not generally know the result-vector for a stream item since we do not generally evaluate all the filters; here  we are only conditioning on the event that the stream item y has a specific result vector   for the sake of analysis. given a specific set opt  let ¦£   opt  denote the event that the result vector of y is   and the optimal algorithm
selected the set opt during the processing of y. recall that g1 represents the initial residual graph  before the harmonic algorithm evaluated any filters. partition the set of edges in the residual graph g1 across the filters in opt such that filter f ¡Ê opt covers the set of edges m f  in g1.
¡¡recall that in the harmonic algorithm  every filter is owned by at most one edge; further  each edge could own multiple filters but at most one of them collects a non-zero hrev ¡¤   since the first filter to collect a non-zero revenue removes the edge that owns it from the residual graph. if such a non-zero revenue collecting filter f is owned by edge e  we define price e  = hrev f ; otherwise  we set price e  = 1. clearly  hrev is the sum of the prices paid by all the edges in the harmonic algorithm. next  consider a specific filter in f ¡Ê opt and the set of edges m f . w.l.o.g.  let e1 e1 ... e|m f | be the list of edges in m f  sorted according to the reverse order in which they were covered by the harmonic algorithm: i.e.  e1 is the last edge in the set m f  to be covered the harmonic algorithm. for i = 1 ... |m f |  let zi be the random variable denoting the sum of prices paid by the edges e1  e1  ...  ei. we define z1 = 1. we prove by induction on i that e zi  ¡Ü ¦Â ¡¤ orev f   even after conditioning on the set of filters owned by the edges ej  where the ej's represents the set of edges covered before ei in the harmonic algorithm.
¡¡the base case for induction is i = 1. in this case z1 = 1 ¡Ü ¦Â ¡¤ orev f  and the induction hypothesis holds vacuously. fix any r ¡Ê 1 ... m f    1. assume that the induction hypothesis holds for all i ¡Ü r. we now prove the claim for i = r + 1. consider the edge er+1 = {a b}. let d er+1  denote the set of filters that could potentially cover edge er+1 in any algorithm. since filter a is an end-point of er+1  we always have a ¡Ê d er+1 . next  since f covered edges e1 e1 ... e|m f | in the optimal algorithm  we have f ¡Ê d ei  for every ei ¡Ê m f . let n b  denote the set of filters contained in query b. any filter a¡ä 1= a¡ä  and a¡ä ¡Ê n b  will cover edge {a b} if and only if result a¡ä  = false. hence  except a  only those a¡ä ¡Ê n b  such that result a¡ä  = false  according to the result-vector    belong to the set d er+1 . edge er+1 can own at most one filter a¡ä ¡Ê d er+1 . let 'er+1 got d er+1 ' denote the event that er+1 owns some filter in the set d er+1 . for a specific a¡ä ¡Ê d er+1   let 'er+1 owns a¡ä ¡Ê d er+1 ' denote the event that er+1 owns a specific element a¡ä ¡Ê d er+1 . since  er+1 chooses filters with probability inversely proportional to their weights  it follows that pr er+1 owns a1 ¡Ê d er+1  | e got d er+1   =
.
¡¡we now make two important observations. first  observe that price er+1  = 1 if er+1 does not own any a¡ä ¡Ê d er+1 ; it is equal to the weight w a¡ä  if it owns a specific filter a¡ä ¡Ê d er+1 . hence  we have  i : e price er+1   = a¡ä¡Êd er+1  pr er+1 owns a¡ä ¡Ê d er+1   ¡¤ w a¡ä  = a¡ä¡Êd er+1  pr er+1 got d er+1  ¡Á
pr er+1 owns a¡ä ¡Ê d er+1  | e got d er+1   ¡¤ w a¡ä  ¡Ü
pr . second  since f
	a	 e +1 	   
covers er+1 in opt  observe that f ¡Ê d er+1 . in this case  it is easy to verify that w f  is equal to orev f . edges e1 ... er pay non-zero prices only when er+1 does not own f; otherwise  f would have have covered these r edges at zero price along with er+1 in the harmonic algorithm. let er+1 owns f ¡Ê d er+1  denote the event that er+1 does not
own f. we have  pr er+1 owns f ¡Ê d er+1  =
1   pr er+1 got d er+1  ¡Á
pr er+1 owns f ¡Ê d er+1  | er+1 got d er+1   =
. combining this
with  i   the induction assumption  the fact that w f  = orev f   and the fact that |d er+1 | ¡Ü ¦Â  we have:
e zr+1  ¡Ü e price er+1  +

pr er+1 owns f ¡Ê d er+1  ¡¤e zr | er+1 owns f ¡Ê d er+1  

this completes the induction argument.1 since e z|m f |  = e¡Êm f  e price e    we have: e hrev | ¦£   opt   = e e price e  | ¦£   opt   =
	f¡Êopt	e¡Êm f  e price e  | ¦£   opt   ¡Ü
   f¡Êopt ¦Â ¡¤ orev f  ¡Ü ¦Â ¡¤ orev . in other words  we have shown that conditioned on any event ¦£   opt   the expected revenue collected by the harmonic algorithm is at most ¦Â times the revenue collected by the optimal algorithm. hence  by deconditioning  we have e hrev   = ¦Âe orev  . combining this with lemma 1  we have: e hcost  = e hrev   ¡Ü 1¦Âe ocost  which completes the proof of the theorem. 
1. experimental evaluation
¡¡we have implemented a prototype system for evaluating queries over data streams and conducted extensive experiments in the context of image stream analysis. the results show that our greedy algorithm consistently outperforms other algorithms under various settings. in this section  we first describe our implementation and evaluation methodology  and then present the experimental results in detail.
1 prototype implementation
¡¡our prototype system is based on java and provides simple interfaces for the users to submit continuous queries and register their own filters. at its core  our system consists

1
 it is instructive to consider the case where r = 1 in this proof. in this case  e zr  = 1  and e zr+1  =
e. this is the simply the harmonic mean of the weights of the filters in the set d er+1  and hence  by the property of harmonic means  is
¡Ü |d er+1 |mina¡ä¡Êd er+1  w a¡ä  ¡Ü ¦Â ¡¤ orev f . the last inequality follows from the facts that there are at most ¦Â elements in the set d er+1   f is one of them  and w f  = orev f .

parameterdescriptiondefault valuesnqnumber of queries1nfnumber of unique filters1nonumber of streaming data objects1modelpopularity distribution of the filterszipf  ¦Á = 1 lenquery length  i.e.  number of filters in a query 1table 1: default parameters in the experiments



figure 1: evaluation time vs. number of queries

number of unique filters
figure 1: evaluation time vs. number of unique filters

of a server that maintains the current set of queries and evaluates them against the incoming stream data. to expedite the execution of the evaluation algorithms  we have developed several efficient data structures on the server side. since these algorithms need to manipulate the query graph extensively  it is critical to make the graph lookup and update operations fast. for this purpose  we store the query graph using two hash tables. the first one is indexed by the query ids and  for a given query  stores a list of pointers to the remaining filters on this query. similarly  the second table is indexed by the filter ids and  for a given filter  stores a list of pointers to the unresolved queries that contain this filer. as such  the query graph is stored twice  one in each table  and lookup/update can be done in o 1  time for both queries and filters. also  for the greedy algorithm  we maintain the prices of the filters in a heap  indexed by another hash table. as a result  it takes o 1  time to find the minimum price and logarithmic time to update the price.
1 evaluation methodology
¡¡our system provides generic support for various formats of data streams and their associated filters. for our experiments  we have used a set of 1 filters obtained from the multimedia analysis and retrieval system  imars    a publicly available multimedia analysis tool. each of these filters detects the presence of one specific concept in an image  and internally uses a concept model learned using support vector machines through a large training set. some of concepts are general  e.g.  water  sky  curve   while the others are more domain specific  e.g.  basketball  swimming . for our test dataset  we used 1 images obtained by crawling the flickr website  www.flickr.com   which we fed into our system.
¡¡we evaluated the performance of our systems on a linux server with a xeon 1ghz cpu and 1 gb memory  using a variety of queries generated from different models and parameters. in particular  the popularity distribution of the filters in the queries plays an important role in our system  as it impacts the degree of filter sharing across the queries. most of our experiments are conducted using the zipf distribution  which is well known as a good fit for keyword popularity in text-based searches. with zipf distribution  the popularity of the i-th most popular filter is inversely proportional to its rank i  i.e.  fi ¡Ø 1/i¦Á. in addition  we have also cross-validated our findings using other popularity models  such as uniform distribution where each filter's popularity weight is randomly chosen from  1   details in section 1.1 .
¡¡the metric of our interest is the evaluation time  which is defined as the average time taken by the system to evaluate one image against all existing queries. as shown before in figure 1  the imars filter evaluation is very expensive  with an average processing time of 1 second to match one image against one filter. as such  the evaluation time is dominated by the imars processing time  i.e.  the total cost of filters that are actually evaluated by the system. nevertheless  we also report the execution efficiency of different filter ordering algorithms in section 1.1.
¡¡for comparison purposes  we also implemented the greedy algorithm due to munagala et al. . it has a structure similar to our greedy algorithm  with the only difference in the definition of the price function used to rank the filters. in this algorithm  the price  or  prank   of a filter a is defined as   where c a  and s a  are the cost and selectivity of a respectively  and p a  is the number of unresolved query that a occurs in. in what follows  we refer to it as s-greedy to avoid confusion with our proposed greedy algorithm. to achieve fair and meaningful comparison  in one set of experiments  we vary only one parameter while fixing the other parameters as the default values shown in table 1  unless otherwise mentioned. for each set of parameters  we repeat the experiments for 1 times and the results presented are the average of multiple runs.
1 experimental results
¡¡now we present the experimental results and compare our proposed greedy and harmonic algorithms to s-greedy.
1.1 event-query matching
¡¡the first question we seek to answer in the experiments is how well different algorithms perform when the query graph grows. note that the size of a query graph can be measured by both the number of queries and the number of filters. therefore  we conduct two sets of experiments to investigate these two aspects respectively. we first fix 1 imars filters and gradually increase the number of concurrent queries from 1 to 1. then we fix the number of queries as 1 and increase the number of unique filters from 1 to 1  at which point we have exhausted the available pre-trained imars image filters.
¡¡the results of these experiments are shown in figure 1 and figure 1. we can make several important observations from these figures. first  the greedy algorithm consistently outperforms the other two algorithms. for example  with 1k queries and 1 filters  the evaluation time with greedy is 1 seconds  while the evaluation time with s-greedy and harmonic are 1 and 1 seconds respectively. in other words  greedy can reduce the evaluation time by 1 seconds  or 1%  as compared to s-greedy  and 1 seconds  or 1%  as compared to harmonic.
¡¡secondly  the system performance is more sensitive to the increase in the number of filters  as compared to that of queries. this is because all these algorithms use unique filters as the units in the evaluation process. as more queries come into the system  subscribing to the same set of filters  each filter has a larger chance of being evaluated during runtime  thus increasing the evaluation time as well. nevertheless  figure 1 shows that the performance of greedy degrades slower than the other two algorithms as the number of filters increases. the performance gap between greedy and s-greedy  or harmonic  is more significant as the system provides or accommodates more variety of filters for the users to compose their queries.
¡¡lastly  we can see that all three algorithms can achieve sub-linear increase of evaluation time when the query graph grows. moreover  both greedy and s-greedy perform much better than harmonic in these experiments. this suggest that their common greedy structure helps in finding the filter whose evaluation is most beneficial  as opposed to the randomized filter selection in harmonic.
1.1 impact of filter popularity distribution
¡¡given that the greedy  harmonic and s-greedy algorithms are all optimized for overlapping queries  their performance clearly depends on the degree of filter sharing in the actual queries. intuitively  the more sharing among the queries  the better performance one would expect from these algorithm. however  the sharing structure in a large query graph is difficult to characterize and may be affected by many factors. in what follows  we study the impact of two of the factors  namely the filter popularity distribution and the query length.
¡¡as discussed earlier  we use the zipf distribution  with ¦Á = 1  in most experiments  because it has been shown to fit fairly well with user search behavior in practice. with zipf distribution  a few most popular filters appear in many queries  while many unpopular filters appear in only a few

figure 1: evaluation time vs. filter popularity distribution

figure 1: evaluation time vs. query length
queries. with a larger ¦Á  the most popular filters gain even more weight  leading to an even greater variation in popularity among the filters. we have tried zipf distribution with different parameters  as well as the uniform distribution as the comparison base. in these experiments  the number of queries and filters are fixed as 1 and 1k respectively. however  the resulting query graphs vary significantly with different popularity distributions.
¡¡figure 1 shows the results of evaluation time with different distributions. we can make two observations from it. first  the relative order of the three algorithms' performance is not affected by the popularity distribution. in other words  the comparison presented elsewhere in this section is quite general and holds for a wide variety of scenarios. secondly  as we have expected  all three algorithms can benefit from the increased degree of query overlap  as the distribution changes from uniform to zipf and then the exponent ¦Á in zipf increases. in fact  as the sharing continues to enhance  the performance gap between these algorithms shrinks. this is because with highly unbalanced filter popularities  many queries become largely similar  if not identical  to each other.
1.1 impact of query length
¡¡next we study the impact of query length  i.e.  the number of filters in a query   which also implicitly controls the degree of query overlap. in each of these experiments  we fix the other parameters while varying the query length from 1 to 1  with an increment of 1.
¡¡the evaluation time with queries of different lengths is plotted in figure 1. one can readily see that all three algo-

figure 1: execution time of different algorithms
rithms benefit from longer queries  and that the impact of query length is almost the same for greedy and s-greedy. this seems consistent with our intuition that the longer queries are more likely to share filters  hence the algorithm performance can be improved. however  there is another factor that also plays a role here: given the same selectivities of the filters  a longer query is less likely to be satisfied. note that if a query is satisfied  any algorithm must evaluate all its filters; however  if a query is not satisfied  in an ideal case  evaluating only one of its filters would suffice. in this sense  with longer queries  the amount of evaluation  workload  drops. figure 1 shows that these three algorithms can indeed leverage such an opportunity and reduce their respective evaluation time.
1.1 runtime efficiency
¡¡finally we present in figure 1 the execution time of the evaluation algorithms themselves  as the number of queries increases. the number of filters are fixed as 1 in these experiments. we can see that the harmonic algorithm is much faster than both greedy and s-greedy. moreover  its execution time increases much slower as the underlying query graph grows. this is because harmonic does not need to maintain or update any states  e.g.  price in greedy and rank in s-greedy  for the filters. such runtime efficiency makes harmonic a good candidate for those scenarios in which the system needs to handle a vast amount of concurrent data streams  hence the server running the evaluation algorithm may become the performance bottleneck.
1. related work
¡¡as noted earlier  the shared filter ordering problem was introduced by munagala et al. . they observe that this problem is a probabilistic generalization of the classical setcover problem; consequently they show that shared filter ordering is np-hard and hard to approximate within a factor of o logn  of the optimal solution  where n is the number of queries. they also compare adaptive and non-adaptive filter orderings1 and show that there exist problem instances where the cost of any non-adaptive ordering is   ¦Ì  times the cost of the optimal adaptive ordering  where ¦Ì is the maximum number of queries that share a filter. this motivates the need for adaptive algorithms for filter evaluation; munagala et al.  present a query-coverage based greedy algorithm with an approximation ratio of o log1 n ¡¤log      where n is the number of queries and   is the number of filters in the problem instance. they also consider the special-case where each query has at most ¦Â filters  where ¦Â is a small value and design a hypergraph vertex-covering based o ¦Â1 factor approximation algorithm.
¡¡our results improve upon those of  in the following ways. we propose a new edge-coverage based greedy algorithm whose performance is guaranteed to be within  1 + log n  + log ¦Á  -factor of the optimal algorithm  where n and ¦Á are the number of queries and the average number of filters per query respectively. in terms of provably-good performance guarantee  our greedy algorithm approaches the o log n  -hardness bound  which is essentially the best possible approximation guarantee for any polynomial-time algorithm for this problem. for the special cases where each query has at most ¦Â filters  our harmonic algorithm achieves an approximation ratio of at most 1¦Â. thus  in terms of analytical performance guarantees  our edge-coverage based greedy algorithm substantially improves upon the querycoverage based greedy algorithm of   while our harmonic algorithm substantially improves upon the hypergraph vertex covering based algorithm of . our experiments also show that our greedy algorithm consistently outperforms that of   and in a multi-server parallel environment  our harmonic algorithm also performs much better than the greedy algorithm of .
¡¡from an algorithmic perspective  the shared filter ordering problem is a probabilistic generalization of the standard set-cover problem which has been studied extensively  and a special case of the stochastic set-cover problem which has been a focus of recent research. the various non-trivial approximation algorithms for set cover achieve one of two approximation-ratios: o logn   where n is the number of elements to be covered  or ¦Â which is the maximum frequency with which an element appears across the given sets . our greedy algorithm as well as our harmonic algorithm are inspired by algorithms for the standard set-cover problem  although our design and analysis is much more subtle than traditional set-cover due to the probabilistic coverage property of our sets: filters  which correspond to sets  may be true or false with some probability  and this affects their coverage. the stochastic set-cover problem  is a generalization of both shared filter ordering and the standard set-cover problems  where each set can cover different elements depending on the outcome of a probabilistic event. prior to our work  the best known approximation algorithm for stochastic set-cover is due to goemans and vondrak   who show an n-approximation using very different techniques compared to our work.1 our work substantially improves upon this result: a simple extension to our greedy algorithm yields an approximation ratio of 1+log n +log ¦Á  for the stochastic set-cover problem  where n is the number of elements to be covered  and ¦Á is the average over all elements of the number of sets that could cover the element with non-zero probability. as mentioned in the introduction  the key insight which is at the heart of our superior approximation guarantees is that we treat the shared filter ordering problem  or more generally  the stochastic set-cover problem  as a problem of covering the edges in the queryfilter bipartite graph  element-set bipartite graph   rather than that one of covering the queries  elements  themselves.
¡¡in conventional relational databases  the problem of multiquery optimization has been tackled in  1  1 . these works focus on relational operators  while we consider a different case with arbitrary operators. continuously adaptive continuous queries  cacq   seeks to optimize the evaluation of continuous queries defined on data streams by sharing relational operators  selections and join state  across queries. it also adapts to changes in operator costs and selectivities over time. niagaracq and it's extensions  1  1  propose different grouping mechanisms for continuous queries in order optimize a large number of continuous queries in the internet. again  they consider relational operators and not arbitrary operators in queries as in our work. hence  many of the techniques proposed in these works are not applicable in our context. optimization of a single query with expensive filters  also known as pipelined filter ordering  has been considered in  1  1  1  1 . ordering shared filters which are part of overlapping queries  as considered in this work  is clearly a generalization of ordering pipelined filters that are part of a single query. however  the special combinatorial structure in the case of pipelined filter ordering makes it possible to devise algorithms with strong theoretical guarantees: for instance  babu et al.  describe a 1-approximation algorithm for pipelined filter ordering even when filter selectivities are arbitrarily correlated and not independent.
¡¡efficient evaluation of multiple overlapping queries over data streams has received extensive attention in the context of content-based publish-subscribe systems. however  none of the existing approaches for pub-sub environments consider query optimization in the presence of expensive filters  but instead assume that all filter evaluations are cheap. this significantly alters the flavor of the problem as the goal becomes one of optimizing the runtime efficiency of the query evaluation algorithm  rather than minimizing the cost of filter evaluations. for example  the siena   gryphon  and jedi  systems represent stream items using attribute-value pairs  and queries using conjunctions of predicates. these predicates can be evaluated relatively quickly by examining the item  which typically contains smallsized structured text data. predicate evaluation in many semantic pub-sub systems such as ops   s-topss   g-topss   cream   and the system of chirita et al.  can also be accomplished cheaply by simply comparing the arguments  subject and object  of a predicate with the attributes of a stream item. several systems  such as xfilter  xtrie and webfilter  represent items using xml documents and subscriptions using xpath expressions or its variations. again  these xpath expressions can be quickly evaluated by examining the item. since predicate evaluations are not expensive in these systems  their overarching goal is to optimize the runtime efficiency of query resolution algorithms rather than ordering the evaluation of predicates optimally.
1. conclusion
¡¡efficient support for continuous queries over data streams is a critical issue in the emerging stream database systems. in this paper  we presented near-optimal and provably good algorithms for the shared filter ordering problem that arises in the evaluation of multiple overlapping queries. the performance guarantees of our algorithms significantly improve upon those of existing algorithms for this problem  and our experimental results indicate the superiority of our greedy algorithm in various practical scenarios. an interesting avenue of future work is to rigorously analyze the performance of our algorithms when filter selectivities are arbitrarily correlated. designing efficient algorithms with non-trivial performance guarantees in this setting seems very challenging.
acknowledgments
the authors would like to thank dr. rong yan for providing the imars semantic filters and the anonymous reviewers for their constructive comments.
