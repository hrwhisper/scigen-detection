the querying and analysis of data streams has been a topic of much recent interest  motivated by applications from the fields of networking  web usage analysis  sensor instrumentation  telecommunications  and others. many of these applications involve monitoring answers to continuous queries over data streams produced at physically distributed locations  and most previous approaches require streams to be transmitted to a single location for centralized processing. unfortunately  the continual transmission of a large number of rapid data streams to a central location can be impractical or expensive. we study a useful class of queries that continuously report the k largest values obtained from distributed data streams   top-k monitoring queries    which are of particular interest because they can be used to reduce the overhead incurred while running other types of monitoring queries. we show that transmitting entire data streams is unnecessary to support these queries and present an alternative approach that reduces communication significantly. in our approach  arithmetic constraints are maintained at remote stream sources to ensure that the most recently provided topk answer remains valid to within a user-specified error tolerance. distributed communication is only necessary on occasion  when constraints are violated  and we show empirically through extensive simulation on real-world data that our approach reduces overall communication cost by an order of magnitude compared with alternatives that offer the same error guarantees.
1. introduction
모recently  much attention has been focused on online monitoring applications  in which continuous queries operate in near real-time over data streams such as call records  sensor readings  web usage logs  network packet traces  etc.  1  1  1  1 . often  data streams originate from multiple remote sources and must be transmitted to a central processing system where monitoring takes place . the nature of some online monitoring applications is such that streaming data rates may ex-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
ceed the capacity of the monitoring infrastructure  data collection points  transmission lines  processing center  etc.  in terms of storage  communication  and processing resources  1  1 . fortunately  many online monitoring tasks only require that attention be focused on atypical behavior in the environment being monitored  while habitual behavior is to be ignored. for such tasks  it is only necessary to examine a small subset of the data in detail  and most of the data can safely be disregarded.
모for the purposes of many applications  the specific behaviors of interest to monitor in detail are characterized by numeric values or item frequencies that are exceptionally large  or small  relative to the majority of the data. for example  consider the important problem of monitoring computer networks to detect distributed denial-of-service  ddos  attacks. network hosts engaging in certain malevolent behaviors such as ddos attacks may issue an unusually large number of domain name service  dns  lookup requests to multiple distributed dns servers from a single  possibly spoofed  ip address . on the recipient end  ddos attacks targeted at a particular host within the private network of an internet service provider  isp  are typically characterized by a large overall incoming traffic volume spread across several of the isp's border routers  directed at that single host .
모even under normal operation  the volume of streaming data generated by a single dns server or highspeed network router can be very large  and to guard against ddos attacks it may be necessary to monitor a large number of them  making the aggregate volume of streaming data enormous and therefore prohibitively expensive to capture  transmit  and process . since it is crucial to avoid degrading normal service  it is necessary to only perform detailed monitoring of traffic associated with potentially suspicious behavior such as an unusually fast rate of dns lookups or a large volume of traffic directed at a single host  and the ability to identify such behavior at low cost to the monitoring infrastructure is crucial.
as another example  consider wireless sensor networks 
e.g.    for which diverse application scenarios have been proposed including moving object tracking. large vehicles can be tracked using seismic or acoustic sensors scattered over a region of land  and tracking algorithms based on probabilistic models need only query the sensors with the highest vibration or acoustic amplitude readings . power consumption is of significant concern for miniature sensors  which have severely limited batteries that may be impractical to replace. since radio usage is the dominant factor determining battery life  1  1   it is important to identify the sensors with high readings using as little communication as possible  and to restrict subsequent detailed monitoring and querying to only those sensors. other examples of applications in which behaviors of interest can often be characterized by exceptionally large  or small  numeric values include online monitoring of telephone call record statistics  auction bidding patterns  cluster load characteristics  and web usage statistics.
1 distributed top-k monitoring
모in the application scenarios we described  detailed monitoring is only necessary for a subset of the data having corresponding numeric attributes whose values are among the k largest  where k is an applicationdependent parameter. therefore  the transmission  storage  and processing burdens on the monitoring infrastructure can be reduced by limiting the scope of detailed monitoring accordingly. to realize the performance benefits of reduced-scope monitoring  a low-cost mechanism is needed for continually identifying the top k data values in a distributed data set  a task we refer to as distributed top-k monitoring.
모it might appear that when data values at different nodes change independently  effective top-k monitoring requires continual transmission of all data updates to a single location for data to be combined and compared. fortunately  a much cheaper alternative exists. in this paper we present an algorithm for distributed top-k monitoring that performs very little communication while continuously providing high-quality answers. for applications requiring 1% accuracy  our algorithm provides the exact top-k set at all times. in many online monitoring applications approximate answers suffice  1  1   and our algorithm is able to reduce costs further by providing an approximation to the top-k set that is guaranteed to be accurate within a pre-specified error tolerance. the error tolerance can be adjusted dynamically as needed  with more permissive error tolerances incurring lower costs to the monitoring infrastructure. before describing our approach in section 1  we first present a detailed example scenario in section 1  and then define the problem of distributed top-k monitoring formally in section 1.
1 running example
모throughout this paper  we will refer to the following example scenario that represents a typical application of top-k monitoring. it will serve to motivate our approach and we will use it to validate our distributed top-k monitoring algorithm.
모the example application we consider is that of monitoring http requests across a distributed set of mirrored web servers. the organizers of the 1 fifa soccer world cup  one of the world's largest sporting events  maintained a popular web site that was accessed over 1 billion times between april 1  1 and july 1  1  which represents an average of over 1 accesses per minute. the web site was served to the public by 1 servers  each with identical copies of the web content  distributed among 1 geographic locations around the world. cisco distributed director was used to route user requests to one of the four locations based on network latencies. further details can be found in . following are two continuous monitoring queries that the administrators of the world cub web site might have liked to have posed:
모monitoring query 1. which web documents are currently the most popular  across all servers 
모monitoring query 1. within the local cluster of web servers at each of the four geographic locations  which server in the cluster has the lowest current load 
모monitoring query 1 identifies the most popular documents by aggregating recent hit count information from all 1 web servers. this query could be used to provide near real-time feedback to web site designers  allowing them to adapt site content and hyperlink structure in response to observed usage patterns by adjusting headlines  reordering text  adding additional links to popular articles  etc. furthermore  the effects of changes made to the web site would be observable shortly afterward  achieving tight closed-loop interaction with immediate feedback. monitoring query 1  which continuously reports the currently least loaded server within a local cluster in near real-time  could be used to distribute load evenly across all servers within each local cluster.
모for both queries  answer timeliness is clearly an important consideration. online monitoring  assuming it can be performed efficiently  is therefore preferable to retrospective offline analysis in this scenario. we now define the problem of online top-k monitoring formally.
1 formal problem definition
모we consider a distributed online monitoring environment with m + 1 nodes: a central coordinator node n1  and m remote monitor nodes n1 n1 ... nm. collectively  the monitor nodes monitor a set u of n logical data objects u = {o1 o1 ... on}  which have associated numeric  real  values v1 v1 ... vn. the values of the logical data objects are not seen by any individual node. instead  updates to the values arrive incrementally over time as a sequence s of hoi nj  i tuples  which may arrive in arbitrary order. the meaning of the tuple hoi nj  i is that monitor node nj detects a change of    which may be positive or negative  in the value of object oi. a tuple hoi nj  i is seen by monitor node nj but not by any other node nl  l =1 j. insertions of new objects into u can be modeled using update tuples containing object identifiers not currently in u  the prior value of newly inserted objects is assumed to be zero . this data stream model could be termed the distributed cash register model  to extend the terminology of . for each monitor node nj  we define partial data values v1 j v1 j ... vn j representing nj's view of the data stream  where vi j = phoi nj  i뫍s  . the overall logical data value of each object oi  which is not materialized on any node  is defined to be vi = p1뫞j뫞m vi j. many of the symbols introduced here and later are listed in table 1  below  for convenience.

figure 1: distributed top-k monitoring architecture.모we now use this notation to describe the data streams that arise in our world cup example scenario and are relevant to our example queries. in our scenario  the web servers also function as monitor nodes. for monitoring query 1  we have one logical data object for each web document  and the logical data value of interest for each document is the number of times that document has been requested. each page request to the jth server for the ith object  web document  is represented as a tuple hoi nj 1i. for monitoring query 1  we have one logical data object for each web server in a cluster representing that server's current load  so the set of logical data objects is the same as the set of monitor nodes: u = {n1 n1 ... nm}. minimizing total server load is the same as maximizing   1 load   and we could measure load as the number of hits in the last 1 minutes  so each page request to the jth server corresponds to a tuple hnj nj  1i followed 1 minutes later by a canceling tuple hnj nj 1i once the page request falls outside the sliding window of current activity.
모the coordinator is responsible for tracking the top k logical data objects within a bounded error tolerance. more precisely  the coordinator node n1 must maintain and continuously report a set t   u of logical data objects of size |t | = k. t is called the approximate top-k set  and is considered valid if and only if:

where 1 is a user-specified approximation parameter. if  = 1  then the coordinator must continuously report the exact top-k set. for non-zero values of   a corresponding degree of error is permitted in the reported top-k set. the goal for distributed top-k monitoring is to provide  at the coordinator  an approximate top-k set that is valid within  at all times  while minimizing the overall cost to the monitoring infrastructure. for our purposes  cost is measured as the total number of messages exchanged among nodes.
모as discussed above  we expect that in most applications the purpose of continually identifying the top k objects at low cost is to restrict the scope of further  more detailed monitoring such as downloading packet traces from routers  obtaining acoustic waveforms from sensors  etc. if the monitoring objective is to form a ranked list of the top k objects  which may be useful in  e.g.  monitoring query 1  then as detailed monitoring the coordinator can track  approximations of  the logical data values of the objects currently in the top k set t   as in  e.g.    and use this information to perform  approximate  top-k ranking at much lower cost than by tracking all values. our overall distributed monitoring architecture is illustrated in figure 1. many of the details in this figure pertain to our top-k monitoring algorithm  described next.
1 overview of approach
모our overall approach is to compute and maintain at the coordinator an initially valid top-k set t and have the coordinator install arithmetic constraints at each monitor node over the partial data values maintained there to ensure the continuing validity of t . as updates occur  the monitor nodes track changes to their partial data values  ensuring that each arithmetic constraint remains satisfied. as long as all the arithmetic constraints hold across all nodes  no communication is necessary to guarantee that the current top-k set t remains valid. on the other hand  if one or more of the constraints becomes violated  a distributed process called resolution takes place between the coordinator and some monitor nodes to determine whether the current top-k set t is still valid and to alter it when appropriate. afterward  if the top-k set has changed  the coordinator installs new arithmetic constraints on the monitor nodes to ensure the continuing validity of the new top-k set  and no further action is taken until one of the new constraints is violated. the arithmetic constraints maintained at each monitor node nj continually verify that all the partial data values vt j of objects ot in the current top-k set t are larger than all the partial data values vs j of other objects os 뫍 t/   thereby ensuring that the local top-k set matches the overall top-k set t . clearly  if each local top-k set matches t   then t must be valid for any

1.1 adjustment factors and slack
모in general  if the logical data values are not distributed evenly across the partial values at monitor nodes  it is not likely to be the case that the local top-k set of every node matches the global top-k set. for example  consider a simple scenario with two monitor nodes n1 and n1 and two data objects o1 and o1. suppose the current partial data values at n1 are v1 = 1 and v1 = 1 and at n1 are v1 = 1 and v1 = 1  and let k = 1  so the current top-k set is t = {o1}. it is plainly not the case that v1 뫟 v1  as required by the arithmetic constraint maintained at node n1 for t .
모to bring the local top-k set at each node into alignment with the overall top-k set  we associate with each partial data value vi j a numeric adjustment factor 붻i j that is added to vi j before the constraints are evaluated at the monitor nodes. the purpose of the adjustment factors is to redistribute the data values more evenly across the monitor nodes so the k largest adjusted partial data values at each monitor node correspond to the current top-k set t maintained by the coordinator. to ensure correctness  we maintain the invariant that the adjustment factors 붻i   for each data object oi sum to zero-in effect  the adjustments shift the distribution of partial values for the purpose of local constraint checking  but the aggregate value vi remains unchanged. in the above example  adjustment factors of  say  붻1 =  1  붻1 = 1  붻1 = 1  and 붻1 = 1 which are assigned by the coordinator at the end of resolution  could be used  although many alternatives are possible. adjustment factors are assigned by the coordinator during resolution  and as long as the constraints at each monitor node hold for the adjusted partial data values  the continuing validity of the current top-k set with zero error is guaranteed. it may be useful to think of the constraints as being parameterized by the adjustment factors  as illustrated in figure 1.
when a constraint becomes violated  resolution is performed to determine whether the current top-k set is still valid and possibly assign new constraints as required if a new top-k set is selected. at the end of resolution  regardless of whether new constraints are used or the existing ones are kept  the coordinator assigns new adjustment factors as parameters for some of the constraints. adjustment factors are selected in a way that ensures that all parameterized constraints are satisfied and the adjustment factors for each object sum to zero. as illustrated above by our simple two-object example  there is typically some flexibility in the way adjustment factors are assigned while still meeting these requirements. in particular  the distribution of  slack  in the local arithmetic constraints  the numeric gap between the two sides of the inequality  can be controlled. observe that in our example the total amount of slack available to be distributed between the two local arithmetic constraints is v1   v1 = 1 units. to distribute the slack evenly between the two local constraints at 1 units apiece  we could set 붻1 =  1  붻1 = 1  붻1 = 1  and 붻1 = 1  for instance.
모the amount of slack in each constraint is the dominant factor in determining overall cost to maintain a valid approximate top-k set at the coordinator because it determines the frequency with which resolution must take place. intuitively  distributing slack evenly among all the constraints after each round of resolution seems like a good way to prolong the time before any constraint becomes violated again  making resolution infrequent. however  when more than two objects are being monitored  the best way to distribute slack that minimizes overall resolution cost is not always obvious because multiple constraints are required at each node  and at a given node the amount of slack in each constraint is not independent. moreover  the best allocation of slack depends on characteristics of the data such as change rates. finally  as we will see later  in our approach the communication cost incurred to perform resolution is not constant  but depends on the scope of resolution in terms of how many nodes must be accessed to determine whether the current top-k set is valid. therefore  the frequency of resolution is not the only relevant factor.
모in this paper we propose policies for managing slack in arithmetic constraints via the setting of adjustment factors  and evaluate the overall cost resulting from our policies using extensive simulation over real-world data. we show that  using appropriate policies  our distributed top-k monitoring algorithm achieves a reduction in total communication cost of over an order of magnitude compared with replicating all partial data values continually at the coordinator.
1.1 approximate answers
모when exact answers are not required and a controlled degree of error is acceptable  i.e.    1  our approach can be extended in a straightforward manner to perform approximate top-k monitoring at even lower cost. the overall idea remains the same: an initial approximate top-k set t valid within the user-specified approximation parameter  is stored at the coordinator  and arithmetic constraints are installed at the monitor nodes that ensure that all adjusted partial data values of objects in t remain greater than those for objects not in t . to permit a degree of error in the top-k set t of up to   we associate additional adjustment factors 붻1 붻1 ... 붻n 1 with the coordinator node n1  retaining the invariant that all the adjustment factors 붻i   for each object oi sum to zero   and introduce the additional stipulation that for each pair of objects ot 뫍 t and. using appropriate policies for assigning adjustment factors to control the slack in the constraints  our approximate top-k monitoring approach achieves a significant reduction in cost compared with an alternative approach that provides the same error guarantee  by maintaining at the coordinator approximate replicas of all partial data values.
1 outline
모the remainder of this paper is structured as follows. first  in section 1 we situate our research in the context of related work. then  in section 1 we provide a detailed description of our algorithm for distributed top-k monitoring and introduce a parameterized subroutine for assigning adjustment factors. in section 1 we evaluate several alternative adjustment factor policies empirically  and perform experimental comparisons between our algorithm and alternatives. we then summarize the paper in section 1.
1. related work
모the most similar work to our own we are aware of covers one-time top-k queries over remote sources  e.g.   1  1   in contrast to continuous  online monitoring of top-k answers as they change over time. recent work by bruno et al.  focuses on providing exact answers to one-time top-k queries when access to source data is through restrictive interfaces  while work by fagin et al.  considers both exact answers and approximate answers with relative error guarantees. these one-time query algorithms are not suitable for online monitoring because they do not include mechanisms for detecting changes to the top-k set. while monitoring could be simulated by repeatedly executing a one-time query algorithm  many queries would be executed in vain if the answer remains unchanged. the work in both  and  focuses on dealing with sources that have limited query capabilities  and as a result their algorithms perform a large number of serial remote lookups  incurring heavy communication costs as well as potentially high latencies to obtain query answers in a wide-area environment. both  and  present techniques for reducing communication cost  but they rely on knowing upper bounds for the remote data values  which is not realistic in many of the application scenarios we consider  section 1 .
모work on combining ranked lists on the web  1  1  focuses on combining relative orderings from multiple lists and does not perform numeric aggregation of values across multiple data sources. furthermore   1  1  do not consider communication costs to retrieve data and focus on one-time queries rather than online monitoring. recent work in the networking community has focused on online  distributed monitoring of network activity. for example  algorithms proposed in  can detect whether the sum of a set of numeric values from distributed sources exceeds a user-supplied threshold value. however  we are not aware of any work in that area that focuses on top-k monitoring. reference  also studies the problem of monitoring sums over distributed values within a user-specified error tolerance  but does not focus on top-k queries.
모top-k monitoring of a single data stream was studied in   while  and  give approaches for finding frequent items in data streams  a specialization of the topk problem. this work   1  1  1   only considers single data streams rather than distributed data streams and concentrates on reducing memory requirements rather than communication costs. this focus is typical of most theoretical research in data stream computation  which generally concentrates on space usage rather than distributed communication. exceptions include  and   neither of which addresses the top-k monitoring problem.
모top-k monitoring can be thought of as an incremental view maintenance problem. most work on view maintenance  including recent work on maintaining aggregation views  and work by yi et al. on maintaining top-k views   does not focus on minimizing communication cost to and from remote data sources. also  we are not aware of work on maintaining approximate views. our online monitoring techniques involve the use of distributed constraints  and most previous work on distributed constraint checking  e.g.   1  1  1  only considers insertions and deletions from sets  not updates to numeric data values. approaches for maintaining distributed constraints in the presence of changing numeric values were proposed in     and recently in . all of these protocols require direct communication among sources  which may be impractical in many applications  and none of them focus on top-k monitoring  so they are not designed to minimize costs incurred by resolution of the top-k set with a central coordinator.
1. algorithm for distributed top-k monitoring
모we now describe our algorithm for distributed top-k monitoring. for convenience  table 1 summarizes the notation introduced in section 1  along with new notation we will introduce in this section. recall from section 1 that while engaged in top-k monitoring the coordinator node n1 maintains an approximate top-k set t that is valid within . in addition to maintaining the top-k set  the coordinator also maintains n m + 1  numeric adjustment factors  labeled 붻i j  one corresponding to each pair of object oi and node nj  which must at all times satisfy the following two adjustment factor invariants:
invariant 1: for each object oi  the corresponding adjustment factors sum to zero: p1뫞j뫞m 붻i j = 1.
invariant 1: for all pairs hot 뫍 t  os 뫍 u  t i  붻t 1 +
.

table 1: meaning of selected symbols.
the adjustment factors are all initially set to zero.
모at the outset  the coordinator initializes the approximate top-k set by running an efficient algorithm for onetime top-k queries  e.g.  the threshold algorithm from . once the approximate top-k set t has been initialized  the coordinator selects new values for some of the adjustment factors  using the reallocation subroutine described later in section 1  and sends to each monitor node nj a message containing t along with all new adjustment factors 붻  j = 1 corresponding to nj. upon receiving this message  nj creates a set of constraints from t and the adjustment factor parameters 붻  j to detect potential invalidations of t due to changes in local data. specifically  for each pair hot 뫍 t  os 뫍 u   t i of objects straddling t   node nj creates a constraint specifying the following arithmetic condition regarding the adjusted partial values of the two objects at nj:
vt j + 붻t j 뫟 vs j + 붻s j
모as long as at each monitor node every local arithmetic constraint holds1  and adjustment factor invariant 1 above also holds  then for each pair hot 뫍 t  os 뫍 u t i  pp 붻 p
p1뫞j뫞m s j. applying invariant 1  this expression sim붻
plifies to  so the approximate top-k set t is guaranteed to remain valid  by definition from section 1 . on the other hand  if one or more of the local constraints is violated  t may have become invalid  depending on the current partial data values at other nodes. whenever local constraints are violated  a distributed process called resolution is initiated to determine whether the current approximate top-k set t is still valid and rectify the situation if not. resolution is described next.
1 resolution
모resolution is initiated whenever one or more local constraints are violated at some monitor node nf. let f be the set of objects whose partial values at nf are in-
1
 rather than checking all the constraints explicitly every time a partial data value is updated  each node need only track the smallest adjusted partial data value of an object in the top-k set t  after adding its 붻 value  and the largest adjusted partial data value of an object not in t . this tracking can be performed efficiently using two heap data structures.
volved in violated constraints.  f contains one or more objects from t plus one or more objects not in t .  to simplify exposition  for now we make the unrealistic assumption that the resolution process is instantaneous and further constraint violations do not occur during resolution. in our extended paper  we describe how our algorithm handles further constraint violations that may occur while resolution is underway  and we show that the instantaneousness assumption is not necessary for the functioning or correctness of our algorithm.
모our resolution algorithm consists of the following three phases  the third phase does not always occur :
phase 1: the node at which one or more constraints have failed  nf  sends a message to the coordinator n1 containing a list of failed constraints  a subset of its current partial data values  and a special  border value  it computes from its partial data values.
phase 1: the coordinator determines whether all invalidations can be ruled out based on information from nodes nf and n1 alone. if so  the coordinator performs reallocation to update the adjustment factors pertaining to those two nodes to reestablish all arithmetic constraints  and notifies nf of its new adjustment factors. in this case  the top-k set remains unchanged and resolution terminates. if  on the other hand  the coordinator is unable to rule out all invalidations during this phase  a third phase is required.
phase 1: the coordinator requests relevant partial data values and a border value from all other nodes and then computes a new top-k set defining a new set of constraints  performs reallocation across all nodes to establish new adjustment factors to serve as parameters for those constraints  and notifies all monitor nodes of a potentially new approximate top-k set t 1 and the new adjustment factors.
모we now describe each phase in detail. in phase 1  nf sends a message to the coordinator n1 containing a
모list of violated local constraints along with its current partial data values for the objects in the resolution set r = f 뫋t . in the same message  nf also sends its border value bf  which will serve as a reference point for reallocating adjustment factors in subsequent phases. the border value bj of any node nj is defined to be the smaller of the minimum adjusted partial data value of objects in the current top-k set and the maximum adjusted partial data value of objects not in the resolution set:
	bj = min{	min	 vt j + 붻t j  
ot뫍t
모max	 vs j + 붻s j 	} os뫍u r
모in phase 1  the coordinator node n1 attempts to complete resolution without involving any nodes other than itself and nf. to determine whether resolution can be performed successfully between n1 and nf only  the coordinator attempts to rule out the case that t has become invalid based on local state  the current top-k set t and the adjustment factors 붻  1  and data received from nf. a pair of objects hot 뫍 t  os 뫍 u   t i is said to invalidate the current approximate top-k set
t whenever the condition is not met for
that pair of objects. each violated constraint at nf represents a potential invalidation of the approximate top-k set t that needs to be dealt with. the coordinator considers each pair hot 뫍 t  os 뫍 u   t i whose constraint has been violated and performs the following validation test: vt f + 붻t 1 + 붻t f 뫟 vs f + 붻s 1 + 붻s f. if this test succeeds  both adjustment factor invariants are met  which our algorithm will always guarantee to be the case as discussed later   and all constraints not reported violated during phase 1 of resolution remain satisfied  then it must be true thatand thus the pair hot osi does not invalidate t .
모the coordinator applies the validation test to all pairs of objects involved in violated constraints to attempt to rule out invalidations. if the coordinator is able to rule out all potential invalidations in this way  then it leaves the approximate top-k set unchanged and performs a final procedure called reallocation to assign new adjustment factors corresponding to objects in r and nodes n1 and nf. the adjustment factors must be selected in a way that reestablishes all parameterized arithmetic constraints without violating the adjustment factor invariants. reallocation is described later in section 1. the coordinator then finishes resolution by sending a message to nf notifying it of the new adjustment factors to serve as new parameters for its arithmetic constraints  which remain unchanged.
모if the coordinator is unable to rule out all invalidations by examining the partial data values from nf alone during phase 1  then a third phase of resolution is required. in phase 1  the coordinator contacts all monitor nodes other than nf  and for each node nj 1 뫞 j 뫞 m j =1 f  the coordinator requests the current partial data values vi j of objects oi in the resolution set r as well as the border value bj  defined above . once the coordinator receives responses from all nodes contacted  it computes the current logical data values for all objects in r by summing the partial data values from all monitor nodes  recall that those from nf were obtained in phase 1 . it then sorts the logical values to form a new approximate top-k set t 1  which may be the same as t or different. clearly  this new approximate top-k set t 1 is guaranteed to be valid  assuming the previous top-k set was valid prior to the constraint violations that initiated resolution and no violations have occurred since. next  the coordinator performs reallocation  described in section 1  modifying the adjustment factors of all monitor nodes such that new arithmetic constraints defined in terms of the new top-k set t 1 are all satisfied. finally  the coordinator sends messages to all monitor nodes notifying them of the new approximate top-k set t 1  causing them to create new constraints  and their new adjustment factors. we now consider the communication cost incurred during the resolution process. recall that for our purposes cost is measured as the number of messages exchanged.  as we verify empirically in section 1  messages do not tend to be large.  when only phases 1 and 1 are required  just two messages are exchanged. when all three phases are required  a total of 1 m 1 +m = 1m 1 messages are necessary to perform complete resolution. since in our approach the only communication required is that performed as part of resolution  the overall cost incurred for top-k monitoring during a certain period of time can be measured by summing the cost incurred during each round of resolution performed. we are now ready to describe our adjustment factor reallocation subroutine  a crucial component of our resolution algorithm.
1 adjustment factor reallocation
모reallocation of the adjustment factors is performed once when the initial top-k set is computed  and again during each round of resolution  either in phase 1 or in phase 1. we focus on the subroutine for reallocating adjustment factors during resolution; the process for assigning the new adjustment factors at the outset when the initial top-k set is computed is a straightforward variation thereof. first we describe the generic properties and requirements of any reallocation subroutine  and then introduce our algorithm for performing reallocation.
모before proceeding  we introduce some notation that will simplify presentation. when the reallocation subroutine is invoked during phase 1 of resolution  a new top-k set t 1 has been computed by the coordinator that may or may not differ from the original set t . when reallocation is instead invoked during phase 1  the top-k has not been altered. in all cases we use t 1 to refer to the new top-k set at the end of resolution  which may be the same as t or different. finally  we define the set of participating nodes n: if reallocation is performed during phase 1 of resolution  then n = {n1 nf}  where nf is the monitor node that initiated resolution. if reallocation is performed during phase 1 of resolution  then n = {n1 n1 ... nm}.
모the input to a subroutine for adjustment factor reallocation consists of the new top-k set t 1  the resolution set r  the border values bj from nodes nj 뫍 n  {n1}  the partial data values vi j of objects oi 뫍 r from nodes nj 뫍 n  {n1}  the adjustment factors 붻i j corresponding to those partial data values  and the special adjustment factors 붻  1 corresponding to the coordinator node. the output is a new set of adjustment factors 붻i j1 for objects oi 뫍 r and nodes nj 뫍 n.
모an adjustment factor reallocation subroutine is considered to be correct if and only if:
correctness criterion 1: the new adjustment factors selected by the subroutine satisfy the two invariants stated above.
correctness criterion 1: immediately after resolution and reallocation  all new constraints defined by t 1 with adjustment factor parameters are satisfied  assuming that no partial values v    are updated during resolution.
this definition of correctness ensures the convergence property described in .
1.1 algorithm description
모multiple reallocation algorithms are possible due to the fact mentioned earlier that there is some flexibility in the way the adjustment factors are set while still maintaining the two invariants and guaranteeing all arithmetic constraints. in fact  the choice of adjustment factors represents a primary factor determining the overall communication cost incurred during subsequent rounds of resolution: poor choices of adjustment factors tend to result in short periods between resolution rounds and therefore incur high communication cost to maintain a valid approximate top-k set. the flexibility in adjustment factors is captured in a notion we call object leeway. the leeway 뷂t of an object ot in the top-k set is measure of the overall amount of  slack  in the arithmetic constraints  the numeric gap between the two sides of the inequality  involving partial values vt   from the participating nodes.  for this purpose  it is convenient to think of adjustment factor invariant 1 as constituting a set of additional constraints involving partial data values at the coordinator.  for each object  the leeway is computed in relation to the sum of the border values  which serve as a baseline for the adjustment factor reallocation process.
모informally  our adjustment factor reallocation algorithm is as follows. the first step is to assign adjustment factors in a way that makes all constraints  tight   so that each constraint is exactly met based on equality between the two sides with no slack. next  for each object oi in the resolution set  the total leeway 뷂i is computed and divided into several portions  each corresponding to a node nj 뫍 n and proportional to a numeric allocation parameter fj. the portion fj뷂i of oi's leeway corresponding to node nj is then added to the adjustment factor 붻i j1   thereby increasing the slack in the relevant constraints in the case that oi is a top-k object.  our proof of correctness  presented later  demonstrates that even when leeway must be applied to non-top-k objects in the resolution set  the overall slack in each constraint remains positive. 
모the allocation of object leeway to adjustment factors at different nodes is governed by a set of m + 1 allocation parameters f1 f1 ... fm that are required to satisfy the following restrictions: 1 뫞 fj 뫞 1 for all j; p1뫞j뫞m fj = 1; fj = 1 if fj 뫍 n/ . the allocation parameters have the effect that assigning a relatively large allocation parameter to a particular node nj causes a large fraction of the available slack to be assigned to constraints at nj. in section 1 we propose several heuristics for selecting allocation parameters during resolution and evaluate them empirically. the heuristics are designed to achieve low overall cost by managing the slack in constraints in a way that minimizes the likelihood that some constraint will become violated in the near future. before presenting our heuristics and experiments we specify our reallocation algorithm precisely and illustrate its effect with a brief example. detailed proofs that our algorithm meets correctness criteria 1 and 1 are provided in .
모for notational convenience we introduce the following definitions: first  we extend our notation for partial

figure 1: example of reallocation.
data values and border values to the coordinator node by defining vi 1 = 1 for all i and b1 = max1뫞i뫞n oi뫍u r 붻i 1. we also define  for each object oi  vi n = p1뫞j뫞m 
nj뫍n vi j + 붻i j   which we call oi's participating sum. we use similar notation to refer to the sum of the border values across the set n of nodes participating in resolution: bn = p1뫞j뫞m nj뫍n bj. algorithm 1  reallocate .
inputs: t 1; r; {bj}; {vi j}; {붻i j}; {fj} output: {붻i j1 }
1. for each object in the resolution set oi 뫍 r  compute the leeway 뷂i:

1. for each object in the resolution set oi 뫍 r and each monitor node nj 뫍 n participating in resolution  assign:
		otherwise
1.1 example
모a simple example of reallocation is illustrated in figure 1. the chart on the left shows the adjusted partial data values for three objects o1  o1  and o1  shown as light  medium  and dark bars  respectively  at three monitor nodes  denoted n1  n1  and n1  just before an update to the value of o1  the dark bar  is received by node n1. the adjustment factor 붻1 at the coordinator node n1 is non-zero  while 붻1 and 붻1 are both zero. assume the the approximation parameter is  = 1 and size of the top-k set t is k = 1  so t = {o1}.
모after the partial data value v1 is updated to a new larger value  depicted by the dashed lines in the chart on the left-hand side of figure 1  a local constraint is violated at monitor node n1  triggering phases 1 and 1 of resolution between n1 and the coordinator. the value of the adjustment factor 붻1 is large enough that the coordinator node is able to determine that no actual change in the overall top-k set has occurred  i.e. that it is still the case that v1 뫟 v1  so phase 1 of resolution is unnecessary. however  some of the excess slack represented by adjustment factor 붻1 must be transferred to 붻1 so the parameterized constraint at monitor node n1 requiring v1 + 붻1 뫟 v1 + 붻1 will be satisfied once again. the amount of slack transferred in this manner by our reallocation algorithm depends on the settings of allocation parameters f1 and f1. for illustration we study two extreme cases: the chart on the top right of figure 1 illustrates the result when f1 = 1 and f1 = 1  and the chart on the bottom right shows the result when f1 = 1 and f1 = 1.
모notice that either choice of allocation parameters might be preferable depending on the course of future events. if the next data value change received by the monitoring system is another small increase in v1  then the allocation of slack shown on the bottom right of figure 1 is superior  since it can absorb such an increase without triggering a constraint violation. if  on the other hand  the next value change is a small increase in v1  then then the allocation of slack shown in the top right of the figure is preferable. in this second scenario  retaining some slack at the coordinator node through a non-zero 붻1 value allows the expensive phase 1 of resolution to be avoided during a subsequent round of resolution triggered by a small increase in v1. the tradeoffs enabled by different choices of the allocation parameters are explored in more detail in section 1.
1. experiments
모we constructed a simulator and performed experiments to measure message size  section 1   determine good heuristics for setting the allocation parameters  section 1   and evaluate the performance of our algorithm for various top-k monitoring problems  section 1 . in our experiments we used two data sets and a total of three monitoring queries  described next.
1 data and queries
모the first data set we used in our experiments consists of a portion of the http server logs from the fifa world cup web site described in section 1 . two of the monitoring queries we used were over this data set: monitoring queries 1 and 1 from section 1  which monitor popular web documents and server load  respectively. web document popularity was measured as the total number of page requests received for each web document so far during the current day  and server load was estimated by counting the number of page requests received in the last 1 minutes by each of the 1 servers at one geographic location. for monitoring query 1  we used a 1-hour time slice of the server log data consisting of 1 million page requests distributed across 1 servers that were active during that period and serving some 1 distinct files. for monitoring query 1 we used a 1-minute sliding window over seven million page requests from the same time slice.
모the third monitoring query we used in our experiments was over a second data set consisting of a packet trace capturing one hour's worth of all wide-area network tcp traffic between the lawrence berkeley laboratory and the rest of the world   consisting of 1 packets in all. the trace was actually collected at a single node  but for our purposes we treat it as though it were collected in a distributed fashion at four monitor nodes.
모monitoring query 1. which destination host receives the most tcp packets 
we used a 1-minute sliding window over packet counts for this query.
모our goal in selecting monitoring queries for our experiments was to choose ones that are both realistic and diverse. our queries cover two distinct data sets with differing data characteristics and numbers of sources. also  monitoring queries 1 and 1 each apply sliding windows while monitoring query 1 does not. finally  monitoring query 1 represents an example of the following important special case: values are not split across monitor nodes; instead each monitor node has exactly one non-zero partial data value that is actually a full data value  i.e. vi j = vi for i = j and vi j = 1 for i =1 j . for monitoring query 1 we used k = 1  i.e. we monitored the single least loaded server   and for the other two queries we used k = 1.
1 message size
모our first experiment measures the size of messages exchanged between the monitor nodes and the coordinator during resolution. message size is dominated by the number of partial data values or adjustment factors transmitted  so the maximum message size is governed by the size of the resolution set. based on our simulations we found that large resolution sets appear to be rare. to avoid the occasional occurrence of large resolution sets we modified our algorithm to use an alternative resolution procedure  described in   whenever many constraints become violated at the same time. we now report on the message sizes measured for each query when we use our message size reduction procedure.
모first  the largest message exchanged in our experiments on monitoring queries 1 and 1 contained only k + 1 entries. larger messages did occasionally occur for monitoring query 1. however  the average number of entries per message remained small  ranging from k + 1 to k + 1  depending on the choice of parameters used when running the algorithm. no message contained more than k + 1 numeric entries  and fewer than 1% of messages contained more than k+1 entries. therefore we conclude that overall  using our procedure for reducing message sizes  messages tend to be small and our cost metric that counts the number of messages exchanged is appropriate.
1 leeway allocation policies
모recall from section 1.1 that our reallocation algorithm is parameterized by m + 1 allocation parameters f1 ...fm that specify the fraction of leeway allocated to the adjustment factors at each node nj participating in resolution. we considered several heuristics for selecting allocation parameters  all of which use the general policy of first setting f1  the allocation parameter

coordinator allocation parameter  f1 
monitoring query 1

coordinator allocation parameter  f1 
monitoring query 1

coordinator allocation parameter  f1 
figure 1: effect of allocation parameter policies.
pertaining to the coordinator  and then distributing the remaining fraction 1   f1 among the allocation parameters f1 ...fm of the monitor nodes. we investigated two aspects of this general policy:
  what value should be assigned to f1    how should the remaining 1 f1 be divided among f1 ...fm 
모as illustrated in section 1.1  a large value for f1 increases the likelihood that resolution can be limited to two phases because during phase 1 the coordinator attempts to avoid a third phase by applying all available leeway from its local adjustment factors to the adjustment factors of the monitor node whose constraint has been violated. since phases 1 and 1 require few messages compared with phase 1  the number of messages exchanged on average during each round of resolution will be small. on the other hand  a large f1 value implies small f1 ...fm values  so the amount of leeway allocated to the adjustment factors at the monitor nodes during reallocation each time resolution is performed will be small. therefore  the parameterized constraints will have little slack  and as a result the period of time between constraint violations will tend to be short  leading to frequent rounds of resolution. therefore  the choice of f1 offers a tradeoff between frequent constraint violations with  on average  cheap resolution  large f1  and infrequent constraint violations with  on average  more expensive resolution  small f1 .
모we studied the performance of our approach under a broad range of settings for f1  and we tried two alternative heuristics for determining how the remaining 1 f1 should be divided among f1 ...fm. the first heuristic  called even allocation  divides the remaining leeway evenly among monitor nodes participating in resolution  i.e. nodes in n  {n1} . even allocation is appropriate if no effective method of predicting future data update patterns is available. our second heuristic  proportional allocation  sets f1 ...fm adaptively  assigning each fj in proportion to data update rates observed at node fj  this information can be transmitted to the coordinator inside resolution request messages . the idea behind proportional allocation is to provide more leeway to nodes that experience a higher volume of data updates in the hope of reducing the frequency of constraint violations and resolution requests.
모figure 1 shows the results of our experiments on monitoring queries 1 through 1  with the value assigned to f1 on the x-axis and the two allocation heuristics for setting f1 ...fm plotted using different curves. each graph shows measurements for two different error tolerance values:  = 1  and a larger  value that permits a moderate amount of error with respect to the data queried. the y-axis shows total communication cost.  we also studied cost for other  values not plotted to
improve readability. 
모in all cases  the value assigned to f1 turns out to be the largest factor in determining cost. our results suggest that when  is relatively small  a good balance is achieved in the tradeoff between frequent  cheap resolution  large f1  and less frequent yet more expensive resolution  small f1  using a moderate value of f1 = 1. on the other hand  when  is large  the most important factor turns out to be reducing the overall frequency of resolution  so the best results are achieved by setting f1 = 1. we therefore propose a simple heuristic for selecting f1: set f1 = 1 when  is small and f1 = 1 when  is large. the cutoff point between  small  and  large  values of  comes roughly when 1 of the largest value in the data set.  this conclusion is supported by additional experimental results not presented here for brevity.  an estimate of the largest data value to use for this purpose can be obtained easily once top-k monitoring is underway.
모when a good setting for f1 is used  the choice of heuristic for setting the remaining allocation parameters f1 ...fm does not appear have a significant impact on communication cost compared with the impact of f1. however  the proportional allocation method performed somewhat better for small values of   while the even method performed better for large values of . therefore  we employ proportional allocation for small  and



figure 1: comparison against caching strategy.
even allocation for large   and use the same cutoff point between  small  and  large   values used for setting f1.
1 comparison against alternative
모to the best of our knowledge there is no previous work addressing distributed top-k monitoring  and previous work on one-time top-k queries is not appropriate in our setting as discussed in section 1. therefore  we compared our approach against the following obvious approach: the coordinator maintains a cached copy of each partial data value. monitor nodes are required to refresh the cache by sending a message to the coordinator whenever the local  master  copy deviates from the remotely cached copy by an amount exceeding   where  is the overall error tolerance and m is the number of monitor nodes. modulo communication delays  the coordinator is at all times able to supply an approximate top-k set that is valid within  by maintaining a sorted list of approximate logical data values estimated from cached partial values. when  = 1  this approach amounts to transmitting the entire data update stream from each monitor node to the coordinator node. from this point on we refer to this approach as  caching. 
모we compared our algorithm against caching using a
모simulator that for both algorithms assumes that communication and computation latencies are small compared with the rate at which data values change. for our algorithm  we used the heuristics developed in section 1 for setting f1 and for choosing between even and proportional allocation for f1 ...fm.
모figure 1 shows the results for monitoring queries 1 through 1. in each graph the approximation parameter  is plotted on the x-axis  on a nonuniform  discretized scale . the y-axis shows total cost  on a logarithmic scale. in general  communication cost decreases as  increases  with one minor exception: for monitoring query 1 the communication cost of our algorithm for  = 1 is unexpectedly slightly higher than the cost for  = 1. the reason for this anomaly is that  while there were fewer resolution messages when  = 1 than when  = 1  for this particular monitoring query and data set it happened that the expensive third phase of resolution was necessary a higher percentage of the time
when  = 1 versus when
모in all cases our algorithm achieves a significant reduction in cost compared with caching  often by over an order of magnitude. the benefit of our algorithm over caching is most pronounced for small to medium values of the approximation parameter . as  grows large  the cost of both algorithms becomes small.
1. summary
모we studied the problem of online top-k monitoring over distributed nodes with a user-specified error tolerance. in particular  we focused on the case in which the values of interest for monitoring are virtual and are only materialized indirectly in the form of partial values spread across multiple  monitor  nodes  where updates take place. a naive method for continually reporting the top k logical values is to replicate all partial values at a central coordinator node where they can be combined and then compared  and refresh the replicas as needed to meet the error tolerance supplied by the user. we presented an alternative approach that requires very little communication among nodes  summarized as follows:
  the coordinator computes an initial top-k set by querying the monitor nodes  and then installs arithmetic constraints at the monitor nodes that ensure the continuing accuracy of the initial top-k set to within the user-supplied error tolerance.   when a constraint at a monitor node becomes violated  the node notifies the coordinator. upon receiving notification of a constraint violation  the coordinator determines whether the top-k set is still accurate  selects a new one if necessary  and then modifies the constraints as needed at a subset of the monitor nodes. no further action is taken until another constraint violation occurs.
through extensive simulation on two real-world data sets  we demonstrated that our approach achieves about an order of magnitude lower cost than the naive approach using the same error tolerance.
모our work is motivated by online data stream analysis applications that focus on anomalous behavior and need only perform detailed analysis over small portions of streams  identified through exceptionally large data values or item frequencies. using efficient top-k monitoring techniques  the scope of detailed analysis can be limited to just the relevant data  thereby achieving a significant reduction in the overall cost to monitor anomalous behavior.
acknowledgments
we thank our ph.d. advisors  rajeev motwani and jennifer widom  for their support and guidance. this work was funded by a rambus corporation stanford graduate fellowship  babcock  and a chambers stanford graduate fellowship  olston .
