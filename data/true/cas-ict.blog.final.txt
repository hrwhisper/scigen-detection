this paper describes our participation in blog opinion retrieval task this year. we conduct experiments on  firtex  platform that is developed by our lab. language model is used to retrieve related blog unit. interactive knowledge is adopted to expand query for retrieve blog unit include opinion. then we introduce a novel extracting technology to extract text from retrieved blog-post. 
finally  lexicon based method is used to rerank the document by opinion. 
 
1 introduction 
모since opinion retrieval is a new retrieve task emerging this year  our aim to blog opinion retrieval task is to construct an opinion retrieval framework using blog data as an important test bed. compared with traditional retrieval task  opinion retrieval is more challenged since it is very difficult to judge opinion about some topic. 
모after filtering spam permalink data  we obtain about 1g permalink data. statistics language model  is used to retrieve related blog unit. interactive knowledge is adopted to expand query for retrieve blog unit including opinion. then we introduce a novel extracting technology to extract text from the retrieved result. in order to rank the documents with stronger opinion in topper position  lexicon  based method is used to rerank the documents. 
 
1 retrieve model 
모in our system  we used the basic language modeling approach. the language modeling approach to ir is attractive and promising because it connects the problem of retrieval with that of language model estimation. the basic idea behind it can be described as follows. 
모suppose we have a collection with total n documents  whose vocabulary is v = {w1  w1  ...  wl}. given a query q = q1...qm  qk뫍v  1뫞k뫞m   the task is to rank the documents according to the relevance of each document with the query. based on the assumption of words independence  the language model approach views each document d as an observed sequence of words generated by a multinomial language model parameterized by 붿d =  붿d1  붿d1  ...  붿dl 뫍 1 l  where 붿d1 + 붿d1 + ... + 붿dl=1 and 붿di = p wi|붿d   the probability of observing word wi under the multinomial generation model  i.e. unigram model  parameterized by 붿d. and the relevance of document d with the query q is measured by the likelihood of q according to the multinomial model 붿d: 
m
p q | 붿d   =뫊p qi | 붿d   =뫊p w| 붿d  c w q                         1  
	i=1	w뫍q
where c w q  is the frequency of word w occurring in query q. 
모as shown in equation  1   the retrieval problem is now essentially reduced to a multinomial language model estimation problem. the simplest method to estimate 붿d is to utilize the maximum likelihood estimator  mle : 
p w | 붿  d   = pml  w | 붿  d   = c w  d                        1  
|d|
where c w d  is the frequency of word w occurring in document d and |d| is the length of d  i.e. 
|d|=뫉wc w d . 
모however  using mle can cause serious zero probability problem due to data sparseness. a direct solution to this problem is smoothing  which adjusts the mle so as to assign a nonzero probability to the unseen words and improve the accuracy of language model estimation. according to   there are three popular smoothing methods applied to the language modeling approaches to ad hoc ir: jelinek-mercer  dirichlet and absolute discounting  summarized in table 1. notice that |d|u is the number of unique words in document d. the collection language model 붿c is typically estimated by mle based on the whole collection  i.e. 
	p w | 붿 c   = pml  w | 붿 c   = 뫉dc w d 	                      1  
	뫉뫉w	dc w d 
 
table 1. summary of the three popular smoothing methods 
jelinek-mercer p w| 붿  d   =뷂pml  w| 붿  d   +  1 뷂  p w| 붿  c   dirichlet p w | 붿  d   = c w  d  +뷃p w | 붿  c   |d|+뷃absolute discounting p w|붿  d  = max c w d   붻  1  +붻|d|u p w|붿 c   
	|d|	|d| 
모in our experiments  we utilized the dirichlet smoothing method for the document language model estimation  where the smoothing parameter u is set to 1. 
 
1 query expansion using interactive knowledge 
모query expansion is an effect method to bridge the semantic gap between the vocabulary of documents and that of users. there are three kinds of traditional query expansion approaches: lexicon based query expansion  global-analysis based query expansion   local-analysis based query expansion . lexicon based query expansion uses lexicon  such as wordnet  to select terms semantically related to query terms. global-analysis based query expansion selects terms co-occurring in the corpus with query terms  while local-analysis based query expansion selects terms co-occurring in the top-n documents with query terms.  
모query expansion can be performed either manually or automatically. during opinion retrieval task  we are concerned with semi-automatic query expansion. for each query  traditional query expansion often selects expansion term by co-occurrence statistics. we first classify each query into different categories. for example  the query  march of the penguins  belongs to film category. and then select terms that could describe the firm from inquirerbasic lexicon . after obtaining the first initiated result  we get the adj terms around the query from top-1 documents and then manually select some terms much more suitable to describe the film. finally  we use these terms to expand the query and retrieve the top 1 documents. 
1 extracting post and comment based on feature statistics 
모blog extraction is a kind of difficult job because each blog site has a lot of different blog templates which can be chosen by blogers. each template has different styles which greatly enhance the difficulty of extraction. under this condition  we cannot easily build one extracting method for any blog template. in order to address this issue  we need to build up a common infrastructure which grasps the essential factors of blog pages. with the aim to achieve this goal  we combine the similarity of html tag tree with the similarity of content to find the most suitable extracting range. 
모based on above idea  we propose a novel extraction algorithm that can effectively extract the main texts and comments from blog pages. it is described as figure 1. 
 
extracting algorithm: 
1. locate the range of main text. 
1 build a html tag tree. 
1 for each html tag  calculate the number of word in it and itsposition in vision. 
1 choose a tag which has more words and its position is wider thanother sibling tags in html tag tree. 
1 use the range of chosen html tag as the main text range. 
1. locate the title of main text. 
1 for each line in range  find the lines which are highlighted or havebigger font size. 
1 use the words in   title   as a clue to decide which selected line is
title. 
1. locate the start of comments. 
1 find those lines which have some key words of start of comment like  comment    reply . 
1 choose the line which has more few words and has bigger fontsize. 
1. calculate the number of comments. 
1 from the start of comments  find each line which has timeexpression. 
1 compute the difference of those selected lines and choose themore similar lines as a symbol of comment. 
1 compute the start and end of each comment through the html tagtree.  
figure1: the outline of extracting algorithm 
 
1 lexicon based opinion ranking 
모the sentiment lexicon we used for identifying positive and negative terms is taken from the general inquirer   gi for simplicity   which is a dictionary that contains information about english word senses. 
모gi is a system which lists terms as well as different senses for the terms. for each sense it provides a short definition as well as other information about the term. this includes tags that label the term as being positive  negative  a negation term  an overstatement  or an understatement. for example  there are two senses of the word  fun . one sense is a noun or adjective for enjoyment or enjoyable. the second sense is a verb that means to ridicule or tease  to make fun of. the first sense of the word is positive  marked as  positiv   while the second is negative  marked as  negativ . there are other labels for each sense.  
모one difficulty in using this lexicon is that english word always contains many inflections. for example  the word  take  may has four inflections:  takes    taking    took    taken . in order to address this problem we employed the porter stemming algorithm1 . this algorithm does not produce a lemma for a term  but rather maps similar words to a string. after removing word suffix  we obtain 1 negative words and 1 positive words. 
모the method we used to classify a blog-post is very simple and straightforward  that is  only to count positive and negative terms in it. if the blog-post contains more positive than negative terms we deem it as positive  and if the number of negative terms is greater than the number of positive terms we assign it as negative. if a blog-post contains an equal number of positive or negative terms  we say it is neutral. 
 
