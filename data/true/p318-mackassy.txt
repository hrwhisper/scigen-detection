in many applications  large volumes of time-sensitive textual information require triage: rapid  approximate prioritization for subsequent action. in this paper  we explore the use of prospective indications of the importance of a time-sensitive document  for the purpose of producing better document filtering or ranking. by prospective  we mean importance that could be assessed by actions that occur in the future. for example  a news story may be assessed  retrospectively  as being important  based on events that occurred after the story appeared  such as a stock price plummeting or the issuance of many follow-up stories. if a system could anticipate  prospectively  such occurrences  it could provide a timely indication of importance. clearly  perfect prescience is impossible. however  sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently. we describe a process for creating and evaluating approximate information-triage procedures that are based on prospective indications. unlike many informationretrieval applications for which document labeling is a laborious  manual process  for many prospective criteria it is possible to build very large  labeled  training corpora automatically. such corpora can be used to train text classification procedures that will predict the  prospective  importance of each document. this paper illustrates the process with two case studies  demonstrating the ability to predict whether a news story will be followed by many  very similar news stories  and also whether the stock price of one or more companies associated with a news story will move significantly following the appearance of that story. we conclude by discussing how the comprehensibility of the learned classifiers can be critical to success.
1. introduction
　professionals receive increasing amounts of information  some of which is time sensitive and is important for them to consider. business news provides an interesting illustration: the job performance of financial analysts  attorneys 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  september 1  1  new orleans  louisiana  usa.
copyright 1 acm 1-1/1 ...$1.
business-school professors  market makers  portfolio managers  reporters  and many others would benefit from timely attention to certain business news stories. bloomberg  reuters  bridge  and several other companies have profited greatly selling a variety of instant-access  business information services. however  the volume of business news is so large that few professionals can pay attention to it all  let alone do so in a timely fashion.
　information triage is the monitoring of one or more information sources to provide users with well-filtered  prioritized  and/or categorized information  cf.   . our general information-triage framework consists of monitoring a potentially wide range of online information sources- such as news stories  stock data  weather reports  and other computer-based information feeds-and evaluating each item to assess its importance to a given user. although information triage does not require multiple sources of information  we explicitly embrace such situations when they can create synergy and improve the information-triage process.
　one of the key difficulties in building information-triage procedures is building models of importance that will be used to prioritize information. ideally we would like to obtain from a user a direct statement of his or her interests. however  in many cases it is not clear that users can do so effectively. instead information filtering and ranking procedures often rely on indirect statements of interest  such as user-provided  keyword-based profiles  1  1   or samples of information items whose importance has been assessed by the user via relevance feedback methods  1  1  1 . we believe that such methods are crucial components of an effective information triage procedure  but we believe that there are other useful components as well. in this paper we concentrate on prospective indicators.
　often what makes information important is some subsequent occurrence that is directly or indirectly associated with the information. for example  consider the appearance of a news story about a publicly traded company  after which the company's stock value quickly plummets. the importance of the news story is based not solely on the story itself but also on the occurrence of the future event  observed in a separate information feed  in this case stock-market data . in many cases a user will be able to specify what future events would make an information item important-such as a substantial change in the value of a company.
　a problem is that this importance criterion can not be evaluated directly at the time the information appears; its
1
evaluation requires knowledge about the future. however  if there are patterns in the stories-for example  if many stories that are coupled with precipitous drops in a stock price have similar structure or content-we might be able to predict  approximately  that a story will be followed by an important event. as we will show  even an approximate prediction can be quite useful for information triage.
　we propose to have a user specify what would make an article important if  in fact  we could perceive the future behavior of this or other relevant information feeds. we then operationalize  this importance criterion to be evaluable on a given story in the given information feed before we see the future. key to our approach is the application of the user's specification to label historical documents. these data then form a training corpus  to which inductive algorithms will be applied to build a text classifier. although we believe this framework to be complementary to learning from labels elicited via relevance feedback  or other manually created labels   it has the advantage that the labeling can be done automatically  and at a very large scale.
　this paper describes a four-step process for creating and evaluating such operationalized approximations to a user's non-operational specification.
1. elicit from the user and encode a specification of whatfuture events would make a current piece of information interesting-for example  a news story would be interesting if  within the hour of the story being published  there is a significant/unusual move in the price of the stock of any company associated with the news story.
1. use this specification to analyze information feeds received in the past to ascertain whether or not each item was interesting  thereby creating a set of data items labeled by whether or not each was interesting.
1. apply inductive algorithms to these labeled data toform models that can estimate the extent to which an information item is interesting to a user directly from the item itself  without the need to look into the future.
1. analyze the learned model to assess both whether itappears to be a plausible operationalization of the original criterion and whether it is something that appears trustworthy. if the  native  learned form is not easily interpretable  as is the case in the two studies contained herein  then this may first require applying techniques for obtaining an understandable form of the operationalized criterion.
　after providing further details of this process  the paper focuses on two case studies involving two available information feeds  news stories and stock price data. in the first case study we deem a news story important if there are a significant number of subsequent stories that appear similar to it. in the second  a news story is deemed important if in the hour following its appearance the stock-market return of a company associated with the story is more than one standard deviation from its normal hourly return.
1. learning operational information filters
　we now describe in more detail the four-step process for performing one form of information triage: when an item is deemed interesting because something important subsequently happens-something that can be measured objectively  retrospectively  either in the given information feed or some coupled information feed. this process reifies and extends the process used previously by fawcett and provost  and by lavrenko  et al.   where text documents  news stories  were labeled by referencing subsequent stockmarket events.
1 specifying a non-operational criterion
　our first step is to acquire and encode the specification of how an item may be interesting based on possible events that may be observed subsequently. in general this can be a complex process. the non-operational criterion is nonoperational only with respect to a world where no knowledge of the future is available. however  it needs to be fully operational when it does have access to the future  as is the case when it is being used to label data from the past. thus  for example  saying that an article is interesting if it is followed by a substantial movement in a stock lacks important detail. if movement is defined in terms of what is typical  it is necessary to quantify what is typical  as well as to specify how far a value must be from typical. the first step of our process requires that the criterion be stated in unambiguous detail  so it can be applied directly to data.
1 generate and label data
　the specification of a non-operational criterion will generally presume that a particular primary information feed is the focus of the criterion  and that the criterion will look into the future of either this feed or other coupled feeds to assess the interestingness of items obtained on the primary feed. we thus must access data from these historical feeds. once available  we can use the non-operational criterion to label elements of this feed for use in the next step of our process. in some cases the criterion will focus solely on the future of the primary feed-such as if a story is interesting because a large number of follow-up stories are subsequently observed-or it may require access to one or more secondary feeds  such as stock price data.
　once the data have been obtained and transformed into suitable form  they can be labeled using the interestingness criterion. this is performed in a straight-forward fashion. for each information item in the data generated for the primary data feed  pretend that it has just appeared. the items that follow it chronologically represent the future that is about to follow the given item. given access to the item  as well as the other information items that followed it  the item's  future    it becomes possible to use the user's importance criterion to assess this item. the result is a corpus of information items from the past  each labeled by whether it is deemed important according to the user's criterion.
1 applying machine learning
　once the data have been labeled  it is now possible to apply machine learning algorithms to them. note that all knowledge of the future is embodied in the label associated with each item. the learned model therefore can examine the item-with no information about the future-and can make a prediction about what the non-operational criterion will yield on that item. in other words  the learned model is an operationalized  albeit perhaps approximate  form of
1
the importance criterion that can be used directly on items obtained from the information feed.
　the selection of a learning method depends heavily on the nature of the information feeds. if each item is a collection of numerical values  i.e.  attribute/value data   learning methods suitable for such data would be used. in many cases-including those considered in the remainder of this paper-each information item is a text object  and thus text classification methods can be used to form the operationalized importance criterion. the accuracy of any such learning method will be affected by the extent to which the contents of each information item provide clues to what the nonoperational criterion may predict. without at least some correlation of this sort  the operationalization process should perform no better than random prediction. an assessment of the extent to which such correlations exist will usually take place at this stage.
　this assessment is affected by the fact that the data are temporal in nature. in particular  any estimates of the expected predictive accuracy of a learned model must be made on data that appeared later in time than the training data. cross-validation methods are thus not appropriate for use in this context-evaluation must instead guarantee that all test examples appeared chronologically later than all training examples.
1 analysis
　learning a model is only one part of the overall goal of using this framework. also important is an analysis of the learned model  to gain insight into what actually was learned. this is important for two reasons. first and foremost  an analysis can be used to evaluate whether the learned model actually has learned the criterion and does not reflect less-meaningful artifacts present in the data. second  it can be used to gain insight into how the criterion works and can be used to explain what is happening in the model. the final step of our framework thus consists of analyzing the learned model  both with respect to how well it appears to match up with our intuitions about what the non-operational criterion was encoding  as well as simply with respect to whether it appears sufficiently plausible that the user would be willing to place some trust in it.
　performing such an analysis will depend substantially on the form of the learned model. a number of researchers have used machine-learning methods to extract interpretable models from difficult-to-understand models  such as complicated expert systems   neural networks   and ensemble classifiers . we also must do so similarly in our case stud-
ies here. we use our learned difficult-to-understand models to relabel the given data  thereby forming a corpus that reflects the performance of the learned model  rather than reflecting the original labels . we then use the ripper rulelearning system  1  1  to learn  from these relabeled data  a  more  interpretable approximation  explanation  of the original learned model. as we will see  even these approximate models have limited interpretability for our domain experts  leading us to a further stage of analysis.
1. case study i: hot story detection
　for our first case study we focus on news stories from a set of business wires. our goal is to recognize stories that are  hot   in the sense that more similar stories follow them than is typically the case. although similar to the  on-line new event prediction   1  1  or  first story detection   problem within the topic detection and tracking  tdt  initiative  1  1  1  1  1   the problem we are addressing differs in two important ways. first  we do not require a story to be the very first on a topic  but rather that there are more than a normal amount of subsequent stories that are more similar to the story than is typical. second  it demonstrates a different approach to these sorts of problems than is typically taken in such work. rather than requiring a human to manually label stories according to one of a
　fixed number of known topics  we instead elicit and encode a non-operational criterion that can then be used to label arbitrary amounts of past data.
　for this case study  we label a story as interesting by focusing on all stories that appeared within the subsequent 1-hour period and that was associated with any of the companies also associated with the current story.1 if this collection of stories has an average distance that differs substantially from a story's normal distance to subsequent stories  the initial story is deemed interesting. similarity is based on the cosine of the tfidf-vectors of the different news stories . the next section discusses this in further detail.
1 specifying the non-operational criterion
　consider the set of news stories associated with a given stock symbol α. listed chronologically we get a series of stories sα = {s1 s1 ...  si ...}. we can step through each story si in sα and assess the extent to which the stories si+j in sα for j = 1 ...  ti are similar to si  where ti is the number of stories in sα that follow si within 1 hours. if we use sim si sj  to designate the similarity of two stories using the cosine of the tfidf vectors of the two stories   we can identify this raw total similarity of story si  tsim si  by adding the similarity of story si to all stories that follow it:
ti
	tsim si  =	sim si si+j 	 1 
j=1
this raw measure is unduly influenced by the number of α stories occurring in the following 1-hour period. in order to address this  we compute the average per-story similarity value for si  sim si   by dividing tsim si  by ti  the number of α articles following si within 1 hours:
tsim si 
sim si  = 	 1  ti
　to determine whether this quantity is sufficiently far from normal we first need to define normal. we do this by computing the mean simα and standard deviation ρα across all stories si in sα. we then compute t  the average number of stories ti that follow a story in a 1-hour period  to take into account the number of similar stories  to make sure we don't give unnaturally high scores to stories with only a few nearby  albeit high-scoring stories . we then assign a score

1
while the 1-hour time frame was chosen somewhat arbi-
trarily  its arbitrariness is exactly in keeping with the spirit of our approach  in which the user provides an intuitively plausible criterion that depends on the future and it is up to the system to do its best with what it's given.
1

figure 1: dist scores for news stories
score si  to each si as
tsim si 
score si  = 	 1  t
finally  a story is labeled by how many standard deviations  dist  it falls from score si :
	dist 	 1 
ρα
since multiple companies can be associated with a story si  we label si using the maximum of all dist scores for all its associated stock symbols. the values observed in our data range from  1 to 1. figure 1 shows a scatter-plot of the log of the absolute value of these  where the scores are listed left-to-right in chronological order. final labels were assigned by labeling each si as  important  if its dist score was greater than 1
1 generating and labeling data
　for this case study  we consider news stories from a set of public newswires  including business wire  canada newswire  ccn disclosure  internet wire  pr newswire  primezone  and reuters . each story averages roughly 1- 1 words  and each has been analyzed to extract its complete date stamp as well as the stock symbols of all the companies associated with that news story. for the purpose of our experiments  we used a more manageable-sized subset of these data  limited to 1 stories that appeared between january 1  1 and september 1  1  where stories with incomplete time stamps  both date and time of day   duplicate stories  and stories associated with more than eight companies  typical for stories that discuss the market in general rather than a particular stock or segment  were removed. we then applied the non-operational criterion specified in the previous subsection to these data. using 1 as the cutoff for the density resulted in 1 of the stories
being labeled as important.
1 applying machine learning
　given data labeled with our non-operational criterion  we can then proceed to the learning step. to evaluate how well

1
 1 was chosen in a fairly ad hoc fashion  by finding a split that limited the number of positively labeled examples. learning performs we run our learning methods on a per-day basis. for each day we use as training data all stories that appeared before it  skipping all data that appeared earlier in the same day or in the immediately preceding day.  the reason for imposing a gap was to minimize the risk that learning will perform well due to occasional duplicate stories-stories with different headers but identical bodies  something that rarely happens if stories are more than a day apart. 1
　the criterion we use labels only a small number of stories as important. if testing begins too early in the historical data feed  there is the chance that there may be few or no relevant examples of the minority class to learn from. in order for the learner to have sufficient training data  our evaluations thus begin at the chronological date where at least half of the  important  stories will be in the training set. this left 1 stories serving as test data  with 1 of them being labeled  important .
　to evaluate the ability of a learning method to form the approximate operationalization of the importance criterion we present our results using roc curves. roc analysis is an evaluation technique used in signal detection theory  which has seen increasing use for other types of diagnostic  machine-learning  and information-retrieval systems  1  1  1 . roc graphs plot false-positive rates on the x-axis  and true-positive rates on the y-axis. roc curves are generated in a similar fashion to precision/recall curves  by varying a threshold across the output range of a scoring model  and observing the corresponding classification performances . although roc curves are isomorphic to precision/recall curves  they have the added benefits that they are insensitive to changes in marginal class distribution  and that the area under the roc curve has a well-defined statistical meaning .1
　although we used a range of standard text categorization algorithms  all performed roughly comparably. due to their relatively quick run times we therefore only report on results using the naive bayes  and tfidf  classification methods.1 naive bayes estimates the a posteriori probability that an example belongs to a class given the observed feature values of the example  assuming the independence of the features given the class label. the tfidf method  1  1  1  is based on rocchio's  relevance feedback algorithm. a prototype vector is formed for each class from the examples of that class. a new document can then be compared to each prototype by computing the cosine of the prototype vector with the new document vector  and final scores for each class can be assigned by normalizing the cosine-distance values.
　figure 1 shows the resulting roc curve for the two methods as well as the method that selects at random between the choices. it shows that  regardless of what ultimately is the appropriate trade-off between false positives and false negatives  it appears that there is sufficient information solely in the news stories themselves to be able to be able to do substantially better than random. whether this prediction

1
 we also performed experiments without imposing the oneday gap  and observed little effect on the performance of the learned model. 1
 det curves  1  1  are used in a similar fashion in the tdt initiative  1  1  1  1  1  1  1  1  1   and are isomorphic to roc curves  differing primarily on rescaling the axes.
1 we used the versions of these learners found in the publicly available rainbow package .
1

figure 1: roc curve for hot story detection
is good enough depends  of course  on how it will be used. different users have different spans of attention and different needs. the roc curves show that if the stories were to be ranked solely by this single estimation of importance  which more generally would be a component of a greater definition of interestingness   the top of the ranking would be substantially denser with important stories than would the bottom of the ranking. any user restricted to examining only a subset of the stories would examine considerably more important stories.
　to be specific  each day there are on average about 1 stories  using our corpus  which contains a subset of all the business news   and about 1 of them will be important by the current definition. without ranking  if a user selected  randomly  1 stories  1 would be important. with ranking  using tfidf   if a user selected the top 1 stories  1 would be important: an increase in precision of 1%. of course  the relative increase is smaller as one works down the list  but remains impressive. without ranking  if a user selected  randomly  1 stories  1 would be important. with ranking  to get 1 important stories the user would only need to inspect the 1 top-rated stories. conversely  if the user selected the top 1 ranked stories  1 would be important; more than 1% improvement in precision.
　of course  some users may have to read all the news stories  often subject to other filtering criteria . it seems initially that such users would not benefit from such triage. however  this conclusion ignores the issue of timeliness. at any point  a user will have a queue of news stories pending examination. a triage system would maintain a priority queue of news stories  and even users who eventually must read all the stories may benefit in terms of timeliness of information: important stories are more likely to be inserted higher in the priority queue.
1 analysis
　as mentioned previously  it is also important to understand the result of the learning process. the original criterion is specified with respect to future  as yet unseen  information  but its learned form only refers to information present in the given information item. it is important for a user to have confidence that the operationalized criterion matches-even if only in part-the intentions of the original non-operational criterion.
if the learning methods generated interpretable results 

statements net reuters loss
statements press ended current form future risks quarter statements ended research pm statements ended pm research statements release act announces differ statements press act uncertainties contact include statements ended pm receivable statements press ended current form taxes ended reuters statements ended risk commission statements ended loss nasdaq statements release act announces statements ended press equipment statements release announces form statements release act research statements release contact made process statements press act uncertainties

table 1: ripper rules for hot story detection
it may be possible to inspect the results directly to understand what aspects of an information item are correlated with the non-operationalized criterion. however  there is no guarantee that interpretable methods will actually be used in practice  for example if the learning method that yields interpretable results runs slowly or is less accurate. our experiments represent such a case  where we use relatively fast methods that combine scores on words in a holistic fashion  making it difficult to interpret how they behave.  although we did try to provide explanations of our results by extracting words with high information gain  our domain expert did not feel that it gave him any insight into the results. 
　to understand the results of the operationalization process better we approximate the learned classifier using a learning method whose output is more understandable. we step through a collection of data on a day-by-day basis  as was described in the previous section. each day's data are labeled by the results of learning from the earlier days' data. as a result  on a day-by-day basis  we have the  compiled wisdom  of the learned model  as seen in how it labels the data to which it is applied. these labeled data can then be used as input to a learner that will give more interpretable results.
　to demonstrate this approach to analysis we used it to understand the results of the tfidf classifier. this was done using five steps:
1. for each day we learned a classifier to label that day'sdata using earlier data.
1. we extract the top 1 stemmed words from all thedata using standard entropy-based measures.
1. for each news story  we remove words whose stem isnot present in the top 1 words.
1. the resulting labeled data were then given to ripper 1  1   a learning system that forms rules  a representation that is perceived by many as being more understandable. it was run with varying loss-ratios  ranging from 1 through 1 in increments of 1 in order to get a broad spectrum of rule sets.
1. in order to get rules that represent the learned modelwell  a pruning step was then applied. a rule was pruned if it had low total coverage  i.e.  when applied
1
in isolation   a low percentage of true positives to its total coverage  or if it was generated by few ripper runs. in this study  a rule was pruned if it had a coverage of less than 1 stories  had a tp ratio of less than 1%  or if it appeared in fewer than 1 ripper runs.
　table 1 shows the 1 rules that were produced by our analysis of the data labeled by the original learned model. ripper learns patterns that common sense suggests could be correlated with the appearance of new stories. more remarkably  however  we find that many of the words found in these rules can be found in the disclaimer at the end of a story or press release-words such as  statements    information    contact    release    act   and  differ  seem to commonly occur in disclaimers found in the stories matched by these rules. our ongoing work continues this analysis  to go beyond the qualitative results we've thus far obtained to be able to quantify the extent to which the presence of disclaimers in stories provides a helpful clue to the importance of hot stories.
1. case study ii: stock movement
　in our second case study we consider a problem that correlates news stories with stock price movement. we began with a problem that has been studied by others  1  1   labeling a story as interesting if the stock price of any company associated with this news story changes in a way prespecified as being interesting. rather than inheriting from prior work a definition of an interesting change  to evaluate our four-step approach we  started from scratch   going to an expert on financial information systems to obtain his proposed non-operational criterion for this concept.
1 specifying the non-operational criterion
　this case study goes beyond the first case study in one important respect  using a secondary information feed as the basis for assessing the interestingness of a news story. unlike the previous case  the non-operational criterion can be stated more crisply. for each company's stock  we compute the mean and standard deviation of its one-hour return  relative change in price . we then label a story as  important  if the return of any stock associated with the story in the hour after the story appeared was more than one standard deviation from the norm. note that this means that stories whose stock dropped as well as stories whose stock rose were included as being  important. 
1 generating and labeling data
　for this case study we continue with the news-story source used in the first case study  but add a stock price news feed. for our experiments we use the same 1 used in the first case study. we further take only those that appeared during normal trading hours  excluding those appearing in the final hour of trading   as well as stories that were only associated with one company  leaving 1 stories. thus we remove data that are guaranteed to have no change in stock-price values since normal trading has ceased  as well as stories that have a higher probability of being important solely on the fact that they are associated with multiple companies.1

1
 we do not consider the many important news stories that appear  after the bell   focusing here only on stories for which we have trading data.

figure 1: roc curve for stock movement
　the second source of information is trade-level data for over 1 publically traded companies on the nyse  amex and nasdaq exchanges. we have data collected since january  1  and we use this full data set to calculate the onehour mean and standard deviation for each stock. due to the enormous amount of data  we aggregate into 1-minute intervals-for each stock we maintain its price at both the start and end of the interval  as well as its trading volume during that interval. at this point we can apply the nonoperational criterion to the data obtained between january 1  1 and september 1  1  the dates of the selected stories   resulting in 1 of the stories being labeled as important.
1 applying machine learning
　to perform the learning stage of our operationalization approach we followed the procedures laid out in our first case study. we run through the data a day at a time. during training we skip all data from earlier that day and all of the previous data in any evaluations that are performed. we begin evaluations at the chronological date where at least half of the  important  stories will be in the training set. this meant that only 1 instances were being tested with 1 of those being  important. 
　these results use the same set of learners as above  naive bayes and tfidf. the resulting roc curves are shown in figure 1. here again it shows that  regardless of what ultimately is the appropriate trade-off between false positives and false negatives  it appears that there is sufficient information in the two information sources to be able to predict considerably better than random. further  as with the previous experiment  any user restricted to examining only a subset of the stories would examine considerably more important stories. to be specific  each day there are on average about 1 stories in our final data set  and about 1 of them will be important by the current definition. without ranking  if a user selected  randomly  1 stories  1 would be important. with ranking  using the tfidf method   if a user selected the top 1 stories  1 would be important  all appeared in top 1 : an increase in precision of 1%. of course  the relative increase is smaller as one works down the list  but remains impressive. without ranking  if a user se-
lected  randomly  1 stories  1 would be important. with ranking  to get 1 important stories the user would only need to inspect the top 1 stories. conversely  if the user selected
1

share net ended note statements share net share net ended average statements release stock shares results uncertainties directors share quarter net alert nyse results uncertainties act chief share record directors statements actual annual pm markets statements release stock prnewswire share net results statements approximately results actual approximately results actual chief risk statements release stock results statements chief results statements future act

table 1: ripper rules for stock movement
the top 1 ranked stories  1 would be important  more than 1% improvement in precision.
1 analysis
　since this problem also concerns news stories and uses the same suite of learning methods  we use the same methodology for evaluating the results of learning as we did in our first case study. table 1 shows the 1 rules generated in our analysis step.
　to obtain insight into our original learned models we would like to go beyond these rules  to understand if there is a more general phenomenon underlying the words in these rules. to answer this question we exploit the existence of a taxonomy from the accounting literature that can be used to label each story with one or more categories from a list of 1 categories . we further expanded this list into 1 categories  which are shown in table 1.
　this made it possible for us to expand on our earlier analysis in light of these categories:
1. we manually label a random sample of the storiesinto one of the 1 categories. we can use this sample to compare the distribution of the randomly selected stories to the stories that were labeled important by the user importance criterion. these two distributions should be quite different  hopefully pulling out categories that correlate with interesting stories.
1. select prototypical rules that appeared to have significance based on their coverage of the examples and percentage of true positives.
1. manually label all true positive stories covered by these rules and compare their distributions to the distribution of random stories that were truly important.1
we used this analysis here to focus on two prototypical rules that appeared to have some significance in terms of the words within them. in each case we hand-labeled each story that the rule matched with all of the categories that appeared to apply to it.

1 the rules are used to gain insight into the original learned model  and thus looking at false positives would focus on cases that are not part of that model.
codedescriptionprproduct relatedjvjoint venturescmmcapital market/macroeconomeny related faforecast/analysisncnot classifiable mrmanagement relatedeaearnings announcementsacqacquisitionsothother regulatory and legal actions copcompany operations related capcapital/ownership changesdvddividend announcements assasset changesmermergersdivdivestiturelablabor-related spispinoffsfinfinancial distress demde-mergeraccaccounting/corporate incincome-tax related   our additions to the original 1 categories 
table 1: category taxonomy used for story analysis
　in the case of the first rule we selected  results uncertainties directors  ★ interesting  our hypothesis was that this rule covers largely analyses and projections about the future as well as earnings announcements and management changes. indeed  the majority of the matched stories were from these three categories  although a few concerned product-related issues as can be seen in the accompanying distributions  figure 1 . in the case of our second rule  share net ended average  ★ interesting  our conjecture was that these stories were earnings announcements. indeed  almost all were so as can be seen in the distributions  figure 1 .
　why are these results interesting  first  the classifier is remarkably accurate at associating stories with standard accounting categories. the results suggest that it may be feasible to automate the labeling task with high accuracy instead of requiring humans in the loop to conduct the laborious task of labeling stories.

figure 1:	distribution for  results uncertainties directors 
1

figure 1: distribution for  share net ended average 
　second  it may seem odd that the classifier picks up on themes such as earnings announcements as being important. is it the case that all earnings announcements are important  in this case  the classifier would be essentially be filtering important versus non-important categories. or is it identifying a subset of earnings announcements that truly are important  to answer this question we looked at the distributions of important versus non-important stories in general  shown in figure 1. it is interesting that the distributions are not dramatically different  with the major differences occurring in the  not classifiable   1% important versus 1% non-important  and product related categories  1% important versus 1% non-important   with minor differences in the other major categories. this tells us that important stories are not dominated by specific categories. in effect  judging by the words in the rule  the classifier is identifying a subset of stories from earnings announcements that are important.
1. final remarks
　this paper introduced a four-step process for identifying information items that may be important based on their correlation with the occurrence of subsequent events. the paper further presented two case studies of this approach concerning news stories-recognizing  hot stories  that have many similar stories following them  and recognizing stories that are associated with a stock that will have a significant movement in value.
　while the first steps of our process are fairly well understood  we have only begun the final step of the analysis to get a better understanding of the resulting models. the analysis presented in this paper presents us with a good set of human-understandable rules that give us a sense of plausibility for the learned models  but still leaves something to be desired with respect to actually being able to explain and understand the final model or to gain any insight into what makes the criterion effective. we are currently working on more elaborate techniques to discern the underlying rules and correlations  to get a better understanding of the domain and criteria presented in this paper.

figure 1: distribution for  important  and  not important  stories
acknowledgments
we thank tom fawcett for all his help  with conceptualization and with procuring and dealing with the data. we thank stephen ryan and gideon saar for helping us to begin to understand the effects of firm-specific news on market performance  and for directing us to the literature. we thank ted stohr for suggesting the hot story problem. we are grateful to ibm for a faculty partnership award. portions of this work was supported by the binational science foundation  nasa  and the new jersey commission on science and technology.
