as any other problem solving task that employs search  ai planning needs heuristics to efficiently guide the problem-space exploration. machine learning  ml  provides several techniques for automatically acquiring those heuristics. usually  a planner solves a problem  and a ml technique generates knowledge from the search episode in terms of complete plans  macro-operators or cases   or heuristics  also named control knowledge in planning . in this paper  we present a novel way of generating planning heuristics: we learn heuristics in one planner and transfer them to another planner. this approach is based on the fact that different planners employ different search bias. we want to extract knowledge from the search performed by one planner and use the learned knowledge on another planner that uses a different search bias. the goal is to improve the efficiency of the second planner by capturing regularities of the domain that it would not capture by itself due to its bias. we employ a deductive learning method  ebl  that is able to automatically acquire control knowledge by generating bounded explanations of the problem-solving episodes in a graphplan-based planner. then  we transform the learned knowledge so that it can be used by a bidirectional planner.
1 introduction
planning can be described as a problem-solving task that takes as input a domain theory  a set of states and operators  and a problem  initial state and set of goals  and tries to obtain a plan  a set of operators and a partial order of execution among them  such that  when executed  this plan transforms the initial state into a state where all the goals are achieved. planning has been shown to be pspace-complete . therefore  redefining the domain theory and/or defining heuristics for planning is necessary if we want to obtain solutions to real world problems efficiently. one way to define these heuristics is by means of machine learning. ml techniques applied to planning range from macro-operators acquisition  case-based reasoning  rewrite rules acquisition  generalized policies  deductive approaches of learning heuristics  ebl   learning domain models  to inductive approaches  based on ilp   see  for a detailed account .
﹛despite the current advance in planning algorithms and techniques  there is no universally superior strategy for planning in all planning problems and domains. instead of implementing an universal optimal planner  we propose to obtain good domain-dependent heuristics by using ml and different planning techniques. that is  to learn control knowledge for each domain from the planning paradigms that behave well in this domain. in the future  this could lead to the creation of a domain-dependent control-knowledge repository that could be integrated with the domain descriptions and used by any planner. this implies the definition of a representation language or extension to the standard pddl1  compatible both with the different learning systems that obtain heuristics  and also with the planners that use the learned heuristics.
﹛in this paper we describe a first step in this direction by studying the possibility of using heuristics learned on one specific planner for improving the performance of another planner that has different problem-solving biases. although each planner uses its own strategy to search for solutions  some of them share some common decision points  like  in the case of backward-chaining planners  what operator to choose for solving a specific goal or what goal to select next. therefore  learned knowledge on some type of decision can potentially be transferred to make decisions of the same type on another planner. in particular  we have studied control knowledge transfer between two backwardchaining planners: from a graphplan-based planner  tgp   to a state-space planner  ipss  based on prodigy . we have used prodigy given that it can handle heuristics represented in a declarative language. heuristics are defined as a set of control rules that specify how to make decisions in the search tree. this language becomes our starting heuristic representation language. our goal is to automatically generate these control rules by applying ml in tgp and then translate them into ipss control-knowledge description lan-
guage. we have implemented a deductive learning method to acquire control knowledge by generating bounded explanations of the problem-solving episodes on tgp. the learning approach builds on hamlet   an inductive-deductive system that learns control knowledge in the form of control rules in prodigy  but only on its deductive component  that is based on ebl .
﹛as far as we know  our approach is the first one that is able to transfer learned knowledge between two planning techniques. however  transfer learning has been successfully applied in other frameworks. for instance  in reinforcement learning  macro-actions or options obtained in a learning task can be used to improve future learning processes. the knowledge transferred can range from value functions  to complete policies .
﹛the paper is organized as follows. next section provides background on the planning techniques involved. section 1 describes the implemented learning system and the learned control rules. section 1 describes the translation of the tgp learned rules to be used in ipss. section 1 shows some experimental results. finally  conclusions are drawn.
1 planning models and techniques
in this section we first describe an unified planning model for learning. then  we describe the planners ipss and tgp in terms of that unified model. finally  we describe our proposal to transfer control knowledge from tgp to ipss.
1 unified planning model for learning
in order to transfer heuristics from one planner to another  we needed to  re define planning techniques in terms of a unified model  that accounts for the important aspects of planning from a perspective of learning and using control knowledge. on the base level  we have the domain problem-space pd defined by  modified with respect to  and focusing only on deterministic planning without costs :
  a discrete and finite state space sd 
  an initial state 
  a set of goals gd  such that they define a set of nonempty terminal states as the ones in which all of them
are true 
  a set of non-instantiated actions ad  and a set of instantiated actions ad考  and
  a function mapping non-terminal states sd and instantiated actions  into a state.
﹛we assume that both ad考 and fd ad sd  are non-empty. on top of this problem space  each planner searches in what we will call meta problem-space  pm. thus  for our purposes  we define the components of this problem space as:
  a discrete and finite set of states  sm: we will call each state sm ﹋ sm a meta-state. meta-states are planner dependent and include all the knowledge that the planner needs for making decisions during its search process. for forward-chaining planners each meta-state sm will contain only a state sd of the domain problem-space pd. in the case of backward-chaining planners  as we will see later  they can include  for instance  the current state sd  the goal the planner is working on gd ﹋ gd  or a set of assignments of actions to goals
  an initial meta-state  sm1 ﹋ sm
  a set of non-empty terminal states stm   sm
  a set of search operators am  such as  apply an instantiated action to the current state sd   or  select an instantiated action a ﹋ ad for achieving a given goal gd . these operators will be instantiated by bounding their variables  e.g. which action to apply   am考
  a function mapping non-terminal meta-states sm and instantiated operators a考m into a meta-state fm am sm  ﹋
sm
﹛from a perspective of heuristic acquisition  the key issue consists of learning how to select instantiated operators of the meta problem-space given each meta-state. that is  the goal will be to learn functions that map a meta-state sm into a set of instantiated search operators: h sm    am考 . this is due to the fact that the decisions made by the planner  branching  that we would like to guide are precisely on what instantiated search operator to apply at each meta-state. thus  they constitute the learning points  from our perspective. in this paper  we learn functions composed of a set of control rules. each rule will have the format: if conditions sm  then select am考 . that is  if certain conditions on the current meta-state hold  then the planner should apply the corresponding instantiated search operator.
1 ipss planner
ipss is an integrated tool for planning and scheduling that provides sound plans with temporal information  if run in integrated-scheduling mode . the planning component is a nonlinear planning system that follows a means-ends analysis  see  for details . it performs a kind of bidirectional depth-first search  subgoaling from the goals  and executing operators from the initial state   combined with a branchand-bound technique when dealing with quality metrics. the planner is integrated with a constraints-based scheduler that reasons about time and resource constraints.
in terms of the unified model  it can be described as:
  each meta-state sm is a tuple {sd gdp l gd ad ad考 p} where sd is the current domain state  is the set of pending  open  goals  l is a set of assignments of instantiated actions to goals  gd is the goal in which the planner is working on  ad is an action that the planner has selected for achieving is an instantiated action that the planner has selected for achieving gd  and p is the current plan for solving the problem
  the initial meta-state 
  a terminal state will be of the form smt = {sdt  lt    p} such that gd   sdt  lt will be the causal links between goals and instantiated actions  and p is the plan
  the set of search operators am is composed of  given the current meta-state  :  select a goal gd ﹋ gdp    select an action ad ﹋ ad for achieving the current goal gd     select an instantiation ad考 ﹋ ad考 of current action ad   and  apply the current instantiated action ad考 to the current state sd 
﹛in ipss  as in prodigy  there is a language for defining heuristic function h sm  in terms of a set of control rules. control rules can select  reject  or prefer alternatives  ways of instantiating search operators  am考  . the conditions of control rules refer to queries  called meta-predicates  to the current meta-state of the search
.	prodigy already pro-
vides the most usual meta-predicates  such as knowing whether: some literal l is true in the current state l ﹋ sd  true-in-state   some literal l is the current goal l = gd  current-goal   some literal l is a pending goal  some-candidate-goals  or some instance is of a given type  type-of-object . but  the language for representing heuristics also admits coding user-specific functions.
1 tgp planner
tgp is a temporal planner that enhances graphplan algorithm to handle actions of different durations . again  we will only use in this paper its planning component  and we leave working with the temporal part for future work.
﹛the planner alternates between two phases: graph expansion  it extends a planning graph until the graph has achieved necessary  but potentially insufficient  conditions for plan existence; and solution extraction  it performs a backwardchaining search  on the planning graph  for a solution; if no solution is found  the graph is expanded again and a new solution extraction phase starts.
﹛when tgp performs a backward-chaining search for a plan it chooses an action that achieves each goal proposition. if it is consistent  nonmutex   with all actions that have been chosen so far  then tgp proceeds to the next goal; otherwise it chooses another action. an action achieves a goal if it has the goal as effect. instead of choosing an action to achieve a goal at one level  tgp can also choose to persist the goal  selecting the no-op action ; i.e. it will achieve the goal in levels closer to the initial state. after tgp has found a consistent set of actions for all the propositions in one level  it recursively tries to find a plan for the action's preconditions and the persisted goals. the search succeeds when it reaches level zero. otherwise  if backtracking fails  then tgp extends one level the planning graph with additional action and proposition nodes and tries again. in terms of the unified model  it can be described as:
  each meta-state sm is a tuple {pg gdp l l} where pg is the plan graph  gdp is the set of pending  open  goals  l is a set of assignments of instantiated actions to goals  current partial plan   and l is the plan-graph level where search is
  the initial meta-state   where
pgn is the plan graph built in the first phase up to level n  this second phase can be called many times   gd is the set of top level goals  and ln is the last level generated of the plan graph
  a terminal state will be of the form smt = {pgi  l 1} such that the solution plan can be extracted from l  actions contained in l 
  the set of search operators am is composed of only one operator  given the current meta-state {pg gdp l l} :  for each goal gd ﹋ gdp select an instantiated action ad考 ﹋ ad考 for achieving it . if the goal persists  the goal will still be in the gdp of the successor meta-state. otherwise  the preconditions of each  are added toand each gd is removed from gdp. also  the links  ad考 gd  are added to l.
1 transferring heuristics from tgp to ipss
our proposal to transfer control knowledge between two planners p1 and p1 requires five steps:  1  a language for representing heuristics  or function h1 sm   for p1;  1  a method for automatically generating h1 from p1 search episodes;  1  a translation mechanism between meta problem-space of p1 and meta problem-space of p1  which is equivalent to providing a translation mechanism between h1 and h1;  1  a language h1 for representing heuristics for
p1; and  1  an interpreter  matcher  of h1 in p1. given that
ipss already has a declarative language to define heuristics  hipss  step 1   and an interpreter of that language  step 1   we had to define the first three steps. in relation to step 1  it had to be a language as close as possible to the one used in ipss  so that step 1 could be simplified. therefore  we built it using as many meta-predicates as possible from the ones in ipss declarative language. we will now focus on steps 1 and 1  and include some hints on that language.
1 the learning technique
in this section  we describe the ml technique we built  based on ebl  to generate control knowledge from tgp. we have called it gebl  graphplan ebl . it generates explanations for the local decisions made during the search process  in the meta problem-space . these explanations become control rules. in order to learn control knowledge for a graphplanbased planner  we follow a standard four-steps approach: first  tgp solves a planning problem  generating a trace of the search tree; second  the search tree is labelled so that the successful decision nodes are identified; third  control rules are created from two consecutive successful decision points  by selecting the relevant preconditions; and  fourth  constants in the control rules are generalized to variables. in this discussion  the search tree is the one generated while solving the meta problem-space problem. so  each decision point  or node  consists of a meta-state  and a set of successors  applicable instantiated operators of the meta problem-space .
now  we will present each step in more detail.
1 labelling the search tree
we define three kinds of nodes in the search tree: success  failure and memo-failure. success nodes are the ones that belong to a solution path. the failure nodes are the ones where there is not a valid assignment for all the goals in the node; i.e. it is not possible to find actions to achieve all the goals with no mutex relation violated among them or the ones in the currently constructed plan. a node also fails if all of its children fail. and if the planner did not expand a node  the node is labeled as memo-failure. this can happen when the goals were previously memoized as nogoods  failure condition of graphplan-based planning  at that level.
﹛all nodes are initially labelled as memo-failure. if a node fails during the planning process  its label changes to failure. when the planner finds a solution  all the nodes that belong to the solution path are labelled as success.
﹛figure 1 shows an example of a labelled search tree in the zenotravel domain solving one problem. the zenotravel domain involves transporting people among cities in planes  using different modes of flight: fast and slow. the example problem consists of transporting two persons: person1 from city1 to city1  and person1 from city1 to city1. therefore  gd = { at person1 city1   at person1 city1 }. there are 1 fuel levels  fli  ranging from 1 to 1 and there is a plane initially in city1 with a fuel level of 1. tgp expands the planning graph until level 1 where both problem goals are consistent  nonmutex . in this example there are no failures and therefore no backtracking. the initial metastate s1 is composed of the expanded plan graph  pg1  the problem goals gd  assignments    and level 1. the search algorithm tries to apply an instantiation of the search operator of the meta problem-space  selecting an instantiated action for each goal . so  it finds the instantiated action of the domain problem-space  debark person1 plane1 city1  to achieve the goal  at person1 city1  and persists the goal  at person1 city1 . this generates the child node  meta-state s1   that has  as the goal set  the persisted goal and the preconditions of the previously selected action  debark   i.e.  at plane1 city1   in person1 plane1 . the pair  assignment  action  debark person1 plane1 city1  and goal  at person1 city1  is added on top of the child-node assignments. then  the algorithm continues the search at meta-state s1. it finds the action  fly plane1 city1 city1 fl1 fl1  to achieve the goal  at plane1 city1  and it persists the other goals. that operator generates the child node s1  with the preconditions of the action fly and the persisted goals. the new pair action-goal is added on top of the currently constructed plan. the algorithm continues until it reaches level 1 where the actions in the assignment set of the last node  meta-state s1  represents the solution plan.
﹛once the search tree has been labelled  a recursive algorithm generates control rules from all pairs of consecutive success nodes  eager learning . gebl can also learn only from non-default decisions  lazy learning . in this case  it only generates control rules if there is  at least  one failure node between two consecutive success nodes. the memofailure nodes in lazy learning are not considered  because the planner does not explore them. also  from a  lazyness  perspective they behave as success nodes. lazy learning usually is more appropriate when the control knowledge is obtained and applied to the same planner to correct only the wrong decisions.
1 generating control rules
as we said before  control rules have the same format as in
prodigy. the module that generates control rules receives as input two consecutive success decision points  meta-states  with their goal and assignment sets. there are two kinds of possible rules learned from them: a select goals rule to select the goal that persists in the decision point  when only
s1: level: 1  success
goals=  at person1 city1   at person1 city1   assignments=nil
s1: level: 1  success
goals=  at person1 city1   at plane1 city1   in person1 plane1   assignments=   debark person1 plane1 city1   at person1 city1   
s1: level: 1  success
goals=  at person1 city1   in person1 plane1   fuel-level plane1 fl1 
 at plane1 city1  
assignments=   fly plane1 city1 city1 fl1 fl1   at plane1 city1     debark person1 plane1 city1   at person1 city1   
s1: level: 1  success
goals=  in person1 plane1   fuel-level plane1 fl1   at plane1 city1 
 at person1 city1  
assignments=   board person1 plane1 city1   in person1 plane1     debark person1 plane1 city1   at person1 city1  
  fly plane1 city1 city1 fl1 fl1   at plane1 city1     debark person1 plane1 city1   at person1 city1   
s1: level: 1  success
goals=  in person1 plane1   at person1 city1   fuel-level plane1 fl1   at plane1 city1  
assignments=   fly plane1 city1 city1 fl1 fl1   fuel-level plane1 fl1  
  board person1 plane1 city1   in person1 plane1  
  debark person1 plane1 city1   at person1 city1  
  fly plane1 city1 city1 fl1 fl1   at plane1 city1     debark person1 plane1 city1   at person1 city1   
s1: level: 1  success
goals=  at person1 city1   at person1 city1   fuel-level plane1 fl1 
 at plane1 city1  
assignments=   board person1 plane1 city1   in person1 plane1  
  fly plane1 city1 city1 fl1 fl1   fuel-level plane1 fl1  
  board person1 plane1 city1   in person1 plane1  
  debark person1 plane1 city1   at person1 city1  
﹛﹛﹛﹛  fly plane1 city1 city1 fl1 fl1   at plane1 city1     debark person1 plane1 city1   at person1 city1    figure 1: example of tgp success search tree.
one goal persists  and select operator rules to select the instantiated actions that achieve the goals in the decision point  one rule for each achieved goal .
﹛as an example  from the first two decision points s1 and s1 of the example in figure 1  two rules would be generated; one to select the goal  at person1 city1  and another one to select the operator  debark person1 plane1 city1  to achieve the goal  at person1 city1 .
﹛in order to make the control rules more general and reduce the number of true-in-state meta-predicates  a goal regression is carried out  as in most ebl techniques . only those literals in the state which are required  directly or indirectly  by the preconditions of the instantiated action involved in the rule  the action that achieves goal  are included.
﹛figure 1 shows the select goal rule generated from the first two decision points in the example of figure 1. this rule chooses between two goals of moving persons from one city to another  the arguments of the meta-predicates target-goal and some-candidate-goals . one person  person1  is in a city where there is a plane  plane1  with enough fuel to fly. the rule selects to work on the goal referring to this person giving that s/he is in the same city as the plane.
﹛figure 1 shows the select operator rule generated from the example above. this rule selects the action debark for moving a person from one city to another. ipss would try
 control-rule regla-zeno-travel-pzeno-s1
 if  and  target-goal  at  person1   city1   
 true-in-state  at  person1   city1   
 true-in-state  at  person1   city1   
 true-in-state  at  plane1   city1   
 true-in-state  fuel-level  plane1   fl1   
 true-in-state  aircraft  plane1   
 true-in-state  city  city1   
 true-in-state  city  city1   
 true-in-state  flevel  fl1   
 true-in-state  flevel  fl1   
 true-in-state  next  fl1   fl1   
 true-in-state  person  person1   
 true-in-state  person  person1   
            some-candidate-goals   at  person1   city1        then select goals  at  person1   city1    figure 1: example of select goals rule in the zenotravel domain.
 by default  to debark the person from any plane in the problem definition. the rule selects the most convenient plane; a plane that is in the same city as the person with enough fuel to fly.
 control-rule rule-zeno-travel-zeno1-e1
 if  and  current-goal  at  person1   city1   
 true-in-state  at  person1   city1   
 true-in-state  at  plane1   city1   
 true-in-state  fuel-level  plane1   fl1   
 true-in-state  aircraft  plane1   
 true-in-state  city  city1   
 true-in-state  city  city1   
 true-in-state  flevel  fl1   
 true-in-state  flevel  fl1   
 true-in-state  next  fl1   fl1   
 true-in-state  person  person1     
 then select operators  debark  person1   plane1   city1    figure 1: example of select operator rule in the zenotravel domain.
1 translation of learned knowledge
given that meta-states and operators of the meta problemspace of two different planners differ  in order to use the generated knowledge in p1  tgp  by p1  ipss   we have to translate the control rules generated in the first language to the second one. the translation should have in mind both the syntax  small changes given that we have built the controlknowledge language for tgp based on the one defined in ipss  and the semantics  translation of types of conditions in the left-hand side of rules  and types of decisions - operators of the meta problem-space . we have to consider the translation of the left-hand side of rules  conditions referring to meta-states  and the right-hand side of rules  selection of a search operator of the meta problem-space .
﹛therefore  in relation to the translation of the right-hand side of control rules  we found that the equivalent search operators between ipss and tgp are:
  to decide which goal to work on first. when tgp selects the no-op to achieve a goal  this is equivalent to persist the goal  it will be achieved in levels closer to the initial state . ipss has an equivalent search operator for choosing a goal from the pending goals.
  to choose an instantiated operator to achieve a particular goal  both planners have that search operator  though  in the case of ipss it splits it in two: select an action  and select an instantiated action .
﹛so  according to ipss language to define control rules  there are three kinds of rules that can be learned in tgp to guide the ipss search process: select goals  select operator  select an action  and select bindings  select an instantiated action  rules.
﹛the equivalence between meta-states is not straightforward  for translating the conditions of control rules . when ipss selects to apply an instantiated action  the operator of the meta problem-space changes the state sd and the action is added at the end of the current plan p. however  when tgp selects an instantiated action to achieve a goal  it is added at the beginning of the plan l  given that the search starts at the last level  and tgp does not modify the state sd. the difficulty arises in defining the state sd that will create the true-in-state conditions of the control rules. when the rules are learned in tgp  we considered two possibilities: the simplest one is that the state sd is just the problem initial-state; and  in the second one  we assume that when tgp persists a goal that goal would have already been achieved in ipss  so the state sd is the one reached after executing the actions needed to achieve the persisted goals in the tgp meta-state. to compute it  we look in the solution plan  and progress the problem initialstate according to each action effects in such partial plan.
﹛equivalent meta-states are computed during rule generation and a translator makes several transformations after the learning process finishes. the first one is to split the selectoperator control rules in two: one to select the action and another one to select its instantiation. the second transformation is to translate true-in-state meta-predicates referring to variable types into type-of-object metapredicates.1 finally  the translator deletes those rules that are more specific than a more general rule in the set  they are subsumed by another rule  given that gebl does not perform an inductive step.
1 experimental results
we carried out some experiments to show the usefulness of the approach. our goal is on transferring learned knowledge: that gebl is able to generate control knowledge that improves ipss planning task solving unseen problems. we compare our learning system with hamlet  that learns control rules from ipss problem-solving episodes. according to its authors  hamlet performs an ebl step followed by induction on the control rules.
﹛in these experiments  we have used four commonly used benchmark domains from the repository of previous planning competitions:1 zenotravel  miconic  logistics and driverlog
 the strips versions  since tgp can only handle the plain
strips version .
﹛in all the domains  we trained separately both hamlet and gebl with randomly generated training problems and tested against a different randomly generated set of test problems. the number of training problems and the complexity of the test problems varied according to the domain difficulty for ipss. we should train both systems with the same learning set  but gebl learning mode is eager; it learns from all decisions  generating many rules   and it does not perform induction. so  it has the typical ebl problems  the utility problem and the overly-specific generated control knowledge  that can be attenuated using less training problems with less goals. however  hamlet incorporates an inductive module that diminishes these problems. also  it performs lazy learning  since hamlet obtains the control knowledge for the same planner where the control knowledge is applied. therefore  if we train hamlet with the same training set as gebl  we were loosing hamlet capabilities. so  we have opted for generating an appropriate learning set for both systems: we train hamlet with a learning set  and we take a subset from it for training gebl. the number and complexity  measured as a number of goals  of training problems are shown in table 1. we test against 1 random problems in all the domains  except in the miconic that we used the 1 problems defined for the competition  but varying the number of goals: from 1 to 1 goals in the zenotravel  1 to 1 in the miconic  1 to 1 in the driverlog and 1 to 1 in the logistics.
﹛table 1 shows the average of solved  s  test problems by ipss without using control knowledge  ipss   using the hamlet learned rules  hamlet  and using the gebl learned rules  gebl . column r displays the number of generated rules and column t displays the number of training problems  together with their complexity  range of number of goals . the time limit used in all the domains was 1 seconds except in the miconic domain that was 1s.
domainipsshamletgeblssr	tsrtlogistics1%1%1  1 1%1  1 driverlog1%1%	1  1 1%1  1 zenotravel1%1%	1  1 1%1  1 miconic1%1%	1  1 1%1  1 table 1: results of percentage of solved random problems with and without heuristics.
﹛the results show that the rules learned by gebl greatly increase the percentage of problems solved in all the domains compared to hamlet rules  and plain ipss  except in the miconic domain where hamlet rules are slightly better. usually  learning improves the planning task  but it can also worsen it  as hamlet rules in the driverlog domain . the reasons for this behaviour are the intrinsic problems of
hamlet learning technique: ebl techniques have the utility problem and in inductive techniques generalizing and specializing incrementally do not assure the convergence  unless they continuously check for performance against a problem set.
1 conclusions
this paper presents an approach to transfer control knowledge  heuristics  learned from one planner to another planner that uses a different planning technique and bias. first  we have defined a model based on meta problem-spaces that permits to reason about the decisions made during search by the different planners. then  for each decision we propose to learn control knowledge  heuristics  to guide that planner. but  instead of applying the learned knowledge to that planner  we focus on transferring that knowledge to another planner  so that it can use it.
we have implemented a learning system based on ebl 
gebl  that is able to obtain these heuristics from tgp  a temporal graphplan planner  translate them into ipss  and improve ipss planning task. to our knowledge  this is the first system that is able to transfer learned knowledge between two planning techniques. we have tested our approach in four commonly used benchmark domains and compare it with hamlet  an inductive-deductive learning system that learn heuristics in prodigy. in all the domains  gebl rules notably improve ipss planning task and outperform hamlet  except in the miconic domain where the behaviour is similar. we intend to show in the future that the approach is general enough  so that it works also with other combinations  including to learn in ipss and transfer to tgp.
