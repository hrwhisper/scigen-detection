while a variety of lossy compression schemes have been developed for certain forms of digital data  e.g.  images  audio  video   the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. nevertheless  such techniques are clearly motivated by the everincreasing data collection rates of modern enterprises and the need for effective  guaranteed-quality approximate answers to queries over massive relational data sets. in this paper  we propose   a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. is based on the novel idea of exploiting predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate classification and regression tree  cart  models for entire columns of a table. more precisely  selects a certain subset of attributes for which no values are explicitly stored in the compressed table; instead  concise carts that predict these values  within the prescribed error bounds  are maintained. to restrict the huge search space and construction cost of possible cart predictors  employs sophisticated learning techniques and novel combinatorial optimization algorithms. our experimentation with several real-life data sets offers convincing evidence of the effectiveness of 's model-based approach - is able to consistently yield substantially better compression ratios than existing semantic or syntactic compression tools  e.g.  gzip  while utilizing only small data samples for model inference.
1. introduction
　effective exploratory analysis of massive  high-dimensional tables of alphanumeric data is a ubiquitous requirement for a variety of application environments  including corporate data warehouses  network-traffic monitoring  and large socioeconomic or demographic surveys. for example  large telecommunicationproviders typically generate and store records of information  termed  calldetail records   cdrs   for every phone call carried over their network. a typical cdr is a fixed-length record structure comprising several hundred bytes of data that capture information on various  categorical and numerical  attributes of each call; this includes

work done while visiting bell laboratories.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
acm sigmod 1 may 1  santa barbara  california  usa copyright 1 acm 1-1/1 ...$1.
network-level information  e.g.  endpoint exchanges   time-stamp information  e.g.  call start and end times   and billing information  e.g.  applied tariffs   among others . these cdrs are stored in tables that can grow to truly massive sizes  in the order of several terabytes per year. similar massive tables are also generated from network-monitoring tools that gather switch- and router-level traffic data  such as snmp/rmon probes . such tools typically collect traffic information for each network element at fine granularities  e.g.  at the level of packet flows between source-destination pairs   giving rise to massive volumes of table data over time. these massive tables of network-traffic and cdr data are continuously explored and analyzed to produce the  knowledge  that enables key network-management tasks  including application and user profiling  proactive and reactive resource management  traffic engineering  and capacity planning.
　traditionally  data compression issues arise naturally in applications dealing with massive data sets  and effective solutions are crucial for optimizing the usage of critical system resources  like storage space and i/o bandwidth  for storing and accessing the data  and network bandwidth  for transferringthe data across sites . in mobile-computing applications  for instance  clients are usually disconnected and  therefore  often need to download data for offline use. these clients may use low-bandwidth wireless connections and can be palmtop computers or handheld devices with severe storage constraints. thus  for efficient data transfer and client-side resource conservation  the relevant data needs to be compressed. several statistical and dictionary-based compression methods have been proposed for text corpora and multimedia data  some of which  e.g.  lempel-ziv or huffman  yield provably optimal asymptotic performancein terms of certainergodic propertiesof the data source. these methods  however  fail to provide adequate solutions for compressing a massive data table  as they view the table as a large byte string and do not account for the complex dependency patterns in the table.
　comparedto conventional compressionproblems effectively compressing massive tables presents a host of novel challenges due to several distinct characteristics of table data sets and their analysis.
　approximate  lossy  compression. due to the exploratory nature of many data-analysis applications  there are several scenarios in which an exact answer may not be required  and analysts may in fact prefer a fast  approximate answer  as long as the system can guarantee an upper bound on the error of the approximation. for example  during a drill-down query sequence in ad-hoc data mining  initial queries in the sequence frequently have the sole purpose of determining the truly interesting queries and regions of the data table. providing  reasonably accurate  approximate answers to these initial queries gives analysts the ability to focus their explorations quickly and effectively  without consuming inordinate
agesalaryassetscredit1 1 1poor1 1 1good1 1 1good1 1 1poor1 1 1good1 1 1good1 1 1poor1 1 1poorassets = 1
	 a  tuples in table	 b  cart models
figure 1: model-based semantic compression.

amounts of valuable system resources. thus  in contrast to traditional lossless data compression  the compression of massive tables can often afford to be lossy  as long as some  user- or applicationdefined  upper bounds on the compression error are guaranteed by the compression algorithm. this is obviously a crucial differentiation  as even small error tolerances can help us achieve much better compression ratios.
　semantic compression. existing compression techniques are  syntactic  in the sense that they operate at the level of consecutive bytes of data. as explained above  such syntactic methods typically fail to provide adequate solutions for table-data compression  since they essentially view the data as a large byte string and do not exploit the complex dependency patterns in the table. effective table compression mandates techniques that are semantic in nature  in the sense that they account for and exploit both  1  the meanings and dynamic ranges of individual attributes  e.g.  by taking advantage of the specified error tolerances ; and   1  existing data dependencies and correlations among attributes in the table.
our contributions. in this paper  we describe the architecture of
1
　　　　　　  a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. is based on the novel idea of exploiting data correlations and user-specified  loss /error tolerances for individual attributes to construct concise and accurate classification and regression tree  cart  models  for entire columns of a table. more precisely  selects a certain subset of attributes  referred to as predicted attributes  for which no values are explicitly stored in the compressed table; instead  concise carts that predict these values  within the prescribed error bounds  are maintained. thus  for a predicted attribute that is strongly correlated with other attributes in the table  is typically able to obtain a very succinct cart predictor for the values of   which can then be used to completely eliminate the column for in the compressed table. clearly  storing a compact cart model in lieu of millions or billions of actual attribute values can result in substantial savings in storage. in addition  allowing for errors in the attribute values predicted by a cart model only serves to reduce the size of the model even further and  thus  improve the quality of compression; this is because  as is well known  the size of a cart model is typically inversely correlated to the accuracy with which it models a given set of values .

  from webster  spartan: /'spart-*n/  1  of or relating to sparta in ancient greece   1  a: marked by strict self-discipline and avoidance of comfort and luxury  b: sparing of words : terse : laconic.
　example 1. consider the table with 1 attributes and 1 tuples shown in figure 1 a . also  suppose that the acceptable errors due to compression for the numeric attributes age  salary  and assets are 1  1  and 1  respectively. figure 1 b  depicts a classification tree for predicting the credit attribute  with salary as the predictor attribute  and a regression tree for predicting the assets attribute  with salary and age as the predictor attributes . observe that in the regression tree  the predicted value of assets  label value at each leaf  is almost always within 1  the specified error tolerance  of the actual tuple value. for instance  the predicted value of assets for the tuple with salary = 1 is 1 while the original value is 1. the only tuple for which the predicted value violates this error bound is the tuple with salary = 1  which is an marked as an outlier value in both trees. thus  by explicitly storing  in the compressed version of the table  each outlier value along with the cart models and the projection of the table onto only the predictor attributes  age and salary   we can ensure that the error due to compression does not exceed the userspecified bounds. further  storing the cart models  plus outliers  for credit and assets instead of the attribute values themselves results in a reduction from 1 to 1 values for credit  1 labels for leaves + 1 split value at internal node + 1 outlier  and a reduction from 1 to 1 values for assets  1 labels for leaves + 1 split values at internal nodes + 1 outlier . 
　the key algorithmic problem faced by 's compression engine is that of computing an optimal set of cart models for the input table such that  a  the overall storage requirements of the compressed table are minimized  and  b  all predicted attribute values are within the user-specified error bounds. this is a very challenging optimization problem since  not only is there an exponential number of possible cart-based models to choose from  but also building carts  to estimate their compression benefits  is a computation-intensive task  typically requiring multiple passes over the data  1  1  1 . as a consequence  has to employ a number of sophisticated techniques from the areas of knowledge discovery and combinatorial optimization in order to efficiently discover a  good   sub set of predicted attributes and construct the corresponding cart models. below  we list some of 's salient features.
　use of bayesian network to uncover data dependencies. a bayesian network is a dag whose edges reflect strong predictive correlations between nodes of the graph . thus  a bayesian network on the table's attributes can be used to dramatically reduce the search space of potential cart models since  for any attribute  the most promising cart predictors are the ones that involve attributes in its  neighborhood  in the network. our current implementation uses a constraint-based bayesian network builder based on recently proposed algorithms for efficiently inferring predictive structure from data. to control the computational overhead  the bayesiannetwork is built using a reasonablysmall randomsample of the input table.
 novel cart-selectionalgorithms thatminimize storage cost. exploits the inferred bayesian network structure by using it to intelligently guide the selection of cart models that minimize the overall storage requirement  based on the prediction and materialization costs for each attribute. intuitively  the goal is to minimize the sum of the prediction costs  for predicted attributes  and materializationcosts  for attributes used in the carts . we demonstrate that this model-selection problem is a strict gener-
alization of the weighted maximum independent set  wmis  problem   which is known to be -hard. however  by employing a novel algorithm that effectively exploits the discovered bayesian structure in conjunction with efficient  near-optimal wmis heuristics  is able to obtain a good set of cart models for compressing the table.
 improved cart construction algorithms that exploit error tolerances. a signification portion of 's execution time is spent in building cart models. this is mainly because needs to actually construct many promising carts in order to estimate their prediction cost  and cart construction is a computationally-intensive process. to reduce cart-building times and speed up system performance  employs the following three optimizations:  1  carts are built using random samples instead of the entire data set   1  leaves are not expanded if values of tuples in them can be predicted with acceptable accuracy  and  1  pruning is integrated into the tree growing phase using novel algorithms that exploit the prescribed error tolerance for the predicted attribute. then uses the carts built to compress the full data set in one pass.
　we have implemented the system and conducted an extensive experimental study with three real-life data sets to compare the quality of compression due to 's modelbased approach with existing syntactic and semantic compression techniques. for all three data sets  and even for small error tolerances  e.g.  1%   we found that is able to achieve  on an average  1% better compression ratios. further  our experimental results indicate that compresses tables better when they contain more numeric attributes and as error thresholds grow bigger. for instance  for a table containing mostly numeric attributes and for higher error tolerances in the 1% range  outperformedexisting compressiontechniquesby more
than a factor of 1. finally  we show that our improved cart construction algorithms make 's performance competitive  enabling it to compress data sets containing more than half a million tuples in a few minutes.
1. overview of approach
1 preliminaries
definitions and notation. the input to the system consists of a -attribute table   comprising a large number of tuples  rows . we let denote the set of attributes of and represent the domain of attribute .
attributes with a discrete  unorderedvalue domainare referredto as categorical  whereas those with ordered value domains are referred to as numeric. we also use to denote the compressed version of table   and     to denote the storage-space requirements for     in bytes.
　the key input parameter to our semantic compression algorithms is a  user- or application-specified  -dimensional vector of error tolerances that defines the per-attribute acceptable degree of information loss when compressing .  per-attribute error bounds are also employed in the fascicles framework .  intuitively  the entry of the tolerance vector specifies an upper bound on the error by which any  approximate  value of in the compressed table can differ from its original value in . our error tolerance semantics differ across categorical and numeric attributes  due to the very different nature of the two attribute classes.
1. for a numeric attribute   the tolerance defines an upper bound on the absolute difference between the actual value of in and the corresponding  approximate  value in . that is  if   denote the accurate and approximate value  respectively  of attribute for any tuple of   then our compressor guarantees that .
1. for a categorical attribute   the tolerance defines an upper bound on the probability that the  approximate  value of in is different from the actual value in . more formally  if   denote the accurate and approximate value  respectively  of attribute for any tuple of   then our compressor guarantees that .
for numeric attributes  the error tolerance could very well be specified in terms of quantiles of the overall range of values rather than absolute  constant values. similarly  for categorical attributes the probability of error could be specified separately for each individual attribute class  i.e.  value  rather than an overall measure.  note that such an extension would  in a sense  make the error bounds for categorical attributes more  local   similar to the numeric case.  our proposed model-based compression framework and algorithms can be readily extended to handle these scenarios  so the specific definitions of error tolerance are not central to our methodology. to make our discussion concrete  we use the definitions outlined above for the two attribute classes.  note that our error-tolerance semantics can also easily capture lossless compression as a special case  by setting for all . 
metrics. the basic metric used to compare the performance of different compression algoritms is the well-known compression ratio  defined as the ratio of the size of the compressed data representation produced by the algorithm and the size of the original  uncompressed  input. a secondary performance metric is the compression throughput that  intuitively  corresponds to the rate at which a compression algorithm can process data from its input; this is typically defined as the size of the uncompressed input divided by the total compression time.
　our work focuses primarily on optimizing the compression ratio  that is  achieving the maximum possible reduction in the size of the data within the acceptable levels of error defined by the user. this choice is mainly driven by the massive  long-lived data sets that are characteristic of our target data warehousing applications and the observation that the computational cost of effective compression can be amortized over the numerous physical operations  e.g.  transmissions over a low-bandwidth link  that will take place during the lifetime of the data. also  note that our methodology offers a key  knob  for tuning compression throughput performance  namely the size of the data sample used by 's model-construction algorithms. setting the sample size based on the amount of main memory available in the system can help ensure high compression speeds.
1 model-based semantic compression
　briefly  our proposed model-based framework for the semantic compression of tables is based on two key technical ideas. first  we exploit the  user- or application-specified  error bounds on individual attributes in conjunction with data mining techniques to efficiently build accurate models of the data. second  we compress the input table using a select subset of the models built. the basic intuition here is that this select subset of data-mining models is carefully chosen to capture large portions of the input table within the specified error bounds.
　more formally  we define the model-based  compressed version of the input table as a pair where
 1  is a small  possibly empty  projection of the data values in that are retained accurately in ; and   1  is a select set of data-mining models  carefully built with the purpose of maximizing the degree of compression achieved for while obeying the specified error-tolerance constraints. abstractly  the role of is to capture values  tuples or sub-tuples  of the original table that cannot be effectively  summarized away  in a compact datamining model within the specified error tolerances.  some of these values may in fact be needed as input to the selected models.  the attribute values in can either be retained as uncompressed data or be compressed using a conventional lossless algorithm.
　a definition of our general model-based semantic compression problem can now be stated as follows.
 model-based semantic compression  mbsc  : given a multiattribute table and a vector of  per-attribute  error tolerances   find a set of models and a compression scheme for based on these models	such that the specified error bounds are not exceeded and the storage requirements of the compressed table are minimized. 
　given the multitude of possible models that can be extracted from the data  this is obviously a very general problem definition that covers a huge design space of possible alternatives for semantic compression. we provide a more concrete statement of the problem addressed in our work on the system later in this section. first  however  we discuss how our model-based compression framework relates to recent work on semantic compression and demonstrate the need for the more general approach advocated in this paper.
comparison with fascicles. our model-based semantic compression framework  in fact  generalizes earlier ideas for semantic data compression  such as the very recent proposal of jagadish  madar  and ng on using fascicles for the semantic compression of relational tables .  to the best of our knowledge  this is the only work on lossy semantic compression of tables with guaranteed upper bounds on the compression error1. 
　a fascicle basically represents a collection of tuples  rows  that have approximately matching values for some  but not necessarily all  attributes  where the degree of approximation is specified by user-provided compactness parameters. essentially  fascicles can be seen as a specific form of data-mining models  i.e.  clusters in subspaces of the full attribute space  where the notion of a cluster is based on the acceptable degree of loss during data compression. the key idea of fascicle-based semantic compression is to exploit the given error bounds to allow for aggressive grouping and  summarization  of values by clustering multiple rows of the table along several columns  i.e.  the dimensionality of the cluster .
　example 1. consider the table in figure 1 a  described in example 1. error tolerances of 1  1 and 1 for the three numeric attributes age  salary and assets  respectively  result in the following two fascicles:
1 1 1good1 1 1poor1 1 1good1 1 1poorthe tuples in the two fascicles and are similar  with respect to the permissible errors  on the asset and credit attributes  shown in bold . the reason for this is that two attribute values are considered to be similar if the difference between them is at most twice the error bound for the attribute. thus  substituting for each attribute value  the mean of the maximum and minimum value of the attribute ensures that the introduced error is acceptable. consequently  in order to compress the table using fascicles  the single  sub tuple  1  good  replaces the two corresponding  sub tuples in the first fascicle and  1  poor  is used instead of the two subtuples in the second fascicle. thus  in the final compressed table  the maximum error for assets is 1  and the number of values stored for the assets and credit attributes is reduced from 1 to 1. 

 due to space constraints  we omit a detailed discussion of related work; it can be found in the full version of this paper .
　as the above example shows  in many practical cases  fascicles can effectively exploit the specified error tolerances to achieve high compression ratios. there are however  several scenarios for which a more general  model-based compression approach is in order. the main observation here is that fascicles only try to detect  row-wise  patterns  where sets of rows have similar values for several attributes. such  row-wise  patterns within the given error-bounds can be impossible to find when strong  column-wise  patterns/dependencies  e.g.  functional dependencies  exist across attributes of the table. on the other hand  different classes of datamining models  like classification and regression trees  carts   can accurately capture and model such correlations and  thereby  attain much better semantic compression in such scenarios.
　revisiting example 1  the two carts in figure 1 b  can be used to predict values for the assets and credit attributes  thus completely eliminating the need to explicitly store values for these attributes. note that carts result in better compression ratios than fascicles for our example table - the storage for the credit attribute reduces from 1 to 1 with carts compared to 1 with fascicles.
concrete problem definition. the above discussiondemonstrates the need for a semantic compression methodology that is more general than simple fascicle-based row clustering in that it can account for and exploit strong dependencies among the attributes of the input table. the important observation here  already outlined in example 1  is that data mining offers models  i.e.  carts  that can accurately capture such dependencies with very concise data structures. thus  in contrast to fascicles  our general model-based semantic compression paradigm can accommodate such scenarios.
　the ideas of row-wise pattern discovery and clustering for semantic compression have been thoroughly explored in the context of fascicles . in contrast  our work on the semantic compressor reported in this paper focuses primarily on the novel problems arising from the need to effectively detect and exploit  column-wise  attribute dependencies for the purposes of semantic table compression. the key idea underlying our approach is that  in many cases  a small classification  regression  tree structure can be used to accurately predict the values of a categorical  resp.  numeric  attribute  based on the values of other attributes  for a very large fraction of table rows. this means that  for such cases  our compression algorithms can completely eliminate the predicted column in favor of a compact predictor  i.e.  a classification or regression tree model  and a small set of outlier column values. more formally  the design and architecture of focuses mainly on the following concrete mbsc problem.
 	cart-based semantic compression : given a multiattribute table	with a set of categorical and/or numeric attributes   and a vector of  per-attribute  error tolerances	  find a subset	of	and a set of corresponding cart models such that:  1  model	is a predictor for the values of attribute	based solely on attributes in	  for each	;  1  the specified error bounds	are not exceeded; and   1  the storage requirements	of the compressed table	are minimized. 
　abstractly  our novel semantic compression algorithms seek to partition the set of input attributes into a set of predicted attributes and a set of predictor attributes
　　　　such that the values of each predicted attribute can be obtained within the specified error bounds based on  a subset of  the predictor attributes through a small classification or regression tree  except perhaps for a small set of outlier values .  we use the notation to denote a cart predictor for attribute using the set of predictor attributes .  note that we do not allow a predicted attribute to also be a predictor for a different attribute. this restriction is important since predicted values of can contain errors  and these errors can cascade further if the erroneous predicted values are used as predictors  ultimately causing error constraints to be violated. the final goal  of course  is to minimize the overall storage cost of the compressed table. this storage cost is the sum of two basic components:
1. materialization cost  i.e.  the cost of storing the values for all predictor attributes . this cost is represented in the component of the compressed table  which is basically the projection of onto the set of predictor attributes.  the storage cost of materializing attribute is denoted by matercost . 
1. prediction cost  i.e.  the cost of storing the cart models used for prediction plus  possibly  a small set of outlier values of the predicted attribute for each model.  the storage cost of predicting attribute through the cart predictor
is denoted by predcost ; this does not include the cost of materializing the predictor attributes in . 
　we should note here that our proposed cart-based compression methodology is essentially orthogonal to techniques based on row-wise clustering  like fascicles. it is entirely possible to combine the two techniques for an even more effective model-based semantic compression mechanism. as an example  the predictor attribute table derived by our  column-wise  techniques can be compressed using a fascicle-based algorithm.  in fact  this is exactly the strategy used in our current implementation; however  other methods for combining the two are also possible.  the important point here is that  since the entries of are used as inputs to  approximate  cart models for other attributes  care must be taken to ensure that errors introduced in the compression of do not compound over the cart models in a way that causes error guarantees to be violated. the issues involved in combining our cart-based compression methodology with row-wise clustering techniques are addressed in more detail later in the paper.
1 overview of the	system
　as depicted in figure 1  the architectureof the system comprisesof four majorcomponents: the dependencyfinder  the cartselector  the cartbuilder  and the rowaggre-
gator. in the following  we provide a brief overview of each component; we defer a more detailed description of
each component and the relevant algorithms to section 1.

 dependencyfinder. the purpose of the dependencyfinder component is to produce an interaction model for the input table attributes  that is then used to guide the cart building algorithms of . the main observation here is that  since there is an exponential number of possibilities for building cartbased attribute predictors  we need a concise model that identifies the strongest correlations and  predictive  relationships in the input data.
　the approach used in the dependencyfinder component of is to construct a bayesian network  on the underlying set of attributes	. abstractly  a bayesian network imposes a directed acyclic graph  dag  structure	on the set of nodes	  such that directed edges capture direct statistical dependence between attributes.  the exact dependencesemantics of	are defined shortly.  thus  intuitively  a set of nodes in the  neighborhood  of in	 e.g. 	's parents  captures the attributes that are strongly correlated to	and  therefore  show promise as possible predictor attributes for	.
 cartselector. the cartselectorcomponent constitutes the core of 's model-based semantic compression engine. given the input table and error tolerances   as well as the bayesian network on the attributes of built by the de-
pendencyfinder  the cartselector is responsible for selecting a collection of predicted attributes and the correponding cart-based predictors such that the final overall storage cost is minimized  within the given error bounds . as discussed above 
           's cartselector employs the bayesian network built on to intelligently guide the search through the huge space of possible attribute prediction strategies. clearly  this search involves repeatedinteractionswith the cartbuildercomponent  which is responsible for actually building the cart-models for the predictors  figure 1 .
　we demonstrate that even in the simple case where the set of nodes that is used to predict an attribute node in is fixed  the problem of selecting a set of predictors that minimizes the combination of materialization and prediction cost naturally maps to the weighted maximum independent set  wmis  problem  which is known to be -hard and notoriously difficult to approximate . based on this observation  we propose a cart-model selection strategy that starts out with an initial solution obtained from a nearoptimal heuristic for wmis  and tries to incrementally improve it by small perturbations based on the unique characteristics of our problem. we also give an alternative greedy model-selection algorithm that chooses its set of predictors using a simple local condition during a single  roots-to-leaves  traversal of the bayesian network .
　cartbuilder. given a collection of predicted and  corresponding  predictor attributes   the goal of the cartbuilder component is to efficiently construct cart-based models for each in terms of for the purposes of semantic compression. induction of cart-based models is typically a computationintensive processthat requires multiplepasses over the input data  1 
1 . as we demonstrate  however  's cart construction algorithms can take advantage of the compression semantics and exploit the user-defined error-tolerances to effectively prune computation. in addition  by building carts using data samples instead of the entire data set  is able to further speed up model construction.
 rowaggregator. once 's cartselectorcomponent has finalized a  good  solution to the cart-based semantic compression problem  it hands off its solution to the rowaggregator component which tries to further improve the compression ratio through row-wise clustering. briefly  the rowaggregator uses a fascicle-based algorithm  to compress the predictor attributes  while ensuring  based on the cart models built  that errors in the predictor attribute values are not propagated through the carts in a way that causes error tolerances  for predicted attributes  to be exceeded.
1. system components
1 the dependencyfinder component
motivation. as explained in section1  the essenceof 's cart-based semantic compression problem lies in discovering a collection of  strong  predictive correlations among the attributes of an arbitrary table. the search space for this problem is obviously exponential: given any attribute   any subset of could potentially be used to construct a predictor for ! furthermore  verifying the quality of a predictor for the purposes of semantic compression is typically a computation-intensive task  since it involves actually building the corresponding classification or regression tree on the given subset of attributes  1  1  1 . since building an exponentially large number of carts is clearly impractical  we need a methodology for producing a concise interaction model that identifies the strongest predictive correlations among the input attributes. this model can then be used to restrict the search to interesting regions of the prediction space  limiting cart construction to truly promising predictors. building such an interaction model is the main purpose of 's dependencyfindercomponent.
　the specific class of attribute interaction models used in the current implementationis that of bayesiannetworks .
briefly  a bayesian network is a combination of a probability distribution and a structural model in the form of a dag over the attributes in which edges represent direct probabilistic dependence. in effect  a bayesian network is a graphical specification of a joint probability distribution that is believed to have generated the observed data. bayesian networks are an essential tool for capturing causal and/or predictive correlations in observational data ; such interpretations are typically based on the following dependence semantics of the bayesian network structure.
　parental markov condition : given a bayesian network over a set of attributes   any node is independent of all its non-descendant nodes given its parent nodes in  denoted by
 .
　markov blanket condition : given a bayesian network over a set of attributes   we define the markov blanket of
 denoted by   as the union of 's parents  's children  and the parents of 's children in . any node is independent of all other nodes given its markov blanket in .
　based on the above conditions  a bayesian network over the attributes of the input table can provide definite guidance on the search for promising cart predictors for semantic compression.
more specifically  it is clear that predictors of the form
	or	should be considered as prime candidates for
cart-based semantic compression.
construction algorithm. learning the structure of bayesian networks from data is a difficult problem that has seen growing research interest in recent years  1  1  1 . there are two general approaches to discovering bayesian structure:  1  constraint-based methods try to discover conditional independence properties between data attributes using appropriate statistical measures  e.g.  or mutual information  and then build a network that exhibits the observed correlations and independencies  1  1 .  1  scoringbased  or  bayesian  methods are based on defining a statisticallymotivated score function  e.g.  bayesian or mdl-based  that describes the fitness of a probabilistic network structure to the observed data; the goal then is to find a structure that maximizes the score  1  1  1 .  in general  this is a hard optimization problem that is typically -hard .  both methods have their pros and cons. given the intractability of scoring-based network generation  several heuristic search methods with reasonable time complexities have been proposed. many of these scoring-based methods  however  assume an ordering for the input attributes and can give drastically different networks for different attribute orders. further  due to their heuristic nature  such heuristic methods may not find the best structure for the data. on the other hand  constraintbased methods have been shown to be asymptotically correct under certain assumptions about the data   but  typically  introduce edges in the network based on conditional independence  ci  tests that become increasingly expensive and unreliable as the size of the conditioning set increases . also  several constraint-based methods have very high computational complexity  requiring  in the worst case  an exponential number of ci tests.
             's dependencyfinderimplements a constraintbased bayesian network builder based on the algorithm of cheng et al. . unlike earlier constraint-based methods  the algorithm of cheng et al. explicitly tries to avoid complex ci tests with large conditioning sets and  by using ci tests based on mutual information divergence  eliminates the need for an exponential number of ci tests . in fact  given an -attribute data set  our bayesian network builder requires at most ci tests  which  in our implementation  translates to at most passes over the input tuples. recall that 's dependencyfinder uses only a small random sample of the input table to discover the attribute interactions; the size of this sample can be adjusted according to the amount of main memory available  so that no i/o is incurred  other than that required to produce the sample . also  note that the dependencyfinder is  in a sense  out of the  critical path  of the data compression process  since such attribute interactions are an intrinsic characteristic of the data semantics that only needs to be discovered once for each input table. our dependencyfinder implementation adds several enhancements to the basic cheng et al. algorithm  such as the use of bayesian-scoring methods for appropriately orienting the edges in the final network .
1 the cartselector component
　the cartselector component is the heart of 's model-based semantic compression engine. given the input data table and error tolerances  as well as the bayesian network capturing the attribute interactions  the goal of the cartselector is to select  1  a subset of attributes to be predicted and  1  the corresponding cart-based predictors  such that the overall storage cost is minimized within the specified error bounds. recall from section 1 that the total storage cost is the sum of the materialization costs  of predictor attributes  and prediction costs  of the carts for predicted attributes . in essence  the cartselector implements the core algorithmic strategies for solving
           's cart-based semantic compression problem  section 1 . deciding on a storage-optimal set of predicted attributes and corresponding predictors poses a hard combinatorial optimization problem; as the following theorem shows  the problem is hard even in the simple case where the set of predictor attributes to be used for each attribute is fixed.
	theorem 1. consider a given set of	predictors
　　　for all where . choosing a storageoptimal subset of attributes to be predicted using attributes in is -hard. 
　the simple instance of 's cart-basedsemantic compression problem described in the above theorem can be shown to be equivalent to the weighted maximum independent set  wmis  problem  which is known to be -hard. the wmis problem can be stated as follows:  given a node-weighted  undirected graph
　　　　　  find a subset of nodes such that no two vertices in are joined by an edge in and the total weight of nodes in is maximized.  abstractly  the partitioning of the nodes into and corresponds exactly to the partitioning of attributes into  predicted  and  materialized  with the edges of capturing the  predicted by  relation. further  the constraint that no two vertices in are adjacent in ensures that all the  predictor  attributes for a predicted attribute  in   are materialized  which is a requirement of 's compression problem. also  the weight of each node coresponds to the  storage benefit   materialization cost - prediction cost  of predicting the corresponding attribute. thus  maximizing the storage benefit of the predicted attributes has the same effect as minimizing the overall storage cost of the compressed table.
　even though wmis is known to be -hard and notoriously difficult to approximate for general graphs   several recent approximation algorithms have been proposed with guaranteed worstcase performance bounds for bounded-degree graphs . the optimization problem faced by 's cartselector is obviously much harder than simple wmis  since the cartselector is essentially free to decide on the set of predictor attributes for each cart. further  the cartselector also has to
invoke	's cartbuildercomponentto actually build
potentially useful carts  and this construction is itselfa computationintensive task  1  1 .
given the inherent difficulty of the cart-based semantic com-
pression problem  's cartselectorimplementstwo distinct heuristic search strategies that employ the bayesian network model of built by the dependencyfinder to intelligently guide the search through the huge space of possible attribute prediction alternatives. the first strategy is a simple greedy selection algorithm that chooses cart predictors greedily based on their storage benefits during a single  roots-to-leaves  traversal of the bayesian graph. the second  more complex strategy takes a less myopic approach that exploits the similarities between our cartselection problem and wmis; the key idea here is to determine the set of predicted attributes  and the corresponding carts  by obtaining  approximate  solutions to a number of wmis instances created based on the bayesian model of .
the greedy cart selector. briefly  's greedy cartselection algorithm visits the set of attributes in the topologicalsort order imposed by the constructed bayesian network model and tries to build a cart predictor for each attribute based on its predecessors. thus  for each attribute visited  there are two possible scenarios.
1. if has no parent nodes in  i.e.  node is a root of   then greedy concludes that cannot be predicted and  consequently  places directly in the subset of materialized attributes .
1. otherwise  i.e. 	is not empty in	  	's
cartbuilder component is invoked to construct a cartbased predictor for  within the specified error tolerance   using the set of attributes chosen for materialization thus far .  note that possibly irrelevant attributes in will be filtered out by the cart construction algorithm in cartbuilder.  once the cart for is built  the relative storage benefit of predicting can be estimated; is chosen for prediction if this relative benefit is at least  an input parameter to greedy  and materialized otherwise.
　our greedy algorithm provides a simple  low-complexity solution to 's cart-based semantic compression problem.  the detailed pseudo-code for greedy can be found in .  given an -attribute table and bayesian network   it is easy to see that greedy always constructs at most cart predictors during its traversal of . this simplicity  however  comes at a price. more specifically  greedy cart selection suffers from two major shortcomings. first  selecting an attribute to be predicted based solely on its  localized prediction benefit  through its predecessors in   is a very myopic strategy  since it ignores the potential benefits from using itself as a  materialized  predictor attribute for its descendants in . such very localized decisions can obviously result in poor overall predictor selections. second  the value of the  benefit threshold  parameter can adversely impact the performance of the compression engine and selecting a reasonable value for is not a simple task. a high value may mean that very few or no predictors are chosen  whereas a low value may cause lowbenefit predictors to be chosen early in the search thus excluding some high-benefit predictors at lower layers of the bayesian network.

	-infty	1 	1	-infty	-1	-infty	-1
 a  bayesian network g	 b  gtemp  1st iter. 	 c  gtemp  1nd iter. 	 d  gtemp  1rd iter. 
figure 1: example instance for cartselector algorithms.

　example 1. consider the bayesian network graph defined over attributes shown in figure 1 a . let the materialization cost of each attribute be 1. further  let the prediction costs of cart predictors be as follows:
predcostpredcostpredcostpredcostpredcostpredcostsuppose that . since has no parents  it is initially added to . in the next two iterations  since matercost
predcost	and matercost
predcost	 	and	are added
to . finally  is added to since matercost predcost . thus  the overall storage cost of materializing and   and predicting and is
. 
the maxindependentsetcart selector.	's maxin-
dependentset cart-selection algorithm  depicted in figure 1  alleviates the drawbacks of greedy mentioned above. intuitively  the maxindependentset algorithm starts out by assuming all attributes to be materialized  i.e.   step 1   and then works by iteratively solving wmis instances that try to improve the overall storage cost by moving the nodes in the  approximate  wmis solution to the subset of predicted attributes . consider the first iteration of the main while-loop  steps 1 . algorithm maxindependentset starts out by building cart-based predictors for each attribute in based on 's  predictive neighborhood in the constructed bayesiannetwork  steps 1- 1 ; this neighborhood function is an input parameter to the algorithm and can be set to either 's parents or its markov blanket in . then  based on the  predicted-by  relations observed in the constructed carts  maxindependentset builds a node-weighted  undirected graph on with  a  all edges   where
is used in the cart predictor for	  and  b  weights for each node
procedure maxindependentset 	 	 	 	 
input:	-attribute table	;	-vector of error tolerances ; bayesian network on the set of attributes	; function	defining the
	 predictive neighborhood  of a node	in	 e.g. 	or	 .
output: set of materialized  predicted  attributes	 
	  and a cart predictor pred	for each	.
begin
1.  
1. pred	for all	  improve := true
1. while   improve	false   do
1. for each
1. materneighbors
pred
1. buildcartmaterneighbors
1. let pred	materneighbors	be the set of predictor attributes used in
1. costchange
1. for each	such that	pred
1. newpred	pred	pred
1. buildcartnewpred
1. set newpred	to the  sub set of predictor attributes
used in
1. costchange	costchange	predcostpred
predcostnewpred
1. end
1. end
1. build an undirected  node-weighted graph on the current set of materialized attributes	  where:
1.  a 	pair	pred	for some in	pred	 
1.  b  weight	matercost	predcostpred
costchange for each
1.
1. /* select  approximate  maximum weight independent set in */
1. /*	as  maximum-benefit  subset of predicted attributes */
1. if	weight	then improve := false
1. else	/* update	 	  and the chosen cart predictors */
1. for each
1. if   pred	  then
1. pred	newpred
1. end
1.  
1. end
1. end /* while */ end
figure 1: the maxindependentset cart-selection algorithm.

   set equal to the storage-cost benefit of predicting  steps 1- 1 . finally  maxindependentset finds a  near-optimal  wmis of and the corresponding nodes/attributes are moved to the predicted set with the appropriate cart predictors  assuming the total benefit of the wmis is positive   steps 1 .
　note that in step 1  it is possible for materneighbors to be . this could happen  for instance  if is a root of and
　's neighborhood comprises of its parents. in this case  the model returned by buildcart is empty and it does not make sense for to be in the predicted set . we ensure that always stays in by setting predcostpred to if pred   which causes 's weight to become in step 1.
　the wmis solution discovered after this first iteration of maxindependentset can be further optimized  since it makes the rather restrictive assumption that an attribute can only be predicted based on its direct neighborhood in . for example  consider a scenario where contains the directed chain   and the attribute pair provides a very good predictor for   which itself is a good predictor for the value of . then  the initial wmis solution can obviously select only one of these predictors. on the other hand  the above scenario means that  by  transitivity   it is very likely that can also provide a good predictor for  i.e.  only and need to be materialized .
　later iterations of maxindependentset's main while-loop try to further optimize the initial wmis solution based on the above observation. this is accomplished by repeatedly moving attributes from the remaining set of materialized attributes to the predicted attributes . for each materialized attribute   maxindependentsetfinds its materializedneighborhood in the bayesian model   that comprises for each node in the neighborhood of :  1  itself  if and  1  the  materialized  attributes currently used to predict   if  step 1 . a
cart predictor for based on its materialized neighbors is then constructed  step 1 . now  since may already be used in a number of predictors for attributes in   we also need to account for the change in storage cost for these predictors when is replaced by its materialized neighbors used to predict it; this change  denoted by costchange   is estimated in steps 1. the nodeweighted  undirected graph is then built on with the weight for each node set equal to the overall storage benefit of
predicting   including costchange  steps 1 .  note that this benefit may very well be negative.  finally  a  near-optimal  wmis of is chosen and added to the set of predicted attributes with the appropriate updates to the set of cart predictors. note that  since our algorithm considers the  transitive effects  of predicting each materialized node in isolation  some additional care has to be taken to ensure that at most one predic-
tor attribute from each already selected cart in is chosen at each iteration. this is accomplished by ensuring that all attributes belonging to a predictor set pred for some form a clique in the construction of  step 1 . then  by its definition  the wmis solution can contain at most one node from each such set pred . maxindependentset's while-loop continues until no further improvements on the overall storage cost are possible  step 1 .
　example 1. consider the bayesian network graph shown in figure 1 a  and let prediction costs for attributes be as described earlier in example 1. further  suppose the neighborhood function for a node is itsparents. in the first iteration  pred
  pred	  pred	and pred	.
further  since	  costchange	for each
     . as a result  the graph and weights for the nodes are set as shown in figure 1 b . note that node is assigned a weight of because pred . the optimal wmis of is since its weight is greater than the sum of the weights of and . thus  after the first iteration .
in the second iteration  pred	is set to	in steps 1 since and	with pred
   . further  pred and pred . also  since pred   in steps 1  newpred and costchange predcost predcost
　　　　　　. in addition  since and are not predictors for a predicted attribute  costchange costchange . thus  the graph and weights for the nodes are set as shown in figure 1 c . the weight for is essentially matercost
	predcost	costchange
	and the weightfor	is matercost	predcost
         costchange	. the optimal wmis of	is	and thus	after the second iteration.
　finally  figure 1 d  illustrates during the third iteration - node has a weight of since pred and is used to predict both and . thus  while predicting  instead of materializing  results in a decrease of 1 in the cost of   the cost of predicting and using  instead of   increases increases by 1  thus resulting in a net increase in cost of 1. the algorithm terminates since weight of every node in is negative. the end result is a total storage cost of only 1  which is  in fact  the optimal solution for this instance. 
complexity of algorithm maxindependentset. analyzing the execution of algorithm maxindependentset  we can show that  in the worst case  it requires at most invocations of the wmis solution heuristic     and constructs at most cart predictors  where is the number of attributes in and is an upper bound on the number of predictor attributes used for any attribute in . further  under assumptions slightly less pessimistic than the worst case  it can be shown that our maxindependentset algorithm only needs to solve wmis instances and build cart predictors. the details of the analysis can be found in the full paper .
1 the cartbuilder component
             's cartbuilder component constructs a cart predictor for the attribute with as the predictor attributes. the cartbuilder's objective is to construct the smallest  in terms of storage space  cart model such that each predicted value  of a tuple's value for attribute   deviates from the actual value by at most   the error tolerance for attribute .
　if the predicted attribute is categorical  then 's cartbuildercomponentbuildsa compactclassificationtree with values of serving as class labels. cartbuilderemploys classification tree construction algorithms from  to first construct a low storage cost tree and then explicitly stores sufficient number of outliers such that the fraction of misclassified records is less than the specified error bound . thus  cartbuilderguarantees that the fraction of attribute 's values that are incorrectly predicted is less than .
	in the case of numeric predicted attributes	 	's
cartbuilderemploys a novel  efficient algorithm for constructing compact regression trees for predicting with an error that is guaranteed not to exceed . the key technical idea in our algorithm is to integrate building and pruning during the top-down construction of a guaranteed-error regression tree - this is achieved through a novel technique  based on dynamic programming  for computing a lower bound on the cost of a yet-to-be-expanded subtree. due to space constraints  the details of 's regression tree construction algorithm can be found in the full paper .
1 the rowaggregator component
             's cartselector component computes the set of attributes to predict and the cart models
         for predicting them. these models are stored in the compressed version of the table along with   the projection of table on predictor attributes. obviously  by compressing one could reduce the storage overhead of even further. however  while lossless compression algorithms can be used to compress without any problems  we need to be more careful when applying lossy compression algorithms to . this is because  with lossy compression  the value of a predictor attribute in may be different from its original value that was initially used to build the cart models. as a result  it is possible for errors that exceed the specified bounds  to be introduced into the values of predicted attributes. for instance  consider the table from example 1  shown in figure 1 a   and the carts in figure 1 b  contained in the compressed version of the table. suppose that the error tolerance for the salary attribute is 1 and after  lossy  compression  the salary value of 1 is stored as 1 in . consequently  since the classification tree in figure 1 b  is used to predict the value of the assets attribute  the value of the attribute would be wrongly predicted as 1  instead of 1   thus violating the error bound of 1.
             's rowaggregator component uses a fasciclebased algorithm  to further compress the table of predictor attributes. since fascicle-based compression is lossy  in the following  we show how the above-mentioned scenario can be avoided when compressing numeric attributes using fascicles. for a numeric predictor attribute   define value to be a split value for if is a split condition in some cart in . also  in a fascicle  set of records   we say that an attribute is compact if the range of -values in the fascicle  in addition to having width at most   also satisfies the property that for every split value   either or . in our fascicle-based compression algorithm  for each compact attribute   by using as the representative for -values in the fascicle  we can ensure that the error bounds for both predictor as well as predicted attributes are respected. in fact  we can show that the values for predicted attributes are identical prior to and after is compressed using fascicles. this is because for each tuple in   the original and compressed tuple traverse the same path in every cart . for instance  suppose that is a split condition in some cart and is different after compression. then  if
　　　　  it must be the case that for the fascicle containing   for the -value range   . thus  the compressed value for     must also be greater than . in a similar fashion  we can show that when   the compressed value of is also less than or equal to . thus  our more strict definition of compact attributes prevents errors in predictor attributes from rippling through the predicted attributes. further  the fascicle computation algorithms developed in  can be extended in a straightforward manner to compute fascicles containing compact attributes  according to our new definition .
1. experimental study
　in this section  we present the results of an extensive empirical study whose objective was to compare the quality of compression due to 's model-based approach with existing syntactic  gzip  and semantic  fascicles  compression techniques. we conducted a wide range of experiments with three very diverse reallife data sets in which we measured both compression ratios as well as running times for . the major findings of our study can be summarized as follows.
　better compression ratios. on all data sets  produces smaller compressed tables compared to gzip and fascicles. the compression due to is more effective for tables containing mostly numeric attributes  at times outperforming gzip and fascicles by a factor of 1  for error tolerances of 1% . even for error tolerances as low as 1%  the compression due to   on an average  is 1% better than existing schemes.
　small sample sizes are effective. for the data sets  even with samples as small as 1kb  1% of one data set   is able to compute a good set of cart models that result in excellent compression ratios. thus  using samples to build the bayesian network and cart models can speed up significantly.
　best algorithms for components. the maxindependentset cart-selection algorithm compresses the data more effectively that the greedy algorithm. further  since
spends most of its time building carts  between 1% and 1% depending on the data set   the integrated pruning and building of carts results in significant speedups to 's execution times.
　thus  our experimental results validate the thesis of this paper that is a viable and effective system for compressing massive tables. all experiments reported in this section were performed on a multi-processor  1mhz pentium processors  linux server with gb of main memory.
1 experimental testbed and methodology
compression algorithms. we consider three compression algorithms in our experimental study.
 gzip. gzip is the widely used lossless compression tool based on the lempel-ziv dictionary-based compression technique . we compress the table row-wise using gzip after doing a lexicographic sort of the table. we found this to significantlyoutperform the cases in which gzip was applied to a row-wise expansion of the table  without the lexicographic sort .
　fascicles. in   jagadish  madar and ng  describe two algorithms  single-k and multi-k  for compressing a table using fascicles. they recommend the multi-k algorithm for small values of  the number of compact attributes in the fascicle   but the single-k algorithm otherwise. in our implementation  we use the single-k algorithm as described in . the two main input parameters to the algorithm are the number of compact attributes    and the maximum number of fascicles to be built for compression  . in our experiments  for each individual data set  we used values of and that resulted in the best compression due to the fascicle algorithm. we found the single-k algorithm to be relatively insensitive to  similar to the finding reported in   and chose to be for all three data sets. however  the sizes of the compressed tables output by single-k did vary for different values of and so for the corel  forest-cover and census data sets  described below   we set to   and 1  respectively. note that these large values of
justify our use of the single-k algorithm. we also set the minimum size of a fascicle to 1% of the data set size. for each numeric attribute  we set the compactness tolerance to two times the input error tolerance for that attribute. however  since for categorical attributes  the fascicle error semantics differs from our's  we used a compactness tolerance of 1 for every categorical attribute.
　　　　　　. we implementedall componentsof the system as described in section . for the greedy cart-selection algorithm  we used value of 1 for the relative benefitparameter . in the maxindependentsetcart-selection algorithm  for finding the
wmis of the node-weighted graph   we used the qualex software package  www.busygin.dp.ua/npc.html . this software implements an algorithm based on a quadratic programming formulation of the maximum weighted clique problem . the running time is  where is number of vertices in the graph . in our experiments  qualex always found the optimal solution and accounted for a negligible fraction of the overall execution time. we also implemented the integrated building and pruning algorithm in the buildcart component  and used a simple lower bound of	for every  yet to be expanded  leaf node. finally  in the rowaggregator component  we employed the single-k fascicle algorithm  with set to and equal to two-thirds of the number of attributes in . in order to be fair in our comparison with fascicles  we set the error tolerance for categorical attributes to always be 1.
real-life data sets. we used the following real-life data sets with very different characteristics in our experiments.
 census.  www.bls.census.gov/  this data set was taken from the current population survey  cps  data  which is a monthly survey of about 1 households conducted by the bureau of the census for the bureau of labor statistics. each month's data contains around 1 tuples with 1 attributes  of which we used 1 categorical attributes  e.g.  education  race  and 1 numeric attributes  e.g.  age  hourly pay rate . in the final data set  we used data for 1 months  june through october 1  that contained a total of 1 tuples and occupied 1 mb of storage.
corel.  kdd.ics.uci.edu/databases/corelfeatures/ 
this data set contains image features extracted from a corel image collection. we used a mb subset of the dataset which contains the color histogram features of 1 photo images. this data set consists of 1 numerical attributes and contains 1 tuples.
　forest-cover.  kdd.ics.uci.edu/databases/covertype/  this data set contains the forest cover type for meter cells obtained from us forest service  usfs  region resource information system  ris  data. the mb data set contains 1 tuples  and numeric and categorical attributes that describe the elevation  slope  soil type  etc. of the cells.
default parameter settings. the critical input parameter to the compression algorithms is the error tolerance for numeric attributes  note that we use an error tolerance of 1 for all categorical attributes . the error tolerance for a numeric attribute is specified as a percentage of the width of the range of -values in the table. another important parameter to is the size of the sample that is used to select the cart models in the final compressed table. for these two parameters  we use default values of 1%  for error tolerance  and 1kb  for sample size   respectively  in all our experiments. note that 1kb corresponds to 1%  1% and 1% of the total size of the forest-cover  corel and census data sets  respectively. finally  unless stated otherwise always uses maxindependentset for cartselection and the integrated pruning and building algorithm for constructing regression trees.
1 experimental results
effect of error threshold on compression ratio. figure 1 depicts the compression ratios for gzip  fascicles and for the three data sets. from the figures  it is clear that outperforms both gzip and fascicles  on an average  by 1% on all data sets  even for a low error threshold value of 1%. the compression due to is especially striking for the corel data set that contains only numeric attributes. for high error tolerances  e.g.  1%   produces a compressed corel table that is almost a factor of 1 smaller than the compressed tables generated by gzip and fascicles  and a factor of 1 smaller than the uncompressed corel table. even for the census data set  which contains an equal number of numeric and categorical attributes  compresses better than fascicles for smaller and moderate error threshold values  e.g.  1% to 1% ; only for larger error bounds  e.g.  1%  do fascicles perform slightly better than	.

figure 1: effect of error threshold on compression ratio.
　the reason gzip does not compress the data sets as well is that unlike fascicles and it treats the table simply as a sequence of bytes and is completely oblivious of the error bounds for attributes. in contrast  both fascicles and exploit data dependencies between attributes and also the semantics of error tolerances for attributes. further  compared to fascicles which simply cluster tuples with approximately equal attribute values  carts are much more sophisticated at capturing dependencies between attribute columns. this is especially true when tables contain numeric attributes since carts employ semantically rich split conditions for numeric attributes like . another crucial difference between fascicle- and cart-based compression is that  when fascicles are used for compression  each tuple and as a consequence  every attribute value of a tuple is assigned to a single fascicle. however  in   a predictor attribute and thus a predictor attribute value  belonging to a specific tuple  can be used in a number of different carts to infer values for multiple different predicted attributes. thus  carts offer a more powerful and flexible model for capturing attribute correlations than fascicles. as a result  a set of cart predictors are able to summarize complex data dependencies between attributes much more succinctly than a set of fascicles. for an error constraint of 1%  the final corel
           -compressed table contains 1 carts that along with outliers  consume only 1 mb or 1% of the uncompressed table size. similarly  for the forest-cover data set  the number of predicted attributes in the compressed table is 1  1 numeric and 1 categorical  and the cart storage overhead  with outliers  is a measly 1 mb or 1% of the uncompressed table.
　the compression ratios for are even more impressive for larger values of error tolerance  e.g.  1%  since the storage overhead of carts + outliers is even smaller at these higher error values. for example  at 1% error  in the compressed corel data set  carts consume only 1 mb or 1% of the original table size. similarly  for forest-cover  the cart storage overhead reduces to 1 mb or 1% of the uncompressed table. the only exception is the census data set where the decrease in storage overhead is much steeper for fascicles than for carts. we conjecture that this is because of the small attribute domains in the census data that cause each fascicle to cover a large number of tuples at higher error threshold values.
effect of randomsample size on compression ratio. figure 1 a  illustrates the effect on compression ratio as the sample size is increased from 1kb to 1kb for the forest-cover data set. interestingly  even with a 1kb sample  which is about 1% of the total data set size  is able to obtain a compression ratio of approximately 1  which is about 1% better than the compression ratio for gzip and fascicles. further  note that increasing the sample size beyond 1kb does not result in significant improvements in compression quality. the implication here is that it is possible to infer a good set of models even with a small random sample of the data set. this is significant since using a small sample instead of the entire data set for cart model construction can significantly improve 's running time  see running time experiments described later .
effect of cart selection algorithm on compression ratio / running time. in table 1  we show the compression ratios and running times of 's cart-selection algorithms for the three data sets. we consider three cart-selection algorithms - greedy  maxindependentsetwith the neighborhoodfor a node set to it parents and maxindependentsetwith the neighborhood for a node set to it markov blanket  in the bayesian graph . from the table  it follows that the maxindependentset algorithms always compress better than the greedy algorithm. this is because the greedy algorithm follows a very  local  prediction strategy for each attribute  basing the decision on whether or not to predict an attribute solely on how well it is predicted by its materialized ancestors in the bayesian network graph. in contrast  the maxindependentset algorithm adopts a more  global  view when making a decision on whether to predict or materialize an attribute - specifically  it not only takes into account how well an attribute is predicted by attributes in its neighborhood  but also how well it predicts other attributes in its neighborhood. observe that  in general  the version of maxindependentset with the markov blanket as a node's neighborhood performs slightly better than maxindependentset with parents as the neighborhood.
　with respect to running times  in general  we found that maxindependentset with parents performs quite well across all the data sets. this is because  it constructsfew carts  1 for census  1 for corel and 1 for forest-cover  and since it restrictsthe neighborhood for each attribute to only its parents  each cart contains few predictor attributes. while greedy does build the fewest carts in most cases  1 for census  1 for corel and 1 for forest-cover   all the materialized ancestors of an attribute are used as predictor attributes when building the cart for the attribute. as a result  since close to 1% attributes are materialized for the data sets  each cart is built using a large number of attributes  thus hurting greedy's performance. finally  the performance of maxindependentset with markov blanket suffers since it  in some cases  constructs a large number of carts  1 for census  1 for corel and 1 for forest-cover . further  since the markov blanket for a node contains more attributes than simply its parents  the number of predictor attributes used in each cart for markov blanket is typically much larger. as a result  cart construction times for markov blanket are higher and overall execution times for markov blanket are less competitive.

data setcompression ratio/running time  sec greedywmis parent wmis markov corel1 / 11 / 11 / 1forest-cover1 / 1.1 / 1.1 / 1census1 / 11 / 1.1 / 1table 1: effect of cart selection algorithm on compression ratio/running time.
effect of error thresholdand samplesize on running time. in
figures 1 b  and 1 c   we plot the running times for
1
	1	1	1	1	1	1	1	1
	sample size  kb 	error threshold	sample size  kb 
figure 1: effect of error threshold and sample size on compression ratio/running time.
for a range of error threshold values and sample sizes. two trends in the figures that are straightforward to observe are that 's running time decreases for increasing error bounds  and increases for larger sample sizes. the reason for the decrease in execution time when the error tolerance is increased is that for larger error thresholds  carts contain fewer nodes and so cart construction times are smaller. for instance  cart construction times  which constitute approximately 1% of 's total execution time  reduce by approximately 1% as the error bound increases from 1% to 1%. note the low running times for on the corel data set.
　in figure 1 c   we plot 's running time against the random sample size instead of the data set size because 's
dependencyfinderand cartbuildercomponents whichaccount for most of 's running time  on an average 
1% and 1%  respectively  use the sample for model construction. makes very few passes over the entire data set  e.g.  for sampling  for identifying outliers in the data set for each selected cart and for compressing using fascicles   the overhead of which is negligible compared to the overhead of cart model selection. observe that 's performance scales almost linearly with respect to the sample size.
　finally  in experiments with building regression trees on the data sets  we found that integrating the pruning and building phases can result in significant reductions in 's running times.
this is because  integrating the pruning and building phases causes fewer regression tree nodes to be expanded  since nodes that are going to be pruned later are not expanded   and thus improves cart building times by as much as 1%.
1. conclusions
　in this paper  we have described the design and algorithms underlying   a novel system that exploits attribute semantics and data-mining models to effectively compress massive data tables. takes advantage of predictive correlations between the table attributes and the user- or application-specified error tolerances to construct concise and accurate cart models for entire columns of the table. to restrict the huge search space of possible carts  explicitly identifies strong dependencies in the data by constructing a bayesian network model on the given attributes  which is then used to guide the selection of promising cart models. unfortunately  as we have demonstrated in this paper  this model-selection problem is a strict generalization of an
　　-hard combinatorial problem  wmis ; thus  we have proposed a novel algorithm for 's cart-selection component that exploits the discovered bayesian structure in the data in conjunction with efficient  near-optimal wmisheuristics. 's cart-building component also employs novel integrated pruning strategies that take advantage of the prescribed error tolerances to minimize the computational effort involved. our experimentation with several real-life data sets has offered convincing evidence of the effectiveness of 's model-based approach.
