we consider the problem of semantic annotation of semi-structured documents according to a target xml schema. the task is to annotate a document in a tree-like manner where the annotation tree is an instance of a tree class defined by dtd or w1c xml schema descriptions. in the probabilistic setting  we cope with the tree annotation problem as a generalized probabilistic context-free parsing of an observation sequence where each observation comes with a probability distribution over terminals supplied by a probabilistic classifier associated with the content of documents. we determine the most probable tree annotation by maximizing the joint probability of selecting a terminal sequence for the observation sequence and the most probable parse for the selected terminal sequence.
1 introduction
the future of the world wide web is often associated with the semantic web initiative which has as a target a wide-spread document reuse  re-purposing and exchange  achieved by means of making document markup and annotation more machine-readable. the success of the semantic web initiative depends to a large extent on our capacity to move from rendering-oriented markup of documents  like pdf or html  to semantic-oriented document markup  like xml and rdf.
¡¡in this paper  we address the problem of semantic annotation of html documents according to a target xml schema. a tree-like annotation of a document requires that the annotation tree be an instance of the target schema  described in a dtd  w1c xml schema or another schema language. annotation trees naturally generalize flat annotations conventionally used in information extraction and wrapper induction for web sites.
¡¡the migration of documents from rendering-oriented formats  like pdf and html  toward xml has recently become an important issue in various research communities  1; 1; 1; 1 . the majority of approaches either make certain assumptions about the source and target xml documents  like a conversion through a set of local transformations   or entail the transformation to particular tasks  such as the semantic annotation of dynamically generated web pages in news portals  or the extraction of logical structure from page images .
¡¡in this paper  we consider the general case of tree annotation of semi-structured documents. we make no assumptions about the structure of the source and target documents or their possible similarity. we represent the document content as a sequence of observations x = {x1 ... xn}  where each observation xi is a content fragment. in the case of html documents  such a fragment may be one or multiple leaves  often surrounded with rich contextual information in the form of html tags  attributes  etc. the tree annotation of sequence x is given by a pair  y d   where y and d refer to leaves and internal nodes of the tree  respectively. the sequence y = {y1 ... yn} can be seen  on one side  as labels for observations in x  and on the other side  as a terminal sequence for tree d that defines the internal tree structure over y according to the target xml schema.
¡¡in supervised learning  the document annotation system includes selecting the tree annotation model and training the model parameters from a training set s given by triples  x y d . we adopt a probabilistic setting  by which we estimate the probability of an annotation tree  y d  for a given observation sequence x and address the problem of finding the pair  y d  of maximal likelihood.
¡¡we develop a modular architecture for the tree annotation of documents that includes two major components. the first component is a probabilistic context-free grammar  pcfg  which is a probabilistic extension to the corresponding  deterministic  xml schema definition. the pcfg rules may be obtained by rewriting the schema's element declarations  in the case of a dtd  or element and type definitions  in the case of a w1c xml schema  and the rule probabilities are chosen by observing rule occurrences in the training set  similar to learning rule probabilities from tree-bank corpora for nlp tasks. pcfgs offer the efficient inside-outside algorithm for finding the most probable parse for a given sequence y of terminals. the complexity of the algorithm is o n1 ¡¤|n|   where n is the length of sequence y and |n| is the number of non-terminals on the pcfg.
¡¡the second component is a probabilistic classifier for predicting the terminals y for the observations xi in x. in the case of html documents  we use the maximum entropy framework   which proved its efficiency when combining content  layout and structural features extracted from html documents for making probabilistic predictions p y  for xi.
¡¡with the terminal predictions supplied by the content classifier  the tree annotation problem represents the generalized case of probabilistic parsing  where each position i in sequence y is defined not with a specific terminal  but with a terminal probability p y . consequently  we consider the sequential and joint evaluations of the maximum likelihood tree annotation for observation sequences. in the joint case  we develop a generalized version of the inside-outside algorithm that determines the most probable annotation tree  y d  according to the pcfg and the distributions p y  for all positions i in x. we show that the complexity of the generalized inside-outside algorithm is o n1¡¤|n|+n¡¤|t|¡¤|n|   where n is the length of x and y  and where |n| and |t| are the number of non-terminals and terminals in the pcfg.
¡¡we also show that the proposed extension of the inside-outside algorithm imposes the conditional independence requirement  similar to the naive bayes assumption  on estimating terminal probabilities. we test our method on two collections and report an important advantage of the joint evaluation over the sequential one.
1 xml annotation and schema
¡¡xml annotations of documents are trees where inner nodes determine the tree structure  and the leaf nodes and tag attributes refer to the document content. xml annotations can be abstracted as the class t of unranked labeled rooted trees defined over an alphabet ¦² of tag names . the set of trees over ¦² can be constrained by a schema d that is defined using dtd  w1c xml schema or other schema languages.
¡¡dtds and an important part of w1c xml schema descriptions can be modeled as extended context-free grammars   where regular expressions over alphabet ¦² are constructed by using the two basic operations of concatenation ¡¤ and disjunction | and with occurrence operators    kleene closure        and +  a+ = a¡¤a  . an extended context free grammar  ecfg  is defined by the 1-tuple g =  t n s r   where t and n are disjoint sets of terminals and nonterminals in ¦²  ¦² = t ¡È n; s is an initial nonterminal and r is a finite set of production rules of the form a ¡ú ¦Á for a ¡Ê n  where ¦Á is a regular expression over ¦² = t ¡È n. the language l g  defined by an ecfg g is the set of terminal strings derivable from the starting symbol s of g. formally  l g  = {w ¡Ê ¦² |s   w}  where   denotes the transitive closure of the derivability relation. we represent as a parse tree d any sequential form that reflects the derivational steps. the set of parse trees for g forms the set t  g  of unranked labeled rooted trees constrained with schema g.
1 tree annotation problem
when annotating html documents accordingly to a target xml schema  the main difficulty arises from the fact that the source documents are essentially layoutoriented  and the use of tags and attributes is not necessarily consistent with elements of the target schema. the irregular use of tags in documents  combined with complex relationships between elements in the target schema  makes the manual writing of html-to-xml transformation rules difficult and cumbersome.
¡¡in supervised learning  the content of source documents is presented as a sequence of observations x = {x1 ... xn}  where any observation xi refers to a content fragment  surrounded by rich contextual information in the form of html tags  attributes  etc. the tree annotation model is defined as a mapping x ¡ú  y d  that maps the observation sequence x into a pair  y d  where y={y1 ... yn} is a terminal sequence and d is a parse tree of y according to the target schema or equivalent pcfg g  s   y. the training set s for training the model parameters is given by a set of triples  x y d . to determine the most probable tree annotation  y d  for a sequence x  we maximize the joint probability p y d|x g   given the sequence x and pcfg g. using the bayes theorem and the independence of x and
g  we have
¡¡¡¡¡¡¡¡¡¡p y d|x g  = p d|y g  ¡¤ p y|x    1  where p y|x  is the probability of terminal sequence y for the observed sequence x  and p d|y g  is the probability of the parse d for y according the pcfg g. the most probable tree annotation for x is a pair  y d  that maximizes the probability in  1  
	.	 1 
¡¡in the following  we build a probabilistic model for tree annotation of source documents consisting of two components to get the two probability estimates in  1 . the first component is a probabilistic extension of the target xml schema; for a given terminal sequence y  it finds the most probable parse p d|y g  for sequences according to the pcfg g  where rule probabilities are trained from the available training set. the second component is a probabilistic content classifier c  it estimates the conditional probabilities p y|xi  for annotating observations xi with terminals y ¡Ê t. finally  for a given sequence of observations x  we develop two methods for finding a tree annotation  y d  that maximizes the joint probability p y d|x g  in  1 .
1 probabilistic context-free grammars
pcfgs are probabilistic extensions of cfgs  where each rule a ¡ú ¦Á in r is associated with a real number p in the half-open interval  1; 1 . the values of p obey the restriction that for a given non-terminal a ¡Ê n  all rules for a must have p values that sum to 1 
	.	 1 
¡¡pcfgs have a normal form  called the chomsky normal form  cnf   according to which any rule in r is either a ¡ú b c or a ¡Ê b  where a  b and c are nonterminals and b is a terminal. the rewriting of xml annotations requires the binarization of source ranked trees  often followed by an extension of the nonterminal set and the underlying set of rules. this is a consequence of rewriting nodes with multiple children as a sequence of binary nodes. the binarization rewrites any rule a ¡ú b c d as two rules a ¡ú bp and p ¡ú c d  where p is a new non-terminal.
¡¡a pcfg defines a joint probability distribution over y and d  random variables over all possible sequences of terminals and all possible parses  respectively. y and d are clearly not independent  because a complete parse specifies exactly one or few terminal sequences. we define the function p y d  of a given terminal sequence y ¡Ê y and a parse d ¡Ê d as the product of the p values for all of the rewriting rules r y d  used in s   y. we also consider the case where d does not actually correspond to y 
	 	if d is a parse of y
 y	  = 1 	otherwise.
¡¡the values of p are in the closed interval  1; 1 . in the cases where d is a parse of y  all p r  values in the product will lie in the half open interval  1; 1   and so will the product. in the other case  1 is in  1; 1  too. however  it is not always the case that
¡¡the pcfg training takes as evidence the corpus of terminal sequences y with corresponding parses d from the training set s. it associates with each rule an expected probability of using the rule in producing the corpus. in the presence of parses for all terminal sequences  each rule probability is set to the expected count normalized so that the pcfg constraints  1  are satisfied:
.
1 generalized probabilistic parsing
pcfgs are used as probabilistic models for natural languages  as they naturally reflect the  deep structure  of language sentences rather than the linear sequences of words. in a pcfg language model  a finite set of words serve as a terminal set and production rules for non-terminals express the full set of grammatical constructions in the language. basic algorithms for pcfgs that find the most likely parse d for a given sequence y or choose rule probabilities that maximize the probability of sentence in a training set  represent  efficient extensions of the viterbi and baum-welsh algorithms for hidden markov models.
¡¡the tree annotation model processes sequences of observations x = {x1 ... xn} from the infinite set x  where the observations xi are not words in a language  and therefore terminals in t  but complex instances  like html leaves or groups of leaves.
¡¡content fragments are frequently targeted by various probabilistic classifiers that produce probability estimates for labeling an observation with a terminal in
t  p y|xi   y ¡Ê t  where   = 1. the tree annotation problem can therefore be seen as a generalized version of probabilistic context-free parsing  where the input sequence is given by the probability distribution over a terminal set and the most probable annotation tree requires maximizing the joint probability in  1 .
¡¡a similar generalization of probabilistic parsing takes place in speech recognition. in the presence of a noisy channel for speech streams  parsing from a sequence of words is replaced by parsing from a word lattice  which is a compact representation of a set of sequence hypotheses  given by conditional probabilities obtained by special acoustic models from acoustic observations .
1 content classifier
to produce terminal estimates for the observations xi  we adopt the maximum entropy framework  according to which the best model for estimating probability distributions from data is the one that is consistent with certain constraints derived from the training set  but otherwise makes the fewest possible assumptions . the distribution with the fewest possible assumptions is one with the highest entropy  and closest to the uniform distribution. each constraint expresses some characteristic of the training set that should also be present in the learned distribution. the constraint is based on a binary feature  it constrains the expected value of the feature in the model to be equal to its expected value in the training set.
¡¡one important advantage of maximum entropy models is their flexibility  as they allow the extension of the rule system with additional syntactic  semantic and pragmatic features. each feature f is binary and can depend on y ¡Ê t and on any properties of the input sequence x. in the case of tree annotation  we include the content features that express properties on content fragments  like f1 x y  = 1 if y is title and x's length is less then 1 characters  1 otherwise   as well as the structural and layout features that capture the html context of the observation x  like f1 x y = 1 if y is author and x's father is span  1 otherwise .
¡¡with the constraints based on the selected features f x y   the maximum entropy method attempts to maximize the conditional likelihood of p y|x  which is represented as an exponential model:
	  	 1 
where z¦Á x  is a normalizing factor to ensure that all the probabilities sum to 1 
	.	 1 
¡¡for the iterative parameter estimation of the maximum entropy exponential models  we use one of the quasi newton methods  namely the limited memory bfgs method  which is observed to be more effective than the generalized iterative scaling  gis  and improved iterative scaling  iis  for nlp and ie tasks .
1 sequential tree annotation
we use pairs  x y  from triples  x y d  of the training set s to train the content classifier c and pairs  y d  to choose rule probabilities that maximize the likelihood for the instances in the training set. c predicts the terminal probabilities p y|x  for any observation x  while the inside-outside algorithm can find the parse d of the highest probability for a given terminal sequence y.
¡¡by analogy with speech recognition  there exists a naive  sequential method to combine the two components c and g for computing a tree annotation for sequence x. first  from c's estimates p y|x   we determine the  top k  most probable sequences ymax j for x  j = 1 ... k. second  we find the most probable parses for all y ; and fi-
nally  we choose the pair  ymax j dmax j  that maximizes the product p ymax j  ¡Á p dmax j .
¡¡the sequential method works well if the noise level is low  in speech recognition  or if the content classifier  in the tree annotation  is accurate enough in predicting terminals y for xi. unfortunately  it gives poor results once the classifier c is far from 1% accuracy in y predictions  as it faces the impossibility of finding any parse for all the top k most probable sequences ymax j.
example. consider an example target schema given by the following dtd:
 !elementbook author  section+   !element
 !elementsection author title   para | footnote +  
 #pcdata   !elementtitle #pcdata   !elementpara #pcdata   !elementfootnote #pcdata  ¡¡the reduction of the above schema definition to the chomsky normal form will introduce extra nonterminals  so we get the pcfg g =  t n s r   where the terminal set is t={author  title  para  footnote}  the nonterminal set is n= {book  author  se  section  ti  els  el}  s= book  and r includes twelve production rules.
¡¡assume that we have trained the content classifier c and the pcfg g and have obtained the following probabilities for the production rules in r:
 1  1  1  1  1  1  sectionelsauelsebook¡ú¡ú¡ú¡ú¡úsection sectionparaauthor¡úel elau sectionti els 1  1  1  1  1  1  eltibooksectionseels¡ú¡ú¡ú¡ú¡útitlesection sefootnote¡úel elsau seti el.¡¡assume now that we test the content classifier c and pcfg g on a sequence of five unlabeled observations x = {x1 ... x1}. let the classifier c estimate the probability for terminals in t as given in the following table:
x1x1x1x1x1author11111title11111para11111footnote11111¡¡according to the above probability distribution  the most probable terminal sequence ymax is composed of the most probable terminals for all xi  i = 1 ... 1. it is 'title title para footnote title' with probability p ymax  = p ymax|x  = ¦°i ¡¤ p yimax|xi  = 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 = 1. however  ymax has no corresponding parse tree in g. instead  there exist two valid annotation trees for x   y1 d1  and  y1 d1   as shown in figure 1. in figure 1.b  the terminal sequence y1='author title para title para' with the parse d1=book au se section  ti el  section  ti el    maximizes the joint probability p y d|x g   with p y1  = 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 = 1  and p d1 =p book ¡ú au se  ¡¤ p au ¡ú author  ¡Á p se ¡ú section section  ¡¤ p section ¡ú ti el  ¡Á p ti ¡ú title 1 ¡¤ p el ¡ú para 1 ¡Á p section ¡ú ti el 
=1 ¡¤ 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 ¡¤ 1 = 1.
jointly  we have p y1 ¡Áp d1  ¡Ö 1¡¤1. similarly  for the annotation tree in figure 1.a  we have p y1 ¡Áp d1  = 1 ¡¤ 1 ¡Ö 1 ¡¤ 1.
	book	a 	b 	book

y1 author title para footnote para	y1 author title	para title para
x	x1	x1	x1	x1	x1	x	x1	x1	x1	x1 x1
figure 1: tree annotations for the example sequence.
1 the most probable annotation tree
as the sequential method fails to find the most probable annotation tree  we try to couple the selection of terminal sequence y for x with finding the most probable parse d for y  such that  y d  maximizes the probability product p d|y g ¡¤p y|x  in  1 . for this goal  we extend the basic inside-outside algorithm for terminal pcfgs . we redefine the inside probability as the most probable joint probability of the subsequence of y beginning with index i and ending with index j  and the most probable partial parse tree spanning the subsequence yij and rooted at nonterminal a:

¡¡the inside probability is calculated recursively  by taking the maximum over all possible ways that the nonterminal a could be expanded in a parse 

¡¡to proceed further  we make the independence assumption about p y|x   meaning that for any q  i ¡Ü q ¡Ü j  we have p yij|x  = p yiq|x  ¡¤ p yqj+1|x . then  we can rewrite the above as follows
¡¡¡¡ 1  x 	 1 
= maxi¡Üq¡Üjp a ¡ú bc  ¡¤ ¦Âb i q  ¡¤ ¦Âc q + 1 j  1 
¡¡the recursion is terminated at the ¦Âs 1 n  which gives the probability of the most likely tree annotation  y d  
 
where n is the length of both sequences x and y.
¡¡the initialization step requires some extra work  as we select among all terminals in t being candidates for yk:
	¦Âa k k  = maxykp a ¡ú yk  ¡¤ p yk|x .	 1 
¡¡it can be shown that the redefined inside function converges to a local maximum in the  y d  space. the extra work during the initialization step takes o n ¡¤ |t| ¡¤ |n|  time which brings the total complexity of the extended io algorithm to o n1 ¡¤ |n| + n ¡¤ |t| ¡¤ |n| .
¡¡the independence assumption established above represents the terminal conditional independence  p y|x  =   and matches the naive bayes assumption. the assumption is frequent in text processing; it simplifies the computation by ignoring the correlations between terminals. here however it becomes a requirement for the content classifier. while pcfgs are assumed to capture all  short- and long- distance  relations between terminals  the extended inside algorithm  1 - 1  imposes the terminal conditional independence when building the probabilistic model. this impacts the feature selection for the maximum entropy model  by disallowing features that include terminals of neighbor observationsmum entropy extension with hmm and crf modelsyi 1  yi+1  etc  as in the maxi- 1; 1 .
1 experimental results
we have tested our method for xml annotation on two collections. one is the collection of 1 shakespearean plays available in both html and xml format.1 scenes with 1 to 1 leaves were randomly selected for the evaluation. the dtd fragment for scenes consists of 1 terminals and 1 non-terminals. after the binarization  the pcfg in cnf contains 1 non-terminals and 1 rules.
¡¡the second collection  called techdoc  includes 1 technical documents from repair manuals. 1 the target documents have a fine-grained semantic granularity and are much deeper than in the shakespeare collection; the longest document has 1 leaves. the target schema is given by a complex dtd with 1 terminals and 1 nonterminals. the binarization increased the number of non-terminals to 1. for both collections  a content observation refers to a pcdata leaf in html.
¡¡to evaluate the annotation accuracy  we use two metrics. the terminal error ratio  ter  is similar to the word error ratio used in natural language tasks; it measures the percentage of correctly determined terminals in test documents. the second metric is the non-terminal error ratio  ner  which is the percentage of correctly annotated sub-trees.
¡¡as content classifiers  we test with the maximum entropy  me  classifier. for the me model  we extract 1 content features for each observation  such as the number of words in the fragment  its length  pos tags  textual separators  etc. second  we extract 1 layout and structural features include surrounding tags and all associated attributes. beyond the me models  we use the maximum entropy markov models  memm  which extends the me with hidden markov structure and terminal conditional features . the automaton structure used in memm has one state per terminal.
¡¡in all tests  a cross-validation with four folds is used. me and memm were first tested alone on both collections. the corresponding ter values for the most probable terminal sequences ymax serve a reference for methods coupling the classifiers with the pcfg. when coupling the me classifier with the pcfg  we test both the sequential and joint methods. additionally  we included a special case memm-pcfg where the content classifier is memm and therefore the terminal conditional independence is not respected.
¡¡the results of all the tests are collected in table 1. the joint method shows an important advantage over the sequential method  in particular in the techdoc case  where the me content classifier alone achieves 1% accuracy and the joint method reduces the errors in terminals by 1%. instead  coupling memm with the pcfg reports a decrease of ter values and a much less important ner increase.

1 http://metalab.unc.edu/bosak/xml/eg/shaks1.zip.
1
available from authors on request.
methodtechdocshakespeareternerternerme1-1-memm1-1-seq-me-pcfg1111jnt-me-pcfg1111jnt-memm-pcfg1111table 1: evaluation results.
1 relevant work
since the importance of semantic annotation of documents has been widely recognized  the migration of documents from rendering-oriented formats  like pdf and html  toward xml has become an important research issue in different research communities  1; 1; 1; 1 . the majority of approaches either constrain the xml conversion to a domain specific problem  or make different kinds of assumptions about the structure of source and target documents. in   the source html documents are assumed to be dynamically generated through a form filling procedure  as in web news portals  while a portal subject ontology permits the semantic annotation of the generated documents.
¡¡transformation-based learning is used for automatic translation from html to xml in . it assumes that source documents can be transformed into target xml documents through a series of proximity tag operations  including insert  replace  remove and swap. the translation model trains a set of transformation templates that minimizes an error driven evaluation function.
¡¡in document analysis research  ishitani in  applies ocr-based techniques and the xy-cut algorithm in order to extract the logical structure from page images and to map it into a pivot xml structure. while logical structure extraction can be automated to a large extent  the mapping from the pivot xml to the target xml schema remains manual.
¡¡in natural language tasks  various information extraction methods often exploit the sequential nature of data to extract different entities and extend learning models with grammatical structures  like hmm  or undirected graphical models  like conditional random fields . moreover  a hierarchy of hmms is used in  to improve the accuracy of extracting specific classes of entities and relationships among entities. a hierarchical hmm uses multiple levels of states to describe the input on different level of granularity and achieve a richer representation of information in documents.
1 conclusion
we propose a probabilistic method for the xml annotation of semi-structured documents. the tree annotation problem is reduced to the generalized probabilistic context-free parsing of an observation sequence. we determine the most probable tree annotation by maximizing the joint probability of selecting a terminal sequence for the observation sequence and the most probable parse for the selected terminal sequence.
¡¡we extend the inside-outside algorithm for probabilistic context-free grammars. we benefit from the available tree annotation that allows us to extend the inside function in a rigorous manner  and avoid the extension of the outside function which might require approximation.
¡¡the experimental results are promising. in future work  we plan to address different challenges in automating the html-to-xml conversion. we are particularly interested in extending the annotation model with the source tree structures that have been ignored so far.
1 acknowledgement
this work is supported by vikef integrated project co-funded under the eu 1th framework programme.
