this paper presents a study of three statistical query translation models that use different units of translation. we begin with a review of a word-based translation model that uses cooccurrence statistics for resolving translation ambiguities. the translation selection problem is then formulated under the framework of graphic model resorting to which the modeling assumptions and limitations of the co-occurrence model are discussed  and the research of finding better translation units is motivated. then  two other models that use larger  linguistically motivated translation units  i.e.  noun phrase and dependency triple  are presented. for each model  the modeling and training methods are described in detail. all query translation models are evaluated using trec collections. results show that larger translation units lead to more specific models that usually achieve better translation and cross-language information retrieval results. 
categories and subject descriptors general terms keywords 
h.1  information storage and retrieval : retrieval models  
design  algorithms  theory  experimentation 
query translation  cross-language information retrieval  statistical models  linguistic structures 
1. introduction 
query translation is a long standing research topic in the community of cross-language information retrieval  clir . assume that a query is translated using a bilingual dictionary  there are two fundamental research tasks:  1  how to improve the coverage of the bilingual dictionary; and  1  how to select the correct translation of the query among all the translations provided by the dictionary. the second task is also called the problem of translation selection  and is the focus of this paper. 
   we limit our discussion in this paper to statistic models for translation selection. to make a statistical model trainable  we always decompose the translation of a query into a sequence of smaller translation units. given that we will use statistical models to model translation  one important question is: what unit of translation should a statistical model represent   
   a natural unit is words since they are the smallest meaningful linguistic unit. however  a word-based model  though simple to train  is always over-general and may lead to too many translation ambiguities for resolving. in theory  larger units such as phrases are more specific and lead to less translation ambiguities. however they pose bigger challenges in model structuring and training. 
   this paper studies three statistical query translation models that use different translation units. we will begin with a review of a classical word-based translation model that uses cooccurrence statistics to resolve translation ambiguities. then  we will formulate the translation selection problem under the framework of graphic model  gm . we will discuss the modeling assumptions and limitations of the co-occurrence model  and motive our research of finding better translation units.  
   we will then present two other models that use larger  linguistic-motivated translations units. they are noun phrases  nps  and dependencies. a dependency  represented as a triple  is a pair of words that have a syntactic dependency relation  such as verb-objective. in both models  we assume that the selection of a translation only depends upon other selected translations in the same unit. while nps capture dependence of adjacent words in a query  dependency triples can capture syntactic dependences between non-adjacent words. though similar models have been proposed earlier  we will refine those using recent advances in the research community of statistical machine translation  smt . we will show that  1  np and dependency translation can be performed using a reranking approach based on a linear model;  1  the linear model provides a flexible statistical framework to incorporate various kinds of information  defined as feature functions  for resolving translation ambiguities;  1  the parameters of the linear model can be learned discriminatively so as to optimize the translation quality directly  and  1  most effective feature functions used in the linear model can be derived from generative models that are traditionally used in smt  thus the ranking approach provides an appropriate framework to combine the strengths of both generative models and discriminative training methods. 
   we evaluate our query translation models using trec collections. to our knowledge  this is the first systematic comparison of those models on the task of english to chinese clir on gold test sets. we shall demonstrate that linguistic units such as 

 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  august 1  1  seattle  washington  usa. 
copyright 1 acm 1-1/1...$1. 
np and dependency triples are beneficial to query translation if they can be detected and used properly.  
1. co-occurrence model 
a co-occurrence model uses words as the unit of translation. the basic principle of the model is that correct translations of query words tend to co-occur in the target language and incorrect translations do not. therefore  for a given query word in source language  i.e.  english in this study   the likelihood of its translation  i.e.  chinese  is measured via the similarity between a translation candidate  e.g.  provided by a bilingual dictionary  and the other selected translations in the query. the definition of similarity between words can take different forms of cooccurrence statistics. mutual information is among the most commonly used ones .  
   the advantage of the co-occurrence model is that it is easy to train. there is no need to measure cross-language word similarities  e.g.  translation probabilities . only relationships between words of the same language are used. they can be obtained through co-occurrence statistics in a monolingual text corpus. the disadvantage of the model is that it is difficult to find an efficient algorithm that optimizes exactly the translation of a whole query according to the model. we now describe it in detail. 
   given the measurement of term similarity  ideally  we should select for each query term the translation that co-occurs the most often with  or the most similar to  the selected translations of other terms in the same query. however  finding such an optimal translation is computationally very expensive  as will be described below. therefore  one has to use an approximate greedy algorithm as follows  1  1  1 : 
 1  given an english  source language  query e = {e1  e1  ...  en}  for each query term e  we define a set of m distinct chinese translations according to a bilingual dictionary 
d: d ei  = {ci 1  ci 1  ...  ci m} 
 1  for each set d ei  
 a  for each translation ci j ∈ d ei   define the similarity score between the translation wi j and a set d ek   k ≠ i  set as the sum of the similarities between ci j and each translations in the set d ek  according to eq.  1  
 	 	sim ci j  d ek    = ∑sim ci j  ck l  	 1  
ck  l∈d ek  
 b  compute the cohesion score for ci j as 
 	 	cohesion ci j|e d  = log  ∑sim ci j  d ek    	 1  
d ek  
 c  select the translation c ∈ d ei  with the highest cohesion score 
 	 	c = arg maxcohesion ci j|e d  	 1  
ci  j∈d ei  
   apparently  the above algorithm is sub-optimal. as pointed out in   the cohesion score for a translation as in eq.  1  is computed with regard to all possible translations of other query terms. it does not differentiate correct translations from incorrect ones. as a result  the translation of different query terms is determined independently. in spite of the deficiency  the greedy search algorithm has been widely used since an exact algorithm is prohibitively expensive. in the next section  we will formulate the translation selection problem under the framework of gm  e.g.  1   and discuss the underlying assumptions of the greedy algorithm. 
1. gm view 
a query translation model can be viewed as an undirected gm. for example  figure 1 shows a query translation model of a 1term query. each node represents a distribution of a translation set of a query term. the edges of the graph represent a set of independency assumptions among query term translations. the 
task of query translation is to find a set of 
 
 
 
 
 a  
 
 
 	w1 	w1 
 
 
 b  
 
 	w1 w1 
  	 
figure 1. gms the co-occurrence query translation model  a  and its approximation  b . 
 
translations 	that 	maximize 	the 	joint 	probability 
p w1 w1 w1 w1 w1 . 
   the gm view illustrates three research tasks of query translation. the first is how to generate translation candidates for each term  and how to model the distribution of the candidates. traditionally  a bilingual dictionary is used and all translations of a query term are assumed to be uniformly distributed. we may also induce a distribution using a statistical translation model learned from parallel bilingual corpora. 
   the second is how to determine the graph topology  i.e.  what independence assumptions we may use. the third is how to compute the joint probability. these two problems are closely related. the efficiency of the joint probability computing largely depends on the graph topology.  
   in the co-occurrence model as described above  we assume that the selection of each translation is consistent with the selected translations for other query terms. therefore  we assume that the five nodes form a clique as shown in figure 1  a .  suppose that we wish to compute the marginal probability p w1 . we obtain this marginal by summing over the other variables as: 
	p 	 
　　　　　　　　　　　　　　　　　　1	1 where h .  is a feature function  and z is a normalization factor.   
   we see that the computational complexity of p w1  scales as d1  assuming that each query term has d possible translations . this is prohibitively expensive even for a very short query. we therefore resort to an approximated word selection algorithm as described in section 1 by introducing a translation independence assumption. the corresponding gm is shown in figure 1  b . now  p w1  can then be factored as: 
p	 1  
notice that if we define h .  in equation  1  as the similarity between two words  the idea behind equation  1  is similar to that of equation  1   z can be removed when the probability is used to rank translation candidates   where no more than two variables appear together in any summand  and thus the computational complexity is reduced to d1. however  as discussed earlier  the reduction of complexity may come with the sacrifice of accuracy due to the independence assumption used. 
   in general  the computation complexity depends on the largest size of the clique in the graph. the np and dependency translation models described in sections 1 and 1 are used to implement the idea that the linguistic structure of a sentence can be utilized to identify cliques. linguistic units  such as nps or dependency triples  can be translated as unit and the translation can be done accurately using only internal information of the unit. as a consequence  the graph would be divided into a few smaller sub-graphs. the probability of each sub-graph can be inferred independently  with an optimal order that leads to a lower computation complexity.  
   using the three translation models that we propose in this paper  our query translation process can be cast in a sequential manner as follows. 
  identify nps and dependency triples of a query. 
  translate words in nps using the np translation model described in section 1. 
  translate words in dependencies using the dependency translation model described in section 1. 
  translate remaining words using the co-occurrence model. 
1. reranking approach 
this section describes the reranking approach which is the fundamental modeling framework for both np and dependency translation models.  
   given an n-term english query e = {e1  e1  ...  en}  we assume some way of detecting linguistic structures s of e. we also assume some way of generating a set of candidate chinese translations c  denoted by gen e . the task of a query translation model is to assign a score for each of the translation candidates in gen e  and select the one with the highest score: 
	c* = arg maxscore c e s  	 1  
c∈gen e 
   in this study  we assume that the score is assigned via a linear model  which consists of  1  a set of d feature functions that map  c  e  s  to a real value  i.e.  fd c  e  s   for d = 1...d; and  1  a set of parameters  each for one feature  λi for i = 1...d. then the decision rule of equation  1  can be rewritten as 
d
c* = argmax  λd fd c e s  	 1  c	d=1
notice that the linear model of equation  1  is a very general framework . for example  the source-channel models for smt  can be viewed as a special case of the linear model if we define both source model and channel model as feature functions. we shall show that most feature functions can be derived from generative models which are traditionally used in the framework of source-channel models for smt. the values of those feature functions are  log  probabilities that are learned from large monolingual or bilingual corpora via mle. therefore  those features are more informative than binary features that are traditionally used in linear models for classification problems . 
   the model weights λ  as shown in equation  1   are estimated using an iterative procedure that is used for multidimensional function optimization . assume that we can minimize query translation errors with respect to one parameter λ using line search. the procedure works as follows: take λ1  λ1  ...  λn as a set of directions. using line search  move along the first direction so that the number of translation errors on training data is minimized; then move from there along the second direction to the minimal error rate  and so on. cycling through the whole set of directions as many times as necessary  until the error number stops decreasing. in our experiments  we found that the procedure can converge on different minima given different starting points. we thus perform the procedure multiple times  each from a different  random starting point  and pick the parameter setting that achieves the minimal errors. note that this optimization approach is limited to a very small number of model parameters. efficient algorithms for tuning a larger number of model parameters can be found in  1  1 . 
   in the next two sections we will describe in turn the np translation model and the dependency translation model. both models are of the form of linear models in equation  1 . for each model  we will first describe a generative translation model  consisting of a series of component models  under the framework of source-channel models. then  we derive feature functions  e.g.  from the component models  used in the linear models. 
1. np translation model 
the use of np as a unit of translation is motivated by two observations. first  most english nps are translated to chinese as nps. for example  on a 1k-sentence-pair word-aligned english-chinese bilingual corpus  we found more than 1% of english nps being aligned to their translated chinese nps. second  as pointed out in   word selection can almost always be resolved depending solely upon the internal context of the np. 
   the use of translation template between english and chinese np patterns is the fundamental to our np translation model. for example  a  nn-1 nn-1  english phrase is usually translated into a  nn-1 nn-1  sequence in chinese  and a  nn-1 of nn-1  phrase is usually translated into a  nn-1 nn-1  sequence in chinese. the concept of translation templates is very similar to that of alignment templates in . formally  a np translation template  denoted by z  is a triple  e  c  a   which describes the alignment a between an english np pattern e and a chinese np pattern c. the alignment a is represented as a set of pairs  i  j   indicating that the i-th english word class in e is connected to the j-th chinese word class in c. either i or j can be empty  denoted by ε  indicating that an english  or chinese  word class is connected to no chinese  or english  word class. 
   in our experiments  translation templates are extracted from a word-aligned bilingual corpus. we first used an in-house parser to tag pos  base np  and complex np for english sentences. then  for each english np pattern e  we extracted its translated chinese np patterns c and the alignment a. an example is shown in figure 1  where  a  is an english sentence with each word marked by its pos tag and position and elements within  ...  are base nps  or complex nps;  b  is the aligned chinese sentence that has been segmented into a sequence of words;  c  shows the word alignment between the english and chinese sentences; and  d  shows three translation templates extracted respectively for two base nps and for the whole phrase. notice that the word positions in the alignments 
 a    the/dt/1 sales nnp/1  of/in/1  chinese/nnp/1 ships/nnp/1    ...  b   中国/1 船舶/1 销售/1  c   1  ε   1  1   1  ε    1  1   1  1   d  z1 =  e =  dt nnp   c =  nnp   a = { 1  ε    1 }  
z1 =  e =  nnp-1 nnp-1   c =  nnp-1 nnp-1   a = { 1  1    1  1 }  z1 =  e =  base-np-1 of base-np-1   c =  base-np-1 base-np-1   a = { 1  
1    1  ε    1  1 }  figure 1. np translation templates patterns shown in  d  are those in e and c of each z. also notice that translation templates can be recursively defined. 
1 generative model 
given an english np e  we search among all possible translations the most probable chinese np c* as 
	c* = argmaxp c|e  = argmaxp c p e|c  	 1  
	c	c
here  p c  is the chinese language model probability estimated via a trigram model as 
	p c  =p c1  p c1 |c1   ∏p c j |c j 1c j 1   	 1  
j=1...j
p e|c  is the translation probability. formally  the np translation template z is introduced as a hidden variable as 
	p e|c  =∑p z|c p e|z c  	 1  
z
hence  there are two probabilities to be estimated. the probability p z|c  to apply a translation template and the probability p e|z  c  to use a translation template for word selection.  
   first  we describe the way p z|c  is estimated. recall that z =  e  c  a   we call z applicable to c if c matches the np pattern c. let c c  z  be the number of occurrences of c to which z is applicable and c c  be the number of occurrences of c in training data. p z|c  is estimated as 
	p z|c  = c c z  	 1  
c c 
   second  we describe the way p e|z  c  is estimated. we assume that the english words are translated independently. we then decompose the probability as 
	p e|z c  =p e| e c a  c  = ∏p ei |c j   	 1  
 i  j ∈a
here  p e|c  is a translation probability estimated by relative frequencies: 
	p e|c  = c c e  	 1  
c c 
where c c  e  is the frequency that the word c is aligned to the word e  and c c  is the frequency of word c in training data. 
   notice that the model of equation  1  is a deficient model since the constraint Σep e|z  c  = 1 does not hold  as discussed in . however it is not necessary to normalize it since we only use the model as a feature function for ranking translation candidates. we also notice that it is possible to define an alignment in a at the level of base np such as z1 in figure 1  d . as shown in figure 1  d   we assume that all alignments in a are pairs of word positions. therefore  when we apply a in np translation  we recursively map each alignment pair of base np position to a set of pairs of word positions. for example  the pair   1  1  in z1 in figure 1  d   which is an alignment between the positions of two base np  can be mapped into a set of word position pairs using the alignment of z1. 
substituting equation  1  into equation  1   we finally get 
	c* = arg maxp c ∑p z|c p e|z c  	 1  
	c	z
   notice that different component models in equation  1  are trained on different corpora of different sizes. the dynamic value ranges of different component model probabilities can be so different  e.g.  p e|z  c  of equation  1  is not a probability but a score  that it is inappropriate to combine all these models through simple multiplication as in equation  1 . moreover  models that are poorly trained  e.g.  due to lack of training data  should be less weighted than well-trained ones. one way to balance the impact of these models is to introduce for each component model a model weight λ to adjust the model score p .  to p . λ. in our experiments  these weights are optimized so as to minimize the np translation errors on training data under the framework of linear models. it is thus worth noticing that the source-channel models are the rationale framework behind the np translation model. linear models are just another representation based on which we describe the optimization algorithm of model weights. 
1 feature functions 
we used three feature functions. they are derived from the above three component models in equation  1   respectively. 
1. chinese language model feature. it is defined as the logarithm of the chinese trigram model of equation  1   i.e.  hlm c  = logp c  = log p c1 p c1 Πi=1...jp cj|cj-1 cj-1 . 
1. translation template selection model feature. it is defined as the logarithm of p z|c   i.e.  hts z  c  = logp z|c . 
1. word selection model feature. it is defined as the logarithm of p e|z  c  of equation  1   i.e.  hws e  z  c  = logp e| e  c  a   c  = logΠ i j ∈ap ei|cj . 
notice that the linear model of equation  1  does not take into account the sum on z in equation  1   because considering the sum in decoding directly is computationally expensive. therefore  we approximate the sum during decoding: given an english np e  we take the following steps to search for the best chinese translation. 
1. template matching. we find all translation templates that are applicable to the given english np. 
1. candidate generating. for each translation template  we determine a set of chinese words for each english word position. the set of chinese words are all possible translations of the english word  stored in a bilingual dictionary. we then form a lattice for each e. 
1. searching. for each lattice  we use a best-first decoder to find top n translation candidates according to equation  1  where only two features  hlm and hws  are used. 
1. fusion and reranking. we fusion all retained translation candidates  and rerank them according to equation  1   where all features are applied. 
   we can see that the sum on z is approximated in two steps: first  for each z  we find the best translation. second  we select the translation among all retained best translations according to the linear model. 
1. dependency translation model 
a dependency is denoted by a triple  w1  r  w1   representing a syntactic dependency relation r between two words w1 and w1. among all the dependency relations  we only consider the four types that can be detected precisely using our parser and cannot be handled by the np translation model:  1  subject-verb   1  verb-object   1  adjective-noun  and  1  adverb-verb. 
   similar to that of the np translation model  the dependency translation model is also developed based on two hypotheses. first  dependencies have the best cohesion properties across languages . that is  dependency representation usually remains in the translations  and an ideal query translation should contain the same syntactic dependences as in the original query. second  word selection can mostly be resolved via the internal context of the dependency.  
   it is our observation that there is a strong correspondence in dependency relations in the translation between english and chinese  despite the great differences between the two languages. for example  a subject-verb relation in english  e.g.  dog  subject-verb  barking   is usually translated into the same subject-verb relation in chinese  e.g.  狗  subject-verb  吠 .  1  1  also showed that more than 1% of dependency relations of the above four types have one-to-one mappings between english and chinese. 
   this suggests that similar to np translation  there also exist a translation template between english dependency triples and chinese ones. unlike np translation templates  there is only one translation template: an english dependency triple et =  e1  re  e1  is most likely to be translated to a chinese dependency triple ct =  c1  rc  c1   where c1 and c1 are the chinese translations of the english terms e1 and e1  respectively  and rc is the chinese counterpart of re. 
1 generative model 
given an english dependency triple et =  e1  re  e1   and a set of its candidates of chinese dependency triple translation  the best chinese dependency triple ct =  c1  rc  c1  is the one that maximizes the following equation 
	ct* = argmaxp ct|et   = argmaxp ct  p et|ct   	 1  
	c	c
here  p ct  is the a priori probability of words of the translated chinese dependency triple. it can be estimated using mle as 
	p ct   = c nct   	 1  
where c ct  is the number of occurrences of ct in the collection  and n is the number of all dependency triples.  
   p et|ct  is the translation probability. we assume that  1  et and ct can be translated with each other only if they have the same type of dependency relation  i.e.  re = rc;  1  words in a dependency triple are translated independently. we therefore decompose the probability p et|ct  as 
　　　　　p et|ct   =p e1|c1  p e1|c1  δ re  rc    1  where δ re  rc  = 1 if re = rc and 1 otherwise.  
   p e|c  is a word translation probability  which could be estimated on word-aligned bilingual corpus using equation  1 . however  we observe that within a dependency triple  w1  r  w1   the translation selection of a word  e.g.  w1  largely depends on the other word w1 and the relation r. for example  the word  bear  in a dependency triple  bear  verb-object  child  is translated to 怀  while it is most likely to be translated to 忍受 as an individual word  if the translation probability is trained directly on a word-aligned corpus or the translation is obtained via dictionary look up . this suggests that translation probabilities in equation  1  are better trained on a set of aligned bilingual dependency triple pairs. unfortunately  it is difficult to obtain such a corpus in large quantity. therefore  in our model  instead of using a translation probability we assume that the likelihood of c to be translated to e can be measured by their semantic similarity  denoted by sim e  c . notice that e and c are not necessary to be a translation pair but just a pair of cross-
lingual synonyms  e.g.  synonym怀 is not a translation of  bear  defined . since our goal is to obtain good in a dictionary  but a 
ir results  such cross-lingual synonyms may solve the term mismatch problem and boost the clir performance.  
   in this study  we follow the method of  1  1  to estimate the value of sim e  c . the advantage of the method is that sim e  c  can be learned on unrelated english and chinese dependency triple corpora. 
1 feature functions 
we see from equations  1  and  1  that the likelihood of et to be translated to ct  assuming that re = rc  can be scored via two factors:  1  p ct  of equation  1   and  1  sim e  c . similar to the np translation model  we define a feature function for each type of factors  and combine them under the framework of linear models as shown in equation  1 . the two types of features are defined as follows. 
1. chinese language model feature. it is defined as the logarithm of the model of eq.  1   i.e.  hlm ct  = logp ct . 
1. cross-lingual word similarity feature. it is defined as the similarity between two words  i.e.  i.e.  hws et  ct  = sim e  c . since there are 1 dependency relations  each with 1 words  there are in total 1 types of word pair. we define 1 feature functions  each for one type of word pair  such as the similarity between a verb pair in a verb-object dependency.  
1. experiments 
1 settings 
we evaluate the three proposed query translation models on clir experiments on trec chinese collections. the trec-1 collection contains articles published in hong kong commercial daily  hong kong daily news  and takungpao. they amount to 1mb. a set of 1 english queries  with translated chinese queries  has been set up and evaluated by people at nist  national institute of standards and technology . the trec-1 corpus contains articles published in the people's daily from 1 to 1  and a part of the news released by the xinhua news agency in 1 and 1. a set of 1 english queries  with translated chinese queries  has been set up and evaluated by people at nist.  
   all chinese texts  articles and translated queries  are wordsegmented using the chinese word segmentation system msrseg . the system also identifies named entities of various types. then  stop words are removed. each of the trec queries has three fields: title  description  and narratives. in our experiments  we used two versions of queries  short queries that contain titles only and long queries that contain all the three fields. 
   the bilingual dictionary we used is a combination of three human compiled bilingual lexicons  including the ldc englishchinese dictionary and a bilingual lexicon generated from a parallel bilingual corpus automatically. the dictionary contains 1 english entries  including 1 words  and 1 phrases. 
   the okapi system with bm1 weighting  is used as the basic retrieval system. the main evaluation metric is interpolated 1-point average precision. statistical significance test  i.e.  t-test  is also employed. 
1 results 
the main results are shown in tables 1 to 1  i.e.  average precisions  and figures 1 and 1  i.e.  precision-recall curves . to investigate the effectiveness of our models for query translation  three baseline methods are compared  denoted by ml  st and bst  respectively. 
   ml  monolingual . we retrieve documents using the manually translated chinese queries provided with the trec collections. its performance has been considered as an upperbound of clir because the translation process always introduces translation errors. however  recent studies show that clir results can be better than monolingual retrieval results . this is also observed in our experiments. 
   st  simple translation . we retrieve documents using query translation obtained from the bilingual dictionary. phrase entries in the dictionary are first used for phrase matching and translation  and then the remaining words are translated by their translations stored in the dictionary. for each phrase/word with multiple translations stored in the dictionary  we only take the first translation  which is supposed to be the most frequently used translation. we could take more translations for each phrase/words  but our pilot experiments show that it hurts the performance in most cases. 
   bst  best-sense translation . we retrieve documents using translation words selected manually from the dictionary  one translation per word  by a native chinese speaker. if none of the translations stored in the dictionary is correct  the first one is chosen. this method reflects the upper bound performance using the dictionary. 
   cotm is the co-occurrence translation model described in section 1. we implemented a variant  called decaying cooccurrence model . the word similarity is defined as 
table 1: 1-point average precision  ap  for short queries on trec-1 dataset  * indicates that the improvement is statistically significant.  
 	translation model ap % of 
ml impr. over st 1 	ml 1 	  1 	st 1 1% 	 1 	bst 1 1% 1%* 1 	cotm 1 1% 1% 1 	nptm 1 1% 1%* 1 	cotm + nptm 1 1% 1%*  
table 1: 1-point average precision  ap  for long queries on trec-1 dataset  * indicates that the improvement is statistically significant.  
 	translation model 	ap 	% of 	impr. over 
	ml 	st 
1   	ml st bst 	1.1    	1 %  	  %*   
	  	1% 	1
1 	cotm 1  	1% 1%* 1 	nptm 1  	1% 1%* 1 	dptm 1  	1% 1% 1 	nptm+nptm 1  	1% 1%* 1 	cotm+dptm 1  	1% 1%* 1 	cotm+nptm+dptm 1  	1% 1%*  
table 1: 1-point average precision  ap  for long queries on trec1 dataset  * indicates that the improvement is statistically significant.  
 	translation model ap % of 
ml impr. over st 1 	ml 1     1 	st 1 1%   1 	bst 1 1% 1%* 1 	cotm 1 1% 1%* 1 	cotm+nptm 1 1% 1%* 1 	cotm+nptm+dptm 1 1% 1%*  	 	 	 	 

figure 1: precision-recall curves for short queries on trec-1 dataset. 

 figure 1: precision-recall curves for long queries on trec-1 dataset. sim wi  w j   = mi wi  w j  ×d wi  w j    1  where mi .  is the mutual information between two words  and is estimated on a chinese newspaper corpus. d .  is a penalty function  indicating that the mutual information between words decreases exponentially with the increase of the distance between them. it is defined as  
　　　　d wi  w j   = exp  α * dis wi  w j     1    1  where α is the decaying rate  α = 1 in our experiments   and dis wi wj  is the average intra-sentence distance between wi and wj in the chinese newspaper corpus.  
   nptm is the np translation model described in section 1. the translation template selection model  i.e.  value of hts z c   is trained on a word-aligned bilingual corpus containing approximately 1k english-chinese sentence pairs. translation templates are first extracted automatically from the corpus using an in-house chunking parser  and then filtered by a linguist. the probability p z|c  is then estimated according to equation  1 . for each chinese np pattern  there are 1 translation templates on average. the word selection model  i.e.  hws e  z  c   are computed according to equation  1  using the same word-aligned bilingual corpus. the chinese trigram model  i.e.  hlm c   is trained on a word-segmented chinese corpus consisting of about 1 billion words. 
   dptm is the dependency translation model described in section 1. sim e  c  is estimated using two unrelated english and 
chinese corpora  i.e.  1 wsj newswires for english and 1 people's daily articles for chinese . an english and chinese parser nlpwin  is used to extract dependency triples in both corpora. notice that nlpwin is a rule-based parser and performs well only when the input is a grammatical sentence  so we only tested dptm on long queries  i.e.  to parse the descriptions and narratives . 
   the experimental results in tables 1 to 1 and figures 1 and 1 give rise to the following observations. 
   first of all  we see that that cotm brings statistically significant improvements over st for long queries but its improvement over st for short queries is marginal. this is expected because cotm resolves translation ambiguities with resort to context terms. long queries contain much richer contextual information than short queries. 
   secondly  we see that nptm achieves substantial improvements over st for both long and short queries  and even outperforms bst for short queries  as shown in rows 1 and 1 in table 1. it is thus interesting to compare the phrase translation results using nptm and with that using dictionary look-up  rows 1 and 1 in table 1 . a further analysis shows that by using np identification and translation  we obtained better translations. for example  in trec-1 short query retrieval  only 1 multi-word phrases out of 1 queries are stored in the dictionary  and translated as a phrase  whilst using nptm  1 nps are identified and translated. it thus leads to a significant improvement over bst. 
   thirdly  we find that the use of dptm leads to an effectiveness well below that with cotm and nptm. for example  as shown in table 1  rows 1 and 1   the improvement of dptm over st is not statistically significant. this is however expectable because dependency triples have a much lower coverage than the other models. consider trec-1 long query retrieval  only a few triples from 1 queries out of 1 have been translated by dptm. so this  counter-performance  is not surprising. a further analysis shows that from the 1 queries  nlpwin extracted 1 dependency triples which appear at least 1 times in the corpus. the 1 triples include 1 verb-object dependency triples  1 sub-verb triples  1 adjective-noun triples and no adv-verb triple. for these queries  the dependency triple translation has positive impact on the methods of st and cotm for 1 out of the 1 queries  which leads to a statistically significant improvement of 1% over st  and 1% over cotm for the 1 queries. 
   finally  we see that as expected  the combined models  using the sequential combining approach described in section 1  always perform better than each component model to be combined. interestingly  for some queries  their clir results are even better than their monolingual retrieval results. 
1. related work and discussion 
co-occurrence information has been utilized by several recent studies  1  1  1  1  1  to deal with the translation selection problem for clir. one potential problem of most proposed cooccurrence model is the use of the approximate word selection algorithm. as described in section 1  each query term translation is actually determined independently. to remedy the problem  liu et al.  presented a so-called maximum coherent model that is able to estimate translations of multiple query terms simultaneously. in this paper  we remedy the problem simply by combining it with other two translation models using larger  linguistic-motivated units of translation. the basic idea is that the translations of a set of query terms that need to be jointed optimized only when they are really correlated tightly such as query words within a np or a dependency. in this sense  our query translation methods are both stochastically and linguistically motivated: stochastically because we use statistics from corpus  linguistically because the translation units  nps and dependencies  we defined are informed by syntactic analysis. 
   the np translation model is inspired by recent work on phrase-based smt. our np translation template is very similar to the template-based translation model described in . the use of hierarchical structure in our np translation templates can be viewed as a special case of the hierarchical phrase-based model in . there are however two major differences between our work and that of  1  1 . first  the nps that we deal with are syntactically well-defined constitutes.  1  1  extract phrases from bilingual corpus. these phrases are just a sequence of consecutive words  and could be completely meaningless syntactically. second  our translation templates use pos tag as word class while in   the templates use word classes that are automatically learnt from bilingual corpus. in a word  our model is more syntactically-motivated  and would potentially more accurate and efficient. moreover  in our study we view np translation as a subtask of machine translation. we believe that focusing on such a narrower problem would allow more dedicated modeling. koehn  presents a pretty comprehensive piece of work along this line. the rich feature set used for np translation  presented in   might also improve the accuracy of our method. 
   the dependency translation model aims at incorporating syntax information to resolve translation ambiguities. the same goal has also motivated the research of syntax-based mt  which is closely related to our work. similar to our method   also use parsers to identify linguistic structures of both chinese and english languages. then  they identify those sub-structures from both languages that can be mapped. the identified mappings form the so-called transduction grammar. due to the structural difference between source and target language  people also use a parser in one language  and map the extracted linguistic structure to the other language  1  1   assuming that there exist a large set of word-aligned bilingual sentence pairs. there are also some methods that can learn a transduction grammar without parsing monolingual sentences  1  1 . while most previous work requires a large amount of word-aligned bilingual corpus  which is not always available; our model can be learned from unrelated bilingual corpus. this benefit results from the fact that we define dependency translation as a subtask of mt  like the case of np translation model. we also argue that while most existing methods rely on constituency analysis  we believe that dependency analysis bring semantically related words together  and is more effective for resolving translation ambiguities. 
1. conclusions 
this paper presents three statistical query translation models for dealing with the problem of query translation ambiguity. the models differ in the use of translation unit and the use of linguistic information. the co-occurrence model is based on word translation. it does not take into account any linguistic structure explicitly  and simply views a query as a bag of words. the other two models  the np translation model and the dependency translation model  use larger  linguistically motivated translation units  and can exploit linguistic dependency constraints between words in nps or in higher level dependencies. our experiments of clir on trec chinese collections show that models using larger and more specific unit of translation are always better  if the models can be well trained  because more specific models could model more information. this is consistent with the observations on general reasoning: when more information is available and is used in reasoning  we usually obtain better results. the integration of different types of knowledge in query translation is the most apparent in the np and dependency models. both are constructed under the framework of linear models  where different information is combined as feature functions. this combination method is very effective flexible to incorporate more types of information or knowledge when it is available. 
   it is well known that statistical translation models will perform better with larger translation units. it is also well-known that models using larger translation units require more training data. thus  our work can be viewed as finding a tradeoff between specificity and trainability. given a limited amount of training data  we always try to make the model as specific as possible. recently  people have tried to automatically collect bilingual corpora from web  1  1 . since the web provides a potentially unlimited data source  it turns out to be a very promising research area. 
