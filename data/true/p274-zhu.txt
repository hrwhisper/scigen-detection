many classification problems require classifiers to assign each single document into more than one category  which is called multilabelled classification. the categories in such problems usually are neither conditionally independent from each other nor mutually exclusive  therefore it is not trivial to directly employ state-of-theart classification algorithms without losing information of relation among categories. in this paper  we explore correlations among categories with maximum entropy method and derive a classification algorithm for multi-labelled documents. our experiments show that this method significantly outperforms the combination of single label approach.
categories and subject descriptors: h.1  information systems : information search and retrieval general terms: algorithms  experimentation
keywords: multi-labelled classification  maximum entropy method
1. introduction
　data classification is the task of assigning each of the given data to a set of predefined categories. in general  all classification problems can be categorized as either single-labelled  or multi-labelled problems. single-labelled data classification assumes that the predefined data categories are mutually exclusive and each data point can belong to exactly one category. binary classification is the simplest case of the single-labelled problem where each data point is assigned to one of two predefined categories. to date  many classification methods  such as naive bayes  svm  and logistic regression  have been developed to address the single-labelled classification problem. on the other hand  with multi-labelled classification  the data categories may not be either mutually exclusive or conditionally independent  and each data point can belong to multiple categories simultaneously. multi-labelled classification problems are very common in the areas of document analysis and information retrieval. for example  a newspaper article about the
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
presidential election may talk about a wide range of topics such as politics  economy  and foreign relations; an email discussing the ongoing business work may also include topics about the past vacation the sender had experienced with his friends; etc. for document retrieval  a user may want to retrieve the news simultaneously belonging to multiple categories  which requires classifiers to correctly assign documents to all categories.
　despite the value and the significance of the problem  research on multi-labelled classification has received much less attention compared to its single-labelled counterpart. currently the most common solution to the multi-labelled classification problem is to decompose the problem into multiple  independent binary classification problems  and determine the final labels for each data point by aggregating the classification results from all the binary classifiers. more precisely  for a given m predefined categories  m binary classifiers are independently created  one for each category  and are used to determine if a given data point belongs to the corresponding category or not. the final category label for the data point is determined by combining the category labels generated by these m binary classifiers. the advantage of this approach is that a multilabelled classifier can be readily built using many start-of-the-art binary classifiers off the shelf  such as svm. however  when there exist strong correlations among categories  data classification performance may deteriorate because this approach employs a set of independent binary classifiers to conduct data classifications  and mutual correlations among different categories are completely ignored. more specifically  given the input variables  the optimal estimate should be the labels with the largest joint probability  instead of the combination of labels with largest individual probabilities of categories. later we illustrate the difference in section 1.
　to take the dependencies among data categories into account  a straight-forward approach is to transform the multi-labelled classification problem into a single-labelled problem by treating each possible combination of categories as a new class. in other words  a multi-labelled classification problem with ten predefined classes would be transformed to a single-labelled classification problem with 1 classes each of which corresponds to a possible combination of the original data classes. however  this approach faces the problem of data sparseness because there could be very few data points in many combinations of the data classes.
　in this paper  we propose a multi-labelled data classification method by explicitly modelling the mutual correlations among data categories using the maximum entropy principle. our method accomplishes the multi-labelled data classification task by constructing a conditional probability model pr y|x  from the training data set  where x is the feature vector of the input data point  and y is the class membership vector in which each element yi indicates
whether x belongs to the i'th class or not. in contrast to traditional approaches where pr y|x  is usually determined by the class priors and feature vectors of the input data  we construct pr y|x  by including an additional term - the dependencies among the data classes. we employ the maximum entropy  me  method to estimate the parameters during the model construction process. to reflect the estimation errors between the empirical and the real distributions  we introduce the regularization parameters  which serve to avoid the over-fitting problem for the model construction. this measure is in analogy to the penalized logistic regression  except for the items serving for the correlation among category labels. our experimental evaluations show that the proposed multilabelled classification method reveals statistically significant performance improvements compared to traditional approaches.
　the remainder of the paper is organized as follows. the related work is discussed in section 1. in section 1  we describe the model of multi-label maximum entropy. then  we present the experiments and results in section 1. finally section 1 concludes the paper.
1. related work
　there is limited work on the problem of multi-labelled classification. in the literature  many research studies take ranking-based approaches which assign a real-valued score to each documentcategory pair  and classify each document by choosing all the categories with the scores above the given threshold. schapire and singer  proposed boostexter which essentially is an enhancement to adaboost to build the ranks for all document-category pairs by using the boosting techniques. elisseeff and weston  developed a method using a kernel svm as the ranking function for document-category pairs. crammer and singer  proposed a family of one-against-rest online ranking algorithms that create a weight vector for each category  and compute the ranking between a document and a category using the inner product of the document's feature vector and the category's weight vector.
　although ranking-based approaches provide a unique way to handle the multi-labelled classification problem  they generally do not explicitly model the correlations among data categories. another problem such methods are facing with is that it is difficult to determine into how many categories a particular data should be classified  and thresholds are usually selected heuristically.
　another common approach to the multi-labelled classification problem is the modeling of classification using generative probabilistic models. mccallum  described a method based on generative model which assumes that each multi-labelled document is generated by a mixture of single-labelled document models. the method resorts to na： ve bayes model for each category model by assuming the independence between words given category. the method employs the expectation-maximization  em  to estimate the model parameters and the mixture parameters. ueda and saito  also proposed a probabilistic generative model that uses a different mixture approach. the advantage of these methods is that they explicitly model the category correlations  and require no threshold for determining the category label for each data point. however  because these methods usually assume words independence and mixture of category features within their probabilistic models  data classification accuracies could be limited because these assumptions usually do not reflect the real-world data configurations.
　a closely related approach in the literature is the one proposed by godbole and sarawagi  that stacks two levels of svm's with heterogeneous features. each lower level svm is a single-labelled  one-against-rest classifier with the original text features as the input. combining the original text features  the outputs of the lower level svm's are used as the input of the higher level svm's which determine the final category label for each document. in section 1  we implement a variation of this method  and compare it with our proposed multi-labelled data classification method for performance evaluations.
　in addition  there is some other work closely related to the multilabelled classification problem. clare and king  developed a method  which uses a modified entropy measure to extend the algorithm c1 to allow nodes containing multiple labels. the method also uses resampling strategies to deal with classes with small numbers of examples. similarly  comite et al.  extended the alternating decision trees  adtrees  algorithm for multi-labelled problems. each node of their multi-label adtrees is associated with a set of real values  one for each label. har-peled et al.  described a constraint classification framework. under the framework  classification problems are translated into a binary classification in a higher dimensional space with certain constraints. the paper also presented a meta-learning algorithm that learns via a single linear classifier consistent with the constraints. however  the correlations among labels were not explicitly discussed in the paper. cai and hofmann  proposed a hierarchical approach for multi-labelled/multi-class classification problem  where the predefined taxonomy is used to redefine the loss functions. gao et al.  extended their binary maximal figure-of-merit learning algorithm to multi-labelled classification problem. the method optimizes the performance against the approximated evaluation criteria  but the discriminant function for classification is still based on individual categories.
1. the multi-labelled maximum entropy method
　here we briefly review the method of single-labelled data classification using the maximum entropy model. when we consider the relaxation of the constraints  it is equivalent to the penalized logistic regression method. then  we give an example to demonstrate why the approach of combining single-labelled classifiers does not work well for the multi-labelled classification problem  which implies the importance of modeling the dependency among data categories. finally  we propose the multi-labelled maximum entropy approach to model the dependency among category labels. let x =  x1 ，，，  xd   denote the random variable representing feature vectors of the input data; let y denote the category label vector of a particular data point  we describe the details of y in different situations . statistical approaches accomplish the data classification task by estimating the conditional probability pr y|x  from the training data  and determine the category label y  of a given data point with the feature vector x using the following equation:
	y  = argmaxpr y|x .	 1 
y（y
where y represents the label space of the entire data set.
1 maximum entropy method for single-labelled classification
　in this section  we briefly introduce the single-labelled data classification using the maximum entropy model. detailed descriptions of the method can be found in . for simplicity  we only describe the binary classification case. therefore  the label space y = b  where b is a binary space  containing 1 and 1. since y has only one dimension  we denote it as y.

　the principle of the maximum entropy model  mem   is simple: model all that is known and assume nothing about what is unknown. in other words  given a collection of facts  the mem chooses a model which is consistent with all the facts  but otherwise is as uniform as possible. in real implementations  facts are usually represented as a set of constraints  and the optimal model is acquired by maximizing the model's entropy under the given constraints.
　let p  x y   q x y  denote the empirical and the model distributions  respectively. traditional mem-based data classification methods typically use the following constraints for model selection:
hyiq = hyip   
 1 
	hyxliq = hyxlip   	 1 ＋ l ＋ d 
where h，ip denotes the expectation with respect to distribution p; xl represents an element of the feature vector x. the above two constraints serve to force the model under construction to comply with the two statistical properties of the training data set: the prior probability of each category  and the correlations among the categories and features of the given data.
　for the problem of data classification  the model to be estimated is the conditional probability q y|x   denoted as a function of y and x  q y|x   from now on  and the mem obtains the optimal q y|x  by maximizing the following entropy subject to the constraints eq.  1  and py q y|x  = 1. we have
	q = arg maxh x y|q  = arg minhlog q y|x iq  	 1 
	q	q
where h x y|q  is the entropy of x and y given distribution q with parameter q. by expanding h x y|q  and ignoring the constants irrelevant to q q|x   we have the second part of eq.  1 .
　the minimization of eq.  1  is a typical constrained optmization problem that can be solved using lagrange multiplier algorithms. the lagrangian of eq.  1  is:
l q y|x  b w ζ x   = hlog q y|x iq
+ b hyip    hyiq  + xwl hyxlip    hyxliq 
l
	+ xζ x  1   xq y|x   	 1 
	x	y
where b  w =  w1 ，，，  wd   and ζ x  are the lagrangian multipliers. omitting the mathematical derivations  refer to  1  1  for derivation details   the optimal model q takes the form of
	 	 1 
where z x  = py exp y b + w x   is the partition function.
　the constraints in eq.  1  assume the model distribution equals the empirical distribution. however  for a limited number of training data  there exist estimation errors. without considering such errors  the solution may lead to generation errors. to have a robust estimation  chen and rosenfeld  proposed to introduce maximum a posteriori probability  map  model under the gaussian prior into the constraints. assuming η and φl are the estimate errors which follow gaussian distributions with zero means and variances of ση1/n and σφ1/n  n is the number of documents   respectively  we rewrite eq.  1  as eq.  1 .
hyiq = hyip  + η  hyxliq = hyxlip  + φl   1 ＋ l ＋ d   1 
	η1	φ1l
　1	+ xl	φ1	＋ c  1ση/n	1σ /n
where c is a parameter that can be used to set the tolerance of the estimation errors.
with the renewed constraints  the lagrangian becomes:

where b  w =  w1 ，，，  wd    γ  − 1   and ζ x  are the lagrangian multipliers.
by solving eq.  1  and ignoring constants  we have
l b w  = d y b + w x  + log z x ep 
	λb 1	λw	1	 1 
+ b + ||w||   1n	1n
where λb = ση1/γ and λw = σφ1/γ  || ， ||1 denotes the 1-norm. actually  λb and λw serve as regularization coefficients for the bias term and the feature terms  respectively. in many applications  the bias term is not regularized  which means to set λb to zero. in   by adding a constant feature  the bias term is treated the same as the feature terms  which is equivalent to λb = λw. actually  when there are a large number of training data  the difference between these two settings is very small. in our experiments  we set λb = 1. eq.  1  is actually penalized logistic regression  cf.  . the classification task is to find the optimal parameters  b and w  to minimize l b w  in eq.  1 . plugging the optimal parameters   b and w    into eq.  1   we have optimal conditional distribution q  y|x   which is used to classify a given document with feature vector x.
1 why not combine single labels
　for the multi-labelled classification problem  let y =  y1 ，，，  ym   （ y   bm be the label vector of a data point  where m is the total number of categories  and each dimension yi of y indicates the membership of the data point in category i. by assuming the independence among the categories  the approach of combining single-labelled classifiers for multi-labelled data classification can be expressed as follows:
y  = argmax
	y（y i=1	 1 
=，，，  argmaxpr yi|x  ，，， .
yi（b
　the following example shows why combining single-labelled classifiers does not always produce correct results for the multilabelled classification problem when the categories are not independent. assume that the joint distribution pr y1 y1|x  for some data point x is shown in table 1. further assume that we trained
pr y1 y1|x y1 = 1y1 = 1pr y1|x y1 = 111y1 = 1.1.1.1pr y1|x 11   table 1: an example of joint distribution of two labels. two single-labelled classifiers independently  which yields the conditional probabilities pr y1|x  and pr y1|x  shown in the same table. because pr y1 = 1|x  = 1 is less that pr y1 = 1|x  = 1  data x is assigned to the first category y1 = 1. similarly  data x is assigned to the second category y1 = 1 as well. however  according to table 1  pr y1 = 1 y1 = 1|x  = 1  which is less than pr y1 = 1 y1 = 1|x  = 1. this means that the correct category labels for data x is y1 = 1 y1 = 1  and the result generated by combining the two single-labelled classifiers is not correct!
　clearly  the approach of combining single-labelled classifiers without considering the dependence among category labels has its limitation on the multi-labelled classification problem. therefore  we develop a multi-labelled data classifier using the maximum entropy model in the following section.
1 multi-labelled maximum entropy model
　for the multi-labelled classification problem  we can extend the constraints in eq.  1  to

where η's and φ's are estimate errors.
　as the previous example shows  correlations among categories are important to the multi-labelled classification problem. to capture such information  we add a new type of constraints to the maximum entropy model to require the model to comply with the second order statistical property yiyj of the training data.
　　　　hyiyjiq = hyiyjip  + θij   1 ＋ i   j ＋ m   1  where θ's are estimate errors.
　although it is possible to use other higher order statistics to model the category dependencies  the cost of employing such statistics may surpass the benefits they bring about. the higher order the statistics  the more parameters the model needs to estimate. with limited training data  models involving higher order statistics can hardly capture true distributions of the underlying data  and are likely to end up with little difference or even deteriorated performances compared to models using lower order statistics. again  the problem in our hands is to obtain the optimal q y|x  that maximizes the entropy in eq.  1  subject to the constraints in eq.  1    1  and py q y|x  = 1. similar to eq.  1   see appendix for the derivation details   we have
	 	 1 
where z x  = py exp m  y  b+ry+wx   is the partition function; b =  b1 ，，，  b     w  an m〜d matrix   and r  an m〜m strict upper triangle matrix  are lagrangian multipliers that need to be determined. by simplifying the lagrangian and ignoring constants  we have
l b r w  = d y  b + ry + wx  + log z x ep 
	λb	1	λr	1f	λw	1f	 1 
	+||b|| +||r||	+||w||  
	1n	1n	1n
where || ， ||f denotes frobenius norm  λb = ση1/γ  λr = σθ1/γ and λw = σφ1/γ. similar to eq. 1   λb  λr and λw act as regularization coefficients  and there values are to be specified by the user.
　here  the task of finding the optimal q  y|x  becomes the problem of finding the optimal b  w  and r that minimizes the lagrangian:
	b  r   w  = arg minl b r w .	 1 
b r w
eq.  1  can be solved using gradient descent approaches. the derivatives of l with respect to its parameters are
 l	λb  = hyiiq   hyiip  +	bi 
	 bi	n
	 l	λr
	 = hyiyjiq   hyiyjip  +	rij 
	 rij	n
	 l	λw
	 = hyixliq   hyixlip  +	wil.
	 wil	n
there are many gradient descent methods off the shelf. in   malouf compared several algorithms for maximum entropy parameter estimation and suggests that the limited memory variable metric  lmvm  method is the fastest solver for document classification problems. therefore  in our implementation we use lmvm in the tao package  to estimate the parameters. once we have b  r   w    classifying a document with feature vector x is equivalent to
	y  = arg maxy  b  + r y + w  x .	 1 
y（y
to label a data point  we can enumerate all possible label sets in y to find the most probable one using eq. 1 .
1. experiments
　to show the benefit of using multi-labelled maximum entropy method  we evaluate the method against other methods on two real data sets.
1 data description
　the first data set is the reuters-1 document corpus that contains 1 documents collected from the reuters newswire in 1. it is a standard text categorization benchmark test set that consists of 1 document categories. in our experiments  we used the ten  1  largest categories for performance evaluations. table 1 shows the statistics of document labels in our training set. it is observed from the table that only 1% of the documents in the training set possess multiple labels  i.e.  multiple categories .
no. of labelsno. of documentspercentage11%11%11%11%11%total1%table 1: numbers of multi-labelled document in the training set of reuters-1 data set.
　to prepare the features for documents  we follow the widely used bag-of-word approach. the features used in our experiments are words that appear more than once in the corpus. all the documents are processed with the following steps: removing sgml tags  downcasing  removing words on the smart stoplist  stemming. the above pre-processing has resulted in a total of 1 words as the final features. we employed the tfidf weighting scheme and the normalization in creating the feature vector for each document. we used the modified apte   modapte   split to create the training and the testing sets that consist of 1 and 1 documents  respectively.
11%11%11%11%11%total1%　the second data set is an email corpus collected by us from six public domain mailing lists1. our original purpose for creating such an email corpus is to monitor the r&d activities of a project group and discover the contributions of each employee through mining and analysis of emails among the group members. to serve these purposes  we have defined the following nine categories for email classification:  1  topic raising  raise    1  question asking  ask    1  work report  rep    1  information announcement info    1  delegation  del    1  solution proposal  sp    1  positive comments  poscom    1  negative comments  negcom   and  1  others  others .our pre-processing on the email corpus includes removal of irrelevant information and extraction of implicit features. we remove the following items from the body of each email: attachments  pictures  executable codes   marker characters  quoted materials  email header  signature  time information  reply information  debug message  compiling message  source codes  etc. the extracted implicit features include: reply relation  reply indicator  hyper-links  ftp sites  itemization symbols   forwarded  mark in email title  type of attached data  etc. in our email corpus  a large percentage of emails are assigned with multi-labels. for example  1%  1  of the emails in class raise also belong to class ask; 1%  1  of the emails in class ask also belong to class raise. in our experiments  we use the first eight  1  categories  and treat emails in the others category as having no labels. we found that 1% of documents in the training set have more than one label  see table 1 . the percentage of multino. of labels no. of documents percentage
table 1: numbers of multi-labelled document in the mailing list data set.
labelled documents in our email corpus is significantly higher than that of the reuters-1 corpus. the procedures used for creating the feature vector of each email are similar to those for the reuters-1 corpus  which results in a total of 1 words as the final feature set.
　table 1 and 1 shows the mutual information  the p-values of pearson's chi-square test of pairs of categories in the reuters-1 and mailing list data set. from the table  we can see that some values of mutual information are clear not zero and some p-values show that the dependency between categories is significant  the smaller p-values indicate the stronger dependency between categories . hence  the approach of combining single-labelled classifiers is insufficient for these data.

1 methods and evaluation measures
　for performance comparisons  we implemented two traditional methods and conducted performance evaluations using the same data corpora. the first method is the combination of multiple  independent single-labelled classifiers each of which employs the single-labelled maximum entropy model  which is equivalent to the penalized logistic regression   as described in section 1. this method is denoted as  comb  in our experiments. the second method is developed by stacking another layer of the penalized logistic regression on top of the first method  which adopts the idea of the approach described in . we use the penalized logistic regression instead of svm's because we want to use the same loss function for data classification model so that the results are more comparable. this method is denoted as  hf  in our experiments. our proposed multi-labelled classification method based on the maximum entropy model is denoted as  mlme .
　for a given document i  let y i  and y  i  be the true and the predicted label sets  respectively. we use the classification accuracy ac defined below as our performance metric.
ac  
where n denotes the total number of documents in the test  δ x y  is the delta function that equals one if x = y for all dimensions and equals zero otherwise. ac computes the percentage of the documents whose predicted labels are exactly the same as their true labels.
　though the accuracy measures are compatible with the loss function of classification  which is considered as a smoothed version of 1   1 loss  we are also interested in the practical goal of information retrieval. for multiple label data sets  we usually use microaveraged f1 measure  
 
where p and r are the precision rate and the recall rate computed globally over all binary decisions of all document-category pairs  respectively. since the micro-averaged f1 measure computes over all binary decisions  the partial correctness of labeling is credited.
1 experimental results
　the first experiment is on the reuters-1 document corpus. we used ten-fold cross validations to choose optimal regularization parameters for all the three methods. table 1 shows the evaluation results using the optimal parameters on ten 1   1 random splits. to compare the performance of different methods  we use one-sided wilcoxon signed-rank test which is a nonparametric paired test without assuming the underline distribution of the tested values. here  the alternative hypothesis is whether multilabelled maximum entropy method  mlme  has higher accuracies
 or  f1 measures . the p-values of one-sided wilcoxon signedrank test between the given experiment and multi-labelled maximum entropy method are shown in table 1. although the improvement of accuracies and f1 measure is small  the improvement is significant  usually  if the p-value is smaller than 1   the result is significant . the improvement is not much  partially because that the data set only contains 1% multi-labelled documents and the percentage of documents not belonging to the ten categories are relatively large  1%.
table 1: some facts of data set reuters-1. the normalized symmetric mutual information values between categories are shown in the upper triangle. the p-values of pearson's chi-square test  χ1  for pairs of categories are shown in the lower triangle. the numbers in the diagonal are the proportions of categories.
askdelinfonegcomposcomraiserepspask11111111del11111111info1e-1.1.1.1.1.1.1.1negcom11111111poscom11111111raise1e-1.1.1.1.1e-1.1.1.1rep111111e-1.1.1sp1e-1.1e-1.1e-1	1	1.1e-1.1table 1: some facts of the mailing list data set. the normalized symmetric mutual information values between categories are shown in the upper triangle. the p-values of pearson's chi-square test  χ1  for pairs of categories are shown in the lower triangle. theacqcorncrudeearngraininterestmoney.fxshiptradewheatacq1111111111corn1e-1.1.1.1.1.1.1.1.1.1crude1e-1.1.1.1.1.1.1.1.1.1earn1e-1.1e-1.1e-1111111grain1e-111e-1.1.1.1.1.1.1interest1e-1.1.1.1e-1.1e-1.1.1.1.1.1money.fx1e-1.1.1e-1.1e-1.1e-1.1e-1.1.1.1.1ship1e-1.1.1e-1.1e-1.1e-1.1.1.1.1.1trade1e-1.1.1e-1.1.1.1.1.1.1wheat1e-1.1e-1.1.1e-111111　the second experiment is on the mailing list data set. table 1 shows the accuracy and f1 measure for using their optimal parameters on ten 1 random splits. the accuracy and f1 measure of the numbers in the diagonal are the proportions of categories.
mlme method are better than those of the other two methods for every split  and the improvement is statistically significant.
1 the correlations among category labels
　the intention of the proposed multi-labelled maximum entropy model is to include the correlations among categories into the model. since the additional parameter rij is the coefficient of category i and category j in the model  we expect that rij is somewhat related the correlation between category i and category j. figures 1 and 1 plot correlations among categories and corresponding parameters of r from one of the experiment runs. the figures clearly show that the relation between the correlation and parameter r is significant and a pair with large correlation usually has a larger r parameter  especially when the correlations are far from zero. the r parameters enforce the correlations among categories in the model. these figures confirm our assumption and indicate that the correlation terms  strictly speaking the second order moments of labels  are important in these multi-labelled classification problems.
1. concluding remarks
　in this paper  we propose a maximum entropy method for multilabelled classification  in which the correlations among category labels are explicitly considered in the model. the experimental results show that multi-labelled classification is beneficial in the model considering the correlation between classes  especially when the correlation is relatively strong. by examining the parameters of the model  the experiments confirm our assumption that the correlation terms are important in multi-labelled classification tasks. one drawback of this method is in computing the term z of
eq.  1 . one possible solution is to use a stochastic approach.
another possible solution is to approximate z x  with the sum of several important q y|x .
　during the simplification of the model  we assume that estimate errors are independent from each other. we do not know how large the impact is when this assumption does not hold. the future work may also involve the investigation of correlations among estimate errors.
acknowledgement
we would like to thank mei han  tao li and anonymous reviewers for useful comments and discussion. we thank the mathematics and computer science division of argonne national lab for making toolkit for advanced optimization  tao  package publicly available.
appendix
here are some details of how we derive eq.  1 . for parameters ηi and φil in eq.  1  and θij in eq.  1   we regularize them to avoid the extreme results during estimating the parameters of the model. assuming the joint probability of estimate errors should be reasonably large  say greater than a small number   we write
	pr 	 1 
　to simplify this constraint of eq.  1   we assume that those estimate errors are independent to each other. hence  we can rewrite

	mlme	comb	hf
accuracyaverage111p-value11test set111micro-averaged f1
average 1 1 1 p-value 1 1 test set 1 1 1
table 1: the accuracies and f1 measures of experiments on the reuters-1 dataset and their one-sided wilcoxon signedrank test vs mlme.
this constraint in logarithm format as
		 1 
　according to the central limit theorem  the estimation errors follow normal distribution. let ηi ゛ n 1 ση1/n   θij ゛ n 1 σθ1/n  and φil ゛ n 1 σφ1/n   where n is the number of data points. the constraint can be simplified as
		 1 
where c is a constant derived from. the lagrangian of eq.  1  subject to eq.  1  1  1  and py q y|x  = 1 is:
l q y|x  η θ φ  r w γ ζ x   = hlog q y|x i
	．	q
+bi hyiip  + ηi   hyiiq 
i
+ xrij hyiyjip  + θij   hyiyjiq 
i j+	wil hyixlip  + φil   hyixliq  1 xi l
	ηi1	θij1	φ1il
	+ γ  i	1ση1/n + xi j 1σθ1/n + xi l 1σφ1/n   c 
+ xζ x  1   xq y|x   
	x	y
where b  r  strict upper triangle matrix   w  γ  − 1   and ζ are the lagrangian multipliers.

table 1: the accuracies and f1 measures of experiments on the mailing list data set and their one-sided wilcoxon signed-rank test vs mlme.
　the karush-kuhn-tucker  kkt  conditions require the derivatives of the lagrangian with respect to its parameters must be zeros to maximize l. therefore  we have:
	 l	 
 = p  x  log q y|x  + 1   y  b + ry + wx  
 q y|x 
  ζ x  = 1 
	 l	ηi
 ηi = bi + nγση1 = 1 
	 l	θij
 θij = rij + nγ σθ1 = 1 
	 l	φik
 φij = wik + nγ σφ1 = 1.
　when γ is zero  the problem is trivial. now we assume that γ   1. it allows us to express q  η  θ and φ as functions of b  r  w  and γ:
	1	 
q  y|x  =  exp y  b + ry + wx   
z x 
	η1	θ1	σφ1	 1 
η i =  σ	i	 ij =  σ rij 	φ ik =  wik  b  	θ
	nγ	nγ	nγ
where the partition function  z x  = py exp y  b + ry + wx  . by plugging eq.  1  into eq.  1   we have eq.  1 .
