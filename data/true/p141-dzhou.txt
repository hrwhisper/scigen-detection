the web offers rich relational data with different semantics. in this paper  we address the problem of document recommendation in a digital library  where the documents in question are networked by citations and are associated with other entities by various relations. due to the sparsity of a single graph and noise in graph construction  we propose a new method for combining multiple graphs to measure document similarities  where different factorization strategies are used based on the nature of different graphs. in particular  the new method seeks a single low-dimensional embedding of documents that captures their relative similarities in a latent space. based on the obtained embedding  a new recommendation framework is developed using semisupervised learning on graphs. in addition  we address the scalability issue and propose an incremental algorithm. the new incremental method significantly improves the efficiency by calculating the embedding for new incoming documents only. the new batch and incremental methods are evaluated on two real world datasets prepared from citeseer. experiments demonstrate significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method.
categories and subject descriptors
h.1  information search and retrieval : information filtering; g.1  numerical analysis : optimization
general terms
algorithm  experimentation
keywords
recommender systems  collaborative filtering  semi-supervised learning  social network analysis  spectral clustering
 this work was done at the pennsylvania state university.
copyright is held by the international world wide web conference committee  iw1 . distribution of these papers is limited to classroom use  and personal use by others.
www1  beijing  china acm 1-1-1/1.
pennsylvania state university
university park  pa 1
1. introduction
¡¡recommender systems continue to play important and new roles in business on the world wide web  1  1  1  1  1 . per definition  the recommender system is an information filtering technique that seeks to identify a set of items that are likely of interest to users.
¡¡the most popular method adopted by contemporary recommender systems is collaborative filtering  cf   where the core assumption is that similar users on similar items express similar interests. the heart of memory-based cf methods is the measurement of similarity: either the similarity of users  a.k.a user-based cf  or the similarity of items  a.k.a items-based cf  or a hybrid of both. the user-based cf computes the similarity among users  usually based on user profiles or past behavior  1  1   and seeks consistency in the predictions among similar users. but it is known that user-based cf often suffers from the data sparsity problem because most of the user-item ratings are missing in practice. the item-based cf  on the other hand  allows input of additional item-wise information and is also capable of capturing the interactions among them  1  1 . this is a major advantage of item-based cf when it comes to dealing with items that are networked  which are usually encountered on the web. for example  consider the problem of document recommendation in a digital library such as the citeseer  http://citeseer.ist.psu.edu . as illustrated in fig. 1  let documents be denoted as vertices on a directed graph where the edges indicate their citations. the similarity among documents can be measured by their cocitations  cociting the same documents or being cocited by others  1. in this case  document b and c are similar because they are cocited by e.
¡¡working with networked items for cf is of recent interest. recent work approaches this problem by leveraging the item similarities measured on an item graph   modeling item similarities by an undirected graph and  given several vertices labeled interesting  perform label propagation to rank the remaining vertices. the key issue in label propagation

figure 1: an example of citation graph.
on graphs is the measurement of vertex similarity  where related work simply borrows the recent results of the laplacian on directed graphs  and semi-supervised learning of graphs . nevertheless  using a single graph laplacian to measure the item similarity can overfit in practice  especially for data on the web  where the graphs tend to be noisy and sparse in nature. for example  if we revisit fig. 1 and consider two quite common scenarios  as illustrated in fig. 1  it is easy to see why measuring item similarities based on a single graph can sometimes cause problems. the first case is called missing citations  where for some reason a citation is missing  or equivalently is added  from the citation graph. then the similarity between a and b  or c  will not be encoded in the graph laplacian. the second case  called same authors  shows that if a and e are authored by the same researcher z  using the citation graph only will not capture the similarity between d and b  which presumably should be similar because they are both cited by the author z.
	a	a

	 a  missing citations	 b  same authors
figure 1: two common problematic scenarios for measuring item similarities on a single citation graph: missing citations and same authors.
¡¡needless to say  the cases presented above are just two of the many problems caused by the noise and sparsity of the citation graph. noise in a citation graph is a result of a missing citation link or an incorrect one. fortunately  real world data can usually be described by different semantics or can be associated with other data. in the focus of relational data in this paper  we work with several graphs regarding the same set of items. for example  for document recommendation  in addition to the document citation graph  we also have a document-author bipartite graph that encodes the authorship  and a document-venue bipartite graph that indicates where the documents were published. such relationship between documents and other objects can be used to improve the measurement of document similarity. the idea of this work is to combine multiple graphs to calculate the similarities among items. the items can be the full vertex set of a graph  as in the citation graph  or can be a subset of a graph  as in document-author bipartite graph  1.
by doing so  we let data from different semantics regarding the same item set complement each other.
¡¡in this paper  we implement a model of learning from multiple graphs by seeking a single low-dimensional embedding of items that captures the relative similarities among them. based on the obtained item embedding  we perform label propagation  giving rise to a new recommendation framework using semi-supervised learning on graphs. in addition  we address the scalability issue and propose an incremental version of our new method  where an approximate embedding is calculated only for the new items. the new methods are evaluated on two real world datasets prepared from citeseer. we compare the new batch method with a baseline modified from a recent semi-supervised learning algorithm on a directed graph and a basic user-based cf method using singular value decomposition  svd . also  we compare the new incremental method with the new batch method in terms of recommendation quality and efficiency. we observe significant quality improvement in our batch method and significant efficiency improvement with tolerable quality loss for our incremental method.
¡¡the contributions of this work are:  1  we overcome the deficiency of a single graph  e.g. noise  sparsity  by combining multiple information sources  or graphs  via a joint factorization to learn rich yet compact representation of the items in question;  1  to ensure effectiveness and efficiency  we propose several novel factorization strategies tailored to the unique characteristics of each graph type  each becoming a sub-problem in the joint framework;  1  to handle the ever-growing volume of documents  we further develop an incremental updating algorithm that greatly improves the scalability  which is validated on two large real-world datasets.
¡¡the rest of this paper is organized as follows: section 1 introduces how to realize recommendations using label propagation; section 1 describes our method for learning item embedding from three general types of graphs; section 1 further introduces the incremental version of our algorithm; experiments are presented in section 1; section 1 discusses the related work; conclusions are drawn in section 1.
1. recommendation by label propagation
¡¡label propagation is one typical kind of transductive learning in the semi-supervised learning category where the goal is to estimate the labels of unlabeled data using other partially labeled data and their similarities. label propagation on a network has many different applications. for example  recent work shows that trust between individuals can be propagated on social networks  and user interests can be propagated on item graphs for recommendations .
¡¡in this work  we focus on using label propagation for document recommendation in digital libraries. let the document set be d  where |d| is the number of documents. suppose we are given the document citation graph gd =  vd ed   which is an unweighted directed graph. suppose the pairwise similarities among the documents are described by the matrix s ¡Ê r|d|¡Á|d| measured based on gd. a few documents have been labeled  interesting  while the remaining
are not  denoted by positive and zero values in the label vector y. the goal is to find the score vector f ¡Ê r|d| where each element corresponds to the propagated interests. then document recommendation can be performed by ranking the documents by their interest scores. a recent approach addressed the graph label propagation problem by minimizing the regularization loss below :
	 	 1 
where ¦Ì   1 is the regularization parameter. the first term is the cost function for the smoothness constraint  which prefers small differences in labels between nearby points; the second term is the fitting constraint that measures the difference of f from given data label y. setting the  ¦¸ y / f = 1  we can see that the solution f  is essentially the solution to the linear equation:
	 i   ¦Ás f  =  1   ¦Á y 	 1 
where ¦Á = 1/ 1 + ¦Ì . one solution to the above is given in a related work using a power method :
	ft+1 ¡û ¦Ásft +  1   ¦Á y	 1 
where f1 is the random guess and f  = f¡Þ is the solution. here  notice that l =  i ¦Ás  is essentially a variant laplacian on this graph using s as the adjacency matrix; and k =  i   ¦Ás  1 = l 1 is the graph diffusion kernel. thus  one essentially applies f  =  1 ¦Á l 1y  or f  =  1 ¦Á ky
 to rank documents for recommendation.
¡¡now the interesting question is how to calculate s  or equivalently the kernel k  among the set d. however  there has been limited amount of work on obtaining s. for graph data  recent work borrows the results from spectral graph theory  1  1   where the similarity measures on both undirected and directed graphs have been given. for undirected graph  su is simply the normalized adjacency matrix:
	su = ¦° 1w¦° 1	 1 
where ¦° is a diagonal matrix such that we = ¦°e and e is an all-one column vector. for directed graph  where the adjacency matrix is first normalized as a random walk transition matrix p  = ¦° 1w   the similarity measure sd is calculated as:
		 1 
where ¦µ is a diagonal matrix where each diagonal contains the stationary probability on the corresponding vertex 1.
¡¡note that the similarity measures given above are derived from a single graph on d. however  many real world data can be described by multiple graphs  including those within d and between d and another set. such information is of more importance to combine especially when the a single view of the data is sparse or even incomplete. in the following  we introduce a new way to integrate three general types of graphs. instead of estimating s directly  we seek to learn a low-dimensional latent linear space.
1. learning multiple graphs
¡¡the immediate goal of this section is to determine the relative positions of all documents in a k-dimensional latent semantic space  say x ¡Ê r|d|¡Ák  which will combine the social inferences in document citations  authorship and venues.
in the sequel  we assume k is a prescribed parameter which we do not seek to determine automatically. note a contribution of this work is the different strategies used for different graphs based on their characteristics  which are described in the following subsections.
¡¡we begin by a formulation of our problem. let d  a  v be the sets of documents  authors and venues and |d|  |a|  |v| be their sizes. we have three graphs  one directed graph gd on d; one bipartite graph gda between d and a; and one bipartite graph gdv between d and v  which describe the relationship among documents  between documents and authors  and between documents and venues. let the adjacency matrices of gd  gda  gdv be d  a and v . we assume all relationships in question are described by nonnegative values. for example  gd can be considered as to describe the citation relationship among d and di j = 1 if document di cites dj  di j = 1 if otherwise ; ga can be considered as the authorship relationship  an author composes a document  or the citation relationship  an author cites a document  between d and a.
1 learning from citation matrix: d
¡¡in this section  we relate the document embedding x to the citation matrix d  which is the adjacency matrix of the the directed graph gd.
¡¡the citation matrix d include two kinds of document cooccurrences: cociting and being cocited. a cociting relationship among a set of documents means that they all cite a same document; a cocited relation refer to that several documents are cited together by an another document. in many related work  e.g.   on directed graphs  these two kinds of document co-occurrences are used to infer the similarity among documents. probably the most well recognized way to represent the similarities among the nodes of a graph is associated with the graph laplacian   say l ¡Ê r|d|¡Á|d|  which is defined as:
	l = i   ¦Ásd 	 1 
where sd is the similarity matrix on directed graphs as measured in eq. 1; ¦Á ¡Ê  1  is a parameter for the laplacian to be invertible; i is an identity matrix. note that sd is symmetric and positive-semidefinite.
next we give the method to learn from gd.
¡¡objective function: suppose we have a document embedding x =  x1 ...xk  where xi contains the distribution of values of all documents on the i-th dimension of a kdimensional latent space. the overall  lack-of-smoothness 
of the distribution of these vectors w.r.t. to the laplacian l can be measured as
	 	 1 
where x =  x1 ...xk . here we seek to minimize the overall  lack-of-smoothness  so that the relative positions of documents in x will reflect the similarity in sd.
¡¡constraint: in addition to the objective function of x  we enforce a constraint on x so as to avoid getting a trivial solution  note that x = 1 minimizes eq. 1 if there is no constraint on x . we choose to use the newly proposed logdeterminant heuristic on xtx  a.k.a the log-det heuristic 
denoted by log|xtx| . it has been shown that the log|y | is a smooth approximation for the rank of y if y is a positive semidefinite matrix. it is obvious the gram matrix xtx is positive semidefinite. thus  when we maximize log|xtx|  we effectively maximize the rank of x  which is at most k. another way to understand log|xtx| is to note that
| i i ¦Ëi y   is the ith eigen-value of y and ¦Òi x  is the i-th singular value of x. therefore  a full-ranked x is preferred when log|xtx| is maximized. for more reasons on using the log-det heuristic  refer to the comments below and .
¡¡using the log-det heuristic  we arrive at the combined optimization problem:
		 1 
where tr a  is the trace function defined as the sum of diagonal elements of a. it has been shown that max{log|xtx|}  or equivalently min{ log|xtx|}  is a convex problem .
so eq. 1 is still a convex problem.
¡¡comments: first  it is interesting to notice that we did not use the traditional constraint on x  such as the orthonormal constraint of the subspace used in pca  . the reason of choosing log-det heuristic in our case is because that  1  the orthonormal constraint is non-convex while the remaining of the problem is;  1  the orthonormal constraint cannot be solved by gradient-based methods and thus cannot be efficiently solved and cannot be easily combined with the other two factorizations in the following sections;  1  the log-det  log|xtx|  has a small problem scale  k ¡Á k  and can be solved effectively by gradient-based methods. second  note a key difference of this work from related work on link matrix factorization  e.g.   is that we seek to determine x to comply with the graph laplacian  not to factorize the link matrix  which gives us a convex problem that is global optimal.
1 learning from author matrix: a
¡¡here  we show how to learn from an author matrix  a  which is the adjacency matrix of the bipartite graph  gda  that captures the relationship between d and a. we can use gda to encode two kinds of information between authors and documents  one being the authorship and the other being the author-citation-ship. to encode authorship  we let a ¡Ê i|d|¡Á|a|  i ¡Ê {1}   where ai j indicates whether the i-th paper is authored by the j-th author; to encode authorcitation-ship  we assume a ¡Ê r|d|¡Á|a|  where ai j can be the number of times that document i is cited by author j  or the logarithm of the citation count for rescaling .
¡¡we consider both kinds of author-document relationship equivalently using matrix factorization  where authors in both cases are considered social features of documents  inferring similarities between documents. the basic intuition is that the document related to a same set of authors should be relatively close in the latent space x. the inference of this intuition to citation recommendation is that the other work of an author will be recommended given a reader is interested in several work by similar authors.
¡¡given the authorship matrix a ¡Ê r|d|¡Á|a|  we want to use x to approximate it. let the authors be described by an author profile matrix w ¡Ê r|a|¡Ák. we can approximate a by xwt as:
	 	 1 
where x and w are the minimizers. to prevent overfitting  the second term is used  where ¦Ë1 is the parameter. note that later we will combine eq. 1 and eq. 1; so we do not show the constraint on here. it is worth mentioning that the idea of using two latent semantic spaces to approximate a co-occurrence matrix is similar to that used in document content analysis  e.g. the lsa  .
1 learning from venue matrix: v
¡¡in the above  we have given the method for learning a representation of d from a directed citation graph gd and an undirected bipartite graph gda. in this section  we are given an additional piece of categorical information  which can be described by the bipartite venue graph gdv   where one set of nodes are the documents from d and the other set are the venues from v.
¡¡similar to a  we have the venue matrix v ¡Ê i|d|¡Á|v|  where vi j denotes whether document i is in venue j. however  a key difference here is that each row in v has at most one nonzero element because one document can proceed in at most one venue. although we could as well employ xwt to approximate v  as in sec. 1   we will show that the special property of v can help us cancel the variable matrix w  and thus reducing the optimization problem size for better efficiency. accordingly  we follow a similar but different approach. in particular  let us consider to use v to predict the x via linear combinations. suppose we have w1 as the coefficient  we seek to minimize the following:
	.	 1 
one can understand eq. 1 in this way: here each column of w1 can be considered as a cluster center of the corresponding class  i.e.  the venues . then solving eq. 1 in fact simultaneously  1  pushes the representation of documents close to their respective class centers; and  1  optimizes the centers to be close to their members.
¡¡next  we cancel w1 using the unique property of our venue matrix v . setting the derivative to be zero  we have 1 =
   suggesting that
w1 =  v tv   1v tx. note that v tv is diagonal matrix and is thus invertible. plug in w1 back to eq. 1. we arrive at the optimization where w1 is canceled:
	 	 1 
where  v tv   1v t is the pseudo inverse of v . here since v tv is |v| ¡Á |v| diagonal matrix  its inverse can be computed in |v| flops. meanwhile  v  v tv   1v t is block diagonal where each block denotes a complete graph among all documents within the same venue. note that eq. 1 cannot be handled in the same way because  ata  1 is a dense matrix  resulting in a |d| ¡Á |d| dense matrix of a ata  1at  which in practice raises scalability issues.
1 learning document embedding
¡¡we have arrived at a combined optimization formulation given the above sub-problems. we will combine eq. 1  eq. 1 and eq. 1 in a unified optimization framework. define the new objective j x w  as a function of x w. we have an optimization below to learn the document embedding matrix
 	 1  where ¦Ë is the weight of regularization on w; ¦Á is the weight for learning from a; ¦Â is the weight for learning from v . in this paper  we only empirically find the best values for ¦Á and ¦Â that yield the best f-scores for the current data set. future work on how to choose parameter values will be helpful to practitioners.
¡¡the optimization illustrated above can be solved using standard conjugate gradient  cg  method  where the key step is the evaluation of objective function and the gradient. in appendix .1  we show the gradients for the combined optimization.
¡¡after x is calculated  we can use linear model in the recommendation  i.e. f  = x xtx  1xty. we can obtain efficiency advantage over the power method as in eq. 1.
1. incremental update of document embedding
¡¡an incremental version of our new method will be proposed in this section. the goal of incremental update of x is to avoid heavy computation of known documents when there is a small size of update. the incentive for designing an incremental update algorithm is to delay  or avoid  recomputation in a batch approach. the incremental update of x we will give is an efficient approximate solution. in particular  suppose we have used document d1  v1  a1 and their relationship at time t1 to compute a document embedding x1 for the document set d1. now  at time t1  we have observed an additional set of new documents d1. how can we use the pre-computed x1 to compute an embedding of d1 in x1 ¡Ê r|d1|¡Ák efficiently  note that typically |d1| is much smaller than |d1|.
1 rewriting objective functions
¡¡we rewrite the objective function in eq. 1. let x be the minimizer. we assume that the embedding of old documents is in x1 and the x1 ¡Ê r|d1|¡Ák is the embedding for d1. here xt =  x1t x1t . let the updated three graphs be encoded in the three new matrices below:
  
where the a encodes the new document-author relationship; the v encodes the new the document-venue relationship  assuming no emergence of new venues ; and the l denotes the new laplacian calculated on the updated document citation graph. by convention  the index 1 corresponds to the original part of the matrix and the index 1 indicates the new part. for example  v1 is the venue matrix at time t1 and v1 is the venue matrix at time t1.
¡¡consider the objective function in eq. 1. after several rewrites as entailed in appendix .1  the objective function in eq. 1 on the new set of matrices now becomes the following:
	j =	tr 

where the coefficients are l  a  v   and ¦² =   ;
the variables are; the param-
eters are ¦Á  ¦Â  ¦Ë.
1 efficient approximate solution
¡¡we will make the eq. 1 more efficient in this section  hoping to only calculate the incremental part of x for the new documents in d1.
¡¡first  let us assume that the incremental update of x only seek to update the embedding of d1 but does not change the original embedding of d1  i.e. that x1 is fixed. similarly  w1 is fixed for the authors observed before. second  we can see that v1 in v is fixed because documents will not change venues over time. third  we show that the segment in the new laplacian l1 is approximately zero because no old documents can cite new documents which results in relatively small stationary probabilities on the new documents  we will show more details for this proposition in appendix .1 . given the above assumptions and observations  after discarding the constant terms  we have the following optimization for incremental update of x:
where ¦² =   . the variables are x1 and w1 that has |d1| ¡Á k and |a1| ¡Á k elements respectively. since d1 and a1 are very small  the incremental calculation of x1 can be achieved very efficiently. again  this problem can be solved using conjugate gradient method where the gradients of eq. 1 are presented in appendix .1.
1. experiments
¡¡a real-world data set for experimentation was generated by sampling documents from citeseer using combined document meta-data from citeseer and another two sources  the acm guide  http://portal.acm.org/guide.cfm  and the dblp  http://www.informatik.uni-trier.de/ ley/db  for enhanced data accuracy and coverage. the meta-data was processed so that the ambiguous author names and noisy venue titles were canonicalized 1. since the data in citeseer are collected automatically by crawling the web  we may not have enough information about certain authors. accordingly  we collected the documents by those top authors in citeseer ranked by their numbers of documents. then we collected the venues of these documents. similarly  we kept those venues with most documents in the prepared subset and discarded the venues that include fewer documents. following the same procedure  two datasets were prepared with different sizes. the first dataset  referred to as ds1  has 1 authors  1 documents  1 venues  and 1 citations; the second dataset  referred to as ds1  which is larger in size  has 1 authors  1 documents  1 venues  and 1 citations.
1 evaluation metrics
¡¡the performance of recommendation can be measured by a wide range of metrics  including user experience studies and click-through monitoring. for experimental purpose  this paper will evaluate the proposed method against citation records by cross-validation. in particular  we randomly remove t documents  use the remaining documents as the seeds  perform recommendations  and judge the recommendation quality by examining how well these removed documents can be retrieved. as suggested by real user usage patterns  we are only interested in the top recommended documents. quantitatively  we define the recommendation precision  p  as the percentage of the top recommended documents that are in fact from the true citation set. the recall  r  is defined as the percentage of true citations that are really recommended in the top m documents. the fscore  which combines precision and recall is defined as f =  1 + ¦Ä1 rp/ r + ¦Ä1p   where ¦Ä ¡Ê  1 ¡Þ  determines how relatively important we want the recall to be  here we use ¦Ä = 1  i.e. f-1 score  as in many related work.  1 we have introduced a parameter in evaluation  m  which is the number of top documents we evaluate the f-score at.
1 recommendation quality
¡¡this section introduces the experiments on recommendation quality. we compare the recommendation by our algorithm with two other baselines: one based on laplacian on directed graphs  and label propagation using graph laplacian   named as lap  and the other based on singular vector decomposition of the author matrix  named as svd  1. we chose to compare with the lap method to see whether the fusion of different graphs can effectively produce additional information than the original graph citation graph; we chose the svd on author matrix as another baseline because we would like compare our method against the traditional cf method on the additional graph information  as one can argue that the significant improvement of the new method is purely due to the use of the additional information .
f   mm=tm=1m=1ds1f lap  f svd 1
11
11
1f new 111ds1f lap  f svd 1
11
11
1f new 111table 1: the f-score calculated on different numbers of top documents  m.
f   tt=1t=1t=1t=1ds1f lap  f svd 1
11
11
11
1f new 1111ds1f lap  f svd 1
11
11
11
1f new 1111table 1: the f-score w.r.t. different numbers of leftout documents  t.
¡¡table 1 and table 1 list the f-scores  defined in sec. 1  of three different methods  our new method with lap and svd  on two datasets  ds1 and ds1 . table 1 for different number of top documents evaluated on  denoted by m . we are able to see that the new method outperforms both lap and sv d significantly on both datasets in different settings of parameters. in general  the new method are 1   1 times better in f-score than lap and 1 times better than sv d. the lap method under-performs sv d on the very top documents but beats it if evaluated on more top documents. in addition  we notice that the f-scores get better in general as we look at more top documents. also  the f-scores on the smaller dataset ds1 are generally higher than those on the larger dataset ds1. here  we can see that the recommendation quality can be significantly improved by using the author matrix as the additional information. note that the different information  when used individually  such as the lap on the citation graph or the sv d on the author graph  can be not as good. however  if the multiple information are combined  the performance is greatly improved1.
1 parameter effect
¡¡the effect of parameters for the new method is experimented in this section. we experiment with different settings of dimensionality  or k  and weights on authors and venues  or ¦Á and ¦Â. in table 1  we show the f-scores for different k's. it occurs that the f-scores become higher for greater k. we believe this is because the higher dimensional space can better captures the similarities in the original citation graphs. however  on the other hand  we observe that it takes longer training time for greater k. seeking k thus become a trade-off between quality and efficiency. in our experiments  we chose k = 1 as greater k do not seem to give much better results. the cpu time for training at different k's are illustrated in table 1.
f   kk=1k=1k=1k=1ds1.1.1.1.1ds1.1.1.1.1table 1: the f-score w.r.t. different setting of dimensionality  k.
t lap t new time   kk=1k=1k=1k=1ds1s1s1s1s1sds1s1s1s1s1stable 1: the cpu time for recommendations w.r.t. different dimensionality  k.
¡¡fig. 1 illustrates the f-scores for different settings of ¦Á and ¦Â  which are respectively the weights on authors and venues. we determine which of the two components obtains greater improvement if incorporated  search for the best parameter for this component  fix it  and then search for the best parameter for the other component. in our experiments  we observe that adding the author component tends to improve the recommendation quality better so we first tune ¦Á  which yields different f-scores  as shown by the blue curve in fig. 1. then we fix the ¦Á = 1 and tune ¦Â  arriving at the best f-score at ¦Â = 1.

figure 1: f-scores for different settings of weights on the authors  ¦Á  and on the venues  ¦Â. the ¦Á is tuned first for ¦Â = 1; then ¦Â is tuned for the fixed best ¦Á = 1.
1 incremental update
¡¡here we present the experiments for another contribution of this work: incremental update. the incremental update method we propose seeks to determine an approximate embedding of documents by working with the incremental data and the relationship between the new data and the old. we evaluate this new method in its training time  recommendation quality  and propagation of errors.
¡¡fig. 1 illustrates the comparison of training time for the incremental method and batch update by percentage on both datasets. we try to use a fair baseline. in particular  we compare with a percentage of batch update time  where the percentage reflects the relative amount of incremental data. as illustrated in fig. 1  the incremental method takes on average 1   1 of the training time of batch method.
the improvement is more significant on larger dataset  ds1  than small ones  ds1 .

figure 1: training time for incremental update and batch method w.r.t. different percentage of incremental data on ds1 and ds1. the training time for batch method is the corresponding percentage of the overall training time.
¡¡the next natural question to ask is how much quality has to be compromised for the improvement of efficiency. fig. 1 present the comparison of f-scores for different percentage of incremental using the incremental method with the batch method applied to the full data. it turns out that the performance of incremental method deteriorates as the incremental data takes a large percentage. fortunately  the f-scores decrease at a slower ratio for the larger dataset  ds1 . this is because that more information is captured by the larger dataset with larger absolute size. on average  the deterioration of recommendation quality can be significant if the incremental data takes more than 1% of the data. so we would suggest re-run the batch process when the updated corpus exceeds the original size significantly.
¡¡finally  we present the propagation of errors if the incremental update is applied to multiple times. it has come to our attention that the performance deteriorates at a faster pace if one applies multiple steps of incremental updates. fig. 1 illustrates the f-scores w.r.t. different numbers of steps in the incremental updates  for different overall percentage to update. notice that the f-scores deteriorate faster if the overall percentage of update is greater. also  the fscores decrease slower at first 1   1 steps and faster from the 1rd step onwards. it is then suggested that the new incremental method should be used with caution  preferring fewer number of uses or on a larger percentage of data for each use. it seems that the error in the incremental updates is propagated more than linearly.
¡¡the incremental update methods presented in this section addresses the scalability issues in recommendation of large-scale dataset on the web. in practice  we recommend a combination of batch update and incremental update seeking a tradeoff between efficiency and scalability.

figure 1: f-scores for different percentage of incremental data in the incremental update  on ds1 and ds1  w.r.t. the batch method applied to the full data.
 

figure 1: f-scores for different numbers of steps in the incremental updates  for different overall percentage  p  to update  on ds1 and ds1.
1. related work
¡¡this work is first related to a family of work on categorization of networked documents. categorization of networked documents is developed based on the link structure and the co-citation patterns  e.g.  for web document clustering . in   all links are treated as undirected edge of the link graph and the content information is only used for weighing the links by the textual similarity between documents on both ends of the link. very recently  chung has proposed a regularization framework on directed graphs. soon after  zhou et.al.  used this regularization framework on directed graphs for semi-supervised learning  which also seek to explain ranking and categorization in the same semisupervised learning framework. later  a work by zhou et.al. extended the regularization to multiple graphs with the same set of vertices   which  however  is different from this work where the item set can be either a full set or a subset of the graphs in question.
¡¡this work also relates to the category of work that approach document analysis via embedding documents onto a relatively low dimensional latent space  1  1 . latent semantic indexing  lsi   is a representative work in this category that uses a latent semantic space to implicitly capture the information of documents. analysis tasks  such as classification  could be performed on the latent space. another commonly used method  singular value decomposition  svd   ensures that the data points in the latent space can optimally reconstruct the original documents. based on similar idea  hofmann  proposed a probabilistic model  called probabilistic latent semantic indexing  plsi . this work is similar but different to plsi in that we not only approximate one single document matrix but several matrices at the same time.
¡¡finally  this work shares the idea of related work on combining multiple sources of information. in this category  prior work by cohn and hofmann  extends the latent space model to construct the latent space from both content and link information  using content analysis based on plsi and phits   which is a direct extension of plsi on the links. in plsi+phits  the link is constructed with the linkage from the topic of the source web page to the destination web page. in that model  the outgoing links of the destination web page have no effect on the source web page. in other words  the overall link structure is not utilized in phits. communitiy discovery has also been done purely based on document content . recent work.  utilizes the overall link structure by representing links using the latent information of their both end nodes. in this way  the latent space truly unifies the content and the underlying link structure. our work is similar to that of  but we not only considers links but also co-link patterns by using the laplacian on directed graphs.
1. conclusions and future work
¡¡we address the item-based collaborative filtering problem for items that are networked. we propose a new method for combining multiple graphs in order to measure item similarities. in particular  the new method seeks a single lowdimensional embedding of items that captures the relative similarities among them in the latent space. we formulate this as an optimization problem  where the learning of three general types of graphs are formulated as three subproblems  each using a factorization strategy tailored to the unique characteristics of the graph type. based on the obtained item embedding  a new recommendation framework is developed using semi-supervised learning on graphs. in addition  we address the scalability and propose an incremental version of the new method. approximate embeddings are calculated only for new items making it very efficient. the new batch and incremental methods are evaluated on two real world datasets prepared from citeseer. experiments have demonstrated significant quality improvement for our batch method and significant efficiency improvement with tolerable quality loss for our incremental method. for future work  we will pursue other applications of the new graph fusion technique  such as clustering or classification. in addition  we want to extend our framework to graphs with hyperedges.
1. acknowledgments
¡¡this work was funded in part by the national science foundations and the raytheon corporation.
appendix .1	the gradients for eq. 1 and eq. 1
the gradients for eq. 1 are:
		 1 
		 1 
where v   =  v tv   1v t is the pseudo inverse of v . when searching for the solutions  we vectorize the gradients of x w into a long vector. in implementation  different calculation order of matrix product leads to very different efficiency. for example  it is much more efficient to calculate
 v v     i t v v     i x as  v   tv tv v  x   1v v  x + x because v and v   are very sparse. the gradients for eq. 1 are:

where
v1	1	1  	1	1
the gradients of eq. 1 w.r.t. w1 are
		 1 
.1	rewriting the objective functions
¡¡first  for the terms in eq. 1 for learning from a  we introduce another set of variables in w1 ¡Ê r|a1|¡Ák  to describe the new authors in a1. then we have . let the document-author relationship be encoded in the author
matrix. then we have the following:
a	xw	1 =
=
=
and becomes
	.	 1 
¡¡second  for the term in eq. 1 regarding learning from venue matrix  v   we assume that there are no new venues showing up between t1 and t1. so the new v takes the form
as where v1 is the venue matrix at time t1.
let the component v  v tv   1v t = ¦µ. we can see that the learning objective becomes
	 	 1 
where
  1 
and ¦² =    is a diagonal matrix whose inverse is very easy to compute. then we plug eq. 1 into eq. 1. after several simple manipulations  we arrive at the following learning objective for venues:
	 	 1 
where ¦² =  
¡¡third  for the laplacian terms in eq. 1 for learning from the citation graph d  we have the following identities:
tr xtlx 
	=	 
	=	tr  	 1 
and
		 1 
where l1 is a |d1| ¡Á |d1| matrix for the graph on d1; l1 is a |d1| ¡Á |d1| matrix for interaction between d1 and d1; and l1 is a |d1| ¡Á |d1| matrix for the graph on d1.
.1	the laplacian l is almost block diagonal:
here we will show that the laplacian l on the new matrix at time t1 is near block diagonal. recall that the citation
matrix d at time t1 can be written as.
here  d1 is the same as the citation matrix used to compute the laplacian at time t1. remember that l = i   ¦Ás  where s is the similarity measured on the directed graph d in eq. 1:
	 	 1 
where s¡¥ = ¦µ1p¦µ 1 and p is the stochastic matrix normalized from d and ¦µ is a diagonal matrix containing the stationary probabilities. we rewrite s¡¥ as follows:

 1 
where p1 p1 p1 p1 are normalized from d1 d1 d1 d1 and the diagonal matrix ¦µ1  ¦µ1  contains the stationary probabilities on the old  new  documents.
¡¡we further know that p1 = 1 because new documents d1 cannot be cited by the old documents. so we have:
	 .	 1 
and we also know that the new documents d1  with few citations among themselves  mainly cite the old documents in d1. thus  in the case when d1 is much larger than d1  the stationary probabilities on the new documents are very small  i.e. ¦µ1 ¡« 1. this gives us ¦µ 1. so we have shown that is almost diagonal. let us rewrite s¡¥ as:
 . since l = i ¦Ás where
   we know that the new
which is almost block diagonal  i.e. 1. however  note that is not necessarily zero because ¦µ contains
1
both ¦µ1 and ¦µ 1. also  note that we do not claim that l1 in the new l is identical to the original laplacian on d1. nevertheless  we discard the term in eq. 1 because x1 is assumed to be unchanged.
