we address the problem of classification in partially labeled networks  a.k.a. within-network classification  where observed class labels are sparse. techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes. however  relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning  during training phase  and/or inference  during testing phase . this situation arises in realworld problems when observed labels are sparse.
　in this paper  we propose a novel approach to within-network classification that combines aspects of statistical relational learning and semi-supervised learning to improve classification performance in sparse networks. our approach works by adding  ghost edges  to a network  which enable the flow of information from labeled to unlabeled nodes. through experiments on real-world data sets  we demonstrate that our approach performs well across a range of conditions where existing approaches  such as collective classification and semi-supervised learning  fail. on all tasks  our approach improves area under the roc curve  auc  by up to 1 points over existing approaches. furthermore  we demonstrate that our approach runs in time proportional to l ， e  where l is the number of labeled nodes and e is the number of edges.
categories and subject descriptors
h.1  database management : database applications - data mining; i.1  artificial intelligence : learning; i.1  pattern recognition : models - statistical
general terms
algorithms  design  performance  experimentation.
keywords
statistical relational learning  semi-supervised learning  collective classification  random walk.
1. introduction
　we address the problem of within-network classification in sparsely labeled networks. given a network of both labeled and unlabeled
copyright 1 association for computing machinery. acm acknowledges that this contribution was authored or co-authored by an employee  contractor or affiliate of the u.s. government. as such  the government retains a nonexclusive  royalty-free right to publish or reproduce this article  or to allow others to do so  for government purposes only.
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm 1-1-1/1 ...$1.
nodes  our goal is to provide labels for the unlabeled nodes. here  labeling simply means assigning each node a class from among a set of possible classes  see figure 1 .
 b  our solution: 
figure 1: problem definition and our solution. we address the problem of label sparsity in network classification by creating  ghost edges  using random walks with restarts. these ghost edges connect an unlabeled node to the most relevant labeled nodes.
　suppose we want to identify fraudulent users of a cell phone network. in this case  our set of possible classes for users is {fraud  legit}. if a user is known to be involved in fraud  we assign a label of  fraud.  if a user is known to not be involved in fraud  we assign a label of  legit.  otherwise  we leave the node unlabeled. our goal is then to assign labels   fraud  or  legit   to the unlabeled users.
　cell phone fraud is an example where networks are often very sparsely labeled. we have a handful of known fraudsters and legitimate users  but the labels are unknown for the vast majority of users. for such applications  it is reasonable to expect that we have access to labels for fewer than 1%  1%  or even 1% of the nodes.
　in addition to being sparsely labeled  cell phone networks are generally anonymized. that is  the nodes often contain no attributes besides class labels. these sparsely labeled  anonymized networks are the focus of this study. put another way  our work focuses on univariate within-network classification in sparsely labeled networks.
　techniques for statistical relational learning  srl  have been shown to perform well on network classification tasks because of their ability to exploit dependencies between labels of related nodes . these techniques use labeled data to learn a model of the dependencies between labels of neighboring nodes. the labels of unlabeled nodes can then be inferred by propagating information from labeled nodes throughout the network  according to this learned dependency model  i.e.  collective classification . unfortunately  sparse labels cause a number of problems for relational classifiers. first  collective classification techniques can fail without sufficiently many labels to seed the inference process . second  fewer labeled nodes means fewer training examples from which to learn dependencies. finally  even when a node is labeled  many of its neighbors will not be. this is equivalent to having many missing attribute values in a traditional supervised learning setting. the result is that it is very difficult to learn an accurate model of the dependencies present in the data.
　within-network classification can also be viewed as a graphbased semi-supervised learning problem since we have both labeled and unlabeled data available at training time. although semisupervised learning  ssl  is generally applied to non-network data  graph-based ssl approaches can be applied to within-network classification  as we show. ssl techniques have the advantage that they do not rely as heavily on labeled training data and can make use of unlabeled data as well. however  graph-based ssl approaches generally do not learn dependencies from data  but instead assume local label consistency  i.e.  that nearby points tend to have the same label . if the label consistency assumption is not met  ssl techniques can perform extremely poorly. figure 1 provides a preview of our results. it plots classifier performance  measured by area under the roc curve  a.k.a. auc  versus data set id. the data sets are ordered according to their local consistency  a.k.a. homophily score .1 notice that the two proposed ghostedge methods are consistently at the top  while the competing methods may perform very poorly depending on the data set's local consistency.
　in this paper  we explore a novel approach to within-network classification that capitalizes on the strengths of both statistical relational learning  srl  and semi-supervised learning  ssl   to produce a classifier that is robust in the face of both sparse labeling and low label consistency. our contributions are as follows:
  we propose a novel approach to within-network classification  based on the creation of  ghost edges  that enable the propagation of information from labeled to unlabeled nodes.
  we demonstrate that our approach is robust to both sparse labeling and low label consistency  performing well consistently across a range of real world classification tasks where collective classification or semi-supervised learning fail.
  we demonstrate the scalability of our approach. specifically  we show that our methods run in time proportional to l ， e where l is the number of labeled nodes and e is the number of edges in our graph.
　the rest of the paper is organized as follows. in section 1  we review related work. we present our proposed methods in section 1. sections 1 and 1 describe our experimental methodology and results. finally  we offer conclusions in section 1.

figure 1: a preview of our results: classification performance of various approaches when half of the nodes in a graph data set are unlabeled. the solid blue  square icon  and red  circle icon  lines on the top represent our ghostedge approaches  which perform consistently well regardless of the degree of local consistency in the data set.
1. related work
　in recent years  there has been a great deal of work on models for learning and inference in relational data  1  1  1  1  1 . for within-network classification tasks where we have sparse labels  we categorize the previous work into two main groups: collective classification and graph-based semi-supervised learning.
　collective classification. collective classification deals with label sparsity by simultaneously labelling a set of related nodes  allowing estimates of neighboring labels to influence one another. representative work in this line started with the seminal paper of
chakrabarti et al. on using hyperlinks to for hypertext classification . sen et al.  provide a careful empirical study of the various procedures for collective inference. macskassy and provost  provide a nice case-study of previous work in learning attributes of networked data. mcdowell et al.  demonstrate that  cautious  collective classification procedures produce better classification performance than  aggressive  ones. they recommend only propagating information about the top-k most confidently predicted labels. one of the major advantages of collective classification lies in its powerful ability to learn various kinds of dependency structures  positive vs. negative auto-correlation  different degrees of correlation  etc . however  as pointed out in   when the labeled data is very sparse  which is quite common in the sparsely labeled networks that we are particularly interested in  the performance of collective classification might be largely degraded due to the lack of sufficient neighbors. this is exactly one major advantage of the proposed method  - we incorporate informative  ghost edges  into the networks to deal with sparsity issues. from this point of view  our method shares the similar spirit as the work by macskassy . however  in   the additional edges are calculated based on attribute-similarity  specifically  text similarity . if
such information is not available  which is quite common in many real applications and which is the case we are interested in   the method in  is not applicable. on the other hand  we can always leverage our  ghost edges  since they are based on the intrinsic
structure of the networks. lastly  the algorithm proposed in  does not learn the weights  instead it combines the weights through a heuristic.
　graph-based semi-supervised learning. the problem of withinnetwork classification can also be thought of as the graph-based semi-supervised learning problem. here  the basic idea is to estimate a function on the graph which satisfies two kinds of constrains:  1  the consistency with the label information and  1  the smoothness over the whole graph. the methods in this area mainly vary in the different ways to balance these two constraints. for example  zhu's gaussian random field  grf  method  puts a hard constraint on the label consistency and then achieves the smoothness by the harmonic function. zhou's global and local consistency method  combines the two kinds constraints by a regularization parameter and solves a quadratic optimization problem. for more on graph-based semi-supervised learning  we refer the reader to an excellent survey by zhu . by exploring the global structure  i.e. smoothness  over the whole graph  graph-based semi-supervised learning methods usually outperform the traditional methods  particularly when there are very few labeled nodes in the networks. however  the constraint on smoothness implicitly assumes positive auto-correlation in the graph  that is nearby nodes tend to share the same class labels  i.e.  homophily . when such an underlying assumption does not hold  negative auto-correlation  the degree of auto-correlation being small  etc   the performance might be largely affected. this is another advantage of our  ghost edge  method  - it leverages the additional learning stage to recover the intrinsic correlation structure.
1. proposed method
　in this section  we explain the motivation behind using ghost edges for within-network classification and discuss how ghost edges are created. we then describe how we take advantage of ghost edges to improve classification performance.
1 motivation and general approach
　the power of statistical relational learning  srl  lies in the fact that networks generally exhibit predictable relationships between class labels of related nodes. therefore  labeled nodes provide a great deal of information about their unlabeled neighbors. suppose we have an unlabeled node  u  and we have a good understanding of the relationship between the class label of u and the class labels of u's neighbors  n. if all nodes in n are labeled  we should be able to do a very good job of predicting the label of u. however  suppose that very few nodes in n are labeled. there are two ways of looking at this problem:
1. node u shares edges with plenty of other nodes in the network  but too few of those neighboring nodes are labeled.
1. there are plenty of labeled nodes in the network  but node u shares edges with too few of them.
the first way of looking at the problem is addressed by techniques such as collective classification. our approach  based on ghost edges  addresses the second way of looking at the problem.
　the idea behind our approach is to add  ghost edges  between labeled nodes and unlabeled nodes to allow the information from labeled nodes to inform our predictions on unlabeled nodes. of course  in order for this to work  we must carefully select pairs of nodes to connect via ghost edges. in particular  the success of our approach relies on our ability to choose pairs of nodes with labels that relate to each other in a predictable way. our conjecture is that nodes which are  closer  in a network will tend to have more predictable relationships between their class labels  and that nodes which are directly connected by an observed edge are simply a special case of this.
　based on this conjecture  we create ghost edges as follows. we create a single ghost edge between every hlabeled  unlabeledi pair of nodes in our graph. we then assign a weight to each ghost edge based on the proximity of the nodes that the edge connects. a higher weight indicates that the connected nodes are closer together in the network  i.e.  have higher proximity . the following subsection describes our approach to quantifying node proximity.
1 quantifying node proximity using random walk with restart
　to calculate rwr scores between each pair of nodes  we use a variation on the fast random walk with restart method proposed by tong et al. . the end result is that the algorithm can quickly give the proximity score ri j  indicating how easy it is to reach node j from node i. in more detail  ri j gives the steady-state probability to find a particle at node j  when this particle does a random walk with restarts from node i. the score ri j is high if there are several  high-weighted  short paths from i to j. a random walk with restarts  or  equivalently  personalized pagerank  works as follows: a particle starts at node i  moves randomly along the edges of the given graph  and with probability 1   c  say  c = 1   the particle flies back to the initial node i. for more details  see .
1 handling degrees of label consistency
　there is a subtle  but very important step in our rwr algorithm. the problem is to handle varying degrees of local label consistency  including cases where labels of neighbors are inversely related. in cases with high label consistency  i.e.  nearby nodes have similar labels   all methods tend to do well. in cases where consistency is low  as in  say  a near-bipartite graph of dating relationships with mostly male-female edges  the semi-supervised learning methods will not work  exactly because they have the local consistency assumption hardwired in their optimization functions.
　similarly  if we add ghost edges carelessly  we will add a lot of incorrect edges. what are the right edges to add so that our method can easily handle cases with varying degrees of label consistency 
　the idea is to do rwr  but to insist on even-length paths. we shall refer to it as the even-step rwr. mathematically  this means that we replace the adjacency matrix a with its square b = a   a. then  we compute the rwr scores using the b matrix.
　why does this approach work regardless of the degree of label consistency  the reasons are subtle: for the case of inverse classlabel relationship  the immediate neighbors are exactly the ones we want to avoid  which is exactly what the even-step rwr does. for the case of high local consistency  social networks typically have triangles and high  clustering coefficient.  thus  even if we only focus on even step paths  our random walk will still give high scores to nodes that are well-connected. in practice  we find that the even-step approach works well for intermediate consistency values as well.
　in conclusion  with the subtle technique of even-step rwr  our ghostedge methods can handle varying degrees of local label consistency  as we show in the experiments section.
1 classifier design
　we propose two novel approaches to within-network classification: the ghost edge non-learning classifier  ghostedgenl  and the ghost edge learning classifier  ghostedgel . both approaches are based on propagating class labels throughout a network using ghost edges created via random walk with restart  namely  evenstep rwr . however  the two classifiers make use of ghost edges in different ways. there are two ways in which the approaches differ:
1. use of available labels. ghostedgenl is a non-learning method. it simply assumes that neighbors connected by a ghost edge will tend have the same class labels and that this tendency is stronger across edges with higher weights. ghostedgel  on the other hand  uses the labeled nodes in a network as training examples to learn the dependencies between class labels of both observed neighbors and ghost neighbors.
1. use of rwr scores. ghostedgenl makes use of all ghost edges  although it puts more weight on edges with higher proximity scores. ghostedgel bins ghost edges based on their proximity scores and then uses labeled data to learn weights on each bin  based on the predictiveness of its edges.
1.1 the ghostedgenl classifier
ghost edges can be added to any relational classifier. for ghost-
edgenl  we chose to use a simple relational neighbor classifier . this classifier predicts the class of a node based entirely on the class labels of neighboring nodes and performs no learning. it estimates the probability of node u belonging to class c as the weighted proportion of neighboring nodes that belong to class c. ghostedgenl uses the proximity score on the ghost edge between nodes as a weight. ghostedgenl ignores observed edges.
1.1 the ghostedgel classifier
　for ghostedgel  we chose a learning link-based classifier . this classifier uses logistic regression  lr  to build a discriminative model of node i's class given the class labels of nodes directly connected to i. since lr expects a fixed-length feature vector  the set of neighboring class labels is summarized by a statistic such as the count or proportion of neighbors of each class.
　we initially implemented ghostedgel using a standard lr model. however  we found that lr often failed to appropriately weight features based on their predictiveness. we achieved better results using an ensemble of lr models we refer to as logforest. the logforest model is inspired by breiman's random forest classifier . we use a bag of lr classifiers  where each is given a subset of log m  + 1 of the m total features. for this study  our logforest model uses 1 lr classifiers.
　ghostedgel divides ghost edges into six bin as follows: a contains ghost edges with scores in the top 1%  b gets edges scoring between the top 1%-1%  c between 1%-1%  d between 1%  e between 1%-1%  and f between 1%-1%. there is no overlap between bins.
　the ghostedgel classifier uses the following features:  1  count of neighbors of each class across observed edges and  1  count of neighbors of each class across ghost edges for each bin. so  for a binary classification problem with six bins  we have 1 features.
　like any relational learning method  ghostedgel learns the dependencies between class labels of neighboring nodes. however  in addition  the model learns how much weight to put on observed edges vs. ghost edges with different proximity scores  and the model can potentially learn different dependencies for each of these edge types.
1 scalability
　in even-step rwr  the ranking vector ~ri =  ri j  for a given labeled node i is defined as:
~ri t + 1  = ca1~ri t  +  1   c ~ei
where ~ei is the starting vector for the node i  c is the fly-out probability  t is the iteration number  and a is the normalized graph
laplacian .1
we can use the following iterative strategy:
~ri 1  = ~ei
for t = 1 ...  do the following two steps: 1 ~ri t  = a~ri t   1  1 ~ri t  ○ ca~ri t  +  1   c ~ei 1 　the complexity for each step t is clearly o e . so  to get one ranking vector  the complexity is o t ，e   where t is the maximum number of iteration needed to reach the steady state. overall  we need l such ranking vectors  so that we will get all u〜l proximity scores . therefore  the overall complexity is o l，e   omitting the constant t  .
　next  we will justify that the above iterative procedure will actually converge. to see this  we can re-write ~ri t  as:

since a is normalized graph laplacian  we have  1 ＋ λ a  ＋
1  where λ a  is the eigenvalue of a . therefore  a 1t  is bounded. on the other hand  ct ★ 1 with t ★ infinity. thus  k ct a 1t ~eik goes to 1 as t goes to infinity  which completes the proof.
1. experimental design
　our problem setting is within-network classification in sparsely labeled networks. we compare several approaches to overcome label sparsity:  1  collective classification   1  graph-based semisupervised learning methods  and  1  our  ghost edge  label propagation approach  ghostedge . the experiments are designed to answer the following research questions:
  how do the proposed ghostedge methods do against the competition 
  what is the impact of local label consistency  i.e.  homophily  and lack of it 
1 data sets
　we present results on four real-world data sets: political book co-purchases   enron emails   reality mining cell-phone calls   and high-energy physics citations from arxiv  a.k.a. hep-th  . our tasks are to identify neutral political books  enron executives  reality mining study participants  and hep-th papers with the topic  differential geometry   respectively.
　figure 1 summarizes our prediction tasks. the sample column describes the method used to obtain our experimental subset of data from the full data set: use the entire set  full   use a time-slice  time   or sample a continuous subgraph via breadth-first search  bfs . the task column indicates the class label we are trying to predict. the |v |  |l|  and |e| columns indicate counts of total nodes  labeled nodes  and total edges in each network  respectively. the p +  column indicates the proportion of labeled nodes that have the class label of interest  e.g.  1% of the political books are neutral .
　note that for the enron and hep-th tasks we have labels for only a subset of nodes  which we refer to as  core  nodes  and can only train and test our classifiers on these nodes. however  unlabeled nodes and their connections to labeled nodes may still be exploited for label propagation.
data setsampletask|v ||l ||e |p + enronfullexecutives 11.1hep-thtimediff. geometry 11.1political booksbfsneutral 11.1reality miningbfsin study 11.1figure 1: summary of data sets and prediction tasks.
1 competing methods
　from our methods  we use the two versions:  a  ghostedgenl which is the non-learning ghostedge-based approach as described in section 1.1 and  b  ghostedgel which is the learning ghostedgebased approach described in section 1.1.
　the competing methods fall under two categories:  1  collective classification  and  1  graph-based semi-supervised learning methods. both categories of methods were designed to handle label sparsity.
1.1 competing methods
on each classification task  we ran seven individual classifiers:
1. logforest  an ensemble logistic link-based model without collective classification
1. logforest+ica  an ensemble logistic link-based model  which uses the iterative classification algorithm to perform collective classification
1. wvrn  a relational neighbor model without collective classification
1. wvrn+rl  a relational neighbor model  which uses relaxation labeling for collective classification
1. grf  the ssl gaussian random field model
1. ghostedgenl  our ghostedge-based classifier without learning
1. ghostedgel  our ghostedge-based classifier with learning
we describe each of the competing classifiers next.
　logforest is an ensemble of logistic regression classifiers as described in section 1.1. the model takes two features as input: the count of unique neighbors of the positive class and the count of unique neighbors of the negative class. our base logforest classifier does not use collective classification. therefore  any neighbors with missing class labels are simply ignored.
　logforest+ica uses the base logforest classifier  but performs collective classification using the ica algorithm described in section 1.1. we tried the logforest classifier with both the ica and rl collective classification algorithms across our range of classification tasks. the performances of the two algorithms were comparable  but ica performed slightly better overall. this is consistent with previous results .
　wvrn is the weighted-vote relational neighbor classifier . given a node i and a set of neighboring nodes  n  the wvrn classifier calculates the probability of each class c for node n as:
		 1 
where n is the set nodes that neighbor n  z = pni（n w n ni   and w n m  is the weight on the edge between n and m. for the baseline wvrn model  w n m  is simply the number of observed edges between n and m.
　note that in cases where a node has no labeled neighbors  we will end up with p ci = c  = 1 for all c. in such cases  we simply assign probabilities to each class based on priors observed in the training data. our base wvrn classifier does not use collective classification. therefore  any neighbors with missing class labels are simply ignored.
wvrn+rl uses the base wvrn classifier  but performs collec-
tive classification using the rl algorithm described in section 1.1. we tried the wvrn classifier with both the ica and rl collective classification algorithms across our range of classification tasks. the rl algorithm performed better overall. this is consistent with previous results .
　grf uses the gaussian random field approach of zhu et al. . we ported zhu's matlab code1 for use in our experimental framework and double checked our results with the original matlab code. we made one small modification to zhu's original code to allow it to handle disconnected graphs. zhu computes the graph
laplacian as l = d   cw  where c = 1. we set c = 1 to ensure that l is diagonally dominant and thus invertible. we found that our change had no substantial impact on classification performance.
1.1 collective classification
　to perform collective classification  we use both the iterative classification algorithm  ica  and relaxation labeling  rl  . we also ran preliminary experiments using gibbs sampling   which yielded results comparable to ica. this is consistent with findings of other researchers  1  1 . in our experiments  the logforest classifier performed better overall using ica and the wvrn classifier performed better using rl. therefore  we report results only for these combinations.
　ica initially assigns labels to unlabeled nodes  u  based on what is known in each unlabeled node's local neighborhood. nodes with no labeled neighbors are temporarily assigned a label of null. then  until either all class labels have stabilized or a certain number of iterations have elapsed  a new label is assigned to each ui （ u  based on the current label assignments of ui's neighbors. rl is similar to ica except that instead of each ui having a current label assignment  ui has a current probability distribution on the set of labels. thus  the uncertainty associated with a label assignment is retained until the algorithm terminates and a final label is assigned. unlabeled nodes are initially assigned the prior distribution  observed in the training data. we perform simulated annealing to catalyze convergence.
1 experimental methodology
　for all results presented here  the basic experimental setup is the same. each data set contains a set of core nodes for which we have ground truth  i.e.  we know the true class labels . in all cases  classifiers have access to the entire data graph during both training and testing. however  not all of the core nodes are labeled. we vary
the proportion of labeled core nodes from 1% to 1%. classifiers are trained on all labeled core nodes and evaluated on all unlabeled core nodes.
　our methodology is as follows. for each proportion of core nodes labeled  we run 1 trials and report the average performance. for each trial and proportion labeled  we choose a class-stratified random sample containing  1   proportion labeled % of the core instances as the test set and the remaining core instances become the training set. note that for proportion labeled less than 1  or greater than 1 trials   this means that a single instance will necessarily appear in multiple test sets. the test sets cannot be made to be independent because of this overlap. however  we carefully choose the test sets to ensure that each instance in our data set occurs in the same number of test sets over the course of our experiments. this ensures that each instance carries the same weight in the overall evaluation regardless of the proportion labeled. labels are kept on the training instances and removed from the test instances. we use identical train/test splits for each classifier.
　our experimental framework sits on top of the open source weka system . we implement our own network data representation and experimental code  which handles tasks such as splitting the data into training and test sets  labeling and unlabeling of data  and converting network fragments into a weka-compatible form. we rely on weka for the implementation of logistic regression and for measuring classifier performance on individual training/test trials.
　we use the area under the receiver operating characteristic  roc  curve  auc  as a performance measure to compare classifiers. we chose auc because it is more discriminating than accuracy. in particular  most of our tasks have a hight class-skew and accuracy cannot adequately differentiate between the classifiers.
1. experimental results
　in this section  we discuss the results of our experiments. we assessed significance of the results using paired t-tests1  p-values ＋ 1 are considered significant .
1 effects of ghost edges
　figures 1 and 1  respectively  compare the performance of wvrn  with and without rl  to ghostedgenl and logforest  with and without ica  to ghostedgel. we see a consistent and often dramatic increase in performance over the baseline wvrn and logforest models due to the use of ghost edges. ghostedgenl significantly outperforms wvrn on enron ＋ 1  political books 1   1  and hep-th and reality mining for all proportions labeled. ghostedgel significantly outperforms logforest on enron and reality mining ＋ 1  political books − 1  and hep-th at all proportions labeled.
　in many cases  the ghostedge classifiers also outperform collective classification. ghostedgenl significantly outperforms wvrn+rl on enron ＋ 1  hep-th ＋ 1  and reality mining for all proportions labeled. ghostedgel significantly outperforms logforest+ica on enron ＋ 1  hep-th at all proportions labeled  political books − 1  and reality mining ＋ 1.
　figure 1  previewed in section 1  compares the performance of various approaches to handle label sparsity with 1% of core nodes labeled: ghost edges  collective classification  and gaussian random fields. this figure demonstrates the robustness of the ghostedge methods across a range of data sets with varying degrees of local consistency among labels  see figure 1 . both ghostedgenl and ghostedgel are consistently high performers across all tasks. all of the other methods perform poorly  i.e.  − 1 auc points from the top  on at least one of the data sets.
　figure 1 presents a more complete comparison of the approaches as the proportion of labeled nodes varies. here  we see that the ghostedge methods perform well in comparison to other approaches  regardless of the proportion of nodes labeled. for all data sets and proportions labeled  one of the ghostedge classifiers is always the top performer  or tied at the top . we note that there are occasionally substantial differences in performance between ghostedgel and ghostedgenl. we present a more detailed discussion of learning and non-learning methods in section 1.
1 effects of learning
　figure 1 reveals a couple of interesting things about learning vs. non-learning classifiers. first  learning methods in general are hurt more than non-learning methods by a smaller proportion of labeled nodes because learning methods rely on training examples to generate an accurate dependency model. figure 1 shows the average number of training examples available for each data set at each proportion labeled. the political books and hep-th data sets have very few training examples available at the lower proportions of labeled nodes  1 and 1  respectively at 1 labeled . correspondingly  it is on these data sets that we see a dip in the learning methods relative to the non-learning methods at lower proportions of labeled data.
　the second thing to note is that the performance of both grf and wvrn+rl on the reality mining task actually decreases as more labels are made available. this is because there is an inverse relationship between class labels of neighbors  see auto-correlation scores in figure 1 . so  these non-learning methods take whatever truth they are given and use it to make exactly the wrong decision. the more information they get  the worse they perform. we see this same effect in figure 1 with wvrn. ghostedgenl overcomes this problem by using even-step rwr  as described in section 1. the relative performance of logforest+ica increases with respect to grf and wvrn+rl as label consistency decreases since the logforest model is able to learn dependencies among neighboring labels  figure 1 .
1 effects of collective classification and semisupervised learning
　figures 1 and 1 allows us to compare the performance of the collective classification approaches  i.e.  wvrn+rl and logforest+ica  and the grf semi-supervised approach across data sets. on all tasks  the performance of wvrn+rl and grf is essentially equivalent  although grf does perform significantly better than wvrn+rl in terms of auc on all data sets except hepth. we do not report results for wvrn+ica  but we found that wvrn+rl performed much better than wvrn+ica overall. on the other hand  the logforest classifier demonstrated roughly equivalent performance regardless of the collective inference procedure used. these results are consistent with previous findings .
1. conclusions
　we focus on the problem of predicting node labels in a large graph  when  a  there are few labeled nodes and  b  local consistency  a.k.a. homophily  does not necessarily hold. to address the first problem  we introduce 'ghost edges'  by judiciously adding edges between nodes  according to rwr proximity. to address the
　


1	1	1	1	1
political books: proportion of core nodes labeled	1	1	1	1	1
reality mining in-study: proportion of core nodes labeledfigure 1: comparisons of wvrn  wvrn+rl  and ghostedgenl. adding ghost edges boosts performance on all data sets.

	enron: proportion of core nodes labeled	hep-th: proportion of core nodes labeled

1	1	1	1	1
political books: proportion of core nodes labeled	1	1	1	1	1
reality mining in-study: proportion of core nodes labeledfigure 1: comparisons of logforest  logforest+ica  and ghostedgel. adding ghost edges boosts performance on all data sets.
　
1	1	1	1	1
enron: proportion of core nodes labeled
1	1	1	1	1

	1	1	1	1	1
political books: proportion of core nodes labeled	1	1	1	1	1
reality mining in-study: proportion of core nodes labeledfigure 1: comparisons of our approaches  ghostedgel and ghostedgenl  to a purely semi-supervised approach  grf   and two collection classification approaches  logforest+ica and wvrn+rl  as the proportion of labeled nodes varies.hep-th: proportion of core nodes labeled
　
second problem  we propose to bypass all the activation-spreading methods  which implicitly assume homophily   and instead we use a classifier on a carefully chosen set of features from observed  as well as 'ghost-edge' neighbors. a subtle  but vital point is that we consider rwr not on the original matrix  but on its square. this change makes our method robust  regardless of the degree of homophily. in other words  our method does well even when the local consistency assumption is not met.
　we performed experiments on several real  publicly available data sets  measuring the auc. the competitors were carefully chosen to be the state of the art. our method is very robust  performing as well as or better than the best competitor across tasks. all other classifiers we evaluated perform poorly in some cases  depending on the degree of homophily. we also showed that the complexity of our approach is o l ， e   where l is the number of labeled nodes and e is the number of edges. therefore  the approach is suitable for large data sets  provided that known labels and edges are sufficiently sparse.
1. acknowledgements
　this work was performed under the auspices of the u.s. department of energy by lawrence livermore national laboratory under contract de-ac1na1  llnl-conf-1   and based upon work supported by the national science foundation under grant no. iis-1. this work is also partially supported by the pennsylvania infrastructure technology alliance  pita   an ibm faculty award  a yahoo research alliance gift  with additional funding from intel  ntt and hewlett-packard. any opinions  findings  and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation  or other funding parties.
