expert search is about finding people rather than documents. the goal is to retrieve a ranked list of candidates with expertise on a given topic. the task is studied in the context of the enterprise track.
we describe an approach that compares topic profiles and candidate profiles directly. these profiles are not based on unordered sets of documents  but on ranked lists. this allows us to differentiate between documents that are highly related to a topic or a candidate and documents that are only marginally related. the ranked lists for topics and candidates are obtained by simple retrieval queries. the correlation between the ranked list of documents for a topic and the ranked list for a candidate is used as an indicator of the candidate's expertise on the topic. we study different ways to rank documents for the candidate profiles as well as various ways of comparing the document and candidate based ranked lists. experiments show that starting from the right candidate profiles  reasonable results can be obtained. furthermore  it seems important to take a correlation measure that takes into account the orderings of documents in both the candidate profile and the documents profile.
1. introduction
trec's enterprise track focuses on information retrieval problems in an intranet setting. the track tackles the problems of dealing with a heterogeneous collection containing web pages  email archives  meeting reports  technical documents  memos and more. the tasks studied are the following
discussion search retrieve arguments  pro's and con's around a given topic  e.g.  blocking pop-ups or is xhtml more accessible than html
expert search retrieve experts on a given topic  e.g.  rdf data access  svg  or privacy on the web.
we only participated in the expert search task  a task that is common in enterprises and other organisations. the goal of this task is to retrieve a ranked list of at most 1 experts for each topic  and to provide up to 1 documents with each expert that support their expertise. the experts come from a predefined set of candidates  each candidate has an identifier plus a name and email address. topics for the 1 expert search task are created by the participating groups and have the typical trec format with title  description and narrative.
the approach we take is based on comparing topic and candidate profiles. we produce a ranked list for each topic and one for each candidate expert and correlate them. the lists with highest correlations indicate experts on topics.
the remainder of this paper is organised as follows. section 1 discusses other approaches to the same task. section 1 explains how correlations between document rankings can help to identify experts. experimental results are discussed in section 1. the paper ends with a summary of the main conclusions and a discussion of possible points for improvement and future research in section 1.
1. related work
two typical approaches to expert ranking exist  one based on document ranking  the other on candidate profiles. the document ranking approach  1  1  starts with a document ranking for the topic on which we want to find experts. the top ranked documents are then analysed to locate occurrences of candidates  for example by looking for their names and email addresses. candidates are subsequently ordered by decreasing number of occurrences. figure 1-a illustrates this scheme.
the candidate profile approach  1  1  starts by creating a profile for each candidate  for example by gathering all documents authored by the candidate  or all documents mentioning the candidate. these profiles are then treated as  large  documents and ranked according to the topic of interest. the highest ranked profiles are assumed to correspond to the experts on the topic. figure 1-b illustrates this strategy.

the document ranking based approach: rank docu- the candidate profile based approach: construct a ments based on topic  copyright forms   and locate profile for each candidate expert  and rank the profiles candidates in the top ranked documents. the number as if they were documents.
of candidate occurrences is assumed to be an indicator of the candidates expertise.
	 a 	 b 
figure 1: two typical approaches to expert search.the approach discussed here can be viewed as a mix of the document ranking and candidate profiling approaches. while we start from a document ranking  we do not directly identify candidates in the retrieved documents. instead  we investigate whether the retrieved documents are included in the candidate's profile. in our approach  candidate profiles are rankings rather than sets. that means documents can be included in a candidate's profile to different degrees. in a sense the relation between documents and candidates is weighted. others have studied this weighting on the side of topical ranking: candidates matching documents retrieved at a higher rank are more likely to be experts than candidates matching documents further down the topical ranking  see for example . the use of a similar weighting on the candidate side of the coin is something we have not seen before.
1. correlating rankings
our approach to expert search is based on correlating candidate profiles and topic profiles. each of these profiles is a simple document ranking. the correlation between the rankings is an indication of the candidate's topical expertise. the intuition behind this approach is the following. if person x is an expert on topic y  the documents retrieved when searching for person x can be expected to overlap with the documents retrieved when searching for topic y. of course  not all documents on topic y will mention person x  and not all documents that mention the person will be on topic  but a reasonable overlap can be expected  at least an overlap bigger than for non experts on y. we use this intuition and compare the topic based ranking and the candidate based ranking directly.
in our setting  a topical profile is the document ranking obtained from a query on a language modelling based ir system using only the topic's title field. for candidate profiles  we experimented with different variants as explained in the following sub-section.
1 candidate profiles
the candidate profiles we use  are ranked lists of documents. we investigate various ways of constructing these. our baseline is to rank the documents using simple queries based on the names and email addresses of the candidates. no special processing of names or email addresses in collection or queries is used. this means these are tokenised as any other string. this makes this approach a relatively poor baseline since persons sharing a first or last name  or a domain name in their email address are easily confused.
in a second document ranking for the candidates  we tackle these problems by using the w1c corpus with candidate annotation as created by jianhan zhu at the open university. this corpus is a copy of the original w1c collection with the occurrences of the candidates annotated with special tags. a small sample is shown in figure 1. we transform this collection to a collection of only candidates. in this candidate collection  all text from the documents is removed and the candidate tags are replaced by candidate terms  see figure 1 . candidate queries against this candidate collection can easily retrieve the most relevant documents for a given candidate based on term frequency and inverse document frequency of the candidate term.
the two ways of constructing candidate profiles are topic independent. since a person may be an expert on more than a single topic  this may be problematic. therefore  we try to focus the candidate profiles by making them topic specific. this third candidate ranking is based on queries consisting of the original title queries as provided by the
 p the following people have contributed either through feedback or beta-testing  hope everyone is cited : /p   ul 
 li   candidate-1 tim berners-lee /candidate-1   /li 
 li   candidate-1 bert bos /candidate-1   /li 
 li al gilman /li 
 li daniel la liberte /li 
 li  candidate-1 gerald oskoboiny /candidate-1  /li 
 li  candidate-1 ralph swick /candidate-1  candidate-1 swick /candidate-1  /li   li ir e1 ne vatton /li 
﹛ li .. and everyone on the hypermail public mailing list. /li   /ul figure 1: a sample from the tagged w1c collection. doc docno= lists-1   candidate1 candidate1 candidate1 candidate1 candidate1 candidate1 candidate1 candidate1
 /doc 
figure 1: a sample document from the w1c based candidate collection.
track organisers  augmented with a single special candidate term. for example  for topic ex1 and candidate-1  the query would be privacy on the web candidate1. this query is issued against a modified version of the tagged w1c corpus  where candidate tags are replaced with candidate terms and the rest of the text is left untouched.
1 comparing rankings
to compare topic profiles and candidate profiles  or rather the two corresponding document rankings  we use two basic approaches  spearman's rank correlation  and a blind feedback like approach that assumes the top n for one of the rankings  topic based or candidate based  are relevant and measures average precision on the other ranking. the techniques are detailed below.
1.1 spearman rank correlation
spearman's rank correlation  or spearman's 老 is a non-parametric test that measures the strength of the correlation between two variables:
	 	 1 
where d is the difference in rank of corresponding variables  and n is the number of paired values. our variables are the topic based score and candidate based score for a given document. to limit computational costs  we compute spearman's 老 based only on the top k returned documents for topic queries. documents that do not occur in the candidate rankings are assumed to be at rank k +1. note that this is a best case scenario assumption  thus we over-estimate 老.
spearman's 老 measures the correlation for the complete ranked list. in our setting  we would like to emphasise correlations at the top of the ranked list and down-weight the correlations further down. to this end  we use a log based variant of spearman's rho. we first perform a log transform on the ranks before computing the correlation. note that while a log transformation of the scores would not influence the ranking  such a transformation on the ranks does influence the correlation. we refer to the traditional spearman correlation as sp and to the log-based variant as log-sp
1.1 blind feedback ap
another approach to compare the rankings is similar to blind feedback. here  we assume the top k documents for the topic based ranking are relevant  and we compute the average precision  ap  of the candidate ranking based on that. this will reward candidate queries that have retrieved many of the documents in the topic ranking  especially if these are retrieved at the top ranks.
we also tested the reverse approach: assuming the top k documents of the candidate based rankings are relevant and computing ap on the topic based ranking. in this setting  we directly compare absolute performance of a single topic ranking on different tasks. the tasks are to retrieve documents relating to candidate-1  relating to candidate1  etc. in a normal retrieval setting  comparing absolute numbers across tasks does not make sense. here we keep the system fixed  the topic run   and effectively try to find the easy topics for this system  what are the easy candidates for this topic  . this is less elegant than the reverse approach  but it may still help to locate the experts.
in the following we will refer to these ap based variants as ap-trel and ap-crel  for the runs with topic rankings assumed relevant and candidate rankings assumed relevant respectively.
1. experimental results
for our experiments  we use the open source pf/tijah system   a flexible blend of an xquery engine and an xml ir retrieval systems that supports many information retrieval models. to be able to use this system with the w1c data  we cleaned the collection to make sure all documents are valid xml. while the pf/tijah system supports retrieval at the element level  e.g.  retrieve paragraphs or sections   we only use full document queries. documents and queries are stemmed and stopped and language models are used to perform the ranking of documents. for the topic specific ranking pf/tijah was not able to perform the 1+ queries  1 topics times 1 candidates  in the available time. for this variant  we had to use the x1 based retrieval system as used by our group in the terabyte track.
while the official track results include measures based on retrieved experts and measures based on experts retrieved with supporting documents  we ignore the supporting documents and only report on the ability of the various approaches to identify the experts. the metric used is mean average precision  map  over the ranked list of experts. we use k = 1  that means the topical profiles consist of the top 1 documents retrieved for the topic. these 1 documents are the basis for computing the spearman correlation measures and they are regarded relevant in the ap-trel and ap-crel approaches. the influence of k on the results is studied below. table 1 shows the results.1.
one notable result is the low score across the board for the topic specific candidate rankings. intuitively  these rankings are expected to be better  more focused  and more likely to yield appropriate correlations. the low scores in practise can be explained by the fact that the specific candidate terms that were added to the topic query often have little influence on the ranking. one reason for this is that it is just a single extra term on a title query of on average 1 terms. more important though  is that some candidates appear only in very few documents  these few may end up on the top of the ranked list for title+query  but the rest of the list remains unchanged. the effect is that the title+query rankings are often very similar to the title only rankings in particular in the cases where the candidate term matches few or no documents. this means the correlation between the two rankings is higher for candidates that hardly appear in the collection. clearly  these are not necessarily the experts.
also  between the other two candidate rankings  there is a remarkable difference. the annotated candidate run clearly outperforms the name and email variant. this was expected as the annotated variant does not suffer of confusion between people with similar names or email addresses.
finally  the log-sp runs give lower scores than the other correlation variants. a reason could be that the top ranks are emphasised too much now. further analysis is needed to confirm this.
in comparison to the other expert search submissions  these numbers are relatively low. one reason for this is that we did not use any collection specific knowledge. many runs that performed well treated emails different than other documents and included special treatment or weighting for specific fields like an email's sender  receiver or subject. also  many successful approaches ranked and analysed information at the sub-document level  creating candidate profiles based on smaller windows around candidate occurrences rather than on complete documents.

figure 1: the influence of k on mean average precision  map  for different correlations  candidate annotation profile 
1 varying the size of the profiles
the number of documents in the topic and candidate profiles is likely to have an impact on the results. the top ranked documents are probably better indicators of expertise than the ones further down the ranked list. our comparison of rankings takes this into account to some extend  and we tried to emphasise this by using a log-transform on the ranks. still  it may be useful to limit the size of the profiles and look only at the top documents. figure 1 shows the influence of k on mean average precision values for the sp  ap-trel and ap-crel correlations.
first of all we notice that while the performance for the different correlation measures on the top 1  as reported in table 1  are the same  large differences exist for smaller values of k. the ap-crel line shows that taking fewer than 1 documents as a candidate's profile harms the map  note that for the two other correlation measures k relates to the size of the topic profile; the size of the candidate profile for these is fixed at 1 . the size of the topic profile can be significantly smaller; the optimum seems to be around 1 or 1 documents. it seems the experts need to have only the top ranked topical documents in their profile  but it could also be the case that for many topics fewer than 1 documents are relevant and that the profile for these includes random documents to reach a profile size of 1 documents. furhter investigation is needed to check this.
the emphasis on the top ranked documents in the profiles that we wanted to attain with the log-sp run  seems indeed important  but it is not as symmetric as the  failing  logsp run treated it: for the candidate profiles we need many documents  for the topical profiles we need to focus on the top ranks. finally  spearman  sp  based comparison of the profiles gives much higher scores than the average precision  ap  based runs. this indicates that it is important to not only look at the set of documents related to a candidate or a topic  but that the ordering of these documents plays an important role. a candidate is an expert on a topic  if the top ranked documents in the topical profile have a similar ordering in the candidates profile.
table 1: expert search results  map  for various base candidate rankings  rows  and various correlation measures  columns .
correlation typecandidate profilesplog-sp	ap-trelap-crelname + email11	11candidate annotation11	11topic specific11	111. conclusions
directly comparing topic profiles and candidate profiles based on the correlation between document rankings is a novel and interesting approach. it is different from traditional approaches that either construct a descriptive document for each candidate and rank these  or rank documents based on topics only and then extract candidates from the top ranked documents. the correlation based approach directly works on the ranked lists and uses information of the entire lists.
the experiments for the expert search task showed that the approach is feasible given that the initial rankings are good enough. the baseline  name and email address based candidate ranking is clearly not good enough  but a candidate ranking based on the tagged candidates gives good results when used in our correlation method1. the exact correlation method is of minor importance in such a setting  but log-sp is a particular bad choice.
topic specific candidate rankings harm results significantly. the main reason for this is the relatively low number of appearances in the collection for many candidates. more research is needed to investigate ways to compensate for this. a possible solution would be to compare the title + candidate based ranking not only to the topic based ranking  but also to the candidate based ranking  or to measure the overlap between all sub-queries cf. carmel et al .
in comparison with other expert search approaches the performance of the approach described here is relatively poor. one reason for this is we do not use any collection specific information  another is we treat documents as a single entity. the same techniques could easily be adapted to work on windows  xml elements or passages. also in creating the rankings for the topic and candidate profiles  collection specific creation could be incorporated. correlating rankings on top of the successful expert search approaches is an interesting direction for future research.
as a collection independent approach  the correlation based profile comparison gives good results  provided the parameters are chosen correctly. spearman based correlation is than better than the average precision based measures  indicating that it is important to take the ordering of documents in both the topical profile and the candidate profile into account.
