given an n ¡Án grid of squares  where each square has a count cij and an underlying population pij  our goal is to find the rectangular region with the highest density  and to calculate its significance by randomization. an arbitrary density function d  dependent on a region's total count c and total population p  can be used. for example  if each count represents the number of disease cases occurring in that square  we can use kulldorff's spatial scan statistic dk to find the most significant spatial disease cluster. a naive approach to finding the maximum density region requires o n1  time  and is generally computationally infeasible. we present a multiresolution algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure  bounds the maximum score of subregions contained in each region  and prunes regions which cannot contain the maximum density region. for sufficiently dense regions  this method finds the maximum density region in o  nlogn 1  time  in practice resulting in significant  1x  speedups on both real and simulated datasets.
categories and subject descriptors
h.1  database management : database apps- data mining
general terms
algorithms
keywords
cluster detection  spatial scan statistics  biosurveillance
1.	introduction
¡¡one of the core goals of data mining is to discover patterns and relationships in data. in many applications  however  it is important not only to discover patterns  but to distinguish those patterns that are significant from those that are likely to have occurred by chance. this is particularly important in epidemiological applications  where a rise in the number of disease cases in a region may or may not be indicative of an emerging epidemic. in order to
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific
permission and/or a fee.
kdd'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
decide whether further investigation is necessary  epidemiologists must know not only the location of a possible outbreak  but also some measure of the likelihood that an outbreak is occurring in that region. more generally  we are interested in spatial data mining problems where the goal is detection of overdensities: spatial regions with high scores according to some density measure. the density measure can be as simple as the count  e.g. number of disease cases  or units of cough medication sold  in a given area  or can adjust for quantities such as the underlying population. in addition to discovering these high-density regions  we must perform statistical testing in order to determine whether the regions are significant. as discussed above  a major application is in detecting clusters of disease cases  for purposes ranging from detection of bioterrorism  e.g. anthrax attacks  to identifying environmental risk factors for diseases such as childhood leukemia  1  1  1 .  discusses many other applications  including mining astronomical data  e.g. identifying star clusters   military reconnaissance  and medical imaging.
¡¡we consider the case in which data has been aggregated to a uniform  two-dimensional grid. let g be an n ¡Án grid of squares  where each square sij ¡Ê g is associated with a count cij and an underlying population pij. for example  a square's count may be the number of disease cases in that geographical location in a given time period  while its population may be the total number of people  at-risk  for the disease. our goal is to search over all rectangular regions s   g  and find the region s  with the highest density according to a density measure d: s  = argmaxs d s . we use the abbreviations mdr for the maximum density region s   and mrd for the maximum region density d s    throughout. we will also find the statistical significance  p-value  of this region by randomization testing  as described below.
¡¡the density d s  of a region s can be an arbitrary function of the total count of the region  c s = ¡Æs cij  and the total population of the region  p s =¡Æs pij. thus we will often write d c p   where c and p are the count and population of the region under consideration. it is important to note that  while the term  density  is typically understood to mean the ratio of count to population  we use the term in a much broader sense  to denote a class of density functions d which includes the  standard  density function d. for our purposes  we assume that the density function d satisfies the following three properties:
1. for a fixed population  density increases monotonically withcount:	1 for all  c p .
1. for a fixed count  density decreases monotonically with population: p 1 for all  c p .
1. for a fixed ratio cp  density increases monotonically with population:  d 1 for all  c p .
the first two properties state that an overdensity is present when a large count occurs in a small population. in the case of a uniform population distribution  the population of a region is proportional to its area  and thus an overdensity is present when a large count occurs in a small area. the third property states  in essence  that an overdensity is more significant when the underlying population is large. this is true because smaller populations will have higher variance in densities. however  we also allow d to remain constant as population increases for a fixed ratio cp  thus including the standard density function; we do not  however  allow functions where d decreases in this case. in our discussion below  we will also make one more assumption involving the second partials of d; this fourth property is not strictly necessary but makes our computation easier  eliminating the need to check for local maxima of the density function . a large class of functions satisfy all four properties  including kulldorff's spatial scan statistic  discussed in detail below.
1	related work
¡¡this work builds on our previous work on detection of spatial overdensities . the primary difference is in the statement of the problem: the goal of the present work is to detect the most significant rectangular region  as opposed to the most significant square region. this extension is extremely important in epidemiological applications because disease clusters are often elongated: airborne pathogens may be blown by wind  creating an ellipsoid  plume   and waterborne pathogens may be carried along the path of a river. in each of these cases  the resulting clusters have high aspect ratios  and a test for squares  or circles  as in kulldorff's original scan statistic  will have low power for detecting the overdensity. while our discussion below focuses on finding  axis-aligned  rectangular regions  it can be easily extended to find rectangular regions which are not aligned with the coordinate axes. one simple method of doing this is to examine multiple  rotations  of the data  mapping each to a separate grid and computing the maximum density region for each grid. in this case  we must also perform the same rotations on each replica grid  and thus the complexity of the algorithm is multiplied by the number of rotations. however  if we have information about relevant conditions such as wind direction or the flow of a river  we can use this information to align the coordinate axes  reducing or avoiding the need to examine multiple rotations.
¡¡while this change from square to rectangular regions has little effect on the underlying statistics  it creates a much more difficult computational problem. while the maximum density square region can be found naively in o n1  time for an n ¡Án grid  finding the maximum density rectangular region requires o n1   and thus is computationally infeasible for even moderately sized grids. our solution is similar to our previous work in that we propose a multiresolution partitioning algorithm: we divide the grid into overlapping regions  bound the maximum score of subregions contained in each region  and prune regions which cannot contain the maximum density region. within that general framework  however  there are major differences in our multiresolution data structure and the resulting algorithm. our current  center-based  approach  using a novel  overlap-kd tree  data structure  allows us to achieve tighter upper bounds on the score of a region  allowing much more pruning to take place. as a result  our algorithm gives huge speedups  as compared to the naive approach  without relying on approximation; the non-approximate version of our previous algorithm only resulted in speedups when the maximum density region was sufficiently dense  and actually performed slower than naive in some cases. our current algorithm is also more robust: while the previous algorithm was severely slowed by non-uniform populations  these are shown to have little or no effect on the current method.
¡¡various other methods for finding  dense clusters  have been proposed in the data mining literature  including grid-based hierarchical methods such as clique   mafia   and sting . our work differs from these in three main ways: most importantly  as discussed above  our goal is not only to find the highest scoring cluster  but to determine the statistical significance of that cluster  whether it is a true overdensity  or if it is likely to have occurred by chance . second  our method deals with non-uniform underlying populations: this is particularly essential for real-world epidemiological applications  in which an overdensity of disease cases is more significant if the underlying population is large  and is also important in many other applications where an  overdensity  is defined relative to some other statistic  e.g. population  baseline score  or covariate . finally  our method is applicable to a wide class of density measures d  where the other algorithms are specific to the  standard  density measure d1 s = cp  ss  . the d1 measure is the ratio of count per unit population; for example  in epidemiology  maximizing d1 corresponds to finding the region with the highest observed disease rate. however  this is not generally the region we are interested in finding  since a region with a high disease rate and very small population  e.g. one person  who happens to be sick  is not likely to be significant. in fact  the  region  with
                                                         cij the highest d1 will be the single square with highest pij . this is because d1 density is monotonic: if a region s with d1 s = d is partitioned into any set of disjoint subregions  at least one subregion s1 will have d1 s1  ¡Ý d. thus the other algorithms  rather than maximizing d1  search for maximally sized regions with d1 greater than some threshold. there are two disadvantages to this approach: one is the difficulty of measuring statistical significance within this framework. the other is that the algorithms rely heavily on the monotonicity of the d1 measure by first finding  dense  1 ¡Á 1 squares  then merging adjacent squares in bottom-up fashion. for a non-monotonic density measure such as kulldorff's  it is possible to have a large dense region where none of its subregions are themselves dense  so bottom-up methods are not guaranteed to find the correct region. here  we will optimize with respect to arbitrary non-monotonic density measures  and thus require a different approach from clique  mafia  or sting.
1	the spatial scan statistic
¡¡a non-monotonic density measure which is of great interest to epidemiologists is kulldorff's spatial scan statistic   which we denote by dk. this statistic is in common use for finding significant spatial clusters of disease cases  which are often indicative of an emerging outbreak. kulldorff's statistic assumes that counts cij are generated by an inhomogeneous poisson process with mean qpij  where q is the underlying  disease rate   or expected value of
cp . we then calculate the log of the likelihood ratio of two possibilities: that the disease rate q is higher in the region than outside the region  and that the disease rate is identical inside and outside the region. for a region with count c and population p  in a grid with total count ctot and population ptot  we can calculate:
	c	 	ctot   c  	ctot
	dk =clog+ ctot	c logctot log
	p	ptot	p	ptot
if  cptottot   and dk = 1 otherwise.  proved that the spatial scan statistic is individually most powerful for finding a single significant region of elevated disease rate: for a fixed false positive rate  and for a given set of regions tested  it is more likely to detect the overdensity than any other test statistic. again  we note that our algorithm is general enough to use any density measure  and in some cases we may wish to use measures other than kulldorff's. for instance  if we have some idea of the size of the maximum density region  we can use the dr measure  dr 1  with larger r corresponding to tests for smaller clusters. we have also derived a variant of the dr measure for normally distributed counts  where the cumulative statistics of a region are not its raw count and population  but a weighted average of squares' z-scores. we are in the process of using this statistic to look for emerging epidemics based on national sales of over-the-counter medications.
1	randomization testing
¡¡once we have found the maximum density region  mdr  of grid g according to our density measure  we must still determine the statistical significance of this region. since the exact distribution of the test statistic is only known in special cases  such as d1 density with a uniform underlying population   in general we must find the region's p-value by randomization. to do so  we run a large number r of random replications  where a replica has the same underlying populations pij as g  but assumes a uniform disease rate qrep = cptottot  gg   for all squares. for each replica g1  we first generate all counts cij randomly from an inhomogeneous poisson distribution with mean qreppij  then compute the maximum region density  mrd  of g1 and compare this to mrd g . the number of replicas g1 with mrd g1  ¡Ý mrd g   divided by the total number of replications r  gives us the p-value for our maximum density region. if this p-value is less than .1  we can conclude that the discovered region is statistically significant  unlikely to have occurred by chance  and is thus a  spatial overdensity.  if the test fails  we have still discovered the maximum density region of g  but there is not sufficient evidence that this is an overdensity.
1	the naive approach
¡¡the simplest method of finding the maximum density region is to compute the density of all rectangular regions of sizes k1 ¡Ák1  where kmin ¡Ü k1 k1 ¡Ü kmax.1 since there are a total of  n  k1+ 1  n   k1+ 1  regions of each size k1 ¡Á k1  there are a total of o n1  regions to examine. we can compute the density of any region s in o 1   by first finding the count c s  and population p s   then applying our density measure d c p .1 this allows us to compute the mdr of an n ¡Án grid g in o n1  time. however  significance testing by randomization also requires us to find the mrd for each replica g1  and compare this to mrd g . since calculation of the mrd takes o n1  time for each replica  the total complexity is o rn1   and r is typically large  we assume r = 1 . as discussed in   several tricks may be used to speed up this procedure for cases where there is no significant spatial overdensity  but these do not help in cases when an overdensity is found. in general  the o n1  complexity of the naive approach makes it infeasible for even moderately sized grids: we estimate a runtime of 1 days for a 1¡Á1 grid on our test system  which is clearly far too slow for real-time detection of disease outbreaks.
¡¡while one alternative would be to search for an approximate solution using one of the variety of cluster detection algorithms in the literature  we present an algorithm which is exact  always finds the maximum density region  and yet is much faster than naive search. the key intuition is that  since we only care about finding the maximum density region  we do not need to search over every single rectangular region: in particular  we do not need to search a set of regions if we can prove  based on other regions we have searched  that none of them can be the mdr. as a simple example  if a given region has a very low count  we may be able to conclude that no subregion contained in that region can have a score higher than the mrd  and thus we do not need to actually compute the score of each subregion. these observations suggest a top-down  branchand-bound approach: we maintain the current maximum score of the regions we have searched so far  calculate upper bounds on the scores of subregions contained in a given region  and prune regions which cannot contain the mdr. similarly  when we are searching a replica grid  we only care about whether the mrd of the replica is higher than the mrd of the original grid. thus we can use the mrd of the original grid for pruning on the replicas  and can stop searching a replica if we find a region with score higher than this mrd.
1.	overlap-multires partitioning
¡¡our top-down approach to cluster detection can be thought of as a multiresolution search of the space under consideration: we search first at coarse resolutions  large regions   then at successively finer resolutions  smaller regions  as necessary. this suggests that a hierarchical  space-partitioning data structure such as kd-trees   mrkd-trees   or quadtrees  may be useful in speeding up our search. however  our desire for an exact solution makes it difficult to apply these data structures to our problem. in a kd-tree  each spatial region is recursively partitioned into two disjoint  child  regions  each of which can then be further subdivided. the difficulty  however  is that many subregions of the parent are not contained entirely in either child  but overlap partially with each. thus  in addition to recursively searching each child for the mdr  we must also search over all of these  shared  regions at each level of the tree.1 since there are o n1  shared regions even at the top level of the tree  i.e. regions partially overlapping both halves of grid g   an exhaustive search over all such regions is too computationally expensive  and thus a different partitioning approach is necessary.
¡¡an initial step toward our partitioning can be seen by considering two divisions of a rectangular spatial region s: first  into its left and right halves  which we denote by s1 and s1   and second  into its top and bottom halves  which we denote by s1 and s1 . assuming that s has size k1 ¡Ák1  this means that s1 and s1 have size
  and s1 and s1 have size k. considering these four  overlapping  halves  we can show that any subregion of s either a  is contained entirely in  at least  one of s1...s1  or b  contains the centroid of s. thus one possibility would be to search s by exhaustively searching all regions containing its centroid  then recursing the search on its four  children  s1...s1. again  there are o n1   shared  regions at the top level of the tree  i.e. regions containing the centroid of grid g   so an exhaustive search is infeasible.
¡¡our solution  as in our previous work   is a partitioning approach in which adjacent regions partially overlap  a technique we call  overlap-multiresolution partitioning   or  overlap-multires  for short. again we consider the division of s into its left  right  top  and bottom  children.  however  while in the discussion above each child contained exactly half the area of s  now we let each child contain more than half the area. we again assume that region s has size k1 ¡Á k1  and we choose fractions f. then s1 and s1 have size f1¡Ák1  and s1 and s1 have size k1¡Á f1. this

figure 1: overlap-multires partitioning of region s
partitioning  for f1 = f1 =  is illustrated in figure 1. note that there is a region sc commonto all four children; we call this region the center of s. the size of sc is   1  1 k1 ¡Á 1  1 k1   and thus the center has non-zero area. when we partition region s in this manner  it can be proved that any subregion of s either a  is contained entirely in  at least  one of s1...s1  or b  contains the center region sc. figure 1 illustrates each of these possibilities.
¡¡now we can search s by recursively searching s1...s1  then searching all of the regions contained in s which contain the center sc. unfortunately  at the top level there are still o n1  regions contained in grid g which contain its center gc. however  since we know that each such region contains the large region gc  we can place very tight bounds on the score of these regions  often allowing us to prune most or all of them.  we discuss how these bounds are calculated in the following subsection.  thus the basic outline of our search procedure  ignoring pruning  for the moment  is:
overlap-search s  { call base-case-search s  define child regions s 1..s 1  center s c as above call overlap-search s i  for i=1..1 for all s' such that s' is contained in s and contains s c 
call base-case-search s' 
}
¡¡now we consider how to select the fractions f1 and f1 for each call of overlap-search  and characterize the resulting set ¦µ of regions s on which overlap-search s  is called. regions s ¡Ê ¦µ are called gridded regions  and regions s ¡Ê/ ¦µ are called outer regions. for simplicity  we assume that the grid g is square  and that its size n is a power of two. we begin the search by calling overlapsearch g . then for each recursive call to overlap-search s   where the size of s is k1 ¡Ák1  we set f1 =if k1 = 1r for some integer r  and fr for some integer r. we define f1 identically in terms of k1  and then the child regions s1...s1 and the center region sc are defined in terms of f1 and f1 as above. this choice of f1 and f1 has the useful property that all gridded regions have sizes 1r or 1¡Á1r for some integer r. for instance  if the original grid g has size 1¡Á1  then the children of g will be of sizes 1¡Á1 and 1¡Á1  and the grandchildren of g will be of sizes 1¡Á1  1¡Á1  and 1¡Á1. this process can be repeated recur-

figure 1: the first two levels of the overlap-kd tree. each node represents a gridded region  denoted by a thick rectangle  of the entire dataset  thin square and dots .
sively down to regions of size kmin ¡Ákmin  forming a structure that we call an overlap-kd tree. the first two levels of the overlap-kd tree are shown in figure 1. note that even though grid g has four child regions  and each of its child regions has four children  g has only ten  not 1  distinct grandchildren  several of which are the child of multiple regions.
¡¡our overlap-kd tree has several nice properties  which we present here without proof. first  for every rectangular region s   g  either s is a gridded region  contained in the overlap-kd tree   or there exists a unique gridded region s1 such that s is an outer region of s1  i.e. s is contained in s1  and contains the center region of s1 . this means that  if overlap-search is called exactly once for each gridded region  and no pruning is done  then base-case-search will be called exactly once for every rectangular region s   g. in practice  we will prune many regions  so base-case-search will be called at most once for every rectangular region  and every region will be either searched or pruned. the second nice property of our overlap-kd tree is that the total number of gridded regions |¦µ| is o  nlogn 1  rather than o n1 . this implies that  if we are able to prune  almost  all outer regions  we can find the mdr of an n¡Án grid in o  nlogn 1  time. in fact  we may not even need to search all gridded regions  so in many cases the search will be even faster. before we consider how to calculate score bounds and use them for pruning  we must first deal with an essential issue in searching overlap-kd trees. since a child region may have multiple parents  how do we ensure that each gridded region is examined only once  rather than being called recursively by each parent  one simple answer is to keep a hash table of the regions we have examined  and only call overlap-search s  if region s has not already been examined. the disadvantage of this approach is that it requires space proportional to the number of gridded regions  o  nlogn 1   and spends a substantial amount of time doing hash queries and updates. a more elegant solution is what we call lazy expansion: rather than calling overlap-search si  on all four children of a region s  we selectively expand only certain children at each stage  in such a way that there is exactly one path from the root of the overlap-kd tree to any node of the tree. one such scheme is shown in figure 1: if the path between a parent and child is marked with an x  lazy expansion does not make that recursive call. no extra space is needed by this method; instead  a simple set of rules is used to decide which children of a node to expand. a child is expanded if it has no other parents  or if the parent node has the highest priority of all the child's parents. we give parents with lower aspect ratios priority over parents with higher aspect ratios: for example  a 1¡Á1 parent would have priority over a 1¡Á1 parent if the two share a 1¡Á1 child. this rule allows us to perform variants of the search

p in
figure 1: maximizing count cin for a given population pin. count must be less than c1 pin   c1 pin   and c1 pin .
where regions with very high aspect ratios are not included; an extreme case would be to only search for squares  as in our previous work. within an aspect ratio  we fix an arbitrary priority ordering. since we maintain the property that every node is accessible from the root  the correctness of our algorithm is maintained: every gridded region will be examined  if no pruning is done   and thus every region s   g will be either searched or pruned.
1	score bounds
¡¡we now consider which regions can be pruned  discarded without searching  during our multiresolution search procedure. first  given some region s  we must calculate an upper bound on the scores d s1  for regions s1   s. more precisely  we are interested in two upper bounds: a bound on the score of all subregions s1   s  and a bound on the score of the outer subregions of s  those regions contained in s and containing its center sc . if the first bound is less than or equal to the mrd  we can prune region s completely; we do not need to search any  gridded or outer  subregion of s. if
only the second bound is less than or equal to the mrd  we do not need to search the outer subregions of s  but we must recursively call overlap-search on the gridded children of s. if both bounds are greater than the mrd  we must both recursively call overlap-search and search the outer regions.
¡¡now we will explain the calculation of the second bound  on subregions containing the center ; the calculation of the first bound  on all subregions  can be treated as a special case where the population  count  and area of the center are zero. we begin by assuming as known various pieces of information about the subregions of s; we discuss below how these are obtained. this information includes: upper and lower bounds pmax  pmin on the population of subregions s1; an upper bound dmax on the d1 density of s1; an upper bound dinc on the d1 density of s1  sc; and a lower bound dmin on the d1 density of s s1. we also know the count c and population p of region s  and the count ccenter and population pcenter of region sc. let cin and pin be the count and population of s1; these are presently unknown. to find an upper bound on d s1   we must calculate the values of cin and pin which maximize d cin  pin   subject to the given constraints:
1. pcinin  cpcentercenter ¡Ü dinc
1. pin	dmax
1. p pin	dmin
1. pmin ¡Ü pin ¡Ü pmax
this potentially difficult maximization problem could be solved by convex programming  but is made much easier by the properties of
population = p p 1
population = p 1   p center
population = p center
¡¡d 1 density = c center / p centerd 1 density = d incs' population = p 1 p 1
d 1 density = d maxd 1 density = d minfigure 1: division of region s into layers of differing density. in the typical case  subregion s1 includes all but the outer layer.
the density function d. since 1  we know that the maximum value of d for a given pin occurs when cin is maximized subject to the constraints. we solve the first three constraints for cin  giving us cin = min c1 c1 c1   where:
c1 = dincpin   dincpcenter  ccenter = dincpin  b1
c1 = dmaxpin
c1 = dminpin + c dminp = dminpin +b1
in the typical case 1 we have dmin ¡Ü dmax ¡Ü dinc  b1   1  and b1   1: this means that cin =c1 for small pin  cin =c1 for moderate pin  and cin = c1 for large pin  as illustrated in figure 1. thus we can solve for the intersection points p1  p1  and p1  where ci ¡Ü cj for pin ¡Ü pij  and we use these quantities to find the maximum allowable count cin for a given pin. solving the equations  we find
 
that p	1 we have 1   p1 ¡Üminp1  and¡Ü p1 =¡Þ.dmaxinb this1dmin case . in thewe
typical case  use the values of p1 and p1  and the value p1 is not needed. then the count cin = ccenter +dinc pin   pcenter  for pcenter ¡Ü pin ¡Ü p1  cin = dmaxpin for p1 ¡Ü pin ¡Ü p1  and cin = dmaxp1+dmin pin   p1  for pin ¡Ý p1. this is illustrated by figure 1: the region s is separated into four  layers  of differing densities. starting from the inside  we have the center  with a known population pcenter and count ccenter   a layer of high d1 density dinc  a layer of moderate d1 density dmax  and a layer of low d1 density dmin.
¡¡now we can write cin as a function of pin  and thus the score d becomes a function of the single variable pin. where does the maximum of this function occur  again we rely on properties of the function d c p   and a case-by-case analysis is necessary. in the typical case dinc   dmax   cpcentercenter   we know that the score increases with population in the  high density  and  moderate density  layers. this follows from two properties of our density function: 
1 and  in the high density layer  the d1 density
of  increases from cpcentercenter to dmax as we add more population  so the score d is monotonically increasing with population. in the moderate density layer  the d1 density of s1 stays constant  at dmax  as population increases  so again d is monotonically increasing. in the low density layer  d1 density of s1 decreases as population increases: in this case  since count and population are both

figure 1: quartering of region s
increasing  the score may increase or decrease. we assume that the score function d has no local maxima in the interval  p1 p   and thus that the maximum occurs either at  cin  pin = dmaxp1 p1  or at  cin  pin  =  c p .1 we are only interested in finding subregions with scores higher than the parent  so we can ignore the latter case. thus our upper bound on d s1  is d cin  pin   where pin = p1 and cin = dmaxpin. the various special cases  where one or more of the inequalities above are violated  are handled similarly using the intersection points p1  p1  and p1 as necessary. we also must adjust our value of pin if it violates the inequality pmin ¡Ü pin ¡Ü pmax  adjusting cin accordingly given the density of
the layers being added or subtracted.
¡¡we now consider how the bounds on populations and d1 densities are obtained. the simplest method of doing so is to use global values: first  we precompute the minimum and maximum population and d1 density of all single squares sij in the grid. this gives us usable  though very conservative  values for dmin  dmax  and dinc. we can also use the minimum and maximum square populations  together with the minimum and maximum area of a region  to obtain bounds pmin and pmax. slightly less conservative bounds can be obtained using our assumption of a minimum region size kmin = 1. any k1 ¡Ák1 region  where k1 k1 ¡Ý 1  can be tiled with rectangles of sizes 1 ¡Ü k1 k1 ¡Ü 1. thus we can precompute the minimum and maximum d1 density and population per square of all such rectangles in o n1  time  and use these rather than the single square bounds when allowable. for example  when bounding maximum score of the outer regions of s  we can use the less conservative bound for dmax; when bounding maximum score of all subregions of s  we can also use the less conservative bound for dinc.
¡¡these global bounds on populations and d1 densities are inexpensive to compute  we need only compute them once per grid   but are very conservative estimates of the densities of squares in a given region. we use these bounds in our algorithm as a sort of  first pass  which prunes many regions but also leaves many unpruned. if a region survives this round of pruning  we compute much tighter bounds on subregion scores in a  second pass   which is also more computationally expensive. to do so  we obtain tighter bounds on the population and d1 density of s1 using a novel technique we term quartering  then use these constraints to bound d s1  as above.
¡¡given a region s of size k1 ¡Ák1  with a  non-zero  center region sc  the quartering procedure calculates bounds on subregion population and d1 density in o k1  time. the first step of quartering is to divide s into its four  non-overlapping  quadrants s1...s1  as in figure 1. we now consider each si separately  together with the quarter of the center  sci  which overlaps that quadrant. for each quadrant  we consider all rectangles si1 with one corner at the centroid of s  and one corner outside sci  i.e. on one of the dots in figure 1 . note that there are o k1  such rectangles  and thus we can search over all of these regions si1 in quadratic time. our search procedure is very simple: given a region si1  let pin  cin  and ain denote its population  count  and area; let pout  cout  and aout denote the population  count  and area of si ; and let pdif   cdif   and adif denote the population  count  and area of ssci. we then calculate the d1 density d and the average population per square ps for each of si1  si  si1  and ssci: c apinin   and the other quantities  dout  ddif   ps out  ps dif   are defined similarly. we then set dmax equal to the maximum of all din  dinc equal to the maximum of all ddif   and dmin equal to the minimum of all dout. similarly  we take the minimum and maximum values of ps in  ps out  and ps dif ; we can use these to calculate bounds pmin and pmax once we are given the minimum and maximum area of s1. in essence  what we have done is bounded the d1 densities and populations for the piece of region s1 contained in each quadrant. then since d1 density is monotonic  we know that the d1 density of the entire region s1 is bounded by the maximum of the maxdensities and the minimum of the min-densities computed for all regions si1. population per square is also monotonic  so an identical argument applies. another way to think of this is that we are calculating bounds on population and d1 density for all the irregular  but rectangle-like  regions containing the center cs and consisting of one rectangle in each quadrant  as drawn in figure 1; then these quantities are also bounds on the population and density of all rectangles which contain cs.
¡¡we do not provide a formal proof here  but we note that the bounds on population and density derived by quartering are exact  i.e. no rectangle s1   s  such that sc   s1  can have density or population outside these bounds  and that they are much tighter than the global population and density bounds  allowing many more regions to be pruned. however  as noted above  quartering is significantly more computationally expensive than using the global bounds  taking time quadratic in the size of region s  and thus o n1  for large regions. this is why we first use the global bounds for pruning outer regions  and only use quartering on regions that this initial pruning does not eliminate. we also note that quartering can be done in linear time in the special case where the parent region s and its center sc have the same row size k1 or the same column size k1; in this case we need only divide the region into two halves  and each half can be searched linearly and the bounds combined. we apply this linear-time  halving   as well as the standard quadratic-time quartering  in our algorithm presented below.
1	the algorithm
¡¡we now possess all of the algorithmic and statistical tools needed to present our algorithm in full. the basic structure is similar to the top-down  overlap-search  routine presented above  with several important differences. first  we use a best-first search  implemented using a pair of priority queues q1 and q1  rather than a recursive depth-first search. our algorithm has two stages: in the first stage we examine only gridded regions  and in the second stage we search outer regions if necessary. in both stages  we prune regions whenever possible  calculating increasingly tight bounds on subregions' population and d1 density  and using these to calculate upper bounds dmax on d s1  as above. the first stage of our algorithm proceeds as follows  using the  loose  global bounds on population and d1 density to calculate dmax:
add g to q 1.
while q 1 not empty:
get region s with highest d s  from q 1.
if d s    mrd  set mdr = s and mrd = d s .
if d max s' in s    mrd  add gridded children of s to q 1  using lazy expansion . if d max s' in s containing s c    mrd  add s to q 1.
¡¡thus  after the first stage of our algorithm  we have searched or pruned all gridded regions  requiring at most o  nlogn 1  time   and the current mdr is the gridded region with highest d s . q1 now contains the subset of gridded regions whose outer regions have not yet been pruned  prioritized by their upper bounds dmax. the second stage of our algorithm proceeds as follows:
while q 1 not empty  and some s on q 1 has d max s    mrd :
get region s with highest d max s  from q 1.
use quartering to calculate tighter pop and density bounds for s.
recalculate d max s  using these bounds.
if d max s    mrd  then search-outer-regions s .
¡¡now the only question left is how to perform the search-outerregions procedure. we first note that a rectangular region requires four coordinates for specification: we use the row size k1  the column size k1  the minimum row x1  and the minimum column x1. then a naive search of the outer regions of s could be done using four nested loops  stepping over each legal combination of these four coordinates  i.e. such that the resulting region s1 is in s and contains sc . we also use four nested loops  in the order k1  x1  k1  x1   but take several more opportunities for pruning. once we have fixed k1 s1  and x1 s1   we can obtain a very tight bound on dmax s1  by expanding the center region sc and contracting the parent region s such that k1 s  = k1 sc  = k1 s1  and x1 s  = x1 sc  = x1 s1 . we then recalculate bounds on the d1 density and population using the new s and sc  this can be done in linear time using  halving   the special case of quartering  since the parent and center have the same k1   and finally recompute dmax for the new parent and center. only if dmax is greater than the mrd do we need to loop over k1 and x1 for that combination of k1 and x1. finally  once we have fixed k1  we can recompute dmax again  since we now precisely know the area of s1  giving us much tighter population bounds. only if dmax is greater than the mrd must we search all x1 for that combination of k1  x1  and k1.
¡¡thus the second stage of our algorithm can be seen as a series of  screens  that an outer region must pass through if it is to be searched. the first screen is whether the parent region is taken off q1 and examined  the second screen is whether the parent region passes the  quartering  test  the third screen is whether the new parent region  formed after k1 and x1 are fixed  passes the  halving  test  and the fourth screen is whether the new parent region passes the  halving  test once the area of s1 is fixed. we now examine the complexity of this procedure  given a large parent region  i.e. one containing o n1  outer regions s1 . if the parent region does not pass the first screen  we have spent only o 1  to search these o n1  regions; if the parent does not pass the second screen  we have spent only the o n1  time required by quartering. if the parent passes the second screen  but none of the new parent regions pass through the third and fourth screens  we have spent only o n1 ¡Á o n   for halving  given each k1 and x1  + o n1   for bounding  given each k1  x1  and k1  = o n1  time. thus only if all four screens fail will the algorithm have o n1  complexity; typically well over 1% of regions are eliminated at each screen  and thus we search only a small fraction of possible regions.
1.	a useful approximation
¡¡as opposed to our previous results   the algorithm presented above gives large speedups as compared to the naive approach without approximation: the algorithm is guaranteed to find the maximum density rectangular region. in some cases  however  very rapid detection may be more important than guaranteed accuracy; thus we present an approximate version of the algorithm which finds the mdr 1x faster while maintaining over 1% accuracy.
¡¡as noted above  the  first pass  of our algorithm uses very conservative bounds on the d1 densities of s1  s1  sc  and s s1  derived from the global minimum and maximum density values. thus one way to increase the speed of the algorithm is to use a closer approximation of these densities as a bound. the disadvantage is that if we use an estimate which is not guaranteed to bound the densities  we may underestimate the score of a region  and hence possibly prune away the mdr and find an incorrect region. here we consider an approximate lower bound on the d1 density of s s1: using this bound instead of a guaranteed but much more conservative bound typically results in large speedups with minimal loss of accuracy.
¡¡to derive tighter bounds on the maximum density of a subregion s1 contained in a given region s  we consider the assumptions being made by our statistical test. kulldorff's statistic assumes  both under the null hypothesis and the alternative hypothesis  that at most one disease cluster sdc exists  and that the disease rate q is expected to be uniform outside sdc  or uniform everywhere  if no disease cluster exists . thus  if sdc is contained entirely in the region under consideration s  we would expect that the maximum density subregion s1 of s is sdc  and that the disease rate of s s1 is equal to the disease rate outside s: e  = cptottot  cp = dout. assuming that the d1 density of s s1 is equal to its expected value dout  we can use the derivation above  using dout in place of dmin  to find the maximum subregion score.
¡¡the problem with this approach is that we have not compensated for the variance in densities. our calculated value of dout is only a lower bound for the d1 density of s   s1 in the most approximate probabilistic sense  in that we expect d1 s s1    dout half the time. we can improve the accuracy of our probabilistic bound by also considering the variance of cp  pcinina inhomogeneous. assumingpoissonthat all counts outside sdc are generated by distribution with parameter qpij  we obtain:
	in	  po q ptot p  i =	q + q	=	q  tot  in 	.
since the actual value of the parameter q is not known  we use a conservative empirical estimate: q = ptotc totpin . from this  we obtain
. then we can compute the
	 	tot
maximum subregion density by using dout  b¦Ò  for some constant b  in place of dmin in the derivation above. one minor complication is that  since ¦Ò is dependent on pin  we must solve equations for p1 and p1 which are quadratic rather than linear; we omit the details of this calculation.
¡¡by adjusting our approximation of dmin in this manner  we compute a higher score d  reducing the likelihood that we will underestimate the maximum subregion density and prune a region that should not necessarily be pruned.1 given a constant b  the d1 density of s s1 will be greater than dout  b¦Ò with probability pr z   b   where z is chosen randomly from the unit normal. for b = 1  there is an 1% chance that we will correctly bound d1 s s1   giving a guaranteed correct upper bound for the maximum subregion score. in practice  the maximum score will be lower than our approximate bound more often than this  since our estimates for the other parameters are conservative. thus  though our algorithm is approximate  it is very likely to converge to the globally optimal mdr.
¡¡one interesting feature of this approximation is that we expect to underestimate the maximum subregion score if the disease cluster sdc is not contained entirely in s  since we are calculating dout based on a region which includes this region of higher density. in cases where there is only a single disease cluster  this is acceptable  and desirable  since a region not containing sdc does not need to be expanded. in applications where multiple disease clusters are present  however  there is a risk that the presence of one significant disease cluster will cause the approximate algorithm to miss another more significant cluster. this phenomenon is visible in our results below: in two of the three real-world datasets  the approximate algorithms did not find the maximum density region  though they did find another cluster that was also significant. in the cases where only one disease cluster was present  as in our simulated trials  the approximate algorithms achieved high accuracy. we present results for the exact and approximate versions of the algorithm below.
1.	results
¡¡we first describe results with artificially generated grids and then real-world case data. an artificial grid is generated from a set of parameters  n  k1  k1  ¦Ì  ¦Ò  q1  q1 . the grid generator first creates an n ¡Án grid  and randomly selects a k1 ¡Ák1  test region.  then the population of each square is chosen randomly from a normal distribution with mean ¦Ì and standard deviation ¦Ò  populations less than zero are set to zero . finally  the count of each square is chosen randomly from a poisson distribution with parameter qpij  where q = q1 inside the test region and q = q1 outside the test region.
¡¡for all our simulated tests  we used grid size n = 1  and a background disease rate of q1 = .1. we tested for four different combinations of test region parameters  k1 ¡Ák1  q1 :  1¡Á1  .1    1¡Á1  .1    1¡Á1  .1   and  1¡Á1  .1 . these represent the cases of an extremely dense disease cluster  large and small disease clusters which are significant but not extremely dense  and no disease cluster respectively. we used three different population distributions for testing: the  standard  distribution  ¦Ì = 1  ¦Ò= 1   and two types of  highly varying  populations. for the  city  distribution  we randomly selected a 1¡Á1  city region : square populations were generated with ¦Ì = 1¡Á1 and ¦Ò= 1¡Á1 inside the city  and ¦Ì = 1 and ¦Ò= 1 outside the city. for the  high-¦Ò  distribution  we generated all square populations with ¦Ì = 1 and ¦Ò= 1¡Á1. for each combination of test region parameters and population distribution  run times were averaged over 1 random trials  and an additional 1 trials  for a total of 1  were used to test accuracy. a trial was counted  correct  if the algorithm either found the test region sdc  or another region s with d s  ¡Ý d sdc ; we emphasize that this is always the case for the exact version of our algorithm  which always finds the maximum density region.1we also recorded the average number of regions examined; for our algorithm  this includes calculation of score bounds as well as scores of individual regions. separate results are presented for the original grid and for each replica; for a large number of random replications  r = 1  the results per replica dominate  since total

figure 1: emergency department dataset. the left picture shows the  population  distribution and the right picture shows the  counts.  the winning region is shown as a rectangle.
run time is torig +r trep  to search the original grid and perform randomization testing. see table 1 for results.
¡¡our first observation was that the run time and number of regions searched were not significantly affected by the underlying population distribution; typically the three results differed by only 1%  and in many cases test regions were found faster for the highly varying distributions than the standard distribution. thus table 1  rather than presenting separate results for each population distribution  presents the average performance over all three population distributions for each test. this result demonstrates the robustness of the algorithm to highly non-uniform populations; this is very different than our previous work   where the algorithm was severely slowed by highly varying populations. the exact algorithm achieved average speedups ranging from 1x  for no test region   to 1x  for an extremely dense test region  as compared to the naive approach. we note that  for the case of no test region  it is typically not necessary to run more than 1 randomizations before vconcluding that the discovered region is not significant; thus our true average  worst-case  results will be closer to the 1x speedup on small  significant  but not extremely dense  test regions. since the naive approach requires approximately 1 days for a 1 ¡Á 1 grid with r = 1  this suggests that our exact algorithm can complete the same task in less than 1 hours. we also tested two approximate variants of the algorithm   approx-1  and  approx-1   with adjustments for density variance b = 1 and b = 1 respectively. these variants achieved up to 1x speedups  with over 1x speedups whenever a test region was present  and 1x speedups in the  no test region  case  enabling us to find the maximum density region and its significance in less than 1 hour. while all variants of the algorithm achieved 1% accuracy on the very dense test regions  the approximate versions missed some of the less dense test regions. for the larger  1 ¡Á 1  test regions  approx-1 and approx-1 achieved 1% and 1% accuracy respectively  averaged over the 1 trials ; for smaller  1 ¡Á 1  test regions  these accuracies were reduced to 1% and 1% respectively. in some cases  the guaranteed accuracy of the exact algorithm may be more necessary than the additional speedups gained by the approximate algorithm; in other cases  extremely fast results are needed  and an approximation may be sufficient.
¡¡we now discuss the performance of the algorithm on various real-world datasets. our first test set was a database of anonymized emergency department data collected from western pennsylvania hospitals in the period 1. this dataset contained a total of 1 records  each representing a single ed visit and giving the latitude and longitude of the patient's home location to the nearest .1 degreesmile  a sufficiently low resolution to ensure anonymity . these locations were mapped to three grid sizes:
¡¡¡¡¡¡¡¡¡¡¡¡¡¡table 1: performance of algorithm  simulated datasets  n = 1 test	method	sec/orig	speedup	sec/rep	speedup	regions  orig 	regions  rep 	accuracy
allnaive1x1x1.1b1b1%1  1exact1x1.1x1k1k1%approx-1.1x1.1x1.1k1%approx-1.1x1.1x1.1k1%1  1exact1x1.1x1.1m1k1%approx-1.1x1.1x1.1k1k1%approx-1.1x1.1x1k1k1%1  1exact1x1.1x1.1m1m1%approx-1.1x1.1x1k1k1%approx-1.1x1.1x1k1k1%no regionexact1x1.1x1.1m1m-approx-1.1x1.1x1.1m1m-approx-1.1x1.1x1.1m1m-n = 1  1  and 1. for each grid  we tested for spatial clustering of  recent  disease cases: the  count  of a square was the number of ed visits in that square in the last two months  and the  population  of a square was the total number of ed visits in that square. see figure 1 for a picture of this dataset  including the highest scoring region. for each of these grids  the exact and approximate versions of our algorithm found the same  statistically significant region  p-value 1  as the naive approach. the major difference  of course  was in runtime and number of regions searched  see table 1 . our algorithms found the mdr of the original grids 1x faster than the naive approach; however  much faster performance was achieved when searching the replica grids. the exact algorithm achieved speedups increasing from 1x to 1x as grid size increased from 1 to 1; the approximate versions did even better  achieving 1x speedups.
¡¡our second test set was a nationwide database of retail sales of over-the-counter cough and cold medication. sales figures were reported by zip code; the data covered 1 zip codes across the u.s.  with highest coverage in the northeast. in this case  our goal was to see if the spatial distribution of sales on a given day  1/1  was significantly different than the spatial distribution of sales a week before  1/1   and to identify a significant cluster of increased sales if one exists. thus we used the sales on 1 as our underlying population distribution  and the sales on 1 as our count distribution. slight modifications to kulldorff's statistic were necessary to deal with regions with zero population and nonzero count  i.e. sales on 1 but not 1 . we created four grids from this data  two using all of the national data  and two using only data from the northeast  where a greater proportion of zip codes report sales data . for both  national  and  regional  over-the-counter data  we created grids of sizes n = 1 and n = 1  converting each zip code's centroid to a latitude and longitude. for each of these four grids  our exact algorithm found the same statistically significant region  p-value 1  as the naive approach  and achieved speedups of 1x on the 1¡Á1 grids and 1x on the 1 ¡Á 1 grids. the approximate versions of the algorithm did not find the correct region on these four grids  and thus we do not include these in table 1. we note  however  that they did find another statistically significant region  though with a lower score than the mdr; it is possible that the presence of this region caused the algorithms to miss the most significant region  as discussed above.
¡¡thus the exact version of our algorithm found the maximum density region in all of our simulated and real-world trials  while achieving speedups of at least 1x  and typically much larger  as compared to the naive spatial scan. the approximate versions of the algorithm achieved much larger speedups  though at the cost of occasionally failing to find the correct region. this speedup is extremely important for the real-time detection of disease outbreaks: if a system is able to detect an outbreak in minutes rather than days  preventive measures or treatments can be administered earlier  possibly saving many lives. we believe that our algorithm will be useful for rapid detection of significant spatial clusters in a variety of other applications as well.
1.	conclusions and future work
¡¡thus we have presented a fast multiresolution partitioning algorithm for detection of significant spatial overdensities  and demonstrated that this method results in significant  1x  speedups on real and artificially generated datasets. we are currently applying this algorithm to national-level hospital and pharmacy data  attempting to detect disease outbreaks based on statistically significant changes in the spatial clustering of disease cases. our eventual goal is the automatic real-time detection of outbreaks  and application of a fast partitioning method using the techniques presented here may allow us to achieve this difficult goal.
¡¡additionally  we are extending the algorithm  and the underlying overlap-kd tree data structure  in various ways  making it usable for a broader range of application domains. most importantly  overlapkd trees can be extended to higher dimensions  as can the other techniques  e.g. quartering  used in our multiresolution search. we note  however  that various quantities  for example  number of children of a node  grow exponentially with dimension  so overlap-kd trees are probably not appropriate for very high dimensional data. nevertheless  we hope that our techniques will be useful for various 1-d  and higher dimensional  applications  including the discovery of regions of significantly increased brain activity  corresponding to given cognitive tasks  using fmri data. as discussed above  we also are actively engaged in deriving more powerful statistical tests for overdensities  and the corresponding density functions  under a variety of application-specific models  for example  normally distributed counts inferred from the time series of previous counts  applied to the over-the-counter retail data . as long as the density function satisfies the simple conditions described above  our algorithm can be used to rapidly find the maximum density region according to this function. finally  we are interested in extending our search for overdensities to more general classes of multivariate density functions  thus allowing the discovery of clusters which are significant even after adjusting for multiple covariates.
             table 1: performance of algorithm  real-world datasets test	method	sec/orig	speedup	sec/rep	speedup	regions  orig 	regions  rep 
ednaive1x1x1.1m1m n = 1 exact1x1.1x1.1m1kapprox-1x1.1x1.1m1approx-1x1.1x1.1m1ednaive1x1x1.1b1b n = 1 exact1x1.1x1.1m1kapprox-1x1.1x1.1m1kapprox-1x1.1x1.1m1kednaive1x1x1.1b1b n = 1 exact1x1.1x1.1b1kapprox-1x1.1x1.1b1kapprox-1x1.1x1.1b1knational otcnaive1x1x1.1m1m n = 1 exact1x1.1x1k1knational otcnaive1x1x1.1b1b n = 1 exact1x1.1x1.1m1kregional otcnaive1x1x1.1m1m n = 1 exact1x1.1x1k1kregional otcnaive1x1x1.1b1b n = 1 exact1x1.1x1.1m1k1.	acknowledgements	
this work was supported in part by nsf grant iis-1.
