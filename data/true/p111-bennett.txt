text classifiers that give probability estimates are more readily applicable in a variety of scenarios. for example  rather than choosing one set decision threshold  they can be used in a bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time. however  the quality of the probability estimates is crucial. we review a variety of standard approaches to converting scores  and poor probability estimates  from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the  extremely irrelevant    hard to discriminate   and  obviously relevant  items are often significantly different. finally  we analyze the experimental performance of these models over the outputs of two text classifiers. the analysis demonstrates that one of these models is theoretically attractive  introducing few new parameters while increasing flexibility   computationally efficient  and empirically preferable.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval; i.1  artificial intelligence : learning; i.1  pattern recognition : design methodology
general terms
algorithms  experimentation  reliability.
keywords
text classification  cost-sensitive learning  active learning  classifier combination
1. introduction
모text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking. for example  rather than choosing one set decision threshold  they can be used in a bayesian risk model  to issue a runtime decision which minimizes the expected cost of a user-specified
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
cost function dynamically chosen at prediction time. this can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time. furthermore  the costs can be changed without retraining the model. additionally  probability estimates are often used as the basis of deciding which document's label to request next during active learning  1  1 . effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly . finally  they are also amenable to making other types of cost-sensitive decisions  and for combining decisions . however  in all of these tasks  the quality of the probability estimates is crucial.
모parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data. since many text classification tasks often have very little training data  we focus on parametric methods. however  most of the existing parametric methods that have been applied to this task have an assumption we find undesirable. while some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances  they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes. we introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models. additionally  these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the  extremely irrelevant    hard to discriminate   and  obviously relevant  items.
모we first review related work on improving probability estimates and score modeling in information retrieval. then  we discuss in further detail the need for asymmetric models. after this  we describe two specific asymmetric models and  using two standard text classifiers  na몮 ve bayes and svms  demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores. we then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods. finally  we summarize our contributions and discuss future directions.
1. related work
모parametric models have been employed to obtain probability estimates in several areas of information retrieval. lewis & gale  use logistic regression to recalibrate na몮 ve bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning. manmatha et. al  introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines. they use a different parametric distribution for the relevant and irrelevant classes  but do not pursue two-sided asymmetric distributions for a single class as described here. they also survey the long history of modeling the relevance scores of search engines. our work is similar in flavor to these previous attempts to model search engine scores  but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.
모focus on improving probability estimates has been growing lately. zadrozny & elkan  provide a corrective measure for decision trees  termed curtailment  and a non-parametric method for recalibrating na몮 ve bayes. in more recent work   they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na몮 ve bayes and a linear svm. while they compared their methods to other parametric methods based on symmetry  they fail to provide significance test results. our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue. in addition  their methods reduce the resolution of the scores output by the classifier  the number of distinct values output   but the methods here do not have such a weakness since they are continuous functions.
모there is a variety of other work that this paper extends. platt  uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an svm. his work showed that this post-processing method not only can produce probability estimates of similar quality to svms directly trained to produce probabilities  regularized likelihood kernel methods   but it also tends to produce sparser kernels  which generalize better . finally  bennett  obtained moderate gains by applying platt's method to the recalibration of na몮 ve bayes but found there were more problematic areas than when it was applied to svms.
모recalibrating poorly calibrated classifiers is not a new problem. lindley et. al  first proposed the idea of recalibrating classifiers  and degroot & fienberg  1  1  gave the now accepted standard formalization for the problem of assessing calibration initiated by others  1  1 .
1. problem definition & approach
모our work differs from earlier approaches primarily in three points:  1  we provide asymmetric parametric models suitable for use when little training data is available;  1  we explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results;  1  we target text classifier outputs where a majority of the previous literature targeted the output of search engines.
1 problem definition
모the general problem we are concerned with is highlighted in figure 1. a text classifier produces a prediction about a document and gives a score s d  indicating the strength of its decision that the document belongs to the positive class  relevant to the topic . we assume throughout there are only two classes: the positive and the negative  or irrelevant  class  '+' and '-' respectively .
모there are two general types of parametric approaches. the first of these tries to fit the posterior function directly  i.e. there is one

figure 1: we are concerned with how to perform the box highlighted in grey. the internals are for one type of approach.
function estimator that performs a direct mapping of the score s to the probability p +|s d  . the second type of approach breaks the problem down as shown in the grey box of figure 1. an estimator for each of the class-conditional densities  i.e. p s|+  and p s|    is produced  then bayes' rule and the class priors are used to obtain the estimate for p +|s d  .
1 motivation for asymmetric distributions
모most of the previous parametric approaches to this problem either directly or indirectly  when fitting only the posterior  correspond to fitting gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters. we can visualize this as depicted in figure 1. since increasing s usually indicates increased likelihood of belonging to the positive class  then the rightmost distribution usually corresponds to p s|+ .

figure 1: typical view of discrimination based on gaussians
모however  using standard gaussians fails to capitalize on a basic characteristic commonly seen. namely  if we have a raw output score that can be used for discrimination  then the empirical behavior between the modes  label b in figure 1  is often very different than that outside of the modes  labels a and c in figure 1 . intuitively  the area between the modes corresponds to the hard examples  which are difficult for this classifier to distinguish  while the areas outside the modes are the extreme examples that are usually easily distinguished. this suggests that we may want to uncouple the scale of the outside and inside segments of the distribution  as depicted by the curve denoted as a-gaussian in figure 1 . as a result  an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.
모ideally  i.e. perfect classification  there will exist scores 붿  and 붿+ such that all examples with score greater than 붿+ are relevant and all examples with scores less than 붿  are irrelevant. furthermore  no examples fall between 붿  and 붿+. the distance | 붿    붿+ | corresponds to the margin in some classifiers  and an attempt is often made to maximize this quantity. because text classifiers have training data to use to separate the classes  the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved. this is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents  the similarity function  and the length and type of query.
모perfect classification corresponds to using two very asymmetric distributions  but in this case  the probabilities are actually one and zero and many methods will work for typical purposes. practically  some examples will fall between 붿  and 붿+  and it is often important to estimate the probabilities of these examples well  since they correspond to the  hard  examples . justifications can be given for both why you may find more and less examples between 붿  and 붿+ than outside of them  but there are few empirical reasons to believe that the distributions should be symmetric.
모a natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution  e.g. the laplace or the gaussian. an asymmetric laplace distribution can be achieved by placing two exponentials around the mode in the following manner:

where 붿  붹  and 붺 are the model parameters. 붿 is the mode of the distribution  붹 is the inverse scale of the exponential to the left of the mode  and 붺 is the inverse scale of the exponential to the right. we will use the notation 붦 x | 붿 붹 붺  to refer to this distribution.

figure 1: gaussians vs. asymmetric gaussians. a shortcoming of symmetric distributions - the vertical lines show the modes as estimated nonparametrically.
we can create an asymmetric gaussian in the same manner:

 1 
where 붿  횳  and r are the model parameters. to refer to this asymmetric gaussian  we use the notation 붞 x | 붿 횳 r . while these distributions are composed of  halves   the resulting function is a single continuous distribution.
모these distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters. we could instead try mixture models for each component or other extensions  but most other extensions require at least as many parameters  and can often be more computationally expensive . in addition  the motivation above should provide significant cause to believe the underlying distributions actually behave in this way. furthermore  this family of distributions can still fit a symmetric distribution  and finally  in the empirical evaluation  evidence is presented that demonstrates this asymmetric behavior  see figure 1 .
모to our knowledge  neither family of distributions has been previously used in machine learning or information retrieval. both are termed generalizations of an asymmetric laplace in   but we refer to them as described above to reflect the nature of how we derived them for this task.
1 estimating the parameters of the asymmetric distributions
모this section develops the method for finding maximum likelihood estimates  mle  of the parameters for the above asymmetric distributions. in order to find the mles  we have two choices:  1  use numerical estimation to estimate all three parameters at once  1  fix the value of 붿  and estimate the other two  붹 and 붺 or 횳 and r  given our choice of 붿  then consider alternate values of 붿. because of the simplicity of analysis in the latter alternative  we choose this method.
1.1 asymmetric laplace mles
모for d = {x1 x1 ... xn} where the xi are i.i.d. and x 몲 붦 x | 붿 붹 붺   the likelihood is. now  we fix 붿 and compute the maximum likelihood for that choice of 붿. then  we can simply consider all choices of 붿 and choose the one with the maximum likelihood over all choices of 붿.
모the complete derivation is omitted because of space but is available in . we define the following values:

모note that dl and dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution  respectively  and 붿. finally the mles for 붹 and 붺 for a fixed 붿 are:

these estimates are not wholly unexpected since we would obtain  if we were to estimate 붹 independently of 붺. the elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it  i.e. the closer dl and dr are to being equal  the closer the resulting inverse scales .
모by continuity arguments  when n = 1  we assign where 1 is a small constant that acts to disperse the distribution to
a uniform. similarly  when  we assign
where inf is a very large constant that corresponds to an extremely sharp distribution  i.e. almost all mass at 붿 for that half . dr = 1 is handled similarly.
모assuming that 붿 falls in some range  뷋 뷍  dependent upon only the observed documents  then this alternative is also easily computable. given nl sl nr sr  we can compute the posterior and the mles in constant time. in addition  if the scores are sorted  then we can perform the whole process quite efficiently. starting with the minimum 붿 = 뷋 we would like to try  we loop through the scores once and set nl sl nr sr appropriately. then we increase 붿 and just step past the scores that have shifted from the right side of the distribution to the left. assuming the number of candidate 붿s are o n   this process is o n   and the overall process is dominated by sorting the scores  o nlogn   or expected linear time .
1.1 asymmetric gaussian mles
for d = {x1 x1 ... xn} where the xi are i.i.d. and x 몲
붞 x | 붿 횳 r   the likelihood is. the mles can be worked out similar to the above.
모we assume the same definitions as above  the complete derivation omitted for space is available in    and in addition  let:
		.
the analytical solution for the mles for a fixed 붿 is:
		 1 
	.	 1 
by continuity arguments  when n = 1  we assign r = 횳 =
inf  and when  and dl1 = 1  resp. dr1 = 1   we as-
  resp.  . again  the same computational complexity analysis applies to estimating these parameters.
1. experimental analysis
1 methods
모for each of the methods that use a class prior  we use a smoothed add-one estimate  i.e. where n is the number of documents. for methods that fit the class-conditional densities  p s|+  and p s|    the resulting densities are inverted using bayes' rule as described above. all of the methods below are fit using maximum likelihood estimates.
모for recalibrating a classifier  i.e. correcting poor probability estimates output by the classifier   it is usual to use the log-odds of the classifier's estimate as s d . the log-odds are defined to be
. the normal decision threshold  minimizing error  in terms of log-odds is at zero  i.e. p +|d  = p  |d  = 1 .
모since it scales the outputs to a space       the log-odds make normal  and similar distributions  applicable . lewis & gale  give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors. in general  fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.
gaussians
모a gaussian is fit to each of the class-conditional densities  using the usual maximum likelihood estimates. this method is denoted in the tables below as gauss.
asymmetric gaussians
모an asymmetric gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above. intervals between adjacent scores are divided by 1 in testing candidate 붿s  i.e. 1 points between actual scores occurring in the data set are tested. this method is denoted as a. gauss.
laplace distributions
모even though laplace distributions are not typically applied to this task  we also tried this method to isolate why benefit is gained from the asymmetric form. the usual mles were used for estimating the location and scale of a classical symmetric laplace distribution as described in . we denote this method as laplace below.
asymmetric laplace distributions
모an asymmetric laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above. as with the asymmetric gaussian  intervals between adjacent scores are divided by 1 in testing candidate 붿s. this method is denoted as a. laplace below.
logistic regression
모this method is the first of two methods we evaluated that directly fit the posterior  p +|s d  . both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels. as opposed to the above methods  one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier. when this is desired  these methods may be more appropriate. the previous methods will mostly preserve the rankings  but they can deviate if the data dictate it. thus  they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.
모lewis & gale  use logistic regression to recalibrate na몮 ve bayes for subsequent use in active learning. the model they use is:
	.	 1 
모instead of using the probabilities directly output by the classifier  they use the loglikelihood ratio of the probabilities    as the score s d . instead of using this below  we will use the logodds ratio. this does not affect the model as it simply shifts all of the scores by a constant determined by the priors. we refer to this method as logreg below.
logistic regression with noisy class labels
모platt  proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an svm.
모this model differs from the logreg model only in how the parameters are estimated. the parameters are still fit using maximum likelihood estimation  but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled. the noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples  using bayes' rule to infer the probability of incorrect label .
모even though the performance of this model would not be expected to deviate much from logreg  we evaluate it for completeness. we refer to this method below as lr+noise.
1 data
모we examined several corpora  including the msn web directory  reuters  and trec-ap.
msn web directory
모the msn web directory is a large collection of heterogeneous web pages  from a may 1 web snapshot  that have been hierarchically classified. we used the same train/test split of 1 documents as that reported in . the msn web hierarchy is a seven-level hierarchy; we used all 1 of the top-level categories. the class proportions in the training set vary from 1% to 1%. in the testing set  they range from 1% to 1%. the classes are general subjects such as health&fitness and travel&vacation. human indexers assigned the documents to zero or more categories.
모for the experiments below  we used only the top 1 words with highest mutual information for each class; approximately 1k words appear in at least three training documents.
reuters
모the reuters 1 corpus  contains reuters news articles from 1. for this data set  we used the modapte standard train/ test split of 1 documents  1 unused documents . the classes are economic subjects  e.g.   acq  for acquisitions   earn  for earnings  etc.  that human taggers applied to the document; a document may have multiple subjects. there are actually 1 classes in this domain  only 1 of which occur in the training and testing set ; however  we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 limiting to the ten largest classes allows us to compare our results to previously published results  1  1  1  1 . the class proportions in the training set vary from 1% to 1%. in the testing set  they range from 1% to 1%.
모for the experiments below we used only the top 1 words with highest mutual information for each class; approximately 1k words appear in at least three training documents.
trec-ap
모the trec-ap corpus is a collection of ap news stories from 1 to 1. we used the same train/test split of 1 documents that was used in . as described in   see also    the categories are defined by keywords in a keyword field. the title and body fields are used in the experiments below. there are twenty categories in total. the class proportions in the training set vary from 1% to 1%. in the testing set  they range from 1% to 1%.
모for the experiments described below  we use only the top 1 words with the highest mutual information for each class; approximately 1k words appear in at least 1 training documents.
1 classifiers
모we selected two classifiers for evaluation. a linear svm classifier which is a discriminative classifier that does not normally output probability values  and a na몮 ve bayes classifier whose probability outputs are often poor  1  1  but can be improved  1  1  1 .
svm
모for linear svms  we use the smox toolkit which is based on platt's sequential minimal optimization algorithm. the features were represented as continuous values. we used the raw output score of the svm as s d  since it has been shown to be appropriate before . the normal decision threshold  assuming we are seeking to minimize errors  for this classifier is at zero.
na몮 ve bayes
모the na몮 ve bayes classifier model is a multinomial model . we smoothed word and class probabilities using a bayesian estimate  with the word prior  and a laplace m-estimate  respectively. we use the log-odds estimated by the classifier as s d . the normal decision threshold is at zero.
1 performance measures
모we use log-loss  and squared error  1  1  to evaluate the quality of the probability estimates. for a document d with class c d  뫍 {+  }  i.e. the data have known labels and not probabilities   logloss is defined as 붻 c d  + logp +|d  + 붻 c d    logp  |d 
.
where 붻 a b  = 1 if a = b and 1 otherwise. the squared error is 붻 c d  +  1   p +|d  1 + 붻 c d     1   p  |d  1. when the class of a document is correctly predicted with a probability of one  log-loss is zero and squared error is zero. when the class of a document is incorrectly predicted with a probability of one  log-loss is   and squared error is one. thus  both measures assess how close an estimate comes to correctly predicting the item's class but vary in how harshly incorrect predictions are penalized.
모we report only the sum of these measures and omit the averages for space. their averages  average log-loss and mean squared error  mse   can be computed from these totals by dividing by the number of binary decisions in a corpus.
모in addition  we also compare the error of the classifiers at their default thresholds and with the probabilities. this evaluates how the probability estimates have improved with respect to the decision threshold p +|d  = 1. thus  error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates. it is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.
모we use a a standard paired micro sign test  to determine statistical significance in the difference of all measures. only pairs that the methods disagree on are used in the sign test. this test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed. we use a significance level of p = 1.
1 experimental methodology
모as the categories under consideration in the experiments are not mutually exclusive  the classification was done by training n binary classifiers  where n is the number of classes.
모in order to generate the scores that each method uses to fit its probability estimates  we use five-fold cross-validation on the training data. we note that even though it is computationally efficient to perform leave-one-out cross-validation for the na몮 ve bayes classifier  this may not be desirable since the distribution of scores can be skewed as a result. of course  as with any application of n-fold cross-validation  it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier.
1 results & discussion
the results for recalibrating na몮 ve bayes are given in table 1a.
table 1b gives results for producing probabilistic outputs for svms.
log-losserror1errorslog-losserror1errorsmsn webmsn webgauss-111gauss-111a.gauss-111a. gauss-111laplace-111laplace-111a.laplace-11 1a. laplace-111logreg-111logreg-111lr+noise-111lr+noise-111na몮 ve bayes-111linear svmn/an/a1reutersreutersgauss-111gauss-111a.gauss-111a. gauss-111laplace-111laplace-111a.laplace-1 1 1a. laplace-111logreg-111logreg-111lr+noise-111lr+noise-111na몮 ve bayes-111linear svmn/an/a1trec-aptrec-apgauss-111gauss-111a.gauss-111a. gauss-111laplace-111laplace-111a.laplace-11 1a. laplace-11 1 logreg-111logreg-111lr+noise-111lr+noise-111na몮 ve bayes-111linear svmn/an/a1table 1:  a  results for na몮 ve bayes  left  and  b  svm  right . the best entry for a corpus is in bold. entries that are statistically significantly better than all other entries are underlined. a   denotes the method is significantly better than all other methods except for na몮 ve bayes. a   denotes the entry is significantly better than all other methods except for a. gauss  and na몮 ve bayes for the table on the left . the reason for this distinction in significance tests is described in the text.모we start with general observations that result from examining the performance of these methods over the various corpora. the first is that a. laplace  lr+noise  and logreg  quite clearly outperform the other methods. there is usually little difference between the performance of lr+noise and logreg  both as shown here and on a decision by decision basis   but this is unsurprising since lr+noise just adds noisy class labels to the logreg model. with respect to the three different measures  lr+noise and logreg tend to perform slightly better  but never significantly  than a. laplace at some tasks with respect to log-loss and squared error. however  a. laplace always produces the least number of errors for all of the tasks  though at times the degree of improvement is not significant.
모in order to give the reader a better sense of the behavior of these methods  figures 1 show the fits produced by the most competitive of these methods versus the actual data behavior  as estimated nonparametrically by binning  for class earn in reuters. figure 1 shows the class-conditional densities  and thus only a. laplace is shown since logreg fits the posterior directly. figure 1 shows the estimations of the log-odds   i.e.  . viewing the log-odds  rather than the posterior  usually enables errors in estimation to be detected by the eye more easily.
모we can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on. looked at in this way only two methods  na몮 ve bayes and a. gauss  ever have more pairwise wins than a. laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins  i.e. they are dragged down by heavy penalties .
모in addition  this comparison of pairwise wins means that for those cases where logreg and lr+noise have better scores than a. laplace  it would not be deemed significant by the sign test at any level since they do not have more wins. for example  of the 1k binary decisions over the msn web dataset  a. laplace had approximately 1k pairwise wins versus logreg and lr+noise. no method ever has more pairwise wins than a. laplace for the error comparison nor does any method every achieve a better total.
모the basic observation made about na몮 ve bayes in previous work is that it tends to produce estimates very close to zero and one  1  1 . this means if it tends to be right enough of the time  it will produce results that do not appear significant in a sign test that ignores size of difference  as the one here . the totals of the squared error and log-loss bear out the previous observation that  when it's wrong it's really wrong .

figure 1: the empirical distribution of classifier scores for documents in the training and the test set for class earn in reuters. also shown is the fit of the asymmetric laplace distribution to the training score distribution. the positive class  i.e. earn  is the distribution on the right in each graph  and the negative class  i.e.  earn  is that on the left in each graph.

-1	-1	-1	-1	-1	1	1	-1	-1	1	1 s d  = naive bayes log-odds	s d  = linear svm raw score
figure 1: the fit produced by various methods compared to the empirical log-odds of the training data for class earn in reuters.모there are several interesting points about the performance of the asymmetric distributions as well. first  a. gauss performs poorly because  similar to na몮 ve bayes  there are some examples where it is penalized a large amount. this behavior results from a general tendency to perform like the picture shown in figure 1  note the crossover at the tails . while the asymmetric gaussian tends to place the mode much more accurately than a symmetric gaussian  its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate. figure 1 is actually a result of fitting the two distributions to real data. as a result  at the tails there can be a large discrepancy between the likelihood of belonging to each class. thus when there are no outliers a. gauss can perform quite competitively  but when there is an outlier a. gauss is penalized quite heavily. there are enough such cases overall that it seems clearly inferior to the top three methods. however  the asymmetric laplace places much more emphasis around the mode  figure 1  because of the different distance function  think of the  sharp peak  of an exponential . as a result most of the mass stays centered around the mode  while the asymmetric parameters still allow more flexibility than the standard laplace. since the standard laplace also corresponds to a piecewise fit in the log-odds space  this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes. additionally  the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well. even in cases where the test distribution differs from the training distribution  figure 1   a. laplace still yields a solution that gives a better fit than logreg  figure 1   the next best competitor.
모finally  we can make a few observations about the usefulness of the various performance metrics. first  log-loss only awards a finite amount of credit as the degree to which something is correct improves  i.e. there are diminishing returns as it approaches zero   but it can infinitely penalize for a wrong estimate. thus  it is possible for one outlier to skew the totals  but misclassifying this example may not matter for any but a handful of actual utility functions used in practice. secondly  squared error has a weakness in the other direction. that is  its penalty and reward are bounded in  1   but if the number of errors is small enough  it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates. for example  consider a method that only estimates probabilities as zero or one  which na몮 ve bayes tends to but doesn't quite reach if you use smoothing . this method could win according to squared error  but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome. for these reasons  we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced. these observations are straightforward from the definitions but are underscored by the evaluation.
1. future work
모a promising extension to the work presented here is a hybrid distribution of a gaussian  on the outside slopes  and exponentials  on the inner slopes . from the empirical evidence presented in   the expectation is that such a distribution might allow more emphasis of the probability mass around the modes  as with the exponential  while still providing more accurate estimates toward the tails.
모just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line  we could directly fit the log-odds of the posterior with a three-piece line  a spline  instead of indirectly doing the same thing by fitting the asymmetric laplace. this approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric laplace.
모finally  extending these methods to the outputs of other discriminative classifiers is an open area. we are currently evaluating the appropriateness of these methods for the output of a voted perceptron . by analogy to the log-odds  the operative score that apweight perceptrons voting + pears promising is log	.
weight perceptrons voting  
1. summary and conclusions
모we have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier. in addition  we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function. we have given an efficient way to estimate the parameters of these distributions.
모while these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give  the asymmetric gaussian appears to have too great of an emphasis away from the modes. in striking contrast  the asymmetric laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods  though comparable performance is sometimes achieved with one of two varieties of logistic regression. given the ease of estimating the parameters of this distribution  it is a good first choice for producing quality probability estimates.
acknowledgments
we are grateful to francisco pereira for the sign test code  anton likhodedov for logistic regression code  and john platt for the code support for the linear svm classifier toolkit smox. also  we sincerely thank chris meek and john platt for the very useful advice provided in the early stages of this work. thanks also to jaime carbonell and john lafferty for their useful feedback on the final versions of this paper.
