1.1 documents
the document set of a test collection should be a sample of the kinds of texts that will be encounteredin the operational setting of interest. it is important that the document set reflect the diversity of subject matter  word choice  literary styles  document formats  etc. of the operational setting for the retrieval results to be representative of the performance in the real task. frequently  this means the document set must be large. the primary trec test collections contain about 1 gigabytes of text  between 1 and 1 1 documents . the document sets used in various tracks have been smaller and larger depending on the needs of the track and the availability of data.
   the primary trec document sets consist mostly of newspaper or newswire articles  though there are also some government documents  the federal register  patent applications  and computer science abstracts  computer selects by ziff-davis publishing  included. high-level structures within each document are tagged using sgml  and each document is assigned an unique identifier called the docno. in keeping of the spirit of realism  the text was kept as close to the original as possible. no attempt was made to correct spelling errors  sentence fragments  strange formatting around tables  or similar faults.
1.1 topics
trec distinguishes between a statement of information need  the topic  and the data structure that is actually given to a retrieval system  the query . the trec test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. the format of a topic statement has evolved since the beginning of trec  but it has been stable for the past several years. a topic statement generally consists of four sections: an identifier  a title  a description  and a narrative. an example topic taken from this year's filtering track is shown in figure 1.
   the different parts of the trec topics allow researchers to investigate the effect of different query lengths on retrieval performance. the  titles  in topics 1 were specially designed to allow experiments with very short queries; those title fields consist of up to three words that best describe the topic. the description field is a one sentence description of the topic area. the narrative gives a concise description of what makes a document relevant.
   participants are free to use any method they wish to create queries from the topic statements. trec distinguishes among two major categories of query construction techniques  automatic methods and manual methods. an automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. the definition of manual query construction methods is very broad  ranging from simple tweaks to an automatically derived query  through manual construction of an initial query  to multiple query reformulations based on the document sets retrieved. since these methods require radically different amounts of  human  effort  care must be taken when comparing manual results to ensure that the runs are truly comparable.
 num  number: r1
 title  telemarketing practices u.s.
 desc  description:
find documents which reflect telemarketing practices in the u.s. which are intrusive or deceptive and any efforts to control or regulate against them.
 narr  narrative:
telemarketing practices found to be abusive  intrusive  evasive  deceptive  fraudulent  or in any way unwanted by persons contacted are relevant. only such practices in the u.s. are relevant. all efforts to halt these practices  including lawsuits  legislation or regulation are also relevant.
figure 1: a sample trec 1 topic from the filtering track.
   trec topic statements are created by the same person who performs the relevance assessments for that topic  the assessor . usually  each assessor comes to nist with ideas for topics based on his or her own interests  and searches the document collection using nist's prise system to estimate the likely number of relevant documents per candidate topic. the nist trec team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors.
1.1 relevance judgments
the relevance judgments are what turns a set of documents and topics into a test collection. given a set of relevance judgments  the retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. trec almost always uses binary relevance judgments-either a document is relevant to the topic or it is not. to define relevance for the assessors  the assessors are told to assume that they are writing a report on the subject of the topic statement. if they would use any information contained in the document in the report  then the  entire  document should be marked relevant  otherwise it should be marked irrelevant. the assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.
   relevance is inherently subjective. relevance judgments are known to differ across judges and for the same judge at differenttimes . furthermore  a set of static  binary relevance judgmentsmakes no provisionfor the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. despite the idiosyncratic nature of relevance  test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments .
   the relevance judgments in early retrieval test collections were complete. that is  a relevance decision was made for every document in the collection for every topic. the size of the trec document sets makes complete judgments utterly infeasible-with 1 documents  it would take over 1 hours to judge the entire document set for one topic  assuming each document could be judged in just 1 seconds. instead  trec uses a technique called pooling  to create a subset of the documents  the  pool   to judge for a topic. each document in the pool for a topic is judged for relevance by the topic author. documents that are not in the pool are assumed to be irrelevant to that topic.
   the judgment pools are created as follows. when participants submit their retrieval runs to nist  they rank their runs in the order they prefer them to be judged. nist chooses a number of runs to be mergedinto the pools  and selects that many runs from each participant respecting the preferred ordering. for each selected run  the top documents  usually    per topic are added to the topics' pools. since the retrieval results are ranked by decreasing similarity to the query  the top documents are the documents most likely to be relevant to the topic. many documents are retrieved in the top for more than one run  so the pools are generally much smaller the theoretical maximum of the-number-of-selected-runs documents  usually about 1 the maximum size .
   the use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents.
   zobel demonstrated that the quality of the pools  the number and diversity of runs contributing to the pools and the depth to which those runs are judged  does affect the quality of the final collection . he also found that the trec collections were not biased against unjudged runs. in this test  he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. for the trec-1 ad hoc collection  he found that using the unique relevant documents increased a run's 1 point average precision score by an average of 1 %. the maximum increase for any run was 1 %. the average increase for the trec-1 ad hoc collection was somewhat higher at 1 %.
   a similar investigation of the trec-1 ad hoc collection showed that every automatic run that had a mean average precision score of at least .1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents . that investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs  an effect noted by the organizers of the ntcir  nacsis test collection for evaluation of informationretrieval systems  workshop who performedtheir own manual runs to supplement their pools .
   while the lack of any appreciable difference in the scores of submitted runs is not a guarantee that all relevant documents have been found  it is very strong evidence that the test collection is reliable for comparative evaluations of retrieval runs. indeed  the differences in scores resulting from incomplete pools observed here are smaller than the differences that result from using different relevance assessors .
1 evaluation
retrieval runs on a test collection can be evaluated in a number of ways. in trec  all ad hoc tasks are evaluated using the treceval package written by chris buckley of sabir research . this package reports about 1 different numbers for a run  including recall and precision at various cut-off levels plus single-valued summary measures that are derived from recall and precision. precision is the proportion of retrieved documents that are relevant  while recall is the proportion of relevant documents that are retrieved. a cut-off level is a rank that defines the retrieved set; for example  a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. the treceval programreports the scores as averages over the set of topics where each topic is equally weighted.  the alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important. 
   precision reaches its maximal value of 1 when only relevant documents are retrieved  and recall reaches its maximal value  also 1  when all the relevant documentsare retrieved. note  however  that these theoretical maximum values are not obtainableas an averageovera set of topics at a single cut-offlevel because differenttopics havedifferent numbers of relevant documents. for example  a topic that has fewer than ten relevant documents will have a precision score less than one after ten documents are retrieved regardless of how the documents are ranked. similarly  a topic with more than ten relevant documents must have a recall score less than one after ten documents are retrieved. at a single cut-off level  recall and precision reflect the same information  namely the number of relevant documents retrieved. at varying cut-off levels  recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.
   of all the numbers reported by treceval  the recall-precision curve and mean  non-interpolated average precision are the most commonly used measures to describe trec retrieval results. a recall-precision curve plots precision as a function of recall. since the actual recall values obtained for a topic depend on the number of relevant documents  the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. the particular interpolation method used is given in appendix a  which also defines many of the other evaluation measures reported by treceval. recall-precision graphs show the behavior of a retrieval run over the entire recall spectrum.
   mean average precision is the single-valued summary measure used when an entire graph is too cumbersome. the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . the mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. the average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents  and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. geometrically  mean average precision is the area underneath a non-interpolated recallprecision curve.
   only three of the tasks in trec 1  the topic distillation task in the web track  the routing task in the filtering track  and the task in the cross-language track  were tasks that can be evaluated with treceval. the remaining tasks used other evaluation measures that are described in detail in the track overview paper for that task  and are table 1: number of participants per track and total number of distinct participants in each trec
tracktrec111111ad hoc1111---routing111-----interactive--11111spanish--11------confusion---1------database merging---1------filtering---1111chinese----1-----nlp----1-----speech-----11--cross-language-----111high precision-----1----very large corpus------1---query------11--question answering-------11web-------11video---------1novelty----------1total participants111111briefly described in appendix a. the bulk of appendix a consists of the evaluation output for each run submitted to trec 1.
1 trec 1 tracks
trec's track structure was begun in trec-1  1 . the tracks serve several purposes. first  tracks act as incubators for new research areas: the first running of a track often defines what the problem really is  and a track creates the necessary infrastructure  test collections  evaluation methodology  etc.  to support research on its task. the tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. finally  the tracks make trec attractive to a broader community by providing tasks that match the research interests of more groups.
   table 1 lists the different tracks that were in each trec  the number of groups that submitted runs to that track  and the total number of groups that participated in each trec. the tasks within the tracks offered for a given trec have diverged as trec has progressed. this has helped fuel the growth in the number of participants  but has also created a smaller common base of experience among participants since each participant tends to submit runs to fewer tracks.
   this section describes the tasks performed in the trec 1 tracks. see the track reports elsewhere in this proceedings for a more complete description of each track.
1 the cross-language  clir  track
the task in the clir track is an ad hoc retrieval task in which the documents are in one language and the topics are in a
different language. the goal of the track is to facilitate research on systems that are able to retrieve relevant documents regardless of the language a document happens to be written in. the trec 1 cross-language track used arabic documents and english topics. an arabic version of the topics was also developed so that cross-language retrieval performance could be compared with the equivalent monolingual performance.
   the document set was created and released by the linguistic data consortium   arabic newswire part 1   catalog number ldc1 ; it is the same documentcollection that was used in the trec 1 clir track. the collection consists of 1 megabytes of news articles taken from the agence france presse  afp  arabic newswire: 1 articles dated from may 1  1 through december 1  1.
   fifty topics were created for the track using the standard topic development protocol except that topic development took place at the linguistic data consortium  ldc . the assessors were fluent in both arabic and english  for most assessors arabic was their first language . they searched the document collection using a retrieval system developed by the ldc for the task and arabic as the query language. once fifty topics were selected from among the candidate topics  the assessor who developed the topic created the full topic statement first in english and then in arabic. the assessors' instructions were that the arabic version of the topic should contain the same information as the english version  but should be expressed in a way that would seem natural to a native speaker of arabic. the english and arabic versions of the topics were made available to the track participants who were asked to check the topics for substantive differences among the different versions. a few changes were suggested by participants  and those changes were made to produce the final version of the topics.
   forty-one runs from nine different groups were submitted to the track. twenty-three of the runs were crosslanguage runs and eighteen were monolingual runs. one monolingual run was a manual run. two groups submitted only monolingual runs  one group submitted only cross-language runs  and the remaining six groups submitted at least one run of each type.
   the assessment pools were created using all submitted runs and using the top 1 documents from each run. the average size of the pools was 1 documents. the ldc assessors judged each document in the pools using binary  relevant/not relevant  assessments.
   there was some concern over the test collection built in the trec 1 clir track in that the judgment pools were not as complete as they ideally would be. the 1 collection contained 1 topics. for 1 of the topics  at least 1 % of the known relevant documents for that topic were retrieved by one group. further  mean average precision scores decreased by an average of 1 %  with a maximum difference of 1 %  when runs were evaluated without using the group's unique relevant documents. this year's collection has no similar concerns. the average number of relevant documents over the 1 topics is 1  with a minimum of 1 relevant documents and a maximum of 1 relevant documents. only 1 topics had at least 1 % of the relevant documents retrieved by one group. changes in mean average precision scores when unique relevant documents are removed were similar to the trec ad hoc collections: an average decrease of a little less than 1 % with a maximum change of 1 %.
   the average size of the trec 1 pools was larger then the average size of the 1 pools  1 vs. 1  even though the 1 pools used more runs and went deeper into the ranked list. thus  the results produced by different systems are clearly more similar to one another in 1 than in 1. but why this should be so is unclear. it could be that the systems are converging to a single effective strategy. the track made a standard set of resources such as stemmers and bi-lingual dictionaries available to participants; common resources are likely to reduce differences among systems. it may also be that the topic set in 1 was intrinsically easier than the 1 set  though the effectiveness of the best automatic systems is slightly lower in 1 than in 1 which would suggest the opposite conclusion.
   as has become common in the clir track  the most effective runs as measured by mean average precision  map  were cross-language runs  not monolingual runs. the best cross-language run was from the university of massachusetts  run umassx1n  with a map score of .1  while the best monolingual run was from the university of neuchatel  run unine1  with a map score of .1. these two runs were the top two runs as measured by precision at document cut-off level 1 as well  but in this case their order was reversed: umassx1n had a p 1  score of.1 and unine1 a score of .1. the university of massachusetts submitted one monolingual run  map: .1  p 1 : .1  though there is not a corresponding cross-language run from the university of neuchatel. thus it is not possible to tell from this data whether monolingual access is better for high precision searches in general.
1 the filtering track
the filtering task is to retrieve just those documents in a document stream that match the user's interest as represented by the topic. once again there were three tasks in the trec 1 filtering track: an adaptive filtering task  a batch filtering task  and a routing task.
   in the adaptive filtering task  a system starts with a profile derived from the topic statement and a small number of examples of relevant documents  and processes documents one at a time in date order. for each document in turn  the system must make a binary decision whether to retrieve it. if the system decides to retrieve the document  it obtains the relevance judgment for that document  and can modify its profile based on the judgment if desired. the final output is the unranked set of retrieved documents for the topic.
   the batch filtering task is a simpler version of the adaptive task. in this task  the system is given a topic and a  relatively large  set of training documents such that each document in the training set is labeled as relevant or not relevant. from this data  the system creates a profile and a rule for when a document should be retrieved. the rule is applied to each document in the test set of documents without further modification. once again  the final output is an unranked set of retrieved documents.
   in the routing task  the system again builds a profile or query from a topic statement and a trainingset of documents  but then uses the query to rank the test portion of the collection. ranking the collection by similarity to the query  routing  is an easier problem than making a binary decision as to whether a document should be retrieved  batch filtering  because the latter requires a threshold that is difficult to set appropriately. the final output for the routing task is a list of 1 documents ranked by decreasing similarity to the query.
   the trec 1 filtering task used the same corpus as the trec 1 track   reuters corpus  volume 1  english language  1-1 to 1-1  from reuters  http://about.reuters.com/ researchandstandards/corpus/ . this collection consists of approximately 1 news stories from august 1  1 through august 1  1. each document is tagged with reuters category codes  and a hierarchy of the reuters category codes is included with the corpus.
   two distinct types of topics were created for the track. the first set of 1 topics was created by nist assessors using the standard topic development protocol. once a candidate topic was provisionally accepted  the author of the topic was given five document sets of approximately 1 documents each to judge for the topic. these document sets were created at nist by using the relevant documents found in earlier rounds as input to a small set of different feedback systems. the combined set of judged documents was used as the training data for that topic. a second set of 1 topics was created by defining a topic to be the intersection of pairs of reuters category codes. in this case  a document is relevant to the topic if it has been assigned both of the appropriate category labels. the topic statement for an intersection topic is a simple combination of the category descriptors. a filtering track run was required to process all 1 topics.
   since filtering runs do not produce a ranked list  they cannot be evaluated using the usual ir measures. instead  filtering runs are generally evaluated using a utility function whereby a system is rewarded some number of points for retrieving a relevant document and penalized a different number of points for retrieving an irrelevant document. because raw utility scores do not average well  the scores for individual topics are normalized  scaled  and then averaged. details of the trec 1 filtering evaluation measures are given in the filtering track overview paper. routing runs are evaluated using mean average precision since routing runs produce a ranked list of documents.
   seventy-three runs from twenty-one different groups were submitted to the filtering track. of these  1 runs are adaptive filtering runs  1 are batch filtering runs  and 1 are routing runs. the most striking features of the filtering results was the large difference in effectiveness between the assessor-created topics and the intersection topics. system effectiveness was uniformly poor for the intersection topics  including those systems that did relatively well on the human constructedtopics  and even includingrouting runs despite earlier research that shows the initial topic statement is a minor factor in system effectiveness for routing. the intersection topics were included in this year's test set to test whether this very inexpensive topic construction method is adequate for building comparative test collections. until the reasons for the very large difference in effectiveness are understood  we must conclude that intersection topics are not good substitutes for information need statements.
1 the interactive track
the interactive track was one of the first tracks to be introduced into trec. since its inception  the high-level goal of the track has been the investigation of searching as an interactive task by examining the process as well as the outcome.
   the trec 1 track was the second year of a two-year plan to implement a metrics-based comparison of interactive systems as suggested by the sigir 1 workshop on interactive retrieval at trec and beyond . in the first year of the plan during trec 1  participants performed observational studies of subjects using publiclyaccessible tools and the live web to accomplish a search task. the trec 1 track followed the observational studies by laboratory experiments focusing on question answering using web data.
   the track used an  open  version of the .gov collection that was created for the trec 1 web track. the collection was open in the sense that some links to pages outside the collection could be followed. most of the participants accessed the collection through the panoptic search engine made available by csiro  see http:// www.panopticsearch.com/ .
   the track defined eight different search tasks  two instances each for four general searching activities: looking for personal health information; seeing guidance on us government laws and policies; making travel plans; and gathering material for a report on a given subject. the search tasks were formulated such that the searcher was asked to find either any short answers to the question or any web sites that met the need specified in the task. the experimental protocol used in the track was based on the protocol developed for the trec-1 interactive track and allows the comparison of two systems  or system variants . the protocol required a minimum of 1 searchers. each searcher performed all eight tasks  half the task on one system and the other half on the other system. searchers were given at least ten minutes to complete the search  and groups were required to report the results obtained after ten minutes.
   six groups participated in the interactive track. each group examined their own set of hypotheses as suggested by their trec 1 observational studies. see the site reports in these proceedings for details of the individual experiments.
1 the novelty track
the novelty track is a new track in trec 1. the goal of the track is to investigate systems' abilities to locate relevant and new  nonredundant  information within the ranked set of documents returned by a traditional document retrieval system. similar to the question answering track  the motivation for the track is to assist the user of a retrieval system by eliminating extraneous information from the system response.
   the data for the track was taken from trecs 1 ad hoc collections. nist selected 1 topics from that set and selected up to 1 relevant documents for each topic  if there were more than 1 relevant documents  the top 1 according to the document ranking used were selected; in all cases documents from the congressional record were eliminated . each set of relevant documents was ranked at nist using the ordering produced by an effective manual run from the appropriate trec; participants were required to process the documents in this order. each document was also split into sentences at nist and sentences were assigned identifiers.
   a novelty track run consists of two ordered sets of sentence identifiers for each of the 1 topics. the first set of sentences is the set of sentences the system determined to contain relevant information. the second set of sentences  required to be a subset of the first set  is the set of sentences the system determined to contain new information  that is  relevant information that is not contained in earlier sentences.
   judgment data was created by having assessors manually perform the task. each topic was independently judged by two different assessors so that the effects of different human opinions could be assessed. in general  the two different assessors did disagree  though much of the disagreement revolved around how much context to include in the relevant set. that is  one assessor would include a string of sequential sentences in the relevant set while the other assessor would select fewer sentences from the same general area of the document. scoring for the track was based on the smaller relevant set  and its associated new set  because that seemed the best match for the task. participants were told that the scoring would be based on the smaller set before runs were submitted  but  of course  they did not have access to the assessor sentence sets. one assessor disagreed with the original assessor's relevance judgments for topic 1 and could find no relevant sentences in any of the documents. we eliminated that topic from the final test set  so scores were computed over the remaining 1 topics.
   the track guidelines specified sentence set recall and precision as the evaluation measures for the track. let be the number of matched sentences  i.e.  the number of sentences selected by both the assessor and the system  be the number of sentences selected by the assessor  and be the number of sentences selected by the system. then sentence set recall is and precision is . the f measure with recall and precision weighted equally  i.e.    was used as the final score for a topic.
   thirteen groups submitted 1 runs to the novelty track. for all runs  the f score for the relevant sentence sets was greater than the score for the new sentence sets. this might suggest that finding the relevant parts of a document is somewhat easier than finding the nonredundant parts  but is more likely to be a result of the different characteristics of the two tasks. a very small percentage of the total number of sentences were relevant  a median of 1 % across the 1 topics   whereas a very high percentage of the relevant sentences were novel  a median of 1 % across the 1 topics .
   one of the requirements for a new track is to do sanity-checking of the evaluation itself. to this end  nist computed the average f scores for the second human assessor sentence sets and for sets of sentences randomly selected table 1: average f scores for baseline and system results for the novelty track.
relevantnewsecond human judges random sentences thunv1from the target documents. the results are shown in table 1  which also includes the scores for the most effective system run  run thunv1  for comparison. the scores for the best system falls in between the human and random performance  support for a claim that the evaluation is credible.
1 the question answering  qa  track
the question answering track addresses the problem of information overload by encouraging research into systems that return actual answers  as opposed to ranked lists of documents  in response to a question. the trec 1 track contained two different tasks  the main task and the list task. both tasks were also run in trec 1  though there were significant differences in the task definitions between the two years.
   both tasks used a new document collection known as the aquaint corpus of english news text as the source of answers. this corpus is comprised of documents from three different sources: the ap newswire from 1  the new york times newswire from 1  and the  english portion of the  xinhua news agency from 1. there are approximately 1 1 documents and 1 gigabytes of text in the collection. the corpus may be obtained from the linguistic data consortium  www.ldc.upenn.edu  as catalog number ldc1.
   the main task was the focus of the track. as in previous years  participants received a set of fact-based  shortanswer questions  and systems were to return an answer to each question along with the id of a document that supports that answer. in contrast to previous years  systems could return only one response per question  and text snippets containing the answer were not acceptable-systems were required to return nothing more or less than the answer itself. questions were not guaranteed to have an answer in the collection. a system could return  nil  as a response to indicate its belief that the collection did not contain an answer to the question.
   the change to requiring exact answers was motivated by the belief that forcing systems to return precisely the answer is a necessary step in improving qa technology  not that it is a good idea for deployed qa systems. whether an answer was exact was determined by the nist assessor. assessors judged each response by assigning it exactly one of the following judgments:
incorrect: the answer string returned by the system does not contain a correct answer or the answer is not responsive; unsupported: the answer string contains a correct answer but the document returned does not support that answer;
non-exact: the answer string contains a correct answer and the document supports that answer  but the string contains more than just the answer or is missing bits of the answer; correct: the answer string consists of exactly a correct answer and that answer is supported by the document returned.
being  responsive  means such things as including units for quantitative responses  e.g.  $1 instead of 1  and answering with regard to a famous entity itself rather than its replicas or imitations. only the  correct  judgment was accepted for scoring purposes. nil was counted as correct when no correct answer was known to exist in the collection for that question.
   the test set of questions for the main task consists of 1 questions drawn from msnsearch and askjeeves logs. nist fixed the spelling  punctuation  and sometimes the grammar of the questions selected to be in the final question set  but the content of the question was precisely what was in the log.  some errors remained despite nist's attempts to fix such mistakes; questions with errors remained in the test set.  because it is impossible to know what kind of a response is desired for definition questions  e.g.  who is colin powell  what are steroids   when there is no specific target user  none of this type of question was included in the test set. nist made no other attempt to control table 1: number of questions answered correctly and confidence-weighted score for top 1 trec 1 main task qa runs.
confidence-numberweightedruncorrectscorelccmain1exactanswer1pris1irst11ibmpqsqacyc1the relative frequency of different question types. forty-six of the questions have no known correct answer in the document collection.
   systems were required to return exactly one response per question. within the submission file  the questions were ordered from most confident response to least confident response. that is  the question for which the system was most confident that it had returned a correct response was ranked first  then the question that the system was next most confident about  etc. so that the last question was the question for which the system was least confident in its response. the question ordering was done to test a system's ability to recognize whether it had found a good response since the final score assigned to a submission was based on this confidence ranking. the confidence-weighted score was inspired by the uninterpolated average precision measure for ranked retrieval output and is defined as
	number correct in first	ranks

this measure rewards systems for answering questions correctly early in the ranking more than it rewards for answering questions correctly later in the ranking.  this is equivalent to penalizing systems more for incorrectly answering questions early in the ranking. 
   thirty-four different groups participated in the qa track. each participant submitted at least one main task run for a total of 1 main task runs. the confidence-weighted evaluation measure succeeded in rewarding systems that were able to reliably determine whether they had found a good response  as illustrated in table 1. the table shows the number of questions whose answer was marked correct and the confidence-weighted score for the top five main task runs  ordered by confidence-score. the pris1 run has a lower confidence score than the exactanswer run despite answering 1 additional questions correctly.
   the list task required systems to assemble a set of answers as the response for a question. each question asked for a given number of instances of a certain type. for example  one of the questions used in the track was list 1 types of sweet potatoes. the response to a list question was an unordered list of  document-id  answer-string  pairs  where each pair was treated as a single instance. as in the main task  answer-strings were required to be exact.
   the questions for the list task were constructed by nist assessors. the target number of instances to retrieve was selected such that the document collection contained more than the requested number of instances  but more than one document was required to meet the target. a single document could contain multiple instances  and the same instance might be repeated in multiple documents.
   the assessors judged each list as a unit. individual instances were judged as in the main task. in addition  the assessor also marked a set of instances as distinct. the assessor arbitrarily chose any one of a set of equivalent correct instances to mark as the distinct one  and marked the remainder as not distinct. the accuracy score for a list question was computed as the number of correct distinct instances retrieved divided by the number of requested instances. the score for a run was the average accuracy over the 1 questions in the test set. five groups submitted nine runs for the list task.
outdoors: segment contains a recognizably outdoor location.
indoors: segment contains a recognizably indoor location.
face: segment contains at least one human face with the nose  mouth  and both eyes visible.
people: segment contains a group of two more humans  each of which is at least partially visible and is recognizable as a human.
cityscape: segment contains a recognizably city/urban/suburban setting.
landscape: segment contains a predominantly natural inland setting.
text overlay: segment contains superimposed text large enough to be read.
speech: a human voice uttering words is recognizable as such in this segment
instrumental sound: sound produced by one or more musical instruments is recognizable as such in this segment.
monologue: segment contains an event in which a single person is at least partially visible and speaks for a long time without interruption by another speaker.
figure 1: descriptions of features to be detected in the video track's feature extraction task.
1 the video track
trec 1 was the second year for the video track  a track designed to promote progress in content-based retrieval from digital video. this year's track contained three tasks: the shot boundary task  the feature extraction task  and the search task.
   the video data for the track consisted of mpeg-1/vcd recordings from the internet archive  http://www. archive.org/movies  and the open video project  http://www.open-video.org . the track defined a different set of files from these sources as the development sets and test sets for the different tasks. the search test collection contained approximately 1 hours of video  and the feature extraction and shot boundary test collections each contained about five hours of video. for the search and feature extraction tasks  the track also published a reference set of shot boundaries for the video collection. runs for these two tasks returned lists of shots as defined by the reference set.
   the goal in the shot boundary task was to  automatically  identify the shot boundaries in a given video clip. in addition to giving the location of the boundary as a time offset  systems were also required to specify whether the boundary was a cut or a gradual transition. system output was evaluated using automatic comparison to a set of reference shot boundaries created manually at nist  using set recall and precision as the measures. frame recall and frame precision  recall and precision of the individual frames within the shot  were also computed for each gradual transition detected by the system. eight groups submitted 1 shot boundary runs.
   there were two main motivations for the new feature extraction task. first  the ability to detect semantic concepts within video is seen as key to providing content-based access. the task is a first step toward building a benchmark for evaluating the effectiveness of particular feature detection methods. second  the track implemented a plan whereby participants' extraction output for features specific to the search task were made available to other participants. this allowed the track to investigate methods for exploiting detected features in a search task.
   ten different features  shown in figure 1  were specified as test features. shots containing the features were determined by nist assessors  using pools of shots submitted by participants as for document relevance assessing . a shot contains a feature if at least one frame within the shot matches the feature's description  and otherwise does not contain the feature.
   a feature extraction run consisted of a ranked list of up to 1 shots ordered by likelihood that the shot contains the feature. runs were evaluated using precision and recall  as well as uninterpolated average precision. measures were computed for each feature individually  but not averaged across features. eleven groups submitted 1 feature extraction runs.
   the search task was a typical ad hoc retrieval task where the  documents  were video shots and the topics were multimedia statements of information need. a search task run consisted of a ranked list of the top 1 shots ordered by likelihood that the shot satisfies the topic. the track distinguished two type of runs:  manual  runs where a human formulated the query based on the topic but there was no further human interventionin the run  and  interactive  where a human formulated an initial query and then refined the query based on initial search output to form the final ranked list. groups submitting interactive runs were required to report the amount of time the searcher spent to produce the final ranked list. effectiveness was measured using traditional ranked retrieval measures.
