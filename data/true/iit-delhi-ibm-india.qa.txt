a question answering  qa  system aims to return exact answers to natural language questions. while today information retrieval techniques are quite successful at locating within large collections of documents those that are relevant to a user's query  qa techniques that extract the exact answer from these retrieved documents still do not obtain very good accuracies. we approached the trec 1 question answering task as a semantics based question to answer matching problem. given a question we aimed to extract the relevant semantic entities in it so that we can pin-point the answer. in this paper we show that our technique obtains reasonable accuracy compared to other systems.
1 introduction
the goal of the trec qa task is to foster research on qa systems to improve the state of the art. a qa system works by returning exact answers to natural language questions. a question such as  in 1 who was the secretary general of the united nations   has only one exact answer. given such a question a qa system should return the exact answer to it.
　our team took part in the trec qa main task of 1. this was the first time our team participated in the qa task. the main task was the same as in trec 1  in that the test set consisted of question series where each series asked for information regarding a particular target. these targets included people  organizations  events and other entities. the questions on each target comprised of three types  factoid questions  list questions and one  other  question. factoid questions have exactly one correct answer. a list question has a list as its answer and the answer to the  other  question is to be interesting information about the target that is not covered by the preceding factoid and list questions in the series.
　the major difference between the 1 main task and the 1 main task was that questions were asked over both blog documents and newswire articles  rather than just newswire. a blog document is defined to be a blog post and its follow-up comments  a permalink . the blog collection contains well-formed english as well as badly-formed english and spam  and mining blogs for answers introduced significant new challenges in at least two aspects that are very important for functional qa systems: 1  being able to handle language that is not well-formed  and 1  dealing with discourse structures that are more informal and less reliable than newswire.
　in this paper we describe our approach to the trec 1 qa task. we approached the trec 1 question answering task as a semantics based question to answer matching problem. given a question we aimed to extract the relevant semantic entities in it so that we can pin-point the answer. overall our scores were well above the median score of the trec 1 runs from all teams.
1 system overview
our question answering system consists of several phases that work in sequential manner. each phase reduces the amount of data  the system has to handle from then on. the advantage of this approach is that progressive phases can perform more expensive operations on the data. the system is broadly divided into two main modules the question processing module  and the answer retrieval module.
1 question processing module
this is the module which takes the input set of questions and converts into a form that can be processed by the answer retrieval module. this module consists of question pre-processing  keyword generation  significant keyword selection and question classification.
　question pre-processing contains stopword removal and stemming. in question classification the questions are classified to yield their expected answer types and in query generation a parser is used in order to identify certain significant words that are given more weightage than the normal keywords in the construction of the query for both document retrieval and answer retrieval. figure 1 illustrates this module.
1 answer retrieval module
this module takes as input the keywords along with keyword significance scores and expected answer type all produced by the question processing module. using the keywords the answer retrieval module first finds the documents relevant to the question. only the top n documents are used for the next step. this greatly reduces the amount of text that need to be handled in subsequent steps. next from these documents we select the relevant sentences. in this sentence selection phase  all sentences are scored against the question and only the most relevant sentences are picked. in the final phase  we pin-point the answer within a sentence. as per the trec requirement  this answer should be exact and correct. figure 1 illustrates this module.
1 question processing
in this section we explain in more detail our question processing module introduced in the previous section. we will explain the various sub parts of this module in detail here. our goal is to not only extract the keywords but also as much semantic information as possible from the question that helps in getting the exact answer.
1 question pre-processing
this stage implements stop word elimination and stemming. a list if frequently occurring words is considered for stop word removal and porter's stemming algorithm  was used in order to stem the words. our stop word list comprised of 1 common english words such as it  the  at  etc. the output of this step comprises the set of all keywords from the question.
1 significant keywords selection
the quality of the answer-retrieval engine depends on the richness of the query that is given to it. in order to obtain the most relevant answers for the questions  the query tries to highlight certain words present in the question as significant words. the following words within the keywords are considered significant:
1. words referring full or part of the target  asthe target is already provided identifying this is very easy 
1. words that refer to the object of the question question object 
1. noun phrases in the absence of question object.
1.1 question object
we have observed that the object of the question is often one the significant words and hence that has to be identified. this can be detected with the help of a parser  we have used stanford parser  for all parser implementations . we have exploited the feature of this parser that recognizes the dependencies and detects the object.

figure 1: block diagram of question processor　for example in the question  what company produces his records    the parser detects that produces and records are connected and that records is the object.
1.1 significant noun phrases
even though most in most of the questions we are able to identify the object  there are exceptions. in such a case  we try to use all the noun phrases in the question minus the stop words as significant words. these noun phrases are obtained with the help of the parser.
1 question classification
question classification is the process of categorizing the question into one of the predetermined classes. this stage is needed because we need to know the expected answer type before returning an answer. we have used a rule based classifier that classifies a question into fine grained categories and their corresponding coarse categories.
1.1 categories
there are 1 coarse grained categories and 1 finegrained categories. the coarse-grained categories we selected were person  location  organization  number  title  jobtitle  date and money. we deemed these as the important categories based on past trecs. the named entity recognizer in the retrieval part has the same names as these 1 types and therefore  this is helpful in directly matching the indexed documents. the fine-grained types provide additional information for the answer retrieval module. the fine grained classes are almost similar to those present in the uiuc dataset  but with few additions and deletions. the categories used in our system are listed in table 1. to classify a question into the coarse and fine grained classes we defined a set of rules. our rule set comprised of 1 ordered rules. the ordering implied that certain rules had precedence over others. we give some examples here 
1.1 person
all who questions are classified as requiring answer type person. questions containing what  which and name with words like architect  engineer  artist etc. were for example categorized as requiring answer type person - individual.
1.1 location
all where questions are classified as location. further questions containing what  which and name with country  state  city  town  ocean or river refer to more specific locations and can be put in the respective fine grained categories.

figure 1: architecture of answer retrieval module1.1 date
all when questions are classified as date. specific mentions of year  month or time along with what and which are assigned to finer grained classes.
1.1 number
how questions mostly refer to numbers. they are detected by checking for other words like many  far  long  deep  fast  hot  etc. for example  how many always refers to a count.
1.1 organization
what  which and name with other words like team  league  organization  institution  school  college  etc.  are assigned to this class.
1.1 entity
entity types are detected mostly with the specific entities like colour  creature  product etc. that occur along with what  which and name questions.
　we built these rules using past years questions. we also used wordnet and wikipedia to associate entities with their categories. for example  town is assigned to the class location - city using wordnet hypernym.
1 question focus
in the absence of an expected answer type we need some focus to be determined for retrieving the correct answer. for us the question focus is the word that is associated with the question phrase like what  when  etc. generally it is the noun phrase table 1: coarse and fine-grained classes
coarsefineabbrabbreviationexpansiondatedate weekmonth yearotherentyanimal color event language movie plantbody currency
food
letter
music positionbook
disease
instr medicine physt prizeproductreligionsongsportsubstancesymboltvshow wordurlvehiclejobjobtitleloccity othercountry stategeonumage duration phonecount
money
sizedistance percent speedtemptimevolumeweightziporgorganizationperindividualnametitletitleattached to the question phrase  what  when etc.  . for example  for the question what part of the soldiers' anatomy reminded the indians of the buffalo   the focus is part. there was no rule to classify this question  but using question focus  we could classify this as enty:part. we found this module to succeed in a few cases where the question classification failed. this module only fired if the classification didn't return any results.
　the question processing step gives as its output the list of all keywords  the list of significant keywords and the expected answer type. the significant keywords and the expected answer type are the extra semantic information that have been extracted to aid in the answer retrieval.
1 answer retrieval module
in this module given a question and a set of documents  we retrieve the exact answer to the question from the given documents. it is possible that there is no answer in the given document collection for a given question. in this section we enumerate the steps to getting the answer to a question.
1 document retrieval module
the first step is the information retrieval task. from the set of all given documents we identify the top relevant documents for a given question. the document retrieval module enables this in a fast and efficient manner.
1.1 indexing
the goal of storing an index is to optimize the speed and performance of finding relevant documents for a search query. without an index  the search would scan every document in the corpus  which would take a considerable amount of time and computing power. for example  an index of 1 documents can be queried within milliseconds  where a raw scan of 1 documents could take hours. the trade off for the time saved during retrieval is that additional storage is required to store the index and that it takes a considerable amount of time to update. lucene is a free and open source information retrieval library  originally implemented in java. it is suitable for any application which requires full text indexing and searching capability. we indexed the complete trec 1 qa data collection using lucene.
1.1 full-text searching
the document retrieval module identifies the documents or paragraphs in the document set that are likely to contain the answer. using the keywords found by the question processing module we retrieve the relevant documents. we give more weightage to significant keywords determined by the question processing module in ranking the retrieved documents. we keep only the top n ranked documents for a given query.
1 exact answer selection
in this module given the relevant documents we now select the exact answer to a given question. we implemented four separate ways of doing this.
1.1 semantic type recognition
the semantic type recognizer extracts the answer based on the expected answer type.
　example: in what year was the prius concept car introduced   ques. 1  has answer 1  date type 
　who won the 1 snooker world championship   ques. 1  has answer shaun murphy  person type 
　we also built a number identifier which is able to identify any number present in a sentence. for example 1  one hundred  or one million.
　example: how much wine does australia export to the u.s.   ques. 1  has answer 1 million  number 
　the semantic type matching is based on a named entity recognizer which extracts all the named entities from the answer text.
　we have used statistical model based named entity recognizer  ner  which is trained using newswire training set. ner seeks to locate and classify elements in text into predefined categories person  location  organization  number  title  jobtitle  date  money  etc.
　the first step in answer extraction is to get the most relevant sentence that is likely to contain the answer. two factors are considered for ranking the sentences: number of keywords occurring in the sentence  and whether the sentence contains the same answer type as the question. the sentences are scored using tf/idf . the tf/idf score was scaled by the count of the query terms that appear within the sentence. each sentence  sentence of the candidate articles  is scored. once the most significant sentence has been found  the named entity with the correct answer type is selected as the answer to the given question. the tf/idf weight  term frequency inverse document frequency  is a weight used in information retrieval and text mining. a high weight in tf/idf is reached by a high term frequency  in the given document  and a low document frequency of the term in the whole collection of documents  the weight hence tends to filter out common terms.
1.1 text pattern matching
some questions are difficult to answer using semantic type based strategy. we developed simple patterns for answering such questions. specifically  we developed patterns for acronym expansion questions  date of birth questions and location questions. these patterns are derived from the answers to the questions of previous years trec data. we extracted about hundred patterns for these three class of questions. this approach extracts answers from the surface structure of the retrieved documents by relying on an extensive list of patterns . although building extensive lists of such patterns is time consuming  this approach has high precision. the approach is based on the assumption that answers can be identified by their correspondence to patterns describing the structure of strings carrying certain semantics. these patterns  are like regular expressions.
　example: what does the abbreviation cspi stand for   ques. 1 
　the text contains  former north carolina basketball coach dean smith  former nebraska football coach tom osborne  1 university presidents  the american medical association and the center for science in the public interest  cspi  have not....  the pattern matcher for this is based on matching the first letters of adjacent words
example: which year was mozart born 
　the string  mozart  1   contains answers to the questions about mozart's birth and death years  allowing construction of the pattern:  capitalized word; parenthesis; four digits; dash; four digits; parenthesis  .
1.1 semantic class recognition
for many of the questions the question processing module is not able to return the expected answer type. for such questions we exploited knowledge about hypernym relationship in wordnet. wordnet is the well-known english ontology freely available on the web and covers the vast majority of nouns  verbs  adjectives  and adverbs from the english language. for questions asking about certain categories such as animal  disease  plant  color  etc.  we evaluated each noun or noun phrase in the sentence using knowledge about hypernym relationship in wordnet. for example  in what us state was barack obama born   ques. 1  the answer type is state. for each candidate answer  we used wordnet to check whether state is one of its hypernyms. hawaii has state as it's hypernym  and hence it is chosen as final answer.
1.1 external resource
we also implemented a module for retrieving answers from the wikipedia  infobox. we compared keywords from the question with infobox entries to retrieve the exact answer. we use the  target  as the basic element to retrieve the wikipedia page  and then use its infobox to get specific answers.
1 experimental	setup	and results
we used the data available for participants of the qa track of the 1 trec conference. this data includes aquaint news-wire data and blog data. the questions in trec 1 are grouped by topic. the competition consisted of 1 topics  with a total of 1 questions. these questions are divided into three different types: factoid  list and other. factoid questions require a single fact as answer. lists asks for a list of answers and other is answered by giving any additional information about the topic. there are 1 factoid questions in the question set.
1 question processing
as detailed in section 1 we obtained the keywords  and their significance. we also obtained the expected answer type for each question. to obtain the expected answer type we made rules based on the previous years' trec questions. 1% of the trec1 questions were classified out which 1% were correct.
this year 1%  1  of all the 1 factoid and list questions could be classified and the classifier accuracy stood at 1%  1 correct classifications . our rule-based classifier produces better results compared with machine-learning based classifiers. for example  the machine-learning classifier by li and roth  gives an accuracy of only about 1 percent with fine-grained classes.
1 answering factoid questions
factoid questions have only one correct answer. using the keywords extracted we select the top 1 documents for a question. next we select the relevant sentences based on keyword matching. these sentences are ranked and the best phrase  with the highest score and matching the expected answer type  is returned as answer to a factoid question. we combined the answers returned by the exact answer selection modules in a weighted manner. we got an accuracy value of 1 in factoid questions which is higher than the median score which is 1. we got 1% correct answers from blog data and rest 1% correct answers from aquaint data. it was expected to get more correct answers from newswire data as we trained and tested our system on newswire data.
1 answering list questions
list questions are similar to factoid question  except there is more than one correct and distinct answer to the question. by analyzing candidate answers produced by an existing question answering system  we can have multiple answers for a question. instead of giving single answer  the system will produce a list of top k candidate answers. the average f score over 1 list questions of 1.
1 answering other questions
we observe that target followed by vb  verbphrase  is a good candidate for giving interesting information about the target. sentences those are having numbers/date  can be considered as priority candidates for answering other question. the average pyramid f score over 1 'other' questions is 1 for our system which is higher than median score of 1.
1 conclusion	and	future work
we believe there is a lot of promise in the semantics based approach that we followed this year. our system performed reasonably well in the trec 1 qa task. our scores were well above the median.
table 1: results of our system compared to other systems
run1run1run1medfactoid
accuracy1111incorrect11unsupported11inexact11locally correct11globally correct11   list avg f score1111other
pyr f score1111　average per-ser score1111　we observed that we got more answers from the newswire data than the blog data. this could be because we didn't have any modules in place to take care of the informal nature of blog text.
　our qa system is still in the development stage. some of the subsystems had not been fully tested before the trec experiments due to time constraints. we are continuing our effort to develop and improve the system.
