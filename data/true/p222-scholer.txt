compression reduces both the size of indexes and the time needed to evaluate queries. in this paper  we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms  considering two approaches to improving retrieval efficiency: better implementation and better choice of integer compression schemes. first  we propose several simple optimisations to well-known integer compression schemes  and show experimentally that these lead to significant reductions in time. second  we explore the impact of choice of compression scheme on retrieval efficiency.
　in experiments on large collections of data  we show two surprising results: use of simple byte-aligned codes halves the query evaluation time compared to the most compact golomb-rice bitwise compression schemes; and  even when an index fits entirely in memory  byte-aligned codes result in faster query evaluation than does an uncompressed index  emphasising that the cost of transferring data from memory to the cpu cache is less for an appropriately compressed index than for an uncompressed index. moreover  byte-aligned schemes have only a modest space overhead: the most compact schemes result in indexes that are around 1% of the size of the collection  while a byte-aligned scheme is around 1%. we conclude that fast byte-aligned codes should be used to store integers in inverted lists.
categories and subject descriptors
h.1  information storage and retrieval : systems and software performance evaluation  efficiency and effectiveness ; e.1  data : coding and information theory data compaction and compression
general terms
performance  algorithms  design
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  tampere  finland.
copyright 1 acm 1-1/1 ...$1.
keywords
inverted indexes  retrieval efficiency  index compression  integer coding
1. introduction
　search engines have demanding performance requirements. users expect fast answers to queries  many queries must be processed per second  and the quantity of data that must be searched in response to each query is staggering. the demands continue to grow: the google search engine  for example  indexed around one billion documents a year ago and now manages more than double that figure1. moreover  the increasing availability and affordability of large storage devices suggests that the amount of data stored online will continue to grow.
　inverted indexes are used to evaluate queries in all practical search engines . compression of these indexes has three major benefits for performance. first  a compressed index requires less storage space. second  compressed data makes better use of the available communication bandwidth; more information can be transfered per second than when the data is uncompressed. for fast decompression schemes  the total time cost of transfering compressed data and subsequently decompressing is potentially much less than the cost of transferring uncompressed data. third  compression increases the likelihood that the part of the index required to evaluate a query is already cached in memory  thus entirely avoiding a disk access. thus index compression can reduce costs in retrieval systems.
　we have found that an uncompressed inverted index that stores the location of the indexed words in web documents typically consumes more than 1% of the space required to store the uncompressed collection of documents.  web documents often include a great deal of information that is not indexed  such as html tags; in the trec web data  which we use in our experiments  on average around half of each document is indexable text.  when the index is compressed  the index size is reduced to between 1%-1% of that required to store the uncompressed collection; this size includes document numbers  in-document frequencies  and word positions within documents. if the index is too large to fit entirely within main memory  then querying the uncompressed index is slower: as we show later  it is up to twice as slow as the fastest compressed scheme.
in this paper  we revisit compression schemes for the in-

1
see http://www.google.com/
verted list component of inverted indexes. we also propose a new method for decoding lists. there have been a great many reports of experiments on compression of indexes with bitwise compression schemes  1  1  1  1  1   which use an integral number of bits to represent each integer  usually with no restriction on the alignment of the integers to byte or machine-word boundaries. we consider several aspects of these schemes: how to decode bitwise representations of integers efficiently; how to minimise the operations required for the most compact scheme  golomb coding; and the relative performance of elias gamma coding  elias delta coding  golomb coding  and rice coding for storing indexes.
　we question whether bitwise compression schemes are the best choice for storing lists of integers. as an alternative  we consider bytewise integer compression schemes  which require that each integer is stored in an integral number of blocks  where each block is eight bits. the length of each stored integer can therefore be measured in an exact number of bytes. an additional restriction is to require that these eight-bit blocks must align to machine-word or byte boundaries. we propose and experimentally investigate several variations of bytewise schemes.
　we investigate the performance of different index compression schemes through experiments on large query sets and collections of web documents. we report two surprising results.
  for a 1 gigabyte collection  where the index is several times larger than main memory  optimised bytewise schemes more than halve the average decoding time compared to the fastest bitwise approach.   for a much smaller collection  where the index fits in main memory  a bytewise compressed index can still be processed faster than an uncompressed index.
these results show that effective use of communication bandwidths is important for not only disk-to-memory transfers but also memory-to-cache transfers. the only disadvantage of bytewise compressed indexes is that they are up to 1% larger than bitwise compressed indexes; the smallest bitwise index is around 1% of the uncompressed collection size  while the bytewise index is around 1%.
1. inverted indexes
　an inverted index consists of two major components: the vocabulary of terms-for example the words-from the collection  and inverted lists  which are vectors that contain information about the occurrence of the terms .
　in a basic implementation  for each term t there is an inverted list that contains postings   fd t d   where fd t is the frequency f of term t in the ordinal document d. one posting is stored in the list for each document that contains the term t. inverted lists of this form-along with additional statistics such as the document length ld  and ft  the number of documents that contain the term t-are sufficient to support ranked and boolean query modes.
　to support phrase querying or proximity querying  additional information must be kept in the inverted lists. thus inverted list postings should be of the form
  fd t d  o1 d t ...ofd t d t   
the additional information is the list of offsets o; one offset is stored for each of the fd t occurrences of term t in document d. postings in inverted lists are usually ordered by increasing d  and the offsets likewise ordered within the postings by increasing o. this has the benefit that differences between values-rather than the raw values-can be stored  improving the compressibility of the lists.
　other arrangements of the postings in lists are useful when lists are not necessarily completely processed in response to a query. for example  in frequency-sorted indexes  1  1  postings are ordered by fd t  and in impact-ordered indexes the postings are ordered by quantised weights . these approaches also rely on compression to help achieve efficiency gains  and the improvements to compression performance we describe in this paper are as applicable to these methods as they are to the simple index representations we use as a testbed for our compression methods.
　consider an example inverted list with offsets for the term  matthew :
  1  1 1     1     1  1   
in this index  the terms are words  the offsets are word positions within the documents  and the lists are ordered by d. this inverted list states that the term  matthew  occurs 1 times in document 1  at offsets 1  1  and 1. it also occurs once in document 1 at offset 1  and twice in document 1  at offsets 1 and 1.
　ranked queries can be answered using the inverted index as follows. first  the terms in the user's query are located in the inverted index vocabulary. second  the corresponding inverted lists for each term are retrieved from disk  and then processed by decreasing ft. third  for each posting in each inverted list  an accumulator weight ad is increased; the magnitude of the increase is dependent on the similarity measure used  and can consider the weight wq t of term t in the query q  the weight wd t of the term t in the document d  and other factors. fourth  after processing part  1  1  or all of the lists  the accumulator scores are partially sorted to identify the most similar documents. last  for a typical search engine  document summaries of the top ten documents are generated or retrieved and shown to the user. the offsets stored in each inverted list posting are not used in ranked query processing.
　phrase queries require offsets and that a given sequence of words be contiguous in a matching document. for example  consider a combined ranked and phrase query:
 matthew richardson  richmond
to evaluate such a query  the same first two steps as for ranked querying are applied. then  instead of accumulating weights  it is necessary to construct a temporary inverted list for the phrase  by fetching the inverted list of each of the individual terms and combining them. if the inverted list for  matthew  is as above and the inverted list for  richardson  is
  1      1  1      1   
then both words occur in document 1 and as an ordered pair. only the word  richardson  is in document 1  both words occur in document 1 but not as a pair  and only  matthew  occurs in document 1. the list for  matthew richardson  is therefore
  1   
after this  the ranking process is continued from the third step  where the list for the term  richmond  and the newly created list are used to adjust accumulator weights. phrase queries can involve more than two words.
1. compressing inverted indexes
　special-purpose integer compression schemes offer both fast decoding and compact storage of inverted lists  1  1 . in this section  we consider how inverted lists are compressed and stored on disk. we limit our discussions here to the special-purpose integer compression techniques that have previously been shown to be suitable for index compression  and focus on their use in increasing the speed of retrieval systems.
　without compression  the time cost of retrieving inverted lists is the sum of the time taken to seek for and then retrieve the inverted lists from disk into memory  and the time taken to transfer the lists from memory into the cpu cache before they are processed. the speed of access to compressed inverted lists is determined by two factors: first  the computational requirements for decoding the compressed data and  second  the time required to seek for and retrieve the compressed data from disk and to transfer it to the cpu cache before it is decoded. for a compression scheme to allow faster access to inverted lists  the total retrieval time and cpu processing costs should be less than the retrieval time of the uncompressed representation. however  a third factor makes compression attractive even if cpu processing costs exceed the saving in disk transfer time: compressing inverted lists increases the number of lists that can be cached in memory between queries  so that in the context of a stream of queries use of compression reduces the number of disk accesses. it is therefore important that a compression scheme be efficient in both decompression cpu costs and space requirements.
　there are two general classes of compression scheme that are appropriate for storing inverted lists. variable-bit or bitwise schemes store integers in an integral number of bits. well-known bitwise schemes include elias gamma and delta coding  and golomb-rice coding . bytewise schemes store an integer in an integral number of blocks  where a block is eight bits in size; we distinguish between blocks and bytes here  since there is no implied restriction that a block must align to a physical byte-boundary. a simple bytewise scheme is variable-byte coding  1  1 ; uncompressed integers are also stored in an integral number of blocks  but we do not define them as bytewise schemes since  on most architectures  an integer has a fixed-size representation of four bytes. in detail  these schemes are as follows.
　elias coding  is a non-parameterised bitwise method of coding integers.  non-parameterised methods use static or fixed codes to store integers.  the elias gamma code represents a positive integer k by 1 + stored as a unary code  followed by the binary representation of k without its most significant bit. using elias gamma coding  small integers are compactly represented; in particular  the integer 1 is represented as a single 1-bit. gamma coding is relatively inefficient for storing integers larger than 1 .
　elias delta codes are suited to coding larger integers  but are inefficient for small values. for an integer k  a delta code stores the gamma code representation of 1 +  and then the binary representation of k without its most significant bit.
　golomb-rice bitwise coding  has been shown to offer more compact storage of integers and faster retrieval than the elias codes ; indeed  it is bitwise optimal under the assumption that the set of documents with a given term is random. the codes are adapted to per-term likelihoods via a parameter that is used to determine the code emitted for an integer. in many cases  this parameter must be stored separately using  for example  an elias code. for coding of inverted lists  a single parameter is used for all document numbers in a postings list  but each posting requires a parameter for its offsets. the parameters can be calculated as the lists are decoded using statistics stored in memory and in the lists  as we discuss later.
　coding of an integer k using golomb codes with respect to a parameter b is as follows. the code that is emitted is in two parts: first  the unary code of a quotient q is emitted  where + 1; second  a binary code is emitted for the remainder r  where r = k1. the number of bits required to store the remainder is eitheror
. to retrieve the remainder  the value of the  toggle point is required  where  indicates a left-shift operation. after retrieving  bits of the remainder r  the remainder is compared to t. if r   t  then one additional bit of the remainder must be retrieved. it is generally thought that caching calculated values of log1 b is necessary for fast decoding  with a main-memory penalty of having to store the values. however  as we show later  when the standard log library function is replaced with a fast bit-shifting version  caching is unnecessary.
　rice coding is a variant of golomb coding where the value of b is restricted to be a power of 1. the advantage of this restriction is that there is no  toggle point  calculation required  that is  the remainder is always stored in exactly  bits. the disadvantage of this scheme is that the choice of value for b is restricted and  therefore  the compression is slightly less effective than that of golomb coding. for compression of inverted lists  a value of b is required. witten et al.  report that for cases where the probability of any particular integer value occurring is small-which is the usual case for document numbers d and offsets o-then b can be calculated as: b = 1 〜 mean k 
for each inverted list  the mean value of document numbers d can be approximated as k = n/ft where n is the number of documents in the collection and ft is the number of postings in the inverted list for term t . this approach can also be extended to offsets: the mean value of offsets o for an inverted list posting can be approximated as k = ld/fd t where ld is the length of document d and fd t is the number of offsets of term t within that document. as the statistics n  ft  and l are often available in memory  or in a simple auxiliary structure on disk  storage of b values is not required for decoding; approximate values of l can be stored in memory for compactness   but use of approximate values has little effect on compression effectiveness as it leads to only small relative errors in computation of b.
　in bytewise coding an integer is stored in an integral number of eight-bit blocks. for variable-byte codes  seven bits in each block are used to store a binary representation of the integer k. the remaining bit is used to indicate whether the current block is the final block for k  or whether an additional block follows. consider an example of an integer k in the range of 1 = 1 to 1 = 1. two blocks are required to represent this integer: the first block contains the seven least-significant bits of the integer and the eighth bit is used to flag that another block follows; the second block contains the remaining most-significant bits and the eighth bit flags that no further blocks follow. we use the convention that the flag bit is set to 1 in the final block and 1 otherwise.
　compressing an inverted index  then  involves choosing compression schemes for the three kinds of data that are stored in a posting: a document number d  an in-document frequency fd t  and a sequence of offsets o. a standard choice is to use golomb codes for document numbers  gamma codes for frequencies  and delta codes for offsets .  we explore the properties of this choice later.  in this paper  we describe such a choice as a gold-gamf-delo index.
1 fast decoding
　we experiment with compression of inverted lists of postings that contain frequencies fd t  documents numbers d  and offsets o. for fast decompression of these postings  there are two important considerations: first  the choice of compression scheme for each component of the posting; and  second  modifications to each compression scheme so that it is both fast and compatible with the schemes used for the other components. in this section  we outline the optimisations we use for fast decompression. our code is publically available and distributed under the gnu public licence.1
bitwise compression
we have experimented with a range of variations of bitwise decompression schemes. williams and zobel  reported results for several efficient schemes  where vectors that contain compressed integers are retrieved from disk and subsequently decoded.1 in their approach  vector decoding uses bitwise shift operations  bit masks  multiplication  subtraction  and function calls to retrieve sequences of bits that span byte boundaries. in our experiments on intel pentiumbased servers running the linux operating system  we have found that bitwise shift operations are usually faster than bit masks  and that the function calls are slow. by optimising our code to use bitwise shifts and to remove nested function calls  we have found that the overall time to decode vectors-regardless of the compression scheme used-is on average around 1% of that using the code of williams and zobel.
　other optimisations that are specific to golomb-rice coding are also of value. golomb-rice decoding requires that log1 b is calculated to determine the number of remainder bits to be retrieved. it is practicable to explicitly cache values of log1 b in a hash table as they are calculated  or to pre-calculate all likely-to-be-used values as the retrieval query engine is initialised. this saves recalculation of logarithms when a value of b is reused in later processing  with the penalty of additional memory requirements for storing the lookup table.
we measured the performance of golomb coding with and

1
 the search engine used in these experiments and our integer compression code is available from http://www.seg.rmit.edu.au/
1
 the code used by williams and zobel in their experiments is available from http://www.cs.rmit.edu.au/ ~hugh/software/
without caching. timings are average elapsed query evaluation cost to process index information for 1 queries on a 1 gigabyte  gb  collection of web data  using our prototype retrieval engine on a gold-gamf-golo index  that is  golomb document numbers  gamma frequencies  golomb offsets ; we discuss collection statistics and experimental design further in section 1. the cache lookup table size is unrestricted.
　we found that  without caching of log1 b values  the average query evaluation time is 1 seconds. caching of log1 b values as they are calculated during query processing roughly halves the average query evaluation time  to 1 seconds. pre-calculating and storing the values offers almost no benefit over caching during query processing  reducing the time to 1 seconds; this reflects that only limited b values are required during query evaluation. caching of toggle points yields 1 seconds. as toggle points are calculated using bitwise shifts  addition  and subtraction  this is further evidence that bitwise shifts are inexpensive on our hardware.
　an alternative approach to managing log computations is to replace the standard library log function with a loop that determines  using bitwise shifts and equality tests; the logarithm value can be determined by locating the position of the most-significant 1-bit in b. we found that this led to slight additional improvements in the speed of decoding golomb codes  outperforming explicit caching. all golomb-rice coding results reported in this paper are computed in this way.
bytewise compression
we have experimented with improvements to variable-byte coding. unlike in bitwise coding  we have found that masking and shifting are equally as fast because of the large number of shifts required. we use shifts in our experiments.
　perhaps the most obvious way to increase the speed of variable-byte decoding is to align the eight-bit blocks to byte boundaries. alignment with byte boundaries limits the decoding to only one option: the flag bit indicating if this is the last byte in the integer is always the most significant bit  and the remaining seven bits contain the value. without byte alignment  additional conditional tests and operations are required to extract the flag bit  and the seven-bit value can span byte boundaries. we would expect that byte alignment would improve the speed of decoding variable-byte integers. figure 1 shows the effect of byte alignment of variablebyte integers. in this experiment  variable-byte coding is used to store the offsets o in each inverted list posting. the optimised golomb coding scheme described in the previous section is used to code document numbers d and elias gamma coding is used to store the frequencies fd t. we refer to this as a gold-gamf-vbyo index.
　the graph at the left of figure 1 shows total index size as a percentage of the uncompressed collection being indexed. the first bar shows that  without byte alignment  the gold-gamf-vbyo index requires almost 1% of the space required by the collection. the second bar shows that padding to byte alignment after storing the gamma-coded fd t values increases the space requirement to just over 1% of the collection size. we discuss the other schemes in this figure later in this section.
　the graph at the right of figure 1 shows elapsed query evaluation times using different index designs. timings are

figure 1: variable-byte schemes for compressing offsets in inverted lists in a gold-gamf-vbyo index. four different compression schemes are shown and  for each  both original and scanning decoding are shown. scanning decoding can be used when offsets are not needed for query resolution.
the average elapsed query evaluation cost to process the inverted lists for 1 queries on a 1 gb collection of web  data using our prototype retrieval engine. queries are processed as conjunctive boolean queries. the first bar shows that the average time is around 1 seconds for the gold-gamf-vbyo index without byte alignment. the second bar shows that the effect of byte alignment is a 1% reduction in average query time. therefore  despite the small additional space requirement  byte-alignment is beneficial when storing variable-byte integers.
　a second optimisation to variable-byte coding is to consider the query mode when processing the index. for querying that does not use offsets-such as ranked and boolean querying-decoding of the offsets in each posting is unnecessary. rather  all that is required are the document numbers d and document frequencies fd t. an optimisation is therefore to only examine the flag bit of each block and to ignore the remaining seven bits that contain the value. the value of fd t indicates the number of offsets o stored in the posting. by examining flag bits until fd t 1-bits are processed  it is possible to bypass the offsets with minimal processing. we call this approach scanning.
　scanning can also be used in query modes that do require offset decoding. as we discussed earlier  phrase querying requires that all terms are present in a matching document. after processing the inverted list for the first term that is evaluated in a phrase query  a temporary inverted list of postings is created. this temporary list has a set d of documents that contain the first term. when processing the second term in the query  a second set of document numbers d are processed. offsets for the posting associated with document can be scanned  that is  passed over without decoding  if d is not a member of d.  at the same time  document numbers in d that are not in d are discarded. 
　we show the performance of scanning in figure 1. the fifth and sixth bars show how scanning affects query evaluation time for variable-bytes that are either unaligned and aligned to byte boundaries in the gold-gamf-vbyo index. scanning removes the processing of seven-bit values. this reduces the cost of retrieving unaligned variable-bytes to less than that of the aligned variable-byte schemes; the small speed advantage is due to the retrieval of smaller lists in the unaligned version. scanning has little effect on byte-aligned variable bytes  reflecting that the processing of seven-bit values using shift operations has a low cost. overall  however  byte-alignment is preferred since the decoding cost of offsets is expensive in an unaligned scheme.
　a third optimisation is an approach we call signature blocks  which are a variant of skipping. skipping is the approach of storing additional integers in inverted lists that indicate how much data can be skipped without any processing . skipping has the disadvantage of an additional storage space requirement  but has been shown to offer substantial speed improvements . a signature block is an eight-bit block that stores the flag bits of up to eight blocks that follow. for example  a signature block with the bitstring 1 represents that five integers are stored in the eight following eight-bit blocks: the string 1 represents that the first three blocks store one integer each; the string 1 represents that the fourth integer is stored over three blocks; and  the string 1 represents that the final integer is stored over two blocks. as all flag bits are stored in the signature block  the following blocks use all eight bits to store values  rather the seven-bit scheme in the standard variable-byte integer representation.
　the primary use of signature blocks is skipping. to skip offsets  fd t offset values must be retrieved but not processed. by counting the number of 1-bits in a signature block  the number of integers stored in the next eight blocks can be determined. if the value of fd t exceeds this  then a second or subsequent signature block is processed until fd t offsets have been skipped. the last signature block is  on average  half full. we have found that bitwise shifts are faster than a lookup table for processing of signature blocks.
　the speed and space requirements are also shown in figure 1. not surprisingly  the signature block scheme requires more space than the previous variable-byte schemes. this space requirement is further increased if byte alignment of blocks is enforced. in terms of speed  the third and fourth bars in the right-hand histogram show that signature blocks are slower than the original variable-byte schemes when offsets are processed in the gold-gamf-vbyo index. these results are not surprising: signature blocks are slow to process when they are unaligned  and the byte-aligned version is slow because processing costs are no less than the original variable-byte schemes and longer disk reads are required.
　as shown by the seventh bar  when offsets are skipped the unaligned signature block scheme is slower than the original variable-byte scheme. the savings of skipping with signature blocks are negated by more complex processing when blocks are not byte-aligned. in contrast  the right-most bar shows that the byte-aligned signature block scheme with skipping is slightly faster on average than all other schemes. however  we conclude-given the compactness of the index and good overall performance-that the best all-round scheme is the original variable-byte scheme with byte alignment. therefore  all variable-byte results reported in the section 1 use the original byte-aligned variable-byte scheme with scanning.
customised compression
combinations of bitwise and bytewise compression schemes are also possible. the aim of such approaches is to combine the fast decoding of bytewise schemes with the compact storage of bitwise schemes. for example  a simple and efficient custom scheme is to store a single bit that indicates which of two compression schemes is used  and then to store the integer using the designated compression scheme. we have experimented with several approaches for storing offsets. the simplest and most efficient approach we tested is as follows: when fd t = 1  we store a single bit indicating whether the following offset is stored as a bitwise elias delta code or as a bytewise eight-bit binary representation. when storing values  we use elias delta coding if the value is greater than 1 and the binary scheme otherwise. this scheme has the potential to reduce space because in the median posting fd t is 1 and the average offset is around 1. selective use of a fixed-width representation can save storage of the 1-bit prefix used to indicate magnitude in the corresponding delta code.
　we report the results with this scheme  which we call custom  in the next section. this was the fastest custom scheme we tested. other approaches we tried included switching between variable-byte and bitwise schemes  using the custom scheme when fd t is either 1 or 1  and other simple variations. we omit results for these less successful approaches.
1. results
　all experiments described in this paper are carried out on an intel pentium iii based machine with 1 mb of mainmemory running the linux operating system. other processes and disk activity was minimised during timing experiments  that is  the machine was under light-load.
　a theme throughout these experiments and greatly impacting on the results is the importance of caching. on a modern machine  caching takes place at two levels. one level is the caching of recently-accessed disk blocks in memory  a process that is managed by the operating system. when the size of the index significantly exceeds memory capacity  to make space to fetch a new inverted list  the blocks containing material that has not been accessed for a while must be discarded. one of the main benefits of compression is that a much greater volume of index information can be cached in memory. for this reason  we test our compression schemes with streams of 1 or 1 queries extracted from a query log   where the frequency distribution of query terms leads to beneficial use of caching. again  queries are processed as conjunctive boolean queries.
　the other level at which caching takes place is the retention in the cpu cache of small blocks of data  typically of 1 bytes  recently accessed from memory. cpu caching

figure 1: performance of integer compression schemes for offsets in inverted lists  in an index with golomb document numbers and gamma frequencies. in this experiment  the index fits in main memory. a 1 mb collection is used  and results are averaged over 1 queries.
is managed in hardware. in current desktop computers  as many as 1 instruction cycles are required to fetch a single machine-word into the cpu. at a coarser level  compression of postings lists means that the number of fetches from memory to cache during decompression is halved.
small collection
figure 1 shows the relative performance of the integer compression schemes we have described for storing offsets  on a 1 mb collection of 1 web documents drawn from the trec web track data ; timing results are averaged over 1 queries drawn from an excite search engine query log . the index contains 1 terms.
　these results show the effect of varying the coding scheme used for document numbers d  frequencies fd t  and offsets o. in all cases where both bitwise and variable-byte codes are used  the bitwise codes are padded to a byte boundary before a variable-byte code is emitted; thus  for example  in a goldgamf-vbyo index  there is padding between the gamma frequency and the sequence of variable-byte offsets. not all code combinations are shown; for example  given that the speed advantage of using variable-byte document numbers is small  we have not reported results for index types such as vbyd-gamf-ricd  and due to the use of padding a choice such as vbyd-gamf-vbyd. given the highly skew distribution of fd t values  golomb or rice are not suitable coding methods  so these have not been tried.
　in the  no compression  case  fixed-width fields are used to store postings. document numbers are stored in 1 bits  frequencies in 1 bits  and offsets in 1 bits; these were the smallest multiples of bytes that would not overflow for reasonable assumptions about data properties.
　the relative performance of elias delta and gamma  rice  and golomb coding is as expected. the non-parameterised elias coding schemes result in larger indexes than the paramterised golomb-rice schemes that  in turn  result in slower query evaluation. the average difference between offsets is greater than 1  making elias delta coding more appropriate overall than gamma coding; the latter is both slower and less space-efficient.
　on the lower graph in figure 1  comparing the fourth and fifth columns and comparing the fifth and eighth columns  it can be seen that choice of golomb or rice codes for either offsets or document numbers has virtually no impact on index size. comparing the fifth and eighth columns on the upper graph  the schemes yield similar decoding times for document numbers. however  rice codes are markedly faster for decoding offsets  because no toggle point calculation is required. among the bitwise schemes  we conclude that rice coding should be used in preference to other schemes for coding document numbers and offsets.
　the most surprising result is the effect of using the optimised byte-boundary variable-byte scheme for coding offsets. despite the variable-byte index being 1% larger than the corresponding rice-coded index  the overall query evaluation time is 1% less. further speed gains are given by coding all values in variable-byte codes. indeed  variable-byte decoding is faster even than processing uncompressed lists. this result is remarkable: the cost of transfering variablebyte coded lists from memory to the cpu cache and then decoding the lists is less than the cost of transferring uncompressed lists. to our knowledge  this is the first practical illustration that compression improves the efficiency of an in-memory retrieval system. we conclude from this that variable-byte coding should be used to store offsets to reduce both disk retrieval and memory retrieval costs.
　in experiments with integers  williams and zobel found that variable-byte coding is faster than the bitwise schemes for storing large integers of the magnitude stored in inverted lists . our result confirms this observation for retrieval systems  while also showing that the effect extends to fast retrieval from memory and that improvements to variablebyte coding can considerably increase decoding speed.
　the custom scheme uses both elias delta and a binary bytewise scheme  reducing query evaluation to around 1% of the time for the elias delta scheme. however  the custom scheme is almost twice as slow as the variable-byte scheme and  therefore  has little benefit in practice.
large collection
figure 1 shows the results of a larger experiment with an index that does not fit within the main-memory of our ma-

figure 1: the performance of integer compression schemes for compressing offsets in inverted lists  with golomb-coded document numbers and gammacoded offsets. in this experiment  the index is several times larger than main memory. a 1 gb collection is used  and results are averaged over 1 queries.
chine. exactly the same index types are tried as for the experiment above. a 1 gb collection of 1 1 web documents drawn from the trec web track data  is used and timing results are averaged over 1 boolean queries drawn from an excite search engine query log . the index contains 1 1 terms. we include only selected schemes in our results.
　we again note that we have not used heuristics to reduce query evaluation costs such as frequency-ordering or early termination. indeed  we have not even used stopping; with stopwords removed  query times are greatly impoved. our aim in this research is to measure the impact on index decoding time of different choices of compression method  not to establish new benchmarks for query evaluation time. our improvements to compression techniques could  however  be used in conjunction with the other heuristics  in all likelihood further reducing query evaluation time compared to the best times reported previously.
　the relative speeds of the bitwise golomb  elias delta  and variable-byte coded offset schemes are similar to that of our experiments with the 1 mb collection. again  variablebyte coding results in the fastest query evaluation. perhaps unsurprisingly given the results described above  an uncompressed index that does not fit in main-memory is relatively much slower than the variable-byte scheme; the disk transfer costs are a larger fraction of the overall query cost when the index does not fit in memory  and less use can be made of the memory cache. indexes with variable-byte offsets are twice as fast as indexes with golomb  delta  or gamma offsets  and one-and-a-half times as fast as indexes with rice offsets. vbyd-vbyf-vbyo indexes are twice as fast as any index type with non-variable-byte offsets.
　in separate experiments we have observed that the gains demonstrated by compression continue to increase with collection size  as the proportion of the index that can be held in memory declines. despite the loss in compression with variable-byte coding  indexes are still less than one-seventh of the size of the indexed data  and the efficiency gains are huge.
1. conclusions
　compression of inverted lists can significantly improve the performance of retrieval systems. we have shown that an efficiently implemented variable-byte bytewise scheme results in query evaluation that is twice as fast as more compact bitwise schemes. moreover  we have demonstrated that the cost of transferring data from memory to the cpu cache can also be reduced by compression: when an index fits in main memory  the transfer of compressed data from memory to the cache and subsequent decoding is less than that of transferring uncompressed data. using byte-aligned coding  we have shown that queries can be run more than twice as fast as with bitwise codes  at a small loss of compression efficiency. these are dramatic gains.
　modern computer architectures create opportunities for compression to yield performance advantages. once  the main benefits of compression were to save scarce disk space and computer-to-computer transmission costs. an equally important benefit now is to make use of the fact that the cpu is largely idle. fetching a single byte from memory involves a delay of 1 to 1 cpu cycles; a fetch from disk involves a delay of 1 1 cycles. compression can greatly reduce the number of such accesses  while cpu time that would otherwise be unused can be spent on decoding. with fast decoding  overall costs are much reduced  greatly increasing query evaluation speed. in current computers such architecture considerations are increasingly important to development of new algorithms for query processing. poor caching has been a crucial shortcoming of existing algorithms investigated in this research.
　there are several possible extensions to this work. we plan to investigate nibble-coding  a variant of variable-byte coding where two flag bits are used in each variable-byte block. it is likely that this approach may improve the performance of signature blocks. we will also experiment with phrase querying in practice and to explore the average query evaluation speed when partial scanning is possible.
