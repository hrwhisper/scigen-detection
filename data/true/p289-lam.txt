we propose a novel named entity matching model which considers both semantic and phonetic clues. the matching is formulated as an optimization problem. one major component is a phonetic matching model which exploits similarity at the phoneme level. we investigate three learning algorithms for obtaining the similarity information of basic phoneme units based on training examples. by applying this proposed named entity matching model  we also develop a mining framework for discovering new  unseen named entity translations from online daily web news. this framework harvests comparable news in different languages using an existing bilingual dictionary. it is able to discover new name translations not found in the dictionary.
categories and subject descriptors: h.1  information systems : information storage and retrieval general terms: algorithms  design
keywords: text mining  named entity translation  learning phonetic information
1. introduction
　many existing systems dealing with cross-language documents make use of bilingual dictionaries. in all these systems  a fixed dictionary is used throughout the process implying that only those terms exist in the dictionary can be handled. obviously  these systems encounter difficulties when they process new or unseen terms which are common especially for named entities. a study has shown that many terms in the submitted queries for news search consist of named entities or proper nouns . one promising ap-

 the work described in this paper was substantially supported by grants from the research grant council of the
hong kong special administrative region  china  project nos: cuhk 1e and cuhk 1e  and a grant from the chinese university of hong kong strategic grant  no. 1 .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
proach is to discover new  unseen bilingual name translations automatically.
　translations of many named entities involve both semantic meaning and pronunciation. we investigate a named entity matching model which considers both semantic and phonetic clues given two bilingual entities. the matching is formulated as an optimization problem. one major component in our named entity matching model is a phonetic matching model which exploits phonetic information. in particular  it considers similarity at the phoneme level. we investigate three learning algorithms for obtaining the similarity information of basic phoneme units based on a set of training data.
　by applying this proposed named entity matching model  we also develop a mining framework for discovering new  unseen named entity translations from online daily web news. this framework first harvests comparable news in different languages based on an unsupervised learning technique using an existing bilingual dictionary. it employs automatic named entity extraction and information retrieval techniques. new name translations not found in the existing dictionary can be discovered regularly.
　there have been some related work on extracting term translations. one early work on automatic identification of word translations from nonparallel corpus was presented in . it mainly makes use of co-occurrence statistics. a method called convec was developed to generate bilingual lexicon from comparable corpus . an attempt for mining term translations from web anchor texts has been investigated in . they discovered that anchor texts linking to the same page may contain terms with similar semantics  and that some of them may be written in different languages. however  this approach is restricted to discover those terms appeared in the anchor texts. nie et al. proposed a technique for mining parallel documents from parallel web sites . an alignment model was proposed to generate parallel sentences for cross-lingual information retrieval.
　one major drawback of the above approaches is that they do not consider phonetic information. a similarity-based backward transliteration approach was proposed in  to automatically acquire phonetic similarities. a shortcoming of this approach is that it does not take into account semantic information. however  many named entity translations involve both semantic and phonetic information at the same time.
　a major advantage of our proposed approach over the above existing methods is that our approach analyzes both semantic and phonetic information and formulates the problem as a number of optimization models. the basic phonetic similarity information can be obtained via a learning process. one feature of our mining framework is that comparable news are automatically detected to facilitate new name translation discovery.
1. named entity matching model
1 problem nature
　the objective of our named entity matching model is to compute the similarity between two given named entities written in two languages. note that this is a different problem from cross-language transliteration. crosslanguage transliteration attempts to generate the translation of a term in one language given a term in another language. however  in this named entity matching problem  we attempt to compute a kind of similarity between two given entities in two languages.
　given a pair of named entities which are translation of each other  it is common to find part of the entity is matched based on semantic and the remaining part is based on phonetic clues. for example  consider the english entity  university of akron  and its corresponding translated chinese entity    . if we just adopt the semantic clue  we can match the term  university  with     based on a bilingual dictionary. however   akron  vs     will be missed as they may not be found in a typical dictionary. on the contrary  if we just consider the phonetic clues   akron  can match with     due to the similarity of their pronunciations but  university  and     do not match.
　one issue needs to be addressed for named entities in different languages is the tokenization of the entities. for a given entity in chinese  we need to break it into appropriate tokens to facilitate the matching. for a given entity in english  it is usually composed of separate terms. however  some terms may need to be grouped in order to be effectively mapped to chinese terms phonetically.
　another issue is to consider partial matching. for example  consider the entity  palo alto chamber of commerce  and its corresponding translated chinese entity  
　　　 . after looking up the bilingual dictionary  translations for the english terms  chamber  and  commerce  can be found. in particular   chamber  can match with the chinese word segment     in the chinese entity. one of the translations found for the term  commerce  is    . although this translation cannot be fully matched in the chinese entity  it can be partially mapped to the chinese entity via the term    . other translations of the term  commerce  cannot be mapped to any word segments in the chinese entity at all.
　for phonetic matching  one major issue is to deal with out-of-vocabulary terms which may exist in either language.
1 matching model investigation
　to tackle the above issues  we investigate a named entity matching model which analyzes both semantic and phonetic similarities between different tokens of the named entity pair. the semantic mapping is mainly determined based on the bilingual dictionary. the phonetic similarity is handled by the phonetic matching model described in section 1.
　consider an english entity e represented by terms   t1 ...  tm1   and a chinese entity c represented by chinese characters  s1 ...  sn1  . for each english term ti  the bilingual dictionary is looked up. the current bilingual dictionary we use is derived from the one provided by linguistic data consortium  ldc  with additional entries inserted manually. typically  a set of chinese translations are found in the dictionary for ti. the chinese entity c is scanned to get those word segments which can fully or partially match with any of the chinese translations. therefore  the term ti may map to some chinese word segments. each word segment is composed of consecutive chinese characters. let the matched word segments be represented as
	  where dtj fi	and dtj li	denote
the starting and ending position of the chinese character respectively in c for the j-th matched word segment. in other words   dtj fi  dtj li   corresponds to certain consecutive chinese characters  sk1 sk1+k1 . there is also a weight associated with each word segment reflecting the degree of matching. similarly  an english term may be able to match with certain word segments in the chinese entity c phonetically. for example  the term  alto  can map to the word segment     based on phonetic evidence. in general  for an english term ti  it can match with some word segments in c phonetically. let the phonetically matched word segments
t1t1t1: : be represented as   pt1i f pt1i l    pt1i f pt1i l  ...  where ptj fi and ptj li denote the starting and ending position of the chinese character respectively in c for the j-th matched word segment. there is also a weight associated with each word segment reflecting the degree of phonetic matching. fig. 1 shows a diagram illustrating the modeling. s1	，，，，，，
figure 1: the modeling of the named entity matching
　armed with the above modeling  the objective is to find a set of mapping between english terms and chinese word segments such that the total weight is maximized subject to constraints that each english term can only be mapped at most once and the mapped chinese word segments can-
not be overlapping. let   qfti1  qlti1    qfti1  qlti1  ...  denote a particular solution. each  qftj qltj  can be either  dtk fj  dtk lj   or   . each tj can only appear one time and each word segment represented by q cannot be overlapping.
　in principle  the solution for the maximization problem can be found using an exhaustive search. however  the complexity is very high in practice especially when a large number of english and chinese entity pairs need to be evaluated in a timely fashion as exemplified by the mining framework for discovering new  unseen translations from daily news described in the later part of this paper. after taking all the factors into consideration  we develop a named entity matching model which can address all the issues mentioned above and possesses reasonable computational complexity. this named entity matching model is composed of two tasks. the first task is to conduct tokenization on both english and chinese named entities. the second task is to make use of a hybrid semantic and phonetic matching algorithm which formulates the problem as a bipartite weighted graph problem. these two tasks of the named entity matching model are presented in detail below.
1 tokenization
　the terms appeared in the named entities need to be organized into appropriate tokens to facilitate the matching. the tokenization process of both english and chinese entities is performed based on the bilingual dictionary. consider a pair of english and chinese named entities. each term in the english named entity is looked up in the bilingual dictionary. typically  a set of chinese translations are found the dictionary for a particular english term. the chinese entity is scanned to get those word segments which can maximally match with one of the chinese translations. if the degree of this maximal matching exceeds or reaches a certain threshold θt  the english term as well as those chinese word segments are treated as separate tokens. the degree of matching ρ is defined as the number of matched characters divided by the total number of characters in the corresponding term of the chinese translation. returning to the above example  the term  commerce  matches with the term     with ρ = 1. the english terms  chamber  and  commerce  as well as the chinese segments     and     are treated as separate tokens.
　the next step of the tokenization process is to group adjacent terms which do not involve in the dictionary mapping. these adjacent terms are concatenated to form a single token to facilitate possible mapping due to phonetic similarity. returning to the above example  the english terms  palo  and  alto  will be concatenated to form a token  palo alto . the chinese segment     will be treated as a single token. as a result  the english entity will be broken into four tokens  namely   palo alto    chamber    of   and  commerce  whereas the chinese entity will be broken into three tokens  namely            and    .
1 hybrid semantic and phonetic matching algorithm
　after tokenization  the entities are represented by a sequence of tokens. let the english entity  e  be represented as tokens  e1 ...  em  and the chinese entity  c  be represented as tokens  c1 ...  cn . we can formulate the matching problem via an undirected bipartite weighted graph with vertex set v and edge set l. each token is associated with a graph vertex. the vertex set v is set to {ve “ vc} where ve = {e1 ...  em} and vc = {c1 ...  cn}. if there is a mapping found semantically or phonetically between an english token ei and a chinese token cj  there will be an edge  ei cj  （ l.
　the weight μ ei cj  of the edge is determined by the degree of mapping of the associated tokens. generally  semantic mapping is relatively more reliable than phonetic mapping. the edge construction process starts with considering the semantic mapping using the bilingual dictionary as described in the tokenization process above. if there is a mapping found with sufficient degree  an edge  ei cj  is formed with the weight μ ei cj  set to ρ as defined in the tokenization process. it can be easily shown that θt ＋ ρ ＋ 1. next  we consider phonetic mapping between tokens. for each english token  ei  which does not have semantic mapping with chinese tokens  we compute the phonetic similarity  μ ei cj  between ei and each chinese token  cj. the phonetic similarity is calculated using our phonetic matching model described below. the range of this phonetic similarity value is  1 . if the phonetic similarity is larger than zero  an edge is constructed with the phonetic similarity assigned to the weight of the edge.
　after the edges and weights of the graph have been constructed  the matching problem is reduced to finding a set of edges such that the total weight   is maximized and each token can only be mapped to a single token on the other side. this requirement can be formulated as a bipartite weighted graph matching problem . formal description of the problem is given as follows:
           maximize   =  ei cj （l μ ei cj x ei cj   1  subject to
cj: ei cj （l	i cj  = 1 x e ei （ veej: ej ci （l	j ci  = 1 x e ci （ vc	x ei cj  − 1	  ei cj  （ l
where x ei cj  is a binary variable representing whether the mapping between ei and cj is chosen in the solution.
　to solve the maximization problem  one can formulate it as a minimum cost assignment problem . the minimum cost assignment problem attempts to find a mapping for every token. we conduct a series of transformation on the bipartite graph in order to fulfill the requirement of the cost assignment problem. the first transformation is to remove those tokens which do not associate with any edges in the original bipartite graph. suppose m1 and n1 is the number of vertices for english and chinese tokens respectively after this vertex removal step. then  we construct some dummy vertices to balance the number of vertices between the english and chinese tokens. for example  if m1   n1  we add nk vertices representing dummy chinese tokens such that m1 = n1 + nk. dummy edges with weight zero are then added for connecting each dummy vertex to each vertex on the other side. the next transformation is to convert the weight of each edge μ ei cj  to the cost Φ μ ei cj   where Φ = max ei cj （lμ ei cj . the maximization problem is now turned to a minimization problem. we adopt the hungarian search algorithm  which can solve this minimization problem efficiently. the hungarian algorithm generates all independent sets of the matrix and computes the total costs of each assignment. the optimal solution can be obtained with polynomial time complexity.
　the optimal solution of the minimum cost assignment problem can be easily converted to the optimal solution of the maximization of the total weight   presented in eqn. 1. the total weight is then normalized by the smaller number of tokens between the english and chinese entities  i.e.   /min m n  .
1. phonetic matching model
　the phonetic matching model aims at determining the similarity of two terms or word segments based on pronunciation. the first step is to generate a phonetic representation for each term. then the similarity calculation is performed using a modified longest common subsequence algorithm.
1 generating phonetic representation
　english terms will be processed by a phonetic generation procedure whose purpose is to generate the phonetic representation. we adopt the phonetic representation used in pronlex  a resource provided by ldc. for example  the phonetic representation of the english term  father  is  fadr . to get the phonetic representation of an unseen english term  a letter-to-phoneme tagging lexicon and a set of transformation rules are used. we employ a machine learning technique and conduct a training process to obtain these rules. details of the training process can be found in .
　two popular spoken chinese dialects are mandarin and cantonese. chinese terms pronounced in mandarin and cantonese are converted into pronunciation representation by using pin-yin symbols and jyut ping symbols respectively. to transform mandarin terms  a pin-yin table  provided by ldc  containing the pin-yin representation of most chinese characters is used. an example for pin-yin symbol is   = gang1 . to transform cantonese terms  a jyut ping table  provided by the linguistic society of hong kong  lshk 1  is used. an example for jyut ping symbol is   = baa1 .
　the next step is to segment the phonetic representation into basic phoneme units to facilitate phonetic matching. normally a basic phoneme unit consists of a consonant followed by a vowel. if there is no consonant-vowel pattern  we extract the consonant. if there is no consonant  the vowel will be extracted as a phoneme unit. for example  the english term  beckham  will be transformed to basic phonetic unit sequence  be kx m . the chinese term  
　　　  pronounced in mandarin will be transformed to basic phoneme unit sequence  bei ke han mu . in our current model  there are 1  1 and 1 english  mandarin and cantonese basic phoneme units in total respectively.
1 phonetic matching algorithm
　given an english term and a chinese term both represented as a sequence of basic phoneme units  we wish to calculate a similarity value which indicates how similar their pronunciations are. to achieve this  we first prepare a phoneme pronunciation similarity  pps  table capturing the pronunciation similarity value between each possible english-chinese basic phoneme unit pair. there are two such tables  one for english-mandarin and the second one for english-cantonese matching. two different pps tables are needed due to different pronunciations of the two dialects. the range of the pronunciation similarity values is between 1 and 1  which indicates how similar the two basic phoneme units of different languages pronounce.
　we assigned the pronunciation similarity values in both pps tables manually. the complete english-mandarin and english-cantonese pps tables contain 1 and 1 entries respectively. the pronunciation similarity values of most entries are zero due to completely dissimilar pronunciations. we only retain those entries which exhibit certain similarity resulting in significantly less entries. in particular  the number of entries for english-mandarin and englishcantonese pps tables are 1 and 1 respectively. table 1 depicts some sample entries in the manually prepared english-mandarin pps table.
for english-mandarin pps table  we also investigate sev-

1
the url of lshk is http://cpct1.cityu.edu.hk/lshk.
eral learning algorithms which can determine the similarity values automatically using a set of training data. the details of the learning algorithms are described in section 1.
english-mandarinpronunciationbasic phoneme unit pairsimilarity valuegi - gian1wa - hiao1he - wiang1zu - xu1a - ou1table 1: sample entries in english-mandarin phoneme pronunciation similarity  pps  table with values assigned manually
　suppose an english term  a  is represented by basic phoneme unit sequence  a1 ...  ama . a chinese term  b  is represented by basic phoneme unit sequence  b1 ...  bmb  . the objective of the phonetic matching model is to compute the longest matched subsequence between two phoneme sequences. the mapping must be in the same order  but not necessarily consecutive. this problem can be formulated as finding longest common subsequence  lcs . dynamic programming can be employed to find the optimal solution for lcs efficiently. however  the basic lcs algorithm only considers the presence or absence of mapping between ai and bj. in contrast  in our phonetic matching problem  the matching similarity can take any value between 1 and 1. therefore  we modify the standard dynamic programming to handle real-valued matching similarity. let si j be the similarity score of the optimal longest common subsequence for the sequences  a1 ...ai  and  b1 ...  bj . the corresponding recursive formula is depicted as follows:
	1	when i = 1 or j = 1 
	 	 1 
	 	otherwise
where vai bj represents the pronunciation similarity value of phoneme unit pair involving ai and bj. this similarity value should be found in the pps table. sma mb becomes the matching score of the intended optimal solution. this score is then normalized by the maximum length of the two sequences.
1. learning phonetic similarity
　as mentioned above  the phonetic matching model requires a pps table to determine the optimal mapping subsequence between two phonetic representation. a simple strategy for preparing the pps table is to assign the elements in the table manually. however  it is not easy even for experts to assign good precise values. for english-mandarin pps table  we investigate several learning algorithms for obtaining the similarity values in the pps table using a set of training data. we extracted 1 chinese-english person name pairs from the chinese-english named entity corpus provided by ldc as the training data. all these name pairs are translations of each other. the names are transformed into basic phoneme units through the procedures described in section 1. let ma and mb be the number of phoneme units of the two given names after transformation. consider a pps table v with elements vi j where i and j refer to a specific english and chinese phoneme unit respectively.
consider the k-th pair of names. let uk i j be a binary variable indicating the presence of the phoneme unit pair involving english phoneme unit i and chinese phoneme unit j. the similarity score of the pair of names is proportional to i j vi juk i j. the goal is to obtain v such that this similarity score is as high as possible for each correct name pair while the score is low for other names which are not the translation of each other. we investigate three learning algorithms described below suitable for this problem setting.
1 the widrow-hoff algorithm
　consider the difference of the computed similarity score yk and the actual one zk for the k-th name pair. one can attempt to minimize the sum of the square of the difference of all training name pairs. formally  it can be expressed as:
	minimize	k  yk   zk 1	 1 
subject to
	yk =	i j vi juk i j/max ma mb 
	1   vi j ＋ 1	  vi j （ v
　this problem appears to be a standard large-scale linear regression. the widrow-hoff  wh  algorithm could solve this problem in gradient descent fashion . however  when the entries in the pps table v changes  uk i j will also vary as determined by the lcs algorithm. the standard wh algorithm assumes that the variable uk i j is fixed throughout the training process. therefore  a modified wh algorithm similar to the one proposed in  is investigated. ideally  zk should be set to the actual similarity value of the k-th name pair. to reduce manual effort  we use an approximation for zk. specifically  zk is set to 1 for positive training examples and 1 for negative examples. in each iteration  the wh algorithm works on one name pair. each name pair is processed sequentially and the whole training set of name pairs are processed multiple times. at iteration t + 1  suppose it is handling the k-th name pair. the lcs algorithm is applied to obtain uk i j. then  the similarity value  vi j t + 1   of a particular entry in the pps table is updated by:
       vi j t + 1  = vi j t  + ψ yk t    zk uk i j t   1  where yk t  is given in eqn. 1 for iteration t and ψ   1 is the learning rate. this process continues until a terminating condition is met. a different set of name pairs called the validation set is used to implement the terminating condition. specifically  the validation set is evaluated for every full iteration of processing all training examples one time. if the performance of the latest trained pps table is not improved for three full iterations  the terminating condition is met.
1 the exponentiated-gradient algorithm
　the second algorithm we investigate is the exponentiatedgradient  eg  algorithm which was introduced in  for linear classifiers. the top level framework of eg is similar to wh in that it processes one training name pair at a time and updates the pps table entries immediately. eg requires that the elements in v are nonnegative and sum to 1. equivalently  v belongs to the probability simplex. v is always maintained as a probability simplex in the whole training process. when v is used for calculating the similarity between two phoneme sequences  we magnify elements in v so that the value is comparable to the original design of the pps table. specifically  each element in v is divided
by maxi j vi j . let  . we define as:
	 	 1 
the updating formula is given by:

where κ   1 is the learning rate. Ψ is a normalization expression which is the sum of the updated vi j. both wh and eg attempt to minimize the squared loss expressed in eqn. 1. at the same time  they control the elements of the new vector v to be close to the old one. since ui j is binary  there is a theoretical justification suggesting that eg might perform well on this learning problem.
1 the genetic algorithm
　one way to view the learning problem is to formulate it as an optimization problem as follows:
	maximize	vi juk i j	 1 
                                   k subject to	1   vi j ＋ 1	  vi j （ v
where the summation is conducted over all positive training examples. due to the nature of the dependency of ui j on v   this optimization problem cannot be solved analytically. we investigate a genetic algorithm to solve this optimization problem. genetic algorithms have been shown to perform well in different kinds of optimization problems. in our genetic algorithm formulation  a chromosome represents all the elements in the pps table. each gene in a chromosome corresponds to a particular element in the table. an initial population of chromosomes is prepared. standard genetic operators such as crossover and mutation are employed.
1. experiments on named entity matching model
　to measure the performance of the named entity matching model  two sets of experiments have been conducted. the first set of experiments is to evaluate the phonetic similarity learning. we collected 1 pairs of person names from the chinese-english named entity corpus provided by ldc as the training data. a validation set of 1 pairs of person names were collected from the same corpus for the termination condition in wh and eg algorithms. we further collected 1 person name pairs different from the training and validation data to evaluate the learning performance. the second set of experiments is to evaluate the performance of the overall named entity matching model. we collected 1 named entities from the same corpus for evaluation in the second set of experiments.
　for evaluation purpose  all english entities and chinese entities are regarded as two sets of entities. for each chinese entity  a similarity score is computed between the chinese entity and each english entity. then  the english entities are ranked by their similarity scores in descending order. the average reciprocal rank  arr   is used to measure the performance as follows:
		 1 
where n is the total number of unique chinese entities; ri is the rank of the corresponding correct english entity. the value of arr is between 1 and 1. the higher the arr value  the better is the performance. arr rewards correct translations near the front of the ranked list and penalizes translations near the end of the list.
1 result for learning phonetic similarity
　three learning approaches  namely  wh  eg  and genetic algorithm were evaluated and compared. the initial vi j 1  for both wh and eg algorithms were initialized to the elements similar to the manual pps table. for wh algorithm  the learning rate  ψ  was set to 1 1. for eg algorithm  the learning rate  κ  was set to 1. for genetic algorithm  the initial populations were initialized based on the manual pps table. the crossover rate and the mutation rate were obtained using a validation data set via a tuning process. the best crossover rate and mutation rate were found to be 1 and 1 respectively. for all three learning algorithms  after a pps table has been learned  we applied the pps table on the testing data and evaluated by the arr score as defined in eqn. 1.
　the result is shown in table 1. the performance of the manual assignment of phonetic similarity information in the pps table is 1. as expected  all learning algorithms can produce a pps table with better performance than the manual assignment. in addition  both eg and genetic algorithms perform slightly better than wh.
training algorithmarrwh1eg1genetic algorithm1table 1: the arr results of different learning algorithms
1 result for named entity matching
　a set of named entities is used to evaluate the performance of the named entity matching model. we also conducted the same experiment on the pure phonetic model and the pure semantic model for comparison. the pure phonetic model only makes use of phonetic information without using the bilingual dictionary. it is implemented by restricting the named entity matching model described in section 1 to only using phonetic evidences. the pure semantic model only considers translations found in the dictionary without using phonetic information. it is implemented by restricting the named entity matching model to only using the dictionary.
modelarrnamed entity matching model1pure semantic model1pure phonetic model1table 1: the arr results of the named entity matching  pure semantic  and pure phonetic mod-
els
　the arr of named entity matching  pure semantic  and pure phonetic models are 1  1 and 1 respectively as shown in table 1. the result of the pure phonetic model has demonstrated that the phonetic model alone is not a good model to handle named entities. this is because most of the names consist of multiple terms  with part of them translated based on meaning rather than pronunciation. moreover  the terms are not translated according to the original order. thus  when we simply transform the names into phonetic representation and conduct the matching  the effectiveness is limited. the result of the pure semantic model is better than the pure phonetic model indicating that many named entities possess a considerable amount of semantic evidence. the performance of our proposed named entity matching model is the best among the other models. it clearly demonstrates that considering both semantic and phonetic information is an advantage.
1. miningnewentitytranslations from news
　a useful application of our named entity matching model is to discover new  unseen named entity translations from online daily web news. we develop a mining approach that first automatically harvests bilingual comparable news by analyzing the content of the news stories. unsupervised learning technique using a bilingual dictionary is employed to detect comparable news. new named entity translations not found in the existing bilingual dictionary are then discovered by applying our named entity matching model.
　online daily web news stories from different sources are downloaded and collected. english and chinese news from seven sources are fetched as shown in table 1.
http://www.cnn.com
source: cable news network  cnn 	lang: englishhttp://www.un.org/av/radio/news/latenews.htm
source: united nations radio  unr 	lang: englishhttp://www.rthk.org.hk/rthk/news/englishnews
source: radio television hong kong  rthk 	lang: englishhttp://www1.chinadaily.com.cn/gb/worldinfo/foreign.html
source: china daily  cnd 	lang: chinesehttp://news.yam.com/afp/international
source: agence france-presse  afp 	lang: chinesehttp://www.zaobao.com
source: zao bao  zao 	lang: chinesehttp://www.rthk.org.hk/rthk/news/expressnews
source: radio television hong kong  rthk 	lang: chinesetable 1: online news sources
　automatic named entity extraction methods  one for each language  are developed to extract the named entities from each story. people names  place names  and organization names are extracted from the news content. the remaining content of news is processed using information retrieval techniques. for english news  stemming and stop-word removal are applied. for chinese news  stop-word removal is applied. the content terms together with named entities are used to represent a news story.
1 harvesting comparable news
　unsupervised learning is employed for the task of generating multilingual comparable news. in dealing with crosslanguage comparison  we conduct context-based gloss translation on chinese terms of chinese stories. chinese content terms will be translated into english content terms so that we can perform unsupervised learning on a uniform language representation. gloss translation approach is adopted instead of full-fledged machine translation since our objective is to translate chinese terms adequately for comparable news harvesting purpose. for each chinese term  we look up the bilingual dictionary for the english translation. the translated english terms will then replace the original chinese term to represent the story. typically  a set of translated english terms can be found. each translated term is associated with a term weight. the term weights are adjusted so that more suitable translated terms will receive higher weights. we apply a context-based model derived from  to perform term disambiguation. this model is based on the principle that correct translations tend to co-occur together. mutual information is employed in the co-occurrence model.
　incremental clustering is used for the harvesting process. an incoming story is compared to all existing comparable news clusters according to a similarity measure. the closest comparable news cluster can be determined. if the final normalized similarity of the story to the closest comparable news cluster is larger than a user defined threshold  the story will be added to the cluster. in this case  the comparable news cluster representation will be updated. otherwise  the story will form a new cluster on its own representing a new comparable news cluster. a sample of a harvested comparable news cluster is given in table 1.
titlesourcerelease dateturkey buries latest bomb victimscnn1 nov 1arrests made in turkey blastscnn1 nov 1arrests over istanbul bombingsrthk1 nov 1afp1 nov 1cnd1 nov 1table 1: a sample of a comparable news cluster
1 named entity cognate generation
　when a comparable news cluster is updated  it is examined. if the composition of the cluster includes certain number of both english and chinese news  it is further processed for generating named entity cognates. specifically  we extract people names  place names  and organization names from the stories in this particular cluster resulting in a pair of english and chinese named entity cognates. there is a cognate weight associated with each name in the named entity cognates. the cognate weight attempts to reflect the importance of the name in the corresponding comparable news cluster. a sample of a pair of named entity cognates for a particular cluster is shown in table 1.
english cognate abdullah gul 1    istanbul 1  
 abu musab 1   ，，，chinese cognate 	1    	1 
 	1  ，，，
table 1: a sample pair of named entity cognates
　the cognate weight is calculated separately for people  place  and organization names  as well as for english and chinese names. consider a particular english cognate g  the cognate weight  n pl   of each people name pl in the english named entity cognate is calculated as follows:
	ν pl  =	t sj pl 	 1 
sj
	 	 1 
where t sj pl  represents the weight of the people name pl in the story sj; sj is an english news story belongs to g. the cognate weight for each place name and organization name in g is calculated in a similar way. hence  we can generate an english named entity cognate by collecting the names from the english news in the comparable news cluster and calculate their corresponding cognate weights. the chinese named entity cognate is processed in a similar manner. as a result  a pair of bilingual named entity cognates is generated from each cluster as shown in table 1.
1 entity matching
　the matching makes use of the named entity matching model as well as cognate weight. we observe that if both of the english and chinese named entities are of relatively high cognate weights in a particular news cluster  they are more likely to be matched. otherwise  they are unlikely. consider an english entity e and a chinese entity c. the formula for measuring the cognate weight similarity score  sw e c   is defined as follows:
	sw e c  = min n e  n c  	 1 
where n e  and n c  are the cognate weight of the english entity and that of the chinese entity respectively. this formula rewards those entities which are both with high cognate weights.
　the final similarity score sf e c  for e and c is given as follows:
	sf e c  = sh e c αh + sw e c  1   αh 	 1 
where sh e c  is the similarity value contributed by the named entity matching model mentioned in section 1; αh is a parameter controlling the relative contribution of named entity matching model and cognate weight.
　our mining system attempts to discover new name translations on a daily basis. on each day  the system produces possible name discovery result from each news cluster. for a given cluster  the final similarity score sf e c  of each possible pair of the names is calculated. for a given chinese name  the corresponding english names in the cognate will be returned according to the final similarity scores and sorted in descending order. those english names with the final similarity score greater than a threshold  φ  will be returned.
1. experiments on mining new translations
　we have conducted an experiment to evaluate the named entity discovery performance. news articles as shown in table 1 from 1 november 1 to 1 december 1 were collected for conducting the experiment. there are 1 english news and 1 chinese news in total. the comparable news discovery process was conducted incrementally in batch in each day. each batch contains news from four consecutive days  resulting in 1 batches in total. comparable news clusters are generated for each batch. a number of entity translations are discovered from the clusters in each day  including some seen and unseen names. the unseen translations  not found in the bilingual dictionary  are automatically archived. in our experiments  αh in eqn. 1 was set to 1 and the output threshold φ mentioned in section 1 was set to 1.
　table 1 shows the unseen names discovered in day 1. it is interesting to see that some of the english translations are found to match with several chinese entities due to slightly different translations from various news agencies. for example  both     and     are mapped with  blair .
there are in total 1 unseen name translations discovered. appendix i lists some sample discovery results of a number of days. the results demonstrate that our system can successfully discover some unseen name translations not found in the existing bilingual dictionary from daily online news.
: abdullah gul  mr erdogan: robertson  ritz: bush: gul  kerem: john: blair  bush: clark  mr randt　　: kurdish  kimmitt  puk  kurdistan  mark kimmitt  krivo: blair  tony blair  therese munn: alan greenspan  mr greenspan: munn  therese munn: blair  london: ivanov  mr ivanov  gor ivanovtable 1: unseen name translations discovered in day 1
　to further evaluate the quality of the mining system  we adopt arr as the evaluation metric to measure how accurate our model can rank the correct translations for unseen names. suppose we only consider those discovered chinese named entities with the corresponding english entity appeared in the output. the average arr across all 1 days for all the named entities was 1. we also conducted an investigation on the performance of different kinds of entities. we found that the arr for person names was 1 and the arr for place and organization names was 1. as demonstrated in the arr scores  the overall performance is very encouraging.
1. conclusions
　we have developed a novel named entity matching model which considers both semantic and phonetic information. we have investigated three learning algorithms on training the phonetic similarity information from training examples. our model has been compared with the pure phonetic and pure semantic models. the experimental results show that our hybrid model can handle named entity matching in a more flexible and comprehensive way. we have also applied our named entity matching model on mining new  unseen named entity from real-world online daily web news. translations not found in the dictionary can be effectively discovered.
