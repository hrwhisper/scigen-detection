when investigating alternative estimates for term discriminativeness  we discovered that relevance information and idf are much closer related than formulated in classical literature. therefore  we revisited the justification of idf as it follows from the binary independent retrieval  bir  model. the main result is a formal framework uncovering the close relationship of a generalised idf and the bir model. the framework makes explicit how to incorporate relevance information into any retrieval function that involves an idf component.
　in addition to the idf -based formulation of the bir model  we propose poisson-based estimates as an alternative to the classical estimates  this being motivated by the superiority of poisson-based estimates for the within-document term frequencies. the main experimental finding is that a poisson-based idf is superior to the classical idf   where the superiority is particularly evident for long queries.
categories and subject descriptors: h.1  information storage and retrieval : information search and re-
trieval
general terms: theory  experimentation
keywords: information retrieval  relevance feedback  probability of relevance  binary independent retrieval model  inverse document frequency
1. introduction
　virtually all information retrieval systems estimate the relevance of documents to a query by comparing term statistics for the document at hand to those of the collection and  if available  a set of known relevant and/or non-relevant documents. combining within-document term frequency  referred to as tf   with the inverse document frequency  referred to as idf   has consistently been proven successful on a variety of retrieval benchmarks.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
　the intuition underlying this combination of tf and idf is that frequent occurrence of query terms in a document makes the document more likely to be relevant to the query  but only if these terms are also discriminative. a term is discriminative if it does not occur in many different documents  or  in other words  when its inverse document frequency is high. many publications point out how a tf ， idf measure results naturally from either purely probabilistic or information theoretic arguments  and although they differ in the details  in the end  all of these authors conclude that the theoretical arguments presented fit nicely with the well-known experimental success.
　our investigation considers the role of relevance information in retrieval systems based on tf ， idf ranking functions. exploiting relevance information to revise the importance weights of given query terms and/or to expand the query with new terms usually improves the retrieval quality significantly  i.e.  leads to a better ranking of documents than the one obtained by the initial provided by the user.
　the main goal of this paper is to investigate how the estimate of term discriminativeness  represented by the term's idf   should be updated after relevance information becomes available. from an information theoretic argument  adding relevance information to a retrieval system corresponds to a loss of entropy. without relevance information  the corpus of all documents is the foundation for estimating the discriminativeness  informativeness  of each term. with relevance information  we should remove the documents for which relevance information is available  let it be positive or negative relevance feedback  from the corpus of all documents.
　we view idf primarily as a summary statistic of a set of documents. church and gale have demonstrated that idf is an attractive summary statistic of term sets  because it is more stable across collections of different years and different sources than alternatives such as  e.g.  the variance of term occurrence in documents .
　the classical binary independent retrieval  bir  model  explains idf as the ranking that results from the situation where we have no relevance information. we have found that considering idf as a statistic of the sets of relevant and nonrelevant documents provides an even stronger connection to the bir model. we observed that the relevance-based term weights in the weighting schemes known as f1-f1 can be expressed conveniently as summations of the idf values of the different sets involved.
an open question remains how the discriminativeness of
a term should be measured. the classical idf is based on the occurrence probability p t|c  = nd t c /nd c   where nd t c  is the number of documents in collection c in which term t occurs and nd is the number of documents in the collection. for the within-document term frequency  it is known that alternative estimates  such as those based on 'lifting'  and those based on poisson approximations  1  1   lead to improved retrieval results. the question that follows directly is whether the arguments motivating these alternative estimations apply equally to the estimate used for term discriminativeness.
　the paper is structured as follows. section 1 gives a concise overview of related work. section 1 presents the main results of our research  where we relate the idf summary statistic over sets of relevant and non-relevant documents to the term weighting schemes of the binary independent retrieval  bir  model. section 1 discusses the impact of our results for retrieval systems using tf，idf ranking  and section 1 presents an experimental evaluation. we summarise the contributions of this work in the final section 1.
1. background
　robertson recently surveyed research attempting to find plausible theoretical models explaining why tf ， idf is a good approach to ranking . the same work also presents the bm1 ranking function as another instantiation of tf，idf for ranked retrieval. although we did not set out to provide a theoretical explanation for tf ， idf approaches  the results obtained are related to these works  especially where we contribute into revealing a closer relationship of idf measures to the binary independent retrieval  bir  model .
　from the start  our primary goal has been to investigate the role of the idf summary statistic in the incorporation of relevance information. ruthven and lalmas' review of approaches to relevance feedback  pointed us into the direction of updating idf according to the classic probabilistic model. like   we approach idf as a robust statistic of term occurrence that helps identify 'good' keywords. other related work in information retrieval theory includes the discussion of disjoint term spaces in   and in particular their proposal to represent discriminativeness probabilistically.
　our research can also be viewed as an extension of   where inverse document frequency has been related to the probability of being informative. especially the results obtained on the trec collections presented in section 1 can be considered an experimental investigation into the applicability of the idea proposed there to define the probability of a term being informative based on the assumption of documents as independent rather than disjoint events  which motivates the use of poisson probabilities.
1. idf and the probability of relevance
　this section discusses the relationship between the idf and the probability of relevance. we review briefly the idf definition  section 1   and the probability of relevance  section 1   and the bir model  section 1 . then  section 1 presents the idf -based formulation of the bir model. section 1 gives an intuitive explanation  based on the idea of
'virtual documents'  for the parameter smoothing applied for dealing with singularities of the bir model. finally  we propose and investigate in section 1 poisson-based estimates for probabilities such as p t|r  and p t|c   probability that t occurs in relevant documents and probability that t occurs in the documents of the collection .
1 idf
　the idf is defined as the logarithm of the probability that a term occurs in the collection. let c be the collection  let t be a term  let nd c  be the number of documents in c  and let nd t c  be the number of documents in c in which t occurs. then  the idf is defined as follows:

	idf t c 	:=	 logp t|c 
it reflects the discriminativeness of term t in collection c.
1 probability of relevance
　the probability of relevance is denoted as p r|d q   where r is the relevance event  d is the document event  and q is
the query event.1
bayes' theorem gives:

next  we split the conjunction r d q. once we let d depend on q  and once we let q depend on d.
		 1 
		 1 
equation 1 is the foundation of the binary independent retrieval  bir  model and equation 1 is the foundation of the language modelling  lm  approaches.
　using the odds o r|d q  = p r|d q /p ．r|d q  instead of p r|d q  leads to the same ranking  but p d q   p d  and p q  drop out.

this elegant formulation of the bir model  the lm approaches  and their differences  can be found in .
　now  to instantiate the model  documents and queries are considered as conjunctions of features  denoted with xt. if we assume the features to be independent  we obtain:

usually  the words  terms  occurring in documents and queries are considered as features.
1 binaryindependentretrieval bir model
　the bir model estimates the probability p d|q r  based on the presence and absence of terms. the probability p xt = 1|q r  is the probability that term t occurs in relevant documents. analogously  p xt = 1|q r  is the probability of its absence.
　as usual in formulations of the bir model  we define abbreviations for those probabilities. we set at := p xt = 1|q r  and bt := p xt = 1|q r． .  we deviate from the more common choice of pt and qt because this frequently causes confusion with probabilities and queries .
　since xt is a binary random variable  we can rewrite the probability for an event xt as follows :
p xt|q r =p xt|q r． =we obtain:

the log transformation yields:

q is a constant depending only on the query. since it has no effect on the ranking of the documents  we do not consider it further.
term weight ht  the term discriminativeness of t  equals:

　we write h t c  to refer to the discriminativeness of term t in collection c.
　estimation of probabilities at and bt uses the proportions of relevant and non-relevant documents in the collection. first  we introduce the classical and our alternative notation  also used in  . as the reader will notice  the classical formulation makes use of r  which also denotes the relevance event introduced before. we apologise for this overloading  but apply the classical notation where it improves the readability.
rt	nd t r 	number of relevant documents with t r	nd r 	number of relevant documents nt	nd t c 	number of documents with t n	nd c 	number of documents
　review that  in our notation  given in the second column   r and c are sets of documents: r is the set of relevant documents and c is the set of all documents  the collection . this notation is better suited to clarify dualities between estimates involving all documents  the relevant documents  or the non-relevant documents  and the document or  withindocument  term frequencies.
　next  consider the four alternatives for estimation  classically denoted by f1 to f1 .
f1: : term occurrence is independent in all documents; consider occurrence only  not absence
f1: : term occurrence is independent in the non-relevant documents; consider occurrence only  not absence
f1: : term occurrence is independent in all documents; consider occurrence and absence
f1: : term occurrence is independent in the non-relevant documents; consider occurrence and absence
1 idf and the bir model
　from the formulation in the previous section  the term weights ht can be viewed as sums of idf -values. reconsider the definition of idf :
idf
using the set relevant documents instead of the full collection  we obtain:
idf
this dual statistic reflects the discriminativeness of term t in the set of relevant documents r.
　consider the sum of idf -values we obtain for each of the four settings of ht:
f1: ht =  idf t r  + idf t c : here  the bir model can be explained as follows: given relevance information  the idf of the terms based on the relevant documents is subtracted from the standard idf . that means: if an overall rare term occurs rarely among the relevant documents  then idf t r  is large  thus  ht is significantly smaller than idf t c . it is smaller  since  although the term is discriminative overall  for this particular query  the term is not leading to relevant documents. if an overall rare term is frequent among the relevant documents  then idf t r  is small  and we obtain ht 「 idf t c . we obtain ht = idf t c  if t occurs in all relevant documents. without relevance information  each term occurs in all relevant documents  because the set of relevant documents is empty.
f1: ht =  idf t r +idf t r． : here  ．r denotes the set of nonrelevant documents  ．r = c r  also known as the 'complement method' . positive and negative relevance feedback is exploited in this second possible setting of ht. the discriminativeness of a term is now based on the set of non-relevant documents  rather than on the set of all relevant as it was the case for the first setting.
f1: ht =  idf t r  idf t c．  +idf t r．  +idf t c : the third setting shows all four combinations of term occurrence or absence  and relevant or all documents.
 idf t r  is the discriminativeness of t in relevant documents. reduces ht only slightly for terms frequent in r  but strongly for non-frequent terms.
 idf t c．   reduces ht only slightly if the event  t is absent in the collection  is frequent. then  t occurs rarely  i.e.  idf t c  is large.
idf t r．   is the discriminativeness of the absence of t in r. if the event  t is absent in relevant documents  is rare  then t occurs frequently in r  and this leads to a high ht. thus  idf t r．   supports the message of  idf t r ; i.e.   rare absence of terms in relevant documents is good  means  frequent occurrence in relevant documents is good .
f1: ht =  idf t r  idf t ． r． +idf t r． +idf t r．  : the fourth setting shows all possible four combinations of term occurrence or absence  and relevant or non-relevant documents.
 idf t r  and idf t ． r．  have been discussed for the previous cases.
 idf t ． r．  supports  idf t r．   i.e.  if the event  t is absent in non-relevant document  is frequent  means  t hardly occurs in non-relevant documents    idf t ． r．  diminishes  i.e.  it leads to little reduction for ht.
idf t r．  is the discriminativeness of t in non-relevant documents. without relevance information  idf t r．  = idf t c .
this representation of the term weights of the bir model based on a sum of idf -values gives a clear justification of the idf through the bir model. it generalises the results and claim in   page 1:  it is now apparent that we can regard idf as a simple version of the rsj weight applicable when we have no relevance information. 
　when no relevance information is available  maximum entropy   the classical idf corresponds to the measure of discriminativeness. the more relevance information becomes available  the more the term's discriminativeness  in the remainder of the collection  will decrease  a loss of entropy . it decreases because the corpus of documents for which no relevance information is available is smaller  and  more importantly  because the discriminativeness of the query term in the relevant documents is subtracted from the discriminativeness obtained for all  or all non-relevant  documents.
1 dealing with the singularities
　the estimates for the discriminativeness weights ht derived above have to be adapted for dealing with singularities. the following cases require adaptation of the estimates:
  r = nd r  = 1: no relevance information given.
  rt = nd t r  = 1: term t does not occur in any relevant document. then  logp t|r  is not defined.
  n = nd c  = 1: empty collection.
  nt = nd t c  = 1: term t does not occur in any document.
  r = rt = nd r  = nd t r : term t occurs in all relevant documents. singularity for f1 and f1.
  n = nt = nd c  = nd t c : term t occurs in all documents. singularity for f1 and f1.
  nt = rt = nd t c  = nd t r : all documents in which term t occurs are relevant. singularity for f1.
  n r = nr rt = nd c  nd r  = nd t c  nd t r : term t occurs in all non-relevant documents. singularity for f1.
a common reformulation  smoothing  of the estimates is to add constants to the expressions in numerator and denominator.
　the classic smoothing proposed for f1  see   adds a constant value 1 to each document count:

　although the literature has given various mathematical motivations for this constant  we like to offer a particularly intuitive explanation. consider that we add two virtual documents to each collection  one of which is relevant. assuming that each term occurs in half of the virtual and relevant documents gives the following probability estimates:

based on these adapted estimates  we obtain the smoothed f1:

so  extending the document sets with two virtual documents is sufficient to explain the classical parameter adaptation. next  consider four virtual documents; two of which are relevant  all terms occur in half of the documents  and all terms occur in half of the relevant documents.

this solution feels more comfortable  because all numbers are integers; they can be interpreted as document counts. we obtain:

as a general explanation  we derive:

the number of virtual documents that is added to the collection reflects the uncertainty about relevance. we assume that each term t occurs in half of the virtual documents  since occurrence is a binary event   that half of the virtual documents are relevant  since relevance is a binary event   and that each term t occurs in half of the relevant documents. as demonstrated  these assumptions explain the classical smoothing proposed for f1 by defining
1 poisson-based probability estimation
　probability estimation for p t|c  and p t|r  and related probabilities have so far been estimated on what we refer to as n/n estimates and their adaptations. consider a set of n trials  and let nt be the number of trials in which the event t is true. then  p t  = nt/n is the probability that the event is true among n trials. we refer to this estimate as the disjointness-based estimate  since it can be explained by the theorem of total probability as p t  := pd p t|d ，p d   where the events d are disjoint and exhaustive.
　the appropriateness of these estimates can however be questioned  in particular in the light of the summation of idf -values as follows from the bir model. for  the sets involved  set of relevant documents  set of non-relevant documents  set of all documents  are different in cardinality: usually  only a small fraction of all documents forms the set of relevant documents.
　compare this to estimates for p t|d   i.e.  the probability that a term describes  occurs in  a document. while the disjointness-based probability corresponds to the probability that t occurs  the poisson-based probability of term occurrence is the probability that t occurs at least once in nt trials. the usefulness of a poisson-based probability is wellknown for the within-document term frequency  referred to as tf or lf   of a term t in a document d  1  1 .
　let nl t d  be the number of locations at which t occurs in d. then  the poisson-based estimate of term  location  frequency is defined as follows:

here  p t|d  is the probability that term t is representative for document d. the steep rise of the within-document term frequency for small occurrences  nl t d  relatively small  leads to a superior retrieval quality. for example  let k be the average occurrence frequency  k := 1/nt ，pt nt  where nt is the number of events/terms . we obtain p t  = 1 if nt is the average occurrence  p t    1 for nt less than the average occurrence  and p t    1 for nt greater than the average occurrence.
　this effect of a steep rise is also reflected in the wellknown lifting function with lifting  zooming  factor zr   and related publications :
tf
for zd := 1  the lifting yields tf t d  「 1  if nl t d  = 1. compare this to the poisson approximation  where for kd = 1  we obtain tf t d  = 1  if nl t d  = 1.
　looking at the sum of idf -values as presented in section 1  we deal with sets of significantly different cardinality. the set of relevant documents is much smaller than the set of non-relevant and the set of all documents. this different cardinality and the success of the lifting of the probability p t|d  are the motivations for investigating poissonbased estimates for p t|c  and p t|r .
　review the poisson-based probability estimation for p t|c   and p t|r :
	p t|c 	:=	df
	p t|r 	:=	df
compare these also to the formulation of p t|d  given before. the notation shows the strong analogy of the probabilities involved.
　section 1 reports the experimental results obtained for poisson-based estimates. before we started the experimental investigation  we looked at an analytical experiment to investigate the nature of the poisson-based estimate. consider the analytical experiment in table 1. let a collection with nd ．r  = 1  one million documents  be given  and let a query with nd r  = 1  i.e.  ten relevant documents  be given. for ease of reading  we use logarithm base 1 for the numerical illustration  the base of the logarithm does not matter for ranking purpose  as the base is a constant  term-independent factor .
　the table shows some numerical values for the discriminativeness ht = idf t r．    idf t r   earlier also referred to as the f1 estimate. the table shows six cases of possible term distributions  and the cases  a    d    e  and  f   bold face  illustrate the difference between the n t x /n and the n t x / kx + n t x   estimates. here  x represents a set of documents: the set r of relevant documents  or the set r． of non-relevant documents. the main findings from this analytical investigation are:
1. the classical estimate nt/n leaves ht to be high for terms rare in the collection; the effect of the occurrence of the term in the relevant document is minor  see  a    b   and  c  . ht is mainly determined by idf t r． .
1. the poisson-based estimate leads to a dramatic impact on ht for terms that occur rarely in the relevant documents  see  a  and  d  . for a rare term  the sum of idfp-values is zero  case  a  ! for a more frequent term  idfp t r．  remains the main impact of t on the rsv  cases  b  and  c  .
1. the classical estimate nt/n leaves idf t r．  to be the dominant factor even for terms that are relatively frequent in ．r  and the frequency of t in r has hardly an effect  see  d    e   and  f  .
1. the poisson-based estimate leads to small idfp t r．  values for terms that are relatively frequent. the impact of idfp t r  is strong  see  d   for terms rare among the relevant. for a term relatively frequent in relevant and non-relevant documents  the effect on the rsv is minimal  see  e  and  f  . as the analytical investigation shows  this cancelling out of terms is already happening for terms that are little to medium frequent  if the parameter k of the poisson-approximations is small  for the analytical experiment  kr． = kr = 1 .
this analytical experiment illustrates the different nature of the classical nt/n  idf   estimate and the poisson-based estimate n/ k+n   idfp . as our trec experiments show  the setting of k has a major impact on the retrieval quality. before we report on the experiments  we first summarise the impact of our results on tf ， idf-based retrieval functions.
1. impact on tf-idf-based ranking
table 1: analytical experiment: comparison of ht for n t x /n and n t x / kx + n t x  
n t x /nn t x / 1 + n t x  nd t r nd t r． idf t r idf t r． ht =
  idf t r  + idf t r． idfp t r idfp t r． ht =
  idfp t r  + idfp t r． a b c
d e
f1
1
1
1
1
1
1
1
1
1 log 1
 log 1  log 1  log 1
 log 1
 log 1 log 1
1
 log 1
1
 log 1
　　　　1  log 1
　　　　1  log 1
　　　　1  log 1
1 1 + 1 = 1 「 idf t r． 
  1 「 idf t r． 
1 = idf t r． 
 1 + 1 = 1 「 idf t r． 
  1 「 idf t r． 
1 = idf t r．  log 1
 log 1
 log 1
 log 1
 log 1
 log 1 log 1
 log 1
 log 1
 log 1
 log 1
 log 1
「 idfp t r． 
「 idfp t r． 
「  idfp t r 
「 1
「 1　one result of our investigation is that any retrieval system based on tf ， idf ranking may incorporate relevance information by replacing its idf -component by ht  where ht is a sum of idf -values derived from the sets of relevant and nonrelevant documents as presented in section 1. this result coincides with the bm1 ranking formula  and extends the relationship of idf and bir model as presented in .
　consider the definition of a tf，idf-based retrieval function  and we illustrate the replacement of idf by the discriminativeness measure ht:
	rsvtf idf	:= xidf t c  ， tf t d  ， tf t q 

the probabilities based on the within-document and withinquery term frequencies are estimated as follows:

the factors kd and kq serve for normalisation purposes. the bm1  as the currently most successful tf ， idf-based retrieval function  proposes h t c  in place of idf   and adds various constants to the retrieval function. the motivation for the constants is to leverage the effect of h t c   tf t d   and tf t q   and to fit the retrieval function to the nature of the collection for which it is applied.
　to illustrate  we substitute h t c  with the idf -based formulation of f1:
rsvtf idf = x  idf t r  + idf t c   ， tf t d  ， tf t q 
t
the factor idf t c  is the classical idf . the factor  idf t r  represents the relevance information. this factor is negative  since h t c  is smaller than idf t c  for terms that occur rarely in relevant documents. the example shows that for h t c  based on f1  rsvtf，idf corresponds to a classic tf ， idf if no relevance information is available  and  the formulation also shows that with idf t c  only we actually assume that t occurs in all relevant documents  idf t r  = 1 .
　this framework makes the relationship between idf and relevance information fully explicit.
1. experiments
　so far  the results of this paper have been of theoretical nature: relevance feedback leads to a revised idf in tf ， idfbased retrieval functions. in the derivation of this result  we proposed poisson-based probability estimates instead of the classic  n/n-based  ones. as the weighting schemes corresponding to the bir model combine idf -values of sets with highly different cardinalities  we investigate experimentally whether a poisson-based estimation makes sense for probabilities p t|c  and p t|r .
1 experimental setup
　the experimental evaluation consists of a first series of adhoc retrieval experiments to investigate the effect of poissonbased estimates for idf in a setting in absence of relevance information  and a subsequent series of routing experiments to investigate its effect on processing relevance information using the four variants of the bir model to exploit relevance information.
　we have used the trec test collections developed at trec-1 and trec-1  using the data of 'disks 1 and 1'  and topic sets 1 and 1 respectively . preprocessing of the documents included neither stop-word removal nor stemming  resulting in the same experimental setup as the one described in . however  terms in the query that occur on van rijsbergen's stop-word list have been removed from the query text.
　like the adhoc experiment  the routing experiment follows the setup described in   where the training data consists of the los angeles times articles and the test data consists of the remaining data. this division of the trec data in training and testing is a natural one for a routing task  because the training articles date from 1 and 1  in time preceding the rest of the collection which contains publications from 1 until 1.
　the retrieval system has been developed by extending the open-source main memory database system monetdb1 with some minimal extra functionality to support information retrieval. to validate the basic workings of this retrieval system implementation  we first reproduced the baseline adhoc retrieval approach presented in   and obtained indeed the mean average precision  map  reported there.1 we then proceeded to implement the bm1 ranking  where we choose k1 = 1 and k1 = 1 following the okapi trec papers. we have set the length normalisation parameter b to 1  as derived analytically in .
1.1 adhoc experiments
　table 1 presents an overview of the results on the trec-1 and trec-1 adhoc test collections. first  compare the bottom row displaying the baseline results of a standard bm1 ranking using idf estimation  to the other rows  showing the results for bm1 using idfp instead of idf . on the short table 1: mean average precision of adhoc retrieval experiments for topics created from title  t   description  d  and narrative  n . the bottom line is the normal idf estimate; the others are poissonestimated idf with varying k  where n = 1  and nbt = n1 pt nt 「 1.
trec-1trec-1kttdtdnttdtdn1.1.1.1.1.1.1nt c111111n/1.1.1.1.1.1.1n/1.1.1.1.1.1.1n/1.1.1.1.1.1.1n/1.1.1.1.1.1.1n/1.1.1.1.1.1.1n111111-111111table 1: mean average precision of adhoc retrieval experiments for topics created from title  t   description  d  and narrative  n   using idf only.
ttdtdntrec-1.1.1.1trec-1.1.1.1trec-1 queries  all of the idfp results outperform the baseline. if k is sufficiently high  the mean average precision of runs using idfp is better than the baseline regardless the topic length; k = n/1 gives near-best results for all of the experimental conditions. from these experiments  it seems fair to conclude that it makes sense to apply the poissonestimation to the occurrence frequencies underlying the inverse document frequency as a model of the discriminative power of query terms.
　tables 1 and 1 present the results obtained using idf t c  weighting only  using idf and idfp respectively. performance of the title-only queries is still reasonably good when compared to the results in table 1. using idf on its own degrades however for longer queries  whereas idfp results are impressive regardless the topic length.
1.1 routing experiments
　the routing experiments use the relevance assessments on the la times articles in the estimation of idf t r . the baseline results are those obtained on the test data  not including the training data  without using any relevance information. we then apply the four weighting schemes of the bir model  comparing the effect of relevance information obtained with the standard idf to that using the poissonbased idfp.
table 1: mean average precision of adhoc retrieval experiments for topics created from title  t   description  d  and narrative  n   using idfp only  k = n/1 .
ttdtdntrec-1.1.1.1trec-1.1.1.1table 1: mean average precision of routing experiments with weighting strategies  for topics created from title  t   description  d  and narrative  n . the poisson-based idf  denoted idfp  is parameterised with k = n/1.
trec-1trec-1methodttdtdnttdtdn-idf111111f1idf111111f1idf111111f1idf111111f1idf111111-idfp111111f1idfp111111f1idfp111111f1idfp111111f1idfp111111table 1: mean average precision of routing experiments with idfp for k = 1  for weighting strategies  topics created from title  t   description  d  and narrative  n .
trec-1trec-1methodt	td	tdn	t	tdtdn	-	idfp11	11	11f1	idfp11	11	11f1	idfp11	11	11f1	idfp11	11	11f1	idfp11	11	11　table 1 summarises the routing results. whereas with classical idf   relevance feedback obtained from the assessments on the los angeles times articles improves results over the baseline  applying the bir relevance weighting using the poisson-based idfp does not lead to results that improve upon its baseline. notice however that all experimental results using idfp result in higher mean average precision scores than all of those using idf   with or without relevance information. while for long queries  a marginal performance improvement is observed with standard idf weighting  the results after feedback remain significantly below the idfp baseline. the differences in mean average precision between f1 and f1 seem negligible  as well as those between f1 and f1. surprisingly  the weighting schemes based on term presence only  f1 and f1  outperform consistently those on term presence and absence  f1 and f1 .
　we presented only the experimental results for k = n/1  but the observations made hold for other choices of k as well. table 1 shows an adverse effect of the poisson weighting on the routing task performance  especially on the longer queries . again  these are representative for all settings with small k.
1 results and analysis
　the difference in performance between short and long queries in tables 1 and 1 can be partially explained by assuming that users stating a short 'title' query select very carefully the most discriminative terms with respect to relevance  while long queries like the description and narrative also contain noisy terms that do not directly relate to the underlying information need.
the idf values do not differentiate between discriminative yet non-relevant terms and non-discriminative yet relevant terms in the query. the poisson-based idf yields the same order of terms with respect to their discriminativeness as the classical idf does  but reduces the influence of non-discriminative terms on the ranking. therefore  the effect of using one or the other approach is stronger for long queries than for short queries.
　setting the k-parameters of the poisson-estimates to small values corresponds to weakening the effect of idf on the rsv  whereas a large k strengthens the effect of idf . consistently  we can observe that a strong idf leads to better retrieval results.
　a poisson-based idf with large k is consistently superior to the classical idf . this experimental result can be explained as follows from a theoretical point of view. the poisson-based idf distinguishes stronger between discriminative terms  and distinguishes less between non-discriminative terms than the classical idf . as our results show  this nature of the poisson-based idf of being stronger for rare terms and less strong for frequent terms leads to better retrieval quality. this result perfectly coincides with the superiority of the poisson-based probability reflecting the within-document frequency.
　further in-depth analysis is needed to give an explanation as to why the ample training data in the routing experiment did not lead to a considerable improvement on the test data; though we like to point out that  also reports disappointing results with feedback in the routing experiment performed; so  maybe in the specific experimental setup chosen it is especially hard to gain improvements  and we should repeat the experiment on another  trec  routing or filtering task.
1. summary and conclusions
　this paper makes explicit the relationship between idf and the binary independent retrieval  bir  model. the impact of this result is that relevance information can be incorporated into any tf ， idf-based retrieval function by revising the idf according to the relevance information available. our result makes explicit that relevance information can be viewed as a loss of entropy. the discriminativeness of terms  initially high because of maximal entropy  decreases for two reasons. first  the event space for estimating term discriminativeness after feedback is smaller. second  the discriminativeness of terms in relevant documents is subtracted from the original idf .
　in addition to this theoretical framework for relating idf and the bir model  and for justifying the incorporation of relevance information into a revised idf -component  we investigated poisson-based probability estimations. motivated by the success of poisson-based estimates for withindocument frequencies  we applied poisson-estimates for the occurrence probabilities p t|r   term occurrence in relevant documents  and p t|c   term occurrence in all documents   which form the basis for the discriminativeness measures. the overall result is that the poisson-based idf is superior to the classical idf   in particular for long queries. unfortunately  we could not prove experimentally a positive effect in improving retrieval by relevance feedback.
　our next research steps include the transformation of idf values for measuring discriminativeness  informativeness  respectively  into probabilities. this is a key issue for probabilistic retrieval models and probabilistic reasoning. for example  the framework described in  is based on a disjoint space of concepts  where we could apply an idf -based probability as an estimate of the probability of term informativeness. the refinement of those term space probabilities according to relevance feedback data has been an open problem. though using idf -values as a basis for probability estimation poses a number of problems  the result of this paper on how to manage relevance information in an idf based space might lead to new insights in how information theory and probability theory interrelate.
acknowledgement
we thank djoerd hiemstra for providing pre-processed trec data.
