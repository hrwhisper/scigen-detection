existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. one major problem is the high memory dependency: either the gigantic data structure built is assumed to fit in main memory  or the recursive mining process is too voracious in memory resources. another major impediment is the repetitive and interactive nature of any knowledge discovery process. to tune parameters  many runs of the same algorithms are necessary leading to the building of these huge data structures time and again. this paper proposes a new disk-based association rule mining algorithm called inverted matrix  which achieves its efficiency by applying three new ideas. first  transactional data is converted into a new database layout called inverted matrix that prevents multiple scanning of the database during the mining phase  in which finding frequent patterns could be achieved in less than a full scan with random access. second  for each frequent item  a relatively small independent tree is built summarizing co-occurrences. finally  a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed. experimental studies reveal that our inverted matrix approach outperform fp-tree especially in mining very large transactional databases with a very large number of unique items. our random access disk-based approach is particularly advantageous in a repetitive and interactive setting.
categories and subject descriptors
h.1  database management : data mining
keywords
association rules  frequent patterns mining  cofi-tree  inverted matrix
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigkdd '1  august 1  1  washington  dc  usa copyright 1 acm 1-1/1 ...$1.
1.	introduction
모recent days have witnessed an explosive growth in generating data in all fields of science  business  medicine  military  etc. the same rate of growth in the processing power of evaluating and analyzing the data did not follow this massive growth. due to this phenomenon  a tremendous volume of data is still kept without being studied. data mining  a research field that tries to ease this problem  proposes some solutions for the extraction of significant and potentially useful patterns from these large collections of data. one of the canonical tasks in data mining is the discovery of association rules. discovering association rules  considered as one of the most important tasks  has been the focus of many studies in the last few years. many solutions have been proposed using a sequential or parallel paradigm. however  the existing algorithms depend heavily on massive computation that might cause high dependency on the memory size or repeated i/o scans for the data sets. association rule mining algorithms currently proposed in the literature are not sufficient for extremely large datasets and new solutions  that do not depend on repeated i/o scans and less reliant on memory size  still have to be found.
1	problem statement
모the problem of mining association rules over market basket analysis was introduced in . association rules are not limited to market basket analysis  but the analysis of sales or what is known as basket data  is the typical application often used for illustration. the problem consists of finding associations between items or itemsets in transactional data. the data could be retail sales in the form of customer transactions or even medical images . association rules have been shown to be useful for other applications such as recommender systems  diagnosis  decision support  telecommunication  and even supervised classification . formally  as defined in   the problem is stated as follows: let i = {i1 i1 ...im} be a set of literals  called items. m is considered the dimensionality of the problem. let d be a set of transactions  where each transaction t is a set of items such that t   i. a unique identifier tid is given to each transaction. a transaction t is said to contain x  a set of items in i  if x   t. an association rule is an implication of the form  x   y    where x   i  y   i  and x 뫌y =  .
an itemset x is said to be large or frequent if its support s is greater or equal than a given minimum support threshold . the rule x   y has a support s in the transaction set d if s% of the transactions in d contain x 뫋y . in other words  the support of the rule is the probability that x and y hold together among all the possible presented cases. it is said that the rule x   y holds in the transaction set d with confidence c if c% of transactions in d that contain x also contain y . in other words  the confidence of the rule is the conditional probability that the consequent y is true under the condition of the antecedent x. the problem of discovering all association rules from a set of transactions d consists of generating the rules that have a support and confidence greater than a given threshold. these rules are called strong rules. this association-mining task can be broken into two steps: 1. a step for finding all frequent k-itemsets known for its extreme i/o scan expense  and the massive computational costs; 1. a straightforward step for generating strong rules.
1	related work
모several algorithms have been proposed in the literature to address the problem of mining association rules  1  1 . one of the key algorithms  which seems to be the most popular in many applications for enumerating frequent itemsets is the apriori algorithm . this apriori algorithm also forms the foundation of most known algorithms. it uses a monotone property stating that for a k-itemset to be frequent  all its k-1-itemsets have to be frequent. the use of this fundamental property reduces the computational cost of candidate frequent itemsets generation. however  in the cases of extremely large input sets with outsized frequent 1-items set  the apriori algorithm still suffers from two main problems of repeated i/o scanning and high computational cost. one major hurdle observed with most real datasets is the sheer size of the candidate frequent 1-itemsets and 1-itemsets.
park et al. have proposed the dynamic hashing and pruning algorithm  dhp  . this algorithm is also based on the monotone apriori property  where a hash table is built for the purpose of reducing the candidate space by precomputing the proximate support for the k+1 item set while counting the k-itemset. dhp has another important advantage  the transaction trimming  which removes the transactions that do not contain any frequent items. however this trimming and the pruning properties cause problems that make it impractical in many cases .
모the partitioning algorithm proposed in  reduced the i/o cost dramatically. however  this method has problems in cases of high dimensional itemsets  i.e. large number of unique items   and it also suffers from the high false positives of frequent items. the dynamic itemset counting  dic  reduces the number of i/o passes by counting the candidates of multiple lengths in the same pass. dic performs well in cases of homogenous data  while in other cases dic might scan the databases more often than the apriori algorithm.
모another innovative approach of discovering frequent patterns in transactional databases  fp-growth  was proposed by han et al. in . this algorithm creates a compact tree-structure  fp-tree  representing frequent patterns  that alleviates the multi-scan problem and improves the candidate itemset generation. the algorithm requires only two full i/o scans of the dataset to build the prefix tree in main memory and then mines directly this structure. this special memory-based data structure becomes a serious bottleneck for cases with very large databases.
1	motivations and contributions
모apriori-like algorithms suffer from two main severe drawbacks: the extensive i/o scans for the databases  and the high cost of computations required for generating the frequent items. these drawbacks make these algorithms impractical in cases of extremely large databases. other algorithms like fp-tree based depend heavily on the memory size as the memory size plays an important role in defining the size of the problem. memory is not only needed to store the data structure itself  but also to generate recursively in the mining process a set of smaller trees called conditional trees. as argued by the authors of the algorithm  this is a serious constraint . other approaches such as in   build yet another data structure from which the fp-tree is generated  thus doubling the need for main memory. one can argue that the tree structure  such as fp-tree  could be stored on disk. indeed  using a b+tree  as suggested by the original authors of fp-growth  one could efficiently store the prefix tree. however  no one has really experimented this approach or reported on it. we have analyzed the use of a b+tree to store the fp-tree and found out that the number of i/os increases significantly in the mining phase of the tree  defeating the purpose of building the tree structure in the first place.
모the current association rule mining algorithms handle only relatively small sizes with low dimensions. most of them scale up to only a couple of millions of transactions and a few thousands of dimensions  1  1 . none of the existing algorithms scales to beyond 1 million transactions  and hundreds of thousands of dimensions  in which each transaction has an average of at least a couple of dozen items. this is the case for large businesses such as walmart  sears  ups  etc.
모in this paper we are introducing a new association rule mining disk-based algorithm that is based on the conditional pattern concept . this algorithm is divided into two main phases. the first one  considered pre-processing  requires two full i/o scans of the dataset and generates a special diskbased data structure called inverted matrix. in the second phase  the inverted matrix is mined using different support levels to generate association rules using the inverted matrix algorithm explained later in this paper. the mining process might take in some cases less than one-full i/o scan of the data structure in which only frequent items based on the support given by the user are scanned and participate in generating the frequent patterns.
모the reminder of this paper is organized as follows: section 1 illustrates the transactional layout and the motivations of the inverted matrix approach. section 1 describes the design and constructions of the co-occurrence frequent item trees. section 1 depicts the inverted matrix algorithm. experimental results are given in section 1. finally  section 1 concludes by discussing some issues and highlights our future work.
1.	transaction layout
1	observations on superfluous processing
모frequent itemset mining algorithms mine the database on a given fixed support threshold. if the support threshold changes  the mining process is repeated. in practice  since the minimum support is not necessarily known and needs tuning  the mining process is interactively repeated with different values for the support threshold. in particular  if the support is consecutively reduced  k new scans of the database are needed for the apriori-based approaches  and a new memory structure is built for fp-growth like methods. notice that in each run of these algorithms  previous accumulated knowledge is not taken into account. for instance  in the simple transactional database of figure 1a  where each line represents a transaction  called horizontal layout   we can observe that when changing support one can avoid reading some entries. if the support level is greater than 1  then figure 1b highlights all frequent items that need to be scanned and computed. non-circled items in figure 1b are not included in the generation of the frequent items  and reading them becomes useless. it is known that all of the existing algorithms scan the whole database  frequent and non-frequent items more than once generating a huge amount of useless work  1  1  1 . we call this superfluous processing. figure 1c represents what we actually need to read and compute from the transactional database based on a support greater than 1. obviously  this may not be possible with this horizontal layout  but with a vertical layout avoiding these useless reads is possible.

t#            items
t#            items
t#         itemst1     a   b    c    d    e t1     a   e    c    h    g t1     b   c    d    a    e t1     f   a    h    g    j t1     a   b    c    e    i t1     k   a    e    i     c t1     a   h    e    g    i t1     k   l    m    n    o t1     l    r    q    a    ot1  p    n    b    a    m t1     a      c      et1     a      e      ct1     c      a      et1     a     t1     a      c      et1     a      e      t1     a      t1   a   t1     a   b    c    d    e 
t1     a   e    c    h    g 
t1     b   c    d    a    e 
t1     f   a    h    g    j 
t1     a   b    c    e    i 
t1     k   a    e    i     c 
t1     a   h    e    g    i 
t1     k   l    m    n    o 
 	t1     l    r    q    a    o t1  p    n    b    a    m 
	 a 	 b 	 c 
figure 1: a: transactional database  b : frequent items circled  c : needed items to be scanned     1.
모the transaction layout is the method in which items in transactions are formatted in the database. currently  there are two approaches: the horizontal approach and the vertical approach. in this section these approaches are discussed and a new transactional layout called inverted matrix is presented and compared with the existing two methods.
1	horizontal vs. vertical layout
모the relational database model consists of storing data into two-dimensional arrays called tables. each table is made of n rows called features or observations  and m columns called attributes or representing variables. the format of storing transactions in the database plays an important role in determining the efficiency of the association rule-mining algorithm used. existing algorithms use one of the two layouts  namely horizontal and vertical. the first one  which is the most commonly used  relates all items on the same transaction together. in this approach the id of the transaction plays the role of the key for the transactional table. figure 1a represents a sample of 1 transactions made of 1 items. the vertical layout relates all transactions that share the same items together. in this approach the key of each table 1: transactions presented in vertical layout
itemtransaction ida11111b11c111d1e111f1g11h11i11j1k1l1m1n1o1p1q1r1record is the item. each record in this approach has an item with all transaction numbers in which this item occurs. this is analogous to the idea of inverted index in information retrieval where a word is associated with the set of documents it appears in. here the word is an item and the document is a transaction. transactions in figure 1a are presented by using the vertical approach in table 1. the horizontal layout has a very important advantage  which is combining all items in one transaction together. in this layout and by using some clever techniques  such as the one used in   the candidacy generation step can be eliminated. on the other hand  this layout suffers from limitations such as the problem mentioned above that we called superfluous processing since there is no index on the items. the vertical layout  however  is an index on the items in itself and reduces the effect of large data sizes as there is no need to always re-scan the whole database. on the other hand  this vertical layout still needs the expensive candidacy generation phase. also computing the frequencies of itemsets becomes the tedious task of intersecting records of different items of the candidate patterns. in  a vertical database layout is combined with clustering techniques and hypergraph structures to find frequent itemsets. the candidacy generation and the additional steps associated with this layout make it impractical for mining extremely large databases.
1	inverted matrix layout
모the inverted matrix layout combines the two previously mentioned layouts with the purpose of making use of the best of the two approaches and reducing their drawbacks as much as possible. the idea of this approach is to associate each item with all transactions in which it occurs  i.e. an inverted index   and to associate each transaction with all its items using pointers. similar to the vertical layout  the item is the key of each record in this layout. the difference between this layout and the vertical layout seen previously is that each attribute on the inverted matrix is not the transaction id  but a pointer that points to the location of the next item on the same transaction. the transaction id could be preserved in our layout  but since it is not needed table 1: phase 1  frequency of each item
itemfrequencyitemfrequencyitemfrequencyp1f1q1r1j1o1d1k1l1m1n1i1g1h1b1c1e1a1for the purpose of frequent itemset mining  it is discarded. the pointer is a pair where the first element indicates the address of a line in the matrix and the second element indicates the address of a column. each line in the matrix has an address  sequential number in our illustrative example  and is prefixed by the item it represents with its frequency in the database. the lines are ordered in ascending order of the frequency of the item they represent. table 1 represents the inverted matrix corresponding to the transactional database from figure 1a.
모building this inverted matrix is done in two phases  in which phase one scans the database once to find the frequency of each item and orders them into ascending order  such as in table 1 for our illustrative example. the second phase scans the database again once to sort each transaction into ascending order according to the frequency of each item  and then fills in the matrix appropriately. to illustrate the process  let's consider the construction of the matrix in table 1. the first transaction in figure 1a has items  a  b  c  d  e . this transaction is sorted into  d  b  c  e  a  based on the item frequencies in table 1 built in the first phase of the process. item d has the physical location line 1 in the inverted matrix in table 1  b has the location line 1  the location of c is line 1  e is in line 1 and finally a is in line 1. this is according to the vertical approach. item d has a link to the first empty slot in the transactional array of item b that is 1. consequently   1  entry is added in the first slot of item d to point to the first empty location in the transactional array of b. at the first empty location of b  1  an entry is added to point to the first empty location of the next item c that is  1 . the same process occurs for all items in the transaction. the last item of the transaction  item a produces an entry with pointer null  뷋 뷋 . the same is performed for every transaction.
모building the inverted matrix is assumed to be pre-processing of the transactional database. for a given transactional database  it is built once and for all. the next section presents an algorithm for mining association rules  or frequent itemsets  directly from this matrix. the basic idea is straight forward. for example  if the user decides to find all frequent patterns with support greater than 1  it suffices to start the mining process from location line 1. line 1 represents the item c which has the frequency 1. since the lines of the matrix are ordered  along with c  only the items that appear after c are frequent. all the other items are irrelevant for this particular support threshold. by following in the inverted matrix the chain of items starting from the c location  we can rebuild parts of the transactions that contain only the frequent items. thus  we avoid the superfluous processing mentioned before. table 1 represents the sub-transactions that can be generated from the inverted matrix of table 1 by following the chains starting from location line 1. the mining algorithm described in the next section targets these sub-transactions  and passes over all other parts dealing with de-facto non-frequent items. the sub-transactions of frequent items such as in table 1 are never built at once. as will be explained in the next section  these sub-transactions are considered one frequent item at a time. in other words  using the inverted matrix  for each frequent item x  the algorithm would identify the sub-transactions of frequent items that contain x. these sub-transactions are then represented in a tree structure  that we call co-occurrence frequent item tree  which is mined individually.
table 1: sub-transactions with items having support greater than 1.  a  list of sub-transactions;  b  condensed list.

c  
c  
c    e     a  e     a  e     ac    e     ac    e     ae    a
frequent itemsoccursc    e     a1e    a 1	 a 	 b 
1. co-occurrence frequent-itemtrees: designandconstruction
모the generation of frequencies is considered a costly operation for association rule discovery. in apriori-based algorithms this step might become a complex problem in cases of high dimensionality due to the sheer size of the candidacy generation . in methods such as fp-growth   the candidacy generation is replaced by a recursive routine that builds a very large number of sub-trees  called conditional fp-trees  that are on the same order of magnitude as the frequent patterns and proved to be poorly scalable as attested by the authors .
모our approach for computing frequencies relies first on reading sub-transactions for frequent items directly from the inverted matrix  then building independent relatively small trees for each frequent item in the transactional database. we mine separately each one of the trees as soon as they are built  with minimizing the candidacy generation and without building conditional sub-trees recursively. the trees are discarded as soon as mined.
모the small trees we build  co-occurrence frequent item tree  or cofi-tree for short  are similar to the conditional fp-tree in general in the sense that they have a header with ordered frequent items and horizontal pointers pointing to a succession of nodes containing the same frequent item  and the prefix tree per-se with paths representing subtransactions. however  the cofi-trees have bidirectional links in the tree allowing bottom-up scanning as well  and the nodes contain not only the item label and a frequency counter  but also a participation counter as explained later in this section. another difference  is that a cofi-tree for a given frequent item x contains only nodes labeled with items that are more frequent or as frequent as x.
locindextransactional arraytable 1: inverted matrix
11111 p 1  1 1 f 1  1 1 q 1  1 1 r 1  1 1 j 1  1 1 o 1  1  1 1 d 1  1  1 1 k 1  1  1 1 l 1  1  1 1 m 1  1  1 1 n 1  뷋 뷋  1 1 i 1  1  1  1 1 g 1  1  1  1 1 h 1  1  1  1 1 b 1  1  1  1  1 1 c 1  1  1  1  1  1 1 e 1  1  1  1  1  1  1 1 a 1  뷋 뷋  뷋 뷋  뷋 뷋  뷋 뷋  뷋 뷋  뷋 뷋  뷋 뷋  뷋 뷋  뷋 뷋 모to illustrate the idea of the cofi-trees  let us consider an example of sub-transactions of frequent items. assume we have a transactional database that has the following frequent items  a  b  c  d  e  and f   where a is the most frequent table 1: example of sub-transactions with frequent items
frequent itemsoccurs togethercd1cb1ea1fb1cda1cba1item  and f is the least frequent item in the database. assume also that these frequent items occur in the database following the scenario of table 1. these sub-transactions are generated from a given inverted matrix. to generate the frequent 1-itemsets  the apriori algorithm would need to generate 1 different patterns out of the 1 items {a  b  c  d  e  f}. finding the frequency of each pattern and removing the non-frequent ones is necessary before even considering the candidate 1-itemsets. in our approach  itemsets of different sizes are found simultaneously. in particular  for each given frequent 1-itemset we find all frequent k-itemsets that subsume it. for this  a cofi-tree is built for each frequent item except the most frequent one  starting from the least frequent. no tree is built for the most frequent item since by definition a cofi-tree of an item x contains items that are more frequent than x.
모with our example  the first co-occurrence frequent item tree is built for item f. in this tree for f  all frequent items which are more frequent than f and share transactions with f participate in building the tree. the tree starts with the root node containing the item in question  f. for each subtransaction containing item f with other frequent items that are more frequent than f  a branch is formed starting from the root node f. if multiple frequent items share the same prefix  they are merged into one branch and a account for each node of the tree is adjusted accordingly. figure 1 illustrates all cofi-trees for frequent items of table 1. in

figure 1: cofi-trees
figure 1  the round nodes are nodes from the tree with an item label and two counters. the first counter is a support for that node while the second counter  called participationcount  is initialized to 1 and is used by the mining algorithm discussed later. the nodes have also pointers: a horizontal link which points to the next node that has the same itemname in the tree  and a bi-directional vertical link that links a child node with its parent and a parent with its child. the bi-directional pointers facilitate the mining process by making the traversal of the tree easier. the squares are actually cells from the header table as with the fp-tree. this is a list made of all frequent items that participate in building the tree structure sorted in ascending order of their global support. each entry in this list contains the item-name and a pointer to the first node in the tree that has the same item-name.
모notice that the cofi-tree for f  figure 1  is made of only two nodes: the root node containing f and one child node for b with frequency = 1  this is because item f occurs twice only with item b in the database presented in table 1. the same thing happens with item e  but it occurs with item a twice. item c occurs with 1 items  namely a  b and d  and consequently 1 nodes are created as cba: 1 forms one branch with support = 1 for each node in the branch. cda: 1 creates another branch with support =1 for the branch except node c as its support becomes 1  1 . pattern cd: 1 already has a branch built  so only the frequency is updated  c becomes 1  and d becomes 1. finally cb: 1 already shares the same prefix with an existing branch so only counters are updated and thus c becomes 1 and b becomes 1. the d tree is made of one branch as item d occurs once with an item that is more frequent than d  which is in da: 1 in cda: 1. finally item b occurs 1 times with item a from cba: 1  c is ignored in the last two cases as it is less frequent than b and a . the header in each tree  like with fp-trees  constitutes a list of all frequent items to maintain the location of first entry for each item in the cofi-tree. a link is also made for each node in the tree that points to the next location of the same item in the tree if it exists.
모the cofi-trees of all frequent items are not constructed together. each tree is built  mined  then discarded before the next cofi-tree is built. the mining process is done for each tree independently with the purpose of finding all frequent k-itemset patterns that the item on the root of the tree participates in. a top-down approach is used to generate and compute maximum n patterns at a time  where n is the number of nodes in the cofi-tree that is being mined excluding the root node of the tree. the frequency of other sub-patterns can be deduced from their parent patterns without counting their occurrences in the database.
모steps to produce frequent patterns related to the c item for example  are illustrated in figure 1. from each branch of the tree  using the support count and the participation count  candidate frequent patterns are identified and stored temporarily in a list. the non-frequent ones are discarded at the end when all branches are processed. figure 1 shows the frequent itemsets containing c discovered assuming a support threshold greater than 1. mining the  cofi-tree of item c  starts from the most frequent item in the tree  which is item a. item a exists in two branches in the c tree which are  a: 1  b: 1  and c:1  and  a: 1  d: 1  and c: 1 . the frequency of each branch is the frequency of the first item in the branch minus the participation value of the same node. item a in the first branch has a frequency value of 1 and participation value of 1 which makes the first pattern abc frequency equals to 1. the participations values for all nodes in this branch are incremented by 1  which is the frequency of this pattern. in the first pattern abc: 1  we need to generate all sub patterns that item c participates in which are ac: 1 and bc: 1. the second branch that has a generates the pattern adc: 1 as the frequency of a on this branch is 1 and its participation value equals to 1. all participation values on these nodes is incremented by 1. sub-patterns are also generated from the adc pattern which are dc: 1 and ac: 1. the second pattern already exists with support value equals to 1  and only updating its value is needed to make it equal to 1. the second frequent item in this tree   b  exists in one branch  b: 1 and c: 1  with participation value of 1 for the b node.  bc: 1  is produced from this branch and since bc pattern already exists with a frequency value equals to 1  then only its frequency is updated to become 1. finally  the d item is the last item to test as it exists in one branch   d: 1  c: 1  with participation value of 1 for the d node. a pattern dc: 1 is produced and its value is added to the existing dc: 1 pattern to make it dc: 1. finally all non-frequent patterns are omitted leaving us with only frequent patterns that item c participate in. the cofi-tree of item c can be removed at this time and another tree can be generated and tested to produce all the frequent patterns related to the root node.

figure 1: steps needed to generate frequent patterns related to item c
1.	inverted matrix algorithm
모the inverted matrix association rule algorithms are sets of algorithms with the purpose of mining large transactional databases with minimal candidacy generation and reducing the effects of superfluous work. these algorithms are divided among the two phases of the mining process namely the pre-processing in which the inverted matrix is built and the mining phase in which the discovery of frequent patterns occurs.
1	building the inverted matrix
모the inverted matrix is a disk-based data layout that is made of two parts: the index and the transactional array. the index contains the items and their respective frequency. the transactional array is a set of rows in which each row is associated with one item in the index part. each row is made of pairs representing pointers  where each pair holds 1 information: the physical address in the index part of the next item in the same transaction  and the physical address in the row of the next item in the same transaction. building the inverted matrix is done in two passes of the database during the pre-processing phase. the first pass scans the whole database to find the frequency of each item. the item list is then ordered in ascending order according to their frequency. pass two of the database reads each transaction from the database and orders it also into ascending order based on the frequency of each item. in the index part  the location of the first item in the transaction is sought and an entry to its transactional array is added that holds the location of the next item in this transaction. for the second item the same process occurs  in which an entry in the transactional table of the second item is added to hold the location of the third item in the transaction. the same process is repeated for all items in this transaction. the following transaction is read next and the same occurs for all its items. this process repeats for all transactions in the database. algorithm 1 depicts the steps needed to build the inverted matrix.
algorithm 1: inverted matrix  im  construction
input : transactional database  d  output : disk based inverted matrix
method :
pass i
1. scan d to identify unique items with their frequencies.
1. sort the items in ascending order of their frequency.
1. create the index part of the im using the sorted list.
pass ii
1. while there is a transaction t in the database  d  do
1 sort the items in the transaction t into ascending order according to their frequency
1 while there are items si in the transaction do
1.1 add an entry in its corresponding transactional array row with 1- parameters
 a  location in index part of the im of thenext item si+1 in t null if si+1 does not exist.
 b  location of the next empty slot in thetransactional array row of si+1  null if si+1
does not exist.
1 goto 1
1. goto 1
1	mining the inverted matrix
모association rule mining starts by defining the support level . based on the given support  the algorithm finds all frequent patterns that occur more than . the objectives behind the inverted matrix mining algorithm are two fold: first  minimizing the candidacy generation; second  eliminating the superfluous scans of non-frequent items. to accomplish this  a support border is defined. this border indicates where to slice the inverted matrix to gain direct access to those items that are frequent. in other words  the border is the first item in the index of the inverted matrix that has a support greater or equal to . for each item i in the index of the slice of the inverted matrix is considered at a time starting from the least frequent  a co-occurrence frequent item tree for i is built by following the chain of pointers in the transactional array of the inverted matrix. this i-cofi-tree is mined branch by branch starting with the node of the most frequent item and going upward in the tree to identify candidate frequent patterns containing i. a list of these candidates is kept and updated with frequencies of the branches where they occur. since a node could belong to more than one branch of the tree  a participation count is used to avoid re-counting items and patterns. algorithm 1 presents the steps needed to generate the cofi-trees and mining them.
모algorithm 1: creating and mining cofi-trees input: inverted matrix  im  and a minimum support threshold 
output: full set of frequent patterns
method:
1. frequency location = apply binary search on the index part of the im to find the location of the first frequent item based on .
1. while  frequency location   im size  do
1 a = frequent item at location  frequency location 
1 a transactional = the transactional array of item a
1 create a root node for the  a -cofi-tree with frequency-count and participation-count = 1
1 index of transactionalarray = 1
1 while  index of transactionalarray   frequency of item a 
1.1 b = item from transactional array at location
 index of transactionalarray 
1.1 follow the chain of item b to produce sub-transaction c
1.1 items on c form a prefix of the  a -cofi-tree.
1.1 if the prefix is new then
1.1 set frequency-count= 1 and participationcount= 1 for all nodes in the path
.	else
1.1 adjust the frequency-count of the already exist part of the path.
1.1 adjust the pointers of the header list if needed
1.1 increment index of transactionalarray
1.1 goto 1
1 minecofi-tree  a 
1 release  a  cofi-tree
1 increment frequency location //to build the next cofi-tree
1. goto 1
function: minecofi-tree  a 
1. nodea = selectnext node //selection of nodes will start with the node of most frequent item and following its chain  then the next less frequent item with its chain  until we reach the least frequent item in the header list of the  a -cofi-tree
1. while there are still nodes do
1 d = set of nodes from nodea to the root
1 f= frequency-count-participation-count of nodea
1 generate all candidate patterns x from items in d. patterns that do not have a will be discarded
1 patterns in x that do not exist in the a-candidate list will be added to it with frequency = f otherwise just increment their frequency with f
1 increment the value of participation-count by f for all items in d
1 nodea = selectnext node
1 goto 1
1. based on support threshold  remove non-frequent patterns from a candidate list.
모in our previous example in table 1  if  is greater than 1 then the first frequent item will be item c at location 1 in the index part of the inverted matrix. the first element in the transactional array for item c denotes that it shares the same transaction with the item at location 1 which is e. at location  1  we find that the other item a at location 1  shares with them the same transaction. from this  the first child node of c is created holding an entry for item e  and another child node from e is created holding an entry for item a. the frequency of all these items are set to 1 and their participation is set to 1. the second entry of the transactional array of item c is  1   and at location  1  we find an entry of  1 . this means that items e  and a also share another transaction with item c. since entries for these items have already been created in the same order  then there will be no need to create new nodes as we will only increment their frequencies. by scanning all entries for item c with their chain  we can build the c-cofi-tree as in figure 1a. methods in algorithm 1 are applied on the c-cofi-tree to generate all frequent patterns related to c  which are ce:1  ca:1  and cea:1. the c-cofi-tree can be released at this stage  and its memory space can be used for the next tree.

figure 1: cofi-trees  a  item c   b  item e
모the same process happens for the next frequent item that is at location 1  item e . figure 1b presents its cofi-tree which generates the frequent pattern ea:1.
1. experimental evaluations and performance study
모to test the efficiency of the inverted matrix approach  we conducted experiments comparing our approach with a two well-known algorithms namely: apriori and fp-growth. to avoid implementation bias  third party apriori implementation  by christian borgelt   and fp-growth  written by its original authors are used. the experiments were run on a 1-mhz machine with a relatively small ram of 1mb. transactions were generated using ibm synthetic data generator . we conducted different experiments to test the inverted matrix algorithm when mining extremely large transactional databases. we tested the applicability and scalability of the inverted matrix algorithm. in one of these experiments  we mined using a support threshold of 1% transactional databases of sizes ranging from 1 million to 1 million transactions with an average transaction length of 1 items. the dimensionality of the 1 million transaction dataset was 1 items while the datasets ranging from 1 million to 1 million transactions had a dimensionality of 1 unique items. table 1 and figure 1 illustrate the comparative results obtained with apriori  fp-growth and the inverted matrix. apriori failed to mine the 1 million transactional database and fp-tree couldn't mine beyond the 1 million transaction mark. the inverted matrix  however  demonstrates good scalability as this algorithm mines 1 million transactions in 1s  about 1 min . none of the tested algorithms  or reported results in the literature reaches such a big size.
table 1: time needed in seconds to mine different transaction sizes
mining different sizessupport  1% 1m1m1m1m1m1malgorithmaprioritime
in
sec1n/an/an/an/an/afp-tree1n/an/an/an/ainverted matrix111
figure 1: time needed in seconds to mine different transaction sizes
모to test the behavior of the inverted matrix vis-a`-vis different support thresholds  a set of experiments was conducted on a database size of one million transactions  with 1 items and an average transaction length of 1 items. the matrix was built in about 1 seconds and it occupied a size of 1mb on the hard drive. the original transactional database with a horizontal layout uses 1mb. the mining process tested different support levels  which are 1% that revealed almost 1k frequent patterns  1% that revealed nearly 1k frequent patterns  1% that generated 1k frequent patterns and 1 that returned 1k frequent patterns. table 1 reports the time needed in seconds for each one of these runs. the results show that the inverted matrix algorithm outperforms both apriori and fp-growth algorithms in all cases. figure 1 depicts the results of table 1. it is true that there was an overhead cost which was not recorded in table 1  namely the cost of building the inverted matrix. in this particular reported result we meant to focus on the actual mining time. the inverted matrix is built once for all and used to mine with four different support thresholds. the total execution time needed for fp-growth to mine these four cases is 1s  while apriori needed 1s  and the inverted matrix needed only 1s  in addition to the 1s needed to build the matrix on disk. this makes the total execution time for the inverted matrix algorithms about 1s  one third of the time needed by fpgrowth. building the disk-based data structure once and mining it many times by using different supports  saves the overall execution time in comparison with other algorithms. this total time for all runs is illustrated in figure 1. more time improvements could be achieved if more support levels are tested  amortizing the building time over many runs.
notice that given the highly interactive nature of most kdd processes  a  build-once-mine-many  approach is always desirable.
table 1: time needed to mine 1m transactions with different supports level
mining 1m transactionssupport  % 1111algorithmaprioritime
in
sec.11fp-tree11inverted matrix11 apriori	 fp-tree	 inverted matrix
support  % figure 1: time needed to mine 1m transactions with different supports levels

figure 1: accumulated time needed to mine 1m transactions using four different support levels
1.	discussion and future work
모finding scalable algorithms for association rule mining in extremely large databases is the main goal of our research. to reach this goal  we propose a new set of algorithms that uses the disk to store the transactions in a special layout called inverted matrix. it also uses the memory to interactively mine relatively small structures called cofi-trees. the experiments we conducted showed that our algorithms are scalable to mine tens of millions of transactions  if not more. our study reinforces that in mining extremely large transactions; we should not work on algorithms that build huge memory data structures  nor on algorithms that scan the massive transactions many times. what we need is a disk-based-algorithm that can store the massive size and allow random access  and small memory structures that can be independently created and mined based on the available absolute memory size.
모while the results seem promising  there are still many improvements that can be done to further develop the inverted matrix approach. we are currently focusing on building a parallel framework for association rule mining for large-scale data that would use the matrix idea in a cluster context. the improvements we are currently investigating are issues related to the reduction of the inverted matrix size  i.e. compression   the reduction of the number of i/os when building the cofi-trees  the update of the matrix by addition and deletion of transactions  and the parallelization of the construction and mining of the inverted matrix. 1 compressing the size of inverted matrix
모compressing the size of the inverted matrix without losing any data is an important issue that could improve the efficiency of the inverted matrix algorithm. to achieve this  one could merge similar transactions into new dummy ones  or even merge sub-transactions. for example in figure 1a  we can find that the first two transactions contain items a  b  c  d  e and a  e  c  h  g. ordering both transactions as usual into ascending order according to their frequency produces two new transactions  which are d  b  c  e  a and g  h  c  e  a. both transactions share the same suffix  which is c  e  and a. consequently  we can view them as one transaction consisting ofd bg h  c 1  e 1  a 1    where any number between brackets represents the occurrences of the item preceding it. using the same methodology we can find that the inverted matrix can be compressed. the compressed inverted matrix corresponding to figure 1a is depicted in table 1. with such compressed matrix we can dramatically reduce the number of i/os and thus improve further the performance.
table 1: compressed inverted matrix
locindextransactional array11 p 1  1  1 1 f 1  1  1 1 q 1  1  1 1 r 1  1  1 1 j 1  1  1 1 o 1  1  1  1  1 1 d 1  1  1 1 k 1  1  1  1  1 1 l 1  1  1  1  1 1 m 1  1  1  1  1 1 n 1  뷋 뷋  1  1  1 1 i 1  1  1  1  1  1  1 1 g 1  1  1  1  1 1 h 1  1  1  1  1 1 b 1  1  1  1  1 1 c 1  1  1 1 e 1  1  1 1 a 1  뷋 뷋  1 1	reducing the number of i/os needed
모the inverted matrix groups the transactions based on their frequency. frequent items are clustered at the bottom of the inverted matrix. traversing one transaction can be done by calling more than one page from the database. we are currently investigating the possibility of reducing the number of pages read from the database by clustering the same transactions on the same pages at the database level.
1	updateable inverted matrix
모with a horizontal layout  adding transactions is simply appending those transactions to the database. with a vertical layout  each added transaction results in updates in the database entries of all items in the transaction. the inverted matrix is neither horizontal nor vertical but a combination  making the addition of new transactions a complex operation. updateable inverted matrix is an important issue in our research. one of the main advantages of the inverted matrix is that changing the support level does not mean re-scanning the database again. changing the database either by adding or deleting transactions changes the inverted matrix  leading to the need of re-building it again. we are investigating efficient ways to update the inverted matrix without having to rebuild it completely or jeopardizing its integrity.
1	parallelizing the inverted matrix
모the inverted matrix could be built in parallel. each processor could build its own inverted matrix that reflects all transactions on its node in the cluster. the index part of the small inverted matrices would reflect the global frequency of the items in all transactions. building these distributed inverted matrices would also be done using two passes over the local data. the first pass or scan to generate the local frequency for each item. generating the global frequency of each item could be done either by broadcasting or scattering these local supports. the second pass for each local node is almost identical to the second pass of the sequential version  where communication between nodes is minimal. mining the distributed inverted matrices would start by finding the support border based on the given support by the user. each node creates its conditional pattern for each item and sends it to a designated processor for the said item to generate its global conditional pattern  which in turn elicits the frequent patterns.
1.	acknowledgments
모we would like the thank jian pei for providing us with the executable code of the fp-growth program used in our experiments. this research is partially supported by a research grant from nserc  canada.
