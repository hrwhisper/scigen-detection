recent work in deduplication has shown that collective deduplication of different attribute types can improve performance. but although these techniques cluster the attributes collectively  they do not model them collectively. for example  in citations in the research literature  canonical venue strings and title strings are dependent-because venues tend to focus on a few research areas-but this dependence is not modeled by current unsupervised techniques. we call this dependence between fields in a record a cross-field dependence. in this paper  we present an unsupervised generative model for the deduplication problem that explicitly models cross-field dependence. our model uses a single set of latent variables to control two disparate clustering models: a dirichlet-multinomial model over titles  and a non-exchangeable string-edit model over venues. we show that modeling cross-field dependence yields a substantial improvement in performance-a 1% reduction in error over a standard dirichlet process mixture.
categories and subject descriptors
h.1  information systems : database applications- data mining; h.1  information storage and retrieval : information search and retrieval-clustering
general terms
algorithms  experimentation
1. introduction
　deduplication is an important and difficult preprocessing step in knowledge discovery. for example  consider the venue portion of citations in the bibliographies of research papers. a single venue can be named by dissimilar strings-

 
 current address: computer science division  university of california  berkeley  ca 1
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  las vegas  nevada  usa. copyright 1 acm 1-1-1/1 ...$1.
for example  aaai and proceedings of the fourteenth national conference on artificial intelligence  and other variants caused by typographical errors. alternatively  different entities can be denoted by identical strings-for example  iswc is a commonly-used abbreviation both for the international semantic web conference  and for the international symposium on wearable computers. the deduplication problem is to group a set of these noisy strings  which are called mentions  by which underlying entity they refer to. in addition  entities may have attributes  also called fields   such as the title and venue of a research paper  that can be used to improve deduplication. given clean venue data  one can imagine computing many interesting bibliographic measures  such as which venues are most concentrated around a small set of authors  which venues adopt new terminology most frequently  and so on. but such measures will always be suspect unless the deduplication problem is solved well.
　recent work has shown that collective inference-in which many deduplication decisions are made simultaneously and an optimal set of decisions chosen globally-can significantly improve performance over approaches in which individual decisions are made independently. this has been demonstrated in such areas as collective clustering over mentions  1  1  1   collective extraction and deduplication   and collective deduplication of different attribute types  1  1  1  1 . but although some methods compute clusters collectively  there is an important sense in which they do not model clusters collectively. current generative models focus on modeling the manner in which a canonical attribute  such as a paper's true title  is distorted in a noisy observation  such as a citation in a later paper. crucially  however  current models do not incorporate the fact that different attribute types are dependent. for example  research venues tend to focus on specific research areas  and those areas are reflected in the titles of the papers that they publish. we call this a cross-field dependence  because the values of different fields are probabilistically dependent.
　in this paper  we demonstrate the benefits of modeling cross field dependencies in the task of deduplicating research paper venues. we show that modeling the dependence between venue strings and paper titles yields a significant increase in deduplication performance from unlabeled data. in particular  we present a dirichlet process mixture model that uses a single set of mixture components to combine two disparate clustering models: a dirichlet-multinomial mixture for the titles  and a non-conjugate string-edit distortion model for the venues. in this way  each venue has a characteristic distribution not only of venue strings  but also of title strings. this encourages merging venue clusters with similar title distributions  even if their distributions over venue strings are somewhat different.
　the two different distortion models for titles and venues reflect the fact that we expect different kinds of noise in both types of fields. for observed title strings  we expect that many citations will list the canonical title  while others have small  weakly correlated typographical errors. for observed venue strings  on the other hand  the edit distance between coreferent strings is much larger: several words may be added or deleted. furthermore  while it is reasonable to model typos in title strings as independent  in venue strings often several variants appear equally commonly.
　the performance of these models depends crucially on approximate inference. we describe inference based on markov chain monte carlo  mcmc  methods  which are complicated by the nature of the string-edit distortion model that we use for venue strings. in addition  we compare an mcmc sampler based on gibbs sampling to a recently-proposed split-merge sampler   and find that the split-merge sampler performs significantly better.
　we evaluate our models on real-world citation data that is specifically designed to be hard for this task. a model that incorporates cross-field dependence performs substantially better than a standard dp mixture  yielding a 1% reduction in error over a standard dp mixture  and a 1% reduction in error over a reasonable heuristic baseline.
1. model
　in this section  we describe our model of venue and title mentions. each mention m contains a paper's title tm and venue vm  such as from the bibliography of a citing research paper. the task is to determine which venue strings refer to the same underlying venue. the data set as a whole is a set of mentions . each venue mention
vm is a sequence of words  vm1 vm1 ...vm n vm   and each title mention a sequence of words  tm1 tm1 ...tm n tm  . this is an unsupervised problem  in the sense that we are not provided with training mentions which are known to be either duplicates or not.
　we describe our model by incrementally augmenting a simple finite mixture model. all of our models are mixture models in which each mixture component is interpreted as an underlying venue. first  we describe a finite mixture model of the venue mentions only  using a string-edit model customized for this task  section 1 . second  we modify this model to allow an infinite number of components by using a dirichlet process mixture  section 1 . then  we augment this model with title mentions that are drawn from a per-venue unigram model  section 1   modeling a type of cross-field dependence. finally  we describe a venue-title model in which the titles are drawn from a latent dirichlet allocation  lda  model   section 1 .
1 finite mixture model over venues
　first we describe a finite mixture model  where the number of venues c is chosen in advance. the main idea is that each true entity is modeled by a mixture component  where each component generates canonical strings and observed venue strings via a string-edit distortion model. more specifically  the mixture proportions β are sampled from a symmetric dirichlet with concentration parameter α. each cluster c （ {1...c} is associated with a canonical venue string xc  which is sampled from a unigram language model with uniform emission probabilities. for each mention  the model selects a venue assignment cm  which is an index into the set of venues  according to the venue proportions β.
　finally  we generate the venue mentions vm = vm 1 ，，，vm a for each mention of each cluster c. the venue mentions are generated by distorting the venue's canonical string xc = xc 1 ，，，xc b by an hmm string-edit model denoted p vm|xc . note that this model conditions on the canonical string of the cluster. the hmm string-edit model has three edit operations: substitute in which a token of the canonical string is replaced by a token of the observed string  insert which generates a token of vm  and delete which removes a token of xc. each edit operation corresponds to a single state of the hmm. we choose transition probabilities p si = insert|si 1  = p si = delete|si 1  = 1 and p si = substitute|si 1  = 1  so the model disfavors words that occur in only one of the two strings.
　now we describe the emission distributions for each state  that is  the distribution over the tokens that each state inserts into the observed string. the delete state deterministically emits the empty token. the insert state has uniform emission probability over the vocabulary of venue tokens. finally  the substitute state has a custom emission distribution  to model the fact that acronyms are common in venue strings. if vm j is the current venue token and xc i the current canonical token  then the emission distribution is
p vm j|si j = substitute xc i  =
	 	 1
	a xc i 	if xc i starts with vm j
	 1	 1 
	l vm j 	if vm j starts with xc i 
where a w  is the number of words in the vocabulary that are prefixes of w  and l w  is the number of words for which w is a prefix. calculating the probability of a canonical string generating a venue mention requires summing over all sequences of edit states  which can be done efficiently using the the forward algorithm for hmms. in summary  the finite-mixture model is
β ゛ dirichlet α u 
	xc ゛ unigram u 	 1 
cm |β ゛ β
vm |x cm ゛ stringedit xcm  
in this notation  u is a uniform vector over the vocabulary  that is  it is a vector of length v with each element ui = 1/v . here we write the dirichlet distribution with two parameters  a base measure which we choose to be u  and a scalar concentration parameter α  which we choose as described in section 1. the notation unigram denotes a unigram language model. essentially  the prior on canonical strings xc is a uniform distribution over strings. this an improper prior  but the posterior is still well-normalized.
　the graphical model for the finite mixture is shown in figure 1. this model requires choosing the number of venues in advance  which is undesirable. in the next section  we remove this requirement.
1 dirichlet process mixture over venues

figure 1: at left  finite mixture model over venues. the dp mixture is the infinite limit of this model. in middle  venue-title dp mixture  dpvt . at right  venue-title dp mixture with special words  dpvsw .　the finite mixture model requires specifying a number of clusters a priori  which is unrealistic. for this reason  recent work in unsupervised coreference  1  1  1  has focused on nonparametric models  and in particular the infinite limit of  1   which is the dirichlet process  dp  mixture. a dp mixture is attractive for two reasons: first  the number of components of the mixture can be inferred from data; and second  samples from the induced cluster identities display a rich-get-richer property that is natural in many domains. for a review of modeling and inference using dps and dp mixtures  see teh et al. .
　probably the most intuitive way to understand the resulting distribution over venue strings is the chinese restaurant process representation. this is a metaphor for describing how samples are drawn from a dp mixture. here we imagine that each venue mention corresponds to a customer at a restaurant that contains an infinite number of tables. each table c represents a cluster of mentions  in other words  a true venue   and associated with each table is a canonical string xc  the  dish  served at that table .
　suppose we have a set of mentions that have already been generated at tables 1...c  where each venue c contains nc mentions. to generate a new mention  first we generate which table the mention sits at  that is  which true venue is assigned to the mention. this table cm is selected from the
following distribution: 
nc
p cm = c|c1...m 1  『if c is an existing tableαif c = c + 1  i.e.  a new table 1 
the parameter α   1 is a parameter of the dirichlet process  and affects how likely the model is to generate new tables. if the mention does sit at a new table  then we generate a canonical string for the new venue  from a uniform distribution over strings. once that the mention has chosen a table cm  it generates an observed venue string vm from the canonical string xcm at that table. the observed string is generated from the string-edit model of the last section. this completes the description of the model.
　it can be shown that this model is actually the infinite limit of the finite mixture model  as the number of mixture elements goes to infinity. this is the infinite limit of the graphical model shown in figure 1  left .
　to describe this more formally  consider a random variable c that ranges over partitions of the integers {1...m}. the chinese restaurant process defines a distribution over c-to see this  imagine labeling the customers 1...m. denote this distribution as crp α . using this representation we can describe the dp mixture model as c ゛ crp α 
	xc ゛ unigram 1 	for c in 1...|c|	 1 
vm |x cm ゛ stringedit xcm 
in the above  cm refers to the index of the set in the partition c that contains the integer m.
　the advantage of the dp mixture is that it automatically determines the number of clusters from the data. this statement might seem disingenuous  because perhaps we have swapped the problem of selecting the number of clusters for the problem of selecting the parameter α. in practice  however  this is typically not an issue  because usually the number of clusters selected by the model is not sensitive to α  which indeed is the case in our setting  see section 1 .
1 dp mixture over venues and titles
　in the previous section  we presented an infinite mixture model over venue strings. but such a model can be improved dramatically by also considering information from paper titles  and demonstrating this is a key contribution of our work. in this section  we present a model that does this. we will call it the dp venue-title model  dpvt .
　the dpvt model jointly clusters venues and titles using a single set of latent variables that control both a string-edit model for the venues and a dirichlet-multinomial distribution for the titles. each venue c generates a distribution θc over title words  in other words  a probability vector with one element for each word in the vocabulary. every mention m now generates all of its title word tmi by a discrete distribution with parameters θcm.
　this model contains all of the factors in the venue-only model  and in addition:
θc ゛ dirichlet λ u 
tmi |cm {θc} ゛ θcm
as before  u is a uniform probability vector over the vocabulary  and λ is the concentration parameter of the dirichlet. to see how this model incorporates cross-field dependence between venues and titles  consider the graphical model in the middle of figure 1. note that although the canonical venues xc and title distributions θc are independent in the prior  they are dependent in the posterior  because they are coupled by the cm variables once the mentions are observed.
1 dp mixture with venues and special-word title model
　in the first venue-title model  every venue has a multinomial distribution over title words. but we may hope to achieve better performance by using a more flexible model over title strings  for example  one that separates out common words from venue-specific words. also  such a model allows reporting title words that are strongly associated with particular venues  which may be of interest in itself.
　for this reason  in this section we describe an alternative title model in which the titles are generated by latent dirichlet allocation . one topic is dedicated solely to each venue  and a single  general english  topic is shared across all venues. this is a simple version of the special words model of chemudugunta  smyth  and steyvers   so call this model the dp mixture with venues and special-word title model  dpvsw .
　this model includes all of the factors of the venue-only dp model  and in addition:
θg ゛ dirichlet λ1 u 
	θc ゛ dirichlet λ u 	 1 
γc ゛ beta 1 
zmi |c γc ゛ bernoulli γcm 

here θg is a single corpus-wide distribution over title words  while as before each θc is a venue-specific distribution over title words. for each word i of title mention m  this model includes an indicator variable zmi which is 1 if word i was generated from the global distribution θg  and 1 if it was generated from the venue-specific distribution θc. each γc controls how often venue c uses its venue-specific title distribution as opposed to the general distribution. the graphical representation of this model is shown in figure 1.
1. inference
　in this section we discuss two markov chain monte carlo  mcmc  samplers for our models. given a set of observed venue mentions v = {v1 ...vm}  our concern will be to sample from the resulting posterior distribution p {xc} {cm}|v  over venue assignments cm for each mention and canonical strings xc for each venue. a common choice is to resample each cm using a gibbs sampler  but this is not straightforward in our models because the distribution p vm |xcm  is not conjugate to the prior over canonical strings p xcm . in many applications of the dp mixture model  the analogs of those two distributions are conjugate  and in those cases inference is simplified considerably.
1 dp venue model
　first we describe the samplers for the basic dp venue model. the state of the sampler is the set of all cluster indices c = {cm} for each mention m and of canonical strings x = {xc} for each cluster c from 1...c. the main idea is to use a block gibbs sampler  alternating between sampling the cluster identities and the canonical strings.
　the first part of the outer block gibbs sampler is to sample the cluster identities. for this we use a metropolishastings step. we consider two different proposals: one which is only a slight modification of the gibbs sampler  so we call it almost-gibbs  and another based on a split-merge proposal. the gibbs proposal is a modification to neal . for every mention m （ {1...m}  we propose a new cluster c m from the distribution:

is an existing cluster
xm = vm 
	 :	if c m 1（ {1...c}
                                                    1  that is  if the proposed cluster is one that already exists  the proposal is proportional to the prior p c m |c m  times the probability that the canonical string xc m would be distorted into the observed string vm of the current mention. this is exactly the gibbs proposal. if the proposed cluster is new  then the proposal is proportional to the prior times the probability that the observed string would be distorted into itself. this is the part that is different from the gibbs proposal. ideally  we would sample xc m in this case; neal  suggests using the prior p x   but this would lead to a string that would hardly ever be close to vm. here we are mainly interested in finding a high-probability configuration  which makes our choice seem reasonable.
　the second proposal distribution that we use is the splitmerge proposal of dahl . here the basic idea is to pick two mentions m and m1 at random. if the mentions are in different clusters  the proposal merges them. if the mentions are in the same cluster  the proposal splits that cluster into two  one containing m and one containing m1  using a procedure similar to sequential importance sampling.
　the second part of the outer block gibbs sampler is to sample the canonical strings xc. here we use a gibbs step  but with the restriction that xc must be identical to one of the observed venue strings in the cluster. this restriction is a slight abuse  but seems to work well in practice. more specifically  let k in 1...nc index the mentions assigned to cluster c. then the new canonical string x c is sampled from the distribution
                   nc p x c |c v  『 y p vk|x c 1{x c = vk for some k in 1...nc} 
k=1
 1 
where 1{，，，} is an indicator function that enforces the restriction that canonical strings be somewhere observed.
　all the mcmc methods described here require a choice of starting configuration  which can greatly impact their effectiveness. we initialize the samplers by placing each venue mention in its own cluster.
1 venue-title model
　for the venue-title model  we use the same samplers as above  except that when we propose a new cluster assignment cm we must take into account the distribution over title words  integrating out the mean vector θc. denote by p tm|c m t m  the probability of the title mention tm being generated by the proposed cluster  conditioned on the titles of the other mentions in that cluster  and integrating out the distribution θc over title words. this probability can be computed using a polya urn scheme:
 
 1 
where n{tmi=tmj;j i} is the number of words in tm that precede word i and are identical to it; and n{t m c=tmi} is the number of occurrences of the token tmi in the other titles t m c in cluster c; and v the vocabulary size. finally  recall that n tm  is the number of words in the title mention tm. now in the almost-gibbs proposal  we sample a new cluster assignmentfrom the distribution

p c m|c m v  『	 	 	m	j	 
	   :p m	 m p	m	mif c mm（1 {1m...cp }m  m
　　　　　　　　　　　　　　　　　　　　　　　　　 1  the difference from the venue-only version is the inclusion of the term p tm|c m .
1 venue-special words title model
　finally  we describe the modifications to the sampler for the venue-special words title model. for this model  we add to the state of the sampler the indicator variables zm = {zmi}  which for each title word i in mention m  indicate whether the word is to be sampled from the venue multinomial with mean θc or from the general multinomial with mean θg.
　the venue-special words model requires two changes to the sampler for the venue-title model. first  we add a step to the outer block gibbs sampler that resamples all of the z variables given the cluster assignments and canonical strings. as before  let tmi be the title word of mention m in position i; θc tmi  be the element of the title mean vector θc for the word tmi; and θg tmi  be the analogous quantity in the general english multinomial vector. then  during the additional gibbs step  each zmi is sampled from the distribution

where zmi = 1 indicates that tmi is sampled from the venuespecific distribution.
　the second change is that the proposal distribution in the cluster assignment step changes slightly  because now the distribution over titles depends on z. the new proposal distribution is
	1  	 	 
 p cm |c m p vm|cm p tm|cm t m zm  for existing cluster j
	m	 	 	 
	   :p m	 m p	m	m  m	vifmc m p （1tm{|1c...cm zm} .
　　　　　　　　　　　　　　　　　　　　　　　　　 1  observe that this gibbs sampling scheme depends crucially on the fact that it is reasonable to change the cluster identity cm but without changing zm. such a move is in fact reasonable because the semantics of the zm variables do not depend on the venue identity.
1. related work
　the literature on deduplication is extensive.  ironically  deduplication is also known as coreference  record linkage  and identity uncertainty.  a growing body of work has shown that incorporating global information can improve coreference. for example  mccallum and wellner  show that incorporating transitive closure improves performance over making pairwise coreference decisions independently. culotta and mccallum  have applied similar models to venue coreference  finding that jointly modeling coreference of records and fields improves performance. additionally  singla and domingos  1  1  perform simultaneous coreference of authors  papers  and venues using conditional undirected models  and find a similar improvement. these models are all supervised  so our approaches have the advantage of not requiring labeled training data  although they can readily exploit labeled coreference data if it is available.
　several authors have used dp mixture models for deduplication. the general framework of using a per-cluster mixture model for coreference of research papers was introduced by pasula et al. . a more detailed description of a similar model is given by milch . these models generate the number of venues from a log normal distribution. a variant of this model which models the venue assignments with a hierarchical dp was reported by carbonetto et al.   although they do not report a comparison with the log normal model. similarly  bhattacharya and getoor  use a dp mixture model to perform deduplication of authors in research papers.
　none of the models above  however  incorporate cross-field dependencies. for example  in the model of carbonetto et al.  every canonical paper has a true title and distribution over observed title strings  and every canonical author has a distribution over observed author strings. but the model does not represent that canonical authors tend to favor certain words in their canonical titles. similarly  the singla and domingos models  do incorporate the constraint that if two paper mentions are identical  then so are the corresponding venue mentions. but they do not have weights that say if one title appears in a venue  then distinct titles with similar words are likely to also appear in that venue. the key contribution of our work is to explicitly model this cross-field dependence  showing that this leads to dramatically better performance on venue coreference.
　another related model is by haghighi and klein   which applies dp mixtures to noun-phrase corference  which is the problem of determining which noun phrases in a document refer to the same entity  such as george w. bush and he. this work is in similar spirit to ours  in that it augments the
basic dp mixture with additional variables tailored to a specific coreference task. however  the specifics of their model are very different  because they need to model notions such as that pronouns can only refer to entities of a particular gender  and that more salient entities in the discourse are referred to using different language than less salient ones.
1. experiments
　in this section  we compare the performance of the various dp mixture models on citation data. we first obtain a list of automatically extracted citations from the rexa database  http://rexa.info . the citations are first segmented automatically using a conditional random field documents into plaintext. this process is imperfect  so the fields contain extraction errors as well as the expected typographical errors. the data consists of the resulting were mapped onto venuetitle pairs  and duplicate citations  those that were string identical in both fields  were collapsed.
　we choose a dozen test venues  and assemble a corpus of about 1 citations per venue  for a total of 1 citations1. reflecting the coverage of a large-scale digital library  the venues cover a range of topics including: artificial intelligence  machine learning  computational physics  biology  the semantic web  and wearable computing. the venues are also specifically chosen to be hard for the coreference task; for example  several venue pairs are included that have string-identical abbreviations. after removal of stopwords and punctuation symbols  there are 1 unique venue strings. also  in any mention that consists entirely of a string of capital letters  we treat each capital letter as a separate word. this allows the distortion model to more easily align acronyms with their full names.
　we compare four models:  i  the dpv model  which is the dirichlet process mixture only  section 1    ii  the dpvt model  section 1    iii  the dpvsw model  section 1   and  iv  a baseline heuristic str. in this heuristic  first remove stopwords such as  an    of   and  proceedings   then merge string-identical venues  and finally  merge all venue clusters that contain string-identical titles. for each of the generative models we performed 1 iterations of block gibbs sampling. to select the hyperparameters α and λ  we perform a parameter sweep on a separate small validation set.
　we measure performance using the b1 metric of bagga and baldwin . for each mention mi  let ci be the set of predicted coreferent mentions  and ti be the set of truly coreferent mentions. the precision for mi is the number of correct mentions of entity i-that is  those that appear in both ci and ti-divided by the number of mentions in ci. the recall is the number of correct mentions of entity i divided by the size of ti. these are averaged over all mentions in the corpus to obtain a single pair of precision and recall numbers. f1 is the harmonic mean of precision and recall.
1 comparison of models
　coreference performance for each of the four systems is shown in table 1. the baseline str heuristic demonstrates the difficulty of performing coreference on this dataset: string identical mentions are not necessarily coreferent  and different strings often refer to the same venue. the best perfor-
modelprecisionrecallf1dpvt1＼11＼11＼1dpvsw1＼11＼11＼1dpv1＼11＼11＼1str111table 1: percent b1 venue coreference performance for the four systems. the smaller numbers indicate the standard deviation across five independent realizations of the markov chain.

iteration
figure 1: comparison of gibbs and split/merge samplers on the dpvt model.
mance overall is obtained by the dpvt system  where we set the concentration over title unigram distributions to be λ = 1. this setting has the effect of favoring more peaked unigram distributions over title words  where the peaks correspond to words particular to that cluster. these results demonstrate a marked improvement in coreference performance by modeling the titles of the papers. f1 is increased from 1% to 1% by adding title modeling to the dp mixture  a 1% error reduction. the error reduction over the str baseline is 1%.
　the dpvsw model has slightly higher precision than the dvpt model  but at a high cost to recall. also  the data set contains two venues which share the name  iswc   for international semantic web conference and international symposium on wearable computing   which the dpvsw model is able to disambiguate more accurately  as shown in table 1. the standard dirchlet process mixture  on the other hand  will almost always merge identical venue strings. shown in table 1 are some example per-venue distributions over words that were generated by the dpvsw model. while common words such as a and for are highly weighted in these clusters  so are less frequent venue-specific words.
　we test statistical significance using a stratified bootstrap sampler. namely  we bootstrap a confidence interval for the b1 of all the methods by  for each true venue v with nv mentions  we sample nv new mentions from v uniformly with replacement. the performance difference between dpvt and str is highly significant  p   1 .

icdarj. comput. phys.intl. symp. wearable comp.aequationsrealtimedocumentforpositioningrecognitionnumericalahandwrittenandsystemoninwearableandmethodnovelusingwithsensingofofforforthecomputerslineapersonaltable 1: most probable title words for three example clusters. icdar is the international conference on document analysis and recognition.
intl. symp. on wearable comp.intl. semantic web conf.realtime personal positioning system for wearable computers.benchmarking daml+oil repositoriesacceleration sensing glove  asg .triple - a query  inference  and transformation language for the semantic web.table 1: examples of ambiguous acronyms that are correctly disambiguated by the dpvsw model. shown in bold are the canonical strings for the clusters. all of these mentions have the venue string  iswc. 
modelsplit-mergegibbsdpvt1＼11＼1dpvsw1＼11＼1dpv1＼11＼1figure 1: sensitivity of dpvt model to titledirichlet parameter λ.
1 comparison of sampling algorithms
　we also compare the split-merge and gibbs proposal distributions  described in section 1. although in the infinite limit both techniques sample from the same distribution  for finite sample sizes one sampler might converge significantly faster to the posterior. the split-merge sampler that we use  has been relatively recently proposed  and has not to our knowledge been used for dp models of coreference  so it is interesting to see if it performs better than more typical inference algorithms. we measure this by the b1 performance after each iteration of both samplers. this is shown in figure 1. clearly  the samples from split-merge perform much better than those of gibbs sampling. the split-merge sampler at 1 iterations finds venue assignments that are comparable to those of the gibbs sampler at 1 iterations. interestingly  however  the split-merge sampler shows the greatest benefit for the dpvt model  see table 1 . for the other models  the improvement due to the split-merge sampler is modest.
table 1: comparison of b1 f1 performance between the two proposals used in metropolis hastings sampling. means and standard deviations are computed from 1 independent trials.
1 sensitivity to hyperparameters
　there are two hyperparameters that must be tuned in the dp venue-topic models: the strength parameter α of the dp prior  and the concentration parameter λ that controls how similar the per-venue title distributions are across venues. we choose these parameters by parameter sweep on a small development set of 1 mentions and 1 venues.  this is about 1% of the size of the test set.  the results of the parameter sweep are shown in figure 1. the model is somewhat sensitive to the choice of the concentration λ of the title dirichlet prior. it is possible to sample λ based on the training data   so that no labeled validation set is required  but we leave that to future work.
　the model is  however  not sensitive to choice of the dp parameter α  not shown in the figure . over 1 orders of magnitude  taking α （ {1 1 1} yields comparable performance  1  1  and 1 f1 respectively .
1. conclusions and future work
　we present an unsupervised nonparametric bayesian model for coreference of research venues. although related models have been applied to coreference of paper titles and authors  research venues have several unique characteristics that warrant special modeling. by exploiting the fact that research venues have a characteristic distribution over titles  we ob-
tain a dramatic increase in performance on venue coreference. in particular  the model is even able to accurately split up venues that have string-identical abbreviations.
　several directions are available for future work. first  if labeled training data is available  then this model readily lends itself to semi-supervised prediction. this could be necessary to match the performance of discriminative coreference systems. it would also be interesting to extend this model to deduplicate papers and authors.
acknowledgments
this work was supported in part by the center for intelligent information retrieval  in part by lockheed martin through prime contract #fa1-c-1 from the air force office of scientific research  in part by the central intelligence agency  the national security agency and national science foundation under nsf grant #iis-1  and in part by the defense advanced research projects agency  darpa   through the department of the interior 
nbc  acquisition services division  under contract number nbchd1  and afrl #fa1-d-1. any opinions  findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.
