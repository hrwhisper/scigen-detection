the main conclusion from the metrics-based evaluation of video retrieval systems at trec's video track is that non-interactive image retrieval from general collections using visual information only is not yet feasible. we show how a detailed analysis of retrieval results - looking beyond mean average precision  map  scores on topical relevance - gives significant insight in the main problems with the visual part of the retrieval model under study. such an analytical approach proves an important addition to standard evaluation measures.
모we investigate  informally  two aspects of the results of a generative probabilistic image retrieval model on the video track search task: how is image similarity captured and how do the visual results contribute to the map score. we then take a closer look at the ability of the retrieval model to capture both colour and texture information  and investigate the influence of model building initialisation on the retrieval results. we demonstrate that colour is predominant over texture in the current model  once more showing the difficulty in combining evidence from different sources of information. a final experiment demonstrates that  although the model building process is sensitive to its  random  initialisation  this does not harm retrieval results.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval; h.1  database management : database applications-image databases
general terms
experimentation
keywords
multimedia retrieval  gaussian mixture models  results analysis

 
a colour version of this paper is available from
http://www.cwi.nl/ thijs/pub/sigir1.html
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
1. introduction
모in the last few decades  we have seen an enormous interest in the subject of multimedia retrieval. numerous methods and techniques for disclosing image and video archives have been developed and claimed successful. however  these claims are often based on observations in limited domains. often retrieval systems are evaluated on relatively easy datasets  with clear distinctions between several sets of homogeneous images. it is relatively easy to find other examples of say sunsets in such a setting  where the collection consists of clearly defined and distinct sets of for example sunsets  zebras and aeroplanes. we will not argue that experiments in these settings are not useful  however one has to be careful not to draw general conclusions from them.
모in 1  trec started a video track  where the retrieval of video material from general collections is evaluated  1  1 . many of the 'standard' content based image retrieval techniques developed over the last decades did not perform very well on this collection. there was some speculation on the reasons for this lack of success at the trec workshops and in the corresponding papers  e.g.  low quality of data  specificity of queries   but a thorough analysis of the results has never been published. in this paper  we go beyond the mean average precision scores of our own runs and dive into the results to find out what works and what does not. we show that an informal  yet detailed failure analysis is a useful tool for getting insight in the capabilities and weaknesses of a specific retrieval model. the extent to which these findings generalise to other image or video retrieval systems is unknown  but at a minimum the findings can suggest hypotheses for other researchers to investigate. the paper is organised as follows. section 1 describes the retrieval model used. section 1 introduces trec's video track and its results. next  informal results analysis identifies two hypotheses  that are further investigated in section 1. the final section draws conclusions for future research.
1. retrieval model
모the retrieval model we use to rank video shots is a generative model inspired by the language modelling approach to information retrieval  1  1  and a similar probabilistic approach to image retrieval . we present - concisely - the visual part of the model  referring the interested reader to  for more details. the visual model ranks images by their probability of generating the samples  pixel blocks  in query example s   1  1 . the model is smoothed using background probabilities  calculated by marginalisation over the collection. so  a collection image 뷎i is compared to an example image x consisting of n samples  x =  x1 x1 ... xn   by its ability to explain the samples x. the retrieval status value  rsv  of an image 뷎i is defined as:
	rsv 	 1 
where 뷁 is a mixing parameter.
모collection images 뷎i are modelled as mixtures of gaussians with a fixed number of components c  1  1 :
nc
	p x|뷎i  = xp ci c  g x 뷃i c 쑐 c  	 1 
c=1
where nc is the number of components in the mixture model  ci c is component c of class model 뷎i and g x 뷃   is the gaussian density with mean vector 뷃 and co-variance matrix :
 
where n is the dimensionality of the feature space and  x 뷃 t is the matrix transpose of  x   뷃 .
모the samples are 1 by 1 pixel blocks  described by their dct coefficients; the models are trained on these using standard em.
1. trec video track experiments
모the goal of trec's video track is to promote progress in contentbased retrieval from digital video via open  metrics-based evaluation. the video track consists of three separate tasks: shot boundary extraction  feature extraction and search. this paper focuses on the search task and ignores the other two tasks. last year's search task was defined as follows:1
given a multimedia statement of information need and the common shot boundary reference for the search test collection  return a ranked list of 1 shots from the standard set  which best satisfy the need.
a distinction is made between interactive runs  in which a user can interact with a retrieval system to locate relevant shots  and noninteractive runs  in which a user has one go at creating a query from a topic description and then submits this query to the system to retrieve relevant shots; non-interactive runs are called manual runs. since we are interested in how well a multimedia retrieval system can work with a minimum amount of user effort  the paper concentrates on manual runs.
모a video retrieval topic is a short statement of an information need  possibly accompanied by one or more image  video and audio examples. an example topic is shown in figure 1. the search collection consists of over 1 hours of mpeg-1 material from the internet archive1 and the open video project1. an additional 1 hours of material from the same sources was available for training.
1 experimental setup
모since our retrieval model works on still images  we have to select frames to represent the shots. for this  we simply take the middle frame of each shot and use this as the key-frame representing the shot.
모for each representative frame  we build a model in the way described in section 1. this generative model is then assumed to describe the shot. we also build models from the example images

find shots of price tower  designed by frank lloyd wright and built in bartlesville  oklahoma.
figure 1: video topic 1
from the topics. from the mixture components of these models we obtain a clustering of the blocks in each example image. the assignment of the image blocks to the individual components makes it possible to use separate coherent parts of an image as individual queries. in addition to the examples provided with the topics  we have built models from additional examples found using google's image search1.
1 retrieval results
모the results on the non-interactive search task are all rather disappointing. the mean average precision  map  across all participants and all systems is only .1  for interactive .1 . if we ignore runs that used speech recognition  the average score drops further to .1. an observation made by some groups at trec was that content-based techniques have trouble generalising  e.g.   . this becomes apparent when we take a closer look at the queries. four out of the 1 topics contained video examples that were taken from the search collection and most systems were able to re-locate these shots  map for manual systems without asr over these 1 queries is .1 . if we ignore these four topics  map across these systems drops to .1  a decrease of over 1% from the average over all queries . so  one thing we learned from trec is that we can relocate a known item if the known-item itself is the query - not too surprising in our case  given the generative nature of the retrieval model used.
1 queries and relevance judgements
모in video information retrieval  we can distinguish between two different types of information needs: visual and topical. a topical information need is the type we know from text retrieval: somebody is looking for information on a specific topic. in the video retrieval case  the collection to be searched for information on this topic happens to be a visual one  but still  the topic can be purely topical: somebody might be looking for shots of james h. chandler because she wants to know more about this person. in this case she might not care too much about the visual quality of the shot. if however  a user is primarily interested in the visual content itself  e.g. when a producer needs footage of james h. chandler for use in a documentary   the information need becomes visual and shots in which james h. chandler is clearly visible are wanted. since one of the main goals of the video track is to promote progress in content-based retrieval from digital video  it would be useful to focus on visual information needs and corresponding relevance judgements. currently however  shots are regarded relevant even if the searched item is only marginally visible. for example a shot with only a small and hazy golden gate bridge in the background is marked equally relevant as a full pan of the bridge. if somebody searches for visual material of the golden gate bridge  a tiny glimpse of the bridge in the background will not do  just as a brief mention of the bridge in a text document on san francisco is not relevant for somebody looking for information on the golden gate bridge. for this reason  it would be useful to introduce 1 fold visual judgements in the video track  highly-relevant  relevant  irrelevant  or to make an explicit distinction between visually relevant and topically relevant.
1. informal results analysis
모since the scores on trec are so low  especially if we ignore the video-example topics   it is hard to draw conclusions from these experiments other than 'we are not there yet.'. to get a better idea of what content-based retrieval can and cannot do  we forget about improving the 'raw' map scores for now. instead  we investigate  informally  two aspects of experimental results by visually inspecting retrieval results:
  how is image similarity captured by the generative model  looking at the representative frames from the retrieved shots within the top 1 results  and
  how did the visual results contribute to the map score  looking at the representative frames from the retrieved relevant shots within the top 1 results.
we studied in detail these two questions  both for querying using the full image examples and for querying using individual components. the following sections present the  qualitative  observations obtained.
1 shot relevance vs. frame relevance
모since we retrieve shots looking only at one key-frame while the relevance judgements are done looking at full shots  it happens that shots are regarded relevant for a specific query while our representative key-frame is not. according to the relevance judgements  for example  1 relevant shots exist for topic vt1  'find shots of the golden gate bridge' . when we inspect their corresponding key-frames  only 1 of those actually show the bridge. therefore  our retrieval results might benefit from a more clever way to select key-frames and from selecting multiple frames from one shot. a possible approach is to identify the frame most similar to the rest of the shot  often not the middle frame.
1 colour
모although the dct coefficients capture both colour and texture information  it seems that colour plays a  too  important role. the top ranked documents often match the query samples in colour only  even when the samples have a clear and specific texture  like in figure 1; here  if we simply ignore the colour components when calculating results  we retrieve  meaningless  greyscale documents only. apparently  the gaussian mixture models trained on colour frames in the collection are unable to explain the texture in this query  while mixture models trained from greyscale frames are the only ones that capture texture information. a more extensive discussion of this observation and its possible explanation is deferred to section 1.
1 homogeneous queries
모queries composed of image blocks that all belong to the same component are more or less homogeneous: all blocks have more or less the same colour and texture and often there is a clear semantics. at trec  we have seen that such queries sometimes yield better results  i.e.  higher scores  than full example queries. we also noted that we need further analysis to fully understand the role of these components. when we manually inspect the retrieval results for the homogeneous component queries  we see that results are often intuitive  i.e.  there is a visual similarity between the query blocks and the top retrieved documents. if the query samples also have a clear semantics  e.g.  sky   then the results are often useful  figure 1 . sometimes however  a component carries no true semantics. in these cases  results are merely visually similar. figure 1 shows examples of this: looking at the components without the context of the full example  the audience can no longer be identified as such and the grass looks like water. consequently  the results are visually similar but no longer meaningful.
1 heterogeneous queries
모in contrast to image components  full example images are often heterogeneous; they contain many different colours and textures  and often show many different things. if we use a full example image as a query  we observe in many cases that only parts of the presented samples contribute to the score. for example  most images retrieved for a query image showing sky  grass and cows  figure 1  explain the sky samples only. the figure shows for each document a heat-map  indicating how well the document's model explains the individual blocks in the query image. the heat-map is a visual representation of the blocks in the query image  where a colour code is assigned to each block to indicate its probability: lighter colours1indicate higher probabilities  and thus better explanations.
모if we query using the grass samples only  we retrieve the documents shown in figure 1. looking at the heat-maps of these  we see that overall these documents do not explain the grass samples too well  darker colours . note that these heat-maps have been created from the full query example after retrieval  using models of the images retrieved with the grass samples only. some of these images retrieved still explain the sky samples as good as or better than the grass samples. in other words  no document in the collection explains these grass samples well. this might explain why the sky samples prevail.
모another possible explanation of the overemphasis on sky samples is the fact that these are easily explained from any document  i.e.  their background probability is high . now  if we smooth using background probabilities  indeed we find some images with brownish bits near the bottom  but still the sky seems to remain the most important part  figure 1 .
모background probabilities seem to help somewhat  but the predominance of a subset of the samples remains a problem. the current retrieval model looks for documents that explain the set of samples best and ignore the scores for the individual samples. a model which favours documents with balanced individual sample scores might give better retrieval results.
1. further experiments
모more controlled experiments are needed to find out in more detail what determines the visual similarity in the current generative probabilistic model. we have created two smaller collections based on the trec collection  with special characteristics: one to assess
모
q:

top 1:

figure 1: predominance of colour over texture  vt1: 'find shots with microscopic views of living cells' .
q:

top 1:

figure 1: top 1 results for a homogeneous query with clear semantics  'sky' 

	full query	audience component	grass component
top 1 audience:

top 1 grass:

figure 1: top 1 results for homogeneous queries without clear semantics
q:

top 1:

figure 1: top 1 results for cattle query example  뷂 = 1   with heat-maps of the query image  showing probabilities for individual query samples given document q:

top 1:

figure 1: top 1 results for grass samples from cattle query example  뷂 = 1   with heat-maps of query sample probabilities
q:

top 1:

figure 1: top 1 results for cattle query example using background probabilities  뷂 = 1   with heat-maps of query sample
모
probabilities.
the hypothesised emphasis on colour information  and the other to investigate the role of the initialisation of the gaussian mixtures. the experiments presented here focus on the desired behaviour  not using trec topics and relevance judgements.
1 colour
모the following test validates the assumption that colour matching dominates the retrieval results. we build a collection in which we have two copies of each frame: the original colour image and a greyscale version. we use different ways of building models  i.e.  with and without colour information  and use frames from this collection as queries. in addition  we vary the number of gaussian mixture components in the models  since this might influence the possibility to capture different aspects of an image  like colour and texture . we test using 1  1  1 or 1 components  resulting in 1 different settings  1 sets of features 뫄 1 different numbers of components .
모table 1 reports the mean distance between the pairs in the ranked list. the desired behaviour is that this distance equals 1  i.e.  that colour and greyscale versions of the same frame immediately follow each other in the ranking. we differentiate between the distance for the pair with the query frame  mean query pair distance  mqpd  and the distance between any pair  mean pair distance  mpd . the former distance will be more important  since we care mainly about the behaviour at the top of the ranked list. each document from the collection is used as a query  measuring the difference in ranks between the greyscale and the colour versions of the same frames  both for the query frame and for other frames .
모looking at the results for models built without colour information  g1 through g1   we see that the mqpd for these settings is relatively small. this means that the query frame and its counterpart are on average ranked close to each other. obviously  if no colour information is captured in the models  then colour and greyscale versions of the same frame are pretty much the same. the interesting case here is when models can capture both colour and texture information  settings c1 through c1 . comparing the grey q and colour q columns for these settings  we see that colour queries are better at retrieving their greyscale counterpart than vice versa. since we rank documents by their ability to explain the query  this implies that the models built from greyscale images describe their colour counterparts relatively well. apparently  if we start from greyscale images  we can build a model that captures some of the texture information. conversely  models built from colour images are not as good at describing the greyscale variants. if we start from a colour image  the texture information is mostly lost. clearly  the components are fit on colour rather than texture information and this explains why colour often dominates the retrieval results.
모colour models with more components are better at retrieving greyscale images from colour queries. indeed more components can capture more of the texture information in the models of the greyscale frames and thus they can better describe the colour counterparts. however models with many components probably suffer from over-fitting. further research is needed to conclude if colour information can be ignored altogether  or perhaps information from the user is needed to decide whether colour information is important or not.
table 1: average distance between pairs of frames  colour and greyscale versions of the same frame  for different indexing methods  colour  c  an greyscale  g  with varying numbers of components . the table shows mean query pair distance  mqpd   the difference in ranks between query frame  grey or colour  and its counterpart  and mean pair distance  mpd   the distance between any pair. distances are averaged over all queries. we differentiate between greyscale and colour queries.
	indexing method	mqpd	mpd

grey qcolour qgrey qcolour qc1.1.1.1.1c1.1.1.1.1c1.1.1.1.1c1.1.1.1.1g1.1.1.1.1g1.1.1.1.1g1.1.1.1.1g1.1.1.1.1
1 em initialisation
모it is a well known fact that the em algorithm is sensitive to its initialisation. building the gaussian mixture models starts from a random initialisation  thus we may end up with different models if we build two models from the same frame. this might explain why the difference between the ranks of colour and greyscale versions of the same frame is not always 1  even when ignoring colour information in building the models.
모this section investigates the influence of the em initialisation on the final ranking of the documents. to do so  we build a collection with several models for each frame and compare the scores of the different versions on a number of queries. we concentrate on the effects of initialisation on top ranking documents  i.e.  the documents that are most similar to the query.
모we design our collection in such a way that we have different levels of  assumed  similarity:
  we select two videos from the trec-1 collection;
  from each shot in each of the videos we select five frames  evenly distributed over the shot ;
  for each frame we build 1 models  using em from random initialisations .
this way  we can differentiate between exact matches  i.e.  different models of the same frame   frames from the same shot  frames from the same video  and frames from different videos. the middle frames of the shots were used as queries.
모we expect that different models from the same frame generate roughly the same score for each query. the scores for the models of frames from the same shot should not vary much either.
모we calculate scores as follows. we run a query against the collection and rank the results. for each frame in the collection  we compute the average rank  and standard deviation  of all 1 models representing this frame. these scores are then averaged over all frames and over all queries. the results are shown in table 1.
ranksset	mean	std-devframe	1	1shot1	1모if em was insensitive to its initialisation  all models for a given frame would have been exactly the same and they would have been ranked in sequence  yielding the best possible standard deviation table 1: mean rank with standard deviation for different models of the same frame. averaged over all queries and over different sets of frames

figure 1: mean and standard deviation of shot ranks for different queries
for 1 models: 1. the table shows that this is not the case  thus indeed  initialisation influences em. however  on average all 1 models of the query frame are near the top of the ranked list  mean rank 1 std dev. 1 . furthermore  different models of frames from the same shot are on average closer together  and closer to the top of the list  than models from other frames. in fact  the mean rank of a set of frames correlates with the standard deviation of this rank. frames that rank higher are in general closer together. figure 1 shows mean rank and standard deviation for different queries in a single plot. on the x-axis we have all queries sorted by mean shot-rank  i.e.  the mean rank of all models of all 1 frames from the same shot as the query . the solid blue line corresponds to the left y-axis and shows the mean shot-ranks for each query; the green dots correspond to the right y-axis and show the standard deviation in shot-rank  i.e.  the standard deviation in ranks within this set of 1 models of 1 frames from the same shot as the query frame . the plot shows that if the queries get harder  i.e. mean ranks for frames from same shot get higher   the different models of frames from the query shot get more spread out  i.e. standard deviation goes up . we can conclude that although em is sensitive to its initialisation  this mainly has an effect on the lower part of the ranking. the top ranking documents are rather insensitive to differences in the em starting points.
1. conclusions
모we have argued that content based video retrieval has not matured enough for metrics-based evaluation on topical relevance. to get insight in what does and does not work in this field  our deep analysis of results has been a useful alternative. we analysed results informally  which gave us some interesting starting points for further research. two aspects of results produced by our retrieval model were studied more closely: the influence of colour on the ranking and the influence of the em initialisation on retrieval results. the experiments performed confirm the observation that colour information is predominant in the current model setting. again  the combination of evidence from multiple representations  in this case colour and texture information  has proved difficult. future research is needed to identify whether ignoring colour information in our model harms retrieval results  and if representing pixel blocks by something other than dct coefficients diminishes the influence of colour on retrieval results.
모on the positive side  we have shown that retrieval results are not harmed much by the sensitivity of em to its starting point. future research should concentrate on solving the problem with heterogeneous queries  where results seem to be dominated by only a subset of the query-samples.
1. acknowledgements
모the authors thank the anonymous reviewers for their contribution.
