a variety of text processing tasks require or benefit from semantic resources such as ontologies and lexicons. creating these resources manually is tedious  time consuming  and prone to error. we present a new algorithm for using the web to determine the correct concept in an existing ontology to lexicalize previously unknown words  such as might be discovered while processing texts. a detailed empirical comparison of our algorithm with two existing algorithms  cilibrasi & vitanyi 1  maedche et al. 1  is described  leading to insights into the sources of the algorithms' strengths and weaknesses.
1 introduction
semantic resources  such as ontologies and lexicons  are widely used in natural language processing  nlp . automated text processing and understanding  word sense disambiguation  information extraction  speech recognition  and machine translation all require lexical resources. ontologies are used in information retrieval for automatic query expansion  and the word sense disambiguation task can be made easier by domain-specific ontologies that reflect the distribution of word senses in a particular domain. outside the realm of nlp  ontologies are widely used in the semantic web to allow computer programs to  understand  web content.
　acquiring knowledge by hand is extremely costly and time consuming. construction of ontologies such as word net  fellbaum 1   cyc  lenat 1   and edr  yokoi 1  required enormous investments in both time and money. in addition  hand-crafted lexical resources are prone to human errors  biases  and inconsistencies in judgment.
　we propose a novel method for learning lexical semantics. the ontological concept that best captures the lexical semantics of a new word is determined using statistical analysis of web pages containing the word. all experiments reported in this paper use the ontosem ontology  nirenburg & raskin 1   a large ontology and knowledge representation system for language understanding tasks developed by the institute for language and information technologies  ilit . we perform a detailed comparison of the proposed method with two existing methods  and in the process identify the individual components that most significantly impact performance on the task of statistics-based extension of symbolic knowledge for language processing tasks.
　lexical acquisition methods use different sources of data for learning. historically  a static corpus has been used for training purposes  widdows & dorow 1 . recently  the web has become a popular alternative. we compare our algorithm to that of cilibrasi and vitanyi  cilibrasi & vitanyi 1   which uses the web to extract the meaning of words and phrases.
　our work is related to work on learning ontologies. lin  lin 1  constructs an ontology from scratch by identifying similar words in a parsed corpus. agirre et al.  agirre et al. 1  use the web to enrich existing concepts in ontologies. each concept in wordnet  fellbaum 1  is associated with a topic signature  lists of topically related words . to identify the sense of a new word  they build a binary hierarchical clustering of all possible senses  compare the new word's context with the topic signatures for each branch and  similar to a decision tree  choose the highest ranking branch at each step. maedche et al.  maedche et al. 1  determine the class of a new word by employing tree-descending and tree-ascending classification algorithms. our algorithm determines which concept in an existing ontology is the best candidate for the meaning of a new word via tree descending  and is compared to the algorithm of maedche et al.
1 methods
this section describes the proposed approachforlearning lexical semantics and summarizes two existing methods that are used for comparison.
1 proposed approach
the ontosem ontology consists of concepts and taxonomic relations between concepts. to enrich existing concepts in the ontology with new words  we need to find in the hierarchy of concepts the concept that best approximates the meaning of a new word. this is accomplished using the notion of word similarity  for which we use latent semantic analysis  lsa   deerwester et al. 1 .
　to decide the most appropriate concept that a new word should belong to  we need to determine the similarity of the new word with words already lexicalized with the ontology.
to do this  we obtain a training set t of words from the ontology. for each word w  where w is in t or is the new word  document w.txt is created by tokenising the list of definitions retrieved after querying google with  define:w . the resulting document collection is then passed as input to the lsa algorithm. using the world wide web to dynamically build a document collection relevant to a given set of words allows ad hoc word learning.
　words are stopped using smart's stop list  stemmed using porter's stemming algorithm  and filtered based on the logarithm of their term frequencies. a word i with term frequency freqi is included in the term-documentmatrix only if 1+log freqi  − t. rather than represent raw term frequencies of the remaining content words in the term-document matrix  we use tfidf.
　we treat the ontosem ontology similar to a decision tree. starting at the root  we attempt to descend the ontology tree until we reach a leaf concept. at every tree node  the decision of which path to follow is made by choosing the child concept that is most similar to the new word. to compute similarity  for each child of the ontology node we currently are at  we randomly pick a representative set of words that belong to the subtree rooted at the child.
　we then obtain definitions for the words in the representative sets and the word we are trying to learn. for every word  we create a new document that contains a list of definitions for the word. so  at the end  we have a document collection that contains a document for every word that we extracted from the various ontological sub-trees and a document for the new word. we then invoke the lsa algorithm which gives us word and document vectors in the reduced space. we compute cosine similarity scores for the word vectors. finally  the 1-nearest neighborclassifier assigns the new word to the child concept that subsumes the word that was most similar to the new word. the process is repeated until the search reaches a leaf node.
1 automatic meaning discovery using google
this subsection describes a method proposed by cilibrasi and vitanyi  cilibrasi & vitanyi 1  to automatically learn the meaning of words and phrases. intuitively  the approach is as follows. words that are semantically similar will co-occur more than words that are semantically unrelated. conditional probabilities can be used as a measure of the co-occurrence of terms. p x|y  gives the probability of term x accompanying term y while p y|x  gives the probability of term y accompanying term x. these two probabilities are asymmetric. the semantic distance between terms x and y can be obtained by taking the negative logarithm of conditional probabilities p x|y  and p y|x   selecting the one that is maximum and normalizing it by the maximum of log1/p x   log1/p y .

on the web  probabilities of term co-occurrence can be expressed by google page counts. the probability of term x is given by the number of web pages returned when google is presented with search term x divided by the overall number of web pages m possibly returned by google. the joint probability p x y  is the number of web pages returned by google  containing both the search term x and the search term y divided by the overall number of web pages returned by google. the conditional probability p x|y  is defined as p x y /p y .
　we attempt to use the google method to extend the ontosem ontology. once again  we use the tree descending algorithm to assign a new word to a concept. for each child of the ontology node we currently are at  we randomly pick a representativeset of words. we then compute the distance between the new word and every word extracted from the ontological subtrees by submitting queries to google via the java url interface and scraping the page counts from the web pages returned. the decision of which path to follow down the tree is made by choosing the child concept that subsumes a word which is closest to the new word by the distance measure.
1 category based tree descending algorithm
maedche et al.  maedche et al. 1  use a category-based method to determine the child concept that is most similar to a new word. for the ontology node that they are currently at  they gather all hyponyms at most 1 levels below. then then build a generalized class vector for each child concept by adding up all the vector representations of hyponymsgathered that belong to the subtree rooted at the child and normalize the resulting vector to unit length. similarity between the unit vector of a new word and the class vectors is measured by means of three different similarity metrics: jaccard's coefficient  l1 distance  and skew divergence. the child concept whose class vector is most similar to the new word's vector is chosen as the next node in the path down the ontology tree.
　the utility of using the category-based method is that besides reducing computational cost  it summarizes the characteristics of each class by combining multiple prevalent features together  even if these features are not simultaneously present in a single vector.
　we use the maedche method to extend the ontosem ontology. at each ontology node  we gather all hyponyms at most three levels below  build a term-document matrix for the relevant document collection obtained from google define  obtain the normalized class vectors and use the l1 similarity metric to choose the child concept that is closest to the new word.
1 experiments
this section describes the experiments performed in the process of identifying the components that work best for the statistics-based refinement of taxonomic relations. it also describes the experiments conducted to evaluate the performance of the proposed lsa-based tree descending method against the google method and the category-based tree descending method.
　since document w.txt is created by obtaining context for word w  similarity between two words w1 and w1 can be measured by computing similarity between documents w1.txt and w1.txt. in all experiments  we compute similarity between document vectors and not word vectors because we empirically found that using document vectors gave marginally better results than word vectors.
　for all experiments  we performed leave-one-out crossvalidation  that is  one word was held back as the test word and the remaining words constituted the training set. performance was measured in terms of the number of words correctly classified. a word is correctly classified if it is assigned to the correct child concept of an ontology node.
　the value of t  that is  the threshold for filtering out low frequency words was fixed at t=1  so words that occurred fewer than two times in the corpus were filtered out.
1 impact of corpus on the learning task
we wanted to gauge the impact of the corpus on the lsabased tree descending algorithm. to do this  we randomly picked at most fifty words from each of the subtrees rooted at the 1 child concepts of the social-role node in the ontosem ontology  giving a total of 1 words  after removing polysemous words .
　we used the wall street journal corpus to obtain contexts. for each word w  document w.txt was created by extracting a window of width seven words centered on each of the first 1 occurences of w in the corpus. we then replaced the wsj document collection with the google define document collection  while keeping everything else the same. for each word w  document w.txt was created by submitting to google the query  define:w .
　the results of using the google define corpus vs. the wsj corpus are shown in figure 1. accuracies for both corpora are calculated as the number of words correctly classified over the total number of words n that could possibly be classified correctly. for the wsj corpus  n=1 and for the google define corpus  n=1. the discrepancy between n and the number of words originally extracted from the ontology is due to the fact that not all words occur in the wsj corpus or have definitions on the web  or if they do  they occur fewer times than the threshold frequency.
　it can be seen from figure 1 that over all values of k  the number of singular values retained when running lsa  the system performs significantly better when fed with the google define corpus than when it is fed with the wsj corpus. the google define corpus has a best accuracy of 1% for k=1 and k=1 while the wsj corpus has a best accuracy of 1% at k=1.
　thus  the google define corpus dramatically improves the performance of the system because it contains fewer extraneous and irrelevant words that could potentially distract the lsa algorithm.
1 impact of similarity metric
maedche et al. make use of three different similarity metrics: jaccard's coefficient  l1 distance  and skew divergence in their category-based tree descending algorithm. we show that the performance of the category-based tree descending algorithm can be significantly improved by using the cosine similarity metric.
　initially  we implemented the category-basedmethod using the l1 similarity metric because maedche et al. empirically

figure 1: performance of the lsa-based method using the wsj corpus and google define corpus
nodewords classified correctlysocial-role1mammal1animate1object1table 1: results obtained for the category-based method using the l1 metric
show that it gives better results than jaccard's coefficient and skew divergence.
　we carried out experiments at four nodes in the ontology tree. for each node  we created a relevant document collection by picking all words at most three levels below the node and googling for the words' definitions. we measured similarity between the normalized category-based vector representations obtained by summing individual document vectors. table 1 shows the results obtained. each cell in the table shows the numberof words classified correctlyout of the total number of words that could possibly be classified correctly.
　analysis of the results shows that in almost all cases  a new word was found to be similar to the child concept that in comparison to the other child concepts subsumed the fewest number of words. for example  at the social-role node  a vast majority of words were found to be similar to the celebrity concept which had just one word under it  while at the mammal node  most words were found similar to the conceptfreshwater-mammal which had just one word under it. this observation corroborates the findings of maedche et al. in which they observed that most words were assigned to concepts that subsumed few words. given two words and their length n vectors  x and y  the l1 distance between them is given by:

from the definition  we can see that the distance between x and y will be small if the differencebetween xi and yi is small for all i. this means that  if x was a document vector for a
nodewords classified correctlysocial-role1mammal1animate1object1table 1: results obtained for the category-based method using the cosine similarity metric
new word and y was a generalized class document vector  the distance between them would be small if two conditions were satisfied. first  the documents for x and y contained the same words and second  the documents for y as a whole contained few other words. child concepts that subsume few words in the ontology tree are represented by few documents which means fewer words in y as a whole and thus for such concepts  by default  the second condition is satisfied.
　we replaced the l1 metric with the cosine similarity metric  and keeping everything else the same  carried out the experiments at the four ontology nodes again. table 1 shows the results of using the cosine similarity metric. it can be seen that the cosine similarity metric does significantly better than the l1 metric at all four nodes.
1 evaluation of the three methods
this section describes the experiments carried out to evaluate the performanceof the lsa-based tree descending algorithm  the google method and the category-based tree descending algorithm.
　we picked a path in the ontology from the root to a leaf node and carried out experiments at every node on this path. thus  we carried out experiments at the all  object  physical-object  animate  animal  vertebrate  mammal  primate  human  and social-role nodes. the goal was to evaluate the performance of the three methods based on the number of words that were classified correctly at each nodeon the path. a word is classified correctly if it is assigned to the correct child concept.
lsa-based tree descending method
for every ontology node on the path  we randomly picked at most 1 words from each of the subtrees rooted at the child concepts of the node. so  at the all node  we would have words picked from the object  event and property subtrees  at the object node we would have words picked from the physical-object  intangible-object  mental-object  social-object and temporalobject subtrees. thus  each node on the path had a different set of words on which the experiments were carried out. for each set of words  we used cross-validation testing to determine the number of words that were classified correctly.
　to generalize the results over the random element involved in picking words  at each node  we carried out the experiments 1 times  each time picking a different set of words and performing cross-validation testing. we used values of k when running lsa that were empirically good.
nodebaselinelsagooglecategoryall1111object1111phys-obj1111animate1111animal1111vertebrate1111mammal1111primate1111human1111social-role1111table 1: average accuracy  %  of the lsa-based tree descending method  the google method and the category-based tree descending method
google method
as with the lsa-based tree descending method  for every node on the path  we randomly picked at most 1 words from each of the subtrees rooted at the child concepts of the node. thus  at each node we had a different set of words for which we measured the performance of the google method. the experimentswere carried out 1 times at each node  each time with a different set of words.
category-based tree descending method
we carried out the experiments for the category-based tree descending method proposed by maedche et al.  except that we replaced the metrics used by them with the cosine similarity metric. for every node on the path  we gathered all words at most three levels below from each of the subtrees rooted at the child concepts of the node. since there was no random element involved  we carried out the experiments just once at each node.
results
to have a benchmark for the performance of the three methods  a baseline was calculated  which was the number of words classified correctly when a new word was randomly assigned to a child concept. since the probability of getting a word correct by chance depends on the number of child concepts of a given node  the baseline varies with the number of child concepts of a node.
　table 1 shows the results for the three methods. each cell shows the percentage accuracy for a given method  at a given node. for the lsa-based and google methods  this value is the average of the accuracies obtained for each of the 1 runs of the experiment at a given node. since google's estimates of page counts can vary quite a bit for a given search term  the results for the google method are as per the page counts returned at the time of performing the experiments.
　from the table we can see that at all nodes  the categorybased method outperforms the lsa-based and google methods. the lsa-based method does better than the google method at all nodes except at the primate node.
　at every node  we performed a t-test on the accuracy distributions of the lsa-based and google methods using the pub-
yyxabxcdtable 1: contingency table for the co-occurrence of x and y
licly available simple interactive statistical analysis  sisa  package. the t-test results show that the difference in performance of the lsa-based and google methods is statistically significant at all nodes other than at the all and primate nodes.
　similarly  the z-scores for the category-basedmethod show that the difference in performance of the lsa-based and category-based methods is statistically significant at all nodes other than at the animate and animal nodes.
1 analysis
the experimental results show that the category-based tree descending method outperforms the other two methods while the lsa-based tree descending method outperforms the google method. in this section  we attempt to identify the reasons for the superior performanceof the category-based method as well as the flaws in the google method.
1 analysis of the google method
in this subsection  we argue that the google method performs poorly because the ngd measure does not capture the true semantic distance between polysemous terms. given two words x and y  the normalized google distance  ngd  between them is given by:

words x and y will be perceived to be semantically similar if the distance between them is small. that is  the numerator of  1  should be small while the denominator should be large. minimizing the numerator means maximizing both p x|y  and p y|x  while maximizing the denominator means minimizing either p x  or p y .
　table 1 shows a contingency table for the co-occurrence of terms x and y. from the table:
	p x ” y 	a
p x|y  =	= p y 	a + c
	p x ” y 	a
p y|x  =	= p x 	a + b
in order to maximize p x|y  and p y|x   c and b respectively should be minimized. also  from the table 
　　　　　a + b p x  = 
a + b + c + d
　　　　　a + c p y  = 
a + b + c + d
to maximize the denominator  we minimize either p x  or p y . to minimize p x   we need to maximize c and d. however  maximizing c is not possible because c needs to be minimized to minimize the numerator. to minimize p y   we need to maximize b and d. maximizing b is not possible because b should be small to minimize the numerator. thus  in either case  d needs to be maximized.
　it is clear that the google method will say that x and y are similar if three conditions are satisfied. first  x and y cooccur in a certain number of web pages  cell 1   second  the number of web pages in which either of x or y occurred without the other is minimal  cells 1 and 1  and  finally  there are infinitely many web pages in which neither x nor y occurred
 cell 1 .
　the web  by virtue of its unrestricted size satisfies the last condition. however  at the same time  the size of the web can be a problem too. due to the sheer mass of web pages on almost every conceivable topic  it is likely that a word on the web will be used in more than one sense. this means that  even if two words x and y are semantically related  there are likely to be a significant number of web pages in which x is used in a sense not related to y and vice-versa. thus  for almost all pairs of words  the second condition would not be satisfied. for example  the words monk and pope are both religious-roles that a human plays. since the two words are semantically related  they co-occur in a significant number of pages. a google query with the words  monk and pope  retrieves 1 web pages. however  the query  monk and not pope  retrieves 1 1 web pages while the query  pope and not monk  retrieves 1 1 pages.
1 analysis of the category-based method
rather surprisingly  the category-based method  which computes word similarities in the original vector space performs better than the more sophisticated lsa-based method. to identify the reasons for the category-based method performing better than the lsa-based method  we carried out a set of controlled experiments.
　we carried out experiments with the lsa-based method and the category-based method at the physical-object 
animate  animal  mammal and social-role nodes. the category-based method was modified slightly in that  at a given node  at most 1 words were randomly picked from each of the subtrees rooted at the child concepts. this modification was made so that  other than the use of lsa in the first method and category-based vector representations in the second method  the two methods were identical in all respects.
　for the experiments at each node  two control groups were used. the first  control group  is a method that computes word similarity using document vector representations in the original vector space  that is  it neither uses lsa nor category-based vector representations. the second  control group  is a method that computes word similarity using category-based vector representations in the reduced vector space  that is  it uses both lsa and category-based vector representations.
　at each node  we evaluated the performance of the four methods 1 times each  using a different set of words for each evaluation.
　tables 1 through 1 show the results obtained at each node. for each table  the first cell shows the average percentage accuracy for the method that uses both lsa and category-
category categorylsa11 lsa11table 1: results obtained at the physical-object node  k= 1
category categorylsa11 lsa11table 1: results obtained at the animate node  k= 1
based vector representations  the second cell shows the average accuracy for the lsa-based method  the third cell shows the average accuracy for the category-based method and the fourth cell shows the average accuracy for the non-lsa  non category-based method. the values of k at which the experiments were carried out were the  good  values which were empirically chosen.
　by summing the accuracies in the tables row-wise  we can see that  at each node  the non-lsa method does better than the lsa method. summing the accuracies columnwise. we see that  at the physical-object  mammal and social-role nodes  the non-category method seems to be better  while at the animate and animal nodes  the use of category-based vectors seems to give better performance. looking at individual cells  we see that the non-lsa  category-based method does best at all nodes except at the physical-object node. in fact  even the simplistic nonlsa  non category-based method does better than the lsabased method at all nodes except at the social-role node.
1 conclusions
we presented a novel method that used both symbolic knowledge and statistical techniques to learn lexical semantics. we found that using good context dramatically improved the performance of the system. we then used the google method and the category-based tree descending method to perform the task of ontology learning. we discovered that the performance ofthe category-basedtree descendingmethod could be significantly improved using the cosine similarity metric. the modified category-based tree descending method was found to be the best approach for extending an existing ontology with new words. we showed that the google method performed poorly because the ngd measure did not capture the true semantic distance between polysemous terms. we also showed that our proposed method performed poorly because of the lsa algorithm.
category categorylsa11 lsa11table 1: results obtained at the animal node  k= 1
category categorylsa11 lsa11table 1: results obtained at the mammal node  k= 1
category categorylsa11 lsa11table 1: results obtained at the social-role node  k= 1
1 acknowledgments
this research was supported in part by nsf career award 1. we would like to thank the anonymous reviewers for their many suggestions on ways to improve the paper.
