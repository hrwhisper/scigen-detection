in the implementation of hosted business services  multiple tenants are often consolidated into the same database to reduce total cost of ownership. common practice is to map multiple single-tenant logical schemas in the application to one multi-tenant physical schema in the database. such mappings are challenging to create because enterprise applications allow tenants to extend the base schema  e.g.  for vertical industries or geographic regions. assuming the workload stays within bounds  the fundamental limitation on scalability for this approach is the number of tables the database can handle. to get good consolidation  certain tables must be shared among tenants and certain tables must be mapped into fixed generic structures such as universal and pivot tables  which can degrade performance.
　this paper describes a new schema-mapping technique for multi-tenancy called chunk folding  where the logical tables are vertically partitioned into chunks that are folded together into different physical multi-tenant tables and joined as needed. the database's  meta-data budget  is divided between application-specific conventional tables and a large fixed set of generic structures called chunk tables. good performance is obtained by mapping the most heavily-utilized parts of the logical schemas into the conventional tables and the remaining parts into chunk tables that match their structure as closely as possible. we present the results of several experiments designed to measure the efficacy of chunk folding and describe the multi-tenant database testbed in which these experiments were performed.
categories and subject descriptors
h.1  information systems applications : miscellaneous;
h.1  information systems : database management- logical design
general terms
design  performance
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
add features
decrease capital expenditures
decrease operational expenditures
	on-premises software	software as a service
figure 1: software development priorities
1. introduction
　in the traditional on-premises model for software  a business buys applications and deploys them in a data center that it owns and operates. the internet has enabled an alternative model - software as a service  saas  - where ownership and management of applications are outsourced to a service provider. businesses subscribe to a hosted service and access it remotely using a web browser and/or web service clients. hosted services have appeared for a wide variety of business applications  including customer relationship management  crm   supplier relationship management  srm   human capital management  hcm   and business intelligence  bi . idc estimates that the worldwide revenue associated with saas reached $1 billion in 1 and that it will reach $1 billion in 1  representing a compound annual growth rate  cagr  of 1% .
　design and development priorities for saas differ greatly from those for on-premises software  as illustrated in figure 1. the focus of on-premises software is generally on adding features  often at the expense of reducing total cost of ownership. in contrast  the focus of saas is generally on reducing total cost of ownership  often at the expense of adding features. the primary reason for this is  of course  that the service provider rather than the customer has to bear the cost of operating the system. in addition  the recurring revenue model of saas makes it unnecessary to add features in order to drive purchases of upgrades.
　a well-designed hosted service reduces total cost of ownership by leveraging economy of scale. the greatest improvements in this regard are provided by a multi-tenant architecture  where multiple businesses are consolidated onto the same operational system. multi-tenancy invariably occurs at the database layer of a service; indeed this may be the only place it occurs since application servers for highly-scalable web applications are often stateless .
　the amount of consolidation that can be achieved in a multi-tenant database depends on the complexity of the ap-
size of host machine

hcm
complexity of application
figure 1: number of tenants per database  solid circles denote existing applications  dashed circles denote estimates 
plication and the size of the host machine  as illustrated in figure 1. in this context  a tenant denotes an organization with multiple users  commonly around 1 for a small to mid-sized business. for simple web applications like business email  a single blade server can support up to 1 tenants. for mid-sized enterprise applications like crm  a blade server can support 1 tenants while a large cluster database can go up to 1. while the total cost of ownership of a database may vary greatly  consolidating hundreds of databases into one will save millions of dollars per year.
　one downside of multi-tenancy is that it can introduce contention for shared resources   which is often alleviated by forbidding long-running operations. another downside is that it can weaken security  since access control must be performed at the application level rather than the infrastructure level. finally  multi-tenancy makes it harder to support application extensibility  since shared structures are harder to individually modify. extensibility is required to build specialized versions of enterprise applications  e.g.  for particular vertical industries or geographic regions. many hosted business services offer platforms for building and sharing such extensions  1  1  1 .
　in general  multi-tenancy becomes less attractive as application complexity increases. more complex applications like enterprise resource planning  erp  and financials require more computational resources  as illustrated in figure 1  have longer-running operations  require more sophisticated extensibility  and maintain more sensitive data. moreover  businesses generally prefer to maintain more administrative control over such applications  e.g.  determining when backups  restores  and upgrades occur. more complex applications are of course suitable for single-tenant hosting.
1 implementing multi-tenant databases
　in order to implement multi-tenancy  most hosted services use query transformation to map multiple single-tenant logical schemas in the application to one multi-tenant physical schema in the database. assuming the workload stays within bounds  the fundamental limitation on scalability for this approach is the number of tables the database can handle  which is itself dependent on the amount of available memory. as an example  ibm db1 v1  allocates 1 kb of memory for each table  so 1 tables consume 1 mb of memory up front. in addition  buffer pool pages are allocated on a per-table basis so there is great competition for the remaining cache space. as a result  the performance on a blade server begins to degrade beyond about 1 tables.

figure 1: chunk folding
　the natural way to ameliorate this problem is to share tables among tenants. however this technique can interfere with a tenant's ability to extend the application  as discussed above. the most flexible solution is to map the logical tables into fixed generic structures  such as universal and pivot tables . such structures allow the creation of an arbitrary number of tables with arbitrary shapes and thus do not place limitations on consolidation or extensibility. in addition  they allow the logical schemas to be modified without changing the physical schema  which is important because many databases cannot perform ddl operations while they are on-line. however generic structures can degrade performance and  if they are not hidden behind the query transformation layer  complicate application development.
　our experience is that the mapping techniques used by most hosted services today provide only limited support for extensibility and/or achieve only limited amounts of consolidation. in particular  simpler services on the left side of figure 1 tend to favor consolidation while more complex services on the right side tend to favor extensibility.
1 contributions of this paper
　this paper describes a new schema-mapping technique for multi-tenancy called chunk folding  where the logical tables are vertically partitioned into chunks that are folded together into different physical multi-tenant tables and joined as needed. the database's  meta-data budget  is divided between application-specific conventional tables and a large fixed set of generic structures called chunk tables. as an example  figure 1 illustrates a case where the first chunk of a row is stored in a conventional table associated with the base entity account  the second chunk is stored in a conventional table associated with an extension for health care  and the remaining chunks are stored in differentlyshaped chunk tables. because chunk folding has a generic component  it does not place limitations on consolidation or extensibility and it allows logical schema changes to occur while the database is on-line. at the same time  and in contrast to generic structures that use only a small  fixed number of tables  chunk folding attempts to exploit the database's entire meta-data budget in as effective a way as possible. good performance is obtained by mapping the most heavily-utilized parts of the logical schemas into the conventional tables and the remaining parts into chunk tables that match their structure as closely as possible.
　this paper presents the results of several experiments designed to measure the efficacy of chunk folding. first  we characterize the performance degradation that results as a standard relational database handles more and more tables. this experiment is based on a multi-tenant database testbed we have developed that simulates a simple hosted crm service. the workload contains daily create  read  update  and delete  crud  operations  simple reporting tasks  and administrative operations such as modifying tenant schemas.
the experiment fixes the number of tenants  the amount of data per tenant  and the load on the system  and varies the number of tables into which tenants are consolidated.
　our second experiment studies the performance of standard relational databases on oltp queries formulated over chunk tables. these queries can be large but have a simple  regular structure. we compare a commercial database with an open source database and conclude that  due to differences in the sophistication of their query optimizers  considerably more care has to be taken in generating queries for the latter. the less-sophisticated optimizer complicates the implementation of the query-transformation layer and forces developers to manually inspect plans for queries with new shapes. in any case  with enough care  query optimizers can generate plans for these queries that are efficiently processed. a goal of our on-going work is to compare the penalty of reconstructing rows introduced by chunk folding with the penalty for additional paging introduced by managing lots of tables.
　this paper is organized as follows. to underscore the importance of application extensibility  section 1 presents a case study of a hosted service for project management. section 1 outlines some common schema-mapping techniques for multi-tenancy  introduces chunk folding  and surveys related work. section 1 outlines our multi-tenant database testbed. section 1 describes our experiments with managing many tables. section 1 describes our experiments with queries over chunk tables. section 1 summarizes the paper and discusses future work.
1. the case for extensibility
　extensibility is clearly essential for core enterprise applications like crm and erp. but it can also add tremendous value to simpler business applications  like email and project management  particularly in the collaborative environment of the web. to underscore this point  this section presents a case study of a hosted business service called conject .
　conject is a collaborative project-management environment designed for the construction and real estate industries. users of the system include architects  contractors  building owners  and building managers. participants interact in project workspaces  which contain the data and processes associated with building projects. within a project workspace  participants can communicate using email  instant messaging  white boards  and desktop sharing. all discussions are archived for reference and to resolve any subsequent legal disputes. documents such as drawings can be uploaded into a project workspace  sorted  selectively shared among participants  and referenced in discussions. tasks may be defined and assigned to participants and the progress of the project is tracked as tasks are completed. integrated reports can be issued for the control and documentation of a project. requests for bids can be created and bids can be submitted  viewed  and accepted.
　at present  conject has about 1 projects shared by 1 registered participants across 1 organizations. the system maintains 1 tb of documents in a file-based store and 1 gb of structured data in a commercial database. data is maintained in conventional  application-specific tables that are shared across projects  participants  and their organizations. a single database instance on a 1 ghz dualcore amd opteron 1 machine with 1 gb of memory manages the full load of 1 million transactions per year.
　in the future  conject will support more sophisticated business processes such as claim management  defect management  issue management  and decision tracking. such processes can vary greatly across projects and must be specifically designed by the participants. current plans are to allow participants to associate an object with additional attributes  a set of states  and allowable transitions between those states. participants will be able to save  reuse  and share these configurations. to implement these features  the current fixed database schema will have to be enhanced to support extensibility using techniques such as the ones discussed in this paper.
1. schema-mapping techniques
　this section outlines some common schema-mapping techniques for multi-tenancy  introduces chunk folding  and surveys related work. figure 1 illustrates a running example that shows various layouts for account tables of three tenants with ids 1  1  and 1. tenant 1 has an extension for the health care industry while tenant 1 has an extension for the automotive industry.
basic layout. the most basic technique for implementing multi-tenancy is to add a tenant id column  tenant  to each table and share tables among tenants. this approach provides very good consolidation but no extensibility. as a result of the latter  it cannot represent the schema of our running example and is not shown in figure 1. this approach is taken by conventional web applications  which view the data as being owned by the service provider rather than the individual tenants  and is used by many simpler services on the left side of figure 1.
private table layout - figure 1 a . the most basic way to support extensibility is to give each tenant their own private tables. in this approach  the query-transformation layer needs only to rename tables and is very simple. since the meta-data is entirely managed by the database  there is no overhead for meta-data in the data itself  the gray columns in figure 1 . however only moderate consolidation is provided since many tables are required. this approach is used by some larger services on the right side of figure 1 when a small number of tenants can produce sufficient load to fully utilize the host machine.
extension table layout - figure 1 b . the above two layouts can be combined by splitting off the extensions into separate tables. because multiple tenants may use the same extensions  the extension tables as well as the base tables should be given a tenant column. a row column must also be added so the logical source tables can be reconstructed. the two gray columns in figure 1 b  represent the overhead for meta-data in the data itself.
　at run-time  reconstructing the logical source tables carries the overhead of additional joins as well as additional i/o if the row fragments are not clustered together. on the other hand  if a query does not reference one of the tables  then there is no need to read it in  which can improve performance. this approach provides better consolidation than the private table layout  however the number of tables will still grow in proportion to the number of tenants since more tenants will have a wider variety of basic requirements.
　this approach has its origins in the decomposed storage model   where an n-column table is broken up into n 1-column tables that are joined through surrogate values. this model has then been adopted by column-oriented account1	account1
aid name	hospital beds	aid name

1acmest. mary1gumpstate1
1ballaccount1
aid name dealers
1big1 a  private table layout
healthcareaccount
accountext	tenant row hospital beds
1st. mary1state11acme11gump11ball11bigtenant row aid name
automotiveaccount
tenant row dealers
11 b  extension table layout
universal
tenant table col1 col1	col1	col1 col1 col1
11acmest. mary1  11gumpstate1  11ball    11big1    c  universal table layout
pivotint	pivotstr
tenant table col row int	tenant table col row	str

111111111111111
11acme11st. mary11gump11state11ball11big d  pivot table layout
chunkint|str
tenant table chunk row int1	str1
11acme11st. mary11gump11state11ball11big11  e  chunk table layout
accountrow
tenant row aid name
11acme11gump11ball11bigchunkrow
tenant table chunk row int1	str1
11st. mary11state11  f  chunk folding
figure 1: account table and its extensions in different layouts.  gray columns represent meta-data.  databases  1  1   which leverage the ability to selectively read in columns to improve the performance of analytics  and rdf data . the extension table layout does not partition tables all the way down to individual columns  but rather leaves them in naturally-occurring groups. this approach has been used to map object-oriented schemas with inheritance into the relational model .
universal table layout - figure 1 c . generic structures allow the creation of an arbitrary number of tables with arbitrary shapes. a universal table is a generic structure with a tenant column  a table column  and a large number of generic data columns. the data columns have a flexible type  such as varchar  into which other types can be converted. the n-th column of each logical source table for each tenant is mapped into the n-th data column of the universal table. as a result  different tenants can extend the same table in different ways. by keeping all of the values for a row together  this approach obviates the need to reconstruct the logical source tables. however it has the obvious disadvantage that the rows need to be very wide  even for narrow source tables  and the database has to handle many null values. while commercial relational databases handle nulls fairly efficiently  they nevertheless use some additional memory. perhaps more significantly  fine-grained support for indexing is not possible: either all tenants get an index on a column or none of them do. as a result of these issues  additional structures must be added to this approach to make it feasible.
　this approach has its origins in the universal relation   which holds the data for all tables and has every column of every table. the universal relation was proposed as a conceptual tool for developing queries and was not intended to be directly implemented. the universal table described in this paper is narrower  and thus feasible to implement  because it circumvents typing and uses each physical column to represent multiple logical columns.
　there have been extensive studies of the use of generic structures to represent semi-structured data. florescu et al.  describe a variety of relational representations for xml data including universal and pivot tables. our work uses generic structures to represent irregularities between pieces of schema rather than pieces of data.
pivot table layout - figure 1 d . a pivot table is an alternative generic structure in which each field of each row in a logical source table is given its own row. in addition to tenant  table  and row columns as described above  a pivot table has a col column that specifies which source field a row represents and a single data-bearing column for the value of that field. the data column can be given a flexible type  such as varchar  into which other types are converted  in which case the pivot table becomes a universal table for the decomposed storage model. a better approach however  in that it does not circumvent typing  is to have multiple pivot tables with different types for the data column. to efficiently support indexing  two pivot tables can be created for each type: one with indexes and one without. each value is placed in exactly one of these tables depending on whether it needs to be indexed.
　this approach eliminates the need to handle many null values. however it has more columns of meta-data than actual data and reconstructing an n-column logical source table requires  n   1  aligning joins along the row column. this leads to a much higher runtime overhead for interpreting the meta-data than the relatively small number of joins needed in the extension table layout. of course  like the decomposed storage model  the performance can benefit from selectively reading in a small number of columns.
　the pathfinder query compiler maps xml into relations using pivot-like tables . closer to our work is the research on sparse relational data sets  which have thousands of attributes  only a few of which are used by any object. agrawal et al.  compare the performance of pivot tables  called vertical tables  and conventional horizontal tables in this context and conclude that the former perform better because they allow columns to be selectively read in. our use case differs in that the data is partitioned by tenant into well-known dense subsets  which provides both a more challenging baseline for comparison as well as more opportunities for optimization. beckman et al.  also present a technique for handling sparse data sets using a pivot table layout. in comparison to our explicit storage of meta-data columns  they chose an intrusive approach which manages the additional runtime operations in the database kernel. cunningham et al.  present an  intrusive  technique for supporting general-purpose pivot and unpivot operations. chunk table layout - figure 1 e . we propose a third generic structure  called a chunk table  that is particularly effective when the base data can be partitioned into wellknown dense subsets. a chunk table is like a pivot table except that it has a set of data columns of various types  with and without indexes  and the col column is replaced by a chunk column. a logical source table is partitioned into groups of columns  each of which is assigned a chunk id and mapped into an appropriate chunk table. in comparison to pivot tables  this approach reduces the ratio of stored metadata to actual data as well as the overhead for reconstructing the logical source tables. in comparison to universal tables  this approach provides a well-defined way of adding indexes  breaking up overly-wide columns  and supporting typing. by varying the width of the chunk tables  it is possible to find a middle ground between these extremes. on the other hand  this flexibility comes at the price of a more complex query-transformation layer.
chunk folding - figure 1 f . we propose a technique called chunk folding where the logical source tables are vertically partitioned into chunks that are folded together into different physical multi-tenant tables and joined as needed. the database's  meta-data budget  is divided between application-specific conventional tables and a large fixed set of chunk tables. for example  figure 1 f  illustrates a case where base accounts are stored in a conventional table and all extensions are placed in a single chunk table. in contrast to generic structures that use only a small  fixed number of tables  chunk folding attempts to exploit the database's entire meta-data budget in as effective a way as possible. good performance is obtained by mapping the most heavilyutilized parts of the logical schemas into the conventional tables and the remaining parts into chunk tables that match their structure as closely as possible.
　a goal of our on-going work is to develop chunk folding algorithms that take into account the logical schemas of tenants  the distribution of data within those schemas  and the associated application queries. because these factors can vary over time  it should be possible to migrate data from one representation to another on-the-fly.
1. the mtd testbed
　this section describes the configurable testbed we have developed for experimenting with multi-tenant database implementations. the testbed simulates the oltp component of a hosted crm service. conceptually  users interact with the service through browser and web service clients. the testbed does not actually include the associated application servers  rather the testbed clients simulate the behavior of those servers. the application is itself of interest because it characterizes a standard multi-tenant workload and thus could be used as the basis for a multi-tenant database benchmark.
　the testbed is composed of several processes. the system under test is a multi-tenant database running on a private host. it can be configured for various schema-mapping layouts and usage scenarios. a worker process engages in multiple client sessions  each of which simulates the activities of a single connection from an application server's database connection pool. each session runs in its own thread and gets its own connection to the target database. multiple workers are distributed over multiple hosts.
　the controller task assigns actions and tenants to workers. following the tpc-c benchmark   the controller creates a deck of  action cards  with a particular distribution  shuffles it  and deals cards to the workers. the controller also randomly selects tenants  with an equal distribution  and assigns one to each card. finally  the controller collects response times and stores them in a result database. the timing of an action starts when a worker sends the first request and ends when it receives the last response.
1 database layout
　the base schema for the crm application contains ten tables as depicted in figure 1. it is a classic dag-structured oltp schema with one-to-many relationships from child to parent. individual users within a business  a tenant  are not modeled  but the same tenant may engage in several simultaneous sessions so data may be concurrently accessed. every table in the schema has a tenant-id column so that it can be shared by multiple tenants.
　each of the tables contains about 1 columns  one of which is the entity's id. every table has a primary index on the entity id and a unique compound index on the tenant id and the entity id. in addition  there are twelve indexes on selected columns for reporting queries and update tasks. all data for the testbed is synthetically generated.
　in order to programmatically increase the overall number of tables without making them too synthetic  multiple copies of the 1-table crm schema are created. each copy should be viewed as representing a logically different set of entities. thus  the more instances of the schema there are in the database  the more schema variability there is for a given amount of data.
　in its present form  the testbed models the extension table layout with many base tables but no extension tables. this is sufficient for the experiments on schema variability presented in the next section. the testbed will eventually offer a set of possible extensions for each base table.
1 worker actions
　worker actions include crud operations and reporting tasks that simulate the daily activities of individual users. the reporting tasks model fixed business activity monitor-

figure 1: crm application schema

select light  1%  selects all attributes of a single entity or a small set of entities as if they were to be displayed on an entity detail page in the browser.
select heavy  1%  runs one of five reporting queries that perform aggregation and/or parent-child-rollup.
insert light  1%  inserts one new entity instance into the database as if it had been manually entered into the browser.
insert heavy  1%  inserts several hundred entity instances into the database in a batch as if they had been imported via a web
service interface.
update light  1%  updates a single entity or a small set of entities as if they had been modified in an edit page in the browser. the set of entities is specified by a filter condition that relies on a database index.
update heavy  1%  updates several hundred entity instances that are selected by the entity id using the primary key index. administrative tasks  1%  creates a new instance of the 1table crm schema by issuing ddl statements.

figure 1: worker action classes
ing queries  rather than ad-hoc business intelligence queries  and are simple enough to run against an operational oltp system. worker actions also include administrative operations for the business as a whole  in particular  adding and deleting tenants. depending on the configuration  such operations may entail executing ddl statements while the system is on-line  which may result in decreased performance or even deadlocks for some databases. the testbed does not model long-running operations because they should not occur in an oltp system  particularly one that is multitenant.
　to facilitate the analysis of experimental results  worker actions are grouped into classes with particular access characteristics and expected response times. lightweight actions perform simple operations on a single entity or a small set of entities. heavyweight actions perform more complex operations  such as those involving grouping  sorting  or aggregation  on larger sets of entities. the list of action classes  see figure 1  specifies the distribution of actions in the controller's card deck.
　the testbed adopts a strategy for transactions that is consistent with best practices for highly-scalable web applications  1  1 . the testbed assumes that neither browser nor web service clients can demarcate transactions and that the maximum granularity for a transaction is therefore the duration of a single user request. furthermore  since longrunning operations are not permitted  large write requests such as cascading deletes are broken up into smaller independent operations. any temporary inconsistencies that result from the visibility of intermediate states must be eliminated at the application level. finally  read requests are always performed with a weak isolation level that permits unrepeatable reads.
schema number of tenants per total tables variability instances instance

	1	1 1
	1	1	1 1
	1	1	1 - 1 1
	1	1	1 - 1 1
	1	1	1 1

table 1: schema variability and data distribution
1. handling many tables
　the following section describes an experiment with our multi-tenant database testbed which measures the performance of a standard relational database as it handles more and more tables. conventional on-line benchmarks such as tpc-c  increase the load on the database until response time goals for various request classes are violated. in the same spirit  our experiment varies the number of tables in the database and measures the response time for various request classes. the testbed is configured with a fixed number of tenants - 1 - a fixed amount of data per tenant - about 1 mb - and a fixed workload - 1 client sessions. the variable for the experiment is the number of instances of the crm schema in the database  which we called the schema variability in section 1.
　the schema variability takes values from 1  least variability  to 1  most variability  as shown in table 1. for the value 1  there is only one schema instance and it is shared by all tenants  resulting in 1 total tables. at the other extreme  the value 1 denotes a setup where all tenants have their own private instance of the schema  resulting in 1 tables. between these two extremes  tenants are distributed as evenly as possible among the schema instances. for example  with schema variability 1  the first 1 schema instances have two tenants while the rest have only one.
　the experiment was run on a db1 database server with a 1 ghz intel xeon processor and 1 gb of memory. the database server was running a recent enterprise-grade linux operating system. the data was stored on an nfs appliance that was connected with dedicated 1 gbit/s ethernet trunks. the workers were placed on 1 blade servers with a 1 gbit/s private interconnect.
　the experiment was designed in a manner that increasing the schema variability beyond 1 taxes the ability of the database to keep the primary key index root nodes in memory. schema variability 1 has 1 tables  which at 1 kb per table for db1 consumes about 1 mb of memory. the operating system consumes about 1 mb  leaving about 1 mb for the database buffer pool. the page size for all user data  including indexes  is 1 kb. the root nodes of the 1 primary key indexes therefore require 1 mb of buffer pool space. the buffer pool must also accommodate the actual user data and any additional index pages  and the dataset for a tenant was chosen so that most of the tables need more than one index page.
　the raw data collected by the controller was processed as follows. first  the ramp-up phase during which the system reached steady state was stripped off. then rollups of the results were taken across 1 minute periods for an hour  producing two runs. this process was repeated three times  resulting in a total of six runs. the results of the runs were consistent and so only the first run is reported for each value of the schema variability; see table 1.

	metric	schema variability

	1	1	1	1	1

	baseline compliance	 % 	1	1	1	1	1

1% response timeselect light ms 111select heavy ms 11111insert light ms 11111insert heavy ms 11111update light ms 1 1 1 1 1update heavy ms 1 1 1 1 1bufferpool hit ratiodata % 11111index % 11111table 1: experimental results
query percentage	transactions/minute	buffer hit ratio  % 

	1.1.1.1	1.1.1.1	1.1.1.1
	schema variability	schema variability	schema variability
	 a  baseline compliance	 b  database throughput	 c  buffer hit ratios
figure 1: results for various schema variability　the first line of this table shows the baseline compliance  which was computed as follows. the 1% quantiles were computed for each query class of the schema variability 1 configuration: this is the baseline. then for each configuration  the percentage of queries within the baseline were computed. the lower the baseline compliance  the higher the percentage of queries whose response time is above the baseline. per definition  the baseline compliance of the schema variability 1 configuration is 1%. starting around schema variability 1 the high response times are no longer tolerable. the baseline compliance is also depicted in figure 1 a . the second line of table 1 is the database throughput in actions per minute  computed as an average over the 1 minute period. the throughput is also depicted in figure 1 b .
　the middle part of table 1 shows the 1% quantiles for each query class. for the most part  the response times grow with increasing schema variability. we hypothesize that the exceptions occur for the following reasons. first  for low schema variability  there is more sharing among tenants and therefore more contention for longer running queries and tuple inserts. since the heavyweight select queries do aggregation  sorting  or grouping  multiple parallel query instances have impact on each other. the query execution plans show that these queries do a partial table scan with some locking  so the performance for this class degrades. for insert operations  the database locks the pages where the tuples are inserted  so concurrently running insert operations have to wait for the release of these locks. this effect can be seen especially for the lightweight insert operations. second  there is a visible performance improvement for the insert operations at schema variability 1  where the database outperforms all previous configurations. we hypothesize that this behavior is due to the fact that db1 is switching between the two insert methods it provides. the first method finds the most suitable page for the new tuple  producing a compactly stored relation. the second method just appends the tuple to the end of the last page  producing a sparsely stored relation.
　the last two lines of table 1 show the buffer pool hit ratio for the data and the indexes. as the schema variability increases  the hit ratio for indexes decreases while the hit ratio for data remains fairly constant. inspection of the query plans shows that the queries primarily use the indexes for processing. the hit ratios are also depicted in figure 1 c .
　these experimental results are consistent with anecdotal practical experience using mysql and the innodb storage engine. the hosted email service zimbra  initially experimented with a design in which each mailbox was given its own tables . when thousands of mailboxes were loaded on a blade server  performance became unacceptably slow because pages kept getting swapped out of the buffer pool. in addition  when a table is first accessed  innodb runs a set of random queries to determine the cardinality of each index  which resulted in a large amount of data flowing through the system. the performance problems disappeared when tables were shared among tenants.
1. querying chunk tables
　this section describes the transformations needed to produce queries over chunk tables by considering the simpler case of pivot tables. the generalization to chunk tables is straight-forward. this section also discusses the behavior of commercial and open-source databases on those queries.
1 transforming queries
　in the running example of figure 1  consider the following query from tenant 1 over the private table layout:
select beds
	from account1	 q1 
where hospital='state'.
　the most generic approach to formulating this query over the pivot tables pivotint and pivotstr is to reconstruct the original account1 table in the from clause and then patch it into the selection. such table reconstruction queries generally consists of multiple equi-joins on the column row. in the case of account1  three aligning self-joins on the pivot table are needed to construct the four-column wide relation. however in query q1  the columns aid and name do not appear and evaluation of two of the three mapping joins would be wasted effort.
　we therefore devise the following systematic compilation scheme that proceeds in four steps.
1. all table names and their corresponding columns inthe logical source query are collected.
1. for each table name  the chunk tables and the metadata identifiers that represent the used columns are obtained.
1. for each table  a query is generated that filters thecorrect columns  based on the meta-data identifiers from the previous step  and aligns the different chunk relations on their row columns. the resulting queries are all flat and consist of conjunctive predicates only.
1. each table reference in the logical source query is extended by its generated table definition query.
　query q1 uses the columns hospital and beds of table account1. the two columns can be found in relations pivotstr and pivotint  respectively. for both columns  we know the values of the tenant  table  and col columns. the query to reconstruct account1 checks all these constraints and aligns the two columns on their rows:
 select s.str as hospital  i.int as beds
from pivotstr s  pivotint i
where s.tenant = 1
	and i.tenant = 1	 q1account1 
and s.table = 1 and s.col = 1
and i.table = 1 and i.col = 1 and s.row = i.row .
　to complete the transformation  query q1account1 is then patched into the from clause of query q1 as a nested subquery.
　when using chunk tables instead of pivot tables  the reconstruction of the logical account1 table is nearly identical to the pivot table case. in our example  the resulting from clause is particularly simple because both requested columns reside in the same chunk  chunk = 1 :
select beds
from  select str1 as hospital 
int1 as bedsfrom chunkint|str
where tenant = 1
and table = 1
and chunk = 1  as account1 q1chunk where hospital='state'.
　the structural changes to the original query can be summarized as follows.
  an additional nesting due to the expansion of the table definitions is introduced.
