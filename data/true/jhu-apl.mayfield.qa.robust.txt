the johns hopkins university applied physics laboratory  jhu/apl  focused on the robust and 
question answering information retrieval  qair  tracks at the 1 trec conference.  for the 
robust track  we attempted to use the difference in retrieval scores between the top retrieved and the 1th document to predict performance; the result was not competitive.  for qair  we augmented each query with terms that appeared frequently in documents that contained answers to questions from previous question sets; the results showed modest gains from the technique.
haircut
the hopkins automated information retriever for combing unstructured text  haircut   is a document retrieval system developed at the applied physics laboratory. it uses a traditional inverted index  a unigram language model for its similarity metric  1  1   and a flexible tokenizer.  the tokenizer supports words  stems  character n-grams  word n-grams and phrases.  we have focused on language-independent techniques in developing haircut.  it has been evaluated in trec  clef  and ntcir in at least sixteen languages  and routinely performs among the top systems for both monolingual and translingual ad hoc retrieval.
question answering information retrieval track
we were interested in whether we could identify terms that would be indicative of answers to particular types of questions.  for example  it is reasonable to think that words such as famous  himself and died might be found frequently in or near answers to person questions.  if such terms can be accurately identified  they might be used to influence a document's ranking.
we used the trec-1  -1  and -1 questions and judgments to identify indicator terms.  our process begins by identifying a small taxonomy of question types  comprising how  how many  what is  when  where  who and other questions.  to automatically assign a question to its question type  we first parse the question using the charniak parser .  we then use a simple patternmatching approach to map linearized parse trees onto question types.  any question that does not match a pattern is assigned to an 'other' category.
once the training questions are partitioned into question types  we want to use those assignments to identify indicator terms.  we exploited the relevance judgments  qrels  from trecs 1 through 1 for this purpose.  for each question type  we identify two document sets: those that were listed as 'relevant' for questions of that type  and those that appeared in the judgments but that were judged not relevant.  the qa judgments are not binary  but have four relevance values  descriptions taken from the track guidelines :
  incorrect: the answer-string does not contain a correct answer or the answer is not responsive;
  unsupported: the answer-string contains a correct  answer but the document returned does not support that answer;
  non-exact: the answer-string contains a correct answer and  the document supports that answer  but the string contains more than just the answer  or is missing bits of the answer ;
  correct: the answer-string consists of exactly a correct  answer and that answer is supported by the document returned.
for our purposes  we treat correct and non-exact as relevant  and all others as non-relevant.
given these two document sets  judged relevant and judged non-relevant   we want to extract terms that are prominent in the relevant documents but not in the non-relevant documents.  to do so  we first consider each document set separately  extracting words that appear frequently in that set  but relatively infrequently in the collection as a whole.  we use a home-brew 'affinity' statistic for this purpose  but other measures  such as mutual information or the dice coefficient  might work as well.  the result is an ordered list of scored terms.  we then score each term by the difference of its scores in the two sets.  finally  we select the top scoring terms as the expansion terms.
howwhenwherebuiltsingerwarparkdeathnorthscientiststhoughtagoorbitamericarockbecamewarwestnasaspacenearsunhistorymuseumwaterkingacrossfoundwilliambeganthoughtiimilestable 1.  sample augmentation terms for three question categories
we generated augmentation terms for each question type.  sample augmentation terms are show in table 1.  some augmentation terms are clearly sensible  such as north  near and miles for where questions.  others  such as rock and ago  make less intuitive sense.  using retrieved but non-relevant documents to provide terms that should not appear in the augmentation lists eliminates most question-specific terms.  however  there would certainly appear to be a fair amount of noise in these augmentation term lists.  nonetheless  we used the unaltered lists in our experiments. 
to process each question  we first assign it a question type.  we then build a new query  comprising the terms from the original query  plus the expansion terms for the selected question type.  we weight query terms at a ratio of 1 relative to the expansion terms.  this weighting was selected arbitrarily with no experimentation; a more accurate tuning of this parameter might lead to significant additional gains from the technique.  finally  we process the augmented query normally; we used haircut with words as indexing terms  a unigram language model with ¦Á=1  and no blind relevance feedback.
our results showed a modest improvement from the use of this technique.  mean average precision without augmentation was 1  while with augmentation it was 1.  the latter score ranked fifth among the submitted systems.  augmentation produced a 1% relative improvement.  this gain was achieved with a simplistic question type assignment mechanism  and with no tuning of the weighting parameter.  it is reasonable to expect that further gains might be seen if these two weaknesses were addressed.
robust track
in the robust we looked for a simple predictor of retrieval effectiveness.  we use a unigram language model for our similarity metric  and we were curious whether the document scores could tell us anything about performance.  we therefore correlated various combinations of the scores of the top document  the tenth document  and the 1th document  with the average precision of the query  using the trec-1 robust track data as training data.  we found the best correlations when taking the ratio of the score of the 1th document with the score of the top document.  for 1-grams  the correlation was 1  while for words it was 1.  we therefore used this ratio as our predictor for success on each query.
we were also interested in whether the use of phrases mined from the target collection could improve performance.  we used suffix arrays  to identify all high frequency phrases in the document collection.  we deleted leading and trailing closed class words from these phrases  and added the resulting phrase list as additional indexing terms.
we submitted five runs:
  apl1pd: description-only run using phrases.
  apl1prf: description-only run using phrases and blind relevance feedback.
  apl1pt: title-only run using phrases.
  apl1cmb: a combination of a word run  a character 1-gram run  apl1pd and apl1prf.  we used z-score normalization to combine the runs  and weighted the four runs evenly.
  apl1: under the theory that high-performing queries are more likely to benefit from blind relevance feedback  we selected each query's answer from either apl1pd or apl1prf according to whether it was predicted to be in the lower performing half or the upper performing half respectively.  we used the ratio described above to predict into which half a query would fall.
performance was uniformly poor relative to the field.  results are shown in table 1. based on these results  we cannot recommend the simple query difficulty metric we used in these studies.
runmapgeometric mapapl1pd11apl1prf11apl1pt11apl1cmb11apl111table 1.  performance on apl's five robust track runs.
conclusions
using ratios of language model retrieval scores to predict retrieval performance was not particularly effective relative to other techniques described at trec-1; we do not recommend its use.  assigning different document priors based on question type can produce a boost in retrieval performance.  our technique of adding query terms based on the question type is a way to exploit nonuniform document priors in any retrieval system without modifying the index.  our experiments were not finely tuned; our question type assignment was simplistic  and we tried only a single query term weight assignment.  we nonetheless got a performance boost from the technique in the trec1 qair task  as well as in our own cross-validation experiments on the question sets from the three prior trecs.  we therefore recommend the technique as a way to produce a modest boost in retrieval effectiveness for question answering systems.  careful tuning of the approach would likely increase that boost.
