most data mining algorithms require the setting of many input parameters. two main dangers of working with parameter-laden algorithms are the following. first  incorrect settings may cause an algorithm to fail in finding the true patterns. second  a perhaps more insidious problem is that the algorithm may report spurious patterns that do not really exist  or greatly overestimate the significance of the reported patterns. this is especially likely when the user fails to understand the role of parameters in the data mining process. 
data mining algorithms should have as few parameters as possible  ideally none. a parameter-free algorithm would limit our ability to impose our prejudices  expectations  and presumptions on the problem at hand  and would let the data itself speak to us. in this work  we show that recent results in bioinformatics and computational theory hold great promise for a parameter-free datamining paradigm. the results are motivated by observations in kolmogorov complexity theory. however  as a practical matter  they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen or so lines of code. we will show that this approach is competitive or superior to the stateof-the-art approaches in anomaly/interestingness detection  classification  and clustering with empirical tests on time series/dna/text/video datasets.  
categories & subject descriptors 
h.1  database management : database applications - data mining. 
general terms: algorithms  experimentation. 
keywords 
parameter-free data mining  anomaly detection  clustering. 
1. introduction 
most data mining algorithms require the setting of many input parameters. there are many dangers of working with parameterladen algorithms. we may fail to find true patterns because of poorly chosen parameter settings. a perhaps more insidious problem is that we may find patterns that do not exist   or greatly overestimate the significance of a pattern because of a failure to understand the role of parameter searching in the data mining process . in addition  as we will show  it can be very difficult to compare the results across methods or even to reproduce the results of heavily parameterized algorithms.    
data mining algorithms should have as few parameters as possible  ideally none. a parameter-free algorithm prevents us from imposing our prejudices and presumptions on the problem at hand  and let the data itself speak to us. 
in this work  we introduce a data-mining paradigm based on compression. the work is motivated by results in bioinformatics and computational theory that are not well known outside those communities. as we will demonstrate  our approach allows parameter-free or parameter-light solutions to many data mining tasks  including clustering  classification  and anomaly detection. our approach has the following advantages  which we will empirically demonstrate with extensive experiments: 
1  it allows true exploratory data mining  rather than forcing us to impose our presumptions on the data. 
1  the accuracy of our approach can be greatly superior to those of parameter-laden algorithms  even if we allow these algorithms to search exhaustively over their parameter spaces. 
1  our approach is based on compression as its cornerstone  and compression algorithms are typically space and time efficient. as a consequence  our method is generally much more efficient than other algorithms. 
1  many parameterized algorithms require the data to be in a special format. for concreteness  consider time series data mining . here  the euclidean distance requires that the dimensionality of two instances being compared is exactly the same  and dynamic time warping  dtw  is not defined if a single data point is missing . in contrast  our approach works for time series of different lengths  sampling rates  dimensionalities  with missing values  etc. 
in this work  we decided to take the unusual step of reproducing our entire actual code  rather than just the pseudocode. there are two reasons for doing this. first  free access to the actual code combined with our policy of making all data freely available allows independent confirmation of our results. second  it reinforces our claim that our methods are very simple to implement.  the rest of the paper is organized as follows. in section 1  we discuss the results in bioinformatics and computational theory that motivate this work. in section 1  we consider the minor changes and extensions necessary to extend these results to the classic data 

permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd '1  august 1  1  seattle  washington  usa. 
copyright 1 acm 1-1/1...$1. 
 
mining tasks of anomaly/interestingness detection  classification  and clustering. section 1 sees an exhaustive empirical comparison  in which we compare dozens of algorithms to our approach  on dozens of datasets from several domains  including time series  video  dna  and text. finally  in section 1  we discuss many avenues for possible extensions. 
1. background and related work 
we begin this section by arguing that a contribution made by a parameter-laden algorithm can be difficult to evaluate. we review some background material on kolmogorov complexity  which motivates the parameter-free compression-based dissimilarity measure  cdm   the technique at the heart of this paper.  
1 the perils of parameter-laden algorithms  
a recent paper in a top-tier journal introduced a new machinelearning framework and noted that it  ...abounds with parameters that can be tuned   our emphasis . it is surely more accurate to state that the approach has parameters that must be tuned. when surveying the literature on this topic  we noted that while there are many techniques for automatically tuning parameters  many of these techniques themselves have parameters  possibly resulting in an infinite regression. 
an additional problem of parameter-laden algorithms is that they make it difficult to reproduce published experimental results  and to understand the contribution of a proposed algorithm. a recently published paper introduced a new time series distance measure. the algorithm requires the setting of two parameters  and the authors are to be commended for showing the results of the cross-product: sixteen by four possible parameter choices. of the sixty-four settings  eleven are slightly better than dtw  and the authors conclude that their approach is superior to dtw. however  the authors did not test over different parameters for dtw  and dtw does allow a single parameter  the maximum temporal distortion  the  warping window   . the authors kindly provided us with the exact data they used in the experiment  and we reproduced the experiment  this time allowing a search over dtw's single parameter. we discovered that over a wide range of parameter choices  dtw produces a near perfect accuracy  outperforming all sixty-four choices of the proposed algorithm.  
although the above is only one anecdotal piece of evidence  it does help make the following point. it is very difficult to evaluate the contribution of papers that introduce a parameter-laden algorithm. 
in the case above  the authors' commendable decision to make their data public allows the community to discover that dtw is probably a better distance measure  but only at the expense of some effort on the readers' behalf. in general  the potential asymmetry in parameter tuning effort effectively prevents us from evaluating the contribution of many papers.  here  the problem is compounded by the fact that the authors created the dataset in question. creating a dataset may be regarded as a form of meta parameter tuning  since we don't generally know if the very first dataset created was used in the paper  or many datasets were created and only the most satisfactory one was used. in any case  there are clearly problems in setting parameters  training  and reporting results  testing  on the same dataset . in the field of neural networks  flexer  noted that 1% of papers did just that. while no such statistics are published for data mining  an informal survey suggests a similar problem may exist here. in section 1.1  we will empirically reinforce this point by showing that in the context of anomaly detection  parameter-laden algorithms can have their parameters tuned to achieve excellent performance on one dataset  but completely fail to generalize to a new but very similar dataset.  
before leaving this section  it would be remiss of us not to note that many papers by the authors of this manuscript also feature algorithms that have  too  many parameters. indeed  the frustration of using such algorithms is one inspiration for this work. 
1 kolmogorov complexity 
the proposed method is based on the concept of kolmogorov complexity. kolmogorov complexity is a measure of randomness of strings based on their information content. it was proposed by a.n. kolmogorov in 1 to quantify the randomness of strings and other objects in an objective and absolute manner. the kolmogorov complexity k x  of a string x is defined as the length of the shortest program capable of producing x on a universal computer - such as a turing machine. different programming languages will give rise to distinct values of k x   but one can prove that the differences are only up to a fixed additive constant. intuitively  k x  is the minimal quantity of information required to generate x by an algorithm. 
hereafter  we will follow the notation of   which was the main inspiration of this work.  the conditional kolmogorov complexity k x|y  of x to y is defined as the length of the shortest program that computes x when y is given as an auxiliary input to the program. the function k xy  is the length of the shortest program that outputs y concatenated to x. 
in   the authors consider the distance between two strings x and y  defined as 
 	dk  x  y  = k x | y  +k y | x  	 1  
k xy 
which satisfies the triangle inequality  up to a small error term.  a more mathematically precise distance was proposed in . 
kolmogorov complexity is without a doubt the ultimate lower bound among all measures of information content. unfortunately  it cannot be computed in the general case . as a consequence  one must approximate this distance. 
it is easy to realize that universal compression algorithms give an upper bound to the kolmogorov complexity. in fact  k x  is the best compression that one could possibly achieve for the text string x. given a data compression algorithm  we define c x  as the size of the compressed size of x and c x|y  as the compression achieved by first training the compression on y  and then compressing x. for example  if the compressor is based on a textual substitution method  one could build the dictionary on y  and then use that dictionary to compress x. 
we can approximate  1  by the following distance measure 
 	dc  x  y  =c x | yc  +xyc  y | x  	 1  
the better the compression algorithm  the better the approximation of dc for dk is. 
in   li et al. have shown that dc is a similarity metric  and can be successfully applied to clustering dna and text. however  the measure would require hacking the chosen compression algorithm in order to obtain c x|y  and c y|x . we therefore decided to simplify the distance even further. in the next section  we will show that a simpler measure can be just as effective. 
