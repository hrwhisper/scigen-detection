the objective of this paper is to present a new technique for computing term weights for index terms  which leads to a new ranking mechanism  referred to as set-based model. the components in our model are no longer terms  but termsets. the novelty is that we compute term weights using a data mining technique called association rules  which is time efficient and yet yields nice improvements in retrieval effectiveness. the set-based model function for computing the similarity between a document and a query considers the termset frequency in the document and its scarcity in the document collection. experimental results show that our model improves the average precision of the answer set for all three collections evaluated. for the trec-1 collection  our set-based model led to a gain  relative to the standard vector space model  of 1% in average precision curves and of 1% in average precision for the top 1 documents. like the vector space model  the set-based model has time complexity that is linear in the number of documents in the collection.
categories and subject descriptors
h.1  information systems : information search and retrieval- retrieval models; h.1  information systems : database applications-data mining; f.1  mathematical logic and formal languages : mathematical logic-set theory
general terms
algorithms  measurement  performance  experimentation
keywords
information retrieval models  closed association rule mining  weighting index term co-occurrences  data mining  set theory
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  tampere  finland.
copyright 1 acm 1-1/1 ...$1.
1. introduction
　in the area of information retrieval  one popular model to locate answers that humans judge to be relevant is the vector space model. in the vector space model query terms and documents are represented as weighted vectors in a vector space  and the answers to a user query are the documents whose representative vectors have highest similarity to the query vector. the computation of similarities is based on weights assigned to the index terms that compose document and query vectors. much of the success of the model is due to the long-term research efforts of salton and his associates  1  1 .
　the term weights can be calculated in many different ways and finding good term weights is a continuing challenge. the best known term weighting schemes use weights that are function of  i  the number of times an index term occurs in a document and  ii  the number of documents of the collection an index term occurs. such term-weighting strategies are called tf 〜idf  term frequency times inverse document frequency  schemes  1  1 .
　in this paper we propose a new model for computing index term weights  based on set theory  and for ranking documents. this model is referred to as set-based model. the set-based model uses a term weighting scheme based on association rules theory . association rules are interesting because they provide all the elements of the tf 〜 idf scheme in an algorithmically efficient and parameterized approach. also  they naturally provide for quantification of representative term co-occurrence patterns  something that is not present in the tf 〜 idf scheme.
　we evaluated and validated the set-based model through experimentation using three test collections. we compared our model against the vector space model and the generalized vector space model . our experimental results show that the set-based model yields higher retrieval performance  which is significantly superior for all the collections considered. for the trec-1 collection   containing 1 megabytes of text and approximately 1 million documents  the set-based model yields average precision curves that are 1% higher than the vector space model. when only the first 1 answers are considered  this margin goes up to 1%. for the wall street journal collection  containing 1 megabytes of text and 1 documents  the set-based model  in comparison with the generalized vector space model  is approximately 1% superior in average precision and approximately 1% superior in precision at the first 1 documents in the answer.
　the set-based model is also competitive in terms of computing performance. both the vector space and the set-based models are linear on the number of documents in the collection. the vector space model is o tn   where t is the size of the vocabulary collection and n is the number of documents in the collection. the setbased model is o cn   where c is the number of closed termsets  a value that is o 1|q|   where |q| is the number of terms in the query. these are worst case measures and the average measures for the constants involved are much smaller. for the trec-1 collection  using the 1 range of topics  where average number of terms in the query is 1  the increase in the average response time for the set-based model over the vector space model is approximately 1%. these are relevant facts from both theoretical and practical points of view  considering that we introduced a new and more complex model for weighting index terms  which will often improve the quality of the results.
　the remaining of the paper is organized as follows. in the next section we discuss some related work. section 1 describes the method for computing co-occurrences among query terms based on a variant of association rules. the set-based model is presented in the section 1. in section 1 we describe the reference collections and the experimental results comparing the vector-space model  the generalized vector-space model  and our model. finally  we present some conclusions and future work in section 1.
1. related work
　the vector space model was proposed by salton  1  1   and different weighting schemes were presented  1  1  1 . in the vector space model  index terms are assumed to be mutually independent. the independence assumption leads to a linear weighting function which  although not necessarily realistic  is ease to compute. simple term weighting was used early on by salton and lesk . spark jones introduced the idf factor   and salton and yang verified its effectiveness for improving retrieval .
　different approaches to account for co-occurrence among index terms during the information retrieval process have been proposed. raghavan and yu  use a statistical analysis of a set of queries in relation to relevant and non-relevant document sets to establish positive and negative correlations among index terms. the work by van rijsbergen  constructs a probabilistic model incorporating dependence among index terms  and experimental results were later presented in a companion paper . the extent to which two index terms depend on one another is derived from the distribution of co-occurrences in the whole collection or in the relevant and non-relevant sets  leading to a non-linear weighting function. as shown in   the resulting formula for computing the dependency factors in  1  1  does not seem computationally feasible  even for a relatively small number of index terms. the work in  presents a study of the necessity of term dependence in a query space for weighted based retrieval.
　the work in  1  1  presents an interesting approach to compute index term correlations based on automatic indexing schemes  defining a new information retrieval model called generalized vector space model. the work shows that index term vectors can be explicitly represented in a 1t-dimensional vector space such that index term correlations can be incorporated into the vector space model in a straightforward manner. it represents index term vectors from a basic set of orthogonal vectors called min-terms  where each min-term represents a pattern of index term co-occurrence inside documents. the vector representing an index term ki is obtained by adding all min-terms related to ki. the model is not computationally feasible for moderately large collections because there are 1t possible min-terms  where t is the number of terms in the collection vocabulary. extensions of the generalized vector space model were used in  1  1 .
　our work differs from the above related works in the following aspects. first  determination of index term weights is derived directly from association rules theory  which naturally considers representative patterns of term co-occurrence. second  experimental results show significant and consistent improvements in average precision curves in comparison to the standard vector space model and to the generalized vector space model. this improvement in average precision does not always occur in the generalized vector space model because exhaustive application of term co-occurrences to all documents in the collection might eventually hurt the overall effectiveness and performance. third  the set-based model algorithm has computational costs that are linear on the number of documents in the collection. thus  the implementation of our model is practical and efficient  with processing times close to the times to process the standard vector space model.
　the work in  introduces a theoretical framework for the association rules mining based on a boolean model of information retrieval known as boolean retrieval model. our work differs from that presented in  in the following way. in our work we use association rules to define a new information retrieval model that provides not only the main term weights and assumptions of the tf 〜 idf scheme  but also provides for quantification of term cooccurrence.
1. closed termsets
　in this section we introduce the concept of closed termsets as a basis for computing term weights. further  we analyze how closed termsets can be generated from a document inverted list and briefly analyze its computational costs.
1 concepts
　let t = {k1  k1  ...  kt} be the vocabulary of a collection of documents d  that is  the set of t unique terms that may appear in a document from d. there is a total ordering among the vocabulary terms  which is based on the lexicographical order of terms  so that ki   ki+1  for 1 ＋ i ＋ t   1. we define an n-termset s as an ordered set of n unique terms  such that s   t and the order among terms follows the aforementioned total ordering. let s = {s1  s1  ...  s1t} be the vocabulary-set of a collection of documents d  that is  the set of 1t unique termsets that may appear in a document from d. each document j from d is characterized by a vector in the space of the termsets.
　with each termset si  1 ＋ i ＋ 1t  we associate an inverted list lsi composed of identifiers of the documents containing that termset. we also define the frequency dsi of a termset si as the number of occurrences of si in d  that is  the number of documents where si   dj and dj （ d  1 ＋ j ＋ n. the frequency dsi of a termset si is the length of its associated inverted list  | lsi | .
　definition 1. a termset si is a frequent termset if its frequency dsi is greater than or equal to a given threshold  which is known as support in the scope of association rules   and refered as minimal frequency in this work.
　definition 1. a closed termset csi is a frequent termset that is the largest termset among the termsets that are subsets of csi and occur in the same set of documents. that is  given a set d   d of documents and the set sd   s of termsets that occur in all documents from d and only in these  a closed termset csi satisfies the property that sj （ sd|csj   sj.
　definition 1. a maximal termset msi is a frequent termset that is not a subset of any other frequent termset. that is  given the set sf   s of frequent termsets that occur in all documents from d  a maximal termset msi satisfies the property that sj （ sf|msi   sj.
　for sake of information retrieval  closed termsets are interesting because they represent a reduction on the computational complexity and on the amount of data that has to be analyzed  without loosing information  since all termsets in a closure are represented by the respective closed termset.
　it is proven that the set of maximal termsets associated with a document collection are the minimum amount of information necessary to derive all frequent termsets associated with a collection . however  they do not embed the same amount of information that closed termsets do  since the occurrence pattern of each termset is discarded  making difficult its use for finding interesting term co-occurrences.

figure 1: sample document collection
　next we present a simple example to illustrate the concepts of frequent  closed and maximal termsets. consider a vocabulary of five terms t = {a  b  c  d  e}  and a collection d of six documents dj  1 ＋ j ＋ 1  that contain these terms  i.e.  d = { a b c e    c d e    a b c e    a c d e    a b c d e    b c d }  as depicted in figure 1. table 1 shows all frequent and closed termsets for the sample document collection and their respective frequencies. the minimum frequency used for selecting the frequent termsets is equal to 1%  that is  a termset is frequent if it appears in at least three of the six documents. notice that the number of frequent termsets  although potentially very large  is usually small in natural language texts. this number is a function of the minimum frequency and document collection characteristics. it is possible to vary the number of frequent termsets by changing the minimum frequency employed. regarding the closed termsets  even in this small example  we can see that the number of closed termsets is significantly smaller than the number of frequent termsets. in our example  there are 1 frequent termsets and just 1 closed termsets  and 1 maximal termsets.
　a major advantage of using closed termsets instead of frequent termsets is that they can be generated very efficiently  as proved in . so far  there is just an upper bound on the number of frequent termsets  which is o nr1l   where n is the number of documents in the collection  r is the number of maximal frequent termsets  and l is the length of the largest frequent termset. in practice  since the query processing is driven by the query  the largest frequent termset is bounded by the number of terms in the query. since the number of closed termsets is at most equal to the number of frequent termsets  we may also use this bound as an upper limit of the number of closed termsets. as we show in section 1  in practice the number of closed termsets is significantly smaller than table 1: frequent closed  and maximal termsets for the sample document collection
frequency  ds frequentclosedmaximaltermsetstermsetstermsets1%  1 cc1%  1 e cece1%  1 a ac ae aceace1%  1 cdcd1%  1 b bcbc1%  1 d cdcd1%  1 ab abc abe be bce abceacbeabce1%  1 de cdecdecdethe number of frequent termsets. for our example  the upper bound of the number of frequent termsets is onr1l = 1   1   1 = 1. however  its total number was 1  which is significantly smaller than its upper bound. regarding the closed termsets  we can see that its number  1  is smaller than the number of frequent termsets.
1 algorithm description
　determining closed termsets is a problem very similar to mining association rules and the algorithms employed for the latter are our starting point . our approach is based on an efficient algorithm for association rule mining  called charm   which has been adapted to handle terms and documents instead of items and transactions  respectively.
　the algorithm for generating termsets is basically an incremental enumeration algorithm that employs a very powerful pruning strategy. more specifically  we start by verifying whether the frequency of 1-termsets is above a given threshold. this approach is based on the fact that  if a term is not frequent  none of the higher-order termsets that include it will be. these frequent 1-termsets will also be defined as closed. we then start grouping frequent termsets and verifying two criteria that are used to determine whether these new termsets are closed or not.
　we determine n + 1 termsets from n termsets  as follows. the process starts by determining all pairs of termsets  si sj   1 ＋ i j ＋ 1t  that have the same first n   1 terms. a new termset is determined by the union of these sets  si “ sj   creating an n + 1 termset snew. the list of documents where snew appears can be easily determined by intersecting the lists of the two concatenated sets  lnew = li ” lj . after this  the first criterion that verifies whether the new termset is frequent  and was introduced by the original apriori algorithm   is employed. an n-termset may be frequent only if all of its n   1-termsets are also frequent. for instance  we may enumerate the 1-termset  {a b c e}  based on the termsets {a b c} and {a b e} and we apply the frequency criterion by verifying whether the other 1-termsets of the new termset  in this case {a b e} and {b c e}  are also frequent. next  the inverted list associated with the new termset is generated and it is verified whether its length is above threshold.
　the second criterion checks whether a frequent termset f being verified is closed. a termset f is defined as closed if there is no closed termset that is also a closure of f. in this case  all current closed termsets are tested for having f as its closure  being discarded if such condition holds. otherwise  f is discarded  since its closure is already determined. in our sample collection  for example  the 1-termset {a} is initially defined as closed. however  when we combine the 1-termsets {a} and {c} into {a c}  we discard {a}  since they occur in the same set of documents. the same happens when we enumerate {a c e} and find out that {a c} and {a c e} occur in the same documents  allowing us to discard {a c}.
　in the next section we describe how we use closed termsets for retrieving documents efficiently.
1. set-based model
　in our set-based model  a document is described by a set of closed termsets  extracted from the document itself. with each closed termset we associate a pair of weights representing  a  its importance in each document and  b  its importance in the whole document collection. in a similar way  a query is described by a set of closed termsets with a weight representing the importance of each closed termset in the query.
　the algebraic representation of the set of closed termsets for both documents and queries correspond to vectors in a 1t-dimensional euclidean space  where t is equal to the number of unique index terms in the document collection. however  in practice  we use an efficient algorithm to obtain closed termsets which is based on association rules theory  thus reducing the number of closed termsets to a number that is linear on the number of documents in the collection  see section 1 and section 1 .
1 termset weights
　a successful ranking algorithm based on index terms weighting has to take into account the following three criteria. first  it has to consider the number of times that an index term occurs in a document. second  emphasis must be given to scarce terms  and this can be accomplished by reducing term weights for terms that appear in many documents. third  long documents with many terms will naturally be favored by the ranking process because they are likely to contain more of any given list of query terms  and usually a normalization factor is introduced to discount the contribution of long documents.
　term weights can be calculated in many different ways  1  1  1 . for the first two problems pointed out above  the best known term weighting schemes use weights that are function of  i  tfi j  the number of times that an index term i occurs in a document j and  ii  dfi  the number of documents that an index term i occurs in the whole document collection. such term-weighting strategy is called tf 〜 idf schemes. the expression for idfi represents the importance of term i in the collection  it assigns a high weight to terms which are encountered in a small number of documents in the collection  supposing that rare terms have high discriminating value.
　in the set-based model  the association rules scheme naturally provides for quantification of representative patterns of term cooccurrences  something that is not present in the tf〜idf approach. to determine the weights associated with the closed termsets in a document or in a query  we also use the number of occurrences of a closed termset in a document  in a query  and in the whole collection. formally  the weight of a closed termset i in a document j is defined as:
		 1 
where n is the number of documents in the collection  sfi j is the number of occurrences of the closed termset i in document j  obtained from the index   and idsi is the inverted frequency of occurrence of the closed termset in the collection. sfi j generalizes tfi j in the sense that it counts the number of times that the closed termset si appears in document j. the component idsi also carries the same semantics of idfi  but accounting for the cardinality of the closed termsets as follows. high-order closed termsets usually have low frequency  resulting in large inverted frequencies. thus  this strategy assigns large weights to closed termsets that appear in small number of documents  that is  rare closed termsets result in greater weights.
1 similarity calculation
　since documents and queries are represented as vectors  we assign a similarity measure to every document containing any of the query closed termsets  defined as the normalized scalar product between the set of document vectors  and the query vector q. the similarity between a document dj and a query q is defined as:
 
where ws j is the weight associated with the closed termset s in document dj  ws q is the weight associated with the closed termset s in query q  cq is the set of all closed termsets such that all s   q  wi j is the weight of term i in document dj  and wi q is the weight of term i in query q. we observe that the normalization  i.e.  the factors in the denominator  is done using the terms  instead of closed termsets  that compose the query and document vectors. this is important to reduce computational costs because computing the norm of a document d using closed termsets might be prohibitively costly. it solves the third problem pointed out in section 1 because it accomplishes the effect of penalizing large documents  the major objective of normalization by document norms. further  it allows efficient computation. our experimental results corroborate the validity of adopting this alternative form of normalization.
1 query mechanisms
　in this section we describe a simple algorithm for implementing a search mechanism based on the set-based model and the indexing information.
　for large document collections  the cost to evaluate the similarity measure is potentially high  requiring the reading and processing of whole inverted lists for each query closed termset. this task may have a high cost since some query terms might occur in a large number of documents in the collection. one of the most effective techniques to compute an approximation of the similarity measure without modifying the final ranking is presented in . it uses thresholds for early recognition of which documents are likely to be highly ranked to reduce costs. in the set-based model we employ a variant of this pruning scheme adapted to handle closed termsets instead of terms.
　the steps performed by the set-based model to the calculation of the similarity metrics are equivalent to the standard vector space model. first we create the data structures that are used for calculating the document similarities among all closed termsets si of a document d. then  for each query term  we retrieve its inverted list  and determine the first frequent termsets  i.e.  the frequent termsets of size equal to 1  applying the minimal frequency threshold. the next step is the enumeration of all closed termsets based on the 1termsets  as presented in section 1. after enumerating all closed termsets  we employ the pruning scheme presented by persin  to the evaluation of its inverted lists  allowing to discard closed termsets that will not influence the final ranking very early. after evaluating the closed termsets  we normalize the document similarities by dividing each document similarity by the respective document's norm. the final step is to select the k largest similarities and return the corresponding documents.
1. experimental results
　in this section we describe the experimental results for the evaluation of the set-based model  sbm  in terms of both effectiveness and computational efficiency. our evaluation is based on a comparison to the standard vector space model  vsm  and to the generalized vector-space model  gvsm . we first present the experimental framework and the reference collections employed  and then discuss the retrieval performance and the computational efficiency of each model.
　in our evaluation we use three reference collections  cfc   trec-1  and wsj  respectively. the wsj collection is a subcollection of trec-1  and is used in our experiments because the full trec-1 collection is too large for running the generalized vector space model. table 1 presents the main features of these collections  which comprise not only the documents  but also a set of example queries and the relevant responses for each query  as selected by experts. we quantify the retrieval effectiveness of the various approaches through standard measures of average recall and precision. the computational efficiency is evaluated through the query response time  that is  the processing time to select and rank the documents for each query.
table 1: characteristics of the reference collections
characteristicscollectioncfcwsjtrec-1number of documents111 1number of distinct terms111 1number of queries11average terms per query111average relevants per query111size  mb 11 1　the experiments were performed on a linux-based pc with a intel pentium iii 1 ghz processor and 1 gbytes ram. next we present the results obtained for the three collections  which are characterized in table 1.
1 retrieval performance
　we start our evaluation by verifying the precision-recall curves for each model when applied to each of the three collections. each curve quantifies the precision as a function of the percentage of documents retrieved  recall . the gvsm could not be evaluated for the trec-1 collection  because of the cost of the min-term building phase  which is exponential in the size of the vocabulary  making the computational cost of the associated experiments not feasible. the results presented for sbm are the best for a range of minimal frequencies from 1 to 1 documents  see figure 1 .
　as we can see in figures 1  1  and 1  sbm yields better precision than both vsm and gvsm  regardless of the collection and of the recall level. further  the gains increase with the size of the collection  because large collections allow computing a more representative set of closed termsets.
　we confirm such observations by verifying the overall average precision achieved for each model in each collection  which is presented in table 1. we also present  in the same table  the gains provided by sbm over the other two models in terms of overall precision. the gains ranged from 1% to 1%  increasing with the size of the collection and being consistently greater for the sbm. we should note that sbm provides significant gains over gvsm 

figure 1: recall-precision for cfc

figure 1: recall-precision for wsj

figure 1: recall-precision for trec-1
although both exploit term co-occurrences. we believe that sbm outperforms gvsm because it considers just some interesting cooccurrence patterns that are uncovered by the closed termsets.
　the set-based model provides even greater gains when we verify the average precision for the top 1 documents retrieved. this metric estimates the user  satisfaction  while using search engines  for instance. the results are presented in table 1. we can see that the gains range from 1 to 1% and the highest gain is associated to the largest collection  trec-1   showing that the sbm is specially powerful for handling large amounts of information. again  although the gains over the gvsm are smaller  they are still very significative  showing that sbm not only improves the overall precision  but also ranks more relevant documents at the top.
table 1: average precision curves and gain provided by the sbm model
collectionaverage precision  % sbm gain % vsmgvsmsbmvsmgvsmcfc11111wsj11111trec-1.1*11** the gvsm could not be evaluated for the trec-1 collection due to the exponential cost of the min-term build phase
table 1: average precision curves for top 1 documents and gains provided by the sbm model
collectionaverage precision at 1  % sbm gain % vsmgvsmsbmvsmgvsmcfc11111wsj11111trec-1.1*11** the gvsm could not be evaluated for the trec-1 collection due to the exponential cost of the min-term build phase
　one of the key features of the set-based model is the possibility of controlling the minimal frequency threshold of a termset. it is interesting to note that the variation of the minimal frequency allows us to exploit the trade-off between precision and the number of termsets taken into consideration. we verified how variations in the minimal frequency threshold employed affect the precision achieved for the trec-1 collection. we performed a set of sbm executions where the minimal frequency threshold is fixed for all queries and varies from 1 to 1 documents. the results are illustrated in figure 1 and we show that the sbm precision is significantly affected by the minimal frequency employed. initially  an increase in the minimal frequency results in better precision  achieving a maximum precision for minimal frequencies between 1 and 1 documents. when we increase the minimal frequency beyond 1  we observe that the precision decreases until it reaches almost 1. this behavior can be explained as follows. first  an increase in the minimal frequency causes irrelevant termsets to be discarded  resulting in a better precision. when the frequency gets above 1  then relevant termsets start to be discarded  leading to a reduction in terms of precision.
　in summary  our set-based model  sbm  is the first information retrieval model that exploits term correlations effectively and provides significant gains in terms of precision  regardless of the size of the collection and of the size of the vocabulary. in the next section we discuss the computational costs associated with sbm.
1 computational efficiency
　in this section we compare our model to the standard vector space model and the generalized vector space model regarding the query response time  in order to evaluate its feasibility in terms of computational costs. this is important because one major limitation of existing models that account for term correlations is their computational cost. several of these models cannot be applied to large or even mid-size collections since their costs increase exponentially with the vocabulary size.
we start our evaluation by verifying the theoretical upper bounds

figure 1: impact of varying minimal frequency threshold on average precision for trec-1
on the number of operations performed by vsm and sbm. the complexity of both models is linear with respect to the number of documents in the collection. formally  the upper bound on the number of operations performed for satisfying a query in the vector space model is o |q|〜n   where |q| is the number of terms in the query and n is the number of documents in the collection. the worst case scenario for the vector space model is a query comprising the whole vocabulary t  |q| = t   which results in a merge of all inverted lists in the collection. as discussed in section 1  the complexity of the set-based model is o r1ln   where r is the number of maximal termsets and l is the largest termset associated with the collection for a given minimal frequency.
　the comparison between t and r1l allows quantifying the worst case cost increase of the sbm model  but the latter is dependent on the document collection and minimal frequency employed. we calculated r and l for each collection employing a 1% confidence interval and they are presented in table 1. for the sbm model  the upper bound is 1 〜1 〜number  of  documents operations.
table 1: number of maximal termsets and largest frequent termset
collectionrlcfc wsj
trec-1.1.1＼＼1.11.1＼＼1.1	＼	＼
　in fact  for sbm  we use the number of closed termsets  whose worst case is r1l. we determined the average number of closed termsets and the average list sizes while using sbm and the results show that the average case scenario is much better than the worst case just presented  as we can see in table 1. considering the trec-1 collection  the average number of closed termsets is 1.1  giving an average upper bound of 1.1 〜 1 operations for the sbm model  where the number 1 is the average size of the inverted lists. the number 1.1 of closed termsets is approximately 1l  where l = 1 from table 1  considering the trec-1 collection  the average number of terms per query is 1  resulting in an average upper bound of 1 〜 1 operations for the vsm model.
　to validate our analysis  we compared the response time for the models and collections considered  which are summarized in table 1. we also calculated the increase of the response time of gvsm and sbm when compared to vsm. we observe that sbm table 1: average number of closed termsets and inverted list
size
collectionclosed termsetsinverted list sizecfc11wsj1.1.1trec-1 11results in a response time increase from 1 to 1% for the reference collections evaluated when compared to vsm. the results confirm the complexity analysis performed  indicating that the time for processing the lists dominates the processing costs for both vsm and sbm  and the sbm is comparable to vsm in terms of computational costs. as expected  the increase for gvsm is much greater  ranging from 1% to 1%.
table 1: average response time and response time increase
collectionaverage response time  s increase % vsmgvsmsbmgvsmsbmcfc11111wsj11111trec-1.1*1*1* the gvsm could not be evaluated for the trec-1 collection due to the exponential cost of the min-term build phase
　we identify two main reasons for the relatively small increase in execution time for sbm. first  there is a small number of query related termsets in the reference collections. as a consequence  the inverted lists associated tend to be small  as presented in table 1  and are usually manipulated in main memory in our implementation of sbm. second  we employ pruning techniques that discard irrelevant termsets early in the computation  as described in section 1.
　as mentioned  we may control the number of closed termsets  and thus the query processing time  by varying the minimal frequency threshold. we varied the minimal frequency used in sbm and measured the response time. figure 1 shows the response time as a function of the minimal frequency employed  where we see that the increase in the minimal frequency causes significant reductions on the response time  at a possible penalty in average precision curves.

figure 1: impact of minimal frequency on response time for trec-1
1. conclusions
　we presented a new approach to compute term weights  which is based on the theory of association rules. we showed that it is possible to significantly improve retrieval effectiveness  while keeping extra computational costs small. the computation of frequent termsets enumerated by an algorithm to generate association rules lead to a direct extension of the vector space model. the formal framework we adopted allowed naturally considering relevant patterns of term co-occurrence  with minimal changes to the standard vector space model formulation.
　we evaluated and validated our proposed extension for the vector space model using three test collections. we showed through curves of recall versus precision that the new model presents results that are superior for all the collections considered and that the additional computational costs are acceptable. in fact  the computational costs measured allow the use of our set-based model with general large document collections.
　recent experimentation  1  1  indicates that measurements involving the distance between term occurrences may provide good relevance assessments. for future work we will extend the setbased model to account for the proximity information about query terms in documents. we will also investigate the behavior of our model when applied to larger document collections and for collections formed by web documents.
