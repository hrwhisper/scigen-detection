the effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribution that generates the training examples and the one from which test examples are to be drawn. the task can be especially difficult when the training examples are from one or several domains different from the test domain. in this paper  we propose a locally weighted ensemble framework to combine multiple models for transfer learning  where the weights are dynamically assigned according to a model's predictive power on each test example. it can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model  which can then be applied on a different domain. importantly  different from many previously proposed methods  none of the base learning method is required to be specifically designed for transfer learning. we show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer. we then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain  and then weighting each model locally according to its consistency with the neighborhood structure around the test example. experimental results on text classification  spam filtering and intrusion detection data sets demonstrate significant improvements in classification accuracy gained by the framework. on a transfer learning task of newsgroup message categorization  the proposed locally weighted ensemble framework achieves 1% accuracy when the best single model predicts correctly only on 1% of the test examples. in summary  the improvement in accuracy is over 1% and up to 1% across different problems.
categories and subject descriptors
h.1  database management : database applications- data mining
general terms
algorithms
1. introduction
　we are interested in transfer learning scenarios where we learn from one or several training domains and make predictions in a different but related test domain. such knowledge transfer is possible when the training domain s  and the test domain have the same set of categories or class labels. we further assume that we are only exposed to some labeled examples from the training domains but do not have any labeled example from the test domain. the study of transfer learning is motivated by the fact that people often exploit knowledge gained from related domains where labeled data are abundant to classify examples in a new domain. unfortunately  traditional supervised learning techniques usually fail to transfer knowledge in this scenario because it requires the training and the test data to be i.i.d. samples from the same distribution.
　there are a few important observations about this problem. we notice that there are usually several classification models available from the training domains. for example  the classifiers can be trained from several relevant domains or built using different learning algorithms on the same domain. different models usually contain different knowledge and thus have different advantages  due to the inductive bias of the specific learning technique as well as the distributional differences among the training domains. therefore  different models may be effective at different regions or structures in the new and different test domain  and no single model can perform well in all regions. we refer to these different models as base models. ideally  we may wish to combine the knowledge from these base models rather than using any single model alone to more effectively transfer the useful knowledge to the new domain. for this task  one would naturally consider model averaging that additively combines the predictions of multiple models. however  the existing model averaging methods in traditional supervised learning usually assign global weights to models  which are either uniform  e.g.  in bagging   or proportional to the training accuracy  e.g.  in boosting   or fixed by favoring certain model  e.g.  in single-model classification . such a global weighting scheme may not perform well in transfer learning because
different test examples may favor predictions from different base models. for example  when the base models carry conflicting concepts at a test example  it is essential to select the model that better represents the true target distribution underlying the example. in fact  based on principles of risk minimization  we can derive that there exists a solution to assign per model and per example weights to combine multiple base models to maximize their combined accuracy on the new domain  and the combined accuracy is higher than any single model acting alone. however  it is impossible to dynamically assign the optimal model weights for each example precisely because p y|x   the true conditional probability of class label y given a test example x  is not known a priori. past practice of cross-validation based weight assignment is inapplicable since the weights would be assigned based on labels in the given training domain s  whose p y|x  could be different from that of the test domain. therefore the focus of this paper is to find an approximation to this optimal local weight assignment for each test example.
　we propose a graph-based approach to approximate the optimal model weights where the local weight for a base model is computed by first mapping and then measuring the similarity between the model and the test domain's local structure around the test example. this similarity is measured by comparing neighborhood graphs  and quantified in the weight assignment equation. intuitively  it favors classifiers whose mapped local structure is similar to the local structure around the test example. for a particular example  if none of the mapped local structures is similar to the original local structure in the target domain  the predicted label will be obtained by voting among its neighbors inside the same local structure of the test set. this strategy ensures that the maximum amount of predictive powers of the labeled information are extracted and transferred to the test domain to make the predictions consistent with its underlying manifold structure.
　our main contributions to the task of transfer learning include the following:  1  we propose a locally weighted ensemble framework to address the transfer learning problem  and demonstrate its superiority over single models in terms of risk minimization when the weights are set optimally.  1  none of the base models is required to be specifically designed for transfer learning  thus providing great flexibility and freedom on what models to use.  1  we propose to approximate the model weights based on the local manifold structures in the test domain  and provide neighborhood graph-based estimation.  1  we provide a prediction adjustment step to propagate labels from nearby examples when all base models are inconsistent with certain test examples.
　we evaluated the proposed framework on three real tasks: spam filtering  text categorization  and network intrusion detection. in each task  the test examples come from a different domain than the training set. our experiment results show that the locally weighted ensemble framework significantly improved the performance over a number of baseline methods on all three data sets  which shows the effectiveness of the proposed framework for transfer learning.
1. locally weighted ensemble
　let us first look at a toy learning problem with two training sets and a test set shown in figure 1. the two training sets have partially conflicting concepts and their decision boundaries are the straight lines. for the test set  however 

figure 1: a motivating example
the optimal decision boundary is the v-shape solid line. as can be seen  the regions r1 and r1 are uncertain  because the two training sets are conflicting there. if we either simply collapse the two data sets and try to train a classifier on the merged examples  or combine the two linear classifiers m1 and m1 trained from the training set 1 and set 1 respectively  then those negative examples in r1 and r1 will be hard to predict. those semi-supervised learning algorithms do not work either because they only propagate the labels of the training examples to the unlabeled examples. in this case  there are conflicting labels in r1 and r1  causing ambiguous and incorrect information to be propagated. but it is obvious that  if m1 is used for predicting test examples in r1 and m1 used for examples in r1  then we can label all test examples correctly. therefore  ideally  one wish to have a  locally weighted  ensemble framework that combines the two models  and weighs m1 higher at r1 and m1 higher at r1. we also observe that this data set has a property that neighbors along the same  clustering-manifold structure  share the same class labels  which is a commonly-held assumption for reasonable problems. below  we first introduce a locally weighted ensemble framework with weights dynamically adjusted according to the model behaviors at each test example. we then present an effective way of approximating the model weights via local structure mapping around each example. the success of the proposed method on this toy data set is demonstrated in section 1.
1 optimal domain transfer weights
　let x be the feature vector and y be the class label where x and y are drawn from feature space x and label space y respectively. for a set of k models m1 ... mk  the general bayesian model averaging approach computes the posterior distribution of   where p y|x d mi  = p y|x mi  is the prediction made by each model and p mi|d  is the posterior of model mi after observing the training set d. however  in transfer learning  since training and test domains are different  we may wish to incorporate information about the test domain and update the model prior for p mi|t   where t is the test set. so p mi|d  should be replaced by p mi|t  in the weighted combination of model predictions. by this replacement  we take the difference between training and test domains into consideration during learning. if the true distribution p y|x  is known  then for predictions on x  the other examples in the test set are irrelevant to the model performance at x. in other words  the model weight p mi|t  is actually p mi|x  at x when p y|x  is available. different from traditional ensemble approaches  this locally weighted model averaging method weights individual models according to their local behaviors at each test example. the final
prediction for x is:
	 	 1 
where wmi x = p mi|x  is the true model weight that is locally adjusted for x representing the model's effectiveness on the test domain.
　the benefits of this locally weighted model averaging approach can be shown as follows. to simplify the problem  we map the label space y to {1 ... c} where c is the number of classes. we then use a c 〜 1 vector f to denote the true conditional probability in the test domain where the i-th element is fi = p y = i|x . supervised learning can output a c〜1 vector h that is close to f for x. let wi = wmi x denote the weight for model mi at test example x  and let w denote the k 〜 1 weight vector. hi represents the predictions made by model mi at x and is again a c〜1 vector where the j-th element is hij = p y = j|x mi . h is used to represent a c〜k matrix with all the model predictions made for x where the ij entry is model mi's predicted p y = j|x mi   i.e.  hij. then the output of the model averaging framework for x is a vector he = hw. note that w satisfies the constraints that
  and thus the output vector hi
from a single model mi is a special case of he when wi = 1 and other weights are zero. but we wish to find a weight vector w which minimizes the distance between f and he. under squared-error loss  the following objective function should be minimized to obtain the optimal w:
w  = argmin f   hw t f   hw  + λ wti   1  	 1  w
where i is a k 〜 1 vector of 1 and λ is the regularization term. it is obvious that eq.  1  represents a least-square linear regression problem and the optimal solution is
	w.	 1 
λ can be further calculated by substituting the above w  to the constraint  w  ti = 1. usually wi  is a value between 1 and 1 so the weight vector of the optimal ensemble is different from that of the single model. therefore  the error of the model averaging framework on each test example x will not be greater than that of any single model:
	 f   hw  t f   hw   ＋  f   hi t f   hi   i	 1 
thus  for each test example  there is a smaller chance to make a mistake if we combine the predictions from different models using the optimal weight vector. it is important to note that the optimal weight vectors are different for different test examples  so weights should be decided locally.
　this locally weighted ensemble framework differs from traditional model averaging methods in the following ways: 1  in transfer learning problems  the traditional methods of assigning model weights based on training set or assigning fixed prior weights are undesirable. instead  we do not assume that training and test domains follow same distributions but rather focus on the test set when deriving the best model weights to transfer knowledge across domains. 1  existing work usually weights each model globally  but the proposed method assigns per example weights to each model to identify variations in model performance for different test examples. as discussed  there may not exist one model globally optimal for all the test examples. usually  different test examples favor different models and therefore the per example weighting scheme is better than the global weighting scheme in terms of classification accuracy.
　one challenge is that the optimal per example weight vectors cannot be computed exactly in reality  since the true target vector f for each test example x is not known a priori. importantly however  from its solution in eq.  1   a model will have a higher weight if its prediction on x is closer to the true p y|x . in the rest of this paper  we propose a graph-based approach to approximate the optimal per example weight wmi x under the  clustering-manifold  assumption that p x  is related to p y|x . other approximation heuristics can be developed under this locally weighted framework as long as the weights reasonably approximate the model performance for given test examples.
1. graph-based weight estimation
　as discussed in section 1  the optimal weights can be approximated by assigning a higher weight to a model that produces a more accurate label prediction for x. so the main task is to formulate similarity between the model predictions and the unknown true target function. to achieve this goal  we can model the underlying p x  from the unlabeled test set in order to infer p y|x . specifically  we make a clustering-manifold assumption  as commonly held in semi-supervised learning  that p y|x  is not expected to change much when the marginal density p x  is high. in other words  the decision boundary should lie in areas where p x  is low. under such an assumption  we can compare the difference in p y|x  between the training and the test data locally with only unlabeled test data. however  probability density estimates are hard to obtain precisely  especially when x is high-dimensional. instead  we propose to cluster the test data and assume that the boundaries between the clusters represent the low density areas. as a result  if the local cluster boundaries agree with the classification boundary of m around x  then we assume that p y|x m  is similar to the true p y|x  around x  and thus the weight for model m ought to be high at x. in the following  we formally give a procedure of computing the weight and illustrate the procedure with an example.
　for a test example x and a base model m to be combined  we first construct two graphs: gt =  v et  and gm =  v em . in both graphs  the vertex set v contains all the test examples. for gm  there is an edge connecting two test examples if and only if the examples are classified into the same class by m. on the other hand  to construct gt  we cluster the test examples into c1 clusters and again  connect two test examples with an edge if and only if the two examples are in the same cluster. then we can approximate the model weight as the similarity between the local structures around x in gt and gm. specifically  under the clustering assumption  it is probable that two examples are in the same class if they belong to the same cluster in gt. so we could use the percentage of common neighbors of x found in gm and gt to approximate the model accuracy on x and set the weight. suppose the sets of neighbors for x in gm and gt are vm and vt respectively. the model weight at x is proportional to the similarity of its local structures between gm and gt:

clustering

figure 1: local neighborhood graphs around x
according to its definition  s gm gt;x  reflects the degree of consistency in labeling the test examples. if x has similar sets of neighbors in gm and gt  it is likely that the model m is consistent with the underlying structure around x. as an example  figure 1 shows the neighborhood graphs of a test example x constructed from two supervised models and the clustering algorithm on the test set. according to eq.  1   the similarity between model 1 and the clustering structure is 1 at x  but that between model 1 and the structure is 1. therefore  for x  model 1's weight will be set higher since it is more consistent with the local structure around x. this is a simple and effective method to compute the similarity.
　the weight approximation is based on the clustering assumption which requires that the manifold structure of the data is related to the conditional probability p y|x . though this is a reasonable assumption for many problems  it may not always hold. without knowing p y|x  a priori  it is impossible to verify the assumption. but this property is usually determined by the nature of the learning tasks. an example where the assumption does not hold is sentiment classification  where the clustering structure of a set of product reviews reveals the topics but may have nothing to do with whether the users like or dislike the product. therefore  we propose to check the validity of the clustering assumption by evaluating the clustering quality on the training set using purity  entropy or f measure. if the task fails the test  we will ignore the weight approximation step  but simply combine the models using uniform weights. this strategy restricts the use of the graph-based weight estimation only to the cases where the clustering assumption is satisfied on both training and test sets. however  the strict checking criteria could guarantee the high accuracy of the proposed method. for the cases where the clustering assumption does not hold  other techniques need to be explored.
　when the condition holds  we compute the per-example model weights based on eq.  1  with a normalization term:
	 	 1 
where mi is one of the k models. then the final prediction of the weighted ensemble e for x is:
xk
	p y|e x  =	wmi xp y|mi x  	 1 
i=1
where p y|mi x  is the prediction made by model mi. then
the predicted label for x goes to y  which minimizes the risk:
z
         y  = argmin	λ y1 y p y|e x dy1	 1  y	y1（y where λ y1 y  is the cost incurred when the true class label is y1 but the prediction goes to y. with the most commonly used zero-one loss function  y  = argmaxy p y|e x .
1 local structure based adjustment
　the weighting scheme shown in eq.  1  works on the basis that at least some of the models do reasonably well on predicting the label for x. however  if the concepts carried by all the models conflict with the actual concept at x  the similarity measure s gm gt;x  is expected to be low for each model m. but after the normalization in eq.  1   the locally weighted ensemble framework would still make decisions based on these models for x and it is probable that the combined output is still in conflict with the true one. in such a scenario  it is reasonable to abandon the labeled information conveyed by the supervised models but rather rely on the local structure around x only.
　since the similarity measure s gm gt;x  reflects the degree of consistency between model m's prediction and x's neighborhood structure  we can use the average s gm gt;x  over all m to judge whether the labeled information is reliable or not. in fact  s gm gt;x   representing the average percentage of common neighbors shared by supervised models and clustering results  is within  1 . to be exact  when a test example shares the same neighbors in two graphs  their similarity is 1  whereas if no common neighbor is found  it is 1. so for example  if only two models are used and their s gm gt;x  are both 1 at x  then we should avoid normalizing the weights into 1 since both models should rather be discarded. let be the average similarity between the base models' predictions on x and the clustering structure around x. then if savg x  − δ  where δ is the threshold  we believe in the prediction obtained from eq.  1 ; otherwise  we discard all the supervised classifiers and construct an  unsupervised  classifier based on the neighborhood of x.
　the  unsupervised  classifier u is not trained on any labeled training set. its prediction on x is mainly determined by the neighbors of x with labels predicted by the combined classifier. specifically  p y|u x  can be decomposed as:
x
	p y|u x  =	p y|u x （ c p x （ c|x .	 1 
c
here  c is one of the clusters in the test set. we assume that the cluster membership is deterministic  then p x （ c|x  is approximated as follows:
1
	1	x （ c
p x （ c|x  =	 1  1	otherwise
hence  p y|u x  is approximately the same as p y|u x （ c  when x belongs to cluster c. we can further approximate p y|u x （ c  as the average p y|e x  for x （ c1 where c1 contains test examples which satisfy both x （ c and savg x  − δ. in other words  only examples that have reliable predictions from the weighted ensemble will count in this procedure. therefore 
1 x
	p y|u x （ c  「 |c1|	1 p y|e x 	 1 
x（c
where |c1| is the size of c1. the above strategy can be simplified if we set p y|e x  = 1 when y is the label for x predicted by e. so p y|u x （ c  can be estimated by a
algorithm: locally weighted ensemble  lwe 
input:  1 a training set d or k training sets d1 ... dk
 1  k classification models m1 ... mk  k   1 
 1  a test set t which comes from a different domain but the classification task is the same.
 1  a threshold δ and cluster number c1.
output: the set of predicted labels y for examples in t. algorithm:
1. perform clustering on the training set s   if the average purity of clustering is less than 1  set wmi x = k1 for all mi and x  and compute the posterior using eq. 1  for each x （ t. return.
1. group test examples into c1 clusters and construct neighborhood graphs based on the clustering results and all the k models. set t1 = Φ.
1. for each x （ t    for each model mi  compute the model weight wmi x according to eq. 1 .
  if savg x  − δ  decide x's label based on the weighted ensemble's output p y|e x  obtained using eq. 1   else put x into t1.
1. for each x （ t1  predict x's label from the  unsupervised classifier u  i.e.  estimate p y|u x  using eq. 1  or eq. 1 . return. figure 1: locally weighted ensemble framework majority vote among examples in c1:
		 1 
where c y c1|e  is the number of examples with label y predicted by ensemble e in c1. so the probability of x having label y is the percentage of examples in the cluster c1 that have y as their class labels  where c1 is the cluster that x belongs to and contains test examples with predicted labels. the final predicted label for x is determined by eq.  1  with p y|e x  replaced by p y|u x . if zero-one loss function is applied  the class label for x whose cluster is c should be the majority label prediction among the test examples which satisfy both x （ c and savg x  − δ.
1 algorithm description
　the framework is summarized in figure 1. we first verify whether the clustering structure is relevant to the classification task by performing clustering on the training set. if the purity of clustering on the training set is below 1  we simply combine models using uniform weights. otherwise  if the clustering quality is satisfactory  in step 1  we construct the neighborhood graphs for both the supervised models and the clustering results. then in step 1  the weight of each model at each test example is computed  which reflects the consistency of model predictions among the test example's neighborhood. we then separate the test examples by checking if its average model weight is greater than a confidence threshold. for those test examples on which cross domain models can make sufficiently accurate predictions  the final label predictions are decided by the locally weighted ensemble. but  for the test examples that the models are not expected to classify correctly  the labels are determined by majority voting among those neighbors with highly confident predictions within the same cluster structure.
table 1: data sets description
taskdata setstrainingtestemail
spam
filteringuser1 u1 
user1 u1 
user1 u1 public messageseach user's emails1 newsgroupcomp vs sci  c vs s  rec vs talk  r vs t 
rec vs sci  r vs s 
sci vs talk  s vs t 
comp vs rec  c vs r 
comp vs talk  c vs t documents from
a set of sub categoriesdocuments from a
different set of sub
categoriesreutersorgs vs people
 o vs pe 
orgs vs place
 o vs pl 
people vs place
 pe vs pl documents from
a set of sub categoriesdocuments from a
different set of sub
categoriesintrusion detectiondosprobing & r1l
intrusionsdos
intrusionsprobingdos & r1l
intrusionsprobing intrusionsr1ldos & probing
intrusionsr1l
intrusions1. experiments
　in this part  we demonstrate the effectiveness of the locally weighted ensemble framework. the algorithms are evaluated on various data sets covering many application domains. results show that the proposed framework could combine the predictive powers obtained from multiple sources and gain great improvements in classification accuracy. the software  datasets and more details about the experiments are available at http://ews.uiuc.edu/゛jinggao1/kdd1transfer.htm.
1 data sets and experiment setup
　we conduct experiments on one synthetic and four real data sets  where training and test distributions are different. synthetic data.
　the two training sets and the test set as shown in figure 1 are generated from several gaussian distributions with the same variance. in each training set  there are 1 positive and 1 negative examples and in the test set  the number of positive and negative examples are 1 and 1 respectively. email spam filtering.
　the email spam data set  released by ecml/pkdd 1 discovery challenge  contains a training set of publicly available messages and three sets of email messages from individual users as test sets. the 1 labeled examples in the training set and the 1 test examples for each of the three different users differ in the word distribution. the aim is to design a server-based spam filter learned from public sources and transfer it to individual users. document classification.
　the 1 newsgroups data set contains approximately 1 newsgroup documents  partitioned across 1 different newsgroups nearly evenly. the reuters-1 corpus contains reuters news articles from 1. from the two text collections  we generate nine cross-domain learning tasks. both text collections have a two-level hierarchy so that each learning task involves a top category classification problem but the training and test data are drawn from different sub categories. for example  the goal is to distinguish documents from two top newsgroup categories: rec and talk. so a train-
ing set involves documents from rec.autos   rec.motorcycles    talk.politics  and  talk.politics.misc   whereas the test set includes sub-categories rec.sport.baseball   rec.sport.hockey    talk.politics.mideast  and  talk.religions.misc . the strategy is to split the sub-categories among the training and the test sets so that the distributions of the two sets are similar but not exactly the same. the tasks are generated in the same way as in  and more details can be found there. intrusion detection.
　the kdd cup'1 data set consists of a series of tcp connection records for a local area network. each example in the data set corresponds to a connection  which is labeled as either normal or an attack  with exactly one specific attack type. some high level features are used to distinguish normal connections from attacks  including host  service and traffic features. in the experiments  we use the 1 continuous features. attacks fall into four main categories: dos denial-of-service   r1l unauthorized access from a remote machine   u1r unauthorized access to local superuser privileges   probing surveillance and other probing . since in reality  we usually encounter the problem of detecting the variants of known attacks  it is realistic to have one type of intrusions in the training set but another type in the test set. we create three data sets  each contains a set of randomly selected normal examples and a set of attacks from one category. since the number of u1r attacks is small  we only use examples from dos  r1l and probing categories. then three cross-domain learning tasks are generated by training from two types of attacks to detect another type of attack. the details of the four real tasks are presented in table 1. baseline methods.
　we compare the weighted ensemble framework with different learning algorithms. in particular  since most data sets are high-dimensional  the following commonly used algorithms are appropriate choices: 1  winnow  wnn  from learning package snow   1  logistic regression  lr  implemented in bbr package ; and 1  support vector machines  svm  implemented in libsvm . when we only have a single source domain in the training  three single classifiers are trained using the above learning algorithms and combined according to the proposed weighted ensemble framework. but note that the proposed method is a general framework so that any kind of models could be plugged in and transferred to the test domain. since semi-supervised learning  transductive learning  is closely related to the problem  we compare the proposed method with transductive support vector machines  tsvm  implemented in svm light . furthermore  in the proposed framework  the two main steps are  predicting labels using weighted classifiers if the classifiers are sufficiently accurate in terms of alignment with clustering structures; and propagating the labels of predicted test examples to the unpredicted ones through the clustering structure. to demonstrate the effectiveness of both steps  we include the following three methods in the comparison: 1  a simple model averaging framework  sma  where all model predictions are combined using uniform weights; 1  the locally weighted ensemble framework without the adjustment step  which simply adopts the weighted prediction for each test example. we call it partial locally weighted ensemble method  plwe ; 1  the locally weighted ensemble frame-

figure 1: performance on synthetic data
work  lwe  involving both classifier combination and local structure based adjustment. note that sma is one of the global ensemble methods where the model weights are set the same for all the test examples. suppose there are k models  then each model will have a weight k1 at every test example. we use the clustering package cluto   which is designed for high-dimensional data clustering  to cluster the test set. again  other clustering algorithms could be used as long as the  clustering  assumption is satisfied.
　we compare with a set of different baseline methods on the synthetic and intrusion detection data sets. in each task  we have two source domains for training and the remaining one for the testing. the proposed weighted ensemble methods  plwe and lwe  are built upon two single models trained from the two source domains using svm. first  we compare plwe and lwe with the simple averaging method  sma  based on the two svm models. second  we can choose the training set as 1  one of the two source data sets  or 1  the union of the two source data sets. on the three possible training sets  we study the performance of supervised learning models  svm  and semi-supervised models  tsvm  and compare them with the proposed methods. performance measures.
   to compare the performance of the classification methods  we look at a set of standard evaluation metrics. first  we use classification accuracy  which is simply defined as the percentage of correct predictions among all test examples. second  under squared loss function  the algorithms can be evaluated using mean squared errors defined as follows:  where f xi  is the output
of the classifier  which is the estimated posterior probability of xi belonging to positive class  p +|xi  is the true posterior probability and represents the test set. another measure is used in evaluating the intrusion detection task: the area under roc curve  auc   the best of which is 1 corresponding to 1% detection and 1% false alarm. in the experiments  we focus on binary classification  but the framework can be easily applied on multi-class tasks.
1 performance evaluation
　in this part  we report the experimental results regarding the effectiveness of the locally weighted ensemble. the results clearly demonstrate that on the transfer learning prob-
table 1: performance comparison on a series of data sets
accuracy
methodsspam filtering1 newsgroupreutersu1u1u1c vs sr vs tr vs ss vs tc vs rc vs to vs peo vs plpe vs plwnn111111111111lr111111111111svm111111111111sma111111111111tsvm111111111111plwe111111111111lwe111111111111mean squared error
methodsspam filtering1 newsgroupreutersu1u1u1c vs sr vs tr vs ss vs tc vs rc vs to vs peo vs plpe vs plwnn111111111111lr111111111111svm111111111111sma111111111111tsvm111111111111plwe111111111111lwe111111111111table 1: performance comparison on intrusion detection data set
accuracy
intrusionsdosprobingr1lallsmaplwelwesvmtsvmsvmtsvmsvmtsvmsvmtsvmdosnana111111111probing11nana1111111r1l1111nana11111auc
intrusionsdosprobingr1lallsmaplwelwesvmtsvmsvmtsvmsvmtsvmsvmtsvmdosnana111111111probing11nana1111111r1l1111nana11111lems where training and testing data have different distributions  the proposed locally weighted ensemble approach greatly outperforms supervised  semi-supervised single-model algorithms  and a simple averaging ensemble. performance study.
　the results of the toy problem introduced in figure 1 are summarized in figure 1. the results of linear svm on the training sets from two domains are the top two on the left  denoted as m1 and m1. due to the difference between training and test distributions  both make incorrect predictions at  mirrored  areas. after merging the training sets  the svm model   all  on top right  still does not work and the constructed hyperplane is obviously a horizontal line. this is due to the fact that there exist conflicting concepts in the merged training set. on the other hand  transductive svm  tsvm bottom left  trained on merged training sets fails as well since the label propagation is confused by the conflicting training examples. simple averaging of m1 and m1  shown as  sma   bottom middle  also makes mistakes in the uncertain areas. however  examples incorrectly classified by these methods are now correctly predicted by the locally weighted ensemble approach  lwe  and the decision boundary matches the v-shape well. to see how this works  first  the clustering algorithm discovers the two clusters above and below the v-shape. for any example x （ r1  its neighbors in the cluster contain the examples in all three regions r1  r1 and r1. at the same time  its neighbors predicted by m1 are those examples （ r1 and r1. importantly  its neighbors predicted by m1 are only examples （ r1. since there are more common neighbors between the clustering structure and m1  m1 will be given higher weight at x. thus  according to m1  the examples in r1 are classified to be negative. similarly  m1 will be chosen to predict examples （ r1 as negative. in summary  by weighting the two models locally according to the degree of consistency between models and clusters  the examples at the uncertain areas are predicted correctly.
　results of all the methods on the email spam filtering  1 newsgroup and reuters sets are summarized in table 1 with best results shown in bold font. refer to table 1 for the details of each task. it is clearly seen that  for all tasks and using any performance measure  the locally weighted ensemble method  lwe  significantly improves the transfer learning performance compared with other baseline methods. we can observe that most of the transfer learning problems are tough due to the unknown discrepancy between the training and the test distributions. the single-model methods  wnn  lr  svm  usually have poor performance with accuracy around 1 and mean squared error greater than 1 on most of the tasks. the simple model averaging algorithm using uniform weights can help reduce the expected error compared with single models. however  its performance is not quite satisfactory since they only rely on the labeled information from the source domain and make no efforts in selecting useful information and transferring the knowledge into the test domain. by incorporating the structure information of the test set into learning  the transductive learning approach can beat the supervised learning methods most of the times. but we can see more improvement achieved by using the proposed locally weighted ensemble framework. after the first step of combining classifiers by weighting them judiciously  both accuracy and mean squared error are improved over all the baselines. then propagating confident predictions along the clustering structure in the test set can significantly boost

　　　　　figure 1: parameter sensitivity the performance further. as an example  on the  c vs s  data set in the 1 newsgroup collection  the worst single model only achieves around 1% accuracy whereas the best single model makes correct predictions for 1% of the test examples. the tranductive svm improves the accuracy to around 1% and lwe outperforms all the other methods by an impressive 1% accuracy. in most of the experiments  the improvement in accuracy after utilizing weighted ensemble is over 1% and up to 1% for some problems. the experimental results on these transfer learning tasks demonstrate the benefits of the empirical approximation of the optimal locally weighted ensemble framework. both per-example weighting scheme and the adjustment step in the framework can successfully filter out the harmful labeled information  and thus help make the most reliable predictions.
　table 1 presents the performance of all methods on the three tasks of intrusion detection. each row corresponds to a learning problem characterized by the test domain and the other two domains act as training  as discussed in section 1. besides the two training domains  a simple combination of examples from the two domains  represented as  all   could be another source of training. based on each training source  we test the performance of svm and tsvm on the test domain. we also build two single models from each training domain and combine them using uniform weights  which corresponds to sma. the proposed plwe and lwe are shown in the last two columns. for the first two learning tasks  it is obvious that the proposed lwe shows dominance for both accuracy and auc. especially on the test set of  probing   the two training domains seem to be conflicting with each other  thus both the models trained from a union of the two domains and the simple averaging of the two models result in an accuracy around 1% to 1%. lwe achieves 1% accuracy by choosing the useful information from the two models. on the last learning task  the algorithm tsvm trained on the combination of training domains wins over the proposed method  which may be due to the fact that one of the single models we are combining has insufficient amount of examples to be relied on. we note that the worst single model's accuracy is around 1% and the simple averaging method even degrades to having 1% accuracy. based on such weak classifiers  we could still improve the accuracy to 1%. in the future  we will explore more strategies to detect the cases when we should combine the source domains rather than building individual models. parameter sensitivity.
　there are two important parameters in the proposed algorithm  the number of clusters c1 in the test set and the selection threshold δ to filter the predictions with low confidence. the traditional way of setting parameters through cross-validation cannot work when the training and test distributions are different. again  since the true target function of the test domain is not known  there may not have effective methods to find the optimal values of the parameters. so here  we just give some sensitivity experimental results and state some basic principles in setting the parameters. we choose one cross-domain learning problem from each of the three data sets: email spam filtering  and 1 newsgroup and reuters set  and the results are shown in figure 1. we vary c1 from 1 to 1 and δ from 1 to 1  and put both of them on the x-axis. we compare the accuracy of lwe approach when the parameters vary  with that of the best accuracy achieved by the baseline methods. we fix δ = 1 when changing c1  and let c1 = 1 when tuning δ. it is clearly seen that when the threshold rises from 1 to 1  the learning performances on all three sets are gradually improving. after the point of 1  the performances maintain stable. this suggests that a low threshold is not desirable since many inaccurate predictions from the supervised models would be used in the adjustment step. therefore 1 up to 1 could be a reasonable range to select the threshold δ. however  the users could choose to lower down or raise the threshold to match their beliefs in the abilities of the supervised models. as for the number of clusters c1  the best performances in the experiments are achieved when c1 = 1. when c1 goes up  the over-fitting could occur when the number of examples in each cluster is not sufficient enough to give an accurate estimate of the model weights  and thus we could observe a drop in accuracy. we could also note that in spite of the changes caused by parameter variation  the proposed lwe improves over the best baseline method most of the time.
1. related work
　the problem with different training and test distributions started gaining much attention very recently. when it is assumed that the two distributions differ only in p x  but not in p y|x   the problem is referred to as covariate shift  1  1  or sample selection bias . the instance weighting approaches  1  1  1  try to re-weight each training example with train    and maximize the re-weighted log likelihood. another line of work tries to change the representation of the observation x hoping that the distributions of the training and the test examples will become very similar after the transformation  1  1 .  transforms the model learned from the training examples into a bayesian prior to be applied to the learning process on the test domain. the major difference between our work and these studies is that they depend on a single source of information and try to learn a global single model that adapts well to the test set.
　constructing a good ensemble of classifiers has been an active research area in supervised learning . by combining decisions from individual classifiers  ensembles can usually reduce variance and achieve higher accuracy than individual classifiers. such methods include bayesian averaging   bagging  boosting and many variants of ensemble approaches  1  1  1  1 . some ensemble methods assign weights locally  1  1   but such weights are determined based on training data only. there has not been much work on ensemble methods to address the transfer learning problem. in  1  1   it is assumed that the training and the test examples are generated from a mixture of different models  and the test distribution has different mixture coefficients than the training distribution. in   a dirichlet process prior is used to couple the parameters of several models from the same parameterized family of dis-
tributions.  extends the boosting method to perform transfer learning. bennett et al.  proposed a methodology for building a meta-classifier which combines multiple distinct classifiers through the use of reliability indicators. the proposed weighted ensemble provides a more general framework for transfer learning because 1  the base models can be heterogeneous and can be any generative or discriminative models  and 1  the method does not depend on specific applications and makes no assumption about the form of distributions generating the training or the test data.
　multi-task learning mtl    which learns several related tasks at the same time with a shared representation  considers single p x  and multiple output variables  so the basic setting is different from our problem. the  clustering  assumption in our work is exploited in some transfer learning and semi-supervised learning works  1  1   where clustering structure is utilized in smoothing predictions among neighbors. our paper differs from these papers by utilizing the assumption in weighting different models locally to combine all sources of labeled information for knowledge transfer.
1. conclusion
　knowledge transfer across domains with different distributions is an important problem in data mining that has not been fully investigated. in this work  we take advantage of the different predictive powers of several models trained on different domains or using different learning algorithms. we propose a locally weighted ensemble framework to transfer the combined knowledge to a new domain that is different from all the training domains. importantly  the base models can be constructed by traditional learning algorithms not specifically designed for transfer learning. we analyze the optimality on expected error reduction by utilizing the locally weighted ensemble framework as compared to both single models and globally weighted ensembles. based on the  clustering  assumption that the local structure of the test set is related to p y|x   we design an effective weighting scheme to approximate the optimal model weights. this is formulated by comparing the neighborhood graphs of each model with those from clustering. the experimental results on four real transfer learning data sets show that the proposed method improves over each base model 1% to 1% in accuracy and is more accurate than both semi-supervised learning and simple model averaging models. these results indicate that: 1  the locally weighted ensemble could successfully identify the knowledge from each model that is useful to predict in the test domain and transfer such information from all available base models; and 1  the proposed graph-based weight estimation method makes the framework practical by effectively approximating the optimal model weights. in the future  we plan to compare lwe with existing single-model based transfer learning algorithms  as well as to explore effective methods to set parameter values.
