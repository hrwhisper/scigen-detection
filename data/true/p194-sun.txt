most previous web-page summarization methods treat a web page as plain text. however  such methods fail to uncover the full knowledge associated with a web page needed in building a high-quality summary  because many of these methods do not consider the hidden relationships in the web. uncovering the hidden knowledge is important in building good web-page summarizers. in this paper  we extract the extra knowledge from the clickthrough data of a web search engine to improve web-page summarization. we first analyze the feasibility in utilizing the clickthrough data to enhance web-page summarization and then propose two adapted summarization methods that take advantage of the relationships discovered from the clickthrough data. for those pages that are not covered by the clickthrough data  we design a thematic lexicon approach to generate implicit knowledge for them. our methods are evaluated on a dataset consisting of manually annotated pages as well as a large dataset that is crawled from the open directory project website. the experimental results indicate that significant improvements can be achieved through our proposed summarizer as compared to the summarizers that do not use the clickthrough data.
categories and subject descriptors
h.1  information systems applications : miscellaneous; i.1  pattern recognition : applications-text processing

 this work was conducted when the first author was visiting microsoft research asia  beijing  china.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
general terms
algorithms  experimentation  verification
keywords
generic web-page summarization  clickthrough data  latent semantic analysis  thematic lexicon
1. introduction
　with the rapid growth of the www  there is an increasing need to succinctly summarize web pages for web users. for example  web portals such as yahoo! and looksmart organize their web pages into a hierarchical structure and provide a short summary for each page. these summaries are useful in satisfying the web users' information needs by means of fast and accurate browsing and search. for a search engine such as google  after a query is issued  a list of url's is returned  each accompanied with a snippet that gives a brief summary of the target page's content. web-page summarization is also useful when users access internet via small-display devices such as the personal digital assistants  pda's   since it is hard to fit the original content of a web page into a small screen . however  it is quite expensive to generate the summaries manually in large scale. therefore  it is a critical issue how to automatically generate high quality web-page summaries from web pages. web-page summaries can be abstracts or extracts. an extract summary consists of sentences extracted from a web page while an abstract summary may contain words and phrases which do not exist in the original document . a web-page summary can also be either generic or query dependent. a query-dependent summary presents the information that is most relevant to the initial search query  while a generic summary gives an overall sense of the document's content. a generic summary should meet two conditions: maintain a wide coverage of the page's topics and keep low redundancy at the same time . in this paper  we focus on extract-based generic web-page summarization. although much work has been done on this kind of summarization  the result is often not satisfying . one reason is that the textual information of a web page may be scarce  and in some cases  the content contained in a web page is very diverse in topics. for the traditional summarization methods which focus on local contents of a document  it is difficult to capture the true meaning of a web page. what's more  web pages usually contain much noise which is hard to be removed simply by statistical methods. additional knowledge will be very helpful in distinguishing the real content of a web page from noise.
　the objective of this research is to study how to use extra knowledge of the clickthrough data to improve web-page summarization. the clickthrough data contain many users' knowledge on web pages' contents. typically  a user's query words issued before a target page is selected often reflect the true meaning of the target web-page content. therefore  it would be helpful if the knowledge contained in the clickthrough data can be uncovered to complement the web page contents. what's more  the clickthrough data are collected from users all over the world. for a web page with multiple topics  web users may submit different queries to find the topic specific to their respective information needs. thus the query-word set of a page may cover the multiple topics of the target web page. if we can use this query-word set for our summaries  it would be easier for us to produce a generic summary to meet the needs of general web users  in case the summary is biased towards a specific user.
　in this paper  we propose a novel approach to utilize the clickthrough data for web-page summarization. to be sure  this is a challenging task. first  web pages may have no associated query words since they are not visited by any web users through a search engine. this is especially true for nearly emerging pages  where no clickthrough data can be used when they are summarized. second  the clickthrough data are often very noisy. web users may click on pages that are not relevant with their interests and the click will be recorded by a search engine.
　both problems above could be solved by our proposed thematic lexicon. by using the annotated hierarchical taxonomy of web pages such as the one provided by odp website  http://dmoz.org/   we can build a thematic lexicon. for each category  the lexicon contains terms submitted by all web users to browse the pages of this category  as well as the weights of these terms. in conjunction with the proposed summarization methods  the thematic lexicon can be used to complement the scarcity problem of web-page content  even when no clickthrough data are collected associated with these pages. in addition  our method can help filter out noise contained in query words for any web page through the use of statistics over all web pages of a given category. the category-specific thematic lexicon provides term distribution at a category level and reflects users' knowledge on term usage when they locate pages of this category. some noisy terms which may be relatively frequent in one page's query words will be given a low weight by our approach. the result is a more robust summarization system.
　in our new algorithm  we adapt two text-summarization methods to summarize web pages. the first approach is based on significant-word selection adapted from luhn's method . the second method is based on latent semantic analysis  lsa  . the experimental results show that both approaches achieve improvements compared with the puretext-based summarization.
　the rest of the paper is organized as follows. in section 1  we present the related works on web-page summarization and clickthrough data analysis. our proposed summarization algorithms are discussed in section 1. in section 1  the experimental results are given as well as some discussions. finally  we conclude our work in section 1.
1. related work
　web-page summarization techniques have been widely applied in many applications  1  1  1 . in order to generate high-quality summaries  several researchers utilized extra information in web-page summarization. a few works utilize the context information constructed by hyperlinks among web pages  1  1 . with the incommonsense system  amitay and paris first extracted the text segments containing a link to a web page. they then chose the most accurate sentence from the text segments as the snippet of the target page . since the context information may be related to the target web page but contains no clues for summarization  delort et al. proposed two enhanced web-page summarization methods using hyperlinks . in their work  the authors studied the characteristics of context information of a web page as well as the relations between the context information and the target web page content. our work differs from these previous ones mainly in the approaches to uncover and utilize the extra knowledge. the previous methods obtain their knowledge from hyperlinks directly which may be sparse and noisy; in contrast  we get the additional knowledge from the search-engines clickthrough data which are filtered through our noise-removal algorithms. the search queries provide a more accurate semantic labeling for the subsequently clicked web pages  and are found to perform better for web-page summarization. in addition  in the previous works  webpage summaries are extracted from the context text segments  while we extract sentences from a web page  with help of the clickthrough data.
　much research has also been done on using clickthrough data analysis to improve the performance of search algorithms  web-page classification methods or metadata extraction techniques. sun et al. proposed a cubesvd approach to utilizing the clickthrough data for personalized web search . liu et al. proposed a technique for categorizing web-query terms from the clickthrough logs into pre-defined subject taxonomy based on their popular search interests . to the best of our knowledge  few reports in the literature have focused on using the clickthrough data for web-page summarization. there are several works on automatic metadata extraction which are related with our work. hulth et al. proposed to extract keywords using domain knowledge . in   classification based methods were proposed in order to extract metadata inherent in web pages and to build relationships among the metadata and web page categories. in   the authors construct taxonomy of queries by retrieving web pages from search engine to help represent query terms. in this paper  we also construct a thematic hierarchy of query terms. however  we use the clickthrough data instead of web pages. in addition  the research works aforementioned did not concern how to use the extracted knowledge data for web-page summarization  which is the focus of this paper.
1. summarize web pages using clickthrough data
　in this section  we first give several observations on clickthrough data  which validate our assumption that the clickthrough data is beneficial for web-page summarization. then we describe two proposed summarization methods which can leverage the knowledge data extracted from the clickthrough data. finally  we propose an approach to building a thematic lexicon as a knowledge source for summarizing web pages which are not covered by the clickthrough data.
1 empirical study on clickthrough data
　consider a typical search scenario: a user  u  submits a query  q  to search engine  the search engine returns a ranked list of web pages. then the user clicks on the pages  p  of interest. after a period  the server side will accumulate a collection of clickthrough data  which can be represented by a set of triples   u  q  p  . thus the clickthrough data records how web users find information through queries. from a statistical point of view  the query word set corresponding with a web page contains human's knowledge on how the pages are related with their issued queries. some users even refine their queries in order to find the desired information. therefore  the collection of queries is supposed to well reflect the topics of the target web page.
　we first conduct an experiment to investigate whether the query words are related to the topics of the web page. for experimental purpose  we have crawled a set of web pages from odp directory. one month's clickthrough data collected by msn search engine is also available for our experiments. in the clickthrough data  among the 1 web pages accessed by users during this month  1 of them contain  keyword  metadata  which is an important indicator of the content of web pages. according to the statistics  1% of the keywords occur in the query words and 1% of query words appear as keywords. this result supports our hypothesis that query words are indicative of web pages' contents  which motivates us to leverage the clickthrough data to improve web-page summarization.
　in order to give more evidence that clickthrough data are helpful in summarizing web pages  we conducted a second experiment. we collected a small dataset consisting of 1 pages which are covered by the clickthrough data. for evaluation purposes  we asked three human evaluators to conduct a manual summarization task on these web pages  without knowing the queries. each evaluator was asked to extract the sentences which are important for a web page. there is no limit on the number of sentences extracted. according to the statistics  about 1% of the sentences in the original web page contain query words and each sentence contains 1 query words on average. however  for manually created summaries  the percentage of sentences containing queries becomes 1% and the average query word length in each sentence becomes 1. the experiment indicates that the human evaluators tend to extract the sentences with query words as web-page summaries. therefore  it is natural for us to pay close attention to the query words when summarizing the web pages automatically.
　from the above empirical study  we conclude that the clickthrough data are useful in improving the quality of webpage summarization.
1 adapted web-page summarization methods
　suppose that we have a set of query terms for each page now  we propose two adapted summarization methods to leverage the clickthrough data.
1.1 adapted significant word  asw  method
　our first summarization method is adapted from luhn's algorithm  a classical algorithm designed for text summarization . in luhn's method  each sentence is assigned a significance factor and the sentences with high significance factors are selected to form the summary. in order to compute the significance factor of each sentence  a set of significant words are constructed first. in luhn's algorithm  significant words are selected according to word frequency in a document. that is  those words with frequency between high-frequency cutoff and low-frequency cutoff are selected as significant words. then the significance factor of a sentence can be computed as follows:  1  set a limit l for the distance at which any two significant words could be considered as being significantly related.  1  find out a portion in the sentence that is bracketed by significant words not more than l non-significant words apart.  1  count the number of significant words contained in the portion and divide the square of this number by the total number of words within the portion. the result is the significance factor of each sentence.
　in order to customize this procedure to leverage query terms for web-page summarization  we modify the significant word selection method. the basic idea is to use both the local contents of a web page and query terms collected from the clickthrough data to decide whether a word is significant or not. each candidate word is assigned with a significance factor wi given in equation 1.
		 1 
in equation 1  tfip and tfiq denote frequencies of the i-th term in the local text content of a web page and in the query set respectively. the significance factor wi is measured in a weighted combination: α is a trade-off parameter when combining the two significance measurements. after the significance factors for all words are calculated  we rank them and select the top n% as significant words. then we employ luhn's algorithm to compute the significance factor of each sentence.
1.1 adapted latent semantic analysis  alsa  method
　an lsa based text summarization method was proposed in . suppose that there are m distinct terms in a n document collection. the corpus can be represented by a termdocument matrix x （ rm〜n  whose component xij is the weight of term ti in document dj. the singular value decomposition  svd  of x is given by:
	x = uΣv t	 1 
in equation 1  u and v are the matrices of the left and right singular vectors. Σ is the diagonal matrix of singular values. lsa approximates x with a rank-k matrix:
	xk = ukΣkvkt	 1 
by setting the smallest r   k singular values to zero  r is rank of x . that is  the documents are represented in the k dimensional space spanned by column vectors of uk .
　the advantage of lsa derives from its ability to capture the latent relations between terms. in the dimensionreduced space  each singular vector corresponds to a latent concept  with the corresponding singular value measuring the importance of the concept. in   gong et al. proposed an extraction based summarization algorithm. firstly  a term-sentence matrix is constructed from the original text document. next  lsa analysis is conducted on the matrix. in the singular vector space  the i-th sentence is represented by the column vector  i =  vi1 vi1 ，，，  vir t of v t. each element in  i measures the importance factor of this sentence on the corresponding latent concept. in the last step  a document summary is produced incrementally. for the most important concept  the sentence having the largest importance factor is selected into the summary. then  the second sentence is selected for the next most important concept. this procedure repeated until a predefined number of sentences are selected.
　our lsa-based summarization method is a variant of gong's method. we utilize the query-word knowledge by changing the term-sentence matrix: if a term occurs as query words  its weight is increased according to its frequency in query word collection. in this approach  we expect to extract sentences whose topics are related to the ones reflected by query words. as discussed in  1  1   lsa is capable of capturing the latent associations among terms. if a word combination pattern is salient and recurring frequently in a document  this pattern will be captured and represented by one of the singular vectors. if we increase the weights of query terms  these terms  as well as others which frequently co-occur  will make more contributions to singular vector formulation. thus in the sentence extraction step  the candidate sentences which are semantically related with query terms will be selected firstly.
　since the term-sentence matrix determines the svd result  its representation may influence the summarization results. the term frequency vector of each sentence can be weighted by different weighting  global weighting and local weighting  and normalization methods. these schemes are studied in . according to their experiments  the global weighting and normalization schemes lower the summarization performance  while the local weighting schemes produce similar results. thus  in this paper  a term frequency  tf  approach without weighting or normalization is used to represent the sentences in web pages. terms in a sentence are augmented by query terms as follows:
		 1 
in this equation  β is a parameter used to tune the weights of query terms. here  tfip is the frequency of term i in a sentence  while tfiq denotes term frequency in query set.
1.1 advantages of the adapted methods
　there are several advantages for both the above modified summarization algorithms. first  the extra knowledge of query terms is utilized to help select significant words and to modify the page representation. because of the subjective characteristic of word usage  some words may have a relatively low term frequency in the web page  even though they are topic-related. the contributions of these words are constrained because both significant-word method and lsa are term-frequency based methods. however  these words may be used as query words by web-search users as they are related with the topics of the web page. second  our approach can  to some extent  handle the noises of query words  because the contribution of each query term is proportional to its frequency in the query term collection. thus a query term with very low frequency makes few contributions to significance factor calculation and latent concept formation respectively. finally  for luhn's method  the frequency-cutoff method may lead to a lot of significant words for long pages. this problem is avoided in our asw approach by keeping the number of significant words to be n% times the number of distinct terms occurring in the web page. therefore  both the proposed approaches are supposed to produce better summarization results by leveraging the clickthrough data.
1	summarize web pages not covered by
clickthrough data
　according to statistics on the one month's clickthrough data  only 1% out of the crawled odp pages  in english  was browsed and associated with query words  the detailed numbers are given in section 1 . thus for pages which are not browsed by web users  neither summarization method proposed in section 1 can be directly applied on them. in this section  we build a hierarchical lexicon using the clickthrough data and apply it to help summarize those pages.
　since all odp web pages have been manually organized into a hierarchical taxonomy  we combine this large knowledge source and the clickthrough data to build a thematic lexicon. for each category of the taxonomy  the lexicon contains all query terms that users have submitted to browse web pages of this category  as well as weights of these terms  where the latter measures the likelihood that web users will use this term to locate pages of this category. in this paper  we use ts c  to represent a set of terms associated with category c  as well as their corresponding weights. thus the thematic lexicon is a set of ts  which correspond with categories in odp and are organized using the odp category structure. the lexicon is built as follows: first  ts corresponding to each category is set empty. next  for each page covered by the clickthrough data  its query words are added into ts of categories which this page belongs to as well as all its parent categories. when a query word is added into ts  its frequency is added to its original weight in ts. if a page belongs to more than one category  its query terms will be added into all ts associated with all its categories. at last  term weight in each ts is multiplied by its inverse category frequency  icf . the icf value of a term is the reciprocal of its frequency occurring in different categories of the hierarchical taxonomy.
　after the hierarchical lexicon is built  we can use it to summarize web pages that are not covered by the clickthrough data. for each web page to be summarized  we first look up the lexicon for ts according to the page's category. then the summarization methods proposed in section 1 are used. weights of the terms in ts can be used to select significant words or to update the term-sentence matrix. if a page to be summarized has multiple categories  the corresponding ts are merged together and weights are averaged. when a ts does not have sufficient terms  ts corresponding with its parent category is used.
　the hierarchical lexicon-based web-page-summarization method has at least two advantages. first  the categoryspecific ts provides a distribution of topic terms in this category  which reflects web users' knowledge on term usage when they locate information of this category. second  some noisy terms which may be relatively frequent in one page's query words will be given a low weight through the use of statistics over all web pages of this category.
1. experiments
　in this section  we will investigate whether the adapted web-page summarization methods are superior to the ones without using clickthrough data. we introduce the experiment data set  the evaluation metrics and the experiment results.
1 data set
　the clickthrough data were collected from the msn search engine. this data set contains about 1 million records of 1 days from dec 1 of 1 to jan 1 of 1. as we collected the clickthrough data  a set of web pages of the odp directory are crawled. among the 1 1 web pages crawled  we removed those which belong to  world  and  regional  categories  as many of them are not in english. at last we got 1 1 web pages  1 of which are clicked by web users using 1 1 different queries.
　two different data sets were used. the first one  denoted by dat1  consists of 1 pages which are selected from the 1 browsed pages. table 1 gives the query numbers associated with each page of dat1. three human evaluators were employed to summarize these pages. each evaluator was requested to extract the sentences which he/she deemed to be the most important ones for a web page. there is no constraint on the number of sentences to be extracted. table 1 describes the overall consistencies among the three evaluators. for example  for the evaluator1:evaluator1 pair  1 means 1% sentences in evaluator1's summary are also included in evaluator1's summary. each number given in this table is an average result over all web pages. from table 1  we can find the three evaluators have a relatively high disparity on the 1 pages. similar observations of high disparities between human evaluators are also observed in previous works  such as . thus  in this paper  the experiments are evaluated using the annotation results of all three evaluators. the average results are also reported.
　we also use a relatively large scale data set  denoted by dat1  to evaluate our summarization methods. we preprocess the 1 pages contained in the clickthrough data using a layout analysis algorithm. after the content body of each page is extracted  the textual content is segmented into sentences by a sentence segmenting program implemented in our group. in addition  descriptions of each page   description  metadata  are also extracted. we keep the web pages with a description of over 1 characters and containing at least 1 sentences  from which 1 pages are randomly selected and constitutes dat1 data set. since the description is provided by the page editor to give a general description of this page  we use it as the ideal summary.
1 performance evaluation
　both intrinsic and extrinsic methodes are proposed for automatic summarization evaluation  1  1 . in this paper  we employ two intrinsic evaluation approaches to evaluate the proposed approaches.
1.1 precision  recall and f1
　precision  recall and f1 are straightforward measures widely used in summarization evaluation. for each document  the manually extracted sentences are considered as the reference table 1: number of queries associated with annotated pages
number of queriesnumber of pages 11 ゛ 11 ゛ 1  1total1table 1: consistiencies among the human evalua-
tors
evaluator1evaluator1evaluator1evaluator111evaluator1.11evaluator1.1.1summary. this approach compares the candidate summary with the reference summary and computes the precision  recall and f1 values:
p = | sref ” scand |;r = | sref ” scand |;f1 = 1pr
	| scand |	| sref |	p + r
 1 
where scand and sref denotes the sentences contained in the candidate summary and the reference summary respectively.
1.1 rouge evaluation
　rouge1 is a software package adopted by duc1 for automatic summarization evaluation . it measures summarization quality by counting overlapping units such as the n-gram  word sequences  and word pairs between the candidate summary and the reference summary. rouge-n is an n-gram recall measure which is defined as follows:
countmatch  gramn 
s（{ref}gramn（s
rouge   n =   1 
count gramn 
s（{ref}gramn（s
in equation 1  n stands for the length of the n-gram 
countmatch  gramn  is the maximum number of n-grams co-occurring in the candidate summary and the reference summary  count gramn  is the number of n-grams in the candidate summary.
　according to   among the evaluation methods implemented in rouge  rouge-n  n=1  1  is relatively simple and works well in both single document summarization evaluation tasks and evaluation of very short summaries. in this paper  we evaluated our experiments using all the methods provided by the rouge package and only reported rouge-n where n=1  since the conclusions drawn from different methods are quite similar.
1 experimental results and analysis
　in this subsection  we conduct experiments to evaluate the summarization methods proposed in section 1. on

the dat1 dataset  in order to evaluate our methods on summaries of different evaluators  we keep the number of sentences extracted be equal with that of the human summary. in this case  the precision  recall and f1 measure are all equal. on both the data sets  the summarization methods are evaluated by the rouge software.
1.1 summarization results on dat1
　we first conduct experiments to investigate whether the adapted summarizers can benefit from query terms associated with each page. for the asw method  we vary α from 1 to 1  in step 1  to change the influence of query terms on significant word selection. the summarization results are listed in figure 1  measured by precision and rouge-1 respectively. when α is 1  the clickthrough data is ignored and only the local features of a web page are used to select significant words. when α is 1  only query words are used. for alsa  the parameter β is also varied from 1 to 1. the results are given in figure 1. from the two figures  we can find that both methods achieve significant improvements when evaluated on the manual summaries of different evaluators. for asw method  the average precision of the three evaluators got a relative improvement of 1%  from 1  when no query terms are used  to 1  when query terms are used and α = 1 . the average rouge-1 measure got a relative improvement of 1%  from 1 to 1 . the alsa method achieves a relative improvement of 1% and 1% when measured by precision and rouge-1 respectively  compared with without using the clickthrough.
　on this data set  we also evaluate our summarization methods using the thematic lexicon approach. we first use the odp directory and the clickthrough data to build a thematic lexicon. since the clickthrough data contains only 1 pages  our lexicon contains 1 categories  which is a subset of the odp category structure. when we summarize a page  we ignore the query words associated with it. as described in section 1  the thematic lexicon is used to help generate query words. we select query terms from the lexicon based on the category of the page to be summarized. if terms under this category have more than p% overlap with distinct terms in the web page  then they are used for summarization. otherwise  we try to use lexicon terms of its parent category. this process continues until we find a category which covers enough query terms or until we reach the root of the thematic lexicon. in this experiment  p is empirically set to 1. evaluation results are described in figure 1 and figure 1. we can find that the asw method achieves an improvement of 1% and 1% measured in precision and rouge-1 respectively. the alsa method achieves an improvement of 1% and 1% when measured in precision and rouge-1 respectively.
1.1 summarization results on dat1
　from the above experimental results  we can find the rouge-1 measure and the precision measure are consistent with each other in most cases  especially when the average results of the three evaluators are averaged. on this data set  only rouge-1 measure is used for evaluation. the extract-summary of each page is generated by our proposed summarization algorithms and the leading characters of length equal with the description sentences are used for evaluation. the results are illustrated in figure 1. in figure 1   text  denotes summarization based on textual content

figure 1: summarization results on dat1  evaluated by rouge-1 measure
of web pages.  query  denotes summarization using query words issued to locate web pages.  lexicon  denotes the page queries are ignored while the thematic lexicon is used to help summarization. since the description length is commonly short and the rouge-1 measure is recall based  the summarization results are relatively poor. from the results in figure 1  we can find that the clickthrough data can improve the web-page summarization. even the real queries of web pages are dropped  the thematic lexicon-based methods can still lead to better summaries compared with local textual content based summarizers  although the performance is not so good as that of page-query based methods.
1.1 discussions
　all the above experiments indicate the clickthrough data are helpful for generic web-page summarization and both our proposed methods can leverage this knowledge source well. when the thematic lexicon is used to help summarize the web pages which are not covered by the clickthrough data  the improvements are not as significant as when queries of a page are directly used. however  the lexicon-based approach achieves better results compared with pure-text-based summarizers. this is because the thematic lexicon built from clickthrough data can discover the topic terms associated with a specific category and the icf-based approach can effectively assign weights to terms of this category. as illustrated in table 1  the top 1 terms of two categories are listed. from this table  we can find that icfbased re-weighting can help discover topic terms of a specific category  while terms like  com    www    download  are assigned with lower weights. although there are high disparities between different human summarizers  improvements can be achieved when the results are evaluated on summaries of each evaluator. this indicates our methods can help produce summaries which meet general web users. in most cases  our methods achieve optimal results when the local contents of a web page and the clickthrough data are combined together  which verify our hypothesis that the clickthrough data can complement the textual contents of web pages for summarization tasks.
1. conclusions and future work
　we leverage extra knowledge from clickthrough data to improve web-page summarization. two extract-based methods are proposed to produce generic web-page summaries. for the pages which are not covered by the clickthrough
　

	weight of query terms: α	weight of query terms: α
	 a  measured by precision	 b  measured by rouge-1
figure 1: summarization results using asw method  on dat1 with query words

	weight of query terms: β	weight of query terms: β
	 a  measured by precision	 b  measured by rouge-1
figure 1: summarization results using alsa method  on dat1 with query words

	weight of query terms: α	weight of query terms: α
	 a  measured by precision	 b  measured by rouge-1
figure 1: summarization results using asw method  on dat1 without queries

	weight of query terms: β	weight of query terms: β
	 a  measured by precision	 b  measured by rouge-1
figure 1: summarization results using alsa method  on dat1 without queries
　
table 1: top 1 terms in thematic lexicon  before and after icf re-weighting approach is applied
 after stemming 
computer/softwarecomputer/hardwarebefore icfafter icfbefore icfafter icfmapquestmapquestcomlogitechcomkazaawwwepsonwwwwinmxlogitechlexmarkfreewebshotepsonpnykazaawinziplexmarktoshibawallpapwinrartoshibafactorydirectscreensavimeshcomputviewsonchristmawinampsharpmicrocentscreenspybottivomsidownloadmicrosofttwmstroxdata  we build a thematic lexicon using the clickthrough data in conjunction with an available hierarchical web directory. the experimental results show that significant improvements are achieved compared with summarizers without using clickthrough logs.
　our experiments indicate the trade-off parameter can influence the summarization result when either proposed method is used. therefore it would be interesting to propose a method to determine its value automatically. besides  we will also study how to leverage other types of knowledge  such as word clusters and thesaurus  hidden in the clickthrough data to enhance web page summarization. we also plan to evaluate our methods using extrinsic evaluation metrics and much larger data sets.
1. acknowledgments
　the authors would like to thank ya-bin kang for his help on organizing the web page annotation process and trying some experiments in this work  and xue-mei jiang for preparing the clickthrough data. the sudy was funded in part by natural science foundation under the grant number 1 and 1. qiang yang and dou shen are supported by hong kong rgc hkust1e.
