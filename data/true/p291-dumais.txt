this paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online.   most question answering systems use a wide variety of linguistic resources.  we focus instead on the redundancy available in large corpora as an important resource.   we use this redundancy to simplify the query rewrites that we need to use  and to support answer mining from returned snippets.  our system performs quite well given the simplicity of the techniques being utilized.  experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages.  simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines. 
categories and subject descriptors 
h.1.  content analysis and indexing   h.1  information search and retrieval . 
general terms 
algorithms  experimentation. 
1. introduction 
question answering has recently received attention from the information retrieval  information extraction  machine learning  and natural language processing communities  the goal of a question answering system is to retrieve 'answers' to questions rather than full documents or even best-matching passages as most information retrieval systems currently do.   the trec question answering track which has motivated much of the recent work in the field focuses on fact-based  short-answer questions such as  who killed abraham lincoln   or  how tall is mount everest  .   in this paper we focus on this kind of question answering task  although the techniques we propose are more broadly applicable. 
the design of our question answering system is motivated by recent observations in natural language processing that  for many applications  significant improvements in accuracy can be attained simply by increasing the amount of data used for learning.  following the same guiding principle we take advantage of the tremendous data resource that the web provides as the backbone of our question answering system.  many groups working on question answering have used a variety of linguistic resources - part-of-speech tagging  syntactic parsing  semantic relations  named entity extraction  dictionaries  wordnet  etc.  e.g.   .we chose instead to focus on the web as gigantic data repository with tremendous redundancy that can be exploited for question answering.   the web  which is home to billions of pages of electronic text  is orders of magnitude larger than the trec qa document collection  which consists of fewer than 1 million documents.   this is a resource that can be usefully exploited for question answering.    we view our approach as complimentary to more linguistic approaches  but have chosen to see how far we can get initially by focusing on data per se as a key resource available to drive our system design. 
automatic qa from a single  small information source is extremely challenging  since there is likely to be only one answer in the source to any user's question.  given a source  such as the trec corpus  that contains only a relatively small number of formulations of answers to a query  we may be faced with the difficult task of mapping questions to answers by way of uncovering complex lexical  syntactic  or semantic relationships between question string and answer string.  the need for anaphor resolution and synonymy  the presence of alternate syntactic formulations  and indirect answers all make answer finding a potentially challenging task.  however  the greater the answer redundancy in the source data collection  the more likely it is that we can find an answer that occurs in a simple relation to the question.  therefore  the less likely it is that we will need to resort to solving the aforementioned difficulties facing natural language processing systems. 
1. exploiting redundancy for qa 
we take advantage of the redundancy  multiple  differently phrased  answer occurrences  available when considering massive amounts of data in two key ways in our system. 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  august 1  1  tampere  finland. 
copyright 1 acm 1-1/1...$1. 
 
enables simple query rewrites.  the greater the number of information sources we can draw from  the easier the task of rewriting the question becomes  since the answer is more likely to be expressed in different manners.  for example  consider the difficulty of gleaning an answer to the question  who killed abraham lincoln   from a source which contains only the text  john wilkes booth altered history with a bullet.  he will forever be known as the man who ended abraham lincoln's life     versus a source that also contains the transparent answer string   john wilkes booth killed abraham lincoln.  
facilitates answer mining.  even when no obvious answer strings can be found in the text  redundancy can improve the efficacy of question answering.  for instance  consider the question:  how many times did bjorn borg win wimbledon    assume the system is unable to find any obvious answer strings  but does find the following sentences containing  bjorn borg  and  wimbledon   as well as a number: 
 1  bjorn borg blah blah  wimbledon blah blah 1 blah  
 1  wimbledon blah blah blah bjorn borg blah 1 blah. 
 1  blah bjorn borg blah blah 1 blah blah wimbledon   
 1  1 blah blah wimbledon blah blah bjorn borg. 
by virtue of the fact that the most frequent number in these sentences is 1  we can posit that as the most likely answer.   
1. related work 
other researchers have recently looked to the web as a resource for question answering.   the mulder system described by kwok et al.  is similar to our approach in several respects.  for each question  mulder submits multiple queries to a web search engine and analyzes the results.  mulder does sophisticated parsing of the query and the full-text of retrieved pages  which is far more complex and compute-intensive than our analysis.  they also require global idf term weights for answer extraction and selection  which requires local storage of a database of term weights.  they have done some interesting user studies of the mulder interface  but they have not evaluated it with trec queries nor have they looked at the importance of various system components. 
clarke et al.  investigated the importance of redundancy in their question answering system.   in  they found that the best weighting of passages for question answering involves using both passage frequency  what they call redundancy  and a global idf term weight.  they also found that analyzing more top-ranked passages was helpful in some cases and not in others.   their system builds a full-content index of a document collection  in this case trec.   in  they use web data to reinforce the scores of promising candidate answers by providing additional redundancy  with good success.   their implementation requires an auxiliary web corpus be available for full-text analysis and global term weighting.   in our work  the web is the primary source of redundancy and we operate without a full-text index of documents or a database of global term weights. 
buchholz's shapaqa nlp system  has been evaluated on both trec and web collections.  question answering accuracy was higher with the web collection  although both runs were poor in absolute terms   but few details about the nature of the differences are provided. 
these systems typically perform complex parsing and entity extraction for both queries and best matching web pages     which limits the number of  web pages that they can analyze in detail.   other systems require term weighting for selecting or ranking the best-matching passages    and this requires auxiliary data structures.  our approach is distinguished from these in its simplicity  simple rewrites and string matching  and efficiency in the use of web resources  use of only summaries and simple ranking .   we now describe how our system uses redundancy in detail and evaluate this systematically.    
1. system overview 
a flow diagram of our system is shown in figure 1.  the system consists of four main components. 
rewrite query. given a question  the system generates a number of rewrite strings  which are likely substrings of declarative answers to the question.  to give a simple example  from the question  when was abraham lincoln born   we know that a likely answer formulation takes the form  abraham lincoln was born on  date  .  therefore  we can look through the collection of documents  searching for such a pattern.   
we first classify the question into one of seven categories  each of which is mapped to a particular set of rewrite rules.  rewrite rule sets range in size from one to five rewrite types.   the output of the rewrite module is a set of 1-tuples of the form  string  l/r/-  weight   where  string  is the reformulated search query   l/r/-  indicates the position in the text where we expect to find the answer with respect to the query string  to the left  right or anywhere  and  weight  reflects how much we prefer answers found with this particular query.  the idea behind using a weight is that answers found using a high precision query  e.g.   abraham lincoln was born on   are more likely to be correct than those found using a lower precision query  e.g.   abraham  and  lincoln  and  born  . 
we do not use a parser or part-of-speech tagger for query reformulation  but do use a lexicon in order to determine the possible parts-of-speech of a word as well as its morphological variants.   we created the rewrite rules and associated weights manually for the current system  although it may be possible to learn query-to-answer reformulations and weights  e.g. see 
agichtein et al. ; radev et al.  .   
the rewrites generated by our system are simple string-based manipulations.  for instance  some question types involve query rewrites with possible verb movement; the verb  is  in the question  where is the louvre museum located   should be moved in formulating the desired rewrite to  the louvre museum is located in .  while we might be able to determine where to move a verb by analyzing the sentence syntactically  we took a much simpler approach.  given a query such as  where is w1 w1 ... wn   where each of the wi is a word  we generate a rewrite for each possible position the verb could be moved to  e.g.  w1 is w1 ... wn    w1 w1 is ... wn   etc .  while such an approach results in many nonsensical rewrites  e.g.  the louvre is museum located in    these very rarely result in the retrieval of bad pages  and the proper movement position is guaranteed to be found via exhaustive search.  if we instead relied on a parser  we would require fewer query rewrites  but a misparse would result in the proper rewrite not being found. 
for each query we also generate a final rewrite which is a backoff to a simple anding of the non-stop words in the query.  we could backoff even further to ranking using a best-match retrieval system which doesn't require the presence of all terms and uses differential term weights  but we did not find that this was necessary when using the web as a source of data.  there are an average of 1 rewrites for the 1 trec-1 queries used in the experiments described below.   
as an example  the rewrites for the query  who created the character of scrooge   are: 
left 1  created +the character +of scrooge  
right 1  +the character +of scrooge +was created +by  
and 1  created  and  +the character  and  +of scrooge  
and 1  created  and  character  and  scrooge  
to date we have used only simple string matching techniques.  soubbotin and soubbotin  have used much richer regular expression matching to provide hints about likely answers  with very good success in trec 1  and we could certainly incorporate some of these ideas in our rewrites.    note that many of our rewrites require the matching of stop words like  in  and  the   in the above example.  in our system stop words are important indicators of likely answers  and we do not ignore them as most ranked retrieval systems do  except in the final backoff and rewrite.   
the query rewrites are then formulated as search engine queries and sent to a search engine from which page summaries are collected and analyzed.  
mine n-grams.  from the page summaries returned by the search engine  n-grams are mined.   for reasons of efficiency  we use only the returned summaries and not the full-text of the corresponding web page.   the returned summaries contain the query terms  usually with a few words of surrounding context.  in some cases  this surrounding context has truncated the answer string  which may negatively impact results.  the summary text is then processed to retrieve only strings to the left or right of the query string  as specified in the rewrite triple.   
1-  1-  and 1-grams are extracted from the summaries.  an n-gram is scored according the weight of the query rewrite that retrieved it.  these scores are summed across the summaries that contain the n-gram  which is the opposite of the usual inverse document frequency component of document/passage ranking schemes .  we do not count frequency of occurrence within a summary  the usual tf component in ranking schemes .  thus  the final score for an n-gram is based on the rewrite rules that generated it and the number of unique summaries in which it occurred.  when searching for candidate answers  we enforce the constraint that at most one stopword is permitted to appear in any potential n-gram answers. 
the top-ranked n-grams for the scrooge query are: 
dickens 1 
christmas carol 1 
charles dickens 1 
disney 1 
carl banks 1 a christmas 1 uncle 1 
filter/reweight n-grams.  next  the n-grams are filtered and reweighted according to how well each candidate matches the expected answer-type  as specified by a handful of handwritten filters.  the system uses filtering in the following manner. first  the query is analyzed and assigned one of seven question types  such as who-question  what-question  or how-many-question.  based on the query type that has been assigned  the system determines what collection of filters to apply to the set of potential answers found during n-gram harvesting. the answers are analyzed for features relevant to the filters  and then rescored according to the presence of such information.   
a collection of approximately 1 filters were developed based on human knowledge about question types and the domain from which their answers can be drawn.  these filters used surface string features  such as capitalization or the presence of digits  and consisted of handcrafted regular expression patterns.  
after the system has determined which filters to apply to a pool of candidate answers  the selected filters are applied to each candidate string and used to adjust the score of the string.  in most cases  filters are used to boost the score of a potential answer when it has been determined to possess the features relevant to the query type. in other cases  filters are used to remove strings from the candidate list altogether. this type of exclusion was only performed when the set of correct answers was determined to be a closed set  e.g.  which continent....    or definable by a set of closed properties  e.g.  how many...   . 
tile n-grams. finally  we applied an answer tiling algorithm  which both merges similar answers and assembles longer answers out of answer fragments.  tiling constructs longer n-grams from sequences of overlapping shorter n-grams. for example   a b c  and  b c d  is tiled into  a b c d.  the algorithm proceeds greedily from the top-scoring candidate - all subsequent candidates  up to a certain cutoff  are checked to see if they can be tiled with the current candidate answer. if so  the higher scoring candidate is replaced with the longer tiled n-gram  and the lower scoring candidate is removed. the algorithm stops only when no n-grams can be further tiled. 
the top-ranked n-grams after tiling for the scrooge query are: 
charles dickens 1 
a christmas carol  1 
walt disney's uncle 1 carl banks 1 uncle 1 
our system works most efficiently and naturally with a backend retrieval system that returns best-matching passages or queryrelevant document summaries.   we can  of course  post-process the full text of matching documents to extract summaries for ngram mining  but this is inefficient especially in web applications where the full text of documents would have to be downloaded over the network at query time. 
1. experiments 
for our experimental evaluations we used the first 1 trec-1 queries  1  . for simplicity we ignored queries which are syntactic rewrites of earlier queries  1   although including them does not change the results in any substantive way.   we used the patterns provided by nist for automatic scoring.  a few patterns were slightly modified to accommodate the fact that some of the answer strings returned using the web were not available for judging in trec-1.   we did this in a very conservative manner allowing for more specific correct answers  e.g.  edward j. smith vs. edward smith  but not more general ones  e.g.  smith vs. edward smith   and simple substitutions  e.g.  1 months vs. nine months .   these changes influence the absolute scores somewhat but do not change relative performance  which is our focus here.   
many of the trec queries are time sensitive - that is  the correct answer depends on when the question is asked.  the trec database covers a period of time more than 1 years ago; the web is much more current.  because of this mismatch  many correct answers returned from the web will be scored as incorrect using the trec answer patterns. 1% of the trec queries have temporal dependencies.  e.g.  who is the president of bolivia   what is the exchange rate between england and the u. s.   we did not modify the answer key to accommodate these time differences  because this is a subjective job and would make comparison with earlier trec results impossible.   
for the main web retrieval experiments we used google as a backend because it provides query-relevant summaries that make our n-gram mining techniques more efficient.  thus we have access to more than 1 billion web pages.  for some experiments in trec retrieval we use the standard qa collection consisting of news documents from disks 1.  the trec collection has just under 1 million documents . 
all runs are completely automatic  starting with queries and generating a ranked list of 1 candidate answers.  candidate answers are a maximum of 1 bytes long  and typically much shorter than that.  we report the mean reciprocal rank  mrr  of the first correct answer  the number of questions correctly answered  numcorrect   and the proportion of questions correctly answered  propcorrect .  correct answers at any rank are included in the number and proportion correct measures.  most correct answers are at the top of the list -- 1% of the correct answers occur in the first position and 1% in the first or second positions. 
using our system with default settings for query rewrite weights  number of summaries returned  etc. we obtain a mrr of 1 and answer 1% of the queries.  the average answer length was 1 bytes  so the system is really returning short answers not passages.   this is very good performance and would place us near the top of 1-byte runs for trec-1.  however  since we did not take part in trec-1 it is impossible to compare our results precisely with those systems  e.g.  we used trec-1 for tuning our trec-1 system increasing our score somewhat  but we return several correct answers that were not found in trec-1 thus decreasing our score somewhat .   
redundancy is used in two key ways in our data-driven approach.  first  the occurrence of multiple linguistic formulations of the same answers increases the chances of being able to find an answer that occurs within the context of a simple pattern match with the query.  second  answer redundancy facilitates the process of answer extraction by giving higher weight to answers that occur more often  i.e.  in more different document summaries .   we now evaluate the contributions of these experimentally. 
1 number of snippets 
we begin by examining the importance of redundancy in answer extraction.  to do this we vary the number of summaries  snippets  that we get back from the search engine and use as input to the n-gram mining process.   our standard system uses 1 snippets.    we varied the number of snippets from 1 to 1.  the results are shown in figure 1.      


number of snippetsfigure 1.  mrr as a function of number of snippets returned.  trec-1  queries 1. performance improves sharply as the number of snippets increases from 1 to 1  1 mrr for 1 snippet  1 mrr for 1  1 mrr for 1  and 1 for 1   somewhat more slowly after that  peaking 1 mrr with 1 snippets   and then falling off somewhat after that as more snippets are included for n-gram analysis.     thus  over quite a wide range  the more snippets we consider in selecting and ranking n-grams the better.  we believe that the slight drop at the high end is due to the increasing influence that the weaker rewrites have when many snippets are returned.  the most restrictive rewrites return only a few matching documents.   increasing the number of snippets increases the number of the least restrictive matches  the and matches   thus swamping the restrictive matches.  in addition  frequent n-grams begin to dominate our rankings at this point. 
an example of failures resulting from too many and matches is query 1: what is the longest word in the english language   for this query  there are 1 snippets matching the rewrite  is the longest word in the english language  with weight 1  1 more snippets matching the rewrite  the longest word in the english language is  with the weight 1  and more than 1 snippets matching the backoff and query   longest  and  word  and  english  and  language   with a weight of 1.  when 1 snippets are used  the precise rewrites contribute almost as many snippets as the and rewrite.   in this case we find the correct answer   pneumonoultramicroscopicsilicovolcanokoniosis   in the second rank.    the first answer   1 letters long   which is incorrect  also matches many precise rewrites such as  the longest word in english is ## letters long   and we pick up on this.    when 1 snippets are used  the weaker and rewrites dominate the matches.  in this case  the correct answer falls to seventh on the list after  letters long    one syllable    is screeched    facts    stewardesses  and  dictionary   all of which occur commonly in results from the least restrictive and rewrite.   a very common and match contains the phrase  the longest one-syllable word in the english language is screeched   and this accounts for two of our incorrect answers.   
using differential term weighting of answer terms  as many retrieval systems do  should help overcome this problem to some extent but we would like to avoid maintaining a database of global term weights.  alternatively we could refine our weight accumulation scheme to dampen the effects of many weakly restrictive matches by sub-linear accumulation  and we are currently exploring several alternatives for doing this.      
our main results on snippet redundancy are consistent with those reported by clarke et al.   although they worked with the much smaller trec collection.  they examined a subset of the trec-1 queries requiring a person's name as the answer.  they varied the number of passages retrieved  which they call depth  from 1 to 1  and observed some improvements in mrr.  when the passages they retrieved were small  1 or 1 bytes  they found improvement  but when the passages were larger  1 or 1 bytes  no improvements were observed.   the snippets we used are shorter than 1 bytes  so the results are consistent.  clarke et al.  also explored a different notion of redundancy  which they refer to as ci .  ci is the number of different passages that match an answer.  their best performance is achieved when both ci and term weighting are used to rank passages.  we too use the number of snippets that an n-gram occurs in.   we do not  however  use global term weights  but have tried other techniques for weighting snippets as described below. 
1 trec vs. web databases  
another way to explore the importance of redundancy is to run our system directly on the trec documents.   as noted earlier  there are three orders of magnitude more documents on the web than in the trec qa collection.     consequently  there will be fewer alternative ways of saying the same thing and fewer matching documents available for mining the candidate n-grams.   we suspect that this lack of redundancy will limit the success of our approach when applied directly on trec documents.   
while corpus size is an obvious and important difference between the trec and web collections there are other differences as well.  for example  text analysis  ranking  and snippet extraction techniques will all vary somewhat in ways that we can not control.  to better isolate the size factor  we also ran our system against another web search engine.     
for these experiments we used only the and rewrites and looked at the first 1 snippets.   we had to restrict ourselves to and rewrites because some of the search engines we used do not support the inclusion of stop words in phrases  e.g.   created +the character +of scrooge . 
1.1 trec database 
the trec qa collection consists of just under 1 million documents.  we expect much less redundancy here compared to the web  and suspect that this will limit the success of our approach.   an analysis of the trec-1 query set  1  shows that no queries have 1 judged relevant documents.   only 1 of the 1 questions have 1 or more relevant documents  which the results in figure 1 suggest are required for the good system performance.  and a very large number of queries  1  have fewer than 1 relevant documents.   
we used an okapi backend retrieval engine for the trec collection.   since we used only boolean and rewrites  we did not take advantage of okapi's best match ranking algorithm.   however  most queries return fewer than 1 documents  so we wind up examining most of the matches anyway. 
we developed two snippet extraction techniques to generate query-relevant summaries for use in n-gram mining.  a contiguous technique returned the smallest window containing all the query terms along with 1 words of context on either side.  windows that were greater than 1 words were ignored.  this approach is similar to passage retrieval techniques albeit without differential term weighting.  a non-contiguous technique returned the union of two word matches along with 1 words of context on either side.  single words not previously covered are included as well.  the search engine we used for the initial web experiments returns both contiguous and non-contiguous snippets.     figure 1 shows the results of this experiment. 
and rewrites only  top 1
	mrr	numcorrect	propcorrectweb1	1	1.1trec  contiguous snippet	1	1.1trec  non-contiguous snippet	1	1.1	figure 1: web vs. trec as data source	 
our baseline system using all rewrites and retrieving 1 snippets achieves 1 mrr  figure 1 .  using only the and query rewrites results in worse performance for our baseline system with 1 mrr  figure 1 .  more noticeable than this difference is the drop in performance of our system using trec as a data source compared to using the much larger web as a data source.   mrr drops from 1 to 1 for contiguous snippets and 1 for non-contiguous snippets  and the proportion of questions answered correctly drops from 1% to 1% for contiguous snippets and 1% for non-contiguous snippets.   it is worth noting that the trec mrr scores would still place this system in the top half of the systems for the trec-1-byte task  even though we tuned our system to work on much larger collections.  however  we can do much better simply by using more data.  the lack of redundancy in the trec collection accounts for a large part of this drop off in performance.  clarke et al.  have also reported better performance using the web directly for trec 1 questions.   
we expect that the performance difference between trec and the web would increase further if all the query rewrites were used.   this is because there are so few exact phrase matches in trec relative to the web  and the precise matches improve performance by 1%  1 vs. 1 .     
we believe that database size per se  and the associated redundancy  is the most important difference between the trec and web collections.  as noted above  however  there are other differences between the systems such as text analysis  ranking  and snippet extraction techniques.  while we can not control the text analysis and ranking components of web search engines  we can use the same snippet extraction techniques.  we can also use a different web search engine to mitigate the effects of a specific text analysis and ranking algorithms.  
1.1 another web search engine 
for these experiments we used the msnsearch search engine.  at the time of our experiments  the summaries returned were independent of the query.  so we retrieved the full text of the top 1 web pages and applied the two snippet extraction techniques described above to generate query-relevant summaries.  as before  all runs are completely automatic  starting with queries  retrieving web pages  extracting snippets  and generating a ranked list of 1 candidate answers.   the results of these experiments are shown in figure 1.   the original results are referred to as web1 and the new results as web1. 
and rewrites only  top 1
	mrr	numcorrect	propcorrectweb1	1	1.1trec  contiguous snippet	1	1.1trec  non-contiguous snippet	1	1.1web1  contiguous snippet	1	1.1web1  non-contiguous snippet	1	1.1	figure 1: web vs. trec as data source	 
the web1 results are somewhat worse than the web1 results  but this is expected given that we developed our system using the web1 backend  and did not do any tuning of our snippet extraction algorithms.  in addition  we believe that the web1 collection indexed somewhat less content than web1 at the time of our experiments  which should decrease performance in and of itself.   more importantly  the web1 results are much better than the trec results for both snippet extraction techniques  almost doubling mrr in both cases.   thus  we have shown that qa is more successful using another large web collection compared to the small trec collection.  the consistency of this result across web collections points to size and redundancy as the key factors. 
1.1 combining trec and web 
given that the system benefits from having a large text collection from which to search for potential answers  then we would expect that combining both the web and trec corpus would result in even greater accuracy.  we ran two experiments to test this.  because there was no easy way to merge the two corpora  we instead combined the output of qa system built on each corpus.  for these experiments we used the original web1 system and our trec system.  we used only the and query rewrites  looked at the top1 search results for each rewrite  and used a slightly different snippet extraction technique.  for these parameter settings  the base trec-based system had a mrr of .1  the web-based system had a mrr of .1. 
first  we ran an oracle experiment to assess the potential gain that could be attained by combining the output of the web-based and trec-based qa systems.  we implemented a  switching oracle   which decides for each question whether to use the output from the web-based qa system or the trec-based qa system  based upon which system output had a higher ranking correct answer.  the switching oracle had a mrr of .1  a 1% improvement over the web-based system.  note that this oracle does not precisely give us an upper bound  as combining algorithms  such as that described below  could re-order the rankings of outputs. 
next  we implemented a combining algorithm that merged the outputs from the trec-based and web-based systems  by having both systems vote on answers  where the vote is the score assigned to a particular answer by the system.  for voting  we defined string equivalence such that if a string x is a substring of y  then a vote for x is also a vote for y.  the combined system achieved a mrr of .1  a 1% improvement over the webbased system  and answered 1 questions correctly. 
1 snippet weighting 
until now  we have focused on the quantity of information available and less on its quality.     snippet weights are used in ranking n-grams.   an n-gram weight is the sum of the weights for all snippets in which that n-gram appears. 
each of our query rewrites has a weight associated with it reflecting how much we prefer answers found with this particular query.  the idea behind using a weight is that answers found using a high precision query  e.g.   abraham lincoln was born on   are more likely to be correct than those found using a lower precision query  e.g.   abraham  and  lincoln  and  born  .  our current system has 1 weights.  
these rewrite weights are the only source of snippet weighting in our system.    we explored how important these weight are and considered several other factors that could be used as additional sources of information for snippet weighting.  although we specify boolean queries  the retrieval engine can provide a ranking  based on factors like link analyses  proximity of terms  location of terms in the document  etc.   so  different weights can be assigned to matches at different positions in the ranked list.  we also looked at the number of matching terms in the best fixed width window  and the widow size of the smallest matching passage as indicators of passage quality. 
rewrite wts uses our heuristically determined rewrite weights as a measure the quality of a snippet.   this is the current system default.  equal wts gives equal weight to all snippets regardless of the rewrite rule that generated them.  to the extent that more precise rewrites retrieve better answers  we will see a drop in performance when we make all weights equal.  rank wts uses the rank of the snippet as a measure of its quality  snippetwt =  1rank /1.  nmatch wts uses the number of matching terms in a fixed-width window as the measure of snippet quality.  length wts uses a measure of the length of the snippet needed to encompass all query terms as the measure of snippet quality.  we also look at combinations of these factors.  for example  rewrite+rank wts uses both rewrite weight and rank according to the following formula  snippetwt = rewritescore +  1rank /1.   all of these measures are available from queryrelevant summaries returned by the search engine and do not require analyzing the full text of the document. the results of these experiments are presented in figure 1. 
weightingmrrnumcorrectpropcorrectequal wts11.1rewrite wts  default 11.1rank wts11.1rewrite + rank wts11.1nmatch wts11.1length wts11.1	figure 1: snippet weighting	 
our current default 1-level weighting scheme which reflects the specificity of the query rewrites does quite well.   equal weighting is somewhat worse  as we expected.   interestingly search engine rank is no better for weighting candidate n-grams than equal weighting.   none of the other techniques we looked at surpasses the default weights in both mrr and propcorrect.  our heuristic rewrite weights provide a simple and effective technique for snippet weighting  that can be used with any backend retrieval engine.   
most question answering systems use ir-based measures of passage quality  and do not typically evaluate the best measure of similarity for purposes of extracting answers.  clarke et al.  noted above is an exception.   soubbotin and soubbotin  mention different weights for different regular expression matches  but they did not describe the mechanism in detail nor did they evaluate how useful it is.  harabagiu et al.  have a kind of backoff strategy for matching which is similar to weighting  but again we do not know of parametric evaluations of its importance in their overall system performance.  the question of what kinds of passages can best support answer mining for question answering as opposed to document retrieval is an interesting one that we are pursuing. 
1. discussion and future directions 
the design of our question answering system was motivated by the goal of exploiting the large amounts of text data that is available on the web and elsewhere as a useful resource.   while many question answering systems take advantage of linguistic resources  fewer depend primarily on data.   vast amounts of data provide several sources of redundancy that our system capitalizes on.   answer redundancy  i.e.  multiple  differently phrased  answer occurrences  enables us to use only simple query rewrites for matching  and facilitates the extraction of candidate answers.    
we evaluated the importance of redundancy in our system parametrically.   first  we explored the relationship between the number of document snippets examined and question answering accuracy. accuracy improves sharply as the number of snippets included for n-gram analysis increases from 1 to 1  somewhat more slowly after that peaking at 1 snippets  and then falls off somewhat after that.  more is better up to a limit.  we believe that we can increase this limit by improving our weight accumulation algorithm so that matches from the least precise rewrites do not dominate. second  in smaller collections  like trec   the accuracy of our system drops sharply  although it is still quite reasonable in absolute terms.     finally  snippet quality is less important to system performance than snippet quantity.  we have a simple 1-level snippet weighting scheme based on the specificity of the query rewrite  and this appears to be sufficient.   more complex weighting schemes that we explored were no more useful.   
the performance of our system shows promise for approaches to question answering which makes use of very large text databases even with minimal natural language processing.  our system does not need to maintain its own index nor does it require global term weights  so it can work in conjunction with any backend retrieval engine.  finally  since our system does only simple query transformations and n-gram analysis  it is efficient and scalable. 
one might think that our system has limited applicability  because it works best with large amounts of data.  but  this isn't necessarily so.   first  we actually perform reasonably well in the smaller trec collection  and could perhaps tune system parameters to work even better in that environment.    more interestingly   brill et al.  described a projection technique that can be used to combine the wealth of data available on the web with the reliability of data in smaller sources like trec or an encyclopedia.  the basic idea is to find candidate answers in a large and possibly noisy source  and then expand the query to include likely answers.   the expanded queries can then be used on smaller but perhaps more reliable collections - either directly to find support for the answer in the smaller corpus  or indirectly as a new query which is issued and mined as we currently do.   this approach appears to be quite promising.   our approach seems least applicable in applications that involve a small amount of proprietary data.   in these cases  one might need to do much more sophisticated analyses to map user queries to the exact lexical form that occur in the text collection rather than depend on primarily on redundancy as we have done. 
although we have pushed the data-driven perspective  more sophisticated language analysis might help as well by providing more effective query rewrites or less noisy data for mining.    
most question answering systems contain aspects of both - we use some linguistic knowledge in our small query typology and answer filtering  and more sophisticated systems often use simple pattern matching for things like dates  zip codes  etc.   
there are a number of open questions that we hope to explore.  in the short term  we would like to look systematically at the contributions of other system components.  brill et al.  have started to explore individual components in more detail  with interesting results.  in addition  it is likely that we have made several sub-optimal decisions in our initial implementation  e.g.  omitting most stop words from answers  simple linear accumulation of scores over matching snippets  that we would like to improve.   most retrieval engines have been developed with the goal of finding topically relevant documents.  finding accurate answers may require somewhat different matching infrastructure.  we are beginning to explore how best to generate snippets for use in answer mining.  finally  time is an interesting issue.  we noted earlier how the correct answer to some queries changes over time.  time also has interesting implications for using redundancy.  for example  it would take a while for a news or web collection to correctly answer a question like  who is the u. s. president   just after an election. 
an important goal of our work is to get system designers to treat data as a first class resource that is widely available and exploitable.   we have made good initial progress  and there are several interesting issues remaining to explore. 
