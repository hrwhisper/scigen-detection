　in this paper we report on our trec-1 sdr system  which combines an adapted version of the limsi 1 hub-1e transcription system for speech recognition with an ir system based on the okapi term weighting function. experimental results are given in terms of word error rate and average precision for both the sdr'1 and sdr'1 data sets. in addition to the okapi approach  we also investiged a markovian approach  which although not used in the trec-1 evaluation  yields comparable results. the evaluation system obtained an average precision of 1 on the reference transcriptions and of 1 on the automatic transcriptions. the word error rate measured on a 1 hour subset is of 1%.
introduction
　there expansion of different media sources for information dissemination  radio  television  internet  has led to a need for automatic processing tools. todays methods for audio segmentation  transcription and indexation are manual  with humans reading  listening and watching  annotating topics and selecting items of interest for the user. even partial automation of some of these activitiescan allow more information sources to be processed and significantly reduce processing costs while eliminating tedious work. some application areas that could benefit from automated transcription and indexing technology include the creation and access to digital multimedia libraries  disclosure of the information content and content-based indexation  such as are under exploration in the olive  project   media monitoring services  selective dissemination of information based on automatic detection of topics of interest  as well as new emerging applications such as news on demand  such as the informedia  project  and internet watch services. such applications are feasible due to the large technological progress made over the last decade  benefiting from advances in micro-electronics which have facilitated the implementation of more complex models and algorithms.
automatic speech recognition is a key technology for au-
dio and video indexing  for data such as radio and television broadcasts. most of the linguistic information is encoded in the audio channel of video data  which once transcribed can be accessed using text-based tools. this is in contrast to the image data for which no common description language is available.
　in this paper we describe the limsi spoken document indexing and retrieval system developed for the trec-1 sdr evaluation. this system combines a state-of-the-art speech recognizer  with an okapi-based ir system. a markovian-based ir system has also been developed and contrastive experimental results using this system are provided. all of our development work was carried out using the trec-1 sdr data set  1 hours  and the associated set of 1 queries. this year's sdr task was quite more challenging than the sdr'1 track in that the audio data was increased to about 1 hours of broadcasts  which has strong implications on the transcription process. the next section describes the limsi speech transcriptionsystem and the modifications made for use in this evaluation  trying to find the best compromise between accuracy and speed. in the following section the two ir systems are presented  and experimental results for various configurations are provided.
transcribingbroadcastnews
　at limsi we have been working on using statistical models to transcribe broadcast news data since 1. due to the availability of large audio and textual corpora via the linguistic data consortium  ldc    most of our work on broadcast news transcription has been carried out on american english. in the context of the ec le olive project   broadcast news transcriptionsystems forfrench and german have recently been developed.
　radio and television broadcast shows are challenging to transcribe as they contain signal segments of various acoustic and linguistic natures. the signal may be of studio quality or may have been transmitted over a telephone or other noisy channel  i.e.  corrupted by additive noise and nonlinear distortions   or can contain speech over music or pure

http://www.ldc.upenn.edu
acoustic models
figure 1: overview of transcription system for audio stream.music segments. gradual transitions between segments occur when there is background music or noise with changing volume  and abrupt changes are common when there is switching between speakers in different locations. the speech is produced by a wide variety of speakers: news anchors and talk show hosts  reporters in remote locations  interviews with politicians and common people  unknown speakers  new dialects  non-native speakers  etc. speech from the same speaker may occur in different parts of the broadcast  and with different background noise conditions. the linguistic style ranges from prepared speech to spontaneous speech. acoustic and language modeling must accurately account for this varied data.
　two principle types of problems are encountered in automatically transcribing broadcast news data: those relating to the varied acoustic properties of the signal  and those related to the linguistic properties of the speech. problems associated with the acoustic signal properties are handled using appropriate signal analyses  by classifying the signal according to segment type and by training acoustic models for the different acoustic conditions. noise compensation is also needed in order to achieve acceptable performance levels. most broadcast news transcription systems make use of unsupervised acoustic model adaptation as opposed to noise cancelation  which allow adaptation without an explicit noise model. in order to address the variability observed in the linguistic properties  the differences in speaking styles need to analyzed withregard to lexical items  word and word sequence pronunciations  and frequencies and distribution of hesitations  filler words  and respiration noises. once such an analysis is carried out. the variability needs to be accounted for in the acoustic and language models .
system overview
　the limsi sdr'1 transcription system shown in figure 1  is based on the limsi 1 hub-1e system which achieved an official word error of 1% in the nov'1 arpa evaluation. prior to recognition the audio stream is first partitioned. data partitioning serves to divide the continuous stream of acoustic data into homogenous segments  associating appropriate labels with each segment. the segmentation and labeling procedure  first detects and rejects non-speech segments  and then applies an iterative maximum likelihood segmentation/clustering procedure to the speech segments. the result of the partitioning process is a set of speech segments with cluster  gender and telephoneband/wideband labels. the speech recognizer uses continuous density hmms with gaussian mixture observation densities for acoustic modeling and 1-gram statistics for language modeling. the states of the context-dependent phone models are tied by means of a decision tree.
audio partitioner
　the goal of partitioning is to divide the continuous audio stream into homogeneous acoustic segments  to remove non-speech segments and to assign bandwidth and gender labels to each segment. the audio partitioning procedure  introduced for the nov'1 evaluation  1  1  and used in the limsi nov'1 hub-1e system   is as follows:
1. first  the non-speech segments are detected  and rejected  using gaussian mixture models  gmms . four gmms  each with 1 gaussians serve to detect speech  pure-music and other  background . all test segments labeled as music or silence are removed prior to further processing.
1. an iterative maximum likelihood segmentation/clustering procedure is then applied to the speech segments using gmms and an agglomerative clustering algorithm. given the sequence of cepstral vectors  the algorithm tries to maximize an objective function which is a penalized log-likelihood. alternate viterbi reestimation and agglomerative clustering yields a sequence of estimates with non decreasing values of the objective function. the algorithm stops when no further merges are possible. the cluster size is constrained to ensure that each cluster corresponds to at least 1s of speech. this procedure is controlled by 1 parameters: the minimum cluster size  1s   the maximum log-likelihood loss for a merge  and the segment boundary penalty.
when no more merges are possible  the segment boundaries are refined  within a 1s interval  using the last set of gmms and an additional relative energy-based boundary penalty. this is done to locate the segment boundaries at silence portions  so as to avoid cutting words.
1. speaker-independent gmms corresponding to wideband and telephone speech  each with 1 gaussians  are then used to label the segment bandwidths. this is followed by segment-based gender identification  using 1 sets of gmms with 1 gaussians  one for each bandwidth . the result of the partitioning process is a set of speech segments with cluster  gender and telephone/wideband labels.
speech recognizer
　as usual  1  1  1  1   the acoustic feature vector contains 1 cepstral parameters derived from a mel frequency spectrum estimated on the 1khz band  or 1.1khz band for telephone data  every 1ms. for each 1ms frame the mel scale power spectrum is computed  and the cubic root taken followed by an inverse fourier transform. then lpc-based cepstrum coefficients are computed. the cepstral coefficients are normalized on a segment-cluster basis using cepstral mean removal and variance normalisation. thus each cepstral coefficient for each clusterhas a zero mean and unity variance. the 1-component acoustic feature vector consists of 1 cepstrum coefficents and the log energy  along with the first and second order derivatives. each phone model is a tied-state left-to-right cd-hmm with gaussian mixtures. the triphone-based context-dependent phone models are word-independent but position-dependent. the tied states are obtained by means of a decision tree.
　the decoding procedure of the limsi nov'1 hub-1e system has been changed in order to reduce the computation time required to process the 1 hours of bn data for the sdr'1 evaluation. word recognition is performed in three passes:
1. word graph generation: an initial hypothesis and word graph are generated using a small bigram-backoff language model and gender-specific sets of positiondependent cross-word triphones.
1. 1-gram decoding with acoustic model adaptation: unsupervised acoustic model adaptation is performed for each segment cluster using the mllr technique  using the initial hypotheses. each segment is decoded with a trigram language model  the adapted acoustic models  and the word graph.
1. 1-gram decoding with acoustic model adaptation: the final hypotheses are generated using a 1-gram language model with acoustic model adaptation using the hypotheses of pass 1.
acoustic model training
　we used the acoustic models of the limsi nov'1 hub1e system. these models were trained on about 1 hoursof broadcast data  only the official hub-1e training data from 1  1  and 1 . the acoustic models are positiondependent triphones with about 1 tied states  1k gaussians   obtained using a divisive decision tree based clustering algorithm. two sets of gender-dependent acoustic models were built using map  adaptation of si seed models for each of wideband and telephone band speech. a portion of the hub-1e training data was also used to build the gaussian mixture models for partitioning speech  music and noise models  and for gender and bandwidth identification. about 1 hours of pure music portions taken from the acoustic trainingdata were used toestimate the music gmm.
language model training
　the language models of the limsi nov'1 hub-1e system were used. the language models are fixed and were obtained by interpolation of backoff -gram language models trained on different data sets. to build the -gram lm  four models trained on the following sources were interpolated:
1. bn transcriptions from ldc  years 1  and frompsmedia  years 1 and 1  the period 1/1/1 was excluded : 1 m words
1. nab newspaper texts and ap wordstream texts priorto september 1: 1 m words
1. nab newspaper texts and ap wordstream texts fromjuly 1 to august 1  the period 1/1/1 was excluded  : 1 m words
1. transcriptions of the acoustic data  bn data  includingthe 1 marketplace data : 1m words
　the interpolationcoefficients of these fourlms were chosen so as to minimize the perplexity on the nov'1 and nov'1 evaluation test sets. a backoff 1-gram lm is then derived from this interpolation by merging the four component lms . bigram and trigram lms were build in a similar manner for use in the first two decoding steps.
　all words occuring a minimum of 1 times in the broadcast news texts  1 words  or at least twice in the acoustic training data  1 mots  were included in the recognition vocabulary  resulting in a 1 word list. the lexical coverage is 1% on the hub-1e nov'1 eval test set and 1% on the hub-1e nov'1 eval test set.
　the bn texts from psmedia  also used for query expansion in our ir system  were processed using a modified version of a perl script from bbn made available by ldc. the bn training texts were cleaned in order to be homogeneous with the previous texts. these texts were processed so as to treat some frequent word sequences as compound words  and to treat the most frequent acronyms in the training texts as whole words instead of as sequences of independent letters.
lexicon
　pronunciations are based on a 1 phone set  1 of them are used for silence  filler words  and breath noises . a pronunciation graph is associated with each word so as to allow for alternate pronunciations  including optional phones. the 1k vocabulary contains 1 words including1 phone transcriptions. frequent inflected forms have been verified to provide more systematic pronunciations. as done in the past  compound words for about 1 frequent word sequences subject to reduced pronunciations were included in the lexicon as well as the representationof frequent acronyms as words.
transcription results
　table 1 reports the word recognition results on the eval test sets from the last three years. all of our system development was carried out using the hub-1e eval1 and the sdr'1 data set. for the sdr'1 data set we built a system respecting the rules from last year's sdr evaluation. since the sdr'1 test data is part of the standard hub-1e training data  acoustic models were trained on only about 1 hours of acoustic data as opposedto 1h. similarlylanguage models were trained using only those texts predating the test epoch  jan'1 .
　the word transcription error is seen to be on the order of 1% on the broadcast data. the better results for the hub1e nov'1  h1  and nov'1  h1  test sets are due to prior selection of the test data to include a higher proportion of prepared speech. the word error of the sdr'1 is about 1% higher than the limsi nov'1 hub-1e system. the difference in performance of the sdr'1 and sdr'1 systems can be attributed to the difference in training data.
test set  word error h1h1h1sdr1sdr1system1 h1 h1 h1 h1 hhub1111--sdr11111table 1: summary of bn transcription word error rates on the 1 last darpa evaluation test sets  h1  h1  h1  and the sdr'1 and'1 test sets using the limsi hub1 systemand the limsi sdr'1 system about 1xrt . resultson the sdr'1test set were obtained with a system trained on about half the amount of acoustic data and less lm texts  in accordence with the sdr'1 evaluation condition.
informationretrieval
　our sdr'1 ir system has been designed following the okapi approach . in order for the same ir system to be applied to different text data types  automatic transcriptions  closed captions  additional texts from newspapers or newswires   all of the documents are preprocessed in a homogeneous manner. this preprocessing or tokenization  described below  is the same as what is done to prepare text sources for training the speech recognizer language models   and attempts to transform them to be closer to the observed american speaking style. there is no stop list  that is to say no words are discarded during the pre-processing stage. the index terms are obtained after translation using a lexicon of stems. query expansion is obtainedvia blindrelevance feedback  brf  using both the sdr'1 audio data collection and a parallel text corpus of broadcast news transcripts.
　all development was carried out using exclusively the sdr'1 evaluation data  consisting of about 1 documents with the associated 1 queries. two approaches for ir were explored  the first based on the okapi term weighting function and the second using a markovian one  1  1 . due to the limited amount of development data and our limited experience with ir systems  we chose to submit the okapi-based system for the evaluation even though comparable results were being obtained with the markovian approach. some comparative results for the two approaches are given at the end of this section.
　the parameter values were chosen to simultaneously optimize performance on automatic recognizer transcripts and the provided manual reference transcriptions. better ir performance can be obtained if the parameters for the two transcription types are optimized independently  but this would result in two different ir systems. it is also worth noting that the reference transcripts of the sdr'1 data are detailed manual transcriptions  whereas for the sdr'1 data these are closed captions. the different transcript types made us uncertain as to the reliability of our development work.
tokenization
　the tokenizer transforms the texts to a unified format. the basic operations include translating numbers and sums into words  removing all the punctuation signs  removing case distinctions and detecting acronyms and spelled names such as k.g.b. however removing all punctuation markers implies that certain hyphenated words such as anti-communist  non-profit are rewritten as anti communist and non profit. while this offers advantages for speech recognition  it can lead to ir errors. to avoid ir problems due to this transformation  the output of the tokenizer  and recognizer  is checked for common prefixes  in order to rewrite the sequence of words anti communist as a single word. the prefixes that are handled include anti  co  bi  counter. a rewrite lexicon containing compound words formed with these prefixes and a limited number of named entities  such as losangeles  saint-tropez  is used to transform the texts. similarly all numbers less thanone hundredare treated as a single entity  such as twenty-seven .
stemming
　in order to reduce the number of lexical items for a given word sense  each word is translated into its stem  as defined in  1  1   or  more generally  into a form that is chosen as being representative of its semantic family. the stemming lexicon  using the umass 'porterized' lexicon   contains about 1 entries and was constructed using porter's algorithm on the most frequent words in the collection  and then manually corrected.
　the ir term list was limited to 1k entries  after stemming  for implementation reasons. for the sdr'1 audio data collection  this filtering only affected the r1 condition where the least frequent terms were removed.
baseline search
　the score of a document for a query is given by the okapi-bm1 formula. it is the sum over all the terms in the query of the following weights:
tf
cw	qtf 	 1  tf
　where tf is the number of occurrences of term in document  i.e. term frequency in document   is the number of documents containing term at least once  is the total number of documents in the collection  is the length of document divided by the average length of the documents in the collection  and qtf the number of occurrences of term in the query.
　the parameter values of the okapi formula were chosen in an attempt to maximize the average precision on the sdr'1 data set. the resulting values were thus a compromise between the optimal configuration for the r1 and s1 conditions  in order to be able to use the same values for both conditions. the s1 transcripts were obtained with a speech recognizer trained on 1 hours of acoustic data and language model training texts predating the test period. the recognition word error rate on this data  using the nist sdr'1 scoring procedure  was 1%  cf. table 1 . the parameters were fixed for all the evaluation conditions at: =1; and =1 for the baseline run withoutquery expansion  and =1 with query expansion.
query expansion
　the text of the query may or may not include the index terms associated with relevant documents. one way to cope with this problem is to use query expansion based on terms present in retrieved documents on the same  blind relevance feedback  or other  parallel blind relevance feedback  data collections. we have experimented withboth approaches  and our submitted system incorporated both brf and pbrf using 1 months of commercially available broadcast news transcripts for the period of june-december 1 . this corpus contains 1 stories and 1 m words.
　for a given query  the terms foundin the top documents from the baseline search are ranked by their offer weight  ow    and the top terms are added to the query. as proposed in  the following formula for ow was used:
	ow		 1 
	where	is the number of documents  among the	doc-
uments  containing the term .
　since only the terms with best offer weights are kept  we filtered the terms using a stop list of 1 common words  inorder to increase the likehoodthat these terms are relevant.
databasebrfpbrfbrf+pbrfr1.1.1.1.1s1.1.1.1.1table 1: development ir results on the sdr'1 data set   =1 
　=1 	=1 	=1  for the baseline system  and with 1 configurations for query expansion.
　four experimental configurations are reported in table 1 for the sdr'1 development data: baseline search  base   query expansion using brf  brf   query expansion with parallel brf  pbrf  and query expansion using both brf and pbrf  brf+pbrf . for brf and pbrf  the terms are added to the query with a weight of 1. for brf+pbrf  the terms from each source are added with a weight of 1. the parameter values used for these experiments are the result of our development work. we felt that it was safest to add only a few terms  assuming that only a small number of documents were relevant. therefore the development experiments compared performance for relatively small values of and   with the best performance being obtained with and
     . the results reported in table 1 clearly demonstrate the interest of using both brf and pbrf expansion techniques with consistent and comparable improvements over the baseline for the two conditions r1 and s1 . as has been previously reported by other sites  there is only a slight performance degradation in going from the r1 condition to the s1 condition  even with a transcription word error of 1%.
evaluationresults
　the parameter setting optimized on the sdr'1 data set  cf. table 1  were used for all our submissions on the sdr'1 data set. table 1 summarizes the results of the limsi ir system for the r1  s1  and cross-recognizer conditions. in addition to the official numbers obtained with query expansion using both brf and pbrf  the results for the 1 other configurations  no query expansion  query expansion with brf and query expansion with pbrf  are also provided.
databasebrfpbrfbrf+pbrfr1.1.1.1.1s1.1.1.1.1b1.1.1.1.1b1.1.1.1.1htk1111att1111shef1111cmu1111table 1: limsi official ir results on the sdr'1 data set   =1  =1  =1  =1 .
　the highest average precision is obtained on the manual transcriptions  r1: 1   but as already observed on our development results the performance degradation using speech recognizer outputs is fairly modest  1% and 1% for the htk and limsi automatic transcriptions . comparing tables 1 and 1  it can be observed that the gain using pbrf for query expansion is smaller on the sdr'1 data set than it was on the sdr'1 data set. this is may be linked to the choice of the epoch for the pbrf corpus or to a suboptimal tuning of the brf parameters.
additionalresults
　in this section some post-evaluation experiments with the okapi-based system are reported. we also report here some of the development experiments comparing the okapi and markovian approaches.
adjusting system parameters
　having no experience with ir system tuning before this evaluation  we found it rather difficult to properly set the okapi parameters   and   and the query expansion parameters   and    so as to maximize the average precision for both the r1 and s1 conditions on the sdr'1 test set with the associated 1 queries.
　extensive experiments were carried out to investigate the ir performance for a range of parameter values. figures 1 through 1 show the effect of the okapi parameters   and   on the average precision for sdr'1-r1  sdr'1-r1 and sdr'1-s1 respectively  using a baseline system without query expansion. the iso-data lines of the resulting surfaces are shown  along with their projections on the base plane which highlightsthe location of the extrema.
　figures 1 through 1 show the effect of the brf parameters   and   on the average precision for sdr'1-r1  sdr'1-r1 and sdr'1-s1 respectively  using the system with query expansion based on both brf and pbrf.
　it is clear from these plots that the best parameter settings for the sdr'1 data set cannot be easily predicted from the sdr'1 results. however it was clearly possible to choose better brf parameter values than those resulting from our development work. in particular too few terms are kept  i.e. the value was really underestimated . new results using
 =1  which corresponds to the best results on the sdr'1 data  are given in table 1  label cw for the okapi term weighting .
markovian term weigthing
　as a natural extension of our work on speech recognitionrelyingon markovian assumptions for bothacoustic and language modeling  we investigated a term weighting function based on a simple query/document model in place of the okapi formula. a comparable approach has been previously employed with success  1  1 . assuming a unigram model  the following term weighting is used:
	mw	qtf	 1 
　table 1 gives the results for both okapi  cw  and markovian  mw  term weightings on the sdr'1 data set with the following parameter settings: =1  =1  =1  =1  =1. in both cases query expansion relies on the term offer weight defined above. it can be seen that very comparable results can be achieved using the two term weighting schemes.
datameth.basebrfpbrfbrf+pbrf1-r1cw1111mw11111-s1cw1111mw11111-r1cw1111mw11111-s1cw1111mw1111table 1: comparison of ir results on the sdr'1 and sdr'1 data setsusingboth okapi andmarkovian term weightings  =1  =1  =1  =1  =1 . r1: reference transcript. s1: automatic speech transcription.

figure 1: plot of average precision vs okapi parameters	and
for the baseline system  no query expansion   sdr'1 - r1.  best
avep is 1 for =1 and	1 

figure 1: plot of average precision vs okapi parameters	and for the baseline system  no query expansion   sdr'1 - r1.  best avep is 1 for =1 and	1 .

figure 1: plot of average precision vs okapi parameters	and for the baseline system  no query expansion   sdr'1 - s1.  best avep is 1 for =1 and	1 .

figure 1: plot of averageprecision vs brf parameters and for brf+pbrf query expansion  sdr1 - r1.  best avep is 1 for =1 and =1 .

figure 1: plot of averageprecision vs brf parameters and for brf+pbrf query expansion  sdr1 - r1.  best avep is 1 for =1 and =1 .

figure 1: plot of average precision vs brf parameters and for brf+pbrfquery expansionsdr1 - s1.  best avep is 1 for =1 and =1 .
summary&discussion
　in thispaper we have presented our complete sdr'1system  and highlighted our development work. this system was built by combining an adapted version of the limsi 1 hub-1e transcription system for speech recognition with an ir system based on the okapi term weighting function. the transcription system achieved a word error of 1% measured on a 1h subset of the sdr'1 data set. using the parameter settings optimized on the sdr'1 data set  average precision of 1 and 1 respectively were obtainedon the sdr'1and sdr'1data sets usingthe transcriptions produced by the limsi recognizer. these values are quite close the the average precisions obtained on manual transcripts  indicating that the transcription quality is not the limiting factor on ir performance. our post-evaluation experiments indicate that  unfortunately  the evaluation settings for the brf were suboptimal.
acknowledgments
　this work has been partially financed by the european commission and the french ministry of defense. the authors gratefully acknowledge the participation of miche`le jardino  remi lejeune and patrick paroubek to this work.
