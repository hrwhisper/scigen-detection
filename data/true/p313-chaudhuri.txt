to ensure high data quality  data warehouses must validate and cleanse incoming data tuples from external sources. in many situations  clean tuples must match acceptable tuples in reference tables. for example  product name and description fields in a sales record from a distributor must match the pre-recorded name and description fields in a product reference relation.  
a significant challenge in such a scenario is to implement an efficient and accurate fuzzy match operation that can effectively clean an incoming tuple if it fails to match exactly with any tuple in the reference relation. in this paper  we propose a new similarity function which overcomes limitations of commonly used similarity functions  and develop an efficient fuzzy match algorithm. we demonstrate the effectiveness of our techniques by evaluating them on real datasets.  
1. introduction 
decision support analysis on data warehouses influences important business decisions; therefore  accuracy of such analysis is crucial. however  data received at the data warehouse from external sources usually contains errors  e.g.  spelling mistakes  inconsistent conventions across data sources  missing fields. consequently  a significant amount of time and money are spent on data cleaning  the task of detecting and correcting errors in data. a prudent alternative to the expensive periodic data cleaning of an entire data warehouse is to avoid the introduction of errors during the process of adding new data into the warehouse. this approach requires input tuples to be validated and corrected before they are loaded. there is much information that can be used to achieve this goal. 
a common technique validates incoming tuples against reference relations consisting of known-to-be-clean tuples. the reference relations may be internal to the data warehouse  e.g.  customer or product relations  or obtained from external sources  e.g.  valid address relations from postal departments . an enterprise maintaining a relation consisting of all its products may ascertain whether or not a sales record from a distributor describes a valid product by matching the product attributes  e.g.  part number and description  of the sales record with the product relation; here  the product relation is the reference relation. if the product attributes in the sales record match exactly with a tuple in the product relation  then the described product is likely to be valid. 
however  due to errors in sales records  often the input product 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod 1  june 1  1  san diego	ca. 
copyright 1 acm 1-1-x/1...$1. 
tuple does not match exactly with any in the product relation. then  errors in the input product tuple need to be corrected before it is loaded. the information in the input tuple is still very useful for identifying the correct reference product tuple  provided the matching is resilient to errors in the input tuple. we refer to this error-resilient matching of input tuples against the reference table as the fuzzy match operation.  
suppose the enterprise wishes to ascertain whether or not the sales record describes an existing customer by fuzzily matching the customer attributes of the sales record against the customer relation. the reference relation  customer  contains tuples describing all current customers. if the fuzzy match returns a target customer tuple that is either exactly equal or  reasonably close  to the input customer tuple  then we would have validated or corrected  respectively  the input tuple. the notion of closeness between tuples is usually measured by a similarity function. as shown in figure 1  if the similarity between an input customer tuple and its closest reference tuple is higher than some threshold  then the correct reference tuple is loaded. otherwise  the input is routed for further cleaning before considering it as referring to a new customer. a fuzzy match operation that is resilient to input errors can effectively prevent the proliferation of fuzzy duplicates  in a relation  i.e.  multiple tuples describing the same real world entity.  

our goal in this paper is to develop a robust and efficient fuzzy match algorithm  applicable across a wide variety of domains. we want a solution that provides a strong foundation for adding domain-specific enhancements. most data warehouses are built atop database systems. consequently  we require besides robustness and efficiency that the fuzzy match solution is implemented over standard database systems without assuming the persistence of complex data structures.  
the critical ingredient of a fuzzy match operation is the similarity function used for comparing tuples. in typical application domains  the similarity function must definitely handle stringvalued attributes and possibly even numeric attributes. in this paper  we focus only on string-valued attributes  where defining similarity and performing fuzzy matching is more challenging. given the similarity function and an input tuple  the goal of the fuzzy match operation is to return the reference tuple-a tuple in the reference relation-which is closest to the input tuple. an extension is to return the closest k reference tuples enabling users  if necessary  to choose one among them as the target  rather than the closest. a further extension is to only output k or fewer tuples whose similarity to the input tuple exceeds a user-specified minimum similarity threshold. this formulation is essentially that of the nearest neighbor problem  but there the domain is typically a euclidean  or other normed  space with well-behaved similarity functions . in our case  the data are not represented in  geometric  spaces  and it is hard to map them into one because the similarity function is relatively complex. 
previous approaches addressing the fuzzy match operation either adopt proprietary domain-specific functions  e.g.  trillium's reference matching operation for the address domain   or use the string edit distance function for measuring similarity between tuples . a limitation of the edit distance is illustrated by the following example. the edit distance function would consider the input tuple i1 in table 1 to be closest to r1 in table 1  even though we know that the intended target is r1. edit distance fails because it considers transforming 'corporation' to 'company' more expensive than transforming 'boeing' to 'bon.' however  we know that 'boeing' and '1' are more informative tokens than 'corporation' and so replacing 'corporation' with 'company' should be considered cheaper than replacing 'boeing' with 'bon' and '1' with '1.'  in yet another example  note that the edit distance considers i1 closer to r1 than to its target r1. this is because it fails to capture the notion of a token or take into account the common error of token transposition. 
  	table 1: organization reference relation 
idorg. namecitystatezipcoder1boeing companyseattlewa1r1bon corporationseattlewa1r1companionsseattlewa1 
	table 1: input organization tuples 	 
idorg. namecitystatezipcodei1beoing companyseattlewa1i1beoing co.seattlewa1i1boeing corporationseattlewa1i1company beoingseattlenull1we start by proposing a new fuzzy match similarity  fms  function  which views a string as a sequence of tokens and recognizes the varying  importance  of tokens by explicitly associating weights quantifying their importance. tuples matching on high weight tokens are more similar than tuples matching on low weight tokens. we adopt the successful inverse document frequency  idf  weights from the ir literature for quantifying the notion of token importance; informally  the importance of a token decreases with its frequency  which is the number of times a token occurs in the reference relation . even though the approach of weight association is common in the ir literature  the effective use of token weights in combination with data entry errors  e.g.  spelling mistakes  missing values  inconsistent abbreviations  has not been considered earlier.  
our notion of similarity between two tuples depends on the minimum cost of  transforming  one tuple into the other through a sequence of transformation operations  replacement  insertion  and deletion of tokens  where the cost of each transformation operation is a function of the weights of tokens involved. for example  it may be cheaper to replace the token 'corp' with 'corporation' than to replace 'corporal' with 'corporation' even though edit distances suggest otherwise. this notion of similarity based on transformation cost is similar to edit distance except that we operate on tokens and explicitly consider their weights.  
the goal of the fuzzy match algorithm is to efficiently retrieve the k reference tuples closest to an input tuple. it is well-known that efficiently identifying the exact k nearest neighbors even according to the euclidean and hamming norms in highdimensional spaces is hard . since the hamming norm is a special case of the edit distance obtained by allowing only replacements  the identification of the exact closest k matches according to our fuzzy match similarity-which generalizes edit distance by incorporating token weights-is essentially hard. therefore  we adopt a probabilistic approach where the goal is to return the closest k reference tuples with high probability. we pre-process the reference relation to build an index relation  called the error tolerant index  eti  relation  for retrieving at run time a small set of candidate reference tuples  which we then compare with the input tuple. our retrieval algorithm is probabilistically safe because we retrieve  with high probability  a superset of the k reference tuples closest to the input tuple. it is efficient because the superset is significantly  often by several orders of magnitude  smaller than the reference relation. the index relation eti is implemented and maintained as a standard relation  and hence our solution can be deployed even over current operational data warehouses.  
our main contributions are the following. we propose a new fuzzy match similarity function that explicitly considers idf token weights and input errors while comparing tuples. we propose the error tolerant index and a probabilistic algorithm for efficiently retrieving the k reference tuples closest to the input tuple  according to the fuzzy match similarity function. finally  we present a thorough empirical evaluation on real datasets. our techniques are extensible to use specialized  possibly domainspecific  token weight functions instead of the idf weights. 
the rest of the paper is organized as follows. in section 1  we discuss related work. in section 1  we define the new similarity function. in section 1  we describe  i  our algorithm to build the eti  and  ii  our retrieval algorithm for efficiently identifying the target reference tuples. in section 1  we discuss a few extensions to the algorithm. in section 1  we discuss a thorough empirical study on real datasets  and conclude in section 1.  
1. related work 
several methods for approximate string matching over dictionaries or collections of text documents have been proposed  e.g.     . all of the above methods use edit distance as the similarity function  not considering the crucial aspect of differences in importance of tokens while measuring similarity.  
approximate string matching methods  e.g.  1  1  preprocess the set of dictionary/text strings to build q-gram tables containing tuples for every string s of length q that occurs as a substring of some reference text string; the record also consists of the list of identifiers  or locations  of strings of which s is a substring. the error tolerant index relation eti we build from the reference relation is similar in that we also store q-grams along with the list of record identifiers in which they appear  but the eti  i  is smaller than a full q-gram table because we only select  probabilistically  a subset of all q-grams per tuple  and  ii  encodes column-boundaries specific to relational domains.  
the information retrieval community has successfully exploited inverse document frequency  idf  weights for differentiating the importance of tokens or words. however  the ir application assumes that all input tokens in the query are correct  and does not deal with errors therein. only recently  some search engines  e.g.  google's  did you mean   feature  are beginning to consider even simple spelling errors. in the fuzzy match operation  we deal with tuples containing very few tokens  many times  around 1 or less  and hence cannot afford to ignore erroneous input tokens  as they could be crucial for differentiating amongst many thousands of reference tuples. for example  the erroneous token 'beoing' in the input tuple  beoing corporation  seattle  wa  null  is perhaps the most useful token for identifying the target from among all corporations in seattle. clustering and reference matching algorithms  e.g.  1  1  1  using the cosine similarity metric with idf weighting also share the limitation of ignoring erroneous input tokens. further  cohen et al. improve efficiency by choosing probabilistically a subset of tokens from each document under the correct input token assumption . in this paper  we propose a similarity function that does not assume correctness of input tokens  and further improve efficiency by exploiting the variance in weights of input tokens. 
as discussed earlier  almost all solutions for the nearest neighbor problem are targeted at data in euclidean/normed spaces  and hence inapplicable to our setting. there has been some recent work on general metric spaces  e.g.  1  1   but their complexity and performance are not suitable for the high-throughput systems of interest here. moreover  many of these solutions cannot be deployed easily over current data warehouses because they require specialized index structures  e.g.  m-trees  tries  to be persisted.  
some recent techniques addressed the related problem of eliminating  fuzzy duplicates  in a relation by using a similarity function and identifying highly similar tuples as duplicates. some are based on the use of edit distance  e.g.  1   some on cosine similarity with idf weights  e.g.  1   some on learning similarity functions from training datasets  e.g.  1  1   and some on the use of dimension hierarchies . however  all such techniques are designed for use in an offline setting and do not satisfy the efficiency requirements of the online fuzzy match operation where input tuples have to be quickly matched with target reference tuples before being loaded into the data warehouse. a complementary use of solutions to both problems is to first clean a relation by eliminating fuzzy duplicates and then piping further additions through the fuzzy match operation to prevent introduction of new fuzzy duplicates. 
several commercial products  e.g.  trillium  vality  axciom  leverage characteristics peculiar to the address domain in their proprietary algorithms for matching addresses and individual or organization records. the record linkage literature-a survey can be found in -also considers the problem of identifying matching records across relations  consisting mainly of census records of individuals   and employs a variety of  domainspecific  similarity functions. in contrast  our goal in this paper is to develop a domain-independent method.  
1. the similarity function 
c o m p      a        n y  c o r  p o r a t i o n 1 1 1 1 1 in this section  we define the fuzzy match similarity  fms  function for comparing tuples. we start with a few definitions. 
edit distance: the edit distance ed s1  s1  between two strings s1 and s1 is the minimum number of character edit operations  delete  
insert  and substitute  required to transform s1 into s1  normalized by the maximum of the lengths of s1 and s1. for the example shown in the adjacent figure the edit distance between the strings 'company' and 'corporation' is 1 1  and the sequence of edit operations is shown. vertical lines indicate either exact matches  cost is 1  or substitutions  cost is 1 . characters in italics are deleted or inserted and always have a unit cost. reference relation: let r tid  a1 ... an  be a reference relation where ai denotes the ith column. we assume that each ai is a string-valued attribute  e.g.  of type varchar . we also assume that tid  for tuple identifier  is a key of r.  we refer to a tuple whose tid attribute assumes value r as the tuple r. we use v i  to denote the value ai in the tuple v r  a1 ... an . 
tokenization: let tok be a tokenization function which splits a string s into a set of tokens  tok s   based on a set of delimiters  say  the white space characters . for example  tok v  of the tuple v =  r1  boeing company  seattle  wa  1  is {boeing  company}. observe that we ignore case while generating tokens. for tokens generated from attribute values of tuples  we associate the column property-the column from which a token originates. for example  the column property of tokens in tok v col   is col. consequently  the token 'madison' in the name column of a customer relation is considered different from the token 'madison' in the city column. the token set tok v  is the multiset union of sets tok a1  ... tok an  of tokens from the tuple v r  a1 ... an . that is  if a token t appears in multiple columns  we retain one copy per column in tok v   distinguishing each copy by its column property. we say that a token t is in tok v  if t is a member of some tok ai   for 1  i   n. 
weight function: we now adapt the idf weight function to the relational domain by treating each tuple as a document of tokens. the motivation for this definition is clear from the following example - we expect the weight of token 'corporation' in the organization-name column to be less than that of 'united' since corporation is a frequent token in that column. let the frequency of token t in column i  denoted freq t  i   be the number of tuples v in r such that tok v i   contains t. the idf value  idf t  i   of a token t with respect to the ith column in the schema of r is computed as follows  when freq t  i    1  
	w  t   i  = idf  t   i  = log	| r |	 
모모모모모모모모모모모모모모모모모모freq  t   i  for a token t whose frequency in column i is 1  our philosophy is that t is an erroneous version of some token in the reference tuple. since we do not know the token to which it corresponds  we define the weight w t  i  to be the average weight of all tokens in the ith column of relation r. for clarity in presentation  when the column property of a token is evident from the context  we use w t  to denote w t  i .  
1 fuzzy similarity function  fms  
informally  the similarity between an input tuple and a reference tuple is the cost of transforming the former into the latter-the less the cost  the higher the similarity. we consider the following transformation operations: token replacement  token insertion  and token deletion. each operation is associated with a cost that depends on the weight of the token being transformed. we now describe the cost of each transformation operation. let u and v be two tuples with the schema r a1 ... an . we will be considering only the case where u is an input tuple and v is a reference tuple  and we are interested in transforming u into v. 
 i  token replacement: the cost of replacing a token t1 in tok u i   by token t1 from tok v i   is ed t1 t1  w t1 i . if t1 and t1 are from different columns  we define the cost to be infinite.  
 ii  token insertion: the cost of inserting a token t into u i  is cins w t  i   where the token insertion factor cins is a constant between 1 and 1.  
 iii  token deletion: the cost of deleting a token t from u i  is w t i . 
observe that the costs associated with inserting and deleting the same token may be different. we believe that this asymmetry is useful  since in many scenarios it is more likely for tokens to be left out during data entry than it is for spurious tokens to be inserted. therefore  absence of tokens is not penalized heavily. 
we ignore the tid attribute while comparing tuples. transforming u into v requires each column u i  to be transformed into v i  through a sequence of transformation operations  whose cost we define to be the sum of costs of all operations in the sequence. the transformation cost tc u i   v i   is the cost of the minimum cost transformation sequence for transforming u i  into v i . the cost tc u  v  of transforming u into v is the sum over all columns i of the costs tc u i   v i   of transforming u i  into v i .  
	tc  u  v  =	tc  u i   v i   
i
the minimum transformation cost tc u i   v i   can be computed using the dynamic programming algorithm used for edit distance computation . 
consider the input tuple u beoing corporation  seattle  wa  
1  in table 1 and the reference tuple v boeing company  seattle  wa  1 . the minimum cost transformation of u into v requires two operations - replacing 'beoing' by 'boeing' and replacing 'corporation' by 'company'.  the function tc u  v  is the sum of costs of these two operations; assuming unit weights on all tokens  this is 1 by adding 1 for replacing 'beoing' with 'boeing' which are at an edit distance 1  and 1 for replacing 'corporation' with 'company' which are at an edit distance 1. in this example  only tc u  v  is nonzero among column-wise transformation costs. 
definition of fms: we now define the fuzzy match similarity function fms u  v  between an input tuple u and a reference tuple v in terms of the transformation cost tc u  v . let w u  be the sum of weights of all tokens in the token set tok u  of the input tuple u. similarity between u and v is defined as: 
	fms  u   v   = 1   min 	tc  u   v    1 .1   
w  u  
in the above example involving i1 and r1  w i1  = 1 because there are five tokens in tok i1  and the weight of each token is 1. therefore  fms i1  r1  = 1 - 1/1 = 1. we define fms asymmetrically because we believe the cost of transforming a dirty input tuple into a clean reference tuple is different from the reverse transformation. also  in this paper  we only transform input tuples into clean reference tuples  and never the other way. 
1 edit distance and fms 
for a broad subclass of errors  we compare the weight assignment strategy implicitly adopted by the edit distance ed with that of the fuzzy match similarity fms  to isolate scenarios when they agree or disagree on fuzzy match. the comparison also justifies  although only informally  our belief that fms is the more appropriate choice in practice. 
we consider the subclass of order-preserving errors. under this class of errors  an input tuple and its target reference tuple are consistent in the ordering among tokens after each input token is mapped to the closest matching reference token  and each input token is transformed to its counterpart in the reference tuple. let u1 ... um be the list of tokens in the input tuple u ordered according to their position in u. let v1 ... vm be the similarly ordered list of tokens in the reference tuple v. in the class of order-preserving errors  for all i  the input token ui is transformed to the reference token vi. let ed u  v  denote the total  minimum  number of edit operations for transforming each ui into vi  normalized by max l u   l v   where the length l z  of a tuple z is the sum of lengths of tokens z1 ... zp in tok z   i.e.  l z = |zi|. we now rewrite ed u  v  to highlight the implicit weight assignment to the ui vi token-mapping.  
ed  u  v  = l u  max | u i |  | vi |  ed  u i   vi       1  max  l u    l v   i l u  
observe that the ui vi mapping gets a weight proportional to max |ui|  |vi| /l u . therefore  ed implicitly assigns weights to token mappings in proportion to their lengths  i.e.  longer tokens get higher weights. for example  'corporation' to 'company' gets a higher weight than 'boeing' to 'bon' thus explaining why ed matches input tuple i1  in table 1  with r1  in table 1  instead of the correct target r1. extensive empirical evidence from the ir application suggests the superiority of idf weights to token lengths for capturing the notion of token importance . hence  we expect fms to be more beneficial than ed in practice.  
1. fuzzy match 
we first formally define the fuzzy match problem before describing the algorithm.  
the k-fuzzy match problem: given a reference relation r  a minimum similarity threshold c  1   c   1   the similarity function f  and an input tuple u  find the set fm u  of fuzzy matches of at most k tuples from r such that  
 i  fms u  v   c  for all v in fm u  
 ii  fms u  v   fms u  v'  for any v in fm u  and v' in r fm u  
observe that by setting the minimum similarity threshold c to be zero  we can simulate the scenario where a user is interested in all closest k reference tuples. when more than k i+1 reference tuples are tied for the ith  ...  kth  i   1  best fuzzy matches  we break ties by choosing an arbitrary subset of the tied reference tuples such that the total number of returned fuzzy matches is k. 
given an input tuple u  the goal of the fuzzy match algorithm is to identify the fuzzy matches-the k reference tuples closest to u. a na ve algorithm scans the reference relation r comparing each tuple with u. a more efficient approach is to build an  index  on the reference relation for quickly retrieving a superset of the target fuzzy matches. standard index structures like b+-tree indexes cannot be deployed in this context because they can only be used for exact or prefix matches on attribute values. therefore  we gather  during a pre-processing phase  additional indexing information for efficiently implementing the fuzzy match operation. we store the additional information as a standard database relation  and index this relation using standard b+-trees to perform fast exact lookups. we refer to this indexed relation as the error tolerant index  eti . the challenge is to identify and to effectively use the information in the indexed relation. our solution is based on the insight of deriving from fms an easily indexable similarity function fmsapx with the following characteristics.  i  fmsapx upper bounds fms with high probability.  ii  we can build the error tolerant index  eti  relation for efficiently retrieving a small candidate set of reference tuples whose similarity with the input tuple u  as per fmsapx  is greater  probabilistically  than the minimum similarity threshold c. therefore  with a high probability the similarity as per fms between any tuple in the candidate set and u is greater than c. from this candidate set  we return the k reference tuples closest to u as the fuzzy matches.  
in section 1  we define fmsapx. in section 1  we describe the eti relation as well as an algorithm for building it. in section 1  we present an efficient algorithm to process fuzzy matching queries  and we discuss their resource requirements in section 1. 
1 approximation of fms  
our goal in this section is to derive fmsapx an approximation of fms for which we can build an indexed relation. fmsapx is a pared down version of fms obtained by  i  ignoring differences in ordering among tokens in the input and reference tuples  and  ii  by allowing each input token to match with the  closest  token from the reference tuple. since disregarding these two distinguishing characteristics while comparing tuples can only increase similarity between tuples  fmsapx is an upper bound of fms. for example  the tuples  boeing company  seattle  wa  1  and  company boeing  seattle  wa  1  which differ only in the ordering among tokens in the first field are considered identical by fmsapx. in fmsapx  we measure the closeness between two tokens through the similarity between sets of substrings- called q-gram sets-of tokens  instead of edit distance between tokens used in fms . further  this q-gram set similarity is estimated well by the commonality between small probabilistically chosen subsets of the two q-gram sets. this property can be exploited  like we do later  to build an indexed relation for fmsapx because for each input tuple we only have to identify reference tuples whose tokens share a number of chosen q-grams with the input tuple. we first define the approximation of the q-gram set similarity between tokens. in lemma 1  we relate this similarity with the edit distance between tokens using an  adjustment term  which only depends on the value of q introduced below. 
q-gram set: given a string s and a positive integer q  the set qgq s  of q-grams of s is the set of all size q substrings of s. for example  the 1-gram set qg1  boeing   is {boe  oei  ein  ing}. because we fix q to be a constant  we use qg s  to denote qgq s .  jaccard coefficient: the jaccard coefficient sim s1  s1  between 
two sets s1 and s1 is ||ss1뫌뫋ss1|| .  
min-hash similarity: let u denote the universe of strings over an alphabet   and hi:u n  i = 1 ... h be h hash functions mapping elements of u uniformly and randomly to the set of natural numbers n. let s be a set of strings. the min-hash signature mh s  of s is the vector  mh1 s   ...  mhh s   where the ith coordinate mhi s  is defined asmhi  s  = argmin hi  a  . let i x  
a뫍s
denote an indicator variable over a boolean x  i.e.  i x  = 1 if x is true  and 1 otherwise. then  as shown in  1  1    
	e sim  s1 s1    = 1	h i mh i  s1  = mh i  s1    
h i=1
computing the min-hash signature is like throwing darts at a board and stopping when we hit an element of s. hence  the probability that we hit an element in s1 s1 before another element in s1us1 is equal to sim s1 s1 . we now define token similarity in terms of the min-hash similarity between their q-gram sets. let q and h be positive integers. the min-hash similarity simmh t1 t1  between tokens t1 and t1 is: 
	sim mh  t1 t1   = 1	h i mhi  qg  t1   = mhi  qg  t1     
h i=1
we define the similarity function fmsapx and then show  i  its expectation is greater than fms  and  ii  the probability of fmsapx being greater than fms can be made arbitrarily large by choosing an appropriate min-hash signature size. 
definition of fmsapx: let u  v be two tuples  and let dq =  1/q  be an adjustment term. fmsapx u v =	1	w t  r뫍maxtok v i   q1 simmh qg t  qg r  +dq  
	w u  i	t뫍tok u i  
consider the tuple i1 in table 1 and the tuple r1 in 1. suppose q=1 and h=1. we use the notation t:w to denote a token with weight w. suppose the tokens and their weights in i1 are company:1  beoing:1  seattle:1  1.1; their total weight is 1. suppose their min-hash signatures are  eoi  ing    com  pan    sea  ttl    1  1   respectively. the tokens in r1 are boeing  company  seattle  wa  1. suppose their min-hash signatures are  oei  ing    com  pan    sea  ttl    wa    1  1   respectively. then  'company' matches with 'company'  'beoing' with 'boeing'  'seattle' with 'seattle'  '1' with '1'. the score from matching 'beoing' with 'boeing' is: w beoing *  *1 +  1/1  =w beoing . since every other token matches exactly with a reference token  fmsapx i1  r1  = 1/1. in contrast  fms i1  r1  will also consider the cost of reconciling differences in order among tokens between i1 and r1  and the cost of inserting token 'wa'. hence  fms i1  r1  is less than fmsapx i1  r1 .  
lemma 1: let 1  붻  1 붼  1  h 뫟 1붻 1 log 붼 1 . then  
 i  e fmsapx u  v    fms u  v  
 ii  p  fmsapx  u v  뫞  1 붻  fms u v   뫞붼 
sketch of proof: we require the following definitions.  
f1 u v  = w 1u  i t뫍tok  u i   w t   r뫍maxtok  v i   1  ed  t r   f1  u v = w 1u  w t  r뫍maxtok v i   q1 sim qg t  qg r   +dq  
	i	t뫍tok u i  
result  i  falls out of the following sequence of observations.  
 i  ignoring the ordering among tokens while measuring f  and allowing tokens to be replaced by their best matches always results in over estimating fms. therefore  f1 u  v   fms u  v . 
 ii  edit distance between strings is approximated by the similarity between the sets of q-grams  lemma 1 below   and max |t|  |r|   |qg t  u qg r |/1. hence  f1 u  v   f1 u  v . 
 iii  min-hash similarity between tokens is an unbiased estimator of the jaccard coefficient between q-gram sets of tokens. therefore  e fmsapx u  v   = f1 u  v   fms u  v . 
since e fmsapx u  v   = f1 u  v   fms u  v  for all h   1  splitting fmsapx u  v  into the average of h independent functions f1'  ...  fh' one for each min-hash coordinate such that fi' has the same expectation as fmsapx and using chernoff bounds   we have the following inequality  which yields result  ii . 
e x    1  붻  f  u v   뫞 e x    1 붻 e x    뫞 e
 
lemma 1 : let t1  t1 be two tokens  and m = max |t1|  |t1| . let d =  1/q   1/m . then  
1   ed  s1  s1  뫞 | qg s 1   뫌 qg s 1   | + d mq
because the probability p fmsapx u  v    1-붻  fms u  v   can be increased arbitrarily  we loosely say that fmsapx upper bounds fms.  
1 the error tolerant index  eti   
the primary purpose of eti is to enable  for each input tuple u  the efficient retrieval of a candidate set s of reference tuples whose similarity with u is greater than the minimum similarity threshold c. recall from the definition of fmsapx that fmsapx u  v  is measured by comparing min-hash signatures of tokens in tok u  and tok v . therefore  for determining the candidate set  we need to efficiently identify for each token t in tok u   a set of reference tuples sharing min-hash q-grams with that of t. consider the example input tuple  beoing company  seattle  wa  1  shown in figure 1. the topmost row in the figure lists tokens in the input tuple  the next row lists q-gram signatures of each token  and the lowest row lists sets  s1 through s1  of tids of reference tuples with tokens whose min-hash signatures contain the corresponding q-gram. for example  the set s1 u s1 is the set of tids of reference tuples containing a token in the org. name column that shares a min-hash q-gram with 'beoing'. extending this behavior to q-gram signatures of all tokens  the union of all si's contains the candidate set s. in order to identify such sets of tids  we store in eti each q-gram s along with the list of all tids of reference tuples with tokens whose min-hash signatures contain s. 
we now formally describe the eti and its construction. let r be the reference relation  and h the size of the min-hash signature. eti is a relation with the following schema:  qgram  coordinate  column  frequency  tid-list  such that each tuple e in eti has the following semantics. e tid-list  is a list of tids of all reference tuples containing at least one token t in the field e column  whose e coordinate -th min-hash coordinate is e qgram . e frequency  is the number of tids in e tid-list . constructing a tuple  s  j  i  frequency  tid-list  in eti requires that we know the list of all reference tuple tids containing ith column tokens with s as their jth min-hash coordinate. the obvious method of computing all eti tuples in main-memory by scanning and processing each reference tuple is not scalable because the combined size of all tid-lists is usually larger than the amount of available main memory. to build the eti efficiently  we leverage the underlying database system by first building a temporary relation called the pre-eti with sufficient information and then construct the eti relation from the pre-eti using sql queries. 
	beoing  company    seattle     wa  	1 

s1   s1       s1     s1       s1   s1      s1       s1   s1 
	figure 1: candidate set generation 	 
the schema of the pre-eti is:  qgram  coordinate  column  tid . we scan the reference relation r processing each tuple v as follows. we tokenize v  and for each ith column token t in tok v   we determine its min-hash signature mh t  of size h. we insert into the pre-eti a row  q  j  i  r  for the jth min-hash coordinate in mh t . for example  if the size-1 signature of the token 'company' belonging to column 1 of the tuple r1 is  com  pan   then we insert the rows  com  1  1  r1    pan  1  1  r1  into the pre-eti. in practice  we can batch such insertions. 
  
table 1: an example eti relation 
q-gram coordinate column frequency tid-list oei ing 
com pan bon orp ati 
sea ttl 
wa 
1 1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 {r1} 
{r1} 
{r1 r1} 
{r1 r1} {r1} 
{r1} 
{r1} 
{r1 r1 r1} 
{r1 r1 r1} 
{r1 r1 r1} 
{r1 r1 r1} {r1} 
{r1} 
{r1}  
all tuples required to compute any one eti tuple occur together in the result of the eti-query:  select qgram  coordinate  column  tid from pre-eti order by qgram  coordinate  column  tid.  we scan the result of the eti-query  and for a group of tuples corresponding to the q-gram s which occurs as the jth minhash coordinate of  multiple  tokens in the ith column  we insert the tuple  s  j  i  freq s  j  i   tid-list  in eti  where freq s  j  i  is the size of the group  and tid-list the list of all tids in the group. qgrams whose frequencies are above a large threshold  called the stop q-gram threshold  set to 1 in our implementation   are considered stop tokens. for such q-grams  we insert a null value in the tid-list column. finally  we build a clustered index on the  qgram  coordinate  column  attribute combination of the eti relation so that queries looking up eti on  qgram  coordinate  column  combinations are answered efficiently. 
an example eti relation for the reference relation in table 1 with q=1 and h=1 is shown in table 1. if the length of a token is less than q  then we assume that its min-hash signature consists of the token itself. the tuple  r1  boeing company  seattle  wa  1  in table 1 with min-hash signatures { oei  ing    com  pan    sea  ttl    wa    1  1 } for its tokens  respectively  has the tid r1 in the tid-lists of each of these q-grams. 
1 query processing 
in this section  we describe the algorithm for processing fuzzy match queries-queries asking for k fuzzy matches of an input tuple u whose similarities  as per fms  with u are above a minimum similarity threshold c. the goal is to reduce the number of lookups against the reference relation by effectively using the eti. we first describe the basic algorithm  which fetches tid-lists by looking up eti of all q-grams in min-hash signatures of all tokens in u. we then introduce an optimization called optimistic short circuiting  which exploits differences in token weights and the requirement to fetch only the k closest tuples to significantly reduce the number of eti lookups. for efficient lookups  we assume that the reference relation r is indexed on the tid attribute  and the eti relation is indexed on the  qgram  coordinate  column  attribute combination. 1.1 basic algorithm 
the basic algorithm for processing the fuzzy match query given an input tuple u is as follows. for each token t in tok u   we compute its idf weight w t   which requires the frequency of t. we can store these frequencies in the eti and fetch them by issuing a sql query per token. however  we assume for now that frequencies of tokens can be quickly looked up from a main memory cache called the token-frequency cache.  see section 1.1 for a discussion on this issue.  we then determine the minhash signature mh t  of each token t.  if |t|  q  we define mh t = t .  we assign the weight w t /|mh t | to each q-gram in mh t . using the eti  we then determine a candidate set s of reference tuple tids whose similarity  as per fmsapx and hence fms  with the input tuple u is greater than c. we fetch from the reference relation all tuples in s to verify whether or not their similarities with u  as per fms  are truly above c. among those tuples which passed the verification test  we return the k tuples with the k highest similarity scores.  
candidate set determination: we compute the candidate set s as the union of sets sk  one for each q-gram qk in the min-hash signatures of tokens in tok u . for a q-gram qk which is the ith coordinate in the min-hash signature mh t  of a token t in the jth column  sk is the tid-list from the record  qk  i  j  freq qk  i  j   sk  in eti. observe that the lookup for  qk  i  j  freq qk  i  j   sk  is efficient because of the index on the required attribute combination of eti. each tid in sk is assigned a score that is proportional to the weight w t  of the parent token t. if a tuple with tid r is very close to the input tuple u  then r is a member of several sets sk and hence gets a high overall score. otherwise  r has a low overall score. tids that have an overall score greater than w u  c minus an adjustment term-a correction to approximate the edit distance between tokens with the similarity between their q-gram sets-constitute the candidate set.  
during the process of looking up tid-lists corresponding to qgrams  we maintain the scores of tids in these tid-lists in a hash table. at any point  the score of a tid equals the sum of weights of all q-grams whose tid-lists it belongs to. the weight w qk  assigned to a q-gram qk in the min-hash signature mh ti  of a token ti is w ti /|mh ti |. if a tid in sk is already present in the hash table  then its score is incremented by w qk . otherwise  we add the tid to the hash table with an initial score of w qk . after all q-grams in the signatures of input tokens are processed  we select a tid r and add it to the candidate set s only if its score is above w u  c  minus the adjustment term .  
an optimization to the after-the-fact filtering of tids with low scores described above is to add a tid to the hash table only if the score it can potentially get after all min-hash q-grams are processed is greater than the threshold. we add a new tid to the hash table only if the total weight  which is an upper bound on the score a new rid can get  of all min-hash q-grams yet to be looked up in the eti is greater than or equal to w u  c. this optimization significantly reduces the number of tids added to the hash table. we summarize the basic algorithm in figure 1. 
fuzzymatch input tuple u  h  eti  r  c  
1. initialize hash table tidscores; adjustmentterm = 1 
1. tokenize u and compute min-hash signatures q of all tokens 
1. assign token weights; remwt = sum of all token weights  
1. threshold = c remwt 
1. for each q-gram s in q s.t. s = mhi t  of token t in column col 
1. if  mhi t  is the first q-gram of mh t  to be looked up   
1. adjustmentterm += w t   1/q     
1. fetch tid-list s  by looking up  s  i  col  against eti 
1. update tidscores  
a. increment scores of existing tids by w t /|mh t | 
b. if remwt  threshold  insert new tids with score w t /|mh t |.  
1. remwt  = w s  
1. fetch tuples from r for tids with score  c-adjustmentterm 
1. compare  using f  each of these tuples with u  
1. return k  or less  most similar tuples with similarity above w u  c  figure 1: basic query processing algorithm we illustrate the above procedure with the example input tuple i1 in table 1 and the eti in table 1. suppose q=1 and h=1. we use the notation  q1  q1 :w to denote the min-hash signature  q1  q1  with each q-gram assigned a weight of w. the tokens and their weights in i1 are beoing:1  company:1  seattle:1  wa:1  1.1; their total weight is 1. suppose their min-hash signatures are  eoi  ing :1   com  pan :1   sea  ttl :1   wa :1   1  1 :1. we lookup eti to fetch the following tid-lists:  {}  {r1}    {r1  r1}  {r1  r1}    {r1  r1  r1}  {r1  r1  r1}    {r1  r1  r1}    {r1  r1  r1}  {r1} . for the purpose of this example  we ignore the adjustment term. r1 gets an overall score of 1  r1 a score of 1  and r1.1. depending on the threshold  the candidate set is a subset of {r1  r1  r1}. for the example in figure 1  suppose we looked up min-hash q-grams 'eoi'  'ing'  'com'  'pan'  'sea'  'ttl'. while processing the q-gram 'wa'  we add new tids to the hash table only if 1 + 1  the total weight of the remaining q-grams  is greater than w u  c. we now formally state that the basic algorithm retrieves the correct fuzzy matches with a high probability. for the purpose of the formal guarantee in theorem 1  we assume that no q-gram is classified as a stop token. alternatively  the stop q-gram threshold is set to at least |r|. we omit the proof due to space constraints. 
theorem 1: let 1  붻  1 붼  1  h 뫟 1붻 1 log 붼 1 . the basic query processing algorithm returns the k reference tuples closest  as per fms  to the input tuple with a probability of at least 1-붼. 
1.1 optimistic short circuiting  osc  
in the basic algorithm  we fetch tid-lists by looking up eti of all q-grams in min-hash signatures of all tokens. we now discuss the short circuiting optimization to significantly reduce the number of eti lookups. the intuition is as follows. weights of input tokens  and hence weights of min-hash q-grams  often vary significantly. therefore  we may look up the eti on just a few important qgrams and-if a fetching test succeeds-optimistically short circuit the algorithm by fetching the current closest k reference tuples. if we are able to efficiently verify-via a stopping test- whether these tuples are actually the closest k tuples then  we can save a significant amount of work:  i  avoid eti lookups on a number of unimportant q-grams  and  ii  avoid initializing and incrementing similarity scores in the hash table for large numbers of tids associated with unimportant high-frequency q-grams. 
we illustrate the intuition using the input tuple i1  the reference relation in table 1  and the eti relation in table 1. suppose k=1  q=1  and h=1. the tokens along with weights in i1 are beoing:1  company:1  seattle:1  wa:1  1.1; their total weight is 1. suppose their min-hash signatures are  eoi  ing :1   com  pan :1   sea  ttl :1   wa :1   1  1 :1. for the purpose of this example  we ignore the adjustment terms. we order q-grams in the decreasing order of their weights  and fetch their tid-lists in this order. we first fetch the tid-list {r1  r1  r1} of q-gram '1.' we cannot yet distinguish between the k and  k+1 th  here  1st and 1nd  best scores. so  we fetch the list {r1} of the next most important qgram '1'. at this point  r1 has the best score of 1  and r1 and r1 have scores of 1. we now estimate  by extrapolating its current score  the score for r1 over all q-grams to be  say  1. the best possible score s1next that r1  the current k+1th highest score tid  can get equals its current score plus the sum of weights of all remaining q-grams: 1+  1-1  = 1. observe that s1next is also greater than the best possible  k+1 th similarity-as per fmsapx and hence fms-among all reference tuples in r. because 1   1  we anticipate the reference tuple r1 to be the closest fuzzy match  fetch it from r  and compute fms u  r1 . if fms u  r1   1/1  we stop and return r1 as the closest fuzzy match thus avoiding looking up and processing tid-lists of q-grams: eoi  ing  com  pan  sea  ttl  wa. however  if fms u  r1   1  we continue fetching the next most important q-gram  here 'wa' .  
the robustness of the stopping test ensures that inaccuracy in estimating the score of r1 over all q-grams does not affect the correctness of the final result. however  it impacts performance. if we over-estimate we may fetch more reference tuples and realize they are not good matches  and if we under-estimate then we may perform a higher number of eti lookups.  
the query processing algorithm enhanced with optimistic short circuiting  osc  differs from the basic algorithm in two aspects:  i  the order in which we look up q-grams against eti  and  ii  the additional short-circuiting procedure we potentially invoke after looking up each q-gram. pseudo code is almost the same as that in figure 1 except for two additional steps: 1  the ordering of tokens  and 1  short circuiting procedure . we order q the set of all q-grams in the min-hash signatures of an input tuple in the decreasing order of their weights  where each q-gram s in the signature mh t  is assigned a weight w t /|mh t |. after fetching tid-list s   step 1 in figure 1  and processing tids in the tid-list  step 1 in figure 1   we additionally perform the short circuiting procedure  new step 1 whose pseudo code is shown in figure 1 . if the short circuiting procedure returns successfully  we skip steps 1  1  and 1.  
the short circuiting procedure consists of a fetching test and a stopping test. the fetching test  step 1 in figure 1  evaluates whether or not the current k tids could be the closest matches. on failure  we return and continue processing more q-grams. on success  we fetch the current best k candidates from the reference relation r  step 1   and compare  using fms  each of them with the input tuple u  step 1 . the stopping test  step 1  confirms whether or not u is more similar to the retrieved tuples than to any other reference tuple. on success  we stop and return the current k candidate tuples as the best k fuzzy matches. on failure  we continue processing more q-grams.  
we now describe the fetching and stopping tests. let w q  denote the sum of weights of all q-grams in a set of q-grams q. let qp= q1 ... qp  denote the ordered list of q-grams in min-hash signatures of all tokens in the input tuple u such that w qi   w qi+1 . let qi denote the set of q-grams  q1 ....  qi .  let ssi r  denote the similarity score of the tid r plus the adjustment term after processing tid-lists of q1 ... qi. suppose ri1 ... rik  rik+1 are the tids with the highest k+1 similarity scores after looking up qgrams q1 ... qi. informally  the fetching test returns true if and only if the  estimated overall score  of rik is greater than the  best possible overall score  of rik+1. we compute the estimated overall score of rik by linearly extrapolating its current similarity score ssi rik  to ssi rik  w qp /w qi   and the best possible overall score of rik+1 by adding the weight  w qp  w qi   of all q-grams yet to be fetched to ssi rik+1 .  
true  ssi rki   w qp  ssi rki+1 + w qp  w qi   
	 =   	w qi 
fetching test
false  otherwise 
the stopping test returns successfully if fms u  rij   ssi rik+1  + w qp  w qi   for all 1  j  k. since ssi rik+1  + w qp  w qi  is the maximum possible overall score any candidate outside the current top k candidates can get  if the similarities  as per fms  are greater than this upper bound we can safely stop because we are sure that no other reference tuple will get a higher score. the following theorem  whose proof we omit  formalizes the guarantees of the algorithm. again  for the purpose of obtaining the formal guarantee  we assume that no q-gram is classified as a stop token.  
theorem 1: let1  붻  1 붼  1  h 뫟 1붻 1 log 붼 1 . the query processing algorithm enhanced with optimistic short circuiting returns the k reference tuples closest according to fms to the input tuple with probability at least 1-붼. 
boolean shortcircuit etilookups tidscores  tuplelist  
//fetchingtest sk  sk+1  
1 identify k+1 tids ri1 ... rik+1 with the highest similarity scores 
1 estimate the score soptk over qp of rik and determine the best 
possible score sbestk +1 over qp of rik+1 
1 if soptk  sbestk+1  
1 fetch r tuples ri1 ... rik  
1 compare them with u to determine fms u  ri1   ...  fms u  rik   //stopping test 
1 if fms u  rij  sbestk +1 for all j  then assign tuplelist =  ri1 ... rik  and return true; else  return false figure 1: short-circuiting decision procedure 1 resource requirements 
we now discuss the resource requirements of the two phases of our algorithm: the eti building and the query processing phases.  
the expensive steps of the eti building phase are:  1  scan of the reference relation r   1  writing the pre-eti   1  sorting the preeti  and  1  writing the eti. the total i/o cost during these phases is o mavg q h |r| + |eti|  1+q   where mavg is the average number of tokens in each tuple  and |eti| is the number of tuples in eti which is less than h n | |q-the maximum number of qgrams times h times the number of columns in r-given that  is the alphabet over which tokens in r are formed from.  
the expensive steps for processing an input tuple are:  1  looking up eti for tid-lists of q-grams   1  processing tid-lists  and  1  fetching tuples in the candidate set. the number of eti lookups is less than or equal to the total number of q-grams in signatures of all tokens of a tuple. on average  this number is mavg h. the number of tids processed per tuple and the size of the candidate set is bounded by the sum of frequencies of all q-grams in the signatures of tokens in a tuple. in practice  the candidate set sizes are several orders of magnitude less than the above loose upper bound. due to its dependence on the variance of token weights of input tuples  the reduction in the number of eti lookups due to osc is hard to quantify.  
1.1 token-frequency cache 
thus far  we assumed that frequencies of tokens are maintained in a main memory token-frequency cache enabling quick computation of idf weights. given current main memory sizes on desktop machines  this assumption is valid even for very large reference relations. for example  a relation customer name  city  state  zip code  with 1 million tuples has approximately 1 distinct tokens  even after treating identical token strings in distinct columns as distinct tokens . assuming that each token and its auxiliary information  1 bytes each for column and frequency  together require on average 1 bytes  we only require 1 mb for maintaining frequencies of all these tokens in main memory. in those rare cases when the token-frequency cache does not fit in main memory  we can adopt one of following approaches. 
cache without collisions: we can reduce the size of the tokenfrequency cache by mapping each token to an integer using a 1 hash function  e.g.  md1  . we now only require 1 bytes of space  as opposed to a higher number earlier  for each token: the hash value  1 bytes   the column to which it belongs  1 bytes   and the frequency  1 bytes . now  the token-frequency cache for the 1 million tuple customer relation requires only around 1mb.  
cache with collisions: a less preferred option is to restrict the size of the hash table to at most m entries allowing multiple tokens to be collapsed into one bucket. the impact on the accuracy and correctness of our fuzzy matching algorithm depends on the collision probability. more the collisions  the more likely we will compute incorrect token weights.   
in our experiments  we assume that the token-frequency cache fits entirely in main memory and hence do not measure the impact of collisions in a size-restricted token frequency cache on accuracy.  
1. extensions 
we now discuss several extensions to the query processing algorithm and the fuzzy match similarity function. 
1 indexing using tokens 
we now extend the eti and the fuzzy match query processing algorithm to effectively use tokens for further improving efficiency. consider the input tuple i1  i1  beoing company  seattle  wa  1  in table 1. all tokens except 'beoing' are correct  and this characteristic of most tokens in an input tuple being correct holds for a significant percentage of input tokens. tokens are higher level encapsulations of  several  q-grams. therefore  if we also index reference tuples on tokens  we can directly look up eti against these tokens instead of several minhash signatures thus potentially improving efficiency of the candidate set retrieval. however  the challenge is to ensure that the candidate set we fetch contains all k fuzzy matching reference tuples. if we do not look up eti on the q-gram signature of a token  say 'beoing'  we may not consider reference tuples containing a token 'boeing' close to 'beoing'. and  it is possible that the closest fuzzy match happens to be the reference tuple containing 'boeing'. so  the challenge is to gain efficiency without losing accuracy. 
our approach is to split importance of a token equally among itself and its min-hash signature by extending the q-gram signature of a token to include the token itself  say  as the 1th coordinate in the signature. the extension modifies the similarity function fmsapx resulting in fmst apx. under the broad assumption that all tokens in an input tuple are equally likely to be erroneous  the new approximation fmst apx resulting from the modification of the token signature is expected to be a rank-preserving transformation of fmsapx. that is  if v1 and v1 are two reference tuples  and u an input tuple then e fmsapx u  v1     e fmsapx u  v1   implies e fmst apx u  v1     e fmst apx  u  v1  . consequently  the fuzzy matches identified by using fmst apx are the same as that identified by using fmsapx. hence  we gain efficiency without losing accuracy. lemma 1 formally states this result. we omit the proof due to space constraints.  
definition of fmst apx: let u be an input tuple  v be a reference tuple  t and r be tokens  q and h be positive integers. define 
sim 'mh  t  r  = 1  i t = r  + 1	i mh i  t  = mh i  r    
	1	h	i
fms t   apx  u  v =	wsim 'mh  t  r  +d   
col t뫍tok  u col   
lemma 1: if the probability of error in an input token is a constant p  1   p   1   then fmst apx is a rank-preserving transformation of fmsapx.  
the construction of the eti index relation has to be modified to write additional tuples of the form  token  1  column  tid-list . we omit details of the eti building and query processing  which are straight-forward extensions of the earlier discussion.  
1 column weights 
our infrastructure can be extended to assign varying importance to columns while matching tuples. let w1 ... wn be the weights assigned respectively to columns a1  ...  an such that w1+...+wn = 1. a higher wi value exaggerates the contribution due to matches and differences between attribute values in the ith column to the overall similarity score. the only aspect that changes is that of weights assigned to tokens during the query processing algorithm. now  a token t in the ith column gets a weight w t  wi where w t  is the idf weight and wi is the column weight. the fuzzy match similarity function  the eti building algorithm  and the rest of the query processing algorithm remain unchanged. 
1 token transposition operation 
the fuzzy match similarity function may also consider additional transformation operations while transforming an input tuple to a reference tuple. we now consider one such operation: the token transposition operation which re-orders adjacent tokens.  
token transposition: let u r  a1 ... an  be an input tuple. the token transposition operation transforms a token pair  t1  t1  consisting of two adjacent tokens in tok ai  where t1 follows t1 into the pair  t1  t1 . the cost is a function  e.g.  average  min  max  or constant  g w t1   w t1   of the weights of t1 and t1. because the token transposition operation only transforms the ordering among tokens the resulting similarity is still less  probabilistically  than fmsapx. therefore  all the analytical guarantees of our fuzzy matching algorithm are still valid when we include the token transposition operation.   
1. experiments 
using real datasets  we now empirically demonstrate  i  the quality of our new similarity function under a variety of commonly encountered errors  and  ii  the efficiency of our fuzzy matching operation.  
1 datasets and setup  
we start with a clean customer name  city  state  zip code  relation consisting of about 1 million tuples from an internal operational data warehouse as the reference relation. we create input datasets by introducing errors in randomly selected subsets of customer tuples. therefore  all characteristics of real data-
e.g.  variations in token lengths  frequencies of tokens-are preserved in the erroneous input tuples. we consider two types of error injection methods. the type i method introduces errors in tokens with equal probability  i.e.  all tokens in a column are equally likely to become erroneous. the type ii method introduces errors in tokens with a probability that is directly proportional to their frequency  i.e.  tokens with higher frequency are more likely to become erroneous. this is a common phenomenon because the more frequently a token occurs the more likely it is to have erroneous versions  e.g.  several different versions of the token 'corporation' are 'corp  co.  corpn  inc.' observe that the type ii error injection method is biased towards fms because errors in low weight high frequency tokens do not significantly reduce fms similarity.  
table 1: types and descriptions of errors 
ej description of error p ej | u i  error  i = 1 i  1 1 spelling error: modify token 1 1 1 token 	replacement: 	replace 	commonly 
abbreviated tokens with abbreviations  1 1 1 missing values: u i  = null 1 1 1 truncation: truncate u i  by 1 or less characters 1 1 1 token merge: remove delimiters in u i  1 1 1 token transposition: reorder adacent tokens  1 1 as shown in table 1  we associate with column i a probability pi  1   pi   1  with which we introduce errors into the value u i  of tuple u. error introduction across columns is independent. if we decide  with probability pi  to introduce an error into u i   we select from among several types of errors with conditional probabilities p ej | u i  error  shown in the table above. we do not introduce missing values in the name column as input tuples with a missing name cannot possibly be matched with their target. hence  we have two conditional probability columns: one each for i=1 and i 1. 
metrics 
we use the following evaluation metrics. 
 1  normalized elapsed time: the elapsed time to process the set of input tuples using the fuzzy match algorithm divided by the elapsed time to process one input tuple using the na ve algorithm  which compares an input tuple with each reference tuple . if the normalized time for a fuzzy match algorithm is less than the number of input tuples  then it outperforms the naive algorithm. 
 1  accuracy: the percentage of input tuples for which a fuzzy match algorithm identifies the seed tuple  from which the erroneous input tuple was generated  as the closest reference tuple is its accuracy.  
parameter settings: in all our experiments  we set k=1  i.e.  we only retrieve the closest fuzzy match   the q-gram size q=1  the minimum similarity threshold c=1  and the token insertion factor  required for measuring fms  cins=1. 
machine specifications: we ran experiments on a 1mhz pentium machine with 1mb ram running microsoft windows xp. we implemented our algorithm on the microsoft sqlserver 1 database system using oledb for database access. 
1 experimental results 
in this section  we use the following notation to denote the approaches and parameters we evaluate. to denote the signature computation strategy  we use a h  a뫍{q  q+t} and h  1. q denotes q-grams only  and q+t denotes q-grams plus token signatures as discussed in section 1. h is the number of q-grams in the signature. for example  q+t 1 is a signature with 1 qgrams and the token; q+t 1 denotes a token only  no q-grams at all  signature.   
1.1 accuracy 
 fms ed accuracy on type i  1% 1% accuracy on type ii 1% 1% we first compare the quality of ed and fms  and then evaluate accuracy of our fuzzy match algorithms. 1.1 comparison between ed and fms  we show that the quality of fms is better than ed using two datasets: one created using type i and the other using type ii error injection methods. each one of these datasets has around 1 tuples. the probabilities of error in columns are 1  1  1  1  respectively. because we want to compare the quality of similarity functions and not the efficiency of algorithms for identifying the fuzzy matches  we use the na ve algorithm to identify the best fuzzy match for each input tuple.  the adjacent table shows the accuracies of fms and ed on each dataset. we observe that fms is better 
than ed. as expected  it is significantly better for the dataset created with type ii errors than it is for the dataset with type i errors. to study the cases that are not biased towards fms  we henceforth consider only datasets created with type i error injection method. 
table 1: error probabilities for creating datasets 
dataset error probabilities:  name  city  state  zip code  d1  1  1  1  1  d1  1  1  1  1  d1  1  1  1  1  1.1 accuracy of algorithms 
we evaluate the accuracy of various strategies on datasets d1  d1  and d1 generated using the type i error injection method. the error probabilities on each column for these datasets are shown in table 1. note that d1 is relatively cleaner than d1  which in turn is cleaner than d1. each of d1  d1  and d1 has 1 tuples. the customer relation which is the reference relation in all our experiments has approximately 1 million tuples. figure 1 shows the results from which we observe the following. 
 i  min-hash signatures significantly improve accuracy: q h  for h 1  is more accurate  1% to 1%  than q+t 1  the tokens only approach .  
 ii  adding tokens to the signature does not negatively impact accuracy  because when h   1  q+t h is as accurate as q h. 
 iii  even small signature sizes yield higher gains in accuracy: q 1 is more accurate than q 1  but the difference in accuracy between q 1 and q 1 is not significant. 
1.1 efficiency 
to demonstrate the overall efficiency of our algorithms  we measure the normalized elapsed time for processing fuzzy match queries  the number of candidate reference tuples fetched per input tuple  and the number of tids processed per input tuple. to demonstrate the effectiveness of the optimistic short circuiting  osc  optimization  we observe the numbers of reference tuples fetched per input tuple when osc succeeded versus when it failed. figure 1 shows the normalized elapsed times  from which we observe the following. 
 i  our algorithms are 1 to 1 orders of magnitude faster than the na ve algorithm: the normalized elapsed time of any of our strategies for processing all 1 input tuples is less than 1. that is  they process all 1 tuples before the na ve algorithm processes 1 or 1 tuples. 
 ii  the query processing time decreases with the signature size. even though we may have to look up eti for more q-grams  the presence of more q-grams helps better distinguish differences between similarity scores of tids. consequently  the average number of reference tuples fetched per input tuple decreases with signature size  also confirmed by figure 1. 
 iii  for all 1 h 1  q+t h is significantly faster than q h thus confirming our intuition  discussed in section 1  that the use of tokens significantly improves efficiency of candidate set retrieval without compromising on accuracy. 
we now discuss results on the average number of reference tuples fetched per input tuple  figure 1   the average number of tids processed per input tuple  figure 1  for d1. the results for d1 and d1 are similar. again  figure 1 shows that more q-grams help decrease candidate set sizes by better distinguishing similarity scores of tids. even though  as shown in figure 1  the average number of tids processed per input tuple increases  it is more than compensated by the average reduction in candidate set sizes. optimization is successful for 1%-1% of the input tuples  and the success fraction increases with signature size. once again  we believe that this behavior is due to the higher distinguishing ability between similarities by using more q-grams. figure 1 also splits the average number of reference tuples fetched into two parts: the number when osc succeeds and the number when osc fails. we observe that when osc succeeds  we retrieve very few  around 1 per input tuple  candidate tuples. for those remaining tuples where osc fails  we fetch a much larger number.  1.1 eti building time 
figure 1 shows the normalized eti building times for various settings. as expected the time for q+t h is greater than q h. observe that the normalized time for any setting is less than 1. thus  if we have more than 1 input tuples to fuzzy match  then it seems advantageous to build the eti  and use our fuzzy match algorithm. because we persist the eti as a standard indexed relation  we can use it for subsequent batches of input tuples if the reference table does not change. due to space constraints  we do not discuss eti maintenance when the reference table changes.  
1. conclusions 
in this paper  we generalized edit distance similarity by incorporating the notion of tokens and their importance to develop an accurate fuzzy match similarity function for matching erroneous input tuples with clean tuples from a reference relation. we then developed the error tolerant index and an efficient algorithm for identifying with high probability the closest fuzzy matching reference tuples. using real datasets  we demonstrated the high quality of our similarity function and the efficiency of our algorithms. 
