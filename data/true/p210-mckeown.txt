we describe a task-based evaluation to determine whether multi-document summaries measurably improve user performance when using online news browsing systems for directed research. we evaluated the multi-document summaries generated by newsblaster  a robust news browsing system that clusters online news articles and summarizes multiple articles on each event. four groups of subjects were asked to perform the same time-restricted fact-gathering tasks  reading news under different conditions: no summaries at all  single sentence summaries drawn from one of the articles  newsblaster multi-document summaries  and human summaries. our results show that  in comparison to source documents only  the quality of reports assembled using newsblaster summaries was significantly better and user satisfaction was higher with both newsblaster and human summaries.
categories and subject descriptors
h.1  information systems applications : communication applications-information browsers
general terms
measurement  experimentation  human factors
keywords
text summarization  evaluation  user study  news browsing
1. introduction
　research on multi-document summarization of news has seen a surge of activity in the past five years  with the
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
development of many multi-document news summarization systems  e.g.   1  1  1  1   and several that run online on a daily basis  1  1  generating hundreds of summaries per day. summarization evaluation methodology has also been actively explored. since 1  duc  document understanding conference   a nist-run annual evaluation conference  has organized quantitative evaluations of multi-document summarization systems which compare system content against a reference set of model summaries. the duc corpus of clustered summary/document pairs has spurred research in evaluation methodology on automation   metrics   and new methods of comparison of multiple models that factor in perceived salience of information  1  1 .
　a significant question remains: will the summaries generated by such systems actually help end-users to make better use of the news  multi-document summaries should enable users to more efficiently find the information they need. to find out whether they do  we performed a taskbased evaluation of summaries generated by newsblaster  a system that provides an interface to browse the news  featuring multi-document summaries of clusters of articles on the same event. we hypothesized that multi-document summaries would enable end users to more effectively complete a fact-gathering task. to this end  we compared the utility of four parallel news browsing systems: one with source documents but no summaries or clusters  one with one-sentence multi-document summaries where the sentence is extracted from one of the articles  one with newsblaster generated multi-document summaries and one with human written summaries. both newsblaster and human summaries were multi-document summaries of the same length  about 1 words ; where newsblaster extracted all of its sentences  however  humans chose content and phrasing without typically using sentence extraction.
　our results show that  in comparison to source documents only  the quality of reports assembled using newsblaster summaries was significantly better and user satisfaction was higher with both newsblaster and human summaries. users of newsblaster and human summaries drew on summaries significantly more often in assembling their
the conflict between israel and the palestinians has been difficult for government negotiators to settle. most recently  implementation of the  road map for peace   a diplomatic effort sponsored by the united states  russia  the e.u. and the u.n.  has suffered setbacks. however unofficial negotiators have developed a plan known as the geneva accord for finding a permanent solution to the conflict.
  who participated in the negotiations that produced the geneva accord 
  apart from direct participants  who supported the geneva accord preparations and how 
  what has the response been to the geneva accord by the palestinians and israelis figure 1: the prompt to one of the four tasks used in the evaluation.
report and were more satisfied  while providing reports of similar quality. more generally  our results demonstrate that full multi-document summarization is a more powerful tool than either documents alone or the one-sentence approach  an approach that is closely related to that used in systems such as google news. they also provide a frame of reference as human summaries are presumably the best summary that could be provided.
　in the following sections  we overview the relevant features of newsblaster and then discuss the design  execution and results of our evaluation.
1 overview of newsblaster
our study drew upon the following key components of
newsblaster:
1. article clustering. newsblaster clusters the articles it retrieves into event clusters about the same realworld topic. on a typical day  newsblaster will identify about 1 such event clusters  which differ greatly in size. some event clusters may contain only a handful of related articles  while major domestic or international stories can involve fifty or more.
1. event cluster summarization. newsblaster generates a concise overview of each event cluster. its techniques for multi-document summarization include sentence extraction  reformulation and rewriting named entities for clarity.
1. user interface. finally  newsblaster presents the clusters  summaries  and links to source articles as a user-friendly  publicly accessible web page. for the task evaluation  we developed a user interface that retained some of these features  e.g.  see figure 1 .
　we did not evaluate other features of newsblaster  including search   update  summaries  and related image collection.
1. related work
　there has been considerable recent work on multidocument summarization  see  for a sample of systems . ours is distinguished by its use of multiple summarization strategies dependent on input document type  fusion of phrases to form novel sentences  and editing of extracted sentences. our task-based or extrinsic  evaluation contrasts with most recent work on evaluation of summaries  which has focused on quantitative evaluation comparing generated summaries against a set of ideal reference models  1  1  1  1 . there have also been earlier organized and individual task-based evaluations of single document summarization. tipster-iii  and others  1  1  used an information retrieval task. time and accuracy were measured to determine how well a user can judge the relevance of a retrieved document to a query. however  other factors such as summary length  type of query  some make it easy to determine relevance   and document type  when key words accurately characterize the text  these measures don't discriminate well between summaries  have a critical impact on task results .
　as in our work  a recent evaluation  also asks subjects to write reports given a topic. however  they treat the resulting reports as focused summaries  reports are restricted to 1 sentences in length  and they evaluate how well different quantitative metrics compute similarity between the reports. thus  their work evaluates evaluation metrics.
1. methods
　we modeled our evaluation on an approach used by the darpa tides program for an integrated feasibility experiment  ife   see  for a description of the system architecture for this experiment . in the ife  users are asked to write a report using a news aggregator as a tool. this task also resembles those that intelligence analysts carry out on a day-to-day basis  .
1 evaluation goals
　in designing our user evaluation  we were interested in whether newsblaster is an effective tool for assisting the processing of large volumes of news. we designed our evaluation to answer the following questions:
  do summaries help the user find information needed to perform a report writing task 
  do users use information from the summary in gathering their facts 
  do summaries increase user satisfaction with the online news system 
  do users create better fact sets with an online news system which includes multi-document summarization than one does that not 
  in the context of a news browser  what is the comparison of information quality in this task  and user satisfaction  when users have access to newsblaster summaries versus minimal or human summaries 
1 design
　each subject was asked to perform four 1-minute fact gathering scenarios using a web interface. each scenario involved answering three related questions about an issue in the news. these questions were presented to the user as part of a prompt  one of which is shown in figure 1. the four tasks were  respectively: the geneva accord in the middle east; hurricane ivan's effects; conflict in the iraqi city of najaf; and attacks by chechen separatists in russia. subjects were given a space to compose their report and a web page that we constructed as their sole resource. they were told to cut and paste facts from either the summaries or articles on the page  or to paraphrase to write a report. the page contained four document clusters  two of which were centrally related to the topic at hand  and two of which figure 1: the evaluation interface screen showing the list of documents that a user sees in the no summary condition.
were peripherally related.1 hence there were sixteen clusters in the study overall. we selected the clusters by doing a manual search through newsblaster clusters to find groups that were either peripherally or closely related. each cluster contained  on average  ten articles. subjects thus had to find relevant information within forty articles to answer in-depth analysis questions for each of four scenarios. while all of the articles were related to the scenario topic  only about half of the articles contained answers to the specific questions.
　there were four summary condition levels in the experiment:
level 1: subjects were given no summaries. the web page presented a list of document headlines  with no grouping by event cluster  that were relevant to the scenario  figure 1 . this included exactly the same documents that appeared in the four event clusters.
level 1: subjects were given a one-sentence summary for each source article  plus a one-sentence summary for each entire cluster. the per-document summary was generated by extracting the article's first sentence  an approach that is used as a baseline in evaluations . the cluster summary was the one-sentence summary of the single article closest to cluster centroid. this is similar to approaches used in commercial online news systems such as google news.
level 1: subjects were given a newsblaster multidocument summary for each cluster.
level 1: subjects were given a human multi-document summary for each cluster. we hired summary writers from outside our research group to write summaries. writers were recruited by posting a notice on a university job and career web site that solicited applicants with good verbal skills  e.g.  english or journalism majors  students with high verbal scores on their gre or sat tests  or other evidence of writing ability.
　subjects had access to source documents in addition to the summaries. links to the documents were available on figure 1: the evaluation interface screen showing a typical page for the single sentence summary condition. the user sees the summary and a list of articles each with its own summary.
the same page when summary level 1 and summary level 1 were used  figure 1  or by clicking on the cluster name when summary levels 1 and 1 were used  figure 1 .
　each scenario was followed by a survey that asked subjects to rate different aspects of their experience  e.g.  difficulty of the task  along a five point scale  as well as some multiple choice questions. at the end of the experiment  each subject answered additional questions about their overall experience and had the opportunity to give comments.
1 study execution
　we recruited 1 subjects for three studies  where subjects wrote reports under the four different summary conditions noted above. our subjects came from a variety of backgrounds: 1% were university students  of whom 1% were engineering students. the rest were undergraduate liberal arts students  journalism students  or law students. a pre-experiment questionnaire revealed that most used online newspapers as their primary news source  and read the news about an hour per day. all were native speakers of american english. subjects were paid for their participation. an additional monetary prize was promised for the five writers whose reports scored the highest.
　the subjects in the first study below alternated between summary level 1 and level 1  i.e.  newsblaster and human summaries ; we controlled for scenario order and level order. the subjects in the next two studies had a single summary condition  summary level 1  no summaries  or level 1  single-sentence summaries   and we controlled for scenario order.1 altogether  a total of 1 reports were written. we aimed at 1 subjects per summary level for each scenario  note that in study a  subjects wrote for only two scenarios and thus we needed to double the number  and more subjects than expected showed up for study c.

figure 1: the evaluation interface screen showing a multi-document summary generated by newsblaster. the user clicks on the cluster title to see the list of associated articles.
study a: 1 subjects wrote reports for two scenarios each in two summary conditions: level 1 and level 1. together  all four scenarios were covered.1
study b: 1 subjects wrote reports for all four scenarios  using summary level 1.
study c: 1 subjects wrote reports for all four scenarios  using summary level 1.
1 scoring the reports
　as illustrated in figure 1  subjects were asked to assemble lists of important facts that addressed a three-part prompt. we scored the quality of the resulting reports on the basis of how well subjects included appropriate content. to do this  we needed a gold standard and a metric for comparing report content against the gold standard. to score the reports  we used the pyramid method for evaluation   which has been demonstrated to be a reliable method for summary evaluation. the method uses multiple models  thus making the report scores less sensitive to the specific model used. the pyramid method allows an importance weight to be assigned to different information units  or content units. this is important for a subjective task such as report writing  where different facts are more or less important.
　as a gold standard  we constructed a pyramid  of facts  or content units  for each scenario question for each summary level  using the reports written by the rest of the study participants for the same question. for example  to score a report from the human summary condition  we constructed the pyramid using reports created using all other conditions  i.e.  no summaries  minimal summary  newsblaster summary   plus the reports written by different people also with human summaries. this yielded  on average  1 reports per pyramid  far greater than the number of summaries  five  needed to yield stable results . using this method  any fact  whether expressed as a word  a modifier  or a clause  that appears in more than one report is included in the pyramid. facts that appear in more reports appear higher in the pyramid and are associated with a weight that indicates the number of times they are mentioned. thus  more important facts have higher weight. if there are n reports  then there will be n levels in the pyramid. the top level will contain those facts that appear in all n articles  the next level facts that appear in n   1 articles  and so forth. a report scu that does not appear in a pyramid has weight 1. repetitions of the same scu also have weight 1 and thus  duplication in a report does not increase the score. an ideal report of length x facts will include all facts from the top level  the next level and so forth  until x facts are included.
　we score a report using the pyramid scoring metric  which computes a ratio of the sum of the weights of report facts to the sum of the weights in an optimal report with the same number of facts. more formally  let tj refer to the jth tier in a pyramid of facts. if the pyramid has n tiers  then tn is the top-most tier and t1  the bottom-most. the optimal score for a report with x facts is:
	 	 1 
where j is equal to the index of the lowest tier an optimally informative report will draw from. then the pyramid score w is the ratio of d  the sum of the fact weights in the report  to max  the optimal report score.
this method has the following desirable properties:
  it avoids postulating an exhaustive ideal report  which would be impossible to reproduce in the 1 minute time frame of the study. in fact  we first attempted to construct such ideal reports  but each took us two days to construct. instead  the pyramid predicts which facts are most important to include in the given time limit by comparing the choices of all participants.
  it predicts that there will be multiple reports of the same length that are equally good. for example  if two reports are of the same length and each draw on different facts from the same tier in the pyramid  they will receive equal scores.
  it takes into account the relative importance of facts according to the report writers themselves.
　no specific instructions were given to the report writers about how long their reports should be. consequently  some people wrote much longer answers to some report questions than did others. figure 1 shows a histogram of lengths of answers to report questions  where length is measured in content units.1 the wide variation in length of answers to any of the three questions from each of the four prompts was

figure 1: distribution of the length of the reports in content units across all four conditions
unexpected  as it did not show up in our pilot study with far fewer subjects  on one scenario. it has been observed that report length has a significant effect on evaluation results . to avoid the distortion that would arise from treating reports of such widely disparate length equivalently  we restricted the length of reports to be no longer than one standard deviation above the mean  removing outliers. to do this  we truncated all question answers to a length of eight content units  which was the third quartile of lengths of all answers.
1 method of analysis
　we used analysis of variance  anova  to study the impact of the type of summary level on report quality. the dependent variable was the score for each report and summary type was used as a factor with four levels: machine multidocument summary  human multi-document summary  no summary at all and minimal summary.
　in addition to the main factor of interest  summary level   we included other factors in the model to estimate their contribution to the report quality. these factors were report writer  report topic  and question.
1. results
　we measured effectiveness of the summaries in a factgathering task in three ways:
1. by scoring the reports and comparing scores acrosssummary conditions;
1. by comparing user satisfaction per summary condition  as reported by the per-scenario surveys; and
1. by comparing whether subjects preferred to draw report content from summaries or from documents  measured by counting the citations they inserted following each extracted fact.
1 content score comparison
　the results of scoring reports for content are shown by summary condition in table 1. the quality of reports tends to improve when subjects carry out the task with better quality summaries; the pyramid scores are lowest when the subjects use documents only and highest when the subjects use the human summaries. differences between the scores are not significant  p=1 from anova analysis   but when we drop the scenario subjects had most difficulty with  differences are significant as noted below.
　we suspected that we would see differences when we looked at different scenarios and  as we shall see  the anova does show that scenario is a significant factor. when scoring reports and in informal discussion with subjects  we observed that some scenarios were more difficult than others; the documents in the clusters for these scenarios did not contain as much information for the answers. for example  in the geneva accord scenario  one subject wrote  the  user study  page brought up a large amount of useless articles and information  and especially on the last article  geneva   only a few of the articles had any relevance  sic  at all.  the hurricane ivan scenario  on the other hand  seemed one for which subjects could provide responses to questions. in retrospect  this may have been due to the fact that the event clusters for geneva contained more editorials with less  hard  news  while the clusters for hurricane ivan contained more breaking news reports.
　given the problematic feedback on scenario 1 and the difference in types of documents in the clusters  we concluded that there was a design problem for this scenario. we removed the geneva accord scenario scores from the mix and recomputed averages of pyramid scores per summary condition as shown in table 1. these results show that report quality is lowest with documents only  improves with minimal one-sentence summaries and improves again with newsblaster summaries. the full anova tables for all three scenarios apart from geneva are shown in table 1; the anova table shows that summary level is a marginally significant factor in the results.
　our primary interest in the experiment was to measure the impact of the different multi-document summaries  determining exactly which summary levels made a difference. so  given the anova model  we compared the report scores under each multi-document summary condition to those written under different summary conditions. 1% simultaneous confidence intervals for the comparisons were computed by the bonferroni method. the only difference that was significant at the 1 level was that between newsblaster summaries and no summaries at all. thus  we conclude that report quality with newsblaster summaries is significantly better than reports produced with documents only. the differences between newsblaster and minimal or human summaries are not significant  although results with human summaries are slightly below newsblaster summaries.
　the anova shows that scenario  question and subject are also significant factors in the result. furthermore  there are significant interactions between summary level and scenario  between summary level and question  and between scenario and question.
1 user satisfaction
predictordfsum of sqmean sqf valuep-valuesummary1.1.1.1.1scenario1.1.1.1.1question1.1.1.1.1user1.1.1.1.1summary:scenario1.1.1.1.1summary:question1.1.1.1.1scenario:question1.1.1.1.1summary:scenario:question1.1.1.1.1table 1: anova analysis of question score depending on summary level  scenario  question and user

summary levelpyramid scorelevel 1  documents only 1level 1  one sentence summary 1level 1  newsblaster summary 1level 1  human summary 1
questionlevel 1level 1level 1　six of the questions in our exit survey required responses along a quantitative continuum. each of the responses was assigned a score from 1 to 1 and a natural-language equivalent  with low scores corresponding to deep dissatisfaction and high scores expressing full satisfaction. for each question  figure 1 shows the questions and the responses for each summary level at the extremes of the possible responses. it also shows the averages of the subjects' responses at the table 1: mean pyramid scores on reports  all scenarios included.
summary levelpyramid scorelevel 1  documents only 1level 1  one sentence summary 1level 1  newsblaster summary 1level 1  human summary 1table 1: mean pyramid scores on reports  scenario 1  geneva accords  excluded.
bottom of the table. this numeric representation of user satisfaction increases monotonically from level 1 to level 1. subjects were asked to compare their experience in the study with the experience they would expect to have on the same task using a web search. subjects were more likely to think the system they used was more effective than a web search when they used newsblaster than when they used either documents only  p=1  or single-sentence summaries  p=1 . users were more likely to feel that they had read more than they needed to with documents only and with single-sentence summaries than with either newsblaster summaries or human summaries. the difference for this question is marginally significant between subjects with human summaries and subjects with no summaries  p=1 
　questions 1 and 1 show that subjects found it easier to assemble their facts with summaries than with documents only to complete the task and that they were more likely to feel they had enough time with summaries than with documents only. a pairwise χ1 test shows the difference is marginally significant for question 1 between human summaries and no summaries  p=1   is significant between one sentence summaries and no summaries  p=1   although not quite significant between newsblaster summaries and no summaries  p=1 . the difference for question 1
1. which was most helpful  source articles helped most1%1%1%equally helpful1%1%1%summaries helped most1%1%1%1. how did you budget your time 
most searching  some writing1%1%1%half searching  half writing1%1%1%mostly writing  some searching1%1%1%figure 1: multiple choice survey questions.
is significant  or marginally so  between each condition with a summary and no summaries  p=1  newsblaster/no summary; p=1 human/no summary; p=1 one sentence/no summary . there is no significant difference between different summary levels for either question 1 or 1. there were no significant differences between responses for the different summary levels for either question 1 or 1.
　responses to the multiple choice questions are shown in table 1. responses to question 1 again show that users were more satisfied with human level summaries than newsblaster summaries and with newsblaster summaries than with one sentence summaries. more than four times the proportion of subjects replied that summaries were more useful than source articles with newsblaster summaries than with one sentence summaries. responses to question 1 show that subjects spent the least time searching when given newsblaster summaries  but unintuitively  the most time when given human summaries.
　in the space for open comments  many subjects commented on the need for a method of searching the interface for events about particular keywords. an efficient searchable interface over summaries is being developed as part of newsblaster  but was not evaluated in this study.
1 citation patterns
　the above results were echoed in the citation habits of subjects. when subjects wrote a report  they were asked to cite the location where they found a fact that they extracted for their report. we compared the number of times a subject extracted facts from source articles with extractions from summaries. the citations in level 1  one sentence summary  reports credited summaries 1% of the time.
questionlevel 1level 1level 1level 1. was the system better or worse than a general web search would have been 
1. a web search would have been a lot more effective
1. newsblaster was a lot more effective than a web search11111. what best describes your experience reading source articles 
1. i read a lot more than i needed to
1. i only read those articles i needed to read11111. how difficult do you think it was to write the report 
1. very difficult
1. very easy11111. do you feel like you had enough time to write the reports 
1. i needed more time
1. i had more than enough time11111. what best describes your experience using article summaries 
1. they had nothing useful to say about the topic
1. they told me everything i needed to know for the reportsn/a1111. did you feel that the automatic summaries saved you time  wasted time  or had no impact on your time budget 
1. summaries wasted time
1. summaries saved me timen/a111average1111figure 1: the survey questions asked after each task  and the average responses per summary level.for levels 1  newsblaster  and 1  human   the proportion was 1% and 1% respectively. this means that report writers were much more likely to reuse text from newsblaster multi-document summaries than from the minimal summaries  p=1 on one-sided t-test ; there was a tendency to include more content when presented with human summaries than with newsblaster summaries  p=1 on one-sided t-test .
1. discussion
　we began by asking whether multi-document summaries help users to find the information they need. our user study shows that when a news browsing system contains such summaries  there is a significant increase over the no-summary condition in the quality of information that they include in their report. users feel that they are able to find substantially more of the information that is relevant. this result demonstrates that summaries do help subjects do a better job of using news to assemble facts on given topics.
　when we developed newsblaster  we speculated that summaries could help users find necessary information in either of the following ways:
1. they may find the information they need in the summaries themselves  thus requiring them to read less of the full articles  or
1. the summaries may link them directly to the relevantarticles and positions within the articles where the relevant information occurs.
　our user study confirms these beliefs and shows that as the quality of the summary increases  from level 1 to level 1   the greater the effect. the increase in citations shows that as quality of the summary increases  users significantly more often find the information they need in the summary without a significant decrease in report quality. at the same time  they report that they read fewer articles when they have either a level 1 or a level 1 summary. this confirms our belief that better multi-document summarization saves reading time and facilitates finding the relevant documents.
this is further reinforced by the fact that almost five times the proportion of subjects using level 1 summaries than those using level 1 summaries reported that summaries were more helpful for the task than source articles. that number almost doubles again for subjects with level 1 summaries.
　there are some issues we need to address in future studies. first  we expected to find a significant increase in report quality as summary quality increased. we only found a significant increase in quality between reports written with documents only and reports written with summaries. the lack of significant increase between summary level 1 and summary level 1 could be due to a number of factors. there were two problems in presentation of information for these two levels. first  the interface for summary level 1 identified individual articles with a title and a one sentence summary; we modeled this design after commercial online news providers. the interface for summary level 1 only had titles for each article. in order to pinpoint the effect of different quality summaries on report quality  we need to run a follow-on study which compares how subjects do with a single sentence multi-document summary paired with a list of article titles only and how they do with a newsblaster summary paired with a list that contains both titles and one-sentence  single document summaries. second  the interface for summary level 1 shows the list of individual articles on the same web page as the multi-document summary for the cluster. in contrast  the interface for summary level 1 shows the multi-document summary and cluster title on the same page and requires the subject to click on cluster title to see the list of individual articles. the different number of clicks required in the interface may have affected time-to-task completion as well as search strategy.
　another problem that we noted was that reports written by subjects were of widely varying length. reports varied from a minimum of 1 words to a maximum of 1 words. we adjusted for this in the current study by truncating reports. lengthy reports not only had more material  but tended to have more duplication of facts  which clearly makes for less effective reports. the impact of truncating reports requires follow-up study. we plan to correct for these two problems with more specific directions about the length and nature of report required. we also will experiment with modifications of the task so that subjects will write coherent reports  rather than cut and paste sentences from documents. we hypothesize that this will require more synthesis of material  and lead to more consistency in length.
　in order to have a realistic task-based evaluation  we developed complex prompts across a range of topics. as a consequence  we could simultaneously investigate a wide range of factors. given that scenario and question had significant effects on report quality  we need to understand more clearly how the four scenarios contrast  and how question difficulty compares within and across prompts. it is also possible that variables we did not explicitly test for  such as cluster size  article length  semantic coherence within clusters  or semantic distance between clusters  influenced the outcome.
1. conclusions
　we have shown that it is feasible to conduct a task-based  or extrinsic  evaluation of summarization that yields significant conclusions. our answer to the question  do summaries help   is clearly yes. our results show that subjects produce better quality reports using a news interface with newsblaster summaries than with no summaries. also  as summary quality increases from none at all to human  user satisfaction increases. in particular  full multi-document summaries  of which newsblaster and human summaries are representative  help users perform better at fact-gathering than they do with no summaries. users are also more satisfied with multi-document summaries than with minimal one-sentence summaries such as those used by commercial online news systems. these results affirm the benefit of research in multi-document summarization.
　however  we have also demonstrated that many factors influence the degree to which summaries help. a complete answer to the question is clearly complex  and a single study can only give partial insight. a secondary contribution of our experiments is the identification of additional possible effects on task completion  such as specific interface design  report length  and scenario design  none of which were predicted by our pilot. these insights provide a road-map for follow-on studies that can even more finely pinpoint the effect of multi-document summaries on task performance.
1. acknowledgements
　we would like to thank andrew rosenberg for his help with the statistical analysis in an early stage of the evaluation. this work was supported in part by the defense advanced research projects agency under tides grant nuu1-1.
