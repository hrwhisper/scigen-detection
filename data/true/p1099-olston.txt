there is a growing need for ad-hoc analysis of extremely large data sets  especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. parallel database products  e.g.  teradata  offer a solution  but are usually prohibitively expensive at this scale. besides  many of the people who analyze this data are entrenched procedural programmers  who find the declarative  sql style to be unnatural. the success of the more procedural map-reduce programming model  and its associated scalable implementations on commodity hardware  is evidence of the above. however  the map-reduce paradigm is too low-level and rigid  and leads to a great deal of custom user code that is hard to maintain  and reuse.
　we describe a new language called pig latin that we have designed to fit in a sweet spot between the declarative style of sql  and the low-level  procedural style of map-reduce. the accompanying system  pig  is fully implemented  and compiles pig latin into physical plans that are executed over hadoop  an open-source  map-reduce implementation. we give a few examples of how engineers at yahoo! are using pig to dramatically reduce the time required for the development and execution of their data analysis tasks  compared to using hadoop directly. we also report on a novel debugging environment that comes integrated with pig  that can lead to even higher productivity gains. pig is an open-source  apache-incubator project  and available for general use.
categories and subject descriptors:
h.1 database management: languages general terms: languages.

atomkins yahoo-inc.com
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
1. introduction
　at a growing number of organizations  innovation revolves around the collection and analysis of enormous data sets such as web crawls  search logs  and click streams. internet companies such as amazon  google  microsoft  and yahoo! are prime examples. analysis of this data constitutes the innermost loop of the product improvement cycle. for example  the engineers who develop search engine ranking algorithms spend much of their time analyzing search logs looking for exploitable trends.
　the sheer size of these data sets dictates that it be stored and processed on highly parallel systems  such as sharednothing clusters. parallel database products  e.g.  teradata  oracle rac  netezza  offer a solution by providing a simple sql query interface and hiding the complexity of the physical cluster. these products however  can be prohibitively expensive at web scale. besides  they wrench programmers away from their preferred method of analyzing data  namely writing imperative scripts or code  toward writing declarative queries in sql  which they often find unnatural  and overly restrictive.
　as evidence of the above  programmers have been flocking to the more procedural map-reduce  programming model. a map-reduce program essentially performs a groupby-aggregation in parallel over a cluster of machines. the programmer provides a map function that dictates how the grouping is performed  and a reduce function that performs the aggregation. what is appealing to programmers about this model is that there are only two high-level declarative primitives  map and reduce  to enable parallel processing  but the rest of the code  i.e.  the map and reduce functions  can be written in any programming language of choice  and without worrying about parallelism.
　unfortunately  the map-reduce model has its own set of limitations. its one-input  two-stage data flow is extremely rigid. to perform tasks having a different data flow  e.g.  joins or n stages  inelegant workarounds have to be devised. also  custom code has to be written for even the most common operations  e.g.  projection and filtering. these factors lead to code that is difficult to reuse and maintain  and in which the semantics of the analysis task are obscured. moreover  the opaque nature of the map and reduce functions impedes the ability of the system to perform optimizations. we have developed a new language called pig latin that combines the best of both worlds: high-level declarative querying in the spirit of sql  and low-level  procedural programming a` la map-reduce.
　example 1. suppose we have a table urls:  url  category  pagerank . the following is a simple sql query that finds  for each sufficiently large category  the average pagerank of high-pagerank urls in that category.
select category  avg pagerank  from urls where pagerank   1
group by category having count *    1
　an equivalent pig latin program is the following.  pig latin is described in detail in section 1; a detailed understanding of the language is not required to follow this exam-
ple. 
good urls = filter urls by pagerank   1; groups = group good urls by category; big groups = filter groups by count good urls  1; output = foreach big groups generate category  avg good urls.pagerank ;
　as evident from the above example  a pig latin program is a sequence of steps  much like in a programming language  each of which carries out a single data transformation. this characteristic is immediately appealing to many programmers. at the same time  the transformations carried out in each step are fairly high-level  e.g.  filtering  grouping  and aggregation  much like in sql. the use of such high-level primitives renders low-level manipulations  as required in map-reduce  unnecessary.
　in effect  writing a pig latin program is similar to specifying a query execution plan  i.e.  a dataflow graph   thereby making it easier for programmers to understand and control how their data processing task is executed. to experienced system programmers  this method is much more appealing than encoding their task as an sql query  and then coercing the system to choose the desired plan through optimizer hints.  automatic query optimization has its limits  especially with uncataloged data  prevalent user-defined functions  and parallel execution  which are all features of our target environment; see section 1.  even with a pig latin optimizer in the future  users will still be able to request conformation to the execution plan implied by their program.
　pig latin has several other unconventional features that are important for our setting of casual ad-hoc data analysis by programmers. these features include support for a flexible  fully nested data model  extensive support for userdefined functions  and the ability to operate over plain input files without any schema information. pig latin also comes with a novel debugging environment that is especially useful when dealing with enormous data sets. we elaborate on these features in section 1.
　pig latin is fully implemented by our system  pig  and is being used by programmers at yahoo! for data analysis. pig latin programs are currently compiled into  ensembles of  map-reduce jobs that are executed using hadoop  an opensource  scalable implementation of map-reduce. alternative backends can also be plugged in. pig is an open-source project in the apache incubator1.
　the rest of the paper is organized as follows. in the next section  we describe the various features of pig  and the underlying motivation for each. in section 1  we dive into the pig latin data model and language. in section 1  we describe the current implementation of pig. we describe our novel debugging environment in section 1  and outline a few real usage scenarios of pig in section 1. finally  we discuss related work in section 1  and conclude.
1. features and motivation
　the overarching design goal of pig is to be appealing to experienced programmers for performing ad-hoc analysis of extremely large data sets. consequently  pig latin has a number of features that might seem surprising when viewed from a traditional database and sql perspective. in this section  we describe the features of pig  and the rationale behind them.
1 dataflow language
　as seen in example 1  in pig latin  a user specifies a sequence of steps where each step specifies only a single  highlevel data transformation. this is stylistically different from the sql approach where the user specifies a set of declarative constraints that collectively define the result. while the sql approach is good for non-programmers and/or small data sets  experienced programmers who must manipulate large data sets often prefer the pig latin approach. as one of our users says 
 i much prefer writing in pig  latin  versus sql. the step-by-step method of creating a program in pig  latin  is much cleaner and simpler to use than the single block method of sql. it is easier to keep track of what your variables are  and where you are in the process of analyzing your data.  - jasmine novak  engineer  yahoo!
　note that although pig latin programs supply an explicit sequence of operations  it is not necessary that the operations be executed in that order. the use of high-level  relational-algebra-style primitives  e.g.  group  filter  allows traditional database optimizations to be carried out  in cases where the system and/or user have enough confidence that query optimization can succeed.
　for example  suppose one is interested in the set of urls of pages that are classified as spam  but have a high pagerank score. in pig latin  one can write:
spam urls = filter urls by isspam url ; culprit urls = filter spam urls by pagerank   1; where culprit urls contains the final set of urls that the user is interested in.
　the above pig latin fragment suggests that we first find the spam urls through the function isspam  and then filter them by pagerank. however  this might not be the most efficient method. in particular  isspam might be an expensive user-defined function that analyzes the url's content for spaminess. then  it will be much more efficient to filter the urls by pagerank first  and invoke isspam only on the pages that have high pagerank.
　with pig latin  this optimization opportunity is available to the system. on the other hand  if these filters were buried within an opaque map or reduce function  such reordering and optimization would effectively be impossible.
1 quick start and interoperability
　pig is designed to support ad-hoc data analysis. if a user has a data file obtained  say  from a dump of the search engine logs  she can run pig latin queries over it directly. she need only provide a function that gives pig the ability to parse the content of the file into tuples. there is no need to go through a time-consuming data import process prior to running queries  as in conventional database management systems. similarly  the output of a pig program can be formatted in the manner of the user's choosing  according to a user-provided function that converts tuples into a byte sequence. hence it is easy to use the output of a pig analysis session in a subsequent application  e.g.  a visualization or spreadsheet application such as excel.
　it is important to keep in mind that pig is but one of many applications in the rich  data ecosystem  of a company like yahoo! by operating over data residing in external files  and not taking over control over the data  pig readily interoperates with other applications in the ecosystem.
　the reasons that conventional database systems do require importing data into system-managed tables are threefold:  1  to enable transactional consistency guarantees   1  to enable efficient point lookups  via physical tuple identifiers   and  1  to curate the data on behalf of the user  and record the schema so that other users can make sense of the data. pig only supports read-only data analysis workloads  and those workloads tend to be scan-centric  so transactional consistency and index-based lookups are not required. also  in our environment users often analyze a temporary data set for a day or two  and then discard it  so data curating and schema management can be overkill.
　in pig  stored schemas are strictly optional. users may supply schema information on the fly  or perhaps not at all. thus  in example 1  if the user knows the the third field of the file that stores the urls table is pagerank but does not want to provide the schema  the first line of the pig latin program can be written as:
good urls = filter urls by $1   1; where $1 uses positional notation to refer to the third field.
1 nested data model
　programmers often think in terms of nested data structures. for example  to capture information about the positional occurrences of terms in a collection of documents  a programmer would not think twice about creating a structure of the form map documentid  set positions   for each term.
　databases  on the other hand  allow only flat tables  i.e.  only atomic fields as columns  unless one is willing to violate the first normal form  1nf  . to capture the same information about terms above  while conforming to 1nf  one would need to normalize the data by creating two tables:
term info:  termid  termstring  ...  position info:  termid  documentid  position 
the same positional occurence information can then be reconstructed by joining these two tables on termid and grouping on termid  documentid.
　pig latin has a flexible  fully nested data model  described in section 1   and allows complex  non-atomic data types such as set  map  and tuple to occur as fields of a table. there are several reasons why a nested model is more appropriate for our setting than 1nf:
  a nested data model is closer to how programmers think  and consequently much more natural to them than normalization.
  data is often stored on disk in an inherently nested fashion. for example  a web crawler might output for each url  the set of outlinks from that url. since pig operates directly on files  section 1   separating the data out into normalized form  and later recombining through joins can be prohibitively expensive for web-scale data.
  a nested data model also allows us to fulfill our goal of having an algebraic language  section 1   where each step carries out only a single data transformation. for example  each tuple output by our group primitive has one non-atomic field: a nested set of tuples from the input that belong to that group. the group construct is explained in detail in section 1.
  a nested data model allows programmers to easily write a rich set of user-defined functions  as shown in the next section.
1 udfs as first-class citizens
　a significant part of the analysis of search logs  crawl data  click streams  etc.  is custom processing. for example  a user may be interested in performing natural language stemming of a search term  or figuring out whether a particular web page is spam  and countless other tasks.
　to accommodate specialized data processing tasks  pig latin has extensive support for user-defined functions  udfs . essentially all aspects of processing in pig latin including grouping  filtering  joining  and per-tuple processing can be customized through the use of udfs.
　the input and output of udfs in pig latin follow our flexible  fully nested data model. consequently  a udf to be used in pig latin can take non-atomic parameters as input  and also output non-atomic values. this flexibility is often very useful as shown by the following example.
　example 1. continuing with the setting of example 1  suppose we want to find for each category  the top 1 urls according to pagerank. in pig latin  one can simply write:
groups = group urls by category; output = foreach groups generate category  top1 urls ;
where top1   is a udf that accepts a set of urls  for each group at a time   and outputs a set containing the top 1 urls by pagerank for that group.1 note that our final output in this case contains non-atomic fields: there is a tuple for each category  and one of the fields of the tuple is the set of the top 1 urls in that category.
　due to our flexible data model  the return type of a udf does not restrict the context in which it can be used. pig latin has only one type of udf that can be used in all the constructs such as filtering  grouping  and per-tuple processing. this is in contrast to sql  where only scalar functions may be used in the select clause  set-valued functions can only appear in the from clause  and aggregation functions can only be applied in conjunction with a group by or a
partition by.
　currently  pig udfs are written in java. we are building support for interfacing with udfs written in arbitrary languages  including c/c++  java  perl and python  so that users can stick with their language of choice.
1 parallelism required
　since pig latin is geared toward processing web-scale data  it does not make sense to consider non-parallel evaluation. consequently  we have only included in pig latin a small set of carefully chosen primitives that can be easily parallelized. language primitives that do not lend themselves to efficient parallel evaluation  e.g.  non-equi-joins  correlated subqueries  have been deliberately excluded.
　such operations can of course  still be carried out by writing udfs. however  since the language does not provide explicit primitives for such operations  users are aware of how efficient their programs will be and whether they will be parallelized.
1 debugging environment
　in any language  getting a data processing program right usually takes many iterations  with the first few iterations usually having some user-introduced erroneous processing. at the scale of data that pig is meant to process  a single iteration can take many minutes or hours  even with largescale parallel processing . thus  the usual run-debug-run cycle can be very slow and inefficient.
　pig comes with a novel interactive debugging environment that generates a concise example data table illustrating the output of each step of the user's program. the example data is carefully chosen to resemble the real data as far as possible and also to fully illustrate the semantics of the program. moreover  the example data is automatically adjusted as the program evolves.
　this step-by-step example data can help in detecting errors early  even before the first iteration of running the program on the full data   and also in pinpointing the step that has errors. the details of our debugging environment are provided in section 1.
1. pig latin
　in this section  we describe the details of the pig latin language. we describe our data model in section 1  and the pig latin statements in the subsequent subsections. the emphasis of this section is not on the syntactical details of pig latin  but on how it meets the design goals and features laid out in section 1. also  this section only focusses on the language primitives  and not on how they can be implemented to execute in parallel over a cluster. implementation is covered in section 1.
1 data model
　pig has a rich  yet simple data model consisting of the following four types:
  atom: an atom contains a simple atomic value such as a string or a number  e.g.  'alice'.
  tuple: a tuple is a sequence of fields  each of which can be any of the data types  e.g.   'alice'  'lakers' .
  bag: a bag is a collection of tuples with possible duplicates. the schema of the constituent tuples is flexible  i.e.  not all tuples in a bag need to have the same number and type of fields  e.g. 
	 	 'alice'  'lakers' 	ff
	`	＞
'alice'   'ipod'  'apple' 
	 	   'lakers'  1  ff  'age'★1  
t = 'alice' 
 'ipod'  1 
let fields of tuple t be called f1  f1  f1expression typeexamplevalue for tconstant'bob'independent of tfield by position$1'alice'field by namef1 	★ 1  
'age'projectionf1.$1   'lakers'  ff
 'ipod' conditional expressionf1#'age' 1 
'adult':'minor''adult'flatteningflatten f1 'lakers'  1
'ipod'  1table 1: expressions in pig latin.
the above example also demonstrates that tuples can be nested  e.g.  the second tuple of the bag has a nested tuple as its second field.
  map: a map is a collection of data items  where each item has an associated key through which it can be looked up. as with bags  the schema of the constituent data items is flexible  i.e.  all the data items in the map need not be of the same type. however  the keys are required to be data atoms  mainly for efficiency of lookups. the following is an example of a map:
	1	 	ff 1
 'lakers' 
'fan of'★
	1	 'ipod' 	1
'age'★1
in the above map  the key 'fan of' is mapped to a bag containing two tuples  and the key 'age' is mapped to an atom 1.
a map is especially useful to model data sets where schemas might change over time. for example  if web servers decide to include a new field while dumping logs  that new field can simply be included as an additional key in a map  thus requiring no change to existing programs  and also allowing access of the new field to new programs.
　table 1 shows the expression types in pig latin  and how they operate.  the flattening expression is explained in detail in section 1.  it should be evident that our data model is very flexible and permits arbitrary nesting. this flexibility allows us to achieve the aims outlined in section 1  where we motivated our use of a nested data model. next  we describe the pig latin commands.
1 specifying input data: load
　the first step in a pig latin program is to specify what the input data files are  and how the file contents are to be deserialized  i.e.  converted into pig's data model. an input file is assumed to contain a sequence of tuples  i.e.  a bag. this step is carried out by the load command. for example 
queries = load 'query log.txt'
using myload  
as  userid  querystring  timestamp ;

figure 1: example of flattening in foreach.the above command specifies the following:
  the input file is query log.txt.
  the input file should be converted into tuples by using the custom myload deserializer.
  the loaded tuples have three fields named userid  querystring  and timestamp.
　both the using clause  the custom deserializer  and the as clause  the schema information  are optional. if no deserializer is specified  a default one  that expects a plain text  tab-delimited file  is used. if no schema is specified  fields must be referred to by position instead of by name  e.g.  $1 for the first field . the ability to operate over plain text files  and the ability to specify schema information on the fly or not at all  allows the user to get started quickly  section 1 . to aid readability  it is desirable to include schemas while writing large pig latin programs.
　the return value of a load command is a handle to a bag which  in the above example  is assigned to the variable queries. this variable can then be used as an input in subsequent pig latin commands. note that the load command does not imply database-style loading into tables. bag handles in pig latin are only logical-the load command merely specifies what the input file is  and how it should be read. no data is actually read  and no processing carried out  until the user explicitly asks for output  see store command in section 1 .
1 per-tuple processing: foreach
　once input data file s  have been specified through load  one can specify the processing that needs to be carried out on the data. one of the basic operations is that of applying some processing to every tuple of a data set. this is achieved through the foreach command. for example 
expanded queries = foreach queries generate userid  expandquery querystring ;
　the above command specifies that each tuple of the bag queries  loaded in the previous section  should be processed independently to produce an output tuple. the first field of the output tuple is the userid field of the input tuple  and the second field of the output tuple is the result of applying the udf expandquery to the querystring field of the input tuple. suppose the udf expandquery generates a bag of likely expansions of a given query string. then an example transformation carried out by the above statement is as shown in the first step of figure 1.
　note that the semantics of the foreach command are such that there can be no dependence between the processing of different tuples of the input  thereby permitting an efficient parallel implementation. these semantics conform to our goal of only having parallelizable operations  section 1 .
　in general  the generate clause can be followed by a list of expressions that can be in any of the forms listed in table 1. most of the expression forms shown are straightforward  and have been included here only for completeness. the last expression type  i.e.  flattening  deserves some attention.
　often  we want to eliminate nesting in data. for example  in figure 1  we might want to flatten the set of possible expansions of a query string into separate tuples  so that they can be processed independently. we could also want to flatten the final result just prior to storing it. nesting can be eliminated by the use of the flatten keyword in the generate clause. flattening operates on bags by extracting the fields of the tuples in the bag  and making them fields of the tuple being output by generate  thus removing one level of nesting. for example  the output of the following command is shown as the second step in figure 1.
expanded queries = foreach queries generate userid  flatten expandquery querystring  ;
1 discarding unwanted data: filter
　another very common operation is to retain only some subset of the data that is of interest  while discarding the rest. this operation is done by the filter command. for example  to get rid of bot traffic in the bag queries: real queries = filter queries by userid neq 'bot';
the operator neq in the above example is used to signify string comparison  as opposed to numeric comparison which is specified via ==. filtering conditions in pig latin can involve a combination of expressions  table 1   comparison operators such as ==  eq  !=  neq  and the logical connectors and  or  and not.
　since arbitrary expressions are allowed  it follows that we can use udfs while filtering. thus  in our less ideal world  where bots don't identify themselves  we can use a sophisticated udf  isbot  to perform the filtering  e.g. 
real queries =
filter queries by not isbot userid ;
1 getting related data together: cogroup
　per-tuple processing only takes us so far. it is often necessary to group together tuples from one or more data sets  that are related in some way  so that they can subsequently be processed together. this grouping operation is done by the cogroup command. for example  suppose we have two data sets that we have specified through a load command:

figure 1: cogroup versus join.results:  querystring  url  position  revenue:  querystring  adslot  amount  results contains  for different query strings  the urls shown as search results  and the position at which they were shown. revenue contains  for different query strings  and different advertisement slots  the average amount of revenue made by the advertisements for that query string at that slot. then to group together all search result data and revenue data for the same query string  we can write:
grouped data = cogroup results by querystring  revenue by querystring;
　figure 1 shows what a tuple in grouped data looks like. in general  the output of a cogroup contains one tuple for each group. the first field of the tuple  named group  is the group identifier  in this case  the value of the querystring field . each of the next fields is a bag  one for each input being cogrouped  and is named the same as the alias of that input. the ith bag contains all tuples from the ith input belonging to that group. as in the case of filtering  grouping can also be performed according to arbitrary expressions which may include udfs.
　the reader may wonder why a cogroup primitive is needed at all  since a very similar primitive is provided by the familiar  well-understood  join operation in databases. for comparison  figure 1 also shows the result of joining our data sets on querystring. it is evident that join is equivalent to cogroup  followed by taking a cross product of the tuples in the nested bags. while joins are widely applicable  certain custom processing might require access to the tuples of the groups before the cross-product is taken  as shown by the following example.
　example 1. suppose we were trying to attribute search revenue to search-result urls to figure out the monetary worth of each url. we might have a sophisticated model for doing so. to accomplish this task in pig latin  we can follow the cogroup with the following statement:
url revenues = foreach grouped data generate
flatten distributerevenue results  revenue  ;
where distributerevenue is a udf that accepts search results and revenue information for a query string at a time  and outputs a bag of urls and the revenue attributed to them. for example  distributerevenue might attribute revenue from the top slot entirely to the first search result  while the revenue from the side slot may be attributed equally to all the results. in this case  the output of the above statement for our example data is shown in figure 1.
　to specify the same operation in sql  one would have to join by querystring  then group by querystring  and then apply a custom aggregation function. but while doing the join  the system would compute the cross product of the search and revenue information  which the custom aggregation function would then have to undo. thus  the whole process become quite inefficient  and the query becomes hard to read and understand.
　to reiterate  the cogroup statement illustrates a key difference between pig latin and sql. the cogroup statements conforms to our goal of having an algebraic language  where each step carries out only a single transformation  section 1 . cogroup carries out only the operation of grouping together tuples into nested bags. the user can subsequently choose to apply either an aggregation function on those tuples  or cross-product them to get the join result  or process it in a custom way as in example 1. in sql  grouping is available only bundled with either aggregation  group-by-aggregate queries   or with cross-producting  the join operation . users find this additional flexibility of pig latin over sql quite attractive  e.g. 
 i frankly like pig much better than sql in some respects  group + optional flatten works better for me  i love nested data structures .  - ted dunning  chief scientist  veoh networks
　note that it is our nested data model that allows us to have cogroup as an independent operation-the input tuples are grouped together and put in nested bags. such a primitive is not possible in sql since the data model is flat. of course  such a nested model raises serious concerns about efficiency of implementation: since groups can be very large  bigger than main memory  perhaps   we might build up gigantic tuples  which have these enormous nested bags within them. we address these efficiency concerns in our implementation section  section 1 .
1.1 special case of cogroup: group
　a common special case of cogroup is when there is only one data set involved. in this case  we can use the alternative  more intuitive keyword group. continuing with our example  if we wanted to find the total revenue for each query string   a typical group-by-aggregate query   we can write it as follows:
grouped revenue = group revenue by querystring; query revenues = foreach grouped revenue generate querystring 
sum revenue.amount  as totalrevenue;
　in the second statement above  revenue.amount refers to a projection of the nested bag in the tuples of grouped revenue. also  as in sql  the as clause is used to assign names to fields on the fly.
　to group all tuples of a data set together  e.g.  to compute the overall total revenue   one uses the syntax group revenue all.
1.1 join in pig latin
　not all users need the flexibility offered by cogroup. in many cases  all that is required is a regular equi-join. thus  pig latin provides a join keyword for equi-joins. for example 
join result = join results by querystring  revenue by querystring;
　it is easy to verify that join is only a syntactic shortcut for cogroup followed by flattening. the above join command is equivalent to:
temp var = cogroup results by querystring  revenue by querystring;
join result = foreach temp var generate
flatten results   flatten revenue ;
1.1 map-reduce in pig latin
　with the group and foreach statements  it is trivial to express a map-reduce  program in pig latin. converting to our data-model terminology  a map function operates on one input tuple at a time  and outputs a bag of key-value pairs. the reduce function then operates on all values for a key at a time to produce the final result. in pig latin 
map result = foreach input generate flatten map *  ; key groups = group map result by $1;
output = foreach key groups generate reduce * ;
　the first line applies the map udf to every tuple on the input  and flattens the bag of key value pairs that it produces.  we use the shorthand * as in sql to denote that all the fields of the input tuples are passed to the map udf.  assuming the first field of the map output to be the key  the second statement groups by key. the third statement then passes the bag of values for every key to the reduce udf to obtain the final result.
1 other commands
　pig latin has a number of other commands that are very similar to their sql counterparts. these are:
1. union: returns the union of two or more bags.
1. cross: returns the cross product of two or more bags.
1. order: orders a bag by the specified field s .
1. distinct: eliminates duplicate tuples in a bag. this command is just a shortcut for grouping the bag by all fields  and then projecting out the groups.
　these commands are used as one would expect. for example  continuing with the example of section 1.1  to order the query strings by their revenue:
ordered result = order query revenues by totalrevenue;
1 nested operations
　each of the pig latin processing commands described so far operate over one or more bags of tuples as input. as illustrated in the previous sections  these commands collectively form a very powerful language. when we have nested bags within tuples  either as a result of  co grouping  or due to the base data being nested  we might want to harness the same power of pig latin to process even these nested bags. to allow such processing  pig latin allows some commands to be nested within a foreach command.
　for example  continuing with the data set of section 1  suppose we wanted to compute for each querystring  the total revenue due to the 'top' ad slot  and also the overall total revenue. this can be written in pig latin as follows:
grouped revenue = group revenue by querystring; query revenues = foreach grouped revenue{ top slot = filter revenue by adslot eq 'top';
generate querystring 
sum top slot.amount  
sum revenue.amount ;
};
　in the above pig latin fragment  revenue is first grouped by querystring as before. then each group is processed by a foreach command  within which is a filter command that operates on the nested bags on the tuples of grouped revenue. finally the generate statement within the foreach outputs the required fields.
　at present  we only allow filter  order  and distinct to be nested within foreach. in the future  as need arises  we might allow other constructs to be nested as well.
1 asking for output: store
　the user can ask for the result of a pig latin expression sequence to be materialized to a file  by issuing the store command  e.g. 
store query revenues into 'myoutput' using mystore  ;
　the above command specifies that bag query revenues should be serialized to the file myoutput using the custom serializer mystore. as with load  the using clause may be omitted for a default serializer that writes plain text  tabdelimited files. our system also comes with a built-in serializer/deserializer that can load/store arbitrarily nested data.
1. implementation
　pig latin is fully implemented by our system  pig. pig is architected to allow different systems to be plugged in as the execution platform for pig latin. our current implementation uses hadoop   an open-source  scalable implementation of map-reduce   as the execution platform. pig latin programs are compiled into map-reduce jobs  and executed using hadoop. pig  along with its hadoop compiler  is an open-source project in the apache incubator  and hence available for general use.
　we first describe how pig builds a logical plan for a pig latin program. we then describe our current compiler  that compiles a logical plan into map-reduce jobs executed using hadoop. last  we describe how our implementation avoids large nested bags  and how it handles them if they do arise.
1 building a logical plan
　as clients issue pig latin commands  the pig interpreter first parses it  and verifies that the input files and bags being referred to by the command are valid. for example  if the user enters c = cogroup a by ...  b by ...  pig verifies that the bags a and b have already been defined. pig builds a logical plan for every bag that the user defines. when a new bag is defined by a command  the logical plan for the new bag is constructed by combining the logical plans for the input bags  and the current command. thus  in the above example  the logical plan for c consists of a cogroup command having the logical plans for a and b as inputs.
　note that no processing is carried out when the logical plans are constructed. processing is triggered only when the user invokes a store command on a bag. at that point  the logical plan for that bag is compiled into a physical plan  and is executed. this lazy style of execution is beneficial because it permits in-memory pipelining  and other optimizations such as filter reordering across multiple pig latin commands.
　pig is architected such that the parsing of pig latin and the logical plan construction is independent of the execution platform. only the compilation of the logical plan into a physical plan depends on the specific execution platform chosen. next  we describe the compilation into hadoop map-reduce  the execution platform currently used by pig.
1 map-reduce plan compilation
　compilation of a pig latin logical plan into map-reduce jobs is fairly simple. the map-reduce primitive essentially provides the ability to do a large-scale group by  where the map tasks assign keys for grouping  and the reduce tasks process a group at a time. our compiler begins by converting each  co group command in the logical plan into a distinct map-reduce job with its own map and reduce functions.
　the map function for  co group command c initially just assigns keys to tuples based on the by clause s  of c; the reduce function is initially a no-op. the map-reduce boundary is the cogroup command. the sequence of filter  and foreach commands from the load to the first cogroup operation c1  are pushed into the map function corresponding to c1  see figure 1 . the commands that intervene between subsequent cogroup commands ci and ci+1 can be pushed into either  a  the reduce function corresponding to ci  or  b  the map function corresponding to ci+1. pig currently always follows option  a . since grouping is often followed by aggregation  this approach reduces the amount of data that has to be materialized between map-reduce jobs.
　in the case of a cogroup command with more than one input data set  the map function appends an extra field to each tuple that identifies the data set from which the tuple originated. the accompanying reduce function decodes this information and uses it to insert the tuple into the appropriate nested bag when cogrouped tuples are formed  recall figure 1 .

figure 1: map-reduce compilation of pig latin.
　parallelism for load is obtained since pig operates over files residing in the hadoop distributed file system. we also automatically get parallelism for filter and foreach operations since for a given map-reduce job  several map and reduce instances are run in parallel. parallelism for  co group is achieved since the output from the multiple map instances is repartitioned in parallel to the multiple reduce instances. the order command is implemented by compiling into two map-reduce jobs. the first job samples the input to determine quantiles of the sort key. the second job rangepartitions the input according to the quantiles  thereby ensuring roughly equal-sized partitions   followed by local sorting in the reduce phase  resulting in a globally sorted file.
　the inflexibility of the map-reduce primitive results in some overheads while compiling pig latin into map-reduce jobs. for example  data must be materialized and replicated on the distributed file system between successive map-reduce jobs. when dealing with multiple data sets  an additional field must be inserted in every tuple to indicate which data set it came from. however  the hadoop map-reduce implementation does provide many desired properties such as parallelism  load-balancing  and fault-tolerance. given the productivity gains to be had through pig latin  the associated overhead is often acceptable. besides  there is the possibility of plugging in a different execution platform that can implement pig latin operations without such overheads.
1 efficiency with nested bags
　recall section 1. conceptually speaking  our  co group command places tuples belonging to the same group into one or more nested bags. in many cases  the system can avoid actually materializing these bags  which is especially important when the bags are larger than one machine's main memory.
　one common case is where the user applies a distributive or algebraic  aggregation function over the result of a  co group operation.  distributive is a special case of algebraic  so we will only discuss algebraic functions.  an algebraic function is one that can be structured as a tree of subfunctions  with each leaf subfunction operating over a subset of the input data. if nodes in this tree achieve data reduction  then the system can keep the amount of data materialized in any single location small. examples of algebraic functions abound: count  sum  min  max  average  variance  although some useful functions are not algebraic  e.g.  median.
　when pig compiles programs into hadoop map-reduce jobs  it uses hadoop's combiner feature to achieve a two-tier tree evaluation of algebraic functions. pig provides a special api for algebraic user-defined functions  so that custom user functions can take advantage of this important optimization.

figure 1: pig pen screenshot; displayed program finds users who tend to visit high-pagerank pages.　nevertheless  there still remain cases where  co group is followed by something other than an algebraic udf  e.g.  the program in example 1  where distributerevenue is not algebraic. to cope with these cases  our implementation allows for nested bags to spill to disk. our disk-resident bag implementation comes with database-style external sort algorithms to do operations such as sorting and duplicate elimination of the nested bags  recall section 1 .
1. debugging environment
　the process of constructing a pig latin program is typically an iterative one: the user makes an initial stab at writing a program  submits it to the system for execution  and inspects the output to determine whether the program had the intended effect. if not  the user revises the program and repeats this process. if programs take a long time to execute  e.g.  because the data is large   this process can be inefficient.
　to avoid this inefficiency  users often create a side data set consisting of a small sample of the original one  for experimentation. unfortunately this method does not always work well. as a simple example  suppose the program performs an equijoin of tables a x y  and b x z  on attribute x. if the original data contains many distinct values for x  then it is unlikely that a small sample of a and a small sample of b will contain any matching x values . hence the join over the sample data set may well produce an empty result  even if the program is correct. similarly  a program with a selective filter executed on a sample data set may produce an empty result. in general it can be difficult to test the semantics of a program over a sample data set.
　pig comes with a debugging environment called pig pen  which creates a side data set automatically  and in a manner that avoids the problems outlined in the previous paragraph.
to avoid these problems successfully  the side data set must be tailored to the particular user program at hand. we refer to this dynamically-constructed side data set as a sandbox data set; we briefly describe how it is created in section 1. pig pen's user interface consists of a two-panel window as shown in figure 1. the left-hand panel is where the user enters her pig latin commands. the right-hand panel is populated automatically  and shows the effect of the user's program on the sandbox data set. in particular  the intermediate bag produced by each pig latin command is displayed. suppose we have two data sets: a log of page visits  visits:  user  url  time   and a catalog of pages and their pageranks  pages:  url  pagerank . the program shown in figure 1 finds web surfers who tend to visit high-pagerank pages. the program joins the two data sets after first running the log entries through a udf that converts urls to a canonical form. after the join  the program groups tuples by user  computes the average pagerank for each user  and then filters users by average pagerank.
　the right-hand panel of figure 1 shows a sandbox data set  and how it is transformed by each successive command. the main semantics of each command are illustrated via the sandbox data set: we see that the join command matches visits tuples with pages tuples on url. we also see that grouping by user creates one tuple per group  possibly containing multiple nested tuples as in the case of amy. lastly we see that the foreach command eliminates the nesting via aggregation  and that the filter command eliminates fred  whose average pagerank is too low.
　if one or more commands had been written incorrectly  e.g.  if the user had forgotten to include group following foreach  the problem would be apparent in the right-hand panel. similarly  if the program contains udfs  as is common among real pig users   the right-hand panel indicates whether the correct udf is being applied  and whether it is behaving as intended. the sandbox data set also helps users understand the schema at each step  which is especially helpful in the presence of nested data.
　in addition to helping the user spot bugs  this kind of interface facilitates writing a program in an incremental fashion: we write the first three commands and inspect the right-hand panel to ensure that we are joining the tables properly. then we add the group command and use the right-hand panel to help understand the schema of its output tuples. then we proceed to add the foreach and filter commands  one at a time  until we arrive at the desired result. once we are convinced the program is correct  we submit it for execution over the real data.
1 generating a sandbox data set
　pig pen's sandbox data set generator takes as input a pig latin program p consisting of a sequence of n commands  where each command consumes one or more input bags and produces an output bag. the output of the data set generator is a set of example bags {b1 b1 ... bn}  one corresponding to the output of each command in p  as illustrated in figure 1. the set of example bags is required to be consistent  meaning that the example output bag of each operator is exactly the bag produced by executing the command over its example input bag s . the example bags that correspond to the load commands comprise the sandbox data set.
　there are three primary objectives in selecting a sandbox data set:
  realism. the sandbox data set should be a subset of the actual data set  if possible. if not  then to the extent possible the individual data values should be ones found in the actual data set.
  conciseness. the example bags should be as small as possible.
  completeness. the example bags should collectively illustrate the key semantics of each command.
　as an example of what is meant by the completeness objective  the example bags before and after the group command in figure 1 serve to illustrate the semantics of grouping by user  namely that input tuples about the same user are placed into a single output tuple. as another example  the example bags before and after the filter command illustrate the semantics of filtering by average pagerank  namely that input tuples with low average pagerank are not propagated to the output.
　the procedure used in pig pen to generate a sandbox database starts by taking small random samples of the base data. then  pig pen synthesizes additional data tuples to improve completeness.  when possible  the synthetic tuples are populated with real data values from the underlying domain  to minimize the impact on realism.  a final pruning pass eliminates redundant example tuples to improve conciseness. the details of the algorithm are beyond the scope of this paper.
1. usage scenarios
　in this section  we describe a sample of data analysis tasks that are being carried out at yahoo! with pig. due to confidentiality reasons  we describe these tasks only at a highlevel  and are not able to provide real pig latin programs. the use of pig latin ranges from group-by-aggregate and rollup queries  which are easy to write in sql  and simple ones are also easy to write in map-reduce  to more complex tasks that use the full flexibility and power of pig latin.
rollup aggregates: a common category of processing done using pig involves computing various kinds of rollup aggregates against user activity logs  web crawls  and other data sets. one example is to calculate the frequency of search terms aggregated over days  weeks  or months  and also over geographical location as implied by the ip address. some tasks require two or more successive aggregation passes  e.g.  count the number of searches per user  and then compute the average per-user count. other tasks require a join followed by aggregation  e.g.  match search phrases with n-grams found in web page anchortext strings  and then count the number of matches per page. in such cases  pig orchestrates a sequence of multiple map-reduce jobs on the user's behalf.
　the primary reason for using pig rather than a database/olap system for these rollup analyses  is that the search logs are too big and continuous to be curated and loaded into databases  and hence are just present as  distributed  files. pig provides an easy way to compute such aggregates directly over these files  and also makes it easy to incorporate custom processing  such as ip-to-geo mapping and n-gram extraction.
temporal analysis: temporal analysis of search logs mainly involves studying how search query distributions change over time. this task usually involves cogrouping search queries of one period with those of another period in the past  followed by custom processing of the queries in each group. the cogroup command is a big win here since it provides access to the set of search queries in each group  recall figure 1   making it easy and intuitive to write udfs for custom processing. if a regular join were carried out instead  a cross product of search queries in the same group would be returned  leading to problems as in example 1.
session analysis: in session analysis  web user sessions  i.e.  sequences of page views and clicks made by users  are analyzed to calculate various metrics such as: how long is the average user session  how many links does a user click on before leaving a website  how do click patterns vary in the course of a day/week/month. this analysis task mainly consists of grouping the activity logs by user and/or website  ordering the activity in each group by timestamp  and then applying custom processing on this sequence. in this scenario  our nested data model provides a natural abstraction for representing and manipulating sessions  and nested declarative operations such as order-by  section 1  help minimize custom code and avoid the need for out-of-core algorithms inside udfs in case of large groups.
1. related work
　we have discussed the relationship of pig latin to sql and map-reduce throughout the paper. in this section  we compare pig against other data processing languages and systems.
　pig is meant for offline  ad-hoc  scan-centric workloads. there is a family of recent  large-scale  distributed systems that provide database-like capabilites  but are geared toward transactional workloads and/or point lookups. amazon's dynamo   google's bigtable   and yahoo!'s pnuts  are examples. unlike pig  these systems are not meant for data analysis. bigtable does have hooks through which data residing in bigtable can be analyzed using a map-reduce job  but in the end  the analysis engine is still map-reduce.
　a variant of map-reduce that deals well with joins has been proposed   but it still does not deal with multistage programs.
　dryad  is a distributed platform that is being developed at microsoft to provide large-scale  parallel  faulttolerant execution of processing tasks. dryad is more flexible than map-reduce as it allows the execution of arbitrary computation that can be expressed as directed acyclic graphs. map-reduce  on the other hand  can only execute a simple  two-step chain of a map followed by a reduce. as mentioned in section 1  pig latin is independent of the choice of the execution platform. hence in principle  pig latin can be compiled into dryad jobs. as with map-reduce  the dryad layer is hard to program to. hence dryad has its own high-level language called dryadlinq . little is known publicly about the language except that it is  sql-like. 
　sawzall  is a scripting language used at google on top of map-reduce. like map-reduce  a sawzall program also has a fairly rigid structure consisting of a filtering phase  the map step  followed by an aggregation phase  the reduce step . furthermore  only the filtering phase can be written by the user  and only a pre-built set of aggregations are available  new ones are non-trivial to add . while pig latin has similar higher level primitives like filtering and aggregation  an arbitrary number of them can be flexibly chained together in a pig latin program  and all primitives can use user-defined functions with equal ease. further  pig latin has additional primitives such as cogrouping  that allow operations such as joins  which require multiple programs in sawzall  to be written in a single line in pig latin.
　we are certainly not the first to consider nested data models. nested data models have been explored before in the context of object-oriented databases . the programming languages community has explored data-parallel languages over nested data  e.g.  nesl   to which pig latin bears some resemblance. just as with map-reduce and sawzall  nesl lacks support for combining multiple data sets  e.g.  cogroup and join .
　as for our debugging environment  we are not aware of any prior work on automatically generating intermediate example data for the purpose of illustrating processing semantics.
1. future work
　pig is a project under active development. we are continually adding new features to improve the user experience and yield even higher productivity gains. there are a number of promising directions that are yet unexplored in the context of the pig system.
 safe  optimizer: one of the arguments that we have put forward in the favor of pig latin is that due to its procedural nature  users have tighter control over how their programs are executed. however  we do not want to ignore databasestyle optimization altogether since it can often lead to huge improvements in performance. what is needed is a  safe  optimizer that will only perform optimizations that almost surely yield a performance benefit. in other cases  when the performance benefit is uncertain  or depends on unknown data characteristics  it should simply follow the pig latin sequence written by the user.
user interfaces: the productivity obtained by pig can be enhanced through the right user interfaces. pig pen is a first step in that direction. another idea worth exploring is to have a  boxes-and-arrows  gui for specifying and viewing pig latin programs  in addition to the current textual language. a boxes-and-arrows paradigm illustrates the data flow structure of a program very explicitly  as reveals dependencies among the various computation steps. the ui should also promote sharing and collaboration  e.g.  one should easily be able to link to a fragment of another user's program  and incorporate udfs provided by other users.
external functions: pig programs currently run as java map-reduce jobs  and consequently only support udfs written in java. however  for quick  ad-hoc tasks  the average user wants to write udfs in a scripting language such as perl or python  instead of in a full-blown programming language like java. support for such functions requires a light-weight serialization/deserialization layer with bindings in the languages that we wish to support. pig can then serialize data using this layer and pass it to the external process that runs the non-java udf  where it can be deserialized into that language's data structures. we are in the process of building such a layer and integrating it with pig.
unified environment: pig latin does not have control structures like loops and conditionals. if those are needed  pig latin  just like sql  can be embedded in java with a jdbc-style interface. however  as with sql  this embedding can be awkward and difficult to work with. for example  all pig latin programs must be enclosed in strings  thus preventing static syntax checking. results need to be converted back and forth between java's data model and that of pig. moreover  the user now has to deal with three distinct environments:  a  the program in which the pig latin commands are embedded   b  the pig environment that understands pig latin commands  and  c  the programming language environment used to write the udfs. we are exploring the idea of having a single  unified environment that supports development at all three levels. for this purpose  rather than designing yet another language  we want to embed pig latin in established languages such as perl and python1 by making the language parsers aware of pig latin and able to package executable units for remote execution.
1. summary
　we described a new data processing environment being deployed at yahoo! called pig  and its associated language  pig latin. pig's target demographic is experienced procedural programmers who prefer map-reduce style programming over the more declarative  sql-style programming  for stylistic reasons as well as the ability to control the execution plan. pig aims for a sweet spot between these two extremes  offering high-level data manipulation primitives such as projection and join  but in a much less declarative style than sql.
　we also described a novel debugging environment we are developing for pig  called pig pen. in conjunction with the step-by-step nature of our pig latin language  pig pen makes it easy and fast for users to construct and debug their programs in an incremental fashion. the user can write a prefix of their overall program  examine the output on the sandbox data set  and iterate until the output matches what they intended. at this point the user can  freeze  the program prefix  and begin to append further commands  without worrying about regressing on the progress made so far.
　while pig pen is still in early stages of development  the core pig system is fully implemented and available as an open-source apache incubator project. the pig system compiles pig latin expressions into a sequence of map-reduce jobs  and orchestrates the execution of these jobs on hadoop  an open-source scalable map-reduce implementation. pig has an active and growing user base inside yahoo!  and with our recent open-source release we are beginning to attract users in the broader community.
acknowledgments
we are grateful to the hadoop and pig engineering teams at yahoo! for helping us make pig real. in particular we would like to thank alan gates and olga natkovich  the pig engineering leads  for their invaluable contributions.
