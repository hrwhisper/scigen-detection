   we analyze the results of several experimental runs submitted to the legal discovery and terabyte tracks of trec 1. in the legal track  the final negotiated boolean queries produced higher mean scores in average precision and precision 1 than a corresponding vector run of the same query terms  but the vector run usually recalled more relevant items by rank 1  and on average the boolean query matched less than half of the relevant items. we also report the impact of query negotiation  metadata and natural language vs. keywords. we find that robust metrics  which expose the downside of blind feedback techniques  are inappropriate for legal discovery because they favour duplicate filtering. we also report the results of depth probe experiments  depth 1 in the legal track and depth 1 in the terabyte track  which provide a lower-bound estimate for the number of unjudged relevant items in each track.
1	introduction
livelink ecm - edocs searchservertm  formerly known as hummingbird searchservertm  is a toolkit for developing enterprise search and retrieval applications. the searchserver kernel is also embedded in other components of the livelink ecm - edocs suite1.
   searchserver works in unicode internally  and supports most of the world's major character sets and languages. the major conferences in text retrieval experimentation  trec   clef  and ntcir   have provided judged test collections for objective experimentation with searchserver in more than a dozen languages.
   this paper describes experimental work with searchserver  experimental post-1 builds  for legal discovery and terabyte retrieval.
1	legal discovery track
the legal discovery track  also known as the trec legal track  was new to trec this year. the collection to be searched was the iit complex document information processing  iit cdip  test collection . it contained 1 1 metadata records from us tobacco companies; 1 1 of the records included document text of varying quality from an optical character reader. uncompressed  the collection was 1 1 bytes  1gb . the average record size  including metadata markup and the ocr document  was 1 bytes.
   in legal discovery  the goal  indeed  legal obligation  is to return all documents responsive  relevant  to a production request. for trec  the organizers created 1 new production requests  topics   numbered from 1 to 1. each topic included a  request text   a natural language description of the request  typically onesentence   a  defendant query   an initial boolean query proposed by the defendant   and a  final negotiated query   a rejoinder boolean query from the plaintiff  which was used as the final query of the negotiations .  examples appear in the topic analysis below.  during the assessing phase  1 topics were dropped  leaving 1 topics. for these  1 records were judged relevant per topic  low 1  high 1  median 1 .
 has more details on the track and task  and  has more background on legal discovery in general.
1	indexing
we indexed the collection twice  once including the full record  metadata plus ocr document   and a second time just indexing the ocr documents. for the full record case  we indexed from the   /tid   tag to the   /record   tag  which meant both the metadata and the ocr document were in the ft text column. for the ocr-only case  we indexed from after the   ot   tag to the   /record   tag  hence just the ocr document was in the ft text column. any tags themselves were indexed  we just didn't bother to discard them . entities  e.g.  &amp;   were converted to the character they represented  e.g.  &  .
   for the full record index  we did not use a stopword list  and we also indexed most punctuation as 1character words  exceptions were the hyphen and apostrophe  which were treated as 1-character stopwords . the contents of the   dd   section of the metadata were additionally indexed in a separate docdate column.
   for the ocr-only index  we used an english stopword list  e.g.  the    of    by   and punctuation was not indexed.
   both indexes just included the surface forms of the words  no stemming . the documents were assumed to be in the windows-1 character set when converted to unicode. words were normalized to upper-case and any accents were dropped.
1	searching
the techniques used for the 1 submitted runs of july 1  and 1 later re-runs  are described below. the relevance ranking approach was the same for all runs. the relevance function dampened the term frequency and adjusted for document length in a manner similar to okapi  and dampened the inverse document frequency using an approximation of the logarithm. for wildcard terms  e.g.  televis!    all variants  e.g.  television    televised    televisions   etc.  were treated as occurrences of the same term for term frequency purposes  and inverse document frequency was based on the most common variant. for terms in phrases or proximity contraints of boolean queries  only occurrences of the term satisfying the phrase or proximity counted towards term frequency.
the 1 submitted runs  and 1 re-runs  were as follows:
   huml1t  main boolean run : the submitted huml1t run used the final negotiated query  respecting the boolean operators such as and  phrase  proximity  not  etc. full wildcard matching was supported. relevance-ranking was still used to order the matching rows. the run was labelled as manual because some hand-editing was done to convert the queries to the searchsql syntax of searchserver  but the run was automatic in spirit because it just implemented the final boolean query intended by the negotiators. for example  for topic 1  for which the final negotiated query was   research or stud! or  in vivo   and pigeon and  death! or dead or die! or dying    the corresponding searchsql statement was
select relevance '1'  as rel  docno
from legal1full
where   ft text contains 'research'  'stud%'  'in vivo' 
	and	ft text contains 'pigeon'
and  ft text contains 'death%'  'dead'  'die%'  'dying'  
order by rel desc;
   huml1t'  updated main boolean run  not submitted  : the huml1t' run was the same as huml1t except that an updated development build was used which fixed a proximity-matching bug that affected 1 of the queries. fortunately  there was little impact on the overall results  more details are in section 1.1 .
   huml1  defendant boolean run : the submitted huml1 run was the same as huml1t except that the defendant query was used instead of the final negotiated boolean query. for example  for topic 1  for which the initial query proposed by the defendant was   animal studies  and  pigeon deaths    the
where clause of the corresponding searchsql statement was
where  ft text contains 'animal studies'
and ft text contains 'pigeon deaths' 
huml1'  updated defendant boolean run  not submitted  : the huml1' run was the same as
huml1 except that the updated development build was used  more details are in section 1.1 .
   huml1tv  main vector run : the submitted huml1tv run was the same as huml1t except that the boolean operators such as and  phrases and proximities were dropped  all operators became an or   and punctuation was dropped. full wildcarding was still respected. for example  for topic 1  the where clause of the corresponding searchsql statement was
where   ft text contains 'research'|'stud%'|'in'|'vivo'|
'pigeon'|'death%'|'dead'|'die%'|'dying'  
   huml1tvo  main ocr-only run : the submitted huml1tvo run was the same as huml1tv except that the ocr-only index was used instead of the full-record index  i.e. the from clause of the searchsql statement referred to the legal1ocr table instead of legal1full . huml1dvo  main natural language run : the submitted huml1dvo run was the same as huml1tvo except that the terms were taken from the request text instead of the final negotiated query. terms in our list of common instruction words  e.g.  find    relevant    document   were discarded  using the same list we used in past years for the description field of an ad hoc topic . for example  for topic 1  for which the request text was  all documents that refer or relate to pigeon deaths during the course of animal studies.   the where clause of the corresponding searchsql statement was
where ft text contains 'all'|'that'|'refer'|'or'|'relate'|'to'|
'pigeon'|'deaths'|'during'|'the'|'course'|'of'|'animal'|'studies'
huml1tve  blind feedback run : the submitted huml1tve run was a blind feedback run based 1% on
huml1tv and 1% each on expansion queries from the first 1 rows of huml1tv. huml1tvc  duplicate filtering run : the submitted huml1tvc run was the same as huml1tv except that rows which appeared to be duplicates of a previous row were discarded. the heuristic filtering approach was to discard a row if all of the passages surrounding matching terms were identical to the set of such passages of any previous row.
   huml1tvz  depth probe run : the first 1 rows of the submitted huml1tvz run were a one percent subset of the first 1 rows of huml1tv  rows 1  1  1  1  ...  1   and its remaining 1 rows were rows 1 of huml1tv.  this run was based on the same approach as the humt1xlz run submitted a month earlier for the terabyte track  described in a later section. 
for each run  only 1 rows were allowed to be submitted for each query.
1	results
tables 1 list several mean scores for the 1 submitted runs  and 1 re-runs in parentheses . the retrieval measures are defined in section 1 of the glossary at the end of the paper. the highest mean scores of each measure are in bold; however  see tables 1 for which mean differences are statistically significant.  the columns of tables 1 are explained in section 1 of the glossary. 
table 1: mean scores of legal discovery runs  all 1 topics 

 huml1t' 111111111huml1t111111111huml1tve111111111huml1tv111111111huml1tvc111111111huml1dvo111111111huml1tvo111111111huml1111111111 huml1' 111111111huml1tvz111111111 on judged gs1js1js1jp1jr-precjbprefjgmapjmapjr 1j huml1t' 111111111huml1t111111111huml1tve111111111huml1tv111111111huml1tvc111111111huml1dvo111111111huml1tvo111111111huml1111111111 huml1' 111111111huml1tvz111111111   table 1 shows that the main vector run had a higher recall by depth 1  r 1 measure  than the corresponding main boolean run  which is unsurprising because the vector run would usually match all of the same rows as the boolean run  and more.
   however  if there was a penalty for retrieving non-relevant items before the relevant items  e.g. if precision was a factor in the measure  then the main boolean run typically had the higher mean score. in particular  the main boolean run had the highest mean average precision  map  of the listed runs of table 1. map is a measure which requires both perfect recall and precision to get a 1 score.
   tables 1 and 1 include measures based on the boolean set size. in particular  recall b was less than 1% for the main boolean run  i.e. the boolean query did not match more than 1% of the  judged  relevant items  on average .
   note that only the first 1 rows submitted for each query were guaranteed to be in the assessing pool  and a few of those still were not assessed . table 1 includes its measures after discarding unjudged items  and the results were similar  e.g. mapj gave almost the same ranking as map . still  the inconsistency in the number of judged items in the boolean sets for different runs  j b column of table 1  is disconcerting.
   the next 1 sections look at the differences between the submitted runs in more detail  followed by an additional section on the updated boolean runs.
1.1	defendant boolean vs. main boolean
the main boolean query  the rejoinder by the plaintiff  was apparently successful at finding more relevant items than the initial boolean query proposed by the defendant. table 1 shows that the main boolean run's recall 1  r 1  was higher for 1 of the 1 topics and tied for the other 1  with no declines  as per the  t-t1  row for r 1 of table 1 . we look at topic 1  for which tables 1 show that the main boolean run was superior on several metrics  and topic 1  for which the defendant boolean run had a much higher precision 1  though still lower recall :
table 1: mean scores of legal discovery runs  1 topics of b 1 

 huml1t' 111111%1%11huml1t111111%1%11huml1tve111111%1%11huml1tv111111%1%11huml1tvc111111%1%11huml1dvo111111%1%11huml1tvo111111%1%11huml1111111%1%11 huml1' 111111%1%11huml1tvz111111%1%11table 1: mean scores of legal discovery runs  1 topics of b1 

 huml1t' 111111%1%11huml1t111111%1%11huml1tve111111%1%11huml1tv111111%1%11huml1tvc111111%1%11huml1dvo111111%1%11huml1tvo111111%1%11huml1111111%1%11 huml1' 111111%1%11huml1tvz111111%1%11  topic 1   pigeon deaths during the course of animal studies  : the defendant boolean query   animal studies  and  pigeon deaths   required both the phrases  animal studies  and  pigeon deaths  to be in the record  which matched no records  and hence all of the relevant items were missed. the main boolean query   research or stud! or  in vivo   and pigeon and  death! or dead or die! or dying   was more lenient  matching 1 records  of which 1 were judged and 1 of those were judged relevant  although 1 other relevant records were not matched .
  topic 1   marketing  placement  or sale of lozenges to children  : the defendant boolean query   marketing or placement or sale  and  lozenges  and  children   required the term  marketing  or  placement  or  sale  to be in the record  which led to high precision in the first 1 items  p1 of 1 . the main boolean query  lozenges and  child! or teen! or juvenile! or kid! or adolescent   dropped the  marketing  requirement  which matched all the same documents and more  1 vs. 1  and found a couple more relevant items  1 instead of 1   but precision was lower  e.g. p1 was 1 . the main boolean query still missed 1 of the 1 relevant items.
   overall  in the negotiations  it appears that the defendant typically proposed a high-precision query  and the plaintiff's rejoinder was to modify it so that it would match the same rows and more  higher recall   though sometimes at the expense of precision.
table 1: impact of legal discovery techniques  gs1j  p1 and r-prec measures 
expt gs1j1% confvs.1 extreme diffs  topic t-t1.1  1  1 1-1.1  1   1  1    1  1 t'-t1'1  1  1 1-1.1  1   1  1    1  1 t-tv1  1  1 1-1.1  1    1  1    1  1 t'-tv1  1  1 1-1.1  1    1  1    1  1 tv-tvo 1  1  1 1-1 1  1    1  1   1  1 dvo-tvo 1  1  1 1-1 1  1   1  1   1  1 tve-tv 1  1  1 1-1 1  1    1  1   1  1 tvc-tv1  1  1 1-1.1  1   1  1   1  1 t'-t1  1  1 1-1.1  1   1  1    1  1 t1'-t1.1  1  1 1-1.1  1   1  1   1  1 t-tve1  1  1 1-1.1  1    1  1    1  1 t'-tve1  1  1 1-1.1  1    1  1    1  1  p1t-t1.1  1  1 1-1.1  1   1  1    1  1 t'-t1'1  1  1 1-1.1  1   1  1    1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo1  1  1 1-1 1  1   1  1   1  1 dvo-tvo1  1  1 1-1.1  1   1  1    1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv 1  1  1 1-1 1  1   1  1   1  1 t'-t 1  1  1 1-1 1  1    1  1   1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve1  1  1 1-1.1  1    1  1    1  1 t'-tve1  1  1 1-1.1  1    1  1    1  1  r-prect-t1.1  1  1 1-1.1  1   1  1    1  1 t'-t1'1  1  1 1-1.1  1   1  1    1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo1  1  1 1-1.1  1   1  1    1  1 dvo-tvo 1  1  1 1-1.1  1    1  1    1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t 1  1  1 1-1.1  1    1  1    1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve1  1  1 1-1.1  1   1  1    1  1 t'-tve1  1  1 1-1.1  1   1  1    1  1 1.1	main boolean vs. main vector
the main vector run used the same query terms as the main boolean run  but all boolean operators such as and  phrases and proximities were dropped  all operators became an or   and punctuation was dropped. full wildcarding was still respected. the vector run always matched at least 1 rows  the limit to submit . topics strongly favoring the boolean run  in at least one metric :
  topic	1	  pigeon	deaths	during	the	course	of	animal	studies  :	this	topic	had	the table 1: impact of legal discovery techniques  bpref  gmap' and map measures 
expt bpref1% confvs.1 extreme diffs  topic t-t1.1  1  1 1-1.1  1   1  1    1  1 t'-t1'1  1  1 1-1.1  1   1  1    1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo 1  1  1 1-1 1  1    1  1   1  1 dvo-tvo1  1  1 1-1 1  1   1  1   1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t 1  1  1 1-1 1  1    1  1   1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve1  1  1 1-1.1  1   1  1    1  1 t'-tve1  1  1 1-1.1  1   1  1    1  1  gmap't-t1.1  1  1 1-1.1  1   1  1  t'-t1'1  1  1 1-1.1  1   1  1  t-tv1  1  1 1-1t'-tv1  1  1 1-1tv-tvo1  1  1 1-1.1  1   1  1   1  1  1 1-1.1  1   1  1  tve-tv1  1  1 1-1.1  1   1  1   1  1  1 1-1 1  1  1 1-1 1  1  1 1-1t-tve1  1  1 1-1t'-tve1
 map  1  1 1-1t-t1.1  1  1 1-1.1  1   1  1  t'-t1'1  1  1 1-1.1  1   1  1  t-tv1  1  1 1-1.1  1   1  1  t'-tv1  1  1 1-1.1  1   1  1  tv-tvo1  1  1 1-1.1  1   1  1   1  1  1 1-1tve-tv1  1  1 1-1.1  1   1  1   1  1  1 1-1 1  1  1 1-1.1  1   1  1  1 1-1t-tve1  1  1 1-1.1  1   1  1  t'-tve1  1  1 1-1.1  1   1  1  biggest difference in p1 in favour of the boolean run  as per the  t-tv  entry for  p1 of table 1 . the main boolean query   research or stud! or  in vivo   and pigeon and  death! or dead or die! or dying   required the term  pigeon  to be in the record  which was good for precision  e.g. p1 of 1 . in the main vector query
 'research'|'stud%'|'in'|'vivo'|'pigeon'|'death%'|'dead'|'die%'|'dying'   the synonyms for  death  dominated the query  and many high-ranking matches did not mention pigeons  hurting precision  e.g. p1 of 1 . at 1 items retrieved  1 judged   the vector run had found 1 of the table 1: impact of legal discovery techniques  r 1 measure 

	expt	 r 1% conf	vs.	1 extreme diffs  topic 


table 1: impact of legal discovery techniques  p b and r b  1 topics of b 1 
expt p b1% confvs.1 extreme diffs  topic t-t1.1  1  1 1-1.1  1   1  1   1  1 t'-t1'1  1  1 1-1.1  1   1  1   1  1 t-tv1  1  1 1-1 1  1   1  1   1  1 t'-tv1  1  1 1-1 1  1   1  1   1  1 tv-tvo 1  1  1 1-1 1  1   1  1   1  1 dvo-tvo 1  1  1 1-1 1  1    1  1   1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t 1  1  1 1-1 1  1    1  1   1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve 1  1  1 1-1 1  1    1  1   1  1 t'-tve 1  1  1 1-1 1  1    1  1   1  1  r bt-t1.1  1  1 1-1.1  1   1  1   1  1 t'-t1'1  1  1 1-1.1  1   1  1   1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo1  1  1 1-1.1  1   1  1    1  1 dvo-tvo1  1  1 1-1.1  1   1  1    1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv1  1  1 1-1.1  1    1  1    1  1 t'-t 1  1  1 1-1 1  1    1  1   1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve1  1  1 1-1.1  1   1  1    1  1 t'-tve1  1  1 1-1.1  1   1  1    1  1 1 relevant items  fewer than the boolean run which found 1 relevant items in the 1 matched records  1 judged .
table 1: impact of legal discovery techniques  pj b and j b  1 topics of b 1 
expt pj b1% confvs.
1 extreme diffs  topic  j bt-t1 1  1  1 1-1.1  1   1  1    1  1 t'-t1' 1  1  1 1-1.1  1   1  1    1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo1  1  1 1-1.1  1   1  1    1  1 dvo-tvo 1  1  1 1-1 1  1    1  1   1  1 tve-tv 1  1  1 1-1 1  1    1  1   1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t1  1  1 1-1.1  1    1  1    1  1 t1'-t1.1  1  1 1-1.1  1   1  1   1  1 t-tve1  1  1 1-1.1  1   1  1    1  1 t'-tve1  1  1 1-1.1  1   1  1    1  1   topic 1   donations or contributions to the libertarian party  : this topic had the biggest difference in r b  map  gmap'  bpref  r-prec and gs1j in favour of the boolean run  as per the  t-tv  entries of tables 1 . the main boolean query    donat! or contrib!  and libertarian  but not  democrat! or republic! or gop or  g.o.p. 
or  grand old party     which excluded any records with words such as  democratic    republican  or  gop   matched 1 records  1 judged   finding 1 of the 1 relevant items  and the 1 relevant items were the first 1 items returned . the main vector query  'donat%'|'contrib%'|'libertarian'|'democrat%'|'republic%'|'gop'|'g'|'o'|'p'|'grand'| 'old'|'party'  dropped the not operator  which hurt precision  the first relevant item was returned at rank 1   though all 1 relevant items were returned in the first 1 items  1 judged .  this was the only topic for which the main boolean query used a not operator. 
  topic 1   contracts with medical supply companies or outfitters  : this topic had the biggest difference in p b and pj b in favour of the boolean run  as per the  t-tv  entries of tables 1 and 1 . the main boolean query   contract! or agreement! or  purchase order  or invoice  and   medical suppl!  or outfitter!   matched 1 records  1 judged  1 judged relevant . the main vector query
 'contract%'|'agreement%'|'purchase'|'order'|'invoice'|'medical'|'suppl%'|
'outfitter%'  found just 1 relevant items in the first 1 records. it appears that the  medical  concept was important for relevance  but the  suppl%  and  purchase  terms had more weight from inverse document frequency  and it hurt precision when these terms were not restricted to being table 1: impact of legal discovery techniques  p b1 and r b1  1 topics of b1 
expt p b1% confvs.1 extreme diffs  topic t-t1.1  1  1 1-1.1  1   1  1   1  1 t'-t1'1  1  1 1-1.1  1   1  1   1  1 t-tv1  1  1 1-1 1  1   1  1   1  1 t'-tv1  1  1 1-1 1  1   1  1   1  1 tv-tvo 1  1  1 1-1 1  1   1  1   1  1 dvo-tvo 1  1  1 1-1 1  1    1  1   1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t1  1  1 1-1.1  1    1  1    1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve 1  1  1 1-1 1  1    1  1   1  1 t'-tve 1  1  1 1-1 1  1    1  1   1  1  r b1t-t1.1  1  1 1-1.1  1   1  1   1  1 t'-t1'1  1  1 1-1.1  1   1  1   1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo1  1  1 1-1.1  1   1  1    1  1 dvo-tvo1  1  1 1-1.1  1   1  1    1  1 tve-tv1  1  1 1-1.1  1   1  1    1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t1  1  1 1-1.1  1    1  1    1  1 t1'-t1 1  1  1 1-1 1  1   1  1   1  1 t-tve1  1  1 1-1.1  1   1  1    1  1 t'-tve1  1  1 1-1.1  1   1  1    1  1 parts of phrases.
topics strongly favoring the vector run  in at least one metric :
  topic 1   company guidelines  strategies  or internal approval for placement of tobacco products in movies that are mentioned as g-rated  : this topic had the biggest differences in r b  p b  pj b and bpref in favour of the vector run  as per the  t-tv  entries of tables 1 . the main boolean query    guide! or strateg! or approv!  and  place! or promot!   and    g-rated  or  g rated  or family  w/1  movie! or film!
or picture!    matched 1 records  1 judged  1 relevant   while the vector query  'guide%'|'strateg%'|'approv%'|'place%'|'promot%'|'g'|'rated'|'g'|'rated'|'family'| 'movie%'|'film%'|'picture%'  found 1 relevant items in the first 1. the boolean query formulation did not match relevant items with passages such as   g  and 'pg -rated movies  or even   g -rated film   because the query only allowed 1 punctuation character between  g  and  rated  . also  the assessor seems to have allowed  pg  as a synonym  e.g.  rated pg  . the boolean query's matches typically had  family  near a word like  moviegoer  or  film   which typically were not relevant. this topic is an example of the difficulties of forming a good boolean query.
  topic 1   documents in which a tobacco company chief executive officer or chief compliance officer expressly refers to the foreign corrupt practices act  : this topic had the 1nd-biggest difference in precision b in favour of the vector run. the main boolean query    chief executive officer  or  chief compliance officer  or ceo table 1: impact of legal discovery techniques  pj b1 and j b1  1 topics of b1 
expt pj b1% confvs.
1 extreme diffs  topic  j b1t-t1 1  1  1 1-1.1  1   1  1    1  1 t'-t1' 1  1  1 1-1.1  1   1  1    1  1 t-tv1  1  1 1-1.1  1   1  1    1  1 t'-tv1  1  1 1-1.1  1   1  1    1  1 tv-tvo1  1  1 1-1 1  1   1  1   1  1 dvo-tvo 1  1  1 1-1 1  1    1  1   1  1 tve-tv 1  1  1 1-1 1  1    1  1   1  1 tvc-tv 1  1  1 1-1 1  1    1  1   1  1 t'-t1  1  1 1-1.1  1    1  1    1  1 t1'-t1.1  1  1 1-1.1  1   1  1   1  1 t-tve1  1  1 1-1.1  1   1  1    1  1 t'-tve1  1  1 1-1.1  1   1  1    1  1 or  c.e.o.  or cco or  c.c.o.   and   foreign corrupt practices act  or fcpa   matched 1 records  all of which were judged  with 1 judged relevant. the vector query  'chief'|'executive'|'officer'|'chief'|'compliance'|'officer'|'ceo'|'c'|'e'|'o'|'cco' |'c'|'c'|'o'|'foreign'|'corrupt'|'practices'|'act'|'fcpa'  found 1 relevant items in the first 1 records. it appears that the key phrase for relevance was  foreign corrupt practices act  and that it was not necessary for a relevant item to contain the phrase  chief executive officer   or its synonyms in the boolean query . some boolean matches were from stray terms such as  fcpa  and  ceo  appearing in a document  probably from ocr errors. another relevant item  jup1  was missed by the boolean query because the ocr outputted  chief sxecutive o officer .
   it appears that the negotiations often led to a boolean query that was superior to a vector query of the same terms  but sometimes  perhaps from the lack of interaction with the documents  the negotiated query was less effective than it could have been. it is known from past trec studies that good manual runs can outperform the top automatic runs. perhaps the negotiated boolean queries can be thought of as  partial manual  queries in that they are manual but hindered from a lack of interactive feedback with the documents.
1.1	impact of metadata
the  tv-tvo  entry in table 1 shows a statistically significant increase in r 1  which intuitively makes sense  more relevant items are found when the metadata is not excluded .
the per-topic differences in map were small  however  no more than 1 on any topic  as per table 1 .
   the mean difference for precision b was skewed by topic 1  for which b=1. without metadata  a relevant item shifted from the 1nd to 1st rank  changing p b from 1 to 1 for that topic.
1.1	natural language vs. keywords
the  dvo-tvo  differences of tables 1 and 1 suggest that using the natural language form of the query tended to have precision 1  but lower recall 1  than using the keyword terms from the boolean query  though neither of these mean differences were statistically significant .
   topic 1  on  pigeon deaths   may be illustrative. the synonyms for  death  in the boolean keyword list led to relatively less weight for  pigeon  in the vector query  hurting precision at the top of the list. but synonyms in the keyword list could produce higher recall  though for this topic  recall actually was still lower at 1 items retrieved .
1.1	impact of blind feedback
the  tve-tv  differences in tables 1 show that the blind feedback technique caused a statistically significant decrease in gs1j  i.e. it pushed down the first relevant item  on average   but a statistically significant increase in pj b and bpref  and nearly significant increases in map and precision b . these results are consistent with what we have seen elsewhere  1  1  1  1 . for example  in   1 other groups' blind feedback systems  of the 1 ria workshop  were studied  and it was found that blind feedback was detrimental to the first relevant item  on average   even though it boosted the traditional trec measures  such as p1  r-prec and map .
   blind feedback is known to be bad for robustness because of its tendency to  not help  and frequently hurt  the worst performing topics    hence most traditional trec measures are non-robust  while measures of the first relevant item  such as s1 and gs1  appear to be robust.
    recently made the  unsupported  claim that for gmap   blind feedback is often found to be detrimental . however  in our past official experiments with gmap    and in the ria experiments    we have seen statistically significant increases in gmap from blind feedback  but no statistically significant decreases.  in the case of our official legal track experiment  the gmap measure was also slightly increased by blind feedback  from 1 to 1 as per table 1  though this particular increase was not statistically significant.  we do not consider gmap to be a robust measure.
   an odd result is that there was a  nearly significant  decrease in r 1 from blind feedback. we suspect this may be a case where the incompleteness of the assessments is producing a misleading result.
1.1	impact of duplicate filtering
the  tvc-tv  differences in table 1 found that duplicate filtering had the opposite impact to blind feedback  increasing robust measures  such as gs1j   and decreasing traditional trec measures  such as p1  rprec and map   though these particular results were not statistically significant  intuitively these results should hold up in larger experiments .
   in legal discovery  one is obliged to turn over all responsive documents. one should not withhold items just because they are suspected of being duplicates. hence  robust  measures are inappropriate for legal discovery.
   in legal discovery  one should be consistent about what one returns  whereas robust metrics such as success 1 encourage diversity .
    duplicate filtering can be considered a special case of the incremental negative blind feedback approach derived for success 1 in . 
   intuitively  the reason traditional trec measures  such as p1  r-prec and map  are not robust is that they encourage retrieval of duplicate information  and penalize duplicate filtering .
   intuitively  a recall-oriented measure would be robust if it just counted distinct aspects of a topic  such as the  instance recall  metric described in   however  we presently have not done experiments with the table 1: precision of legal discovery run  huml1tv  at various depths
	depths	#relevant  over 1 topics 	precision  marginal 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
 instance recall  measure  and such a metric requires special assessor effort to mark the distinct aspects  which was done for 1 topics for the trec-1  1 and 1 interactive tracks  according to   .
   for most ad hoc tasks  we encourage the organizers to use a robust metric as the main measure  but legal discovery is an exception.
   intuitively  for non-feedback experiments  such as weighting experiments  or stemming  or boolean vs. vector   robust and non-robust metrics will tend to favor the same approaches. the differences between robust and non-robust metrics will be most evident for feedback experiments  robust metrics will favor negative feedback  such as duplicate filtering  and non-robust metrics will favor positive feedback  such as pseudo-relevance feedback  blind feedback  .
1.1	depth probe results
table 1 shows the  marginal  precision of the main vector run  huml1tv  at various depths  based on the first 1 rows of huml1tv  which were judged   and the first 1 rows of huml1tvz  rows 1  1  1  ...  1 of huml1tv  plus rows 1 . huml1tvz was given highest judging precedence and most of its first 1 rows were judged.
   as expected  precision tends to drop deeper in the result list  but even by depth 1  the marginal precision was still approximately 1%.
   for the 1 sample points of depths 1  x1 topics   table 1 shows there were 1 relevant items  from 1+1+1+1+1   which projects to an estmate of 1 relevant items in the first 1 retrieved  over 1 topics   or 1 relevant items per topic.  this estimate is more likely to be lower than the true number of relevant items than higher  assuming precision tends to fall deeper in the result list  as our sample point was at the end of each 1 retrieved. also  there are likely relevant items deeper than 1  and there are likely relevant items that are not matched at all by the query. 
   the actual number of judged relevant items was 1 per topic  hence apparently less than 1% of the relevant items are judged. such incompleteness does not mean that the test collection is not useful; indeed  we have been able to learn a lot from the topic analysis above. but in general  one should be cautious about interpreting the mean scores without checking the individual topics.
1.1	updated boolean runs
after the run submission deadline  a bug was found in the proximity matching of the development version used to produce the submitted boolean runs huml1t  huml1 and reference run huml1b  huml1b was the same as huml1t except that all matching rows were retrieved  not just the first 1 rows .
table 1: topics affected by the boolean update
	topic	b	incorrect	missed	b1
111111111111111111   after the fix  1% of the matches in huml1b were the same. 1 of the 1 queries were affected however  as listed in table 1 . in the original huml1b  1% of the rows listed were incorrect  and 1% of the desired matches were missing.
    the issue was just in the development stream  not in released versions . on queries that requested a phrase within x words of a term  some new optimizations caused parts of the matching code to compare character positions to word positions  leading to incorrect results. when incorrect hits were produced  typically the document still satisfied the query if the proximity constraint was relaxed to a boolean-and. 
   when it was announced that 1 of the 1 topics had been dropped during the assessing phase  we originally assumed  incorrectly  that the dropped topics were the 1 affected by the bug  but in fact  as we verified after the conference  all 1 were still included in the final 1 topics.
   table 1 lists the 1 topics affected in the huml1b and huml1t runs.  b  is the original number of matching rows for the topic.  incorrect  is the number of original matching rows that should not have matched.  missed  is the number of desired rows missed by the original huml1b run.  b1  is the number of matches in the updated boolean run.
   in every table reporting mean scores  i.e. tables 1   we have listed both the original submitted boolean runs  huml1t and huml1  and the updated versions of these runs  huml1t' and huml1' . the mean scores of the original and updated runs are very similar.
   for every measure based on the first b items retrieved  i.e. the  b measures of table 1   we have also listed the scores based on the first b1 items retrieved  in table 1 . the mean scores of the  b and  b1 measures are very similar.
   in the difference tables  tables 1   for every comparison involving the boolean runs  t and t1   such as  t-tv   we have also listed the comparison using the updated boolean runs  t' and t1'   e.g.  t'-tv . again  the results are very similar  and in particular the extreme topics are usually the same.
   furthermore  for every measure in tables 1  we have also added direct comparisons of the updated and original runs  as per the  t'-t  and  t1'-t1  entries of these tables . for instance  for the r b1 and p b1 measures  table 1   we see that just 1 topics were affected in the  t'-t  case  topics 1  1 and 1   and the largest difference was in the opposite direction to the other two differences  mostly cancelling their impact on the mean difference .
   of the topics walked through earlier  the only one affected by the update was topic 1  which was the one least affected. the original analysis for this topic remains valid.
   fortunately  it appears that whether one uses the original or updated boolean run for analysis is unlikely to have much impact on one's conclusions.
1	comparison to previous boolean studies
 reported that automatic ranking of natural language queries substantially outperformed manually produced boolean queries in an experiment  which sounds quite different from our result.  used date-ordering on the boolean set instead of relevance-ranking  which might have disadvantaged the boolean run on measures looking less than b items deep. furthermore  many of the metrics used by  looked deeper than the table 1: mean scores of submitted terabyte adhoc runs

humt1xlc111111111humt1xl111111111 humt1l 111111111humt1l111111111humt1xle111111111 humt1 111111111humt1xlz111111111median boolean result set size  e.g. the median was b=1 in the main experiment .  does not report the precision b and recall b measures  arguing that measures based on the boolean set size are potentially biased in favor of the boolean system.
    reported that manually produced boolean queries performed similarly to the top-performing automatic and manual rankings of the trec-1 topics submitted to trec-1  which sounds similar to our result.  used the precision b and recall b measures. the b values ranged from 1 to 1  whereas ours ranged from 1 to 1  excluding topics with b 1 .
    and  both produced their own boolean queries  whereas we used the negotiated boolean queries of the legal track topic set.
   we performed experiments isolating the boolean operators  i.e. vector runs based on the same keywords as the boolean query   whereas  and  did not include such an experiment. the experiments in  and  seem more analogous to comparisons of the request text and the boolean query  which did not always use the same words.
   for our main experiments  we identified and walked through the extreme topics  those with the greatest differences in each direction in various metrics  to help understand and verify the reasons for the differences.
 and  did not include such topic analysis.
1	terabyte track
for the tasks of the terabyte track  the collection to be searched was the gov1 collection  a crawl of most of the .gov domain in early 1. once binaries  such as images  were removed  its size was less than half a terabyte. the gov1 distribution was 1 1 bytes uncompressed  1gb  and consisted of 1 1 documents. more than 1% of the documents were html  1% were  extracted text from  pdf  and the rest were extracted text from other formats  plain text  msword  postscript  etc. . the average document size was 1 bytes.
   we participated in all 1 tasks of the terabyte track: adhoc  efficiency and named page finding. details on these tasks are in the track guidelines .
1	adhoc experiments
in the adhoc task of the terabyte track  there were 1 topics  each with a title  description and narrative field. the terabyte adhoc judgements contained on average 1 relevant documents per topic  low 1  high 1  median 1  counting both  relevant  and  highly relevant  as relevant.
   the techniques used for the 1 submitted runs of june 1  and 1 other diagnostic runs done at the same time :
humt1  not submitted : the searchserver contains predicate was used to perform a boolean-or
of the words of the title field of the topic.  no inflections.  humt1l: same as humt1 except that it included linguistic expansion from english inflectional stemming.
table 1: impact of terabyte adhoc techniques
expt gs1% confvs.1 extreme diffs  topic l  l-none 1  1  1 1-1.1  1   1  1    1  1 x  xl-l 1  1  1 1-1.1  1   1  1    1  1 x1  x1l-l 1  1  1 1-1 1  1    1  1   1  1 c  xlc-xl 1  1  1 1-1.1  1   1  1    1  1 e  xle-xl  1  1  1 1-1 1  1    1  1   1  1  mapx  xl-l 1  1  1 1-1.1  1   1  1    1  1 x1  x1l-l 1  1  1 1-1.1  1   1  1    1  1 l  l-none 1  1  1 1-1.1  1   1  1    1  1 e  xle-xl 1  1  1 1-1.1  1   1  1    1  1 c  xlc-xl  1  1  1 1-1 1  1    1  1   1  1 humt1xl: same as humt1l except that a small additional weight  1%  was put on matching all of
the query words within 1 characters of each other  ignoring stopwords which were not indexed .
   humt1l  not submitted : same as humt1xl except that the weight was 1-to-1 in favour of the proximity predicate instead of the boolean-or.  so this run would be likely to rank all documents which matched the proximity predicate ahead of those that did not.  humt1xle: blind feedback run based 1% on humt1xl and 1% each on expansion queries from the
first 1 rows of humt1xl. humt1xlc: same as humt1xl except that rows which appeared to be duplicates of a previous row were discarded. the heuristic filtering approach was was to discard a row if all of the passages surrounding matching terms were identical to the set of such passages of any previous row. humt1xlz  depth probe run : the first 1 rows of the submitted humt1xlz run were a one percent subset of the first 1 rows of humt1xl  rows 1  1  1  1  ...  1   and its remaining 1 rows were rows 1 of humt1xl.  the organizers requested runs to try to find  unique  relevant items.  for each run  only 1 rows were allowed to be submitted for each query.
   table 1 lists the mean scores of the adhoc runs  and table 1 isolates the differences between the runs. the results were similar to last year's ; in particular  blind feedback had the anticipated effect of boosting the non-robust map measure but decreasing the robust gs1 measure. the main new experiment this year was duplicate filtering  which modestly increased the robust gs1 measure  while causing a statistically significant decrease on the non-robust map measure.  these feedback results are consistent with what we saw in our legal track experiments. 
   table 1 shows the  marginal  precision of the humt1xl run at various depths  based on the first 1 rows of humt1xl and the first 1 rows of humt1xlz.
   if we do a similar calculation to what we described for the legal track  then we produce an estimate of 1 relevant items in the first 1 rows  from  1+1+1 *1   or 1 relevant items per topic. the actual number of judged relevant items was 1 per topic  hence apparently less than 1% of the relevant items are judged.  again  this does not mean that the test collection is not useful  just that one should be cautious. 
   table 1 has a more detailed breakdown of the number and type of items retrieved at each depth. the  relevant  items of table 1 are separated into  highly relevant  and  other relevant  in table 1; we see that highly relevant items are particularly prevalent in the first 1 ranks. the  non-relevant  items of table 1 are separated into   judged  non-relevant  and  unretrieved  in table 1  the latter occurred because in some cases the title query matched fewer than 1 rows . table 1 also includes items retrieved at depths 1  from which we can see the number of unjudged relevant items in an unpooled area of the humt1xl run  e.g. the marginal rate of unjudged items exceeds 1% at depths 1 . the 1rd column of table 1 table 1: precision of terabyte adhoc run  humt1xl  at various depths
	depths	#relevant  over 1 topics 	precision  marginal 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
	1  1  ...  1 rel  1 nonrel  1 unjudged	1	 1 
1  1  ...  1 rel  1 nonrel  1 unjudged1 1 1  1  ...  1 rel  1 nonrel  1 unjudged1 1 1  1  ...  1 rel  1 nonrel  1 unjudged1 1 1  1  ...  1 rel  1 nonrel  1 unjudged1 1 1  1  ...  1 rel  1 nonrel  1 unjudged1 1 table 1: more detailed breakdown of terabyte adhoc run  humt1xl  at various depths
	depths	breakdown of retrieved items  over 1 topics 	#h/#r
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
	1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged	1%
1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged1%1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged1%1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged1%1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged1%1  1  ...  1 highly rel  1 other rel  1 nonrel  1 unret  1 unjudged1%lists the percentage of all relevant items of the depth range which were judged highly relevant; it appears that this percentage is not a constant  but decreases with retrieval depth.
1	efficiency experiments
for the efficiency task  the 1 adhoc topics and 1 named page topics were seeded into a set of 1 queries to run. runs were due before the adhoc and named page topics were identified. we submitted two runs.
   the humte1 run was like the ad-hoc humt1 run except that it used boolean-and instead of booleanor and did not enable document length normalization. it averaged 1 seconds per query on our 1ghz machine. on the ad-hoc topics  it had an mrr of 1 and p1 of 1  and on the named-paged topics  below   it had an mrr of 1 and s1 of 1.
   the humte1 run was like the named-page humtn1pl run  below  except that the phrase matching on the title fields was omitted  though vector matching on the title fields was still included  and query terms in more than 1% of the rows were discarded. it averaged 1 seconds per query. on the ad-hoc topics  it table 1: mean scores of submitted terabyte named page finding runs
rungs1s1s1s1s1mrr humtn1dp 111111humtn1dpl111111humtn1dplc111111humtn1pl111111 humtn1rdpl 111111humtn1l111111table 1: impact of terabyte named page finding techniques

	expt	 gs1% conf	vs.	1 extreme diffs  topic 
p  pl-l 	1	  1  1 	1-1.1  1   1  1   d  dpl-pl 	1	  1  1 	1-1.1  1   1  1  
	 1	  1  1 	1-1.1  1   1  1  
c  dplc-dpl  1  1  1 1-1 1  1   1  1   1  1 r  rdpl-dpl  1  1  1 1-1.1  1    1  1    1  1 
had an mrr of 1 and p1 of 1. on the named-page topics  it had an mrr of 1 and s1 of
1.
1	named page finding experiments
for the 1 queries of the named page finding task  the goal was to find a particular named page. table 1 lists the mean scores of the 1 submitted runs and 1 other runs saved at the same time  and table 1 isolates the differences between the runs. most of the run approaches and results are similar to last year's .
   the main new experiment was the 'c' experiment  duplicate filtering heuristic : table 1 shows that it improved the result for 1 queries and only hurt 1 queries  though one of the hurt queries  topic np1  home reference definition of embryo   had a substantial drop in score that led to a negative mean difference in gs1. however  the result for np1 was because of the assessments not identifying gx1-1  http://ghr.nlm.nih.gov/ghr/glossary/embryo;jsessionid=1be1fda1f1bfee1deb1bc1  as a duplicate of the list of accepted pages  such as gx1-1
 http://ghr.nlm.nih.gov/ghr/glossary/embryo;jsessionid=f1d1c1ce1be1 
 .
1	glossary
1	retrieval measures
the ad hoc retrieval measures of tables 1 and 1 are defined as follows:
  recall 1  r 1 : for a topic  r 1 is the percentage of the relevant items retrieved in the first 1 rows.
  precision n: for a topic   precision  is the percentage of retrieved documents which are relevant.  precision n  is the precision after n documents have been retrieved. this paper lists precision 1  p1  for all ad hoc runs.
  r-precision: for a topic  r-prec is the precision at rank r  where r is the number of relevant items for the topic.
  average precision  ap : for a topic  ap is the average of the precision after each relevant item is retrieved  using zero as the precision for relevant items which are not retrieved .  in this paper  ap is based on the first 1 retrieved items for the legal runs and the first 1 retrieved items for terabyte runs.  the score ranges from 1  no relevants found  to 1  all relevants found at the top of the list .  mean average precision   map  is the mean of the average precision scores over all of the topics  i.e. all topics are weighted equally .
  binary preference: bpref is based just on the relative ranking of judged items  defined in .
  geometric map  gmap : gmap  introduced in   is based on  log average precision  which for a topic is the natural log of the max of 1 and the average precision. gmap is the exponential of the mean log average precision.
  gmap': gmap' linearly maps the 'log average precision' values to the  1  interval  putting the individual topic scores on the same scale as the other measures and allowing the arithmetic mean to be used as normal. gmap' produces the same system rankings as gmap and the same conclusions for statistical significance purposes.
  reciprocal rank  rr : for a topic  rr is where r is the rank of the first relevant item  or zero if no relevant item is retrieved.  mean reciprocal rank   mrr  is the mean of the reciprocal ranks over all the topics.
  success n  s n : for a topic  success n is 1 if a desired page is found in the first n rows  1 otherwise. this paper lists success 1  s1  and success 1  s1  for most runs.  success 1  s1  and success 1  s1  are also sometimes listed. 
  generalized success 1  gens 1 or gs1 : for a topic  gs1 is 1 r where r is the rank of the first relevant item  or zero if no relevant item is retrieved.  this measure was known as  first relevant score   frs  last year.  gs1 is considered a generalization of success 1 because it rounds to 1 for r1 and to 1 for r 1.
  generalized success 1  gens 1 or gs1 : for a topic  gs1 is 1 r where r is the rank of the first relevant item  or zero if no relevant item is retrieved.
   we attach a j suffix to the measure  e.g. gs1j  s1j  s1j  mapj  p1j  r-precj  bprefj  gmapj  mapj  r 1j  when unjudged items are omitted rather than being assumed non-relevant. note that our mapj measure is the same as the  induced ap  indap   measure of .
for all measures  the mean scores weight each included topic equally. the boolean set measures of tables 1 and 1 are defined as follows:
  b: b is the number of items matched by the main boolean run  huml1t . for the  b measures  topics with b1 are omitted  leaving just 1 topics instead of the usual 1 for legal runs .
  precision b: for a topic  precision b  p b  is the precision after b items have been retrieved.  if fewer than b items were retrieved  the number of relevant items retrieved is still divided by b. 
  pj b: for a topic  pj b is the same as p b except that the precision is just based on the judged items in the first b retrieved; if no judged items were retrieved in the first b items  then a score of 1 was assigned.
  recall b: for a topic  recall b  r b  is the percentage of the relevant items retrieved in the first b rows.
  judged b: for a topic  judged b  j b  is the percentage of the first b retrieved items that are judged  relevant or non-relevant . if fewer than b items were retrieved  we  perhaps unfortunately  divided the number of judged items by the actual number of retrieved items  not b .
  retrieved b: for a topic  retrieved b  ret b  is the number of retrieved items divided by b  capped at 1% for each topic .
  b1: b1 is the number of items matched by the updated boolean run  huml1t' . p b1  pj b1  r b1  j b1 and ret b1 are the same as p b  pj b  r b  j b and ret b  respectively  except that the first b1 retrieved items are examined instead of the first b. for the  b1 measures  topics with b11 are omitted  leaving just 1 topics .
1	difference tables
for the comparison tables  tables 1  1 and 1   the columns are as follows:
   expt  specifies the experiment  the codes of the two runs being compared are in parentheses  indicating first run minus second run .
      is the difference of the mean scores of the two runs being compared  the column heading says for which retrieval measure .
   1% conf  is an approximate 1% confidence interval for the mean difference  calculated from plus/minus twice the standard error of the mean difference . if zero is not in the interval  the result is  statistically significant   at the 1% level   i.e. the feature is unlikely to be of neutral impact  on average   though if the average difference is small  e.g.  1  it may still be too minor to be considered  significant  in the magnitude sense.
   vs.  is the number of topics on which the first run scored higher  lower and tied  respectively  compared to the second run. these numbers should always add to the number of topics.
   1 extreme diffs  topic   lists 1 of the individual topic differences  each followed by the topic number in brackets. the first difference is the largest one of any topic  based on the absolute value . the third difference is the largest difference in the other direction  so the first and third differences give the range of differences observed in this experiment . the middle difference is the largest of the remaining differences  based on the absolute value .
1	conclusions
in the legal track  we analyzed several topics in detail and found that the negotiations often led to a boolean query that was superior to a vector query of the same terms  in various retrieval metrics. however  on average  the final negotiated boolean query matched less than half of the judged relevant items.
   in both the legal and terabyte tracks  we looked at both robust and non-robust measures  and they produced the anticipated opposing conclusions about feedback approaches. robust metrics  such as gens 1  favor negative feedback techniques  such as duplicate filtering  while non-robust metrics  such as map  rprec and p1  favor positive feedback techniques  such as pseudo-relevance feedback  blind feedback . for most ad hoc tasks  we would like the organizers to use a robust metric  such as gens 1  as the main measure. but robust metrics are not appropriate for legal discovery because filtering suspected duplicates is not appropriate in this task.
   in our depth probe experiments  depth 1 in the legal track and depth 1 in the terabyte track   we demonstrated that  on average  less than 1% of the relevant items are assessed for the legal topics and less than 1% of the relevant items are assessed for the terabyte adhoc topics. relevant items were found to occur deeply in the results  implying that sampling techniques should look deeper than 1 rows.
