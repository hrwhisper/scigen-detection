xml documents have recently become ubiquitous because of their varied applicability in a number of applications. classification is an important problem in the data mining domain  but current classification methods for xml documents use ir-based methods in which each document is treated as a bag of words. such techniques ignore a significant amount of information hidden inside the documents. in this paper we discuss the problem of rule based classification of xml data by using frequent discriminatory substructures within xml documents. such a technique is more capable of finding the classification characteristics of documents. in addition  the technique can also be extended to cost sensitive classification. we show the effectiveness of the method with respect to other classifiers. we note that the methodology discussed in this paper is applicable to any kind of semi-structured data.
categories and subject descriptors
h.1  database management : data mining
keywords
xml/semi-structured data  classification  tree mining
1. introduction
the classification problem is defined as follows. we have an input data set called the training data which consists of a set of multi-attribute records along with a special variable called the class. this class variable draws its value from a discrete set of classes. the training data is used to construct a model which relates the feature variables in the training data to the class variable. the test instances for the classification problem consist of a set of records for which only

 this work was supported in part by nsf career award iis-1  doe career award de-fg1er1  and nsf grant eia-1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigkdd '1  august 1  1  washington  dc  usa copyright 1 acm 1-1/1 ...$1.
the feature values are known while the class value is unknown. the training model is used in order to predict the class variable for such test instances. the classification problem has been widely studied by the database  data mining and machine learning communities  1  1  1  1  1  1  1  1  1 . however  most such methods have been developed for general multi-dimensional records. for a particular data domain such as strings or text  1  1   classification models specific to these domains turn out to be most effective. in recent years  xml has become a popular way of storing many data sets because the semi-structured nature of xml allows the modeling of a wide variety of databases as xml documents. xml data thus forms an important data mining domain  and it is valuable to develop classification methods for such data. currently  the problem of classification on xml data has not been very well studied  in spite of its applicability to a wide variety of problems in the xml domain.
since xml documents are also text documents  a natural alternative for such cases is the use of standard information retrieval methods for classification. a simple and frequently used method for classification is the nearest neighbor classifier . this method works quite well for most text applications containing a small number of class labels. however  the use of the text format for classification ignores a significant amount of structural information in the xml documents. in many cases  the classification behavior of the xml document is hidden in the structural information available inside the document. in such cases  the use of ir based classifiers is likely to be ineffective for xml documents. a second  but more promising methodology for xml mining is to directly use association based classifiers such as cba   caep  or cmar   on the xml data. even though an xml data record has hierarchical structure  its structure can be flattened out into a set  which allows the use of an association classifier. this also results in loss of structural information  but the overall accuracy is still somewhat better than the  bag of words  approach for a text classifier. recent work has focused on the use of rule based classifiers  as an effective tool for data classification. rule based classifiers have also been extended to the string classification problem . rule based classifiers are an interesting method which integrate the problem of associations and classification. these techniques provide an effective and scalable alternative for classification  and often turn out to be highly interpretable by their nature.
in this paper  we will discuss the problem of constructing structural rules in order to perform the classification task. the training phase finds the structures which are most closely related to the class variable. in other words  the presence of a particular kind of structural pattern in an xml document is related to its likelihood of belonging to a particular class. once the training phase has been completed  we perform the testing phase in which these rules are used to perform the structural classification. we will show that the resulting system is significantly more effective than an association based classifier because of its ability to mine discriminatory structures in the data.
the main contribution of this paper is to propose xrules  a structural rule-based classifier for semi-structured data. in order to do so  we also develop xminer which mines pertinent structures for multiple classes simultaneously. we extend our classifier to the cost sensitive case  so that it can handle normal as well as skewed class distributions. we also show that our class assignment decisions are rooted in bayesian statistics.
1. structural rules: concepts
we model xml documents as ordered  labeled  rooted trees  i.e.  child order matters  and each node has a label. we do not distinguish between attributes and elements of an xml document; both are mapped to the label set.
1 trees and embedded subtrees
we denote a tree  an xml document  as t =  v b   where v is the set of labeled nodes  and b the set of branches. the label of each node is taken from a set of labels  also called items  l = {1 1 ... m}; different nodes can have the same label. each branch  b =  x y   is an ordered pair of nodes  where x is the parent of y. the size of t is the number of nodes in t.
we say that a tree s =  vs bs  is an embedded subtree of t =  v b   denoted as  provided i  vs   v   and ii  b =  x y  ¡Ê bs  if and only if x is an ancestor of y in t.
note that in the traditional definition of an induced subtree  for each branch b =  x y  ¡Ê bs  x must be a parent of y in t. embedded subtrees are thus a generalization of induced subtrees; they allow not only direct parent-child branches  but also ancestor-descendant branches. as such embedded subtrees are able to extract patterns  hidden  or embedded deep within large trees which will not be captured by the traditional definition.   we also say that t contains s. a  sub tree of size l is also called a l- sub tree.
1 cost-based classification
the classification model discussed in this paper can be used for the general case of cost-sensitive classification . in this section  we provide some definitions relevant to this topic. we assume that the training database d for classification consists of a set of |d| structures  each of which is associated with one of k class variables. let c = {c1 ...ck} be the k classes in the data. for a structure t ¡Ê d  we use the notation t.c to refer to the class associated with t. we assume that each of these structures is an xml document that can be represented in tree format.1 therefore the database d is essentially a forest with n components  so that each of the trees in the forest is labeled with a class variable. the class label of each structure in d induces a partition of the database into k disjoint parts. let

1
 tree structured xml documents are the most widely occurring in real applications. we note that even if an xml document is not a tree  it can always be converted into one by using a node splitting methodology .
di = {t ¡Ê d|t.c = ci}  i.e.  di consists of all structures with class ci. clearly .
the goal of classification is to learn a model  r : d ¡ú c  r t  = cj  where t ¡Ê d and cj ¡Ê c   that can predict the class label for an unlabeled test instance. we can find out how well the classifier performs by measuring its accuracy. let d be some collection of structures t with known labels t.c. let ¦Ç d  = |{t ¡Ê d|t.c = r t }| denote the number of correct predictions made by the model for examples in d. thus  ¦Ç di  gives the number of correct predictions for examples with class ci  and the total number of correct predictions made by r over all classes. the accuracy ¦Á of the classification model r on data set d is the ratio of correct predictions to the total number of predictions made: .
for many classifier models  the accuracy is often biased in favor of classes with higher probability of occurrence. in many real applications  the cost of predicting each class correctly is not the same  and thus it is preferable to use the notion of cost-sensitive accuracy. for each class ci  let wi denote a positive real number called weight  with the constraint that
¡¡ the cost-sensitive accuracy  denoted ¦Ács  is defined as the weighted average of the accuracy of the classifier on each class. formally  we define
		 1 
there are several cost-models that one could use to compute the classification accuracy:
  the proportional model uses wi = |di|/|d|  i.e.  weights are proportional to the probability of the class in d.
  the equal model uses wi = 1/k  i.e.  all classes are weighted equally.
  the inverse model uses  i.e.  weights are inversely proportional to the class probability.
  the custom model uses user-defined weights wi.
lemma 1. for proportional model ¦Ács r d  = ¦Á r d .
proof: 

in this paper we will contrast the inverse cost-model with the proportional and equal model. the inverse model works well for binary classification problems with skewed class distribution  since it gives a higher reward to a correct rare class prediction.
1 rule support
let d be any collections of trees with class labels drawn from c. for a tree t  we define its absolute support in d  denoted ¦Ða t d   as the number of trees in d that contain
t  i.e. 
		 1 
the  relative  support of t in d  denoted ¦Ð t d   as the fraction of trees in d that contain t  i.e. 
		 1 
t is said to be frequent in d if ¦Ð t d  ¡Ý ¦Ðmin  where ¦Ðmin is a user defined minimum support threshold. rules are defined as entities which relate the frequent structures on the left hand side to the class variables on the right. such rules are able to relate the complex structural patterns in the data to the class variable. formally  a structural rule is an entity of the form t   ci  where t is a structure  and ci is one of the k classes.
this rule implies that if t is a substructure of a given xml record x  then the record x is more likely to belong to the class ci. the  goodness  of such an implication is defined by two parameters which were refer to as support and strength. the global support of t   ci in the database d  is defined as the joint probability of t and ci  i.e.  the percentage of the trees in the database containing t and having class label ci. formally

the last step follows from equation 1. the local support of a rule t   ci is simply its relative frequency in di  given as ¦Ð t di .
1 rule strength
the strength of a structural rule can be measured by different measures; we focus on three: confidence  likelihood ratio  and weighted confidence  as defined below. confidence: the confidence of the structural rule t   ci is defined as the conditional probability of class ci given t  i.e.  the ratio of the number of trees containing t and having class label ci  to the number of trees containing t in the entire database. formally  we define


let's assume that we have k classes  k ¡Ý 1   and let ci =
c   {ci} be the set of all classes other than ci. we define to be the set of trees in d with their classes taken from. our approach for multi-class problems  with k   1  is to treat them as a binary class problem as follows: we compare each class ci with the rest of the classes taken

as a group to form a negative class ci. that is  we compare

¦Ñ t   ci  with ¦Ñ t   ci .	using the observation that

d = di + di  we can rewrite equation 1 as:
		 1 
it is clear that
likelihood ratio: the likelihood ratio for a rule t   ci is defined as the ratio of the relative support of t in examples with class ci  to the relative support of t in examples having

negative class ci. formally  it is defined as follows:
		 1 
¡¡lemma 1. likelihood ratio for a rule is related to its confidence by the formula:

proof: from equation 1  we get ¦Ða t di  = ¦Ñ t   ci ¡Á

¦Ða t d   similarly for ¦Ða t di  . plugging into equation
	a	|	|	|
	.	
t 
weighted confidence: we define another measure called the weighted confidence  which combines the above two measures  given as follows:
		 1 
we can rewrite the equation 1  as a weighted version of equation 1  as follows:

in other words  while confidence uses absolute supports  weighted confidence uses relative supports  i.e.  weighted by class probability . by next lemma  weighted confidence can also be thought of as a normalized likelihood measure.
¡¡lemma 1. the weighted confidence of a rule is related to its likelihood by the formula:
		 1 
proof: from equation plugging into equation 1  we get:

like ¦Ñ the value of ¦Ñw lies between  1   while ¦Ã can take values between  1 ¡Þ . in our experiments  we will study the effects of using one measure over the other. let ¦Ä denote the measure of strength; for confidence ¦Ä ¡Ô ¦Ñ 1  for weighted confidence ¦Ä ¡Ô ¦Ñw  and for likelihood ¦Ä ¡Ô ¦Ã.
¦Ð ¦Ä
we use the notation t   ci to denote a rule with support ¦Ð and strength ¦Ä. our goal is to learn a structural ruleset   where each rule is of the form
ri : ti ¦Ð ¦Ä  ci  with ¦Ð ¡Ý ¦Ðjmin and with ¦Ä ¡Ý ¦Ämin. that is  rules which satisfy a user-defined level of minimum support ¦Ðmin  and a global minimum strength threshold  ¦Ämin. note that ¦Ämin ¡Ô ¦Ñmin for  weighted  confidence based measure and ¦Ämin ¡Ô ¦Ãmin for likelihood based measure. we set the default minimum strength values to ¦Ñmin = 1 and ¦Ãmin =
1;
1.1 bayesian interpretation of strength

given k classes c = {c1 ... ck}  with ci = c ci. as before

di is the portion of data set d with class ci and di is the remaining data set  with class in. an unseen example t should be assigned to class ci if the probability of class ci given t  p ci|t  is the greatest over all classes  i.e.  assign t to class ci if p ci|t    p cj|t . since we compare a class

ci against the negative class ci  we assign t to class ci if
		 1 
 bayes thm.  1 

	 	p t|ci p ci    p t|ci p ci 	 1 
the three strength measures differ in which equation they use for class prediction. for instance  confidence measure

1 the notation ¡Ô denotes that the two entities are equivalent. directly uses equation 1  since by definition  equation 1   ¦Ñ t   ci  = p ci|t . thus using the confidence measure t

is assigned to class ci if ¦Ñ t   ci    ¦Ñ t   ci .
the likelihood measure uses equation 1. rearranging the terms in equation 1  we get . plugging
  and similarly for p t|ci  
we get: . by definition of likelihood  equation 1   we have . thus bayes rule
 equation 1  assigns t to class.
the likelihood measure assigns t to class ci if ¦Ã t   ci    ¦Ãmin. if we use the default value of ¦Ãmin = 1  this corresponds to ignoring the ratio of class prior probabilities  i.e.  setting the ratio  in general  for proportional or equal cost model  it makes logical sense to use the class priors  since in the absence of any information  we should predict the class of t to be the class with higher prior. however  if ci is rare  inverse cost model   then it is better to ignore the prior  since the prior ratio is biased in favor of the class with higher probability. by setting the prior ratio to 1  we set all classes on an equal footing. finally  the weighted confidence measure uses equation 1  consider its lhs :
p t p t|ci p ci  + p t|ci p ci 1lhs	=	p t|ci p ci  =	p t|ci p ci 
	=	=
	1 + pp  tt||ccii  pp  ccii  	1 + ¦Ð¦Ð  t t ddii   ¡Á pp  ccii  
setting  we get :

once again  ignoring class priors ratio  i.e  setting 
1   we obtain the definition of weighted confidence in equation 1. thus lhs of equation 1 corresponds to ¦Ñw t   ci   and by bayes rules we assign t to class ci if ¦Ñw t   ci    ¦Ñw
as described above  confidence measures strength across the entire database d. on the other hand  likelihood measures the local tendency of the pattern to be associated with the target class; it compares the local support of the rule for the target class  ci  with its local support for the negative

class ci  the rest of the classes . in skewed data sets  with uneven class distributions  confidence is biased in favor of the dominant class  since globally the patterns associated with this class will have higher absolute supports compared to the minority class. likelihood and weighted confidence do not have this bias  since they ignore class priors and use local relative supports.
1. xrules: structural rule-based classification
the classification task contains two phases. the training phase uses a database of structures with known classes to build a classification model; in our case a set of structural classification rules  called a rule-set. the testing phase takes as input a database of structures with unknown classes  and the goal is to use the classification model to predict their classes.
1 training phase
at the beginning of classification we have a database d = k
   i=1 di with known classes; di is the set of structures with class ci. our goal is to learn a structural rule-set r =
{r1 r1 ¡¤¡¤¡¤  rm}  where each rule is of the form ri : ti ¦Ð ¦Ä  cij  with ¦Ð ¡Ý ¦Ðjmin and with ¦Ä ¡Ý ¦Ämin. there are three main steps in the training phase:
  mining frequent structural rules specific to each class  with sufficient support and strength. in this step  we find frequent structural patterns for each class and then generate those rules which satisfy a user-defined level of minimum support for a class ci  ¦Ðimin   and a global minimum strength threshold  ¦Ämin.
  ordering the rules according to a precedence relation. once a set of classification rules have a been generated  a procedure is required to prioritize the rule set in decreasing level of precedence and to prune out unpredictive rules.
  determining a special class called default-class. since a classifier must predict a class for all possible test cases  we need to choose a default class which will be the label of a test example  if none of the rules can be used to predict a label.
1.1 mining structural rules
the first step is accomplished via an efficient structural-rule mining algorithm  xminer  which we will discuss in detail in section 1. for the moment let us assume that xminer can be used to find all structural rules related to any class. xminer accepts as input a list of minimum support thresholds for each class  i.e.  ¦Ðjmin  j = 1 ¡¤¡¤¡¤k. xminer outputs a set of frequent rules for each class   with mj rules  each rule having cj as the consequent  i.e.  ri : ti  ¦Ð cj and ¦Ð ¡Ý ¦Ðjmin.
1.1 pruning and ordering rules
since we want only predictive rules  we need to remove any rule that lacks predictive power. consider a rule  t   ci  ¡Ê ri. if its  weighted  confidence ¦Ñ = ¦Ñw = 1 or if its likelihood ratio ¦Ã = 1  then t cannot distinguish between

the class ci and its negative class ci  and we prune such a rule from ri. in general  the acceptable range of values for a user-defined minimum confidence threshold is ¦Ñmin ¡Ê
¡¡¡¡1   while the acceptable range for minimum likelihood is ¦Ã ¡Ê  1 ¡Þ .
the goal of precedence ordering is to derive the final combined rule-set r from the rule-set of each class based on a precedence relation    which imposes a total order on r  using a method analogous to that proposed in cba .
given any two rules ri : ti ¦Ð i ¦Äi ci and rj : tj ¦Ð j ¦Äj cj  we say that ri precedes rj  denoted i   if the following conditions are met:
1. the strength of ri is greater than that of rj  i.e.  ¦Äi   ¦Äj.
1. ¦Äi = ¦Äj  but the support of ri is greater than that of rj  i.e.  ¦Ði   ¦Ðj.
1. ¦Äi = ¦Äj and ¦Ði = ¦Ðj  but ri contains a smaller number of nodes than rj  i.e.  |ti|   |tj|.
1. if none of the above is true  then ti occurs lexicographically before tj. we note that the lexicographic ordering of tree structures is based on a pre-order traversal of the nodes in the tree.
for precedence ordering  we sort the rules across all classes using  to derive the final ordered rule-set .
in the testing phase  the ordered rules are used in various ways to predict the target class for a new structure with unknown class.
1.1 determining default class
a rule t   ci is said to match a given tree s  when its antecedent  t  is a substructure of . a rule set r is said to cover an example tree s  if at least one rule matches s. in general  a rule set may not necessarily cover all examples  even in the training set d . since a classifier must provide coverage for all possible cases  we need to define a default label  denoted default-class  which will be chosen to be the label of a test example  if none of the rules match it.
let i i i   be the set of examples from the training set d which are not covered by the ordered rule-set r. let  i = {s ¡Ê  |s.c = ci} be the set of uncovered training examples with class ci. a simple way to choose the default-class is to pick the majority class in    i.e.  default-class = arg maxci{| i|}. if   =    then pick default-class to be the majority class in d. the problem with this method is that it does not take into consideration the real cost of the classes  it uses the proportional cost model by default . the approach we adopt is to choose the class that maximizes the cost-sensitive accuracy of the resulting rule-based classifier. let ¦Ç d  denote the number of correct predictions for data set d using our rule-set r. if   1=    then the default class is given as default-class = arg   see lemma below . if
  =    then the default class is the one with maximum weight wi  obtained by setting  i = di . it is clear that such a technique is superior from the perspective of a costsensitive approach.
¡¡lemma 1. the cost-sensitive accuracy is maximized for default-class = argmax.
proof: assume   1=  . the base accuracy for a given class ci in di is given as. by equation 1 the overall base cost-sensitive accuracy is given as

assume that we pick class cj as the default class. this affects only the accuracy of class cj due to the addition of correct predictions for class cj in    whereas the accuracy of all ci 1= cj remains unchanged. therefore  we have ¦Á r dj  =
¦Ç dj +| j|
|dj|	. the new overall accuracy is then given by

after simplifying  we get
since ¦Ácsold r d  remains the same no matter which class we pick as default  the overall accuracy is maximized for the class yielding the maximum value of w|dj| j|j|.
if   =    we set  j = dj. so the class yielding maximum accuracy is the one with maximum wj. 
¡¡corollary 1. for the proportional cost model  the accuracy is maximized if the default class is the majority class in    or in d if   =   .
proof: assume   1=  . substituting  the term to be maximized  we get ||d| i|. this is maximized for the class with the maximum value of | i|  i.e.  the majority class in  . if   =    then setting  i = di gives the desired result. 
as described above we prune all unpredictive rules having ¦Ñmin = 1 or ¦Ãmin = 1. also recall that when building a model we always compare the confidence of the rule on

class ci versus its negative class ci. in some cases  the rules may be poorly related to an example. this happens when the average  weighted  confidence or likelihood of the rules which are matched by a given example are close to 1 or 1  respectively  for a given class ci. this means that the rule is equally predictive of ci as well as thus not suitable for classification. if the user sets ¦Ñmin   1 or ¦Ãmin   1 any example with matching rules having average  weighted  confidence in the range  1   ¦Ñmin ¦Ñmin  or having average likelihood is in the range  1/¦Ãmin ¦Ãmin   is assumed to be an ambiguous case  which cannot be accurately classified. such ambiguous vases are added to the default set    essentially treating them as examples having no matching rule in r   which is used for the final determination of default-class as described above.
1 testing phase
at the end of training our classification model is complete. it consists of an ordered collection of predictive rules r  and a default-class. the testing phase takes as input the classification model  and a data set d1 of examples with unknown classes. the goal of testing phase is to predict the class for each test example. there are two main steps in testing:
  rule retrieval: find all matching rules for an example for a test example.
  class prediction: combine the statistics from each matching rule to predict the most likely class for the test example.
the rule retrieval step is simple; for each test example s in the database d1  we find the set of all matching rules  called the matching rule-set .
for predicting the class of s ¡Ê d1  we can use several different approaches for combining the statistics of the matching rule-set r s . there are two cases to be considered: first  if r s  =    when there are no matching rules. in this case  the class is predicted to be the default class  i.e.  s.c = default-class. on the other hand  if r s  1=    then let |r s | = r. also let ri s  denote the matching rules in r s  with class ci as the consequent  and let |ri s | = ri. each rule in ri s  is of the form  with ¦Äj ¡Ý ¦Ämin.
any matching rule tk ¡Ê r s    ri s  is more predictive of a class other than ci. however  xminer finds the support of tk all classes  see section 1   so we can compute the strength of tk for the negative class the strength of tk for ci  i.e.  the rule  is given as ¦Äk = 1   ¦Än if ¦Ä ¡Ô ¦Ñ  or ¦Ä ¡Ô ¦Ñw   and as ¦Äk = 1/¦Än if ¦Ä ¡Ô ¦Ã  by equations 1  1  1 . thus  for each class ci we can find the strength of each structural rule for that class. a matching rule with ¦Ñ   1 or ¦Ã   1 corresponds to a rule with positive predictive power for ci  while a matching rule with ¦Ñ   1 or ¦Ã   1 is more predictive of the negative class  and thus has negative predictive power for ci. there are several possible methods for combining evidence:
  average strength: compute the average rule strength for each class ci given as then we classify s as having class ci. if ¦Äi¦Ì has default ¦Ämin values  1 for ¦Ñmin and 1 for ¦Ãmin  for all classes  it means that the test instance cannot be easily predicted using the rules  and the class is assigned as the default class. the approach can be generalized to the case where ¦Äi¦Ì is ambiguous  i.e.  when ¦Ì	min min
¦Ñi ¡Ê  1   ¦Ñ  ¦Ñ   for  weighted  confidence  and when ¦Ãi¦Ì ¡Ê  1/¦Ãmin ¦Ãmin  for likelihood. in such a case  we assign s.c to be the default class.
  best rule: find the first rule that matches s  i.e.  the first rule in r s . since the rule set is ordered according to precedence   the first rule t   ci ¡Ê r s  is the best or most predictive  by nature of the total order   a matching rule after this one will either have less strength  or less support or will be more specific . we thus predict s.c = ci.
  best k-rules apply average strength method for the first k rules in r s . this is a simple generalization of the case discussed above.
in our experiments we used the average confidence method for combining evidence  since it gave us the best results. we note that for average strength-based methods  if the classification behavior of a test instance is ambiguous  equal to or close to default ¦Ämin values   the classifier can also output this fact as useful information to the end user. while classifiers traditionally strive for 1% coverage  i.e.  they predict a label of each test case   a practical application may often benefit greatly from knowledge of the fact that certain test instances are harder to classify than others. this results in lower coverage  but a better understanding of the overall classification process.
1. xminer
in order to determine the set of rules  xrules first needs to mine the frequent subtrees in the data. several recent methods for tree mining have been proposed  such as freqt   treeminer   and treefinder . freqt is based on an apriori-style  level-wise  candidate generation and pattern matching based counting approach. a similar approach is described in . treefinder uses an inductive logic programming approach  and it is not a complete method  i.e  it can miss many frequent subtrees  especially as support is lowered or when the different trees in the database have common node labels. treeminer uses a novel vertical representation for fast subtree support counting. it is a complete method  and outperforms a level-wise method similar to freqt. we thus chose treeminer as a basis for xminer. given a dataset d with k classes  and thus k partitions di  one approach to mining structural rules would be to mine each di separately using treeminer  and then to combine the results. there are two problems with this approach: 1  xrules needs to know the support of a tree t in each class  but t may be frequent in one class di  but not in another dj. 1  we would need one extra scan to count such missing class supports  thus this approach is inefficient. xminer extends treeminer to find all frequent trees related to some class  and also incorporates multiple minimum support criteria  one per class. this ensures that any tree generated is suitable for classification purposes. like treeminer  xminer utilizes the vertical tree representation for fast support counting and uses a depth-first  dfs  pattern search.
1 node number  scope  and match label
let x be a k-subtree of a tree t. let xk refer to the last node of x. each node in t has a well-defined number  i  according to its position in a depth-first  or pre-order  traversal of the tree. we use the notation ni to refer to the ith node according to this numbering scheme  i = 1...|t|   1 . let t nl  refer to the subtree rooted at node nl  and let nr be the right-most leaf node in t nl . the scope of node nl is given as the interval  l r   i.e.  the lower bound is the position  l  of node nl  and the upper bound is the position  r  of node nr. figure 1 shows a database of 1 trees  with 1 classes; for each tree it shows the node number ni  node scope  l u   and node label  inside the circle .

figure 1: tree mining example
let d denote a database of trees  i.e.  a forest   and let subtree for some t ¡Ê d. each occurrence of s can be identified by its match label  which is given as the set of matching positions  in t  for nodes in s. more formally  let {t1 t1 ... tn} be the nodes in t  with |t| = n  and let {s1 s1 ... sm} be the nodes in s  with |s| = m. then s has a match label {ti1 ti1 ...tim} if and only if: 1  l sk  = l tik  for all k = 1 ...m  where l n  is the label for node n  and 1  branch b sj sk  ¡Ê s iff tij is an ancestor of tik in t. condition 1  indicates that all node labels in s have a match in t  while 1  indicates that the tree topology of the matching nodes in t is the same as s. a match label is unique for each occurrence of s in t.
1 prefix group and scope lists
we say that two k-subtrees x y are in a prefix equivalence group iff they share a common prefix up to the  k   1 th node. let p be prefix subtree of size k   1. we use the notation  p k 1 to refer to its group  which contain all the last items  k-th node  of trees that share p as their prefix. we use the notation l x  to refer to the scope-list of x. each element of the scope-list is a triple  t s m   where t is a tree id  tid  in which x occurs  s is the scope of xk  and m is a match label for x. since a subtree can occur multiple times in a tree  each tid can be associated with multiple scopes and match labels.
the initial scope-lists are created for single items i that occur in a tree t. let  l u  be the scope of a node with label i. since the match label of item i is simply l we omit storing m when dealing with the scope-lists of single items. we will show below how to compute pattern frequency via joins on scope-lists. figure 1 shows the scope lists for the frequent single items  the minimum support is 1% for both classes . item 1 is not shown  since it is not frequent for any class; it has support 1% in class c1.
1 tree mining
figure 1 shows the high level structure of xminer. the main steps include the computation of the frequent items and the enumeration of all other frequent subtrees via dfs search within each group. xminer also maintains a global class index showing the class for each tree in the database. this index is used to quickly update the per class support for a candidate tree to check if it is frequent in any class. figure 1 shows the class index for the example database.
xminer  d  ¦Ðimin i = 1¡¤¡¤¡¤k :
    p 1 = { frequent 1-subtrees for any class}; enumerate-xrules  p 1 ;
enumerate-xrules  p  :
for all elements x ¡Ê  p  do
 px  =  ;
for all elements y ¡Ê  p  do r = x y;
l r  = l x  ¡É  l y ;
if for any r ¡Ê r  r is frequent for any class then  px  =  px  ¡È {r};
enumerate-xrules  px  ;figure 1: xminer: tree mining for classification
the input to enumerate-xrules is a set of elements of a group  p   along with their scope-lists. frequent subtrees are generated by joining the scope-lists of all pairs of elements  including self-joins . before joining the scope-lists a pruning step can be inserted to ensure that all subtrees of the resulting tree are frequent. if this is true  then we can go ahead with the scope-list join  otherwise we can avoid the join. the collection of candidate subtrees is obtained by extending each tree in a group by adding one more item  the last item  from another tree in the same prefix group. we use r to denote the possible candidate subtrees that may result from extending tree with last node x  with the tree with last item y  denoted x y   and we use l r  to denote their respective scope-lists.
the subtrees found to be frequent at the current level form the elements of groups for the next level. this recursive process is repeated until all frequent subtrees have been enumerated. in terms of memory management it is easy to see that we need memory to store intermediate scope-lists for two groups  i.e.  the current group  p   and a new candidate group  px .
1 scope-list joins  l x  ¡É  l y  
we now describe how we perform the scope-list joins for any two subtrees in a group  p . let sz =  lz uz  denote the scope for a node z. we say the sx is strictly less than sy  denoted sx   sy  if and only if ux   ly. we say that sx contains sy  denoted sx   sy  if and only if lx  = ly and ux  = uy. when we join last elements x y in a group  there can be at most two possible outcomes  i.e.  we either add y as a child of x or as a sibling of x to the class  px . to check if the subtree  obtained when y is added as a child of x  occurs in an input tree t with tid t  it is sufficient to search if there exists triples  ty sy my  ¡Ê l y  and  tx sx mx  ¡Ê l x   such that: i  ty = tx = t  ii  sy   sx  and iii  my = mx.
in other words  we check 1  if x and y both occur in the same tree t with tid t  1  if y is within the scope of x  and 1  that x and y are both extensions of the same prefix subtree   whose match label is mx = my. if the three conditions are satisfied  we add the triple  ty sy {my ¡È lx}  to the scope-list of y in  px . we refer to this case as an in-scope test.
the second pattern checks what happens when y is added as a  embedded  sibling of x. this happens when both x and y are descendants of node at position j in the prefix p  and the scope of x is strictly less than the scope of y. to check if y occurs as an embedded sibling in t with tid t  we need to check if there exists triples  ty sy my  ¡Ê l y  and
 tx sx mx  ¡Ê l x   such that: i  ty = tx = t  ii  sx   sy  and iii  my = mx.
if the three conditions are satisfied  we add the triple  ty sy {my ¡È lx}  to the scope-list of y in  px . we refer to this case as an out-scope test.
figure 1 shows the process of scope-list joins for both inscope and out-scope tests. to check if a new candidate is frequent  one can derive a per class count using the class index. for example  consider the tree in prefix group   with the branch  1 . it appears in tids 1  and 1  we count only once per tid . using the class index we find that is occurs in classes c1  c1  c1 respectively. its support for class c1 is 1 and for class c1 is 1. it is thus 1% frequent locally in both classes.
1. empirical results
we compared our xrules structural classification approach for xml documents against an ir classifier  as well as the cba classifier. for the ir classifier  irc  centroids for each class were constructed using a clustering process . then  a nearest neighbor classifier was implemented on these sets of clusters. the cba implementation was provided to us by its authors .
1 data sets
we evaluate our approach on both real and synthetic classification data sets. the advantage of using synthetic data sets was the additional flexibility in studying the effects of different kinds of embedded patterns and database size. on the other hand  the real data sets help to validate the approach in a practical setting.
1.1 real datasets
we use the log markup language  logml    to describe log reports at the cs department website. logml provides a xml vocabulary to structurally express the contents of the log file information in a compact manner. each user session is expressed in logml as a graph  and includes both structure and content.
the real cslog data set spans 1 weeks worth of such xml user-sessions. to convert this into a classification data set we chose to categorize each user-session into one of two class labels: edu corresponds to users from an  edu  domain  while other class corresponds to all users visiting the cs department from any other domain. as shown in table 1  we separate each week's logs into a different data set  cslogx  where x stands for the week; cslog1 is the combined data for weeks 1 and 1 . notice that the edu class has much lower frequency rate than other. our goal is to minimize the cost of classification inaccuracy based on the various models. we use the notation cslogx y to denote that we trained on cslogx and tested on cslogy. for example  cslog1 means that we learned a model from cslog1 and tested how well we could predict cslog1.
1.1 synthetic datasets
we constructed a synthetic data generation program simulating website browsing behavior. we first construct a master website browsing tree w based on parameters supplied by the user. these parameters include the maximum fanout f of a node  the maximum depth d of the tree  the total number of nodes m in the tree  and the number of node labels l. for each node in master tree w  we assign probabilities of following its children nodes  including the option of backtracking to its parent  such that sum of all the probabilities is 1. using the master tree  one can generate a subtree  by randomly picking a subtree of w as the root of ti and then recursively picking children of the current node according to the probability of following that link. to create a classification data set we group users into two classes  c1 and c1. first we generate a small pool of signature trees for class c1  denoted tp. second  we generate a larger collection of trees  denoted td. subset of trees from tp are selected as training and testing pools  and td is also split into training and testing sets. if a tree t ¡Ê td contains a tree from the signature pool then t has class c1  otherwise it has class c1. to control the effects of structure in the classification process  a fraction fc  called confusion ratio  of trees that belong to one class  c1  are added to other class  c1   after flattening out. this is called one-way addition. if we also allow members of c1 to be added to c1  it is called a two-way addition.
the different synthetic data sets generated are shown in table 1. for the dsx data sets  we trained on dsx-train and tested on dsx-test. the master tree w used the values d = 1 f = 1 m = 1 l = 1. we next generated
|td| = 1 trees for the database and |tp| = 1 trees for the pool. td was split into training and test sets by using table 1: characteristics of datasets
db#sessionseduother%edu%othercslog1111cslog1111cslog1111cslog1111dbtotalc1c1%c1%c1ds1.train11.1.1ds1.train11.1.1ds1.train11.1.1ds1.train11.1.1ds1.train11.1.1ds1.test11.1.1ds1.test11.1.1ds1.test11.1.1ds1.test11.1.1ds1.test11.1.1a 1   1 split. for ds1  the training and testing pool were both of size 1  with half the trees common to both. we set fc = 1  with one-way addition from c1 to c1. for ds1  the training and testing pool were identical  of size 1   and fc = 1 from c1 to c1. ds1 is the same as ds1  with with two-way confusion. finally ds1 is same as ds1  but with two-way addition only half the time  fc = 1 . the small data set ds1 was produced by a different synthetic xml document generator 1.
1 comparative classification results
the irc approach uses the actual text of the data in order to perform the classification. therefore  it uses a greater amount of information than a purely structural classifier like xrules. irc uses both the node content and edge information from the user-sessions. in contrast  xrules uses only the structure  tree-format  for the classification process. cba uses the associations among different nodes visited in a session in order to perform the classification. table 1 shows the weighted accuracy results for the three classifiers on different data sets. the table shows the accuracy for all three cost models. the best accuracy is highlighted in bold. we can see that for all data sets and all cost models  xrules is the best classifier. for the cslog data sets  xrules delivers an accuracy between 1% and 1% for the proportional model compared to irc's accuracy from 1% to 1% and cba's accuracy between 1% to 1%. thus  the accuracy of xrules is about 1% higher  in absolute accuracy  than that of irc and 1% higher than that of cba for the traditional proportional model. for this model  cba appears to be a better classifier than irc. however  the model that cba learns generally has only one rule. this rule always predicts a test case to be other. while this strategy pays off in the proportional cost model  since other is the majority class with 1% occurrence   it does not work for the equal model  1% accuracy  and fails completely for the inverse cost model  1% accuracy . irc does a much better job than cba in distinguishing one class from the other. for example consider the confusion matrix for cslog1 shown in table 1  which shows the number of test cases  by class  that were correctly and incorrectly classified by the three classifiers  with proportional cost-model . cba essentially labels each test case as other  thus it is ineffective for any

1
provided by elio masciari  personal communication 
table 1: accuracy results
dbclassifieraccuracy  % proportionalequalinversecslog1xrules111irc111cba111cslog1xrules111irc111cba111cslog1xrules111irc111cba111cslog1xrules111irc111cba111ds1xrules111cba111ds1xrules111cba111ds1xrules111cba111ds1xrules111cba111ds1xrules111cba111table 1: confusion matrix  cslog1 
predicted classxrulesirccbaeduothereduothereduotheredu111other111cost-model other than the proportional one.
for the equal and inverse cost models  we find that xrules has higher accuracy than cba and irc since it explicitly incorporates cost. in the case of the cslog data sets  the accuracy of xrules is about 1% higher than that of irc and 1% higher than that of cba for the equal cost model. the situation is more pronounced for inverse model  where the accuracy of xrules is 1% higher than that of irc and 1% higher than cba!
on synthetic data sets  which do not have content  only structure   the ir classifier does not work. so we compared only xrules and cba. the results are shown in table 1. once again  we found that cba degenerated into a default classifier most of the time  labeling each test case with the majority class  though it did have a small number of rules  less than 1  relating to the two classes. as we can see for proportional cost model on ds1  ds1  and ds1  cba fails to classify the test cases correctly  delivering an accuracy of only 1%  whereas the accuracy of xrules is 1% higher. on ds1 and ds1 cba has some discrimination power  but the accuracy of xrules is still 1% higher. for the equal and inverse model  xrules outperforms cba by up to 1%!
in summary  xrules gives consistently better performance than the other classifiers for all cost models and data sets. it works better than an associative classification approach like cba  which flattens out the structure into a set representation. it outperforms an ir based classifier which explicitly learns over the content  but only implicitly over the structural information in the xml documents. therefore  the improved results of our structural classification process are especially significant.
1 efficiency results
table 1 shows the number of frequent patterns  rules  mined by xminer  and time for training and testing. the results underscore the high efficiency of that xminer  xm  engine. the frequent trees for classification are determined in less than 1 seconds. the total training and testing time are comparable  since in both cases we have to find the matching rules for each example. this is needed to determine the default class in training  and to find the accuracy in testing. the running time can be improved by storing the rules in an appropriate index structure; currently xrules performs a linear search for matching rules.
table 1: number of rules and time
dbsuprulestrain time  s testing time  s xmtotalcslog11%1.1.1.1cslog11%1.1.1.1cslog11%1.1.1.1cslog11%1.1.1.1ds1.1%1.1.1.1ds1.1%1.1.1.1ds1.1%1.1.1.1ds1.1%1.1.1.1ds1%1.1.1.1.1 choice of rule strength
we next study how the choice of strength measure affects the accuracy of xrules  as shown in table 1. the best results are in bold. for the proportional model  confidence performs better than both likelihood and weighted confidence. its accuracy is typically 1% higher on cslog and as much as 1% higher on dsx data sets. this is in agreement based on the bayesian interpretation in section 1.1. on the other hand  with the exception of ds1 and ds1  likelihood and weighted confidence perform better than confidence with equal cost model. the weighted confidence has a slight  if insignificant  edge over likelihood  for both proportional and equal costs .
the likelihood measure has a slight edge over weighted confidence for the inverse cost model on cslog data sets. these results are in agreement with the discussion in section 1.1. the only exceptions are ds1 and ds1 where confidence does better. the reason is that in these data sets the confusion factor complicates the decision making  since one-way  twoway  addition adds patterns from one class to the other  and vice-versa . on ds1  all measures give the same result. in summary  we conclude that confidence is a better measure for proportional model and either likelihood or weighted confidence is better for equal or inverse costs. the right choice of strength measure depends on the data set characteristics and cost model. if we expect many patterns with similar global supports but different local supports of rare classes  the likelihood/weighted confidence measure will usually provide better results.
1 effect of minimum strength
table 1 shows the effect of varying the minimum likelihood ¦Ãmin on the accuracy of prediction for cslog1. best accuracy for each cost model is in bold. for proportional cost model  the accuracy tends to increase up to a point  1% for ¦Ãmin = 1  and then starts to drop. the same effect table 1: effect of strength measure
dbstrengthproportionalequalinversecslog1¦Ã111¦Ñ111¦Ñw111cslog1¦Ã111¦Ñ111¦Ñw111cslog1¦Ã111¦Ñ111¦Ñw111cslog1¦Ã111¦Ñ111¦Ñw111ds1¦Ã111¦Ñ111¦Ñw111ds1¦Ã111¦Ñ111¦Ñw111ds1¦Ã111¦Ñ111¦Ñw111ds1¦Ã111¦Ñ111¦Ñw111ds1¦Ã111¦Ñ111¦Ñw111is observed for inverse model  but the model continues to improve until ¦Ãmin = 1. for the equal cost model  the accuracy tails off at the very beginning. similar results were obtained for other strength measures. these results suggest that by choosing an appropriate ¦Ãmin one can get a model that can behave like ¦Ñ for the proportional model  e.g.  at ¦Ãmin = 1  we get 1% accuracy compared to 1% accuracy using confidence  in table 1   and can improve the accuracy for the inverse model.
table 1: effect of likelihood ratio  cslog1 
¦Ãminproportionalequalinverse#rulestime1.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111.1.1.111. conclusions and summary
in this paper  we discussed an effective rule based classifier for xml data called xrules. the technique mines frequent structures from the data in order to create the classification rules. xrules is cost-sensitive and uses bayesian rule based class decision making. methods for effective rule prioritization and testing were also proposed in this paper. the technique was implemented and compared against cba as well as an ir classifier. since the technique performs better than the cba classifier  this indicates that the system relies on the classification information hidden in the structures for an effective rule generation process. furthermore  it outperforms the ir based method in spite of the greater amount of input used by the latter. the results show that structural mining can provide new insights into the process of xml classification.
