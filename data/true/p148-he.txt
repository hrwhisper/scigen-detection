to enable information integration  schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sources. while complex matchings are common  because of their far more complex search space  most existing techniques focus on simple 1 matchings. to tackle this challenge  this paper takes a conceptually novel approach by viewing schema matching as correlation mining  for our task of matching web query interfaces to integrate the myriad databases on the internet. on this  deep web   query interfaces generally form complex matchings between attribute groups  e.g.  {author} corresponds to {first name  last name} in the books domain . we observe that the cooccurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes  e.g.  {first name  last name}  tend to be co-present in query interfaces and thus positively correlated. in contrast  synonym attributes are negatively correlated because they rarely co-occur. this insight enables us to discover complex matchings by a correlation mining approach. in particular  we develop the dcm framework  which consists of data preparation  dual mining of positive and negative correlations  and finally matching selection. unlike previous correlation mining algorithms  which mainly focus on finding strong positive correlations  our algorithm cares both positive and negative correlations  especially the subtlety of negative correlations  due to its special importance in schema matching. this leads to the introduction of a new correlation measure  h-measure  distinct from those proposed in previous work. we evaluate our approach extensively and the results show good accuracy for discovering complex matchings.
categories and subject descriptors
h.1  database management : heterogeneous databases; h.1  database management : database applications-data mining
general terms
algorithms  measurement

 this material is based upon work partially supported by nsf grants iis-1 and iis-1. any opinions  findings  and conclusions or recommendations expressed in this publication are those of the author s  and do not necessarily reflect the views of the funding agencies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
keywords
data integration  deep web  schema matching  correlation mining  correlation measure
1. introduction
　in recent years  we have witnessed the rapid growth of databases on the web  or the so-called  deep web.  a july 1 survey  estimated that 1  search cites  and 1 billion content pages in this deep web. our recent study  in december 1 estimated between 1 to 1 deep web sources. with the virtually unlimited amount of information sources  the deep web is clearly an important frontier for data integration.
　schema matching is fundamental for supporting query mediation across deep web sources. on the deep web  numerous online databases provide dynamic query-based data access through their query interfaces  instead of static url links. each query interface accepts queries over its query schemas  e.g.  author  title  subject  ... for amazon.com . schema matching  i.e.  discovering semantic correspondences of attributes  across web interfaces is essential for mediating queries across deep web sources.
　in particular  matching web interfaces in the same domain  e.g.  books  airfares   the focus of this paper  is an important problem with broad applications. in particular  we often need to search over alternative sources in the same domain such as purchasing a book  or flight ticket  across many online book  or airline  sources. given a set of web interfaces in the same domain  this paper solves the problem of discovering matchings among those interfaces. we notice that our input  a set of web pages with interfaces in the same domain  can be either manually  or automatically  1  1  collected and classified.
　on the  deep web   query schemas generally form complex matchings between attribute groups. in contrast to simple 1 matching  complex matching matches a set of m attributes to another set of n attributes  which is thus also called m:n matching. we observe that  in query interfaces  complex matchings do exist and are actually quite frequent. for instance  in books domain  author is a synonym of the grouping of last name and first name  i.e.  {author} = {first name  last name}; in airfares domain  {passengers} = {adults  seniors  children  infants}. hence  discovering complex matchings is critical to integrate the deep web.
　although 1 matching has got great attention  1  1  1  1   m:n matching has not been extensively studied  mainly due to the much more complex search space of exploring all possible combinations of attributes  as section 1 will discuss . to tackle this challenge  we investigate the co-occurrence patterns of attributes across sources  to match schemas holistically. unlike most schema matching work which matches two schemas at a time  we match all the schemas at the same time. this holistic matching provides the co-occurrence information of attributes across schemas and thus

	 a  amazon.com	 b  www.randomhouse.com

	 c  bn.com	 d  1bookstreet.com
figure 1: examples of  fragment  web interfaces.
enables efficient mining-based solutions. for instance  we may observe that last name and first name often co-occur in schemas  while they together rarely co-occur with author  as figure 1 illustrates. more generally  we observe that grouping attributes  i.e.  attributes in one group of a matching e.g.  {last name  first name}  tend to be co-present and thus positively correlated across sources. in contrast  synonym attributes  i.e.  attribute groups in a matching  are negatively correlated because they rarely co-occur in schemas.
　these dual observations motivate us to develop a correlation mining abstraction of the schema matching problem. specifically  given web pages containing query interfaces  this paper develops a streamlined dcm framework for mining complex matchings  consisting of automatic data preparation and correlation mining  as figure 1 shows. since the query schemas in web interfaces are not readily minable in html format  as preprocessing  the data preparation step prepares  schema transactions  for mining  section 1 . then the correlation mining step  the main focus of this paper  discovers complex matchings with dual mining of positive and negative correlations  section 1 . we name the whole matching process as dcm  since the core of the algorithm is the dual correlation mining part.
　unlike previous correlation mining algorithms  which mainly focus on finding strong positive correlations  our algorithm cares both positive and negative correlations. hence  we need to develop measures for both positive correlations and negative ones. our schema matching task is particularly interested in negative correlations  since on one hand  they reflect the synonym relationships among attributes  on the other hand  they have not been extensively explored and applied before.
　to ensure the quality of the mining result  i.e.  the complex matchings   the chosen measures should satisfy some quality requirements  based on our observation of query schemas  section 1 . in particular  from the extremely non-uniform distribution of schema attributes  we identify that: 1  both the positive and negative correlation measures should be robust for the sparseness problem  i.e.  the sparseness of schema data may  exaggerate  the effect of coabsence   which has also been noticed as the  null invariance  property by recent correlation mining work  1  1  1 . 1  the negative correlation measure should be robust for the rare attribute problem  i.e.  the rare attributes may not be convincing to judge their negative correlations . since none of the existing measures  1  1  is robust for both the sparseness problem and the rare attribute problem  we develop a new measure  h-measure  robust against both problems in measuring negative correlations.
　to evaluate the matching performance and h-measure  we test the dcm framework on the datasets in the uiuc web integration repository . first  we test dcm on the tel-1 dataset  which contains raw web pages over 1 deep web sources in 1 popular domains  and the result shows good target accuracy. second  we compare the dcm framework with the mgs framework   which also matches web interfaces with the same insight of exploring a holistic approach  on its bamm dataset. the result shows that dcm is empirically close to mgs in discovering simple matchings and further dcm can find complex matchings  which is not sup-
	qi pages	data preparation	correlation mining
n-ary
	*** ***	form extraction	group discovery	complex matchings
	******** ****	{a} = {b} = {c d e}
	****	type recognition	matching discovery
	****	{f  g} =  {h  i}
****
	syntactic merging	matching selection
figure 1: from matching to mining: the dcm framework.
ported by mgs. third  we compare h-measure with other measures on the tel-1 dataset and the result shows h-measure outperforms the others in most cases.
　there are several applications of our work: first  while pursuing holistic matching  our result can naturally address the pairwise matching problem. for instance  given the matching {author} = {last name  first name} found by our approach  we can match {author} in some schema sa to {last name  first name} in another schema sb. second  our work is a critical step to construct a global web interface for each domain. specifically  among the synonyms in a matching  we can pick the most popular one as the representative in the global interface and use that matching to build the mappings from the global interface to local ones.
　in our development  we also observed several interesting issues. can we mine interesting patterns over cross-domain web interfaces  how to systematically decide the threshold values for mining  how can our approach benefit from exploring other information on the web  we discuss these open issues in section 1. in summary  the contributions of this paper are:
  we build a conceptually novel connection between the schema matching problem and the correlation mining approach. on one hand  we consider schema matching as a new application of correlation mining; on the other hand  we propose correlation mining as a new approach for schema matching.   we develop correlation measures that are robust for not only positive correlations  but also negative correlations. in particular  we identify the problems of existing measures on evaluating negative correlations  due to its special importance in schema matching  and further introduce a new correlation measure  hmeasure  distinct from those proposed in previous work.
　the rest of the paper is organized as follows: section 1 presents our motivating observations of integrating the deep web. section 1 develops the mining and selection algorithms. section 1 proposes a new correlation measure  h-measure. section 1 presents the data preparation step. section 1 reports our experiments. section 1 reviews related work and and section 1 discusses several further opportunities and open issues  and then concludes this paper.
1. motivation: from matching to mining
　as section 1 briefly introduced  our key insight is on connecting matching to mining  which this section further motivates with a concrete example. consider a typical scenario: suppose user amy  who wants to book two flight tickets from city a to city b  one for her and the other for her 1-year old child. to get the best deal  she needs to query on various airfare sources by filling the web query interfaces. for instance  in united.com  she fills the query interface with from as city a  to as city b and passengers as 1. for the same query in flyairnorth.com  she fills with depart as city a  destination as city b  adults as 1  seniors as 1  children as 1 and infants as 1.
　this scenario reveals some critical characteristics of the web interfaces in the same domain. first  some attributes may group together to form a  larger  concept. for instance  the grouping of adults  seniors  children and infants denotes the number of passengers. we consider such attributes that can be grouped as grouping attributes or having grouping relationship  denoted by putting them within braces  e.g.  {adults  seniors  children  infants} .
　second  different sources may use different attributes for the same concept. for instance  from and depart denote the city to leave from  and to and destination the city to go to. we consider such semantically equivalent attributes  or attribute groups  as synonym attributes or having synonym relationship  denoted by  =   e.g.  {from} = {depart}  {to} = {destination} .
　grouping attributes and synonym attributes together form complex matchings. in complex matching  a set of m attributes is matched to another set of n attributes  which is thus also called m:n matching   in contrast to the simple 1 matching . for instance  {adults  seniors  children  infants} = {passengers} is a 1 matching in the above scenario.
　to tackle the complex matching problem  we exploit the cooccurrence patterns to match schemas holistically and thus pursue a mining approach. unlike most schema matching work which matches two schemas at a time  we match all the schemas at the same time. this holistic view provides the co-occurrence information of attributes across many schemas  which reveals the semantics of complex matchings.  such co-occurrence information cannot be observed when schemas are matched only in pairs.  for instance  we may observe that adults  seniors  children and infants often co-occur with each other in schemas  while they together do not cooccur with passengers. this insight enables us to discover complex matchings with a correlation mining approach. in particular  in our application  we need to handle not only positive correlations  a traditional focus  but also negative ones  which have rarely been extensively explored or applied.
　by matching many schemas together  this holistic matching naturally discovers a more general type of complex matching - a matching may span across more than two attribute groups. still consider the amy scenario  if she tries a third airline source  priceline.com  she needs to fill the interface with departure city as city a  arrival city as city b  number of tickets as 1. we thus have the matching {adults  seniors  children  infants} = {passengers} = {number of tickets}  which is a 1:1 matching. similarly  we have two 1:1 matchings {from} = {departure city} = {depart} and {to} = {arrival city} = {destination}. we name this type of matching n-ary complex matching  which can be viewed as an aggregation of several binary m:n matchings.
　in particular  we develop a new approach  the dcm framework  to mine n-ary complex matchings. figure 1 illustrates this mining process: 1  as preprocessing  data preparation  section 1  prepares  schema transactions  for mining by extracting and cleaning the attribute entities in web interfaces. 1  as the main focus of this paper  the correlation mining step  section 1  discovers n-ary complex matchings by first finding potential attribute groups using positive correlations and then potential complex matchings using negative correlations. last  matching selection chooses the most confident and consistent matchings from the mining result. 1  also  since pursuing a mining approach  we need to choose appropriate correlation measures. we discuss this topic in section 1.
1. complex matching as correlation mining
　we view a schema as a transaction  a conventional abstraction in association and correlation mining. in data mining  a transaction is a set of items; correspondingly  in schema matching  we consider a schema as a set of attribute entities. an attribute entity contains attribute name  type and domain  i.e.  instance values . before mining  the data preparation step  section 1  finds syntactically similar entities among schemas. after that  each attribute entity is assigned a unique attribute identifier. while the mining is over the attribute entities  for simplicity of illustration  we use the attribute name of each entity  after cleaning  as the attribute identifier. for instance  the schema in figure 1 c  is thus  as a transaction of two attribute entities  written as {title  author}.
　formally  we consider the schema matching problem as: given the input as a set of schemas si = {s1 ... su} in the same domain  where each schema si is a transaction of attribute identifiers  find all the matchings m = {m1 ... mv}. each mj is an n-ary complex matching gj1 = gj1 = ... = gjw  where each gjk is an attribute group and semantically  each
mj should represent the synonym relationship of attribute groups gj1 ...  gjw and each gjk should represent the grouping relationship of attributes in gjk.
　motivated by our observations on the schema data  section 1   we develop a correlation mining algorithm  with respect to the above abstraction  figure 1 . first  group discovery: we mine positively correlated attributes to form potential attribute groups. a potential group may not be eventually useful for matching; only the ones having synonym relationship  i.e.  negative correlation  with other groups can survive. for instance  if all sources use last name  first name  and not author  then the potential group {last name  first name} is not useful because there is no matching  to author  needed. second  matching discovery: given the potential groups  including singleton ones   we mine negatively correlated attribute groups to form potential n-ary complex matchings. a potential matching may not be considered as correct due to the existence of conflicts among matchings. third  matching selection: to solve the conflicts  we develop a selection strategy to select the most confident and consistent matchings from the mining result. section 1 discusses the group and matching discovery and section 1 the matching selection.
　after group discovery  we need to add the discovered groups into the input schemas si to mine negative correlations among groups.  a single attribute is viewed as a group with only one attribute.  specifically  an attribute group is added into a schema if that schema contains any attribute in the group. for instance  if we discover that last name and first name have grouping relationship  we consider {last name  first name} as an attribute group  denoted by glf for simplicity  and add it to any schema containing either last name or first name  or both. the intuition is that although a schema may not contain the entire group  it still partially covers the concept that the group denotes and thus should be counted in matching discovery for that concept. note that we do not remove singleton groups {last name} and {first name} when adding glf  because glf is only a potential group and may not survive in matching discovery.
1 complex matching discovery
　while group discovery works on individual attributes and matching discovery on attribute groups  they can share the same mining process. we use the term - items - to represent both attributes and groups in the following discussion of mining algorithm.
　correlation mining  at the heart  requires a measure to gauge correlation of a set of n items; our observation indicates pairwise correlations among these n items. specifically  for n groups forming synonyms  any two groups should be negatively correlated  since they both are synonyms by themselves  e.g.  in the matching {destination} = {to} = {arrival city}  negative correlations exist between any two groups . we have similar observation on the attributes with grouping relationships. motivated by such observations  we design the measure as:
		 1 
where m is some correlation measure for two items  e.g.  the measures surveyed in  . that is  we define cmin as the minimal
algorithm: n-aryschemamatching:
input: inputschemas si = {s1 ... su} 
　　　measures mp mn  thresholds tp  tn output: potential n-ary complex matchings begin:
1 /* group discovery */
1 g ○ aprioricorrmining si mp tp 
1 /* adding groups into si */
1 for each si （ si
1 for each gk （ g
1 ifthen si ○ si “ {gk}
1 /* matching discovery */
1 m ○ aprioricorrmining si mn tn 
1 return m end
algorithm: aprioricorrmining:
input: inputschemas si = {s1 ... su}  measures m  thresholds t
output: correlated items begin:
1 x
1 v ○	ut	si si （ si
1 for
1 if m ap aq  − t then x ○ x “ {{ap aq}}
1 l ○ 1
1 /* xl: correlated items with length = l */
1 xl ○ x
1 while
1 construct xl+1 from xl using apriori feature
1 x ○ x “ xl+1
1 xl ○ xl+1
1 return x endfigure 1: algorithm n-aryschemamatching.
value of the pairwise evaluation  thus requiring all pairs to meet
this minimal  strength. 
　cmin has several advantages: first  it satisfies the  apriori  feature and thus enables the design of an efficient algorithm. in correlation mining  the measure for qualification purpose should have a desirable  apriori  property  i.e.  downward closure   to develop an efficient algorithm.  in contrast  a measure for ranking purpose should not have this  apriori  feature  as section 1 will discuss.  cmin satisfies the  apriori  feature since given any item set a and its subset a   we have cmin a  m  ＋ cmin a   m  because the minimum of a larger set  e.g.  min {1 1}   cannot be higher than its subset  e.g.  min {1}  . second  cmin can incorporate any measure m for two items and thus can accommodate different tasks  e.g.  mining positive and negative correlations  and be customized to achieve good mining quality.
　leveraging the  apriori  feature of cmin  we develop algorithm aprioricorrmining figure 1  for discovering complex matchings  in the spirit of the classic apriori algorithm for association mining . that is  we find all the correlated items with length l + 1 based on the ones with length l.
　with cmin  we can directly define positively correlated attributes in group discovery and negatively correlated attribute groups in matching discovery. a set of attributes {a1  ...  an} is positively correlated attributes  denoted by pc  if cmin {a1  ...  an}  mp  − tp  where mp is a measure for positive correlation and tp is a given threshold. similarly  a set of attribute groups {g1  ...  gm} is negatively correlated attribute groups  denoted by nc  if cmin {g1  ...  gm}  mn  − tn  where mn is a measure for negative correlation and tn is another given threshold.
algorithm: matchingselection:
input: potential complex matchings m = {m1 ... mv} 
measure mn
output: selected complex matchings begin:
1 r ○   /* selected n-ary complex matchings */
1 while
1 /* select the matching ranked the highest */
1 mt ○ getmatchingrankfirst m mn 
1 r ○ r “ {mt}
1 for each mj （ m
1 /* remove the conflicting part */ 1mt
1 /* delete	if it contains no matching */
1 if |mj|   1 then m ○ m   {mj}
1 return r end
algorithm: getmatchingrankfirst:
input: potential complex matchings m = {m1 ... mv} 
measure mn
output: the matching with the highest ranking begin:
1 mt ○ m1
1 for each mj （ m 1 ＋ j ＋ v
1 if s mj mn    s mt mn  then
1
1	if	then
1
1 return mt
endfigure 1: algorithm matchingselection.
　algorithm n-aryschemamatching shows the pseudo code of the complex matching discovery  figure 1 . line 1  group discovery  calls aprioricorrmining to mine pc. lines 1 add the discovered groups into si. line 1  matching discovery  calls aprioricorrmining to mine nc. similar to   the time complexity of n-aryschemamatching is exponential with respect to the number of attributes. but in practice  the execution is quite fast since correlations exist among semantically related attributes  which is far less than arbitrary combination of all attributes.
1 complex matching selection
　correlation mining can discover true semantic matchings and  as expected  also false ones due to the existence of coincidental correlations. for instance  in books domain  the mining result may have both {author} = {first name  last name}  denoted by m1 and {subject} = {first name  last name}  denoted by m1. we can see m1 is correct  while m1 is not. the reason for having the false matching m1 is that in the schema data  it happens that subject does not often co-occur with first name and last name.
　the existence of false matchings may cause matching conflicts. for instance  m1 and m1 conflict in that if one of them is correct  the other one will not. otherwise  we get a wrong matching {author} = {subject} by the transitivity of synonym relationship. since our mining algorithm does not discover {author} = {subject}  m1 and m1 cannot be both correct.
　leveraging the conflicts  we select the most confident and consistent matchings to remove the false ones. intuitively  between conflicting matchings  we want to select the more negatively correlated one because it indicates higher confidence to be real synonyms. for example  our experiment shows that  as m1 is coincidental  it is indeed that mn m1    mn m1   and thus we select m1 and remove m1. note that  with larger data size  semantically
ap apaqf1f1f1+ aqf1f1f1+f+1f+1f++figure 1: contingency table for test of correlation.
correct matching is more possible to be the winner. the reason is that  with larger size of sampling  the correct matchings are still negatively correlated while the false ones will remain coincidental and not as strong.
　before presenting the selection algorithm  we need to develop a strategy for ranking the discovered matchings. that is  for any nary complex matching mj: gj1 = gj1 = ... = gjw  we have a score function s mj mn  to evaluate mj under measure mn.
　while section 1 discussed a measure for  qualifying  candidates  we now need to develop another  ranking  measure as the score function. since ranking and qualification are different problems and thus require different properties  we cannot apply the measure cmin  equation 1  for ranking. specifically  the goal of qualification is to ensure the correlations passing some threshold. it is desirable for the measure to support downward closure to enable an  apriori  algorithm. in contrast  the goal of ranking is to compare the strength of correlations. the downward closure enforces  by definition  that a larger item set is always less correlated than its subsets  which is inappropriate for ranking correlations of different sizes. hence  we develop another measure cmax  the maximal mn value among pairs of groups in a matching  as the score function s. formally 

　it is possible to get ties if only considering the cmax value; we thus develop a natural strategy for tie breaking. we take a  topk  approach so that s in fact is a set of sorted scores. specifically  given matchings mj and mk  if cmax mj mn  = cmax mk mn   we further compare their second highest mn values to break the tie. if the second highest values are also equal  we compare the third highest ones and so on  until breaking the tie.
　if two matchings are still tie after the  top-k  comparison  we choose the one with richer semantic information. we consider matching mj semantically subsumes matching mk  denoted by
  if all the semantic relationships in mk are covered in mj. for instance  {arrival city} = {destination} {arrival city} = {destination} because the synonym relationship in the second matching is subsumed in the first one. also  {author} = {first name  last name}  {author} = {first name} because the synonym relationship in the second matching is part of the first.
　combining the score function and the semantic subsumption  we rank matchings with following rules: 1  if s mj mn  s mk mn   mj is ranked higher than mk. 1  if s mj mn  = s mk mn  and is ranked higher than mk. 1  otherwise  we rank mj and mk arbitrarily. algorithm getmatchingrankfirst  figure 1  illustrates the pseudo code of choosing the highest ranked matching with this strategy.
　algorithm matchingselectionshows the selection algorithm. we apply a greedy selection strategy by choosing the highest ranked matching  mt  in each iteration. after choosing mt  we remove the inconsistent parts in remaining matchings  lines 1 - 1 . the final output is the selected n-ary complex matchings without conflict. note that we need to do the ranking in each iteration instead of sorting all the matchings in the beginning because after removing the conflicting parts  the ranking may change. the time complexity of algorithm matchingselection is o v1   where v is the number of matchings in m.
example 1: assume running n-aryschemamatchingin books

figure 1: attribute frequencies in books domain.
domain finds matchings m as  matchings are followed by their scores :
m1: {author} = {last name  first name}  1 m1: {author} = {last name}  1
m1: {subject} = {category}  1
m1: {author} = {first name}  1
m1: {subject} = {last name  first name}   1
m1: {subject} = {last name}  1 and m1: {subject} = {first name}  1.
　in the first iteration  m1 is ranked the highest and thus selected. in particular  although s m1 mn  = s m1 mn   m1 is ranked higher since. now we remove the conflicting parts of the other matchings. for instance  m1 conflicts with m1 on author. after removing author  m1 only contains one attribute and is not a matching any more. so we remove m1 from m. similarly  m1 and m1 are also removed. the remaining matchings are m1  m1 and m1. in the second iteration  m1 is ranked the highest and thus also selected. m1 and m1 are removed because they conflict with m1. now m is empty and the algorithm stops. the final output is thus m1 and m1.	
1. correlation measure
　in this section  we discuss the positive measure mp and the negative measure mn  used as the component of cmin  equation 1  for positive and negative correlation mining respectively in algorithm
n-aryschemamatching  section 1 .
　as discussed in   a correlation measure by definition is a testing on the contingency table. specifically  given a set of schemas and two specified attributes ap and aq  there are four possible combinations of ap and aq in one schema si: ap aq are copresent in si  only ap presents in si  only aq presents in si  and ap aq are co-absent in si. the contingency table  of ap and aq contains the number of occurrences of each situation  as figure 1 shows. in particular  f1 corresponds to the number of copresence of ap and aq; f1 f1 and f1 are denoted similarly. f+1 is the sum of f1 and f1; f+1  f1+ and f1+ are denoted similarly. f++ is the sum of f1  f1  f1 and f1.
　the design of a correlation measure is often empirical. to our knowledge  there is no good correlation measure universally agreed upon . for our matching task  in principle any measure can be applied. however  since the semantic correctness of the mining result is of special importance for the schema matching task  we care more the ability of the measures on differentiating various correlation situations  especially the subtlety of negative correlations  which has not been extensively studied before.

ap apaq11 aq1111
ap apaq11 aq1111
ap apaq11 aq1111 a1  example of sparseness problem b1  example of rare attribute problem c1  example of frequent attribute problemwith measure lift:with measure jaccard:with measure jaccard:less positive correlationap as rare attributeap and aq are independentbut a higher lift = 1.and jaccard = 1.but a higher jaccard = 1.
ap apaq11 aq1111
ap apaq11 aq1111
ap apaq11 aq1111 a1  example of sparseness problem b1  example of rare attribute problem c1  example of frequent attribute problemwith measure lift:with measure jaccard:with measure jaccard:more positive correlationno rare attributeap and aq are positively correlatedbut a lower lift = 1.and jaccard = 1.but a lower jaccard = 1.figure 1: examples of the three problems.　we first identify the quality requirements of measures  which are imperative for schema matching  based on the characteristics of web query interfaces. specifically  we observe that  in web interfaces  attribute frequencies are extremely non-uniform  similar to the use of english words  showing some zipf-like distribution. for instance  figure 1 shows the attribute frequencies in books domain: first  the non-frequent attributes results in the sparseness of the schema data  e.g.  there are over 1 attributes in books domain  but each schema only has 1 in average . second  many attributesare rarely used  occurring only once in the schemas. third  there exist some highly frequent attributes  occurring in almost every schema.
　these three observations indicate that  as the quality requirements  the chosen measures should be robust against the following problems: sparseness problem for both positive and negative correlations  rare attribute problem for negative correlations  and frequent attribute problem for positive correlations. in this section  we discuss each of them in details.
the sparseness problem
　in schema matching  it is more interesting to measure whether attributes are often co-present  i.e.  f1  or cross-present  i.e.  f1 and f1  than whether they are co-absent  i.e.  f1 . many correlation measures  such as χ1 and lift  include the count of co-absence in their formulas. this may not be good for our matching task  because the sparseness of schema data may  exaggerate  the effect of co-absence. this problem has also been noticed by recent correlation mining work such as  1  1  1 . in   the authors use the null invariance property to evaluate whether a measure is sensitive to co-absence. the measures for our matching task should satisfy this null invariance property.
example 1: figure 1 a  illustrates the sparseness problem with an example. in this example  we choose a common measure: the lift  i.e  lift  .  other measures considering f1 have similar behavior.  the value of lift is between 1 to +±. lift = 1 means independent attributes  lift   1 positive correlation and lift   1 negative correlation. figure 1 a  shows that lift may give a higher value to less positively correlated attributes. in the scenario of schema matching  the table in figure 1 a1  should be more positively correlated than the one in figure 1 a1  because in figure 1 a1   the co-presence  f1  is more frequently observed than the cross-presence  either f1 or f1   while in figure 1 a1   the co-presence has the same number of observations as the crosspresence. however  lift cannot reflect such preference. in particular  figure 1 a1  gets a much higher lift and figure 1 a1  is even evaluated as not positively correlated. similar example can also be found for negative correlation with lift. the reason lift gives an inappropriate answer is that f1 incorrectly affects the result. 
　we explored the 1 measures in  and the χ1 measure in . most of these measures  including χ1 and lift  suffer the sparseness problem. that is  they consider both co-presence and coabsence in the evaluation and thus do not satisfy the null invariance property. the only three measures supporting the null invariance property are confidence  jaccard and cosine.
the rare attribute problem for negative correlation
　although confidence  jaccard and cosine satisfy the null invariance property  they are not robust for the rare attribute problem  when considering negative correlations. specifically  the rare attribute problem can be stated as when either ap or aq is rarely observed  the measure should not consider ap and aq as highly negatively correlated because the number of observations is not convincing to make such judgement. for instance  consider the jaccard  i.e.  jaccard   measure  it will stay unchanged when both f1 and f1 + f1 are fixed. therefore  to some degree  jaccard cannot differentiate the subtlety of correlations  e.g.  f1 = 1  f1 = 1 and f1 = 1  f1 = 1   as example 1 illustrates. other measures such as confidence and cosine have similar problem. this problem is not critical for positive correlation  since attributes with strong positive correlations cannot be rare.
example 1: figure 1 b  illustrates the rare attribute problem. in this example  we choose a common measure: the jaccard. the value of jaccard is between 1 to 1. jaccard close to 1 means negative correlation and jaccard close to 1 positive correlation. figure 1 b  shows that jaccard may not be able to distinguish the situations of rare attribute. in particular  jaccard considers the situations in figure 1 b1  and figure 1 b1  as the same. but figure 1 b1  is more negatively correlated than figure 1 b1  because ap in figure 1 b1  is more like a rare event than true negative correlation. 
　to differentiate the subtlety of negative correlations  we develop a new measure  h-measure  equation 1   as the negative correlation mn. the value of h is in the range from 1 to 1. an h value close to 1 denotes a high degree of positive correlation; an h value close to 1 denotes a high degree of negative correlation.
	.	 1 
　h-measure satisfied the quality requirements: on one hand  similar to jaccard  cosine and confidence  h-measure satisfies the null invariance property and thus avoids the sparseness problem by ignoring f1. on the other hand  by multiplying individual effect of-measure is more capable of reflecting subtle variation of negative correlations.
the frequent attribute problem for positive correlation
　for positive correlations  we find that confidence  jaccard  cosine and h-measure are not quite different in discovering attribute groups. however  all of them suffer the frequent attribute problem. this problem seems to be essential for these measures: although they avoid the sparseness problem by ignoring f1  as the cost  they lose the ability to differentiate highly frequent attributes from really correlated ones. specifically  highly frequent attributes may co-occur in most schemas just because they are so frequently used  not because they have grouping relationship  e.g.  in books domain  isbn and title are often co-present because they are both very frequently used . this phenomenon may generate uninteresting groups  i.e.  false positives  in group discovery.
example 1: figure 1 c  illustrates the frequent attribute problem with an example  where we still use jaccard as the measure. figure 1 c  shows that jaccard may give a higher value to independent attributes. in figure 1 c1   ap and aq are independent and both of them have the probabilities 1 to be observed; while  in figure 1 c1   ap and aq are really positively correlated. however  jaccard considers figure 1 c1  as more positively correlated than figure 1 c1 . in our matching task  a measure should not give a high value for frequently observed but independent attributes. 
　the characteristic of false groupings is that the f1 value is very high  since both attributes are frequent . based on this characteristic  we add another measure to filter out false groupings.
specifically  we defi as:
 1 
	 	otherwise 
where td is a threshold to filter out false groupings. to be consistent with mn  we also use the h-measure in mp.
1. data preparation
　the query schemas in web interfaces are not readily minable in html format; as preprocessing  data preparation is essential to prepare  schema transactions  for mining. as shown in figure 1  data preparation consists of: 1  form extraction - extracting attribute entities from query interfaces in web pages  1  type recognition - recognizing the types of the attribute entities from domain values  and 1  syntactic merging - syntactically merging these attribute entities.
　form extraction reads a web page with query forms and extracts the attribute entities containing attribute names and domains. for instance  the attributeabout title in figure 1 c  is extracted as name =  title of book   domain = any  where  domain = any  means any value is possible. this task is itself a challenging and independent problem. we solved this problem by a parsing approach with the hypothesis of the existence of hidden syntax . note that there is no data cleaning in this step and thus the attribute names and domains are raw data.
　after extracting the forms  we perform some standard normalization on the extracted names and domains. we first stem attribute names and domain values using the standard porter stemming algorithm . next  we normalize irregular nouns and verbs  e.g.   children  to  child    colour  to  color  . last  we remove common stop words by a manually built stop word list  which contains words common in english  in web search  e.g.   search    page    and in the respective domain of interest  e.g.   book    movie  .
　we then perform type recognition to identify attribute types. as section 1 discusses  type information helps to identify homonyms  i.e.  two attributes may have the same name but different types  and constrain syntactic merging and correlation-based matching  i.e.  only attributes with compatible types can be merged or matched . since the type information is not declared in web interfaces  we develop a type recognizer to recognize types from domain values.
　finally  we merge attribute entities by measuring the syntactic similarity of attribute names and domain values  e.g.  we merge  title of book  to  title  by name similarity . it is a common data cleaning technique to merge syntactically similar entities by exploring linguistic similarities. section 1 discusses our merging strategy.
1 type recognition
　while attribute names can distinguish different attribute entities  the names alone sometimes lead to the problem of homonyms  i.e. 

figure 1: the compatibility of types.
the same name with different meanings  - we address this problem by distinguishing entities by both names and types. for instance  the attribute name departing in the airfares domain may have two meanings: a datetime type as departing date  or a string type as departing city. with type recognition  we can recognize that there are two different types of departing: departing  datetime  and departing  string   which indicate two attribute entities.
　in general  type information  as a constraint  can help the subsequent steps of syntactic merging and correlation-based matching. in particular  if the types of two attributes are not compatible  we consider they denote different attribute entities and thus neither merge them nor match them.
　since type information is not explicitly declared in web interfaces  we develop a type recognizer to recognize types from domain values of attribute entities. for example  a list of integer values denotes an integer type. in the current implementation  type recognition supports the following types: any  string  integer  float  month  day  date  time and datetime.  an attribute with only an input box is given an any type since the input box can accept data with different types such as string or integer.  two types are compatible if one can subsume another  i.e.  the is-a relationship . for instance  date and datetime are compatible since date subsumes datetime. figure 1 lists the compatibility of all the types in our implementation.
1 syntactic merging
　we clean the schemas by merging syntactically similar attribute entities  a common data cleaning technique to identify unique entities . specifically  we develop name-based merging and domainbased merging by measuring the syntactic similarity of attribute names and domains respectively. syntactic merging increases the observations of attribute entities  which can improve the effect of correlation evaluation.
　name-based merging: we merge two attribute entities if they are similar in names. we observe that the majority of deep web sources are consistent on some concise  core  attribute names  e.g.   title   and others are variation of the core ones  e.g.   title of book  . therefore  we consider attribute ap is name-similar to attribute aq if ap's name   aq's name  i.e.  ap is a variation of aq  and aq is more frequently used than ap  i.e.  aq is the majority . this frequency-based strategy helps avoid false positives. for instance  in books domain  last name is not merged to name because last name is more popular than name and thus we consider them as different entities.
　domain-based merging: we then merge two attribute entities if they are similar in domain values. in particular  we only consider attributes with string types  since it is unclear how useful it is to measure the domain similarity of other types. for instance  in airfares domain  the integer values of passengers and connections are quite similar  although they denote different meanings.
　we view domain values as a bag of words  i.e.  counting the word frequencies . we name such a bag aggregate values  denoted as va for attribute a. given a word w  we denote va w  as the frequency of w in va. the domain similarity of attributes ap and aq is thus the similarity of vap and vaq. in principle  any reasonable similarity function is applicable here. in particular  we choose
.
　the above three steps  form extraction  type recognition and syntactic merging  clean the schema data as transactions to be mined. more detailed discussion about these data cleaning steps can be found at the extended report .
1. experiments
　we choose two datasets  tel-1 dataset and bamm dataset  of the uiuc web integration repository  as the testbed of the dcm framework. the tel-1 dataset contains raw web pages over 1 deep web sources in 1 popular domains. each domain has about 1 sources. the bamm dataset contains manually extracted attribute names over 1 sources in 1 domains  with around 1 sources per domain   which was first used by .
　in the experiment  we assume a perfect form extractor to extract all the interfaces in the tel-1 dataset into query capabilities by manually doing the form extraction step. the reason we do not apply the work in  is that we want to isolate the mining process to study and thus fairly evaluate the matching performance. after extracting the raw data  we do the data cleaning according to the process explained in section 1. then  we run the correlation mining algorithm on the prepared data in each domain. also  in the results  we use attribute name and type together as the attribute identifier for an attribute entity since we incorporate type recognition in data preparation to identify homonyms  section 1 .
　to evaluate the matching performance and the h-measure  we extensively test the dcm framework on the two datasets. first  we test our approach on the tel-1 dataset and the result shows good target accuracy. second  we compare the dcm framework with the mgs framework   which also matches web interfaces by a statistical approach  on its bamm dataset. the result shows that dcm is empirically close to mgs in discovering simple matchings and further dcm can find complex matchings  which is not supported by mgs. third  we compare the h-measure with other measures on the tel-1 dataset and the result shows that hmeasure outperforms the others in most cases.
1 metrics
　we compare experimentally discovered matchings  denoted by mh  with correct matchings written by human experts  denoted by mc. in particular  we adopt the target accuracy  a metric initially developed in   by customizing the target questions to the complex matching scenario. the idea of the target accuracy is to measure how accurately that the discovered matchings answer the target questions. specifically  for our complex matching task  we consider the target question as  given any attribute  to find its synonyms  i.e.  word with exactly the same meaning as another word  e.g.  subject is a synonym of category in books domain   hyponyms  i.e.  word of more specific meaning  e.g.  last name is a hyponym of author  and hypernyms  i.e.  word with a broader meaning  e.g  author is a hypernym of last name .
　it is quite complicated to use different measures for different semantic relationships  we therefore report an aggregate measure for simplicity and  at the same time  still reflecting semantic differences. for our discussion here  we name synonym  hyponym and hypernym together as closenym - meaning that they all denote some degrees of closeness in semantic meanings. our target question now is to ask the set of closenyms of a given attribute.
example 1: for instance  for matching {a} = {b  c}  the closenym sets of attributes a  b  c are {b  c}  {a}  {a} respectively. in particular  the closenym sets of b does not have c since b and c only have grouping relationship. in contrast  for matching {a} = {b} = {c}  the closenym sets of attributes a  b  c are {b  c}  {a  c}  {a  c} respectively. we can see that the difference of matchings can be reflected in the corresponding closenym sets. 
　except this difference in target question  we use the same metric of target accuracy as . specifically  we assume a  random querier  to ask for closenym set of each attribute according to the its frequency. the answer to each question is closenym set of the queried attribute in discovered matchings. we define cls aj|m  as the closenym set of attribute aj. given mc and mh  the precision and recall of the closenym sets of attribute aj are:
 and
.
　since more frequently used attributes have higher probabilities to be asked in this  random querier   we compute the weighted average of all the paj's and raj's as the target precision and target
　　　　　　　　　　　　　　　　　　　　　oj recall. the weight is assigned as ok   where oj is the frequency of attribute aj in the dataset  i.e.  its number of occurrences in different schemas . therefore  target precision and target recall of mh with respect to mc are:
 pt mh mc  =	aj	oojkpaj mh mc  rt mh mc  =	aj	oojkraj mh mc .
1 experimental results
　to illustrate the effectiveness of the mining approach  we only list and count the  semantically difficult  matchings discovered by the correlation mining algorithm  not the simple matchings by the syntactic merging in the data preparation  e.g.  {title of book} to {title} . our experiment shows that many frequently observed matchings are in fact  semantically difficult  and thus cannot be found by syntactic merging.
result on the tel-1 dataset: in this experiment  we run our algorithm  with h-measure as the correlation measure  on the tel-1 dataset. we set the thresholds tp to 1 and td to 1 for positive correlation mining and tn to 1 for negative correlation mining. we empirically get these numbers by testing the algorithm with various thresholds and choose the best one. as section 1 will discuss  more systematic study can be investigated in choosing appropriate threshold values.
　figure 1 illustrates the detailed results of n-ary complex matchings discovered in books domain. the step of group discovery found 1 likely groups  g1 to g1 in figure 1 . in particular  mp gives a high value  actually the highest value  for the group of last name  any  and first name  any . the matching discovery found 1 likely complex matching  m1 to m1 in figure 1 . we can see that m1 and m1 are fully correct matchings  while others are partially correct or incorrect. last  the matching selection will choose m1 and m1  i.e.  the correct ones  as the final output.
　figure 1 shows the results on airfares and movies.  the results of other domains can be found at the extended report  . the third column denotes the correctness of the matching. in particular  y means a fully correct matching  p a partially correct one and n an incorrect one. our mining algorithm does find interesting matchings in almost every domain. for instance  in airfares domain  we find 1 fully correct matchings  e.g.  {destination  string } = {to  string } = {arrival city  string }. also  {passenger  integer } = {adult  integer   child  integer   infant  integer } is partially correct because it misses senior  integer .
stepvalue ofresultcmincmaxgroup discoverygg1 = {last name  unknown   first name  any }1g1 = {title  any   keyword  any }1g1 = {last name  any   title  any }1g1 = {first name  any   catalog  any }1g1 = {first name  any   keyword  any }1matching discoverymm1: {author  any } = {last name  any   first name  any }11m1: {author  any } = {last name  any }11m1: {subject  string } = {category  string }11m1: {author  any } = {last name  any   catalog  any }11m1: {author  any } = {first name  any }11m1: {category  string } = {publisher  string }11matching
selectionrr1: {author  any } = {last name  any   first name  any }1r1:	subject  string 	=	category  string 1	{	}	{	}
figure 1: running algorithms n-aryschemamatching and matchingselection on books domain.
domainfinal output after matching selectioncorrect airfares{destination  string } = {to  string } = {arrival city  string }y{departure date  datetime } = {depart  datetime }y{passenger  integer } = {adult  integer   child  integer   infant  integer }p{from  string   to  string } = {departure city  string   arrival city  string }y{from  string } = {depart  string }y{return date  datetime } = {return  datetime }ymovies{artist  any } = {actor  any } = {star  any }y{genre  string } = {category  string }ycast & crew  any 	= actor  any   director  any y	{	}	{	}
figure 1: experimental results for airfares and movies.

domainptrtptrt 1%  1%  1%  1% books11airfares11.1movies11musicrecords111hotels11.1.1carrentals11.1.1jobs1.1.1.1automobiles111
domainpt h rt h pt ζ rt ζ  1%  1%  1%  1% books111airfares1.1.1.1movies111musicrecords11.1hotels1111carrentals1111jobs1111automobiles11.1　since  as a statistical method  our approach replies on  sufficient observations  of attribute occurrences  it is likely to perform more favorably for frequent attributes  i.e.  the head-ranked attributes in figure 1 . to quantify this  observation  factor  we report the target accuracy with respect to the attribute frequencies. in particular  we consider the attributes above a frequency threshold t  i.e.  the number of occurrences of the attribute over the total number of schemas figure 1: target accuracy of 1 domains.
is above t  in both discovered matchings and correct matchings to measure the target accuracy. specifically  we run the algorithms on all the attributes and then report the target accuracy in terms of the frequency-divided attributes. in the experiment  we choose t as 1% and 1%.
　consider the airfares domain  if we only consider the attributes above 1% frequency in the matching result  only 1 attributes are above that threshold. the discovered matchings in figure 1 become {destination  string } = {to  string }  {departure date  datetime } = {depart  datetime }  and {return date  datetime  = return  datetime }.  the other attributes are removed since they are all below 1% frequency.  these three matchings are exactly the correct matchings the human expert can recognize among the 1 attributes and thus we get 1 in both target precision and recall.
　next  we apply the 1% frequency threshold and get 1 attributes. the discovered matchings in figure 1 are unchanged since all the attributes  in the matchings  are now passing the threshold. compared with the correct matchings among the 1 attributes  we do miss some matchings such as {cabin  string } = {class  string } and {departure  datetime  = departure date  datetime }. also  some matchings are partially correct such as {passenger  integer } = {adult  integer   child  integer   infant  integer }. hence  we get 1 in target precision and 1 in target recall.
　figure 1 lists the target accuracies of the 1 domains under thresholds 1% and 1%. from the result  we can see that our approach does perform better for frequent attributes.
result on the bamm dataset: we test the dcm framework on the bamm dataset used in ; the result shows that dcm is emfigure 1: comparison of h-measure and jaccard.
pirically close to the mgs framework in  on discovering simple 1 matchings and further we can find complex matchings that mgs cannot. since the bamm dataset only contains manually extracted attribute names  we skip the data preparation step in this experiment. the result shows that we can discover almost all the simple 1 matchings found by mgs. in particular  we find {subject} = {category} in books  {style} = {type} = {category} in automobiles  {actor} = {artist} and {genre}= {category} in movies  and {album} = {title} and {band} = {artist} in musicrecords. further  dcm can find the complex matchings {author} = {last name  first name} in books  while mgs can only find either {author} = {last name} or {author} = {first name}.
evaluating the h-measure: we compare the h-measure with other measures and the result shows that h-measure gets better target accuracy. as an example  we choose jaccard  ζ  as the measure we compare to. with jaccard  we define the mp and mn as
f
and mn ap aq 	=	1   ζ ap aq .
　we set the tp and tn for this jaccard-based mp and mn as 1 and 1 respectively. we compare the target accuracy of hmeasure and jaccard in the situation of 1% frequency threshold. the result  figure 1  shows that h-measure is better in both target precision and target recall in most cases. similar comparisons show that h-measure is also better than other measures such as cosine and confidence.
1. related work
　schema matching is important for schema integration  1  1  and thus has got great attention. however  existing schema matching works mostly focus on simple 1 matching  1  1  1  between two schemas. complex matching has not been extensively studied  mainly due to the much more complex search space of exploring all possible combinations of attributes. consider two schemas with u and v attributes respectively  while there are only u〜v potential 1 matchings  the number of possible m:n matchings is exponential. also  it is still unclear that how to compare the similarity between two groups of attributes. in contrast  this paper proposes to discover complex matchings by holistically matching all the schemas together. specifically  we explore the co-occurrences information across schemas and develop a correlation mining approach.
　unlike previous correlation mining algorithms  which mainly focus on finding strong positive correlations  1  1  1  1  1   our algorithm cares both positive and negative correlations. in particular  as a new application for correlation mining  the correctness of schema matching mainly depends on the subtlety of negative correlations. we thus study the rare attribute problem and develop the h-measure  which empirically outperforms existing ones on evaluating negative correlations.
　our previous schema matching work  the mgs framework   also matches web interfaces by exploiting holistic information. although built upon the same insight  dcm is different from mgs in: 1  abstraction: dcm abstracts schema matching as correlation mining  while mgs as hidden model discovery by applying statistical hypothesis testing. the difference in abstraction leads to fundamentally different approaches. 1  expressiveness: dcm finds m:n matchings  while mgs currently finds 1 matchings and it is unclear that how it can be extended to support m:n matchings.
1. concluding discussion
　in our development of the mining-based matching approach  we also observed several further opportunities and open issues that warrant more investigation. first  it is interesting to know whether our observation and approach can cross the domain boundary. specifically  given a set of web interfaces across different domains  we hope to know whether there still are interesting patterns that reveal some semantic relationships among attributes  as we have observed for sources in one domain.
　second  more systematic study can be investigated for choosing appropriate correlation measures and threshold values. in this paper  we choose the h-measure based on the observations of the data and the threshold values according to the empirical experiments. we expect a more formal and systematic study to help the design of measures and the evaluation of threshold values.
　third  to validate and refine the matching results  we may send some trial probings through web interfaces. for instance  given two online movie sources  one using actor and the other using star  we can send some sample queries on these two sources with same values on actor and star. if they often return overlapping results  we consider the matching {actor} = {star} is correct. however  this probing brings new challenges to solve. in particular  for complex matchings  e.g. {author} = {last name  first name}   schema composition has to be done before probing  which is itself a difficult problem. also  it is unclear that how to automatically collect sample queries for each domain. last  with current techniques  it is difficult to accurately compare the query results in web pages.
　in summary  this paper explores co-occurrence patterns among attribute to tackle the complex matching problem. this exploration is well suited for the integration of large-scale heterogenous data sources  such as the deep web. specifically  we abstract complex matching as correlation mining and develop the dcm framework.
further  we propose a new correlation measure  h-measure  for mining negative correlations. our experiments validate the effectiveness of both the mining approach and the h-measure.
