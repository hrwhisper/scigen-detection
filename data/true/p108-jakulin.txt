we propose a simple yet potentially very effective way of visualizing trained support vector machines. nomograms are an established model visualization technique that can graphically encode the complete model on a single page. the dimensionality of the visualization does not depend on the number of attributes  but merely on the properties of the kernel. to represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms  we employ logistic regression to convert the distance from the separating hyperplane into a probability. case studies on selected data sets show that for a technique thought to be a black-box  nomograms can clearly expose its internal structure. by providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors.
categories and subject descriptors
g.1  probability and statistics :  multivariate statistics ; h.1  information interfaces and presentation  e.g.  hci  : user interfaces-theory and methods
general terms
theory  human factors
keywords
nomogram  visualization  support vector machines  machine learning
1. introduction
모within predictive data mining  methods that build classification models have received much attention. these methods consider a set of class-labelled data instances and induce classification models that should both predict well and  preferably and through the model inspection  can uncover interesting relations and patterns. the latter is particularly important when predictive data mining is used for
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  chicago  illinois  usa.
copyright 1 acm 1-1-x/1 ...$1.
knowledge discovery  where presentation of the classification model should help the user to answer questions such as  which are the most important factors that determine the class of the instance    and  what is the magnitude of the effect of these    and  how do various factors interact    and alike.
모a support vector machine  svm   1  1  is a popular and much applied supervised machine learning method. it is known for good predictive performance  but may be at a disadvantage in terms of intuitive presentation of the classifier  particularly when compared to some other supervised learning techniques like classification trees and rules. while an svm model can be presented as a weighted list of support vectors  as a subset of learning instances that defines the decision boundary  this only reduces the number of instances to consider in the interpretation but does not answer any of the questions posed above directly. it is possible to show the svm classifier directly in the attribute space  but this is only appropriate when the attribute space does not have more than two or three dimensions. when the svm model is a hyperplane  we can also present it with the hyperplane's normal vector  but this technique is of limited utility with multi-valued or continuous attributes.
모in the paper  we propose a new approach for visualization of svm models. the main advantage of our approach is that it captures a complete classification model in a single  easyto-interpret graph and for all common types of attributes and even for non-linear svm kernels. the particular model visualization we use is called a nomogram. nomograms were invented by french mathematician maurice d'ocagne in 1 to graphically represent a class of mathematical functions. in the beginning of 1 a search for 'nomogram*' on pubmed/medline  a database of biomedical article citations  yielded over 1 papers  a search for 'support vector machine*' yielded fewer than 1 . a search for nomograms on google resulted in 1 web pages. nomograms are not an uncertain novelty  but a milestone in the history of visualization .
모to visualize a logistic regression model  the use of nomograms was first proposed by lubsen and coauthors . with an excellent implementation of logistic regression nomograms in s-plus and r statistical packages by harrell   the idea has recently been picked up and the nomograms have been used much to present probabilistic classification models in  for instance  clinical medicine and oncology  e.g.   . a na몮 ve bayesian classifier can too be visualized in the form of a nomogram .
the nomograms for support vector machines that we in-

a priori 1 to over 1 a posteriori. the other three attributes carry less importance  especially the pupil-teacher ratio. the nomogram also exposes how different attribute values affect the outcome; for instance  the value of housing goes up when the employment centers are nearby. note that we can include continuous as well as discrete attributes in the nomogram. the nomogram also clearly exposes the  neutral  values of the attributes near 1 on the log or axis. they do not affect the probability of the outcome. if a particular attribute value is not given for the test instance  it is these neutral values that will be effectively imputed.
모nomograms - like the one from our example - are used to assess the probability of the observed outcome  where the effects of the attributes are independent given the class and are added up to form the final prediction. assume an in-figure 1: a nomogram of the svm model that predicts the probability of costly housing in a given boston area. the dots illustrate the classification of a specific instance.
troduce in the paper use a similar presentation as those of harrell for logistic regression. to illustrate the general idea  consider the nomogram in figure 1 which represents a linear svm model induced from the boston housing data set  statlib  http://lib.stat.cmu.edu/datasets/  also see  . the housing data set consists of 1 different instances  areas of boston ; about 1% of the areas have the median value of housing price lower than $1.
모for convenience of this presentation we use only four representative attributes: the average number of rooms per dwelling  rooms   weighted distances to five boston employment centers  employment dis   pupil-teacher ratio by town  pupil-teacher  discretized to two nominal values   and proportion of lower status population  low status  discretized to four nominal values . there are two classes of areas  the expensive with the median values above $1  and the cheap. to make a prediction using a nomogram  the contributions of attributes on the scale of the log odds ratios   topmost axis of the nomogram  are summed up  and used to determine the probability whether the price is less than $1  bottommost axis of the nomogram . for instance summing the effects of 1 rooms per average dwelling  unknown distance from employment centers  a high pupilteacher ratio and a high rate of lower status population results in the log odds ratio 1 + 1 + 1 + 1 = 1 on the 'log or sum' axis. this sum is then projected to the bottommost 'p  = 1 ' probability axis  yielding the final probability of the target class of approximately 1. on the other hand  if the area was known to be far away from employment centers  1   employment dis contribution to final sum would be around 1 instead of 1  and the final probability would be higher than 1.
모besides prediction  nomograms provide a clear and comprehensive presentation of the underlying model. our svm nomogram from fig. 1  for instance  clearly exposes that the housing values in boston from a particular data set are most associated with the average number of rooms. the corresponding line in the nomogram is the longest  and trying to predict housing values for a certain area simply with the information that the average number of rooms is small  1   the probability for price under $1 jumps from stance  x y   where the range of the label is assumed to be
  and x is described by a set of attributes a = {a1 a1 ... ak}. the nomogram can visualize a probability function of the type
k
	p  y = 1|x  := f 붹1 +	fj aj x  	 1 
j=1
where 붹1 is the intercept  a constant delineating the prior probability in the absence of any attributes  fj is an effect function that maps the value of an attribute a for the instance x into a point score  and f is the inverse link function that maps the response of an instance into the outcome probability. the nomogram in fig. 1 is based upon one effect function for each attribute. each line in the nomogram corresponds to a single attribute  and a single effect function. because the effect function flow status high  = 1  the tick corresponding to the value 'high' for the attribute low status is aligned with 1 on the 'log or' axis.
모the class of models of the above type are the generalized additive models  gam   . when each effect function is linear  we speak of a generalized linear model  glm . for a glm  the response or the systematic component is written as 붹1 + j 붹 j x j  where  x j is the j-th coordinate of the vector x. using the dot product we may express it more simply as. we may refer to the vector 붹 as the effect vector.
모we start by showing how support vector machines based on appropriate kernels can be decomposed into the above additive model. to enable the use of the nomograms for support vector machines  we need them to predict outcome probabilities. the basic svm alone does not attempt to model the probability  but attempts to achieve the separation of instances in the feature space with a separating hyperplane  each side of which represents a different class. therefore  the effect functions need to be calibrated and thus placed on the log odds ratio scale. in the experimental section we examine the performance of linear svm in comparison to other methods that can be visualized with nomograms. we also compare linear svm to the svm with the rbf kernel  which cannot be visualized with a low-dimensional nomogram  observing that the losses are not very large. we also show that nomograms are suitable for graphically comparing support vector machines to other generalized additive models  such as the na몮 ve bayesian classifier and logistic regression.
1. methodology
1 overview
모not every support vector machine is appropriate for visualization using a nomogram. the first requirement is the ability to additively separate the contribution towards the response of an individual attribute or of a small group of attributes as in  1 . we achieve this goal by using a kernel of a particular family. the second requirement is related to how we represent the lack of information: ideally  zero value of the transformed attribute should indicate missing attribute values.
모our approach to visualization will take the following steps  which will be addressed in detail in the subsequent sections.
1. transform each instance x into the feature space using a decomposable kernel map 붯 :.
1. train the support vector machine using the dot product kernel  and obtain the hyperplane's normal w and bias b.
1. employ univariate logistic regression to obtain two parameters   and 붮  so that
logit 
is well-calibrated. obtain the effect vector and the intercept on the log-odds scale by elementary algebraic manipulation of the above. this step is only necessary to assure that the log-odds scale can be used in the nomogram.
1. collect the terms of the effect vector that belong toa specific attribute. this results in a possibly nonlinear effect function f a x   for each attribute a. it is also conceivable to visualize effect functions of multiple attributes simultaneously. for example  the joint effect of attributes a b and c would take the form f a x  b x  c x  . such joint effect functions are useful when the attributes in the group interact.
1. visualize the effect functions and the intercept in anomogram.
a simple example. as an example  we have taken the fisher's iris data  selected the i. versicolor and i. virginica species  and the petal length and the petal width attributes. we have trained the svm model with a linear kernel with both attributes standardized  and have used the cross-calibration to obtain probabilistic outputs from svm. the top-down visualization of the data and the model to the left of fig. 1 should be familiar. the true svm's separating hyperplane would lie at the contour corresponding to the probability of approximately 1. the hyperplane is specified with the equation 1length + 1width = 1. the vector  1 1 t can be understood as a weight vector  but we should note that it depends on the scaling of the attributes: it should not seem that the width is more than twice as important as the length. on the other hand  the nomogram shown to the right of fig. 1 clearly shows the effect of individual attribute values on the outcome. unlike the top-down view which is restricted to two attributes and two dimensions  we can include a larger number of attributes in the nomogram without increasing the dimensionality.
1 the kernel map
모support vector machines can be applied purely with a kernel function  and the resulting gram matrix. however  nomogram visualization requires us to concern ourselves with the kernel map from the instance space x into the feature space h using the reproducing kernel map 붯 :. first we will discuss various kernel maps.
later we will describe the notion of a decomposable kernel  which is limited by the number of dimensions required for visualizing the resulting classifier in the form of a nomogram. 1.1 non-linear univariate kernel maps
모all attributes need to be transformed into real-valued variables before a model can be trained. we standardize continuous attributes so that zero implies the mean  and 1 implies one standard deviation distance from the mean. some m-th coordinate of the linearly transformed instance  붯 x  m equals the standardized value of a continuous attribute  a x  뷃a /효. svm based on a purely linear kernel and logistic regression both have linear effect functions  and can therefore be seen as generalized linear models.
모however  attributes may have non-linear effects on the outcome. for example  both very high and very low body temperatures indicate risks when predicting the health status  and this pattern cannot be captured by a single realvalued variable. we can allow for the nonlinear effect functions by employing non-linear kernel maps. this way  a single attribute is internally represented with more than one dimension  and the actual support vector machine can be trained using the dot product on the feature space h  but not on the attribute space.
모the simplest example of a non-linear map relates to handling multi-valued nominal attributes. a discrete attribute b with v values is transformed into a set of v features  x m  x m+1 ...  x m+v  1  so that given the value of b = bv+1   x m+v = 1 and  j = 1 ... v   1 v + 1 ... v :  x m+j = 1. thus  the kernel map assigns its own dimension to each attribute value  and also provides ground to interpret setting all corresponding  x m+j to zero as a missing value.
모the same concept can be applied to continuous attributes. using discretization  we convert a continuous attribute x into a v -valued discrete one  each value of which corresponds to an interval of the range of x. this is an extremely simple method for handling nonlinear effects. for example  we could discretize the body temperature into a 1-valued nominal attribute with the range { 1 1-1  1}. the corresponding effect vector  b1 b1 b1 t is obtained from svm  and the effect function f x  then takes the following form:
	b1	; x   1
f x  = b1 ; 1 뫞 x 뫞 1 b1 ; x   1
discretization essentially involves modelling the effect of an attribute with a piecewise-constant function.

figure 1: a top-down projection of svm  left  as applied to fisher's iris data  separating the i. virginica  top-right  from i. versicolor  bottom-left . the probability contours resulted from cross-calibration using a logit link function. the support vectors are marked. the above model can be summarized in the form of a모using polynomialization we transform a continuous attribute x into a vector of features  . the corresponding effect vector  a1 a1 ... ad  results in a polynomial effect function for x: f x  = a1x + a1 + ... + adxd. other forms of transforming continuous attributes may be employed while maintaining the dot product kernel. such functions can be easily rendered inside the nomogram  so nomogram  right .
that f x  is shown on the horizontal axis  and x on the vertical axis or using a label.
모in addition to discretization and polynomialization  other univariate expansions can be employed  such as piecewiselinear functions  splines  sigmoids  or even univariate radialbasis functions.
visualizing non-linear effects. the effects of continuous attributes can be shown on a single line as in figs. 1 and 1. however  non-linear effect functions  especially nonmonotonic ones  could be confusing if presented in such a way. an alternative approach is illustrated in fig. 1  where the effect of an attribute is presented in the form of a twodimensional graph. the vertical dimension is used to list different values and the horizontal dimension shows the effect of the value on the outcome. the graph reveals how the attribute's impact on the outcome probability gradually changes as its value changes from the lowest to the highest interval. this kind of presentation is also suitable for ordered discrete attributes.
모we have used the 'horse colic' data set from the uci repository . among the attributes  we have chosen the respiratory rate and the body temperature  as they are continuous attributes with potentially non-linear effects. it is clear that there is a particular range of normal body temperatures centered near 1  with low risk. the deviations in any direction  fever  hypothermia  carry increased risk. the pulse appears more monotonic  but the effect of the pulse is distinctly non-linear with respect to the pulse scale.
모the intervals for the piecewise constant effect function were set manually. the rbf effects are defined through 1 radial bases  covering separate intervals of an attribute's range.
1.1 decomposable kernels
discretization and polynomialization correspond to nonlinear kernels  but the non-linearity is always restricted to within a single attribute. we will now employ an example to show why general non-linear kernels introduce problems for nomogram visualization. let us focus on the quadratic kernel . specifically  for an instance x =  x1 x1 t
in a two-dimensional continuous attribute space  we can introduce the following kernel map 붯 x.
then the quadratic kernel can be linearized :

we can see that in addition to polynomializing each attribute to the degree of 1  the quadratic kernel introduces interactions involving each pair of attributes and the label  corresponding to the coordinates x1 and x1. the effect function would take the form of f x1 x1  so that one attribute's effects only appear in a single place. otherwise  the effect of x1 would appear under  x 1 corresponding to x1  under  x 1 corresponding to  and under the intercept term. such effect functions are more difficult to be effectively visualized in two dimensions: the effect of x1 depends on the value of x1. such visualization would involve simulating the third dimension either with color or with shape on a twodimensional computer monitor.
모of course  non-linear kernels can be used for nomogram visualization as well  under some restrictions. we will now describe a general form of a kernel suitable for visualization. assume a partitioning of the set of attributes x into m disjoint subsets s1 s1 ... sm  so that  and
모. the underlying assumption is that all the interactions between attributes happen within each subset si  but not across the subsets. we can then visualize any svm based on such a kernel k that is expressible in terms of such a sum:

figure 1: a nomogram with the univariate rbf effects  left  and the piecewise constant univariate effects
 right  in the 'horse colic' data set. we can observe that increased pulse and increased or decreased body 	 1  temperature all indicate an increased risk of death.
a kernel that can be expressed in such a way will be referred to as an  additively  decomposable kernel. here  k빡 si is an arbitrary nonlinear positive semidefinite sub-kernel that acts upon  x si  the subset of the coordinates of x that correspond to the attributes in si. the full reproducing kernel hilbert space is then a concatenation of the reproducing kernel hilbert spaces for each k빡 si. it is then possible to retrieve the the effect functions f si   localized for each subset of interacting attributes. the dimensionality of the resulting nomogram visualization is maxi |si| + 1. the kernel  1  is a special case of the kernels proposed by  1  1  1 . in particular   motivated the choice of these kernels through the ability to effectively visualize them. visualizing interaction effects. fig 1 shows the comparison between models that use interactions in the learning phase and models that do not. if no interaction is assumed  we employ the linear kernel. if an interaction between attributes a and b is assumed  we employ the following subkernel:
	1	; otherwise
we have used german credit risk data set that contains 1 attributes and 1 past applicants. each applicant was classified according to the risk  high vs. low . the risk is low if the applicant is very likely to return the money  and high otherwise. due to space restrictions  we have used only 1 attributes: the status of applicant's account in the bank  duration of the credit in months  purpose of the credit  the amount of credit asked for  the duration of the applicant's present employment  and applicant's duration of residence.
the joint effect can be illustrated in the nomogram by picking one attribute as the 'control'  a condition. the other attribute's influence is then interpreted in the context of the control. we examined the effect of employment duration  controlling for residence duration. this pair of attributes in 'german-credit' has been identified as significantly interacting  using the methodology of interaction analysis .
모both nomograms are very similar when comparing the first four attributes. among those four  the most influential attribute is the purpose of the credit: buying used cars incurs low risk  and education incurs high risks. the difference between two model occurs in the effect of unemployment on the risk: without the interaction  the model regards unemployment as almost unimportant  while the second regards it as highly important for determining low risk  if residence duration is higher than 1 years  and for determining high risk  if residence duration is less than 1 years . it is probable that people that live at this place for more than 1 years are unemployed because they do not need or can not work  i.e. are retired  while people that are residents for less than 1 years are unable to find a job. this comparison stresses the importance of interactions and shows that they can be effectively visualized with nomograms.
모through this more complex example we show that apart from revealing the structure of the svm classifier  nomograms may be used as a data mining tool to depict different properties of problem domains. gunn and kandola  present examples of how interactions of real-valued variables can be visualized using 1d plots  but not in the context of the nomograms.

figure 1: a nomogram with the linear kernel  left  and the non-linear kernel that assumes a joint effect function for the employment duration and residence duration attributes  right  in the 'german-credit' data set. we can see that controlling for the residence duration amplifies and differentiates the effects of employmentduration on the assessment of credit risk.
1 distance from the separating hyperplane
모given n training instances  x j  y j    j = 1 ... n  the resulting support vector model can be described with the vector 붸 and the bias b. the  signed  separating hyperplane distance of an instance  x y  is denoted as 붻 x . given a dot product kernel  the distance be described as:
		 1 
here  b is the bias  while the scaling constant 뷉 assures that the distance is euclidean. the sign of the hyperplane distance indicates the predicted value of the label.
모because of the kernel map 붯  we work in the feature space with the dot product kernel. we can now remove the reference to support vectors x 몫   and represent the distance with the bias b and the vector w. we now define the weight vector w. it has the same dimensionality as the feature space  and is defined as:
n
	w 	 1 
=1
the length of the weight vector w thus obtained is not 1. for that reason we needed the normalizing constant 뷉 in  1   where. the signed hyperplane distance of an instance x can thus be expressed as
		 1 
to simplify further notation  .
the w is the separating hyperplane normal.
모if we are working with a non-linear kernel of the form  1   it is easy to see that a particular coordinate  w corresponds to one or more attributes  but only within a single group si. of course  multiple coordinates can correspond to a single attribute if nonlinear univariate sub-kernels are used  and one coordinate may correspond to multiple attributes if nonlinear multivariate sub-kernels are used. this way   1  can be written as:
		 1 
therefore  each sub-kernel k빡 si is independently linearized by 붯빡 si. the approach works even if k빡 is an rbf kernel involving a potentially infinite-dimensional reproducing kernel hilbert space: the dimensionality of 붯빡 si  x si  will depend on the size of the data set  but will be finite for a finite data set.
1 cross-calibration
모the horizontal scale in nomogram-based visualizations is based on the probability of the label. however  the signed hyperplane distance 붻 x  of an instance x has no probabilistic meaning. this is the role of the link function. the link function connects probability  the random component  with the response  the systematic component . the link function in classification maps a probability p into a response d. the inverse link function f instead maps a response d into a probability. the most frequently used link functions are the identity p  = p  probit  the inverse of the cumulative gaussian distribution  and logit p  = log p/ 1 p  . the inverse logit link function is f d  = 1/ 1 + expd   and it has been used in the past .
모while the logistic regression too employs a generalized linear model with the logit link  the effect vector 붹 is optimized directly in order to minimize the probabilistic loss  deviance  of the resulting model. the hyperplane distance 붻 does not attempt to optimize the calibration performance using the logit link  merely achieve the separation. for that reason  platt linearly transforms the svm output with two additional parameters  붮 and    using a procedure that resembles univariate logistic regression with the hyperplane distance acting as the independent variable  and the label as the dependent variable. the two parameters ensure that f d  based on d = 붮붻 x  +   is a well-calibrated probabilistic classifier using the logit link.
모it often happens that the separation of the support vector machine on the training set is perfect. in such a case  the inverse of the logistic link will tend towards a step function. however  on a separate test set  the same performance is rarely as good. for that reason platt  proposed performing internal cross-validation where the training set is partitioned into two sets of instances  one is used for svm training  and the other for learning the parameters 붮 and  . the error arising from generalization is thus accounted for: the two parameters capture the uncertainty associated with generalization to unseen data.
모there are two parameters to such a calibration procedure. the first parameter is the data hiding protocol used for separating training from test data. for example  for 1fold cross-calibration  1% of the data is used for training and 1% remains hidden for calibration. the more data we hide  the more conservative are our predictions. the second parameter is the number of replications. a single crosscalibration depends on a particular shuffling of instances. to remove this dependence  the cross-calibration procedure should be replicated as many times as it is practical.
with the logit link the end result can be represented as
		 1 
if we apply logistic regression to the problem of associating the hyperplane distance with the label  we find such values of   and 붮 that maximize the thus defined conditional loglikelihood of y given x in the above model across the n training instances  x j  y j  :
n
	  붮 = argmax	p  y = y j |  붮 붻 x j   	 1 
  붮 j=1
모the calibrated response function on the log odds ratio scale is d x  =   + 붮붻 x . we can now map these symbols   붮 w and b so that they will correspond to the notation of a  linearized  generalized additive model  1  based on the intercept 붹1 and the effect vector 붹. the intercept 붹1 marks both the outcome probability of 1 and the log odds ratio of 1  so it can be seen as probabilistically calibrated bias b. the k-th coordinate  붹 k of the effect vector corresponds to the probabilistically calibrated coordinate of the normal  w k. the mapping is as follows:
	붹1 =   + 붮b 	 붹 k = 붮 w k	 1 
the linear effect function for the set of attributes si is simply
. both fk and 붹1 are on the log
odds ratio scale  and they can thus be directly presented in a nomogram. it is important to distinguish the weight vector w  the hyperplane normal w  the glm effect vector 붹   and the lagrange multipliers 붸: all are different.
1. model comparison
모in this section  we examine the performance of support vector machines in comparison to other methods that can be visualized with the nomograms. to address this  we compare the nomogram-based probability estimations with those obtained from svm with rbf kernel  did we lose anything assuming the decomposability into effect functions   and two popular methods for probabilistic classification  namely logistic regression and the na몮 ve bayesian classifier  what is the overall performance in class probability prediction  . nomograms may be used to study the differences between various modelling methods from the family of generalized figure 1: a general scheme of a cross-calibration procedure  based on n folds  r replications  the response learning algorithm l  the calibration learning algorithm c  and the training data t . r 뫹   {calibration training set.} for all r : 1 뫞 r 뫞 r do {for each replication} f1 뫋 f1 뫋 ... 뫋 fn 뫹 t {generate folds.} for all n : 1 뫞 n 뫞 n do {for each fold}
 {train.}
 i 
for all x 뫍 fn do {for each test instance}
모모r 뫹 r 뫋 붻  x i   y j  {record the distance.} end for end for
end for
 {hyperplane distance.}
 {calibrated prob.}
additive models. we present a nomogram-based comparison of svm and the na몮 ve bayesian classifier model.
1 accuracy
모as for earlier nomograms  all experiments were performed within the orange toolkit . we employed libsvm  with default settings for training the svm classifiers  and iteratively re-weighted least squares fitting  of the logistic regression model  as implemented in the orange extensions package . we experimented on 1 well-known uci  data sets with a binary outcome. for data sets with more than 1 examples  'mushroom' and 'spam base'  we have selected a stratified random subset of 1 examples which were used throughout the experiments.
모we evaluated each method on three criteria: classification accuracy  outcome probability estimation  as measured by brier score  the mean square error of predicted class probabilities given the true class probabilities for each instance    and the area under the receiver operating characteristic. table 1 compares the na몮 ve bayesian classifier  nb   logistic regression  lr   support vector machines with rbf kernels  svm   and support vector machines with a linear kernel  dot and dot'  on each of these three criteria. the first six data sets  the upper part of the table  include no continuous attributes. elsewhere  the continuous attributes were discretized for nb and dot' into 1 intervals with approximately equal number of examples for each discretized value  as to provide the capacity for handling nonlinear effects. in computation of the brier score  the predicted probabilities were calibrated for all methods  except for logistic regression  which is considered not to require calibration . note that brier score measures the loss  so lower values are better than higher.
모the observed methods perform similarly  with some exceptions. for instance  linear svm performs poorly on 'ionosphere' unless the attributes are discretized. this indicates non-linear attribute effects in this data set  and we illustrate an example of them in fig. 1. the svm using the rbf kernel captures this nonlinearity better than any method based on discretization. an unexpectedly good performer is the na몮 ve bayesian classifier  which achieved good probability estimation results and reasonable ranking results.
classification accuracybrier scorearea under rocnblrrbfdotdot'nblrrbfdotdot'nblrrbfdotdot'breast  lju 111111111111breast  wsc 111111111111mushroom111111111111shuttle111111111111titanic111111111111voting111111111111australian111111111111111german111111111111111hepatitis111111111111111horse-colic111111111111111housing111111111111111ionosphere111111111111111liver111111111111111pima111111111111111post-op111111111111111spam base111111111111111table 1: comparison of the na몮 ve bayesian classifier  nb   logistic regression  lr   svm with the rbf kernel  rbf   svm with the linear kernel  dot  and linear svm with discretization  dot'  on several ucidata sets.
모since our paper shows how to visualize svm with linear kernels  it is of interest how much performance needs to be given up by not using the more powerful rbf kernels. as expected  svm with rbf kernels generally performs best of all methods. nonetheless  the difference between svm with rbf and dot kernels is only a few percent  except in the already mentioned 'ionosphere' . we expected that the discretization would alleviate the linear restrictions of the model  but experimental results  dot vs dot'  do not confirm that. still  dot' provided a considerable improvement in the 'ionosphere' and 'liver/bupa' data sets. this indicates that the non-linearities appear only in certain data sets. we need to apply the more sophisticated models and visualizations only if the non-linearity is justified through a higher classification performance.
1 comparing models with nomograms
모judging from the experimental comparison of svm to other machine learning techniques  svm sometimes achieves worse results on brier score while having comparable classification accuracy at the same time. 'shuttle' and 'titanic' are examples of such data sets. the reason for the problem can be easily explained with a nomogram. we will compare the na몮 ve bayesian classifier  nb  and svm to predict the probability for passenger's survival of the hms titanic disaster. the nb nomogram  in fig. 1  the data set was obtained at http://hesweb1.med.virginia.edu/biostat/s/data/   includes three attributes: the passenger status  first  second  and third class  or a crew member   the age  adult or child   and the sex of the passenger.
모for nb  the attribute with the biggest potential influence on the probability of survival is gender of the passenger: being female increases the chances of survival most  log odds of 1   while being male decreases the odds  log odds of about  1 . of the three attributes  the age is apparently the least influential  although children had a higher probability of survival. most lucky were the passengers of the first class for which - considering the status only - the probability of survival was much higher than the prior. comparing this nomogram to the svm nomogram in fig. 1 of 'titanic'  we observed a very interesting difference between them. svm 

figure 1: the nonlinearities in the 'ionosphere' data set. two features were used for each attribute  representing an rbf basis.
as it is known  aims to optimize the classification accuracy and considering this it induced a model that predicts survival of a passenger by considering only the sex attribute. both methods  nb and svm  consider this attribute as very important  but unlike nb  svm disposes of age and status as completely irrelevant attributes. using only the sex attribute  svm achieves comparable classification accuracy  but the fidelity of the outcome probability estimates are slightly worse  as measured by brier score.
1 interpretation of effect functions

figure 1: a na몮 ve bayesian nomogram  left  and the svm nomogram for the 'titanic' data set.모the role of the nomogram is to visualize the probabilistic predictions of a support vector machine without losing any information. the effect function is a full representation of the contribution of an individual attribute towards the probability of the outcome. the visualization is not a partial approximation to the svm model: instead it captures the svm model completely and exactly. clearly  our intuitive conception of attribute 'importance' might not match that of the effect function. there are other ways of interpreting attribute importance that do not correspond to effect functions  perhaps the most popular of which is mutual information.
모there are some pitfalls to interpreting the importance of attributes derived from the nomogram-based visualization of a support vector machine. we distinguish two distinct situations:
  if there are several highly correlated attributes  it is difficult to distinguish the individual effect of any attribute in particular. instead  the effect functions will somewhat arbitrarily allocate the net effect among the attributes. if we hold the value of one attribute constant  the effects of other correlated attributes will decrease. this problem is referred to as a negative interaction or as attribute redundancy . we illustrated this on an example in fig. 1  where all attributes became irrelevant once the sex attribute was accounted for. it does not mean that the other attributes are irrelevant  just that sex takes their credit too.
  in some cases  an attribute a appears to have no effect. however  if we control the value of another attribute b  the effect of a will increase. the example of this phenomenon of a positive interaction or attribute synergy  are the familiar xor and parity problems. we illustrated this on an example in fig. 1  where the influence of the employment attribute increased if residence duration was controlled for.
1. conclusion
모we have shown that support vector machines can be effectively visualized even in attribute spaces with many dimensions  using nomograms. namely  individual attributes are stacked vertically in a nomogram  packing multiple dimensions into a single one. we have described the methodology for converting a support vector machine into the form of a generalized additive model. furthermore  we have extended the form of a nomogram with two-dimensional graph representations of a nonlinear and non-monotonic effect function  as we have seen in sect. 1. in addition to nonlinear univariate effects  we also show how interactions between attributes can be modelled and visualized.
모we did not discuss the problem of determining what decomposable kernel to use in detail. there are three ways of addressing this. first  interaction analysis  is a heuristic that can aid the construction of kernels that capture the interactions. secondly  we can see it as an issue of model selection. finally  it is possible to express a preference for sparse and smooth kernels as a part of the optimization problem  combining the quest for decomposability and the actual learning  1  1 .
모with the example of sect. 1  we pointed out that nomograms may be the right tool for experimental comparison of different models and modelling techniques  as it allows to easily spot the similarities and differences in the structure of the model. furthermore  we can use nomograms to outline possible weaknesses of models  such as those of linear models by comparing them to the models obtained on discretized data.
모kdd practitioners are often concerned with data sets that contain hundreds or thousands of attributes. nomograms have no inherent problems with such situations: the dimensionality of the visualization depends on the structure of interactions  not on the number of attributes. to simplify the interpretation  the attributes should be arranged by importance  and the more important attributes would be examined first. nomograms provide a measure of importance that is based on the length of the effect line: it indicates the range of the effects provided by the attribute. although this measure should be weighted by the frequency of individual attribute values  it it nonetheless intuitive and useful.
모an interesting question is also the stability of the model. the effect of a particular attribute can be thought as an uncertain quantity. to present the uncertainty  we can employ the notion of an error bar or a confidence interval. we obtain the error bars by training a separate svm model for each bootstrap resample of the original data. each separate model results in an effect function  and for each value of the attribute  we can obtain the lower and upper bound of the effect across the resamples. this yields the effect error bar.
모finally  all that was said about classification applies also to regression. the only difference is that the range of the dependent variable replaces the log-odds  and that the calibration is not required.
1. acknowledgements
모the authors are grateful to j. brank for helpful advice. this work was supported by a grant from the slovene ministry of education  science and sports and the ist programme of the european community under sekt semantically enabled knowledge technologies  ist-1-ip  and pascal network of excellence  ist-1 . this publication only reflects the authors' views.
