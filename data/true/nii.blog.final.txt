ranking blog posts that express opinions regarding a given topic should serve a critical function in helping users. we explored three types of opinion retrieval methods in the framework of probabilistic language models. the first method combines topic-relevance model and opinion-relevance model that captures topic dependence of the opinion expressions. the second method makes use of probability that any of opinion-bearing words appear in each target document as document prior probability in query-likelihood model. the third method makes use of probability that any of adjectives or adverbs appear in each target document as document prior probability  assuming opinionated documents tend to contain more adjectives or adverbs than other documents.
1 introduction
the recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data. the field of sentiment classification has recently received considerable attention  where the polarities of sentiment  such as positive or negative  were identified from unstructured text . a number of studies have investigated sentiment classification at document level  e.g.   1  1   and at sentence level  e.g.   1  1  1 ; however  the accuracy is still less than desirable. therefore  ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users.
¡¡for this objective  eguchi and lavrenko  proposed sentiment retrieval models  aiming at finding sentences containing information with a specific sentiment polarity on a certain topic  where the topic dependence of the sentiment was considered. intuitively  the expression of sentiment in text is dependent on the topic. sentiment polarities are also dependent on topics or domains. a couple of examples follow. a negative view for some voting event may be expressed using 'flaw'  while a negative view for some politician may be expressed using 'reckless'. as another example  the adjective 'unpredictable' may have a negative orientation in an automotive review  in a phrase such as 'unpredictable steering'  but it could have a positive orientation in a movie review  in a phrase such as 'unpredictable plot'  as mentioned in  in the context of his sentiment word detection. eguchi and lavrenko's sentiment retrieval models can address both cases based on the framework of generative language modeling  not only assuming query terms expressing a certain topic  but also assuming that the sentiment polarity of interest is specified by the user in some manner.
¡¡for the trec 1 blog track  we first followed   but we set aside the topic dependence of the sentiment polarities and focused on that of the sentiment expressions since the evaluation criteria of the blog track did not distinguish the sentiment polarities. in   sentence level was focused in the experiments; however  the model can be applied to textual chunks of any length. we also explored the use of some document features as document prior probabilities in query-likelihood model .
1 a generative model of opinion
1 definitions
according to   we start by providing a set of definitions that will be used in the remainder of this section. the task of our model is to generate a collection of statements w1...wn. a statement wi is a string of words wi1...wini  drawn from a common vocabulary v. we introduce a binary variable bij¡Ê{s t} as an indicator of whether the word in the jth position of the ith statement will be a topic word or an opinion-bearing word. for our purposes  bij is determined heuristically  automatic annotation   in this paper.
¡¡as a matter of convenience we will often denote a statement as a pair {wis wit}  where wis contains the opinion-bearing words and wit contains the topic words. as we mentioned above  the user's query is treated as just another statement. it will be denoted as a pair {qs qt}  corresponding to opinion-bearing words and topic keywords. we will use p to denote a unigram language model  i.e.  a function that assigns a number p v ¡Ê 1  to every word v in our vocabulary v  such that ¦²vp v =1. the set of all possible unigram language models is the probability simplex ip. we define ¦Ð : ip¡Áip¡ú 1  to be a measure function that assigns a probability ¦Ð p1 p1  to a pair of language models p1 and p1.
1 generative model
using the definitions presented above  and assuming that ¦Ð   is given  we hypothesize that a new statement wi containing words wi1...wim can be generated according to the following mechanism.
1. draw pt and ps from ¦Ð ¡¤ ¡¤ .
1. for each position j = 1...m:
 a  if bij=t: draw wij from pt ¡¤  ;  b  if bij=s: draw wij from ps ¡¤  .
the probability of observing the new statement wi1...wim under this mechanism is given by: m  
	x	y pt wij  if bij=t
	pt p¦Ðs pt ps  j=1 ps wij  otherwise	 1 
we use this simple equation instead of that in  since we can set aside sentiment polarities in this paper. the summation in equation  1  goes over all possible pairs of language models pt ps  but we can avoid integration by specifying a mass function ¦Ð   that assigns nonzero probabilities to a finite subset of points in ip¡Áip. we accomplish this by using a nonparametric estimate for ¦Ð    the details of which are provided below.
1 using the model for retrieval
the generative model presented above can be applied to opinion retrieval in the following fashion. following   we start with a collection of statements c and a query supplied by the user  where qs can be some typical opinion-bearing words with either positive or negative polarity and qs can be words in the title field in the topic given by the blog track organizers. we use the procedure outlined in section 1 to estimate the topic- and opinionrelevance models corresponding to the user's information need  and then determine which statements in our collection most closely correspond to these models of relevance. the topic-relevance model rt and opinion-relevance model rs are estimated in the similar fashion described in  for each query {qs qt}. once we have estimates for the topic and sentiment relevance models  we can rank testing statements w by their similarity to rt and rs. we rank statements using a variation of cross-entropy  which was proposed by  and modified for sentiment retrieval task in :
.
 1 
here the summations extend over all words v in the vocabulary. a weighting parameter ¦Á allows us to change the balance of topic and sentiment in the final ranking formula; its value can be selected empirically.
1 opinion retrieval task
1 using opinion-relevance models
we define a variation of the sentiment retrieval model . as input  we used  1  a set of topic keywords qt and  1  a set of opinion-bearing seed words qs. since we did not have a training data set  all the model parameters were the same as used in . these model parameters are not very appropriate for the opinion retrieval experiments in the blog track  as we describe later in this paper.
¡¡we detected opinion-bearing words using lists of words. we used sentiment word list contained in opinionfinder   which consists of 1 positive and 1 negative words. we extracted opinionbearing expressions using the list of words above.
1 other models
nii1: as a baseline  we carried out experiments using indri . entire corpus with blog documents was indexed. the topics were used as queries and top 1 documents were retrieved using query likelihood approach on the indri platform.
nii1: as another baseline  we used  topic-  relevance model   which was estimated using the  weighted  mixture of each model of a certain number of top-ranked documents. we used the result of the baseline run of nii1  and re-ranked them using the topic-relevance model.
nii1: this is the retrieval model as described in section 1. we used the result of the baseline run of nii1  and re-ranked them using this retrieval model.
table 1: mean average precision of our official runs
runidopinion-relevancetopic-relevancenii1.1.1nii1.1.1nii1.1.1nii1.1.1nii1.1.1nii1: we obtained a list of opinion-bearing words and used it to find out the document prior probability in the language modeling framework. this probability was calculated by finding the total number of opinion-bearing words in a document and dividing it by the total number of words in that document. this probability was multiplied by the query likelihood probability. the query likelihood probability was obtained from the baseline run of nii1.
nii1: we made use of probability that any of adjectives or adverbs appear in each target document as document prior probability in addition to the query-likelihood model on the indri platform  assuming opinionated documents tend to contain more adjectives or adverbs than other documents.
1 results and discussions
according to the relevance judgment results  nii1 and nii1 did not work  unfortunately. a more detailed investigation on these models is in progress  though. as for nii1 and nii1  we used the model parameters estimated in   where sentence-level retrieval experiments were performed  because we could not use training data to estimate the model parameters at document level of blogs. this setting was not appropriate for blog-post retrieval  and so the performance of nii1 and nii1 was not as good as that of nii1. we are currently investigating on all potential causes of the performance problems  such as model parameters and indexing conditions. using the relevance judgment data given by the organizers  we are planning to estimate the model parameters appropriately for the task defined in the trec blog track  and to perform the additional experiments to investigate how the topic-sentiment relevance model actually works at the appropriate setting.
acknowledgments
this work was supported in part by the grant-in-aid for scientific research  #1  #1 and #1  from the ministry of education  culture  sports  science and technology  japan. any opinions  findings and conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect those of the sponsor.
