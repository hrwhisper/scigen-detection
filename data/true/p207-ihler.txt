time-series of count data are generated in many different contexts  such as web access logging  freeway traffic monitoring  and security logs associated with buildings. since this data measures the aggregated behavior of individual human beings  it typically exhibits a periodicity in time on a number of scales  daily  weekly  etc.  that reflects the rhythms of the underlying human activity and makes the data appear non-homogeneous. at the same time  the data is often corrupted by a number of bursty periods of unusual behavior such as building events  traffic accidents  and so forth. the data mining problem of finding and extracting these anomalous events is made difficult by both of these elements. in this paper we describe a framework for unsupervised learning in this context  based on a time-varying poisson process model that can also account for anomalous events. we show how the parameters of this model can be learned from count time series using statistical estimation techniques. we demonstrate the utility of this model on two data sets for which we have partial ground truth in the form of known events  one from freeway traffic data and another from building access data  and show that the model performs significantly better than a non-probabilistic  threshold-based technique. we also describe how the model can be used to investigate different degrees of periodicity in the data  including systematic day-of-week and time-ofday effects  and make inferences about the detected events  e.g.  popularity or level of attendance . our experimental results indicate that the proposed time-varying poisson model provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity.
categories and subject descriptors
i.1  pattern recognition : models-statistical; g.1  probability and statistics : probabilistic algorithms
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  philadelphia  pennsylvania  usa.
copyright 1 acm 1-1/1 ...$1.
general terms
algorithms
keywords
poisson  markov modulated  event detection
1. introduction
　analyzing and understanding patterns of human behavior over time is an area of increasing interest in a number of different data mining applications. examples include analysis and understanding of web access logs  event detection and prediction with vehicular traffic and accident data  and classifying human activities from low-cost observation modalities used for ubiquitous sensing such as rfid  video  et cetera. in this paper we focus on time-series data where time is discrete and n t  is a measurement of the number of individuals or objects recorded over the time-interval  t 1 t . for example  an optical sensor at a door might report an estimate of how many people have entered a building over a 1-minute period  or an inductive loop sensor on a freeway might report an estimate of how many vehicles have passed over the sensor in the previous 1 minutes. since this type of data measures the aggregated behavior of many individuals  it typically exhibits a temporal periodicity on many scales  daily  weekly  etc.  reflecting the rhythms of underlying human activity. it is often also corrupted by sustained  bursty  periods of anomalous behavior  which we will refer to in this paper as events. note that the term event is sometimes used in the time series literature to refer to individual measurements  e.g.  the recording of a single person walking through a door at a particular time . here  however  we will use event in a different manner  to refer to a largescale activity that is unusual relative to normal patterns of behavior  such as a large meeting in a building  a malicious attack on a web server  or a traffic accident on a freeway.
　to fully understand such data  we often care about both the patterns of the typical and predictable behavior  and detecting and extracting information from the deviations from this behavior. however  this leads to an inherent  chicken and egg  deconvolution problem  since detecting anomalous periods of time requires some knowledge of what constitutes normal behavior  but our historical data consists of both normal and anomalous  event  data mixed together.
　as an example  figure 1 shows counts of the estimates number of people entering a building over time from an optical sensor at the front door of a uci campus building. the

figure 1: jittered scatterplot of the number of people entering on any weekday over a four-month period  shown as a function of the time of day  in halfhour intervals . although certain points  e.g.  set a  are clearly  outliers  and represent unusual events with greater than normal attendance  it is less clear which  if any  of the values in set b represent something similar.
data are  jittered  slightly by gaussian noise to give a better sense of the density of counts at each time. once again  there are parts of this signal which are clearly periodic  and other parts which are obvious outliers; but there are many samples which fall into a gray area. for example  set  a  in figure 1 is clearly far from the typical behavior for their time period; but set  b  contains many points which are somewhat unusual but may or may not be due to the presence of an event. in order to separate the two  we need to define a model of uncertainty  how unusual is the measurement    and additionally incorporate a notion of event persistence  i.e.  the idea that a single  somewhat unusual measurement may not signify anything but several in a row could indicate the presence of an event.
　another example of this  chicken and egg  problem is illustrated in figure 1. the top panel shows vehicle counts every five minutes for an on-ramp on the 1 freeway in los angeles located near dodger stadium  where the los angeles dodgers baseball team plays their home games. the darker line shows the average count for the set of  normal  fridays when there were no baseball games  averaged over every non game-day friday for each specific 1-minute time slice . the daily rhythm of normal friday vehicle flow is clear from the data: little traffic in the early hours of the morning  followed by a sharp consistent increase during the morning rush hour  relatively high volume and variability of traffic during the day  another increase for the evening rush hour  and a slow decay into the night back to light traffic.
　the light line in the top panel shows the counts for a particular friday when there was a baseball game: the  event  can be seen in the form of significantly increased traffic around 1 hours  corresponding to a surge of vehicles leaving from the baseball stadium. it is clear that relative to the average profile  the darker line  that the baseball traffic is anomalous and should be relatively easy to detect.
　now consider what would happen if we did not know when the baseball games were being held. the lower panel shows the time series for the same friday as the top panel  the

figure 1: example of freeway traffic data for fridays for a particular on-ramp.  a  average time profile for normal  non game-day fridays  dark curve  and data for a particular friday  1/1  with a baseball game that night  light curve .  b  average time profile over all fridays  dark curve  superposed on the same friday data  light curve  as in the top panel.
lighter line  but now with the average over all fridays superposed  i.e.  the average time-profile including both gameday and non game-day fridays. this average profile has now been pulled upwards around 1 hours and sits roughly halfway between normal traffic for that time of night  the darker line in the top panel  and the profile that corresponds to a baseball event  the light curve . ideally we would like to learn both the patterns of normal behavior and to detect events that indicate departures from the norm. for example  given the time series shown in figure 1  we would like to learn a model that reflects the bimodal nature of such data  namely a combination of the normal traffic patterns to which is occasionally added additional counts caused by aperiodic events.
　in this paper we investigate the use of markov-poisson models for this purpose  and illustrate how to learn such models from data to both characterize normal behavior and detect anomalous events. the model consists of a timevarying poisson process that includes both systematic diurnal  time of day  and calendar  day of week  variation in poisson rates over time  as well as a hidden markov event process. we adopt a bayesian approach to learning and inference  allowing us to pose and answer a variety of queries within a probabilistic framework  queries such as  did any events occur in this time-period     how many additional counts were caused by a particular event     what is the estimated duration of an event    and so forth. different highlevel questions about the data can also be addressed  such as  are monday and tuesday normal patterns the same   or  are the patterns of normal behavior consistent over time or changing   using bayesian model selection techniques.
　the remainder of the paper proceeds by discussing related work in section 1 and then  in section 1  describing in more detail the two data sets  freeway traffic data and  people counter  building data  that we use throughout the paper. section 1 illustrates the limitations of a simple baseline approach to event detection based on thresholding. in section 1 we describe our proposed probabilistic model and section 1 describes how this model can be learned from data using a bayesian estimation framework. section 1 discusses how we can use the learned model for event detection and validates the model's predictions of anomalous events by using known ground-truth schedules of events. we show that our proposed approach is significantly more accurate in practice than a baseline threshold-based method. section 1 uses bayesian model selection techniques to investigate different levels of time-heterogeneity in the model  and section 1 illustrates how the model can be used for estimating event attendance. in section 1 we conclude with a brief discussion of open research problems and summary comments.
1. related work
　there has been a significant amount of prior work in both data mining and statistics on finding surprising patterns  outliers  and change-points in time series. for example  keogh et al.  described a technique that represents a realvalued time-series by quantizing into it a finite set of symbols and then uses a markov model to detect surprising patterns in the symbol sequence. guralnik and srivastava  proposed an iterative likelihood-based method for segmenting a time-series into piecewise homogeneous regions. salmenkivi and mannila  investigated the problem of segmenting sets of low-level time-stamped events into time-periods of relatively constant intensity  using a combination of poisson models and bayesian estimation methods. kleinberg  demonstrated how a method based on an infinite automaton could be used to detect bursty events in text streams.
　all of these approaches share a common goal with that of this paper  namely detection of novel and unusual data points or segments in time-series. however  none of this earlier work focuses on the specific problem we address here  namely detection of bursty events embedded in time series of counts that reflect the normal diurnal and calendar patterns of human activity.
　the model proposed here is derived from the markov- modulated poisson processes used by scott and smyth  for analysis of web surfing behavior and scott  for telephone network fraud detection. we extend the latter model by employing a more flexible model of event-related counts and including missing data  and show that not only is it accurate at detecting the presence of events in two new data sets by using ground truth for validation  but also how it can be used to perform additional tasks such as model selection and inference over other quantities of interest about the event.
1. data set characteristics
　we use two different data sets throughout the paper to illustrate our approach. in this section we describe these data sets in more detail.
　the first data set will be referred to as the building data  consisting of 1 months of count data automatically recorded every 1 minutes at the front door of the calit1 institute building on the uc irvine campus. the data are generated by a pair of battery-powered optical detectors that measure the presence and direction of objects as they pass through the building's main set of doors. the number of  counts  in each direction are then communicated via a wireless link to a base station with internet access  at which they are stored. the observation sequences   people counts   acquired at the front door form a noisy time series with obvious struc-

figure 1:  a  one week of traffic data  light curve  from sunday to saturday  june 1   with the estimated normal traffic profile  estimated by the proposed model described later in the paper  superposed as a dark curve.  b  ground truth list of events  baseball games .
ture but many outliers  see figure 1 . the data is corrupted by the presence of  events -non-periodic activities which take place in the building and  typically  cause an increase in foot traffic entering the building before the event  and leaving the building after the event  possibly with some  churn   people going in and out  during the event. some of these events can be seen easily in the time-series  for example the two large spikes in both entry and exit data on days four and twelve in figure 1. however  many of these events may be less obvious and only become visible when compared to the behavior over a long period of time.
　the second data set will be referred to as the freeway traffic data and consists of estimated vehicle counts every 1 minutes over 1 months from an inductive loop-sensor located on the glendale on-ramp to the 1-north freeway in los angeles . figure 1 shows the temporal pattern for a particular week starting with sunday morning and ending saturday night. the daily rhythms of traffic flow are clearly visible as is the distinction between weekdays and weekends. also visible are  local excursions  corresponding to significantly different counts compared to relatively smooth normal pattern  such as the baseball games on sunday afternoon and every evening except thursday. the lower panel of figure 1 shows a set of known events   ground truth   for this data  which will be unknown to the model and only used for validation  corresponding to the dates and times of baseball games. note that the  on-ramp events  correspond to traffic leaving at the end of a baseball game when large numbers of individuals leave the stadium and get on the freeway-thus  the event has a signature in the data that will tend to lag in time that of the baseball game itself.
　both data sets included a small number of holidays  1  which were removed before modeling  since these days were known apriori to involve relatively different  atypical  behavior.  treating these days as normal tends to slightly decrease their respective days' profiles and may increase probabilities of false alarms  etc.  alternatively  the model described later could be augmented to estimate the profile of holiday behavior separately  if desired.

figure 1:  a  entry data for the main entrance of the calit1 building for three weeks  beginning 1/1  sunday  and ending 1/1  saturday .  b  exit data for the same door over the same time period.1. a baseline model and its limitations
　one relatively straightforward baseline for detecting unusual events in count data is to perform a simple threshold test based on a poisson model for each time period. specifically  let us estimate the poisson rate λ of a particular time and day by averaging the observed counts on similar days  e.g.  mondays  at the same time  i.e.  the maximum likelihood estimate. then  we detect an event of increased activity when the observed count n is sufficiently greater than the average  as measured by the poisson distribution:
p 
and λ   n.
　for some data sets  this approach can be quite adequate- in particular  if the events interspersed in the data are sufficiently few compared to the amount of non-event observations  and if they are sufficiently noticeable in the sense that they cause a large increase in activity. however  these assumptions do not always hold  and we can observe several modes of failure in such a simple model.
　one way this model can fail is because of the  chicken and egg  problem referred to in the introduction and illustrated in figure 1. as discussed earlier  the presence of large events distorts the estimated rate of  normal behavior  increasing it slightly  which causes the threshold test to miss the presence of other events around that same time.
　a second type of failure occurs when there is a slight increase in traffic level which is not of sufficient magnitude to be noticed; however  the increase is sustained over a period of several observations signaling the presence of a persistent event. in figure 1  the event indicated for the first day can be easily found by the threshold model by setting the threshold sufficiently high enough to detect the event but low enough so that there are no false alarms. in order for the threshold model to detect the event on the second day  however  the threshold must be increased  which also causes the detection of two false alarms over the two-day period. anomalies detected by the threshold model are shown in the second panel of the figure while known events  baseball games  are displayed in the third panel.
　a third weakness of the threshold model is its difficulty in capturing the duration of an event. in order to detect

figure 1: illustration of the baseline threshold model set to detect the event on the second day  with  a  original freeway traffic time series in the top panel  light curve  for may 1  and mean profile as used by the threshold model  dark curve    b  events detected by the threshold method in the center panel  and  c  ground truth  known events  in the bottom panel. note the false alarms.

figure 1: same as figure 1 but with an even lower threshold to detect the full duration of the large event on the second day  causing multiple false alarms.
not only the presence of the event on the second day but also its duration  the threshold must be raised to the point that the number of false alarms becomes quite prohibitive  as illustrated in figure 1.  note that the traffic event  corresponding to people departing the game  begins at or near the end of the actual game time. 
　in the remaining sections of the paper we will discuss a more sophisticated probabilistic model that accounts for these different aspects of the problem  and show  in section 1  that it can be used to obtain significantly more accurate detection performance than the simple thresholding method.
1. probabilistic modeling
　let n t   for t （ {1 ... t}  generically refer to the observed count at time t for any of the time-dependent counting processes  such as the freeway traffic 1-minute aggregate count process or either of the two  entering or exiting  building 1-minute aggregate people count processes. in order to model n t   we require both a model of the  normal   typical behavior  intuitively corresponding to the periodic portion of the data   and a model of the event process  intuitively corresponding to rare increases in the number of observed counts . let us assume that the two processes are additive  so that
	n t  = n1 t  + ne t 	 1 
where n1 t  is the number of occurrences attributed to the normal building occupancy  and ne t  represents the number of occurrences attributed to an event at time t. we discuss modeling each of these in turn. note that  although the models described here are defined for discrete time periods  they can also be extended to continuous time measurements  1 .
1 modeling periodic count data
　perhaps the most common probabilistic model for count data is the poisson distribution  whose probability mass function is given by
	p n;λ  = e λλn/n!	n = 1 ...	 1 
where the parameter λ represents the rate  or average number of occurrences in a fixed time interval. when λ is a function of time  i.e. λ t    1  becomes a nonhomogeneous poisson distribution  in which the degree of heterogeneity depends on the function λ t .
　we employ a model derived from that of scott   which has been used to detect and segment fraud patterns in telephone network usage . specifically  we decompose λ t  as
　　　　　　　　λ t  = λ1 δd t  ηd t  h t   1  where d t  takes on values {1 ... 1} and indicates the day on which time t falls  so that sunday = 1  monday = 1  and so forth   and h t  indicates the interval  e.g.  half-hour periods for the building data  in which time t falls. by further requiring that
d
	= 1	and	xηj i = d	 j 
i=1
where d is the number of time intervals in a day  1 for the building data and 1 for the freeway traffic data   we can

figure 1: the effect of δd t   as seen over a week of building exit data. clearly  the relative rates over the weekend  sunday  saturday  are much lower than those on weekdays.

figure 1: the effect of ηd t  h t  in modulating the poisson rate of building exit data over a single day. there is a clear peak around lunchtime  and a heavy bias towards the end of the day.
ensure that the values λ1  δ  and η are easily interpretable: λ1 is the average rate of the poisson process over a full week  δi is the day effect  or the relative change for day i  so that  for example  sundays have a lower rate than mondays   and ηj i is the relative change in time period i given day j  the time of day effect .
　figures 1 illustrate these two effects for the building data. figure 1 shows one week's worth of data alongside the estimated rate with day effect only  i.e.  λ1 δd t ; this is the full poisson rate λ t  averaged over the time of day. figure 1 then shows how ηd t  h t  then modulates λ t  over a single day to achieve a sensible time-dependent rate value.
　figure 1 shows a graphical model in the form of a plate diagram for the periodic data n1 t  and its parameters. a key point is that  given n1 t   the parameters λ1  δ  and η are all independent of n t .
　by choosing conjugate prior distributions for these variables we can ensure that the inference computations in section 1 have a simple closed form:
λ1 ゛ Γ λ;al bl 
dir 
dir 
where Γ is the gamma distribution 
Γ λ;a b  『 λa 1e bλ
and dir ，  is a dirichlet distribution with the specified parameter vector.
1 modeling rare  persistent events
　in the data examined in this paper  the anomalous measurements can be intuitively thought of as being due to relatively short  rare periods in which an additional random

figure 1: graphical model for λ t  and n1 t . the parameters λ1  δ  and η  the periodic components of λ t   couple the distributions over time.
process also contributes to the observations  e.g.  people arriving for an event   increasing the number of observed counts. the model can be easily modified to capture alternative situations  i.e.  the presence of an event suppressing or otherwise altering the number of  normal  counts by changing  1  into a more general relationship; however  in practice our simple additivity assumption seems sufficient.
　to model the behavior of anomalous periods of time  we use a binary process z t  to indicate the presence of an event 
i.e. 
	1	if there is an event at time t
z t  =  
	1	otherwise;
and define the probability distribution over z t  to be markov in time  with transition probability matrix

so that the length of each time period between events is geometric with expected value 1/z1  and the length of each event is geometric with expected value 1/z1. we give z1  z1 priors

where β ，  is the beta distribution.
　given z t   we can model the increase in observation counts due to the event  ne t   as poisson with rate γ t 

and γ t  as independent at each time t
γ t  ゛ Γ γ;ae be .
in fact  γ t  may be marginalized over analytically  since
	z p n;γ Γ γ;ae be  = nbin n;ae be/ 1 + be  	 1 
where nbin is the negative binomial distribution. a graphical model representing the distribution over z t   ne t   and n t  is shown in figure 1. here  z t  provides the time- dependent structure of the process; from figures 1  one can see that n t  has temporal structure both from λ t  and z t .

figure 1: graphical model for z t  and n t . the markov structure of z t  couples the variables over time  in addition to the coupling of n1 t  from figure 1 .
　this type of gated poisson contribution  called a markov- modulated poisson model  is a common component of many network traffic models  1 . in our application we are specifically interested in detecting the periods of time in which our event process z t  is active  and perhaps using the rate γ t  or the associated count ne t  to provide information about its  popularity . while it is also possible to couple the rates γ t  in order to capture the idea that  for example  two detections at times t and t + 1 are likely to be related and thus have correlated count increases  we do not address this additional complexity here.
1. learning and inference
　let us initially assume that our total length of observation comprises some integral number of weeks  so that t = 1   d w for some integer w. although not strictly necessary  this assumption greatly simplifies the inference procedure for estimating the parameters of the model ; nor is it at all restrictive in our setting  since we can always extend a region of interest to cover an integer number of weeks by taking the additional data to be unobserved.
　given the complete data {n1 t  ne t  z t }  it is straightforward to compute maximum a posteriori  map  estimates or draw posterior samples of the parameters λ t  and {z1 z1}  since all variables λ1  δ  η  z1  and z1 are conditionally independent  see figures 1  or section 1 .
　we can thus infer posterior distributions over each of the variables of interest using markov chain monte carlo  mcmc  methods  1 . specifically  we iterate between drawing samples of the hidden variables {z t  n1 t  ne t }  described in section 1  and the parameters given the complete data  described in section 1 . the complexity of each iteration of mcmc is o t   linear in the length of the time series  and experimentally converges quite rapidly. these samples can be used to not only to provide a point estimate of the value of each parameter  for example  its posterior mean  but also to gauge the amount of uncertainty about that value.
1 sampling the hidden variables given parameters
1
1
1
1
figure 1:  a  entry data  along with λ t   over a period of three weeks  sept. 1-oct. 1 . also shown are  b  the posterior probability of an event  p z   and  c  the periods of time in which an event was scheduled for the building. most of the scheduled events are detected  along with a few other time periods  such as a period of greatly heightened activity on the first saturday .　given the periodic poisson mean λ t  and the transition probability matrix m  it is relatively straightforward to draw a sample sequence z t  using a variant of the forward- backward algorithm ; we provide the necessary equations for completeness. specifically  in the forward pass we compute  for each t （ {1 ... t} the conditional distribution   using the likelihood functions
z t  = 1
	  nbin i 	z t  = 1
 where the parameters of nbin ，  are as in  1  . then  for t （ {t ... 1}  we draw samples
.
given z t  = z t   we can then determine n1 t  and ne t   by taking n1 t  = n t  if z t  = 1 and drawing n1 t  from the discrete distribution
n1 t  ゛ f i  『 p n t    i;λ t  nbin i;ae be/ 1 + be  
if z t  = 1  and setting ne t  = n t    n1 t . if n t  is unobserved  missing   n1 t  and ne t  are decoupled given z t  and we may draw them independently.
1 sampling the parameters given the complete data
　because t is an integral number of weeks  t = 1 d  w  we have that the complete data likelihood is given by
nbin ne t  
considering the first term  which only involves λ1  δ  and η  we have

by virtue of choosing conjugate prior distributions  we have that the posteriors are given by distributions of the same form  but with parameters given by the sufficient statistics of the data. defining
si j = x n1 t 
　d t =i  t:
h t =jsi = xsi j
js = xsi
iwe have the posterior distributions
λ1 ゛ Γ λ;al + s bl + t 
	1	d	d
1  δ1 ... δ1  ゛ dir α1 + s1 ... α1 + s1 
	1	h	h
d  ηj 1 ... ηj d  ゛ dir α1 + sj 1 ... αd + sj d .
　sampling z1  z1 is similarly straightforward-we merely compute
	zij =	x	1	for i = 1  j = 1
t:z t =i z t+1 =j
to obtain posterior distributions

as noted by scott   markov-modulated poisson processes appear to be relatively sensitive to the selection of prior distributions over z1 z1 and γ t   perhaps because there are no direct observations of the processes they describe. this appears to be particularly true for our model  which has considerably more freedom in the anomaly process  i.e.  in γ t   than the telephony application of scott . we avoid over-explanation of the data by applying relatively strong priors to the transition parameters of z t  which force the marginal probability of z t  to 1 incidents per day  on average. by adjusting these priors one can increase or decrease the number of events detected; see section 1.
1. adaptive event detection
　one of the primary goals in our application is to automatically detect the presence of unusual events in the observation sequence. the presence or absence of these events is captured by the process z t   and thus we may use the posterior probability p z t |{n t }  as an indicator of when such events occur.
　given a sequence of data  we can use the samples drawn in the mcmc procedure  section 1  to estimate the posterior marginal distribution over events. for comparison to a ground truth of the events in the building data set  we obtained a list of the events which had been scheduled over the entire time period from the building's event coordinator. for the freeway traffic data set  the game times for 1 home games in the la dodgers 1 regular season were used as the validation set. five additional regular season games were not included in this set because they occurred during extended periods of missing loop sensor count information. note that both sets of  ground truth  may represent an underestimate of the true number of events that occurred  e.g.  due to unscheduled meetings and gatherings  concerts held at the baseball stadium  etc. . nonetheless this ground truth is very useful in terms of measuring how well a model
can detect a known set of events.
	1	1	1

	1	1	1
figure 1: data for oct. 1  1  along with rate λ t  and probability of event p z . at 1 p.m. an event was held in the building atrium  causing anomalies in both the incoming and outgoing data over most of the time period.
 c 
figure 1: a friday evening game  apr. 1  1. shown are  a  the prediction of normal activity  λ t ;  b  the estimated probability of an event  p z ; and  c  the actual game time. panels  d - f  show the threshold model's prediction for the same day.
　the results obtained by performing mcmc for the building data are shown in figure 1. we plot the observations n t  together with the posterior mean of the rate parameters λ t  over a three week period  sept. 1-oct. 1 ; figure 1 shows incoming  entry  data for the building. displayed below the time series is the posterior probability of z t  at each time t  drawn as a sequence of bars  below which dashes indicate the times at which scheduled events in the building took place. in this sequence  all of the known events are successfully detected  along with a few additional detections that were not listed in the building schedule. such unscheduled activities often occur over weekends where the baseline level of activity is particularly low.
　figure 1 shows a detailed view of one particular day  during which there was an event scheduled in the building atrium. plots of the probability of an unusual event for both the entering and exiting data show a high probability over the entire period allocated to the event  while slight increases earlier in the day were deemed much less significant due to their relatively short duration.
　the results obtained by performing mcmc for the freeway traffic data for three game-days are shown in figures 1- 1. figure 1 shows a friday game that is more sparsely attended than the friday game plotted in figure 1 and is

figure 1:  a  data for may 1 1  along with rate λ t ;  b  probability of event p z ;  c  actual event times.
　total number mmpp threshold of predicted events model model

1.1% 1% 1.1% 1%
	1.1%	1%
table 1: accuracies of predictions for the building data: the percentage of the 1 known events correctly predicted by each model  for different numbers of total events predicted.
an example of where our model successfully separates the normal friday evening activity from game-day evening activity. the threshold model was able to detect the friday games with heavy attendance  but more sparsely attended games such as this one were missed.
　figure 1 displays the same two-day period where the threshold model was shown to detect false alarms when the threshold level was set appropriately to detect the event on day two  figure 1 . our model detects the two events with no false alarms  and nicely shows the duration of the predicted events.
　tables 1 and 1 compare the accuracies of the markovmodulated poisson process  mmpp  model described in section 1 and the baseline threshold model of section 1 on validation data not used in training the models for both the building and freeway traffic data respectively. for each row in the table  the mmpp model parameters were adjusted so that a specific number of events were detected  by adjusting
　total number mmpp threshold of predicted events model model

	1.1%	1%
	1.1%	1%
	1.1%	1%
	1.1%	1%
table 1: accuracies of predictions for the freeway traffic data: the percentage of the 1 known events correctly predicted by each model  for different numbers of total events predicted.
the priors on the transition probability matrix. the threshold model was then modified to find the same number of events as the mmpp model by adjusting its threshold .
　in both data sets  for a fixed number of predicted events  each row   the number of true events detected by the mmpp model is significantly higher than that of the baseline model. this validates the intuitive discussion of section 1 in which we outlined some of the possible limitations of the baseline approach  namely its inability to solve the  chicken and egg  problem and the fact that it does not explicitly represent event persistence. as mentioned earlier  the events detected by the mmpp model that are not in the ground truth list in many cases plausibly correspond to real events rather than false alarms  such as unscheduled building activities for the building data and accidents and non-sporting events for the freeway traffic data.
1. testing heterogeneity
　one question we may wish to ask about the data is  how time-varying is the process itself  for example  how different is friday afternoon from that of any other weekday  by increasing the number of degrees of freedom in our model  we improve its potential for accuracy but may increase the amount of data required to learn the model well. this also has important consequences in terms of data representation  for example  compression   which may need to be a time- dependent function as well. thus  we may wish to consider testing whether the amount of data we have thus far acquired supports a particular degree of heterogeneity.
　we can phrase many of these questions as tests over submodels which require equality among certain subsets of the variables. for example  we may wish to test for the presence of the day effect  and determine whether a separate effect for each day is warranted. specifically  we might test between three possibilities:
d1 : δ1 = ... = δ1	 all days the same 
d1 : δ1 = δ1 δ1 = ... = δ1  weekends  weekdays the same 
	 all day effects separate 
　we can compare these various models by estimating each of their marginal likelihoods . the marginal likelihood is the likelihood of the data under the model  having integrated out the uncertainty over the parameter values  e.g.  p n|d1  = z p n|λ1 δ η p λ1 δ η   λ1  δ  η
since uncertainty over the parameter values is explicitly accounted for  there is no need to penalize for an increasing number of parameters. moreover  we can use the same posterior samples drawn during the mcmc process  section 1  to find the marginal likelihood  using the estimate of chib .
　computing the marginal likelihoods for each of the models d1 ... d1 for the building data  and normalizing by the number of observations t  we obtain the values shown in table 1. from these values  it appears that d1  all days the same  is a considerably worse model  and that d1 and d1 are essentially equal  indicating that either model will do an equally good job of predicting behavior.
　we can derive similar tests for other symmetries that might exist. for example  we might wonder whether every day has the same time profile.  note that this is possi-
modele log1 p n  t   e log1 p n+ t   d1-1-1d1-1-1d1-1-1table 1: average log marginal likelihood of the data  exit and entry  under various day-dependency models: d1  all days the same; d1  weekends and weekdays separate; and d1  each day separate. there does not appear to be a significant change in behavior among weekend days or among weekdays. parameters ηi j were unconstrained.
modele log1 p n  t   e log1 p n+ t   t1-1-1t1-1-1t1-1-1table 1: average log marginal likelihood under various time-of-day dependency models for the building data: t1  all days have the same time profile; d1  weekend days and weekdays share time profiles; d1  each day has its own individual time profile. there appears to be a only slight improvement at each stage. parameters δi were unconstrained.
ble  since sunday might be a severely squashed version of monday  i.e.  fewer people come to work  but they follow a similar hourly pattern.  alternatively  is each day of the week unique  or  again  might all weekdays be the same  and similarly weekend days  our tests become
t1 :  i η1 i = ... = η1 i	 same time every day 
t1 :  i η1 i = η1 i η1 i = ... = η1 i	 weekends  weekdays 
	 all time effects separate 
the results  shown in table 1  show a small but distinct preference for t1  indicating that although weekends and weekdays have differing profiles  one can better predict behavior by combining data across weekdays and weekends. other tests  such as whether fridays differ from other days  can be accomplished using similar estimates.
1. estimating event attendance
　along with estimating the probability that an unusual event is taking place  as part of the inference procedure our model also estimates the number of counts which appear to be associated with that event. marginalizing over the other variables  we obtain a distribution over how many additional people seem to be entering or leaving the building or the number of extra vehicles entering the freeway during a particular time period. one intriguing use for this information is to provide a score  or some measure of popularity  of each event.
　as an example  taking our collection of la dodgers baseball games  we compute and sum the posterior mean of extra  event-related  vehicles observed  ne t   during the duration of the event detection. figure 1 shows that our estimate of the number of additional cars is positively correlated with the actual overall attendance recorded for the games  correlation coefficient 1 . similar attendance scores can
x 1

figure 1: the attendance of each baseball game  y-axis  shows correlation with the number of additional  event-related  vehicles detected by the model  x-axis .
be computed for the building data  or other quantities such as duration estimated  though for these examples no ground truth exists for comparison.
1. conclusion
　we have described a framework for building a probabilistic model of time-varying counting processes  in which we observe a superposition of both time-varying but regular  periodic  and aperiodic processes. we then applied this model to two different time series of counts of the number of people entering and exiting through the main doors of a campus building and the number of vehicles entering a freeway  both over several months. we described how the parameters of the model may be estimated using mcmc sampling methods  while simultaneously detecting the presence of anomalous increases in the counts. this detection process naturally accumulates information over time  and by virtue of having a model of uncertainty gives a natural way to compare potentially anomalous events occurring on different days or times. we also showed that the detection can be performed in real-time by fixing the parameter distributions obtained during mcmc and performing a simplified form of forward inference.
　using a probabilistic model also allows us to pose alternative models and test among them in a principled way. doing so  we can answer questions about how the observed behavior varies over time  and how predictable that behavior is. finally  we described how the information obtained in the inference process can be used to provide an interesting source of feedback  for example estimating event popularity and attendance.
　an interesting direction for future work is to simultaneously model multiple correlated time-series  such as those arising from people counts from multiple doors  and perhaps from multiple different types of sensors  as well as multiple time-series from different loop sensors along a freeway. more sensors provide richer information about occupancy and behavioral patterns  but it is an open question how these co-varying data streams should be combined  and to what degree their parameters can be shared.
acknowledgments
the authors would like to thank chris davison and anton popov for their assistance with logistics and data collection  and shellie nazarenus for providing a list of scheduled events for the calit1 building. this work was supported in part by the national science foundation under grants no. itr1 and iis-1.
