we propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters. we focus on the motivating application of prospective disease surveillance: detecting space-time clusters of disease cases resulting from an emerging disease outbreak. automatic  real-time detection of outbreaks can enable rapid epidemiological response  potentially reducing rates of morbidity and mortality. building on the prior work on spatial and space-time scan statistics  our methods combine time series analysis  to determine how many cases we expect to observe for a given spatial region in a given time interval  with new  emerging cluster  space-time scan statistics  to decide whether an observed increase in cases in a region is significant   enabling fast and accurate detection of emerging outbreaks. we evaluate these methods on two types of simulated outbreaks: aerosol release of inhalational anthrax  e.g. from a bioterrorist attack  and floo   fictional linear onset outbreak    injected into actual baseline data  emergency department records and over-thecounter drug sales data from allegheny county . we demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low  and show that our new  emerging cluster  scan statistics consistently outperform the standard  persistent cluster  scan statistics approach.
categories and subject descriptors
h.1  database management : database apps- data mining
general terms
algorithms
keywords
cluster detection  space-time scan statistics  biosurveillance
1.	introduction
¡¡in many data mining applications  we are faced with the task of detecting clusters: regions of space where some quantity is signifi-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  chicago  illinois  usa.
copyright 1 acm 1-1-x/1 ...$1.
cantly higher than expected. for example  our goal may be to detect clusters of disease cases  which may be indicative of a naturally occurring disease epidemic  e.g. influenza   a bioterrorist attack  e.g. anthrax release   or an environmental hazard  e.g. radiation leak . in medical imaging  we may attempt to detect tumors or other hazardous growths; in neuroscience  we may be interested in detecting spatial patterns of brain activity  measured by fmri activation  that correspond to various cognitive tasks.  discusses many other applications of cluster detection  including mining astronomical data  identifying clusters of stars or galaxies  and military reconnaissance  monitoring strength and activity of enemy forces . in all of these applications  we have two main goals: to pinpoint the location  shape  and size of each potential cluster  and to determine  by statistical significance testing  whether each potential cluster is likely to be a  true  cluster or simply a chance occurrence.
¡¡while most of the prior work on cluster detection is purely spatial in nature  e.g.  1  1  1    it is clear from the above list of applications that time is an essential component of most cluster detection problems. we are often interested in clusters which are emerging in time: for example  a growing tumor  an outbreak of disease  or an increase in troop activity. in some applications  the time dimension can be dealt with easily  either by applying some purely spatial cluster detection method at each time step  or by treating time as another spatial dimension and thus applying spatial cluster detection in a d+1 dimensional space  d spatial dimensions  plus time . the disadvantage of the first approach is that by only examining one day of data at a time  we may fail to detect more slowly emerging clusters. the disadvantage of the second approach is that we may detect less relevant clusters: those clusters that have persisted for a long time  rather than those that are newly emerging.
¡¡to improve on these methods  it is helpful to consider the guiding question   how is time  as a dimension  different from space   we argue that there are three important distinctions which require us to treat spatio-temporal cluster detection differently from spatial cluster detection. first  time  unlike space  has an important point of reference: the present. we often care only about those space-time clusters that are still  active  at the present time  and in these cases we should use a prospective method  searching for clusters which end at the present time  rather than a retrospective method  searching for clusters which end at or before the present time . second  in the spatial cluster detection framework  we typically assume that we have some baseline denominator data such as a census population  for epidemiology   and that the expected count  e.g. number of disease cases  is proportional to this baseline. in the spatio-temporal framework  we are generally not provided with explicit denominator data; instead  we infer the expected values of the most recent days' counts from the time series of past counts. finally  and most interestingly  time has an explicit direction or  arrow   proceeding from the past  through the present  to the future. we are generally interested in clusters which emerge over time: for example  a disease epidemic may start with only a few reported cases  then increase in magnitude either gradually or rapidly. one major focus of this paper is developing statistical methods which are more appropriate for detecting such emerging clusters.
¡¡we focus here on the motivating application of prospective disease surveillance: detecting space-time clusters of disease cases resulting from an emerging disease outbreak. in this application  we perform surveillance on a daily  or even hourly  basis  with the goal of finding emerging epidemics as quickly as possible. by detecting epidemics rapidly and automatically  we hope to allow more rapid epidemiological response  e.g. distribution of vaccines  public health warnings   potentially reducing the rates of mortality and morbidity from an outbreak. in this application  we are given the number of disease cases of some given type  e.g. respiratory  in each spatial location  e.g. zip code  on each day. more precisely  since we cannot measure the actual number of cases  we instead rely on related observable quantities such as the number of respiratory emergency department visits  or sales of over-thecounter cough and cold medication  in a given spatial location on a given day. we must then detect those increases that are indicative of emerging outbreaks  as close to the start of the outbreak as possible  while keeping the number of false positives low.
1.	the model
¡¡in the general case  we have data collected at a set of discrete time steps t = 1...t  where time t represents the present  at a set of discrete spatial locations si. for each si at each time step t  we are given a count cti  and our goal is to find if there is any region s  set of locations si  and time interval  t = tmin ...tmax  for which the counts are significantly higher than expected. thus we must first decide on the set of spatial regions s  and the time intervals tmin ...tmax  that we are interested in searching. in the scan statistics framework discussed below  we typically search over the set of all spatial regions of some given shape  and variable size. for simplicity  we assume here  as in   that the spatial locations si are aggregated to a uniform  two-dimensional  n ¡Án grid g  and we search over the set of all axis-aligned rectangular regions s   g.1this allows us to detect both compact and elongated clusters  which is important since disease clusters may be elongated due to dispersal of pathogens by wind  water  or other factors. for prospective surveillance  as is our focus here  we care only about those clusters which are still present at the current time t  and thus we search over time intervals with tmax = t; if we were performing a retrospective analysis  on the other hand  we would search over all tmax ¡Ü t. we must also choose the size of the  temporal window  w: we assume that we are only interested in detecting clusters that have emerged within the last w days  and are still present   and thus we search over time intervals tmin ...t for all t  w   tmin ¡Ü t.
¡¡in the disease detection framework  we assume that the count  number of cases  in each spatial region si on each day t is poisson distributed  cti ¡« po ¦Ëti  with some unknown parameter ¦Ëti. thus our method consists of two parts: time series analysis for calculating the expected number of cases  or  baseline   bti = e cti  for each spatial region on each day  and space-time scan statistics for determining whether the actual numbers of cases cti in some region s are significantly higher than expected  given bti  in the last w days. the choice of temporal window size w impacts both parts of our method: we calculate the baselines bti for the  current  days t  w   t ¡Ü t by time series analysis  based on the  past  days 1 ¡Üt ¡Ü t  w  and then determine whether there are any emerging space-time clusters in the last w days. in addition to the temporal window size  three other considerations may impact the performance of our method: the type of space-time scan statistic used  the level on which the data is aggregated  and the method of time series analysis. we discuss these considerations in detail below.
1.	space-time scan statistics
¡¡one of the most important statistical tools for cluster detection is the spatial scan statistic  1  1  1 . this method searches over a given set of spatial regions  finding those regions which maximize a likelihood ratio statistic and thus are most likely to be generated under the alternative hypothesis of clustering rather than under the null hypothesis of no clustering. randomization testing is used to compute the p-value of each detected region  correctly adjusting for multiple hypothesis testing  and thus we can both identify potential clusters and determine whether they are significant. the standard spatial scan algorithm  has two primary drawbacks: it is extremely computationally intensive  making it infeasible to use for massive real-world datasets  and only compact  circular  clusters are detected. in prior work  we have addressed both of these problems by proposing the  fast spatial scan  algorithm  1  1   which can rapidly search for elongated clusters  hyper-rectangles  in large multi-dimensional datasets. as noted above  we choose here to search over rectangular regions  using a space-time variant of the fast spatial scan as necessary to speed up our search.
¡¡in its original formulation  1  1   the spatial scan statistic does not take time into account. instead  it assumes a single count ci  e.g. number of disease cases  for each spatial location si  as well as a given baseline bi  e.g. at-risk population . then the goal of the scan statistic is to find regions where the rate  or expected ratio of count to baseline  is higher inside the region than outside. the statistic used for this is the likelihood ratio d s  = prpr data data|h|h1s     where the null hypothesis h1 represents no clustering  and each alternative hypothesis h1 s  represents clustering in some region s. more precisely  under h1 we assume a uniform disease rate qall  such that ci ¡« po qallbi  for all locations si. under h1 s   we assume that ci ¡« po qinbi  for all locations si ¡Ês  and ci ¡« po qoutbi  for all locations si ¡Êg s  for some constants qin  qout. from this  we can derive an expression for d s  using the maximum likelihood esti-
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡call mates of qin  qout  and qall:  if cbinin   cboutout   and d s  = 1 otherwise  where  in    out   and  all  are the sums of counts and baselines for s  g s  and g respectively. then the most significant spatial region s is the one with the highest score d s ; we denote this region by s   and its score by d . once we have found this region by searching over the space of possible regions s  we must still determine its statistical significance  i.e. whether s  is a significant spatial cluster. to adjust correctly for multiple hypothesis testing  we find the region's pvalue by randomization: we randomly create a large number r of replica grids under the null hypothesis ci ¡« po qallbi   and find the highest scoring region and its score for each replica grid. then the p-value can be computed as rbeatr++1  where rbeat is the number of replica grids with d  higher than the original grid. if this p-value is less than some constant ¦Á  here ¦Á = .1   we can conclude that the discovered region is unlikely to have occurred by chance  and is thus a significant spatial cluster; we can then search for secondary clusters. otherwise  no significant clusters exist.
¡¡the formulation of the scan statistic that we use here is somewhat different  because we are interested not in detecting regions with higher rates inside than outside  but regions with higher counts than expected. let us assume that baselines bi represent the expected values of each count ci; we discuss how to obtain these baselines below. then we wish to test the null hypothesis h1: all counts ci are generated by ci ¡« po bi   against the set of alternative hypotheses h1 s : for spatial locations si ¡Ê s  all counts ci are generated by ci ¡« po qbi   for some constant q   1  and for all other spatial locations si ¡Ê g s  all counts ci ¡« po bi . we then compute the likelihood ratio:
d s  = pr data|h1 s   = maxq¡Ý1¡Çsi¡Ês pr ci ¡« po qbi  
	pr data|h1 	¡Çsi¡Ês pr ci ¡« po bi  
= maxq¡Ý¡Ç1¡Çsi si¡Êbsci iqbe ib ici e qbi = maxq¡Ýe1 qbcinin e qbin
¡Ês
using the maximum likelihood estimate q   we ob-
cin
tain debin cin   if cin   bin  and d s  = 1 otherwise.
as before  we search over all spatial regions s to find the highest scoring region s . then the statistical significance  p-value  of s  can be found by randomization testing as before  where the replica grids are generated under the null hypothesis ci ¡« po bi .
1	the 1-day space-time scan statistic
¡¡to extend this spatial scan statistic to the prospective space-time case  the simplest method is to use a 1-day temporal window  w = 1   searching for clusters on only the present day t = t. thus we wish to know whether there is any spatial region s with higher than expected counts on day t  given the actual counts cti and expected counts bti for each spatial location si. to do so  we compare the null hypothesis h1: cti ¡« po bti   for all si  to the set of alternative hypotheses h1 s : cti ¡« po qbti   for all si ¡Ê s  for some constant q  1  and cti ¡« po bti   elsewhere. thus the statistic takes the same form as the purely spatial scan statistic  and we obtain: d s  = c  ifc   b  and d s  = 1 otherwise  where c = ¡Æsi¡Ês cti and b = ¡Æsi¡Ês bti denote the total count and total baseline of region s on time step t. again  we search over all spatial regions s to find the highest scoring region s  and its score d . to compute the p-value  we perform randomization testing as before  where each replica grid has counts cti generated from po bti   and all other counts cti  t 1= t  copied from the original grid.
1	multi-day space-time scan statistics
¡¡while the 1-day prospective space-time scan statistic is very useful for detecting rapidly growing outbreaks  it may have difficulty detecting more slowly growing outbreaks  as noted above. for the multi-day prospective space-time scan statistics  we have some temporal window w   1  and must determine whether any outbreaks have emerged within the most recent w days  and are still present . in other words  we wish to find whether there is any spatial region s with higher than expected counts on days tmin ...t  for some t  w   tmin ¡Ü t. to do so  we first compute the expected counts bti and the actual counts cti for each spatial location si on each day t  w   t ¡Ü t; we discuss how the baselines bti are calculated in the following section. we then search over all spatial regions s   g  and all allowable values of tmin  finding the highest value of the spatio-temporal score function d s tmin . the calculation of this function depends on whether we are searching for  persistent  or  emerging  clusters  as we discuss below. in any case  once we have found the highest scoring region  s  tmin    and its score d   we can compute the p-value of this region by performing randomization testing as before  where each replica grid has counts cti generated from po bti  for t  w   t ¡Ü t  and all other counts cti copied from the original grid.
¡¡now we must consider how to compute the function d s tmin . the standard method for computing the space-time scan statistic  proposed for the retrospective case by  and for the prospective case by   builds on the kulldorff spatial scan statistic  given above. as in the purely spatial scan  this method assumes that baselines bti are given in advance  e.g. population in each location for each time interval   and that counts cti are generated from poisson distributions with means proportional to bti. then the goal is to find space-time clusters  s tmin  where the rate  ratio of count to baseline  is significantly higher inside the region than outside. as in the purely spatial case  this can be adapted to our framework  in which the goal is to find space-time clusters where the observed counts cti are higher than the expected counts bti. for the  persistent cluster  case  we maintain the other major assumption of the standard model: that the multiplicative increase in counts   relative risk   in an affected region remains constant through the temporal duration of the cluster. for the  emerging cluster  case  we instead make the assumption that the relative risk increases monotonically through the cluster's duration. it is also possible to assume a parametric form for the increase in relative risk over time  e.g. exponential or linear increase   and we consider such statistics in .
1	persistent clusters
¡¡the test for persistent clusters assumes that the relative risk of a cluster remains constant over time; as a result  the score function is very similar to the 1-day statistic  with sums taken over the entire duration of a cluster rather than only a single day.
¡¡as noted above  we must search over all spatial regions s and all values of tmin  where t  w   tmin ¡Ü t   finding the maximum score d s tmin . for a given region s and value tmin  we compare the null hypothesis h1: cti ¡« po bti  for all spatial locations si and all t  w   t ¡Ü t  to the alternative hypothesis h1 s tmin : cti ¡« po qbti  for si ¡Ê s and t = tmin ...t  for some constant q   1  and cti ¡« po bti  elsewhere. thus we can compute the likelihood ratio:
d s tmin  =	¡Ç¡Ý1pr¡Ç cprti ¡« ctipo¡« pobti   qbti   = maxq¡Ç¡Ý1 ¡Çbti  qbcti eti  bctitie qbti maxq
where the products are taken over si ¡Ê s and tmin ¡Ü t ¡Ü t. this simplifies to q¡Ý1 c  bqb   where c and b are the total count
¡Æsi¡Ês ¡Ætmin¡Üt¡Üt cti and total baseline ¡Æsi¡Ês ¡Ætmin¡Üt¡Üt bti respectively.
finally  using the maximum likelihood estimate q   we obtain dc if c   b  and d = 1 otherwise.
1	emerging clusters
¡¡while the space-time scan statistic for persistent clusters assumes that relative risk of a cluster remains constant through its duration  this is typically not true in disease surveillance. when a disease outbreak occurs  the disease rate will typically rise continually over the duration of the outbreak until the outbreak reaches its peak  at which point it will level off or decrease. our main goal in the epidemiological domain is to detect emerging outbreaks  i.e. those that have not yet reached their peak   so we focus on finding clusters where the relative risk is monotonically increasing over the duration of the cluster. again  we must search over all spatial regions s and all values of tmin  where t  w  tmin ¡Ü t   finding the maximum score d s tmin . for a given region s and value tmin  we compare the null hypothesis h1: cti ¡«po bti  for all spatial locations si and all t  w   t ¡Ü t  to the alternative hypothesis h1 s tmin : cti ¡« po qtbti  for si ¡Ê s and t = tmin ...t  for some monotonically increasing sequence of constants 1 ¡Ü qtmin ¡Ü ... ¡Ü qt   and cti ¡« po bti  elsewhere. thus we can compute the likelihood ratio:
d
= max1¡Üqtmin ¡Ü¡Ç... ¡Übtq tcti¡Çe  qbtitbti cti e qtbti
i
where the products are taken over si ¡Ê s and tmin ¡Ü t ¡Ü t. this
simplifies to max1¡Üqtmin ¡Ü...e¡Ü qbt ¡Çqct t e qtbt   where ct and bt are the total count ¡Æsi¡Ês cti and the total baseline ¡Æsi¡Ês bti on day t  and b is the total baseline ¡Æsi¡Ês ¡Ætmin¡Üt¡Üt bti as above.
¡¡now  we must maximize the numerator subject to the constraints on the qt. to do so  let e = e1...ep be a partitioning of tmin ...t into sets of consecutive integers  such that for all t1 t1 ¡Ê ej  qt1 = qt1 =qj  and for all ej1 and ej1  where j1   j1  qj1  qj1. in other words  the ej define a partitioning of tmin ...t into time periods where the relative risk is constant. note that the qt are uniquely defined by the partitions ej and the rates qj. we can then write:
cj e qjbj
	d s tmin  =	e b
where bj = ¡Æsi¡Ês ¡Æt¡Êej bti and cj = ¡Æsi¡Ês ¡Æt¡Êej cti. in   we prove that this expression is maximized when qj = cbjj for all j. this allows us to simplify the expression to:
cj
d s tmin  = eb c max ¡Ç
e1...ep ej
¡¡then the question is how to choose the optimal partitioning e = {ej}  and in  we present the following algorithm. this method uses a stack data structure  where each element of the stack represents a partition ej by a 1-tuple tstart tend cj bj qj. the algorithm starts by pushingonto the stack.
then for each t  from t  1 down to tmin  we do the following:
temp =  t  t  c t  b t  max 1  c t / b t   while  temp.q  = stack.top.q 
temp1 = stack.pop
temp =  temp.start  temp1.end  temp.c+temp1.c  temp.b + temp1.b  max 1   temp.c+temp1.c  /  temp.b+temp1.b   
stack.push temp 
¡¡as we prove in   this  step method  produces the unique optimal partitioning e and rates q  and thus the values of qt that maximize the score subject to the monotonicity constraints above. 1. inferring baseline values
¡¡in order to infer the baselines bti for the  current  days t  w   t ¡Ü t  we must consider two distinct questions: on what level to aggregate the data for time series analysis  and what method of time series analysis to use. we consider three different levels of spatial aggregation  which we term  building-aggregated time series   bats    cell-aggregated time series   cats   and  regionaggregated time series   rats  respectively. for the bats method  we consider the time series for each spatial location independently; for example  we may have a separate time series for each store or hospital  or counts may be already aggregated at some level  e.g. zip code . for each of these locations si  we independently compute the baselines bti  t  w   t ¡Ü t  from the past counts cti  1 ¡Ü t ¡Ü t  w   using one of the time series analysis methods below. then whenever we calculate d s tmin  for a region  we use the baselines bti and counts cti for each location in the region. the cats method first computes the aggregate count cti for each cell of the grid si ¡Êg on each day t  by summing counts of all spatial locations in that cell. then the baselines bti are computed independently for each grid cell si ¡Ê g  and whenever we calculate d s tmin  for a region  it is the cell counts and baselines that we use to compute the score. finally  the rats method  whenever it searches a region s  aggregates the time series of counts ct s   on the fly  by summing counts of all spatial locations in that region  computes baselines bt s  for the  current  days t  w   t ¡Ü t  and applies the score function d s tmin  to the resulting counts and baselines.
¡¡randomization testing must also be performed differently for each of the three levels of aggregation. to generate a replica grid for bats  we independently draw a count for each spatial location si for each current day t  using its baseline bti. to generate a replica grid for cats  we independently draw a count for each cell of the grid si ¡Ê g for each current day t  using the cell baseline bti. finally  randomization testing for rats is somewhat more difficult than for the other methods  since we must produce cell counts from a correlated distribution. various sampling methods can be used to do this  but this makes randomization extremely computationally expensive; see  for more details.
1	time series analysis methods
¡¡for a given location  cell  or region si  our goal is to estimate the expected values of the  current  counts  bti = e cti   t  w  t ¡Ü t  from the time series of  past  counts cti  1 ¡Üt ¡Üt  w. a variety of methods are possible  depending on how we wish to deal with three questions: day of week effects  seasonal trends  and bias. many epidemiological quantities  for example  otc drug sales  exhibit strong day of week and seasonal trends. here we consider three methods of dealing with day of week effects: we can ignore them  stratify by day of week  i.e. perform a separate time series calculation for each day of the week   or adjust for day of week. to adjust for day of week  we assume that the observed count on a given day is the product of an  actual  count and a constant dependent on the day of week. thus we compute the proportion of counts ¦Âi on each day of the week  i= 1...1 . then we transform each past day's observed count by dividing by 1¦Âi  do a single time series calculation on the transformed past counts to predict the transformed current counts  and finally multiply by 1¦Âi to obtain the predicted count for each current day. by adjusting instead of stratifying  more data is used to predict each day's count  potentially reducing the variance of our estimates   but the success of this approach depends on the assumption of a constant and multiplicative day-of-week effect.
¡¡we also consider three methods of adjusting for seasonal trends: to use only the most recent counts  e.g. the past four weeks  for prediction  to use all counts but weight the most recent counts more  as is done in our exponentially weighted moving average and exponentially weighted linear regression methods   and to use regression techniques to extrapolate seasonal trends to the current data. finally  we consider both methods which attempt to give an unbiased estimate of the current count  e.g. mean of past counts   and methods which attempt to give a positively biased estimate of the current count  e.g. maximum of past counts . as we show  the unbiased methods typically have better detection power  but the biased methods have the advantage of reducing the number of false positives to a more manageable level  see section 1 .
¡¡here we consider a total of 1 time series analysis methods  including  all max   bti = maximum count of last 1 days    all mean   bti = mean count of last 1 days    strat max   bti = maximum count of same day of week  1 weeks ago    strat mean   bti = mean count of same day of week  1 weeks ago   two exponentially weighted moving average methods   strat ewma  stratified by day of week   adj ewma  adjusted for day of week   and two exponentially weighted linear regression methods   strat ewlr  stratified by day of week   adj ewlr  adjusted for day of week . our final two methods are inspired by the recent work of kulldorff et al.  on the  space-time permutation scan statistic   so we call them  strat kull   stratified by day of week  and  all kull   ignoring day of week effects . in this framework  the baseline bti is computed as ¡Æ¡Æt tct¡Æi ¡Æi ci tci ti   i.e. space and time are assumed to be independent  so the expected fraction of all cases occurring in location si on day t can be computed as the product of the fraction of all cases occurring in location si and the fraction of all cases occurring on day t. the problem with this method is that the current day's counts are used for prediction of the current day's expected counts. as a result  if there is a cluster on the current day  the baselines for the current day will also be higher  reducing our power to detect the cluster. nevertheless  the strat kull and all kull methods do extremely well when detecting localized clusters  where the increase in counts is noticeable for a small region  but the region is small enough that the total count for the day is essentially unaffected .
¡¡we also note an interesting interaction between the level of aggregation and the method of time series analysis. if the expected counts bti  t  w  t ¡Ü t  are calculated as a linear combination of past counts cti  1 ¡Ü t ¡Ü t  w   and the weights for each past day t are constant from location to location  then we will calculate the same baselines  and thus  the same scores  regardless of whether we aggregate on the building  cell  or region level. this turns our to be true for most of the methods we investigate: all mean  strat mean  strat ewma  strat ewlr  all kull  and strat kull. on the other hand  if we choose different weights for each location  as is the case when we adjust for day of week  as in adj ewma and adj ewlr   we will calculate different baselines  and thus  different scores  depending on our level of aggregation. finally  we have very different results for the  max  methods  strat max and all max  depending on the level of aggregation  because the maximum is not a linear operator. since the sum of the maximum counts of each location  ¡Æsi¡Ês maxt cti  is higher than the maximum of the sum  maxt ¡Æsi¡Ês cti   we always expect bats to predict the highest baselines  and rats to predict the lowest baselines. for the results given below  we only distinguish between bats  cats  and rats aggregation for those methods where the distinction is relevant  allmax  strat max  adj ewma  and adj ewlr .
1.	related work
¡¡in general  spatio-temporal methods can be divided into three classes: spatial modeling techniques such as  disease mapping   where observed values are spatially smoothed to infer the distribution of values in space-time  1  1 ; tests for a general tendency of the data to cluster  1  1 ; and tests which attempt to infer the location of clusters  1  1  1 . we focus on the latter class of methods  since these are the only methods which allow us to both answer whether any significant clusters exist  and if so  identify these clusters. three spatio-temporal cluster detection approaches have been proposed by kulldorff et al.: the retrospective and prospective space-time scan statistics  1  1   and the space-time permutation scan statistic . the first two approaches attempt to detect persistent clusters  assuming that baselines are given based on census population estimates. the retrospective statistic searches over all space-time intervals  while the prospective statistic searches over those intervals ending at the present time. as noted above  these formulations make sense for the case of explicitly given denominator data  and counts proportional to these baselines  e.g. we expect a population of 1 to have twice as many cases as a population of 1  but do not know how many cases we expect to see . they are not appropriate for the case where we infer the expected values of counts from the time series of past counts  e.g. based on past data  we expect to see 1 cases in the first population and 1 cases in the second . even if accurate denominator data is provided  the retrospective and prospective statistics may pick up purely spatial clusters resulting from spatial variation in the underlying rate  e.g. different parts of the country have different disease rates   or purely temporal clusters based on temporal fluctuations in rate  seasonal effects or long-term trends   and thus the detected clusters tend to be less useful for prospective detection of emerging outbreaks.
¡¡the recently proposed  space-time permutation scan statistic   attempts to remedy these problems; like the present work  it allows baseline data to be inferred from the time series of past counts. as noted above  baselines are calculated by assuming that cases are independently distributed in space and time  and a variant of the test for persistent clusters is used  searching for regions with higher rate inside than outside . then randomization testing is done by permuting the dates and locations of cases. this method focuses on detecting space-time interaction  and explicitly avoids detecting purely spatial or purely temporal clusters. the disadvantages of this are twofold. first  it loses power to detect spatially large clusters  because  as noted above  the current day's counts are used to estimate what the current day's counts should be. in the most extreme case  a spatially uniform multiplicative increase in disease rate over the entire search area would be completely ignored by this method  and thus it is unsafe to use for surveillance except in combination with other methods. the second disadvantage is that if the count decreases in one spatial region and remains constant elsewhere  this is detected as a spatio-temporal cluster. this results in false positives in cases where stores in one area are closed and stores in a different area remain open: the open stores are flagged as a cluster even if their counts have actually decreased.
¡¡several other spatio-temporal cluster detection methods have also been proposed. iyengar  searches over  truncated rectangular pyramid  shapes in space-time  thus allowing detection of clusters which move and grow or shrink over time; the disadvantage is that this much larger set of possible space-time regions can only be searched approximately. assuncao et al  assume a spatiotemporal poisson point process: the exact location of each point in time and space is given  rather than aggregating points to discrete locations and intervals. a test statistic similar to the space-time permutation scan statistic is derived  assuming a poisson intensity function that is separable in space and time.
1.	computational considerations
¡¡we begin by making two important observations. first  for any of the time series analysis methods given above  the baselines bti  t  w   t ¡Ü t  can be inferred from the past counts cti  1 ¡Ü t ¡Ü t  w  in o t . second  we can compute the score function d s tmin   for a given spatial region s and for all t  w  tmin ¡Üt  in total time o w   regardless of whether the persistent or emerging scan statistic is used. this is obvious for the persistent statistic since we can simply proceed backward in time  adding the cumulative count ct and cumulative baseline bt for each day t  and recomputing the score.  we can accumulate these counts and baselines in o w  by using the  cumulative counts  trick discussed in  for each of the w current days.  the o w  complexity is less obvious for the emerging statistic  since adding any new day t may result in up to o w  pops from the stack. but each day is pushed onto the stack at most once  and thus the total number of pops for the w days is at most w  giving total complexity o w   not o w1 .
¡¡for the bats method  our computation may be divided into three steps: first  we compute baselines for each spatial location  requiring total time o nst   where ns is the number of locations. second  we aggregate  current  store baselines and counts to the grid  requiring time o n1w  where n is the grid size. third  we search over all spatio-temporal regions  s tmin : for each such region  we must compute the aggregate counts and baselines  and apply the score function d. as noted above  we can do this in o w  per region  but since a naive search requires us to examine all o n1  gridded rectangular regions  the total search time is o n1w   bringing the total complexity to o nst +n1w . for cats  we first aggregate all store baselines and counts to the grid  requiring time o nst +n1t . then we calculate baselines for each of the n1 grid cells  requiring total time o n1t . finally  we search over all spatio-temporal regions; as in bats  this requires time o n1w   bringing the total complexity to o nst +n1t + n1w . for rats  we first aggregate all store baselines and counts to the grid  as in cats   requiring time o nst +n1t . then for each of the n1 regions we search  we must calculate the baselines for  current  days on the fly  requiring time o t   and compute the score function using the counts and baselines for current days  requiring time o w . thus the total complexity is o nst +n1t .
¡¡for large grid sizes n  the o n1  complexity of searching over all spatial regions makes a naive search over all such regions computationally infeasible. however  we can apply the fast spatial scan of  1  1   allowing us to find the highest scoring region and its pvalue while searching only a small fraction of possible regions. in the purely spatial case  the fast spatial scan works by using a multiresolution  branch-and-bound search to prune sets of regions that can be proven to have lower scores than the best region score found so far. we can easily extend this method to the space-time case: given a spatial region s  we must upper bound the scores d s1 tmin  for all regions s1   s and t  w   tmin ¡Ü t. the simplest way of doing so is to compute separate bounds on baselines and counts of s1 for each time step t  using the methods given in   then use these bounds to compute an upper bound on the score. it might also be possible to achieve tighter bounds  and thus  better pruning  by enforcing consistency constraints across multiple days  i.e. ensuring that s1 has the same spatial dimensions on each time step.
1.	results
¡¡we evaluated our methods on two types of simulated outbreaks  injected into real emergency department and over-the-counter drug sale data for allegheny county  pa.1 first  we considered aerosol releases of inhalational anthrax  e.g. from a bioterrorist attack   produced by the bard   bayesian aerosol release detector   simulator of hogan et al. . the bard simulator takes in a  baseline dataset  consisting of one year's worth of emergency department records  and the quantity of anthrax released. it then produces multiple simulated attacks  each with a random attack location and environmental conditions  e.g. wind direction   and uses a bayesian network model to determine the number of spores inhaled by members of the affected population  the resulting number and severity of anthrax cases  and the resulting number of respiratory emergency department cases on each day of the outbreak in each affected zip code. each simulated outbreak can then be injected into the baseline ed dataset  and our methods' detection performance can be evaluated using the testing framework below.
¡¡second  we considered a  fictional linear onset outbreak   or  floo    with a linear increase in cases over the duration of the outbreak. a floo outbreak is a simple simulated outbreak defined by a set of zip codes  a duration tfloo  and a value  . the floo simulator then produces an outbreak lasting tfloo days  with t  respiratory cases in each of the zip codes on day t  1   t ¡Ü tfloo/1  and tfloo /1 cases on day t  tfloo/1 ¡Ü t   tfloo. thus we have an outbreak where the number of cases ramps up linearly for some period of time  then levels off. while this is clearly a less realistic model than the bard-simulated anthrax attack  it does have several advantages. it allows us to precisely control the parameters of the outbreak curve  number of cases on each day   allowing us to test the effects of these parameters on our methods' detection performance. also  it allows us to perform experiments using overthe-counter drug sale data as well as emergency department data  while the bard simulator only simulates ed cases.
¡¡we now discuss our basic semi-synthetic testing framework  followed by a discussion of the performance of our methods on each of the three main experiments  anthrax outbreaks in ed data  floo outbreaks in ed data  and floo outbreaks in otc data .
1	semi-synthetic testing
¡¡our basic goal in the semi-synthetic testing framework is to evaluate detection performance: what proportion of outbreaks a method can detect  and how long it takes to detect these outbreaks. clearly these numbers are dependent on how often the method is allowed to  sound the alarm   and thus we have a tradeoff between sensitivity  i.e. ability to detect true outbreaks  and detection time on the one hand  and specificity  i.e. frequency of false positives  on the other. more precisely  our semi-synthetic framework consists of the following components. first  given one year of baseline data  assumed to contain no outbreaks   we run the space-time scan statistic for each day of the last nine months of the year  the first three months are used to provide baseline data only; no outbreaks in this time are considered . we thus obtain the highest scoring region s   and its score d  =d s    for each of these days. then for each  attack  that we wish to test  we do the following. first  we inject that outbreak into the data  incrementing the number of cases as above. then for each day of the attack  we compute the highest scoring relevant region s  and its score d   where a relevant region is defined as one which contains the centroid of all the cases injected that day. the reason that we only allow the algorithm to search over relevant regions is because we do not want to reward it for triggering an alarm and pinpointing a region which has nothing to do with the outbreak. we then compute  for each day t = 1...toutbreak  where toutbreak is the length of the attack   the fraction of baseline days  excluding the attacked interval  with scores higher than the maximum score of all relevant regions on days 1 to t. this is the proportion of false positives we would have to accept in order to have detected that outbreak by day t. by repeating this procedure on a number of outbreaks  we can obtain summary statistics about the detection performance of each method: we compute its averaged amoc curve   average proportion of false positives needed for detection on day t of an outbreak   and for a fixed level of false positives  e.g. 1 false positive/month   we compute the proportion of outbreaks detected and the average number of days to detection. note that this basic framework does not perform randomization testing  but only compares scores of attack and baseline days. there are several disadvantages to this method: first  since the baselines bti for each day are different  the distribution of scores for each day's replica grids will be different  and thus the highest scoring regions may not correspond exactly to those with the lowest p-values. a second disadvantage is that it does not tell us how to perform calibration: setting threshold p-values in order to obtain a fixed false positive rate in real data. this is discussed in more detail below.
¡¡we tested a total of 1 methods: each combination of the three aggregation levels  bats  cats  rats   five space-time scan statistics  1-day  1-day emerging  1-day persistent  1-day emerging  1day persistent  and the ten methods of time series analysis listed above. we compared these methods against two simple  straw men : a purely spatial scan statistic  assuming uniform underlying at-risk population  and thus setting the baseline of a region proportional to its area   and a purely temporal scan statistic  analyzing the single time series formed by aggregating together all spatial locations  using 1-day all mean . since both the ed and otc datasets were relatively small in spatial extent  containing only records from allegheny county   we used a small grid  n = 1  maximum cluster size = 1   and thus it was not necessary to use the fast spatial scan. for larger datasets  such as nationwide otc data  a much larger grid size  e.g. n = 1  is necessary to achieve adequate spatial resolution  and thus the fast spatial scan will be an important component of our nationwide disease surveillance system.
¡¡for each outbreak type  we compared the detection performance of our methods to the two straw men  and also determined which of our methods was most successful  table 1 . performance was evaluated based on detection rate  proportion of outbreaks detected  at 1 false positive/month  with ties broken based on average number of days to detect; we list both the performance of our  best  spatio-temporal method according to this criterion  as well as a representative  median  method  i.e. the 1th best method out of 1 . we compare the methods in more detail in table 1  giving each method's average number of days to detection at 1 false positive/month  assuming that undetected outbreaks were detected on day toutbreak. for each of the five scan statistics  we report performance assuming its best combination of time series analysis method and aggregation level; for each of the ten time series analysis methods  we report performance assuming its best scan statistic. level of aggregation only made a significant difference for the all max and strat max methods  so we report these results separately for bats  cats  and rats. for each outbreak  we also construct amoc curves of the  best    median   purely temporal  and purely spatial methods; we present three of these curves  one for each outbreak type  in figure 1. we also discuss each outbreak type in more detail below.
1	anthrax outbreaks  ed data
¡¡for the anthrax outbreaks  we began with real baseline data for respiratory emergency department visits in allegheny county in 1. we used this data to simulate epidemics using bard at two different levels of anthrax release: 1  high  and 1  low . for each release amount  1 simulated epidemics were created. separately for the high and low levels  we tested all methods  forming an average amoc curve for each over all simulated epidemics  and measuring detection rate and average days to detect.
¡¡for the high release dataset  all of the methods tested were able to rapidly detect all 1 outbreaks. for a fixed false positive rate of 1/month  every method detected all outbreaks  1% detection rate   with average time to detection ranging from 1 to 1 days. the top method  1 days to detect  was the 1-day statistic using all mean  and half of all methods detected in 1 days or fewer. since the average delay from release to the first reported case was 1 days  these times were close to ideal detection performance. all methods except all max outperformed the purely temporal scan statistic  1% detection rate  1 days to detect   and all methods outperformed the purely spatial scan statistic  1% detection rate  1 days to detect . for this dataset  there was very little difference between the best and worst performing methods  and thus it is hard to draw definitive conclusions. nevertheless  we observed that shorter temporal windows performed better  1-day was best  1day was worst   and there were no significant differences between emerging and persistent scan statistics. looking at the outbreak curve for this epidemic  it is clear why this is the case: all outbreaks have huge spikes in the number of cases starting on day 1 or 1  so there is no advantage to having a longer window; and since there is essentially no  ramp-up  in the number of cases  just the large spike  at which point the outbreak is obvious to any method  there is no advantage to the emerging over persistent statistics. for time series analysis  the all mean method performed best  followed by adj ewma. this result is somewhat surprising  suggesting that the ed baseline data has very little day of week or seasonal trends.
¡¡results on the low release dataset were similar  except for two differences resulting from the amount of release. first  1 of the 1 outbreaks were missed by all of our methods; these outbreaks consisted of a very small number of cases  less than 1 in total   and as a result there was essentially no signal to detect. the other 1 outbreaks typically produced a large and obvious spike in the number of cases  again  with very little ramp-up prior to the spike   though the delay between release and spike was longer on average  1 days from release to first reported case . again  the 1-day window was best  though the 1-day statistics performed almost as well  and again all mean and adj ewma were the top two methods. our spatio-temporal methods again outperformed the straw men  requiring 1 days to detect  best  and 1 days to detect  median  at 1 false positive/month. this was substantially better than the purely temporal and purely spatial methods  which required 1 days and 1 days respectively.
1	floo outbreaks  ed data
¡¡for the floo ed outbreaks  we again began with the 1 allegheny county ed dataset. we injected three types of floo attacks  assuming that only zip code 1  pittsburgh  was affected:   = 1 tfloo = 1     = 1 tfloo = 1   and   = 1 tfloo =
1 . thus the first attack has the fastest-growing outbreak curve  1t cases on day t   and the third has the slowest-growing outbreak curve  t cases on day t . for each outbreak type  we simulated outbreaks for all possible start dates in april-december 1  and computed each method's average performance over all such outbreaks. all the spatio-temporal methods were able to detect all injected outbreaks at a rate of 1 false positive/month; not surprisingly  median number of days to detect increased from 1 for the fastest growing outbreak  to 1 for the slowest growing outbreak. all of these detection times were more than one full day faster than the purely spatial and purely temporal methods  with one exception  1 days faster than purely spatial for   = 1 . again  the all mean method performed well  1-day all mean was the winner for   = 1  with a detection time of 1 days   as did adj ewma and strat ewma  1-day emerging strat ewma was the winner for   = 1 and   = 1  with detection times of 1 and 1 days respectively . our most interesting result was the effect of the temporal window size w: for the fastest growing outbreak  the 1-day method detected outbreaks 1 days faster than the 1-day and 1-day methods  but for the slowest growing outbreak  both 1day and 1-day methods detected outbreaks a full day faster than the 1-day method. emerging methods outperformed persistent methods for approximately 1% of our trials  though the difference in detection time was typically fairly small  1-1 days  depending on the time series analysis method . we also observed that higher aggregation typically performed better for the all max and strat max methods  i.e. rats performed best  and bats worst .
¡¡

	bard  1 	floo ed  1 	floo otc  1 
figure 1: amoc curves for three of the eight datasets. the four curves are for the best spatio-temporal method  1   the median spatio-temporal method      the purely temporal method      and the purely spatial method  + . note that the purely temporal method  unlike the others  is not required to pinpoint the region location  so its amoc will be lower at the start of an attack  before there are a sufficient number of cases to detect ; this is purely a function of the testing methodology  and does not imply better performance.
table 1: summary of performance. detection rate and average days to detect  1 false positive/month  all datasets.
bestmediantemporalspatialdatasetratedaysratedaysratedaysratedaysbest methodbard  1 111111111-day  all meanbard  1 111111111-day  all meanfloo ed  1 111111111-day emerging  strat ewmafloo ed  1 111111111-day emerging  strat ewmafloo ed  1 111111111-day  all meanfloootc  1 111111111-day  strat kullfloootc  1 111111111-day  strat kullfloo otc  all1 111111111-day  strat ewlrtable 1: comparison of methods. average days to detect  1 false positive/month  all datasets.
bardbardfloo edfloo edfloo edfloo otcfloo otcfloo otcmethod 1  1  1  1  1  1  1  all1 1-day111111111-day persistent111111111-day emerging111111111-day persistent111111111-day emerging11111111all max bats11111111allmax cats11111111allmax rats11111111allmean11111111strat max bats11111111strat max cats11111111strat max rats11111111strat mean11111111strat ewma11111111adj ewma11111111strat ewlr11111111adjewlr11111111allkull11111111strat kull11111111¡¡
1	floo outbreaks  otc data
¡¡for the floo otc outbreaks  we began with one year's worth of data for retail sales of over-the-counter cough and cold medication in allegheny county  collected from 1/1/1. we injected three types of floo attacks: for the first two  we again assumed that only zip code 1 was affected  but  since the overall numbers of otc sales were much higher than the overall numbers of ed visits  we injected larger numbers of counts    =1 tfloo = 1  and    = 1 tfloo = 1 . for the third attack  we assumed that all zip codes in allegheny county were affected  using    = 1 tfloo = 1  for each. for each outbreak type  we simulated outbreaks for all possible start dates over the last nine months of our data  and computed each method's average performance over all such outbreaks. our first observation was that these attacks were substantially harder to detect than in the ed data: for the two localized attacks  our median methods only detected 1% and 1% of outbreaks for the faster-growing    = 1  and slower-growing    = 1  outbreaks respectively. it appears that the main reason for this was the difficulty in accurately predicting the otc counts for the baseline days  as we observed huge differences in performance between the various time series analysis methods. the data contained significant seasonal and day of week trends  as well as other irregularities  e.g. large spikes in sales in single stores  probably resulting from promotions   and most of our methods were not entirely successful in accounting for these; nevertheless  they performed much better than the purely spatial and purely temporal methods  which only detected 1% of these outbreaks. our second observation was that the strat kull method performed remarkably well in predicting the localized outbreaks  detecting with 1% accuracy in 1 and 1 days for   = 1 and   = 1 respectively; strat kull and all kull detected the   = 1 outbreaks over two days faster than any other methods. this suggests that those methods were able to predict baselines for the non-attack days much more accurately than any of the other time series analysis methods: using the current day's counts to predict the current day's baselines allows accurate adjustment for seasonal trends  and if the attack is sufficiently localized  only slightly reduces detection power. clearly it would be better to have a method which correctly predicts the trends without using the current day's counts  but none of the methods discussed here were able to do this. for the non-localized attack  cases added to every zip code   the power of strat kull was substantially reduced  and it was only able to detect 1% of outbreaks  while our best-performing method  strat ewlr  detected 1%. and this is far from the worst case for strat kull: since different zip codes have different average sales  adding the same number of counts to each creates a large amount of spacetime interaction. if we had instead multiplied counts in each zip code by the same factor  strat kull would have no power to detect this. we also note that the 1-day statistics performed best for all three outbreak types on the otc data  though the 1-day emerging statistics performed almost as well. again  emerging methods consistently outperformed persistent methods  and the difference in detection time was larger than on the ed data  typically 1-1 days . finally  we note that the lower levels of aggregation  bats and cats  outperformed rats for the  max  methods; this is the opposite result from what we observed on the ed data.
¡¡based on these conflicting results  it is difficult to recommend a single method for use on all datasets and outbreak types. as shown above  the optimal temporal window size depends on how fast the number of cases increases  with longer temporal windows appropriate for more slowly growing outbreaks. the optimal temporal window is also affected by our desired tradeoff between number of false positives and detection time: a lower acceptable false positive rate  and thus  longer acceptable detection time  increases the optimal window size. for example  for the floo ed  1  outbreak  the 1-day emerging statistic has the fastest time to detection at a rate of 1 false positive/month  while the 1-day emerging statistic has the fastest time to detection at a rate of 1 false positive/year. as noted above  the emerging statistics consistently outperform the corresponding persistent statistics  and while the amount of difference is not that large  1-1 days across all outbreaks and methods   even slightly earlier detection may make a substantial difference in the efficacy of outbreak response. it appears that the 1-day emerging statistic is a reasonable compromise solution  at least for the set of outbreaks tested. it may also be a good idea to run emerging statistics with different window sizes in parallel  for better detection of both fast-growing and slow-growing outbreaks; optimal combination of detectors is an interesting and open research question. it is clear that the best time series analysis method depends on the characteristics of the dataset  as well as whether the outbreak is spatially localized or occupies a large spatial region: the strat kull method is excellent for localized outbreaks  but should be used only in parallel with another method that can detect largescale outbreaks. for datasets with little seasonal trend  such as the ed data used here  very simple mean and moving average methods are sufficient  but it is still an open question to find a method which can accurately predict baseline counts for otc data without using the current day's counts to predict the current day's expectations.
1	calibration
¡¡as noted above  our testing framework simply compares scores of the highest scoring regions on each day  and computes amoc curves; no randomization testing is done  and thus we do not actually compute the p-value of discovered regions. because our detection performance is high  it is clear that the attacked regions would have lower p-values than the highest scoring regions on nonattacked days. but this does not answer the question of calibration: at what threshold p-value should we trigger an alarm  if nonattacked days were actually generated under the null hypothesis  we could choose some level ¦Á and be guaranteed that we will only trigger false alarms that proportion of the time  e.g. once every 1 days for ¦Á = .1 . however  our null hypothesis  that each count cti is generated by a poisson distribution with mean bti  is clearly false  since bti is only an estimate of what we expect cti to be  assuming that no outbreak is present. if this estimate were unbiased and exactly precise  zero variance   then we would achieve a false positive rate of ¦Á. in practice  however  this estimate can be both biased and highly imprecise. for any method of calculating baselines that is approximately unbiased  but has non-zero variance  i.e. all of our time series analysis methods except all max and strat max   we expect the proportion of false positives to be greater than ¦Á  since the scan statistic picks out any regions where bti is an underestimate of cti. the all max and strat max methods  on the other hand  are conservatively biased  predicting values of bti which overestimate cti on average  but also have non-zero variances; thus they may result in proportions of false positives either higher or lower than ¦Á. to examine the calibration of our methods  we calculated the p-value for each day in both the ed and otc datasets  with no injected attacks . we used a 1-day emerging scan statistic  bats aggregation  with four different time series analysis methods: two unbiased methods  adj ewlr and all mean  and two conservative methods  all max and strat max . r = 1 randomizations were performed  and we counted the proportion of false positives at ¦Á = 1 and ¦Á = 1 for each method on each dataset. see table 1 for results.
ed datasetotc datasetmethod¦Á = .1¦Á = .1¦Á = .1¦Á = .1adj ewlr1111all mean1111stratmax1111all max1111¡¡as expected  we observe a large number of false positives in both datasets for the unbiased methods. for the otc dataset  we also table 1: proportion of false positives. have high false positive rates even for the conservative methods. what conclusions can we draw from this  because of the variance in our predictions  the baseline data  especially the otc data  is not fit well by the null hypothesis. nevertheless  the likelihood ratio statistic  which serves as a sort of distance away from the null hypothesis  is very successful at distinguishing between attacks and non-attacked days. so how can we calibrate the statistic  one option would be to use an unbiased method with a much lower threshold ¦Á  but the problem with this is that it would require a huge number of randomizations to determine whether the p-value is less than ¦Á. another option would be to use a conservative method  but the problem is that these methods not only record fewer false positives  but also are less able to detect a true positive. in fact  as our results above demonstrate  the conservative methods typically have much less power to distinguish attacks from non-attacked days for a given level of false positives  so this is clearly not a good idea. a better option is to trigger alarms for a given threshold on the score rather than on the p-value  with that threshold learned from previous data  e.g. the year of ed and otc data used here . an even better solution might be to account for the uncertainty of our baseline estimates bti  as discussed below  and thus make our null hypothesis more accurately describe the real data.
1.	conclusions
¡¡we have presented a new class of space-time scan statistics designed for the rapid detection of emerging clusters  and demonstrated that these methods are highly successful on the task of rapidly and accurately detecting emerging disease epidemics. we are currently working to extend this framework in a number of ways. perhaps the most important of these extensions is to account for the imprecision in our baseline estimates bti  using methods of time series analysis which not only predict the expected values of the  current  counts but also estimate the variance in these estimates. our current difficulty is that we are testing the null hypothesis that all counts cti are generated from the estimated values bti  but since these values are only estimates  the null hypothesis is clearly false. as a result  as we demonstrated in the previous section  the standard randomization testing framework results in large numbers of false positives  i.e. on most non-attack days we still observe a pvalue less than 1. the combination of time series methods which account for imprecision of estimates  and scan statistics which use distributions that can account for mean and variance separately  e.g. gaussian or negative binomial distributions  should allow us to correct these problems. this will also make the distinction between building-aggregated  cell-aggregated  and region-aggregated time series methods more relevant  as the variance computations will be very different depending on the level of aggregation. a second  and related  extension is accounting for factors such as overdispersion and spatial correlation between neighboring counts. our current methods assume that each spatial location  cell  or region has an independent time series of counts  and thus infer baselines independently for each such time series. when we extend the model to distributions that model mean and variance separately  we should be able to calculate correlations between time series of neighboring spatial locations  and adjust for these correlations.
¡¡finally  we are in the process of applying our spatio-temporal scan statistics to nationwide over-the-counter drug sales  searching for emerging disease outbreaks on a daily basis. scaling up the system to national data creates both computational issues  the use of the fast spatial scan is essential for searching large grids  as well as statistical issues  dealing with irregularities in the data  such as missing data  and increased sales resulting from product promotions . we are currently working with state and local public health officials to ensure that the clusters we report correspond to relevant potential outbreaks  thus rapidly and accurately identifying emerging outbreaks while keeping the number of false positives low.
