we consider the problem of duplicate document detection for search evaluation. given a query and a small number of web results for that query  we show how to detect duplicate web documents with precision ¡« 1 and recall ¡«1. in contrast  charikar's algorithm  designed for duplicate detection in an indexing pipeline  achieves precision ¡«1 but with a recall of ¡«1. our improvement in recall while maintaining high precision comes from combining three ideas. first  because we are only concerned with duplicate detection among results for the same query  the number of pairwise comparisons is small. therefore we can afford to compute multiple pairwise signals for each pair of documents. a model learned with standard machine-learning techniques improves recall to ¡« 1 with precision ¡« 1. second  most duplicate detection has focused on text analysis of the html contents of a document. in some web pages the html is not a good indicator of the final contents of the page. we use extended fetching techniques to fill in frames and execute javascript. including signals based on our richer fetches further improves the recall to ¡«1 and the precision to ¡«1. finally  we also explore using signals based on the query. comparing contextual snippets based on the richer fetches improves the recall to ¡«1. we show that the overall accuracy of this final model approaches that of human judges.
categories and subject descriptors
h.1  information storage and retrieval :  information search and retrieval 
general terms
algorithms  measurement
keywords
compression distance  duplicate detection  machine learning  search evaluation  similarity hash  web search
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
1. introduction
¡¡detection of duplicate or near-duplicate web pages is both an important problem for web search engines  and a very difficult problem. several algorithms have been proposed and evaluated in recent years  1  1  1  1  1  1 . most approaches can be characterized as different types of distance or overlap measures operating on the html strings fetched for pairs of urls being compared. state-of-the-art algorithms  such as broder et al.'s  and charikar's   achieve reasonable precision and/or recall  but do not approach human-level accuracy.
¡¡the problem we address in this paper is duplicate detection as a part of search evaluation. highly accurate duplicate detection is critical for obtaining accurate relevance measurements. to see this  suppose we are interested in measuring the aggregate relevance of results served by various search engines  for a set of sample queries. we obtain a relevance score from a human judge for each of the top n results of each query  from each engine being measured. we can roll these scores up to an aggregate relevance metric for each engine  using a technique such as ndcg . duplicates can affect our measurement in two ways. first  an engine may receive undue credit for returning duplicate results for a given query. unaccounted for  an engine that returns four copies of the  highly rated  appropriate homepage for a navigational query will receive credit four times over. in reality  users of the engine will be annoyed  not four times happier  and the useless duplicates that are returned will push other potentially good results down in the results list. second  rating noise can be introduced when two duplicate results from different engines are not detected  and thus are judged separately and potentially receive different relevance scores. similarly  rating noise is introduced if two non-duplicates are wrongly detected as duplicates  assuming this means only one of them will be judged and they will receive an identical relevance score. therefore  accurate relevance measurement in this framework requires duplicate detection with both high precision and high recall.
¡¡duplicate detection approaches used by modern search engines are designed for very high scalability. as part of building an index of web pages  an engine must efficiently identify duplicates among the billions of urls being crawled and indexed. because of this  recent approaches have focused on transforming the string returned by a url into a single compact hash or  fingerprint.  comparing these fingerprints is very inexpensive and gives a measure of similarity/distance between the content of the corresponding web pages. typically  these algorithms operate on content that is directly fetched via the url  what is fetched by wget or curl -which in many cases is very different from what a user will see in a browser  due to constructs such as frames  javascript  and other dynamic content. fully  rendering  the page like a browser does  by fetching embedded frames  executing javascript  and so on would simply be too costly. for search evaluation  however  we can afford to explore more expensive approaches. we only need to find duplicates among the top search results for a given query  from some small number of search engines being measured  for instance  the top five results from three engines  giving up to 1 total urls ; and  for search evaluation  we only need to do this for hundreds or thousands of sample queries. even accounting for measurement over time  where we might see a new url tomorrow that is a duplicate of one we rated today   the numbers are not large.
¡¡we present an approach to duplicate detection for search evaluation that achieves precision and recall approaching human-level accuracy. the approach has three key elements:   use of multiple text-based signals  combined using a standard machine-learning classifier  as opposed to a single distance algorithm.
  use of signals derived from both the directly-fetched content and the  rendered  content of each url.
  use of signals that are based on the query for which the urls being compared were returned.
¡¡we evaluate performance using a collection of 1 url pairs derived from major search engines' top results on 1 randomly-selected queries; each pair is labeled by multiple human judges as duplicate or non-duplicate. because we want to maximize both precision and recall in a balanced way  we use a balanced f-score  1pr/ p+r   as an overall accuracy measure  where p represents precision and r recall. taking charikar's  random-hyperplane hashing algorithm as a baseline  we find that using multiple signals combined via a decision-tree classifier improves the f-score by approximately 1 percent. adding in signals derived from rendered content gives an additional 1 percent improvement  and adding query-based signals yields another percent. in aggregate  these methods reach an accuracy level that approaches the average accuracy of individual human judges.
¡¡section 1 describes related work. section 1 presents our duplicate detection method and describes several of the signals we use. section 1 describes our evaluation methodology and the human-scored data we collected. experimental results and a comparison of our method's accuracy to previous algorithms appear in section 1. we provide conclusions and possible extensions in section 1.
1. related work
¡¡the majority of the work on duplicate document detection has focused on the problem of finding all duplicate document pairs without comparing all pairs of documents in the corpus. along these lines  manber  introduced the idea of considering all k-byte sequences in the text and fingerprinting the document with the sequences that hash to a bit sequence ending in c 1's where c is a parameter. this work was built on by heintze  to develop a system for plagiarism detection. broder et al.  applied a shingling approach similar to manber  and heintze  to the web.
this approach was extended by schleimer et al.  so that two documents that overlap for a long enough sequence are guaranteed to have at least one matching fingerprint stored. brin et al.  and shivakumar and garcia-molina  1  1  explored schemes for detecting plagiarism using sentence overlap and word frequency comparisons. these schemes compute multiple fingerprints per document and two documents are considered similar if a large fraction of their fingerprints agree. shivakumar and garcia-molina looked at applying these schemes to the web where naively considering all pairs that have at least one fingerprint match is inefficient .
¡¡charikar  developed a technique for fingerprinting documents that allows one to estimate the cosine similarity between documents quickly. the idea is to choose a set of random hyperplanes and then record for each document which half-space of each hyperplane contains the document term vector. we use an implementation of charikar's algorithm as one of the signals for our system.
¡¡recently  henzinger  compared broder et al.'s algorithm and charikar's to determine the precision of each at a fixed recall. henzinger adjusted the parameters of charikar's algorithm to have the same storage needs as that of broder et al.'s and achieve the same recall. the precision of each algorithm was then estimated. henzinger found that both algorithms had higher precision when comparing documents from different sites. since charikar's algorithm identified a higher percentage of duplicate pairs from different sites  it also achieved higher precision than broder's algorithm. henzinger then used the algorithms in combination to produce a new algorithm that gets a better trade-off between precision and recall.
¡¡calibrasi and vitanyi  worked on clustering groups of similar documents using distance measures based on compression algorithms. given a compression algorithm  the normalized compression distance between two documents a and b is defined as
ncd
where ab is the concatenation of a and b and c x  is the compressed size of document x. intuitively  if two documents are similar  the compression of their concatenation is similar in size to each of their individual compressed sizes  i.e.  one document does not add much content beyond what is already in the other. for this study  one of our signals is based on the normalized compression distance.
¡¡finally  in the area of query-specific duplicate detection  gomes and smith suggested using text around the query terms as a means for comparing documents . this method provides another signal we use in our system.
1. approach
¡¡our approach to duplicate detection differs from the previous approaches in three respects. first  instead of relying on a single predictive signal we make use of multiple signals. the intuition is that each signal uses a different dimension to compare pages  and the combination of these signals is a better predictor than any signal alone. second  we use richer fetching techniques to bring in a rendered body which in some cases is more representative of the page as rendered in a browser. third  since our focus is on duplicate detection in the setting of search evaluation  we consider signals that estimate how similar the pages are with respect to a specific query.
1 rendered bodies
¡¡when considering whether two pages are duplicates  previous approaches have focused on the html that can be fetched with a wget command. in a significant percentage of cases  the contents of the html bodies  fetches  do not reflect what is rendered in a browser. in these situations  algorithmic duplicate determination based on the original fetches is usually not accurate.
¡¡we address two situations in which the fetched html is not representative of the rendered appearance of the page:
redirects: in some cases the initial page returned with a simple fetch quickly redirects to a final  landing  page.
a tool like wget can follow http redirects  but it does not handle  meta refreshes  and page loads initiated from scripts on the page. sometimes these techniques are used to branch between different versions of a site. in this case  very similar initial html pages can lead to widely differing landing pages. conversely  redirects are often used to drive traffic to a single page. in this situation the initial pages tend to be stuffed with keywords for search engines  which do not see the redirect  and the landing page is polished for human users.
frames: a fetch of a web page using frames returns the frameset of the page  i.e.  a description of the frame layout and urls for the frame content. two web pages from a site using a common frame layout will differ only in the frame urls; hence  content analysis needs to include the content of those frame urls. the same holds for pages using inline frames  iframes .
¡¡our approach is to render pages with a mozilla-based fetcher similar to crowbar . after the page is loaded in the fetcher  we execute the equivalent of  view source  in firefox to generate the effective html of the url. we do not wait for redirects with long delays. we incorporate frames by grafting their contents into the surrounding document at the place of the frame declaration; this produces invalid html  but is a good base for our content analysis.
¡¡fetching and rendering is much more expensive than simply fetching the content of the initial url: with external frames and scripts many more urls need to be retrieved and rendering a page in a browser is cpu intensive. to reduce bandwidth requirements  we do not fetch images. moreover  we observe that for many urls  the effective html does not differ much from the original html. therefore an interesting area of future work is to develop heuristics for whether expensive rendering of a page is worthwhile or not.
1 predictive signals
¡¡for a pair of urls  we compute a number of different signals based on the urls  the html bodies  the rendered bodies and the text of the query used to score the relevance of the urls. we use only one url-based signal  namely an exact match on the domain of the url. in addition  we compute body-based signals from both the fetched html and the rendered bodies as well as query-specific signals based on the text appearing near the query terms in the page. we begin by describing the body-based signals and then discuss the query-specific signals.
1.1 body-based signals
¡¡for each pair of urls  we fetch both an html body and a rendered body.  see section 1.  for each of these bodies  we compute a number of distance metrics. some of these metrics have been explored in the literature as standalone duplicate detection algorithms but our focus here is on using them as one of many signals in a larger duplicate detection system.
¡¡exact title match: the simplest signal we use is a check of whether the titles of the two documents exactly match. despite the simplicity of this signal  it turns out be very useful for finding pairs of documents that are incorrectly considered duplicates by more sophisticated algorithms.
¡¡similarity-hash distance: we use charikar's algorithm for hashing document term vectors to a 1-bit hash . this hashing scheme ensures that the fraction of bits that agree between two documents' hashes is  in expectation  equal to the cosine similarity between the documents' term vectors.
¡¡tf-idf distance: the tf-idf score of a term  namely the product of the frequency of the term in a document divided by its frequency in the corpus  is a commonly used concept in information retrieval. for duplicate detection  we compare the top 1 tf-idf terms in each document  using a logarithmic idf table. let lx be the list of terms from document x and pos w lx  be the position of term w in lx. for two documents a and b  we compute a distance between la and lb which is biased towards the movements of top terms in the lists. in particular  for documents a and b and a term w  we define their weight as:
weight
¡¡without loss of generality  assume that |la| ¡Ý |lb|. then the tf-idf distance between a and b is given by:
tf-idf distance a b  = 1  w¡Êla weighta b w 
pw¡Êla weighta a w 
¡¡note that if |lb|   |la| then we normalize the tf-idf distance based on lb instead.
¡¡body-length distance: let len x  be the length of the html body for document x. then we define the bodylength distance between documents a and b as bld a b  =  
	1	if len a  = len b  = 1
otherwise
¡¡modified normalized compression distance  mcd : we apply the concept of compression distance to our duplicate detection problem using the gzip algorithm as the underlying compression algorithm. we modify the formula for compression distance to increase the accuracy for documents that are similar to each other. we define the modified normalized compression distance between documents a and
b as
mcd.
¡¡note that the modified normalized compression distance is 1 if and only if c aa  = c ab  = c bb . in contrast  the normalized compression distance may be non-zero for identical documents since c aa  is not guaranteed to equal c a . our modification means that we can distinguish between document pairs that are identical and pairs that are very similar to each other at the cost of decreasing the accuracy of the measurement for very dissimilar pairs. this is acceptable because the difficult part of accurately detecting duplicate documents lies in distinguishing pairs that are duplicates from pairs that are merely very similar to each other.
1.1 query-specific signals
¡¡in search evaluation  the question of whether two urls are duplicates is made in the context of the query for which these two urls were returned as good search results. two pages are duplicates if their content is essentially the same in light of the query. for each url we extracted contextual snippets from the document that appeared near the query words. this text was similar to the text that users see on a search engine's results page as a description accompanying a search result. however we found that a few modifications allowed the snippets to be more accurate for duplicate detection. first  we used much longer snippets than would typically be presented to a user in a search result display. second  search engines sometimes draw snippets from sources other than the visible text of the page  such as meta tags or odp entries . in our case  we restricted the snippets used as a duplicate detection signal to be from the visible text of the page.
1. methodology
¡¡to generate training and test data  we first randomly sampled 1 english queries from google's us querystream in mid 1. each query was run on three search engines  and the first five search results were recorded  yielding up to 1 unique urls for each query. we then asked four human judges to indicate duplicate/non-duplicate for each pair of urls associated with each query.
¡¡in reality  judging whether two urls are duplicates is often very difficult; there are many  edge  cases. after some initial trials  so that we could get consistent data  we produced fairly detailed instructions to guide our human judges on some of the more common hard cases. examples that we indicated should be considered duplicates included:   pages a and b contain the same content and appear to be from the same  site   but one is a  printer friendly  version of the other.
  pages a and b are the same page  from different mirrors/archive sites.
  pages a and b have essentially the same content and layout  with the exception of non-essential elements such as text advertisements  banner ads  or non-critical site  boilerplate  such as legal and copyright notices.   page a automatically redirects the browser to another page  even after a delay of a few seconds   and this page is a duplicate of page b.
examples that we indicated should not be considered duplicates included:
  pages a and b contain primarily the same content  but have different basic layouts and surroundings.
- example: two pages that contain lyrics to the same song  but from completely different sites with different appearance  background and surrounding content.
- example: two pages contain the same information about a hotel  restaurant  etc.  but are different  aggregator  sites with different appearance  layout  etc.
  pages a and b lead to the same content  but one page initially requires a user login or registration while the other does not.
  pages a and b redirect to one another only under certain conditions. example: http://toolbar.yahoo.com and http://toolbar.yahoo.com/firefox only redirect to one another if the user is using the firefox browser.   pages a and b are essentially the same page  but contain different  sort orders  of the same list of items.
¡¡using these instructions  four judges per url pair had full agreement on 1% of pairs. the remaining 1 pairs were reviewed by additional judges  and in the hardest cases  by the authors   until we had a final judgment for every pair. our data set included 1 duplicates out of 1 pairs. note that the small number of duplicate pairs in the sample is partly due to the fact that we filtered out exact duplicate urls returned from two different engines since these are trivial to label correctly. it should be noted that the rater agreement for the class of duplicate pairs was only 1% whereas for the non-duplicate pairs it was 1%. this demonstrates that the difficulty of detecting duplicate documents  even for human raters  lies in distinguishing the duplicate pairs from the pairs that are merely very similar to each other.
¡¡treating the final judgments as ground truth  an interesting accuracy baseline is the precision and recall of individual judges. we took the first  second and third rater who rated each pair and compared their rating to the final rating. the best f-score obtained was 1% with a precision of 1% and a recall of 1%. since these ratings were also used to determine the final duplicate/non-duplicate judgement  this is possibly a higher score than an independent rater could achieve. interestingly  humans do better at recall but have a lower precision than either charikar's algorithm  or our machine-learning approach.
1. experimental results
¡¡based on preliminary results we did not feel that a single duplicate detection algorithm could effectively distinguish duplicate documents with both high precision and high recall. our goal was to understand how much each of the following three techniques improve duplicate detection:
1. use many signals computed from standard fetches andthe url instead of a single duplicate detection algorithm.
1. do extended fetching to generate rendered bodies whichare sometimes more indicative of how the page will appear in a browser. compute signals from the standard fetched bodies and the rendered bodies.
group keysignalsfsame domain same title fetched-body mcd similarity hash tf-idf distancersame rendered title rendered-body mcd rendered similarity hash rendered tf-idf distancefssnippets from fetched bodiesrssnippets from rendered bodiestable 1: signals used for classification
1. use signals based on the query.
¡¡to study this question  we used the publicly available data-mining toolbench weka  to analyze our data. this tool enabled us to experiment quickly with a large variety of classification algorithms. in our initial tests  we were able to obtain much better results with classifiers that were treebased or rule-based. this matched our intuition that the class of duplicates was not inherently linear  and that there were probably subclasses of duplicates definable by boolean conditions over various combinations of signals. to compare various feature sets  we used the average f-score from two classifiers: jrip  a rule-based classifier  weka's implementation of the ripper algorithm   and j1  a decision-tree classifier  a weka implementation of the c1 algorithm  . each classifier was run using 1-fold cross-validation a total of nine times each using different randomization seeds for learning and cross-validation splits. the numbers reported are for the models with median f-score.
¡¡in order to understand the usefulness of our three ideas for improving duplicate detection we considered the following groups of signals in table 1. group f alone represents a set of signals that can be easily computed from a standard fetch. we use this set to evaluate how much improvement can be made over a single duplicate detection algorithm by combining multiple signals. group r are the signals that can be computed from rendered bodies. we added group r to group f to evaluate the combined effect of using multiple signals and richer fetching techniques. when evaluating query-based signals we discovered that snippets derived from the fetched and rendered bodies behave differently. therefore they are listed separately as groups fs and rs respectively. note that body-length distance was not included in sets f or r. it turns out that body-length distance is not useful when modified normalized compression distance is included in the set of signals. however  in practice  body-length distance is a much faster signal to compute and therefore in section 1 we discuss the effects of replacing modified normalized compression distance with body-length distance.
¡¡we first show how much improvement can be gained from each of these three techniques for our us english-based dataset. we then discuss using this model on different languages and queries from outside the us.
1 results
¡¡we first consider the predictive strength of the individual signals  with precision and recall calculated over data 1
1
1
1
1
figure 1: f-score of duplicate detection techniques: similarity hashing  simhash   tf-idf  f signals only  f + r signals  f + r + rs signals  and a human rater.
for which the appropriate bodies were available  in some cases  we were unable to fetch the rendered bodies . the results appear in table 1  ordered by decreasing f-score. the strongest two signals are drawn from the rendered bodies. this matches our intuition that these rendered bodies more closely match what the user sees  and when available often provide better predictors than signals based on the direct fetches. however  it should be noted that since only 1% of the pairs had rendered bodies for both urls  their recall and f-scores over the full set of url-pairs would be lower.
¡¡figure 1 summarizes the improvement in duplicate detection accuracy as we cumulatively add groups of signals corresponding to our three techniques. we use the similarity hash distance as a baseline since it is commonly used in search-engine indexing systems. we also show the f-score for tf-idf distance since it is actually 1 points higher and represents the best f-score obtained by an individual signal computed from a standard fetch. taking a combination of all fetched-body signals provides a 1 point improvement beyond tf-idf distance. further adding signals computed from the rendered bodies adds another 1 points to the fscore. finally  the addition of signals based on the rendered snippets  text surrounding the query terms in the rendered body  also appears to help  ¡«1 points . table 1 gives the breakdown of precision and recall for each of these signal sets. one interesting thing to note is that the improvement in f-score comes largely as a result of improving the recall while maintaining a high precision.
¡¡in section 1  we repeat this analysis with the signals ordered by increasing computational cost.
¡¡the precision-recall curve for the model including all the signals in f  r  and rs is shown in figure 1. while the overall f-score our algorithm achieves is close to that of a human rater  it does so with noticably higher precision but lower recall. for our algorithm to match human-level recall  its precision would be below 1%. we suspect this may be because our algorithms do not detect bolierplate content au-

signalsjripj1average fprfprfrendered snippet mcd1111111rendered tf-idf distance1111111tf-idf distance1111111snippet mcd1111111similarity hash1111111body mcd1111111rendered body mcd1111111rendered body-length distance1111111body-length distance1111111same rendered title1111111rendered similarity hash1111111same title1111111table 1: individual signal strength. rendered-body data were computed over the 1/1  1%  pairs for which rendered bodies were available.
signalsjripj1average fprfprfsimilarity hash1111111tf-idf distance1111111f1111111f + r1111111f + r + rs1111111human raterp = 1r = 11table 1: precision and recall
signalsff  when fs added similarity hash1-f11f + r11f + r + rs11figure 1: the solid line is the precision-recall curve for f + r + rs signals. the dotted line is the fcontour for the max achieved f-score of 1  just below that of the best human rater.
tomatically whereas humans can quickly eliminate irrelevant sections of a web page and determine that the underlying core content of two pages is the same.
¡¡somewhat surprisingly  the addition of snippets generated from the fetched html of the documents does not improve the f-score. results appear in the second column of table 1  where each row shows the average f-score first with and then without signals based on fetched-body snippets.
table 1: effect of fetched-body snippets
1 cross-location performance
¡¡our models were built from data based on queries that originated in the us and were made in english. a natural question is how well such models extend to queries for other locations and in other languages.
¡¡we tested these classifiers on two data sets from google's united kingdom and chinese querystreams. we obtained our test sets in the same way we built our us training data  except that we sampled only 1 queries for each test set. for uk data we had 1 duplicates out of 1 pairs  and for chinese data a significantly smaller 1 duplicates out of 1 pairs. our results appear in table 1. the average f-score of 1 for uk queries indicates that the models trained over us english queries fit well for uk english queries. however  these models do not translate as well to chinese-query duplicate results  producing a maximum fscore of only 1. the small sample size in this dataset  only 1 duplicates  may be having an effect. in any event  more analysis is required to explain the discrepancy. perhaps for double-byte languages  the results can be improved by explicitly training on double-byte language training sets.

test setjripj1average fprfprfuk1111111chinese1111111table 1: test results for chinese and uk queriesfigure 1: performance gain while adding signals of increasing computational complexity. signals computed from the rendered body are indicated by ' r '. errors bars denote the lower and upper quartile of f-scores per signal set.
1 practical efficiency
¡¡the increased performance of our duplicate detection algorithm comes at a computational cost. in a real implementation  there are three classes of computationally intensive tasks  given below:
1. obtaining the page content  which is made up of fetching and rendering. the rendering task is very cpu intensive and therefore has the dominant cost. fetching tasks add latency because they can result in subsequent fetches to pull in secondary documents and the contents of frames. the cost of fetching and rendering scales linearly with the number of documents. in practice  tens of thousands of urls per minute can be handled with a moderate number of servers.
1. processing individual pages  which includes the cost ofcomputing all signals that are based on a single page  tf-idf terms  snippets  similarity hashes  etc . this step is dominated by the cost of parsing each document since computing the signals is relatively cheap once the document is parsed. this step is considerably cheaper than step 1.
1. duplicate determination. we first consider the simple task of determining whether two given urls are duplicates. for this step we compute all the signals based on pair of pages  such as compression distance and tf-idf distance  and then apply our algorithms to these pair-based signals to determine whether the two pages are duplicates. the complexity of this process is dominated by compression distance computations. in practice  depending on the scale of the application  it may be wise to abandon computing compression distances on the entire bodies  but still use compression distances for the snippets. this change reduces the fscore to 1 which is not a large decrease from the 1 achieved when including compression distances over bodies. we now consider the cost of computing all duplicate pairs in a corpus of documents. the complexity of this step is proportional to the number of pairs of documents. in our case  this is proportional to the number of queries and the square of the number of results per query  which greatly reduces the number of pairs of pages that need to be compared.
¡¡based on the above considerations  the signals can be ordered by increasing computational cost. when building a system  a natural approach is to choose only enough signals to reach a desired f-score. figure 1 shows the marginal gain in f-score obtained by adding increasingly expensive signals to our model. we grouped same title and same domain together because individually they each provide very low f-score. interestingly  combining just the three cheapest signals-same domain  same title and body-length distance-already gives an f-score above 1 which is as good as many of the more expensive signals alone. adding similarity hash brings the f-score above 1. computing the compression distance for fetched bodies does improve the f-score but a bigger increase is realized by adding signals based on the rendered content. this again matches our intuition that the rendered content is more indicative of what users see  and that it is beneficial to combine different kinds of signals  in this case  those from both fetched bodies and rendered bodies .
¡¡a practical embodiment of our methodology handling hundreds of queries and tens of thousands of urls a minute is feasible. figure 1 shows an example model when replacing the expensive modified normalized compression distances on bodies with body-length distance in the signal set f + r + rs.
1. conclusions
¡¡our experimental results show that the accuracy of duplicate detection algorithms can be significantly improved by combining multiple text-based signals and computing those signals over both fetched and rendered bodies. we believe the quality of our models could be increased further by  1  detecting and removing boilerplate from document bodies   1  more fine-grained feature selection   1  using more sophisticated url-matching signals  and  1  training over larger data sets.
¡¡the increased performance we gained comes at a computational cost. the resultant algorithm is only feasible over modest-sized problems. it would be interesting to explore the idea of combining multiple signals in other duplicate detection domains with stricter computational limits  such as web crawling and indexing.
 rendered snippet mcd  = 1  and  same rendered title = true  and  bodylengthdistance  = 1  =  class=dup
 rendered tfidfdistance  = 1  and  rendered snippet mcd  = 1  and
 same rendered title = true  =  class=dup
 tfidfdistance  = 1  and  simhashdistance  = 1  and  same title = true  =  class=dup
 rendered tfidfdistance  = 1  and  rendered bodylengthdistance  = 1  and  rendered simhashdistance  = 1  =  class=dup
 rendered tfidfdistance  = 1  and  rendered snippet mcd  = 1  and  rendered simhashdistance  = 1  =  class=dup
 tfidfdistance  = 1  and  rendered snippet mcd  = 1  =  class=dup
 tfidfdistance  = 1  and  bodylengthdistance  = 1  and  same domain = false  =  class=dup
=  class=notdup
