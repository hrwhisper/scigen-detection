a standard approach to cross-language information retrieval  clir  uses latent semantic analysis  lsa  in conjunction with a multilingual parallel aligned corpus. this approach has been shown to be successful in identifying similar documents across languages - or more precisely  retrieving the most similar document in one language to a query in another language. however  the approach has severe drawbacks when applied to a related task  that of clustering documents 'languageindependently'  so that documents about similar topics end up closest to one another in the semantic space regardless of their language. the problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language  but on the same topic. as a result  when using multilingual lsa  documents will in practice cluster by language  not by topic. 
we propose a novel application of parafac1  which is a variant of parafac  a multi-way generalization of the singular value decomposition  svd   to overcome this problem. instead of forming a single multilingual term-by-document matrix which  under lsa  is subjected to svd  we form an irregular three-way array  each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. the goal is to compute an svd for each language such that v  the matrix of right singular vectors  is the same across all languages. effectively  parafac1 imposes the constraint  not present in standard lsa  that the 'concepts' in all documents in the parallel corpus are the same regardless of language. intuitively  this constraint makes sense  since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations. 
we tested this approach by comparing the performance of parafac1 with standard lsa in solving a particular clir problem. from our results  we conclude that parafac1 offers a very promising alternative to lsa not only for multilingual document clustering  but also for solving other problems in crosslanguage information retrieval. 
categories and subject descriptors 
h.1  information storage and retrieval : information search and retrieval - clustering  retrieval models. 
h.1  information storage and retrieval : systems and software - performance evaluation  efficiency and effectiveness . 
general terms 
algorithms  measurement  design  experimentation  languages  theory  verification. 
keywords 
latent semantic analysis  lsa   information retrieval  multilingual  clustering  parafac1. 
1. introduction 
as the world wide web  www  has developed  content has become readily available in a multitude of languages  and interest has grown in the problem of cross-language information retrieval  clir   see for example  . based on our own fairly informal survey  using google and limiting results of a variety of queries by language   we believe that figure 1 is a reasonable estimate of the distribution of internet content by language. 
 


 
copyright 1 association for computing machinery. acm acknowledges that this contribution was authored or co-authored by an employee  contractor or affiliate of the u.s. government. as such  the government retains a nonexclusive  royalty-free right to publish or reproduce this article  or to allow others to do so  for government purposes only. 
kdd'1  august 1  1  san jose  california  usa. 
copyright 1 acm 1-1-1/1...$1. 
figure 1. estimated www content  distribution by language 
 
 moves in various parts of the world towards political integration are another significant driver for the interest in clir. nowhere is this more evident than in the european union  eu   where official documents are created  and must be managed  in an everincreasing number of languages. indeed  the eu has funded a significant amount of research in recent years into clir; the cross-language evaluation forum  clef   is one example.  
our own interest in clir is as a means to cluster documents from the www. clearly  these documents could be in any language  but we would like to cluster the documents by topic  factoring language out  so that documents on the same topic appear close to one another irrespective of their language. 
in section 1  we outline a standard approach to clir  and in section 1  we describe our implementation of that approach. as described in section 1  we found that we were able to confirm that this approach worked well for certain clir problems  but that it had severe drawbacks when we attempted to use it for crosslanguage clustering. the reasons for this are discussed  and we propose a novel alternative approach using parafac1 instead of standard svd in section 1. we compare how parafac1 measures up to standard lsa in practice in section 1  and conclude on our results in section 1. 
 
1. a standard approach to clir 
a standard approach to cross-language information retrieval uses latent semantic analysis  lsa   in conjunction with a multilingual parallel aligned training corpus. this application of lsa to multilingual data is described in  and used in . a term-by-document matrix of weighted frequencies is formed from the corpus; each 'document' consists of the concatenation of all the languages  so terms from all languages will appear in any given document. a variety of weighting schemes can be used  but the log-entropy weighting scheme is generally believed to be one of the most effective for this purpose . in this scheme  the weighted frequency xt n of a particular term t in a particular document n is given by: 
 xt n = log1  ft + 1     1 + ht / log1  n    where ft is the raw frequency of t in n  ht is a measure of the entropy of the term across all documents  and n is the number of documents in the corpus.  accordingly  log1  n  is the maximum entropy that any term can have in the corpus;  1 + ht / log1  n   is 1 for the most distinctive terms in the corpus  1 for those which are least distinctive.  
in the standard approach  the term-by-document matrix of weighted frequencies x is subjected to svd: x = usvt. the output is a term-by-concept matrix  u  or the matrix of left singular vectors   a set of singular values  s  a diagonal matrix   and a document-by-concept matrix  v  or the matrix of right singular vectors . the number of columns computed for u and v is referred to as the number of lsa dimensions. vectors for new documents  those not in the original parallel corpus  are computed by multiplying the vectors of weighted frequencies of terms in the new documents by us-1. the cosine between any two such vectors is a measure of the similarity between those two documents. 
there are a number of well-understood practical advantages to using an approach like lsa for clir. essentially  the parallel corpus used for training acts like a 'rosetta stone'; it is the key which unlocks the door to comparing documents across language boundaries  while the underlying algorithms remain constant regardless of which languages are being compared. this becomes particularly advantageous when language-specific expertise is in short supply. an alternative approach to clir which is commonly employed  for example  is to translate documents: before computing a similarity  the source document is translated into the language of the target document. however  even if a machine translation  mt  system is used to automate this procedure  it is usually the case that a separate mt system must be put in place for each language pair  and that some familiarity with each language in the pair is required to build each such system. for any significant number of languages  the cost of building the required 'system of systems' is likely to be prohibitive  even if the expertise and resources required to do so are available. another alternative approach  exemplified in   is to use bilingual dictionaries  but these may not be available in all languages. in light of this  it is easy to see the attractiveness of a generic approach like lsa which relies only on the ability to tokenize text at the boundaries between words  or more generally semantic units - a procedure which can be generalized to virtually all languages  even logographic languages like chinese. 
 
1. implementation of the standard approach 
in implementing multilingual lsa  perhaps the major decision to be made is which parallel aligned corpus to use in training. for the work described here  we used the bible. although it is hard to come by reliable statistics which allow direct comparison  the bible is generally believed to be the world's most widely translated book        with at least partial translations into at least 1 languages and full translations into at least 1 languages . a single website  has at least 1 parallel translations in over 1 languages  table 1 lists most of these ; almost all of the translations available for download are publicdomain  and all are in a tab-delimited format which can easily be aligned by verse  see figure 1 for an example . 
 
 
figure 1. sample data from publicly-available parallel corpus 
 

language number of translations available afrikaans 1 albanian 1 arabic 1 aramaic 1 armenian  eastern  1 armenian  western  1 basque 1 breton 1 chamorro 1 chinese  traditional  1 chinese  simplified  1 croatian 1 czech 1 danish 1 dutch 1 english 1 esperanto 1 estonian 1 finnish 1 french 1 german 1 greek  modern  1 greek  new testament  1 hebrew  modern  1 hebrew  old testament  1 hungarian 1 indonesian 1 italian 1 japanese 1 korean 1 latin 1 latvian 1 lithuanian 1 manx gaelic 1 maori 1 norwegian 1 polish 1 portuguese 1 romani 1 romanian 1 russian 1 scots gaelic 1 spanish 1 swahili 1 swedish 1 tagalog 1 thai 1 turkish 1 vietnamese 1 wolof 1 xhosa 1  
total  
1 table 1. languages potentially available for multilingual lsa conveniently for our purposes  all of the languages represented most frequently in the www  see figure 1  are also represented in . the list of represented languages is less biased towards european languages  or at least languages of a particular language group  than is commonly the case with purpose-built parallel corpora  a reflection of the reasons that the translations of the bible exist in the first place.1 in addition  there is evidence that even when sections of the parallel corpus are defective  for example  when only a portion of the bible exists in a particular language   the defective sections can still be used without overall detriment . we estimate  therefore  that using the bible  in the dozens of translations that we have already downloaded  as a parallel corpus for training lsa  we would achieve about 1% coverage of internet content  a coverage which would have been hard to match using parallel text from any other single source. 
a question which is commonly raised is how representative the vocabulary of the bible is of modern vocabulary  and therefore how suitable it is as training data. one answer to this is that it depends on which translations are used; many languages have multiple translations of the bible  among our downloads  for example  there are 1 english translations ranging from the king james version  dating from 1  to the world english bible  dating from 1 . clearly  the more modern the translation  the better will be the coverage of the modern language. according to   the bible's coverage may be somewhere between 1%-1%  the vocabulary which is not covered consisting mostly of technical terms and proper names. our own informal tests confirmed that this estimate is probably not too far off; based on a sample of 1 web pages we collected  and after removing items which were treated as words by our tokenizer but cannot reasonably be considered words  such as ' mf'  ' g'   we believe that our coverage of vocabulary  as opposed to languages  from the www would be around 1%. in any case  there is no reason to suppose that coverage has to approach 1% to allow for effective clir: in fact  we shall present evidence in this paper that vocabulary coverage of even less than 1% is sufficient to allow a high level of precision in solving certain clir problems. and although we have used the bible as the training data  there is no reason that the approach could not be extended to the bible plus additional parallel corpora. 
since the bible is alignable by verse  and there are more than 1 verses in the bible  each averaging about a sentence or two in length  an extremely fine-grained term-by-document matrix can be created. generally  we have found that the finer the granularity  the better clir results we obtain. with 1 parallel versions and using our alignment scheme1  our term-by-document 

matrix was 1 1 by 1. as is typical in natural language processing  this matrix is extremely sparse; the number of nonzeros in this case was 1 1  representing a density of around 1%. we stored the parallel text in a relational sql server database to allow for easy aggregation of the statistics required to form different term-by-document matrices for different language combinations and use by different clir algorithms. to compute the svd  we used either svdpack  or a library called anasazi   which is part of the trilinos framework . in each case  we computed a truncated svd corresponding to the 1 highest singular values. we found  however  that svdpack was unable to cope with the size of term-by-document matrices necessary to process more than around two dozen languages in parallel  and thus we resorted in these cases to using trilinos  which is designed to run on a linux cluster and is consequently considerably more scalable . the results of svd were then imported back into sql server and we used sql scripts to compute the vectors for new documents or queries  for example those in the test set. 
 
1. validation of lsa 
1 test data and method 
the test data we used were the 1 suras  chapters  of the quran  which has also been translated into a number of languages. clearly  test data of this sort are a prerequisite in order to be able to measure effectiveness in multilingual clustering. for most of the work described in this paper  we limited the selection of languages to arabic  english  french  russian and spanish  the respective abbreviations ar  en  fr  ru and es are used hereafter   in both the training and the test data. with this data  the initial term-by-document matrix was 1 by 1 with 1 1 nonzeros. with the five languages  the test data amounted to 1 documents: a relatively small set  but large enough to achieve statistically significant results for our purposes  as will be shown. note also that although the test documents all come from a single topic domain  it is reasonable to assume that the comparative results we will report in this paper are valid in general  because in all tests we describe  we are using the same test set. 
perhaps surprisingly  the bible's coverage of the quran's vocabulary appears to be lower than the bible's coverage of general www vocabulary. of 1 distinct terms in the quran  only 1  or about 1%  appear in the bible. 
we tokenized each of the 1 test documents  applying the weighting scheme described above to obtain a vector of weighted frequencies of each term in the document  then multiplying that vector by u ¡Á s-1  also as described above. the result was a set of projected document vectors in the 1-dimensional lsa space. 
 
1 evaluation measures 
we used four separate measures to evaluate the effectiveness of clir given this data. these measures are listed in table 1. 
table 1. clir measures 
# measure 1 precision at 1 document  for a given source and target language  1 precision at 1  for a given source and target language  1 multilingual precision at 1 documents  for 1 languages  1 multilingual precision at 1  for 1 languages   
there is sometimes confusion about the different measures of precision  so for the avoidance of all doubt  we shall spell out how exactly these measures are calculated. the first of these  precision at 1 document  is the proportion of cases  on average  where the translation was retrieved first. for example  if french sura number 1 was the most similar sura among all the french suras to english sura number 1  then precision at 1 document in this case would be 1  and 1 otherwise. this is a strict measure  since no more credit is given if the translation is ranked second than if it is ranked bottom. the second measure  precision at 1  is less strict. this represents the maximum precision at any level of recall. since we are dealing with translations  only one document is considered relevant  and precision at 1 is therefore the inverse of the ranking of the translation. these first two measures relate to the effectiveness of our clir technique in finding similar documents given the language of the query and the language of the results  and for convenience we will refer to these two measures collectively as 'language-specific' precision metrics. 
measures 1 and 1  on the other hand  relate to the effectiveness of our technique in finding similar documents regardless of source or target language. these measures give an indication of how well multilingual clustering is likely to work. since we have 1 languages  the best result we could achieve for clustering would be to have all five translations ranked in the top 1 in similarity to the query. 'multilingual precision at 1 documents'  therefore  represents the proportion of the top 1 retrieved results which are translations of the query  and 'multilingual precision at 1'  again  the less strict measure  represents the maximum precision at any level of recall after the fifth document. we refer to these two measures as 'multilingual' precision metrics. 
 
1 results with lsa 
using the standard approach and measures 1 and 1 as the evaluation metric  we obtained our best results using lsa with the given languages with 1 dimensions. these results are shown in table 1 and table 1. 
table 1. precision at 1 document with standard lsa 
 ar en es fr ru ar 1 1 1 1 1 en 1 1 1 1 1 es 1 1 1 1 1 fr 1 1 1 1 1 ru 1 1 1 1 1  
table 1. precision at 1 with standard lsa 
 ar en es fr ru ar 1 1 1 1 1 en 1 1 1 1 1 es 1 1 1 1 1 fr 1 1 1 1 1 ru 1 1 1 1 1  
on average  precision at 1 document here is 1  and precision at 1 is 1.  as more parallel translations are added  both these precisions rise further  to around 1 and 1 with 1 languages and 1 parallel translations .  these averages include the diagonal values of 1. these reflect very favorably on the ability of the standard approach to identify translations  providing the search space is limited in each case to a single language: here  almost 1% of the time  the translation is retrieved first. the results also compare favorably with published results which use different methodologies for clir  using a different data set  mcnamee and mayfield report mean average precision of no more than 1 for english-to-spanish clir using 1-grams  . recall that these results were achieved despite the bible's coverage of the quran's vocabulary being less than 1%; proof  it would seem  that even with only partial coverage of the target vocabulary  clir can be very effective. 
under measures 1 and 1  however  a different picture emerges. the relevant results are presented in table 1  not broken down by language pair  because the different languages are now mixed together in the test set . 
table 1. 'clustering' precision with standard lsa 
measure results multilingual precision at 1 documents 1 multilingual precision at 1 1  
it is worth noting that under standard lsa  while languagespecific precision tends to increase as more lsa dimensions are used  at least up to 1 dimensions  which is as far as we have tested   the opposite seems to be true for multilingual precision  at least to a certain point. above 1 dimensions  it appears that multilingual precision generally decreases  see figure 1 . 
precision by number of dimensions  for lsa

figure 1. effect of number of dimensions on lsa 
following the findings in   we attempted to boost either language-specific or multilingual precision by increasing the number of parallel translations used in training lsa. our results did confirm the premise in  that more linguistic parallelism is beneficial to lsa  both for language-specific and multilingual precision . however  even with 1 parallel translations  multilingual precision rises no higher than 1  at 1 documents  and 1  at 1 ; and from figure 1 it will be seen that in the best case  we were unable to achieve multilingual precision at 1 of above around 1 using lsa. considering that these measures can never be lower than 1 with 1 languages  since each document is always most similar to itself  and therefore ranks top in the results   these results are all the more unimpressive: on average  hardly any of the second-to-fifth ranked results are translations of the query. how can this be  when the first two measures produced much more encouraging results  
1 limitations of lsa 
in part  this can be answered by considering what happens when we attempt to use the lsa document vectors to 'map' the documents in a graphical representation  such that similar documents are located close to one another. when we attempted this  we found that the documents essentially cluster by language  not by topic. to understand how this can happen  consider the hypothetical example of some ranked results shown in table 1. 
table 1. example illustrating low multilingual precision 
ranking language of retrieved document relevant  1 english yes 1 english no 1 english no 1 english no 1 english no 1 french yes 1 french no 1 spanish yes 1 russian yes 1 arabic yes  
in this example  while the first document retrieved in each language was the relevant one  many non-relevant english documents were returned before the relevant documents in the other languages. in this example  measures 1 and 1 would each have been 1  but measure 1 would have been only 1. this occurs  we believe  because each language has its own distinctive statistical 'signature'  as is reflected in the differing counts for 'types'  unique terms  versus 'tokens'  instantiations of those terms in the text  in the parallel bible text we used in training. these counts are shown in table 1. 
table 1. types and tokens in the bible by language 
 types tokens arabic 1 1 english 1 1 french 1 1 russian 1 1 spanish 1 1  
total  
1  
1 1  
assuming that the translations of the bible in our parallel corpus are accurate and complete  this table would appear to suggest that arabic takes just over half the number of terms to express the same amount of information as english  that english and french take similar numbers of terms  and so on. intuitively  this seems right given that arabic and russian rely much more than than english  french and spanish on the use of morphology  endings  and so on  to add to or modify the meanings of words. the same phenomenon can also be illustrated well on a small scale by considering the first 'document' in the parallel corpus  the first verse   shown in table 1. it is likely to be no coincidence that the best cross-language prediction results we achieved were for pairs of languages with similar statistics  for example  english and french   and the worst results were for those with dissimilar statistics  such as arabic and english   see table 1 and table 1 . 
table 1. illustration of statistical differences between languages  
 text word count % of 
total ar  .                              
 1 1 en in the beginning god created the heavens and the earth. 1 1 fr au commencement dieu cr¨¦a les cieux et la terre. 1 1 ru §£ §ß§Ñ§é§Ñ§Ý§Ö §ã§à§ä§Ó§à§â§Ú§Ý §¢§à§Ô §ß§Ö§Ò§à §Ú §Ù§Ö§Þ§Ý§ð. 1 1 es en el principio cri¨® dios los cielos y la tierra. 1 1  
total  
1  
1  
the statistical differences can  in fact  be shown to have a detrimental effect on lsa - not just empirically  but theoretically as well. under the standard log-entropy weighting scheme  we can verify whether  according to this scheme  the contribution of each of the 1 languages in our multilingual aligned parallel corpus is equal - which it should be  if the translations are complete and accurate. the computed entropy is a measure of information content: because the same information is being conveyed in the translations of any given document in the training corpus  the total entropy per language  the sum of term entropies of terms in that language  should be constant for any given document. 
upon examination  we found that with the standard log-entropy weighting the computed information content varies quite widely by language  which is perhaps unsurprising. if it takes 1 russian words to express what english says in 1 words  then on average russian words must contain more information  or meaning  than english words  again  a notion which is consistent with what we know about the way words are formed in russian and english . however  since entropy  or information content  in the log-entropy scheme is simply the entropy of a particular term across all documents  and since the scheme takes no account of the specific properties of different languages  there is no guarantee that the contributions of different languages in the parallel corpus will be equal as they should be. in fact  in our parallel corpus  where under lsa all languages are 'mixed together' in the bag-of-words approach  languages which have more terms overall  such as english and french  generally account for a higher percentage of the 'information' in each document. this points to a flaw in standard multilingual lsa  or at least in the log-entropy weighting scheme as applied within that approach. 
one other point to note is the difference between the total of 1 shown in table 1 above  and the figure of 1 mentioned in section 1. the difference of 1 represents those terms that occur in more than one language  such as 'de'  'of' in french and spanish   english 'coin' versus french 'coin'  'corner' . the relatively small number of such terms is unlikely to affect the cross-language precision results significantly  but it is worth pointing out that standard lsa has no way to distinguish between homographs from different languages  and in some cases this could be problematic  especially when the homographs have very different meanings in the different languages.1 
given all this  the statistical explanation seems to be a reasonable one for why  when we attempted to map the documents graphically such that similar documents were close to one another  the documents clustered by language rather than by topic. precisely the same issue has been identified elsewhere in the literature: mathieu et al  report that 'even if the crosslingual similarity measure is designed to behave the same when comparing documents written in the same language and documents written in different ones  our evaluation shows that it still tends to gather in a cluster documents of same language prior to different language ones'. 
 
1. an alternative approach 
as discussed in the previous section  it is a drawback of the standard approach to lsa that there is no delineation between different languages in the training data. all languages are concatenated together in training  so that each 'document' is multilingual. within the lsa framework  however  this is unavoidable  since without the concatenation  lsa is unable to make the associations between words in different languages when they co-occur. 
to overcome this problem  therefore  we propose a novel application of parafac1  as an alternative to lsa. parafac1 is a variant of parafac   a multi-way generalization of the svd. the parafac model is based on a 'parallel proportional profile principle' that applies the same factors across a parallel set of matrices to minimize a leastsquares objective. let the m ¡Á n matrix xk  k = 1  ...  k  denote the kth slice of a three-way data array x  and let r be the number of dimensions of the lsa conceptual space. then the standard parafac model is 
 	xk = u sk vt 	 	 	 1  
where u is an m ¡Á r factor matrix for the terms  sk is an r ¡Á r diagonal matrix of weights for the kth slice of x  and v is an n ¡Á r factor matrix for the documents. in this form  it is easy to see parafac's similarity to the svd. here  though  we find factor matrices u and v that are the same for every matrix xk. however  the factors u and v are not orthogonal as they are for the svd. 
in our application  we can let xk be the term-by-document matrix for the kth language in the parallel corpus. it has mk terms and n documents; however  since the number of rows in each slice differs  the parafac model is not appropriate. parafac1 is a related model that is appropriate because it relaxes the constraint that the u matrix is the same across all slices. thus  we form an irregular three-way array  each slice of which is a separate termby-document matrix for a single language in the parallel corpus. the number of documents in each slice will be the same  since the corpus is parallel  but the number of terms will vary by language. the k=1 slices of x for our application are shown in figure 1. 
 
figure 1. term-by-document matrices by language as a threeway array  x  
the parafac1 has the following form: 
 	xk = uk h sk vt 	 	 	 1  
here  there is an orthonormal mk ¡Á r factor matrix u for each slice of x  and an h matrix of size r ¡Á r. because this model lacks certain uniqueness properties associated with the standard parafac model  an invariance constraint is needed on the left factor matrices  i.e.  the product ukh . to gain uniqueness  harshman    imposed the constraint that the cross product  ukh t ukh  is constant over k  which in this formulation is accomplished with the constraint that h is nonsingular. the parafac1 model is shown in figure 1. 
 
figure 1. the parafac1 model 
conceptually  the goal is to compute something like an svd for each language such that v  analogous to a matrix of right singular 'document' vectors  though not orthonormal  is the same across all languages  although for each language k there will be a separate uk  analogous to a matrix of left singular 'term' vectors for language k  and sk  analogous to singular values . a benefit of parafac1 is that it has a separate mapping for each language into the lsa conceptual space; in particular  each mapping is orthogonal for each language rather than the one large orthogonal mapping for all languages at once. in other words  parafac1 imposes the constraint  not present in standard lsa  that the 'concepts'  i.e.  columns of uk  of any given language in the parallel corpus taken on its own map to those of any other language. intuitively  this constraint makes sense  since the whole purpose of using a parallel corpus is that translations are supposed to render the same concepts in different languages. 
to compute the parafac1 model of x  we implemented a variant of the algorithm outlined in  that is adapted to handle very large and sparse data.  the complete procedure is summarized as follows. 
step 1. initialize v as the r principal eigenvectors of ¡Ækxktxk and initialize h and s1 ... sk as r ¡Á r identity matrices. 
step 1a. compute the svd of zk = hskvtxkt = pk¦²kqk by first computing the r principal eigenvectors of zkzkt to obtain pk and normalizing the columns of zktpk to obtain qk  and then update uk as qkpkt  k = 1 ... k.  
step 1b. update h  v  and s1 ... sk by one iteration of an alternating least squares algorithm for standard parafac  equation  1   applied to the r x n x k three-way array with frontal slices uktxk  k = 1 ... k.   see  for an efficient implementation with largescale data.  
step 1.  repeat step 1 until a maximum number of iterations has been reached or the norm of the residual  ¡Æk ||xk - uk h sk vt||  ceases to change appreciably. 
this algorithm was written in matlab using the tensor toolbox     and the parafac1 model was computed on a dual 1ghz pentium xeon desktop computer with 1gb of ram. 
once the parafac1 model has been computed for all languages according to these constraints  the manner in which new documents are projected into the semantic space is similar to that used in lsa. a vector of weighted term frequencies  the term-bydocument vector  is formed as described in 1 above. the difference is that this vector is multiplied by the uksk-1 specific to the language of the document  rather than the general us-1 for all languages which is the artifact of lsa. this relies  of course  on knowing the language of the new document  but there are a variety of machine learning methods for reliably determining the language of an unseen document; one such method  which achieves an accuracy of over 1%  is mentioned in   and we have achieved similar results by training a neural network on the lsa vectors. thus  it can be seen that the additional step necessitated by parafac1 could easily be automated and is not a significant obstacle to wider deployment. 
the main disadvantage of parafac1 compared to lsa is that more computation is required to obtain the decomposition. in fact  since there is currently no parallel implementation of parafac1  we can compute at most 1 dimensions using parafac1. however  as with lsa  the parafac1 decomposition need only be computed once  and the results are then available for use multiple times  so the one-time cost of using parafac1 is essentially one which can be highly leveraged. 
this disadvantage in performance is also offset by an advantage which applies at run-time: since the language-specific uk matrices are considerably smaller than the general u matrix  the process of matrix multiplication can be considerably faster than it is under lsa. there is another linguistic/theoretical advantage to parafac1  and this has to do with the 'homographs' issue identified in section 1 above. since  under parafac1  we are now delineating between the input of different languages in training  english 'coin' is differentiated from french 'coin' - which  one would assume  can only be advantageous in clir since the homographs in this particular pair are  as far as we know  unrelated in meaning. 
 
1. results using parafac1 
with the same training and test data as described in section 1 above  and using parafac1  we obtained the results shown in table 1  table 1  and table 1. since we were limited to 1 dimensions  for a fair comparison we also recalculated precision under lsa using only the top 1 dimensions. the relevant results are shown in table 1  table 1  and table 1. 
table 1. precision at 1 document with parafac1 
 ar en es fr ru ar 1 1 1 1 1 en 1 1 1 1 1 es 1 1 1 1 1 fr 1 1 1 1 1 ru 1 1 1 1 1  
table 1. precision at 1 with parafac1 
 ar en es fr ru ar 1 1 1 1 1 en 1 1 1 1 1 es 1 1 1 1 1 fr 1 1 1 1 1 ru 1 1 1 1 1  
table 1. 'clustering' precision with parafac1 
measure results multilingual precision at 1 documents 1 multilingual precision at 1 1  
table 1. precision at 1 document - lsa  1 dimensions 
 ar en es fr ru ar 1 1 1 1 1 en 1 1 1 1 1 es 1 1 1 1 1 fr 1 1 1 1 1 ru 1 1 1 1 1  
table 1. precision at 1 - lsa  1 dimensions 
 ar en es fr ru ar 1 1 1 1 1 en 1 1 1 1 1 es 1 1 1 1 1 fr 1 1 1 1 1 ru 1 1 1 1 1  
table 1. 'clustering' precision - lsa  1 dimensions 
measure results multilingual precision at 1 documents 1 multilingual precision at 1 1  
from these results it can be seen that parafac1 outperforms standard lsa by a significant margin on the multilingual precision metrics - 1 compared to 1  or 1 compared to 1 depending on which measure is used. this is empirical confirmation that parafac1 lives up to its promise  which is to ensure that the 'concepts' of the different languages are aligned with one another  and to factor out some of the statistical differences between languages that caused problems for lsa. 
it is interesting to note that  based on this set of results  parafac1 also appears to outperform lsa  by a narrower but still highly significant margin  in the language-specific metrics. the average precision at 1 document is 1 for parafac1 compared with 1 for lsa  and for precision at 1 the averages are 1 and 1 respectively. moreover  it will be seen by comparing table 1 with table 1  and table 1 with table 1  that the results using parafac1 are superior almost across the board. the only exceptions are in precision at 1 document: english-to-arabic was slightly lower for parafac1  and french-to-spanish was a tie. in all cases  precision at 1 is better under parafac1. since the average precisions represent the averages across 1  1 ¡Á 1 ¡Á 1  query submissions  the differences between the results for parafac1 and lsa are highly significant  p ¡Ö 1 ¡Á 1 for overall average precision at 1 document  using a chi-squared test . we repeated the same comparisons at various different numbers of dimensions and found that parafac1 consistently outperformed lsa  no matter how many dimensions the decomposition was computed in  and usually the difference was highly statistically significant. in fact  even our best results using standard lsa1 still could not compare with the parafac1 results in table 1 above. 
for reference and comparison with figure 1  the effect of the number of dimensions on precision under parafac1  to the extent we have run tests  and with lines to interpolate for numbers of dimensions not tested  is shown in figure 1. 
precision by number of dimensions  for parafac1

figure 1. effect of number of dimensions on parafac1 
it seems  therefore  that the effect of number of dimensions upon precision under parafac1 follows a pattern similar to that for lsa. 
 
1. conclusion 
in summary  the evidence appears to be highly compelling that parafac1 is a superior alternative to standard lsa for multilingual information retrieval  at least for the two clir problems we want to solve. in line with our expectations  we found that this was particularly true for multilingual document clustering. however  since we had achieved respectable 'language-specific' results using lsa and thus already found it an effective tool for identification of translations  it was more unexpected for us to find that parafac1 essentially beats lsa 'at its own game'. even by the language-specific metrics which portray lsa in a good light  parafac1 is a more effective tool than standard lsa. 
in section 1  we outlined some of the qualitative features which make lsa attractive as a vehicle for clir: essentially  its extensibility to virtually all languages  particularly when used in conjunction with a widely-translated parallel corpus such as the bible. it is important to note that all of these qualitative advantages apply just as much to parafac1 as they do to lsa. 
although parafac1 has a greater lead over standard lsa in the metrics which relate to multilingual clustering than it does in those that relate to language-specific clir  it has to be said that the initial baseline set by lsa was much lower  1 for multilingual precision at 1  compared with 1 for languagespecific precision at 1 . further  even with the boost that parafac1 provides for multilingual precision  the highest multilingual precision that we were able to attain  scarcely over 1  is not as high as we had hoped  and we are still doubtful that this level of precision will overcome the problem that we had hoped to solve  that of preventing documents from simply clustering by language in a graph-based analysis. 
nevertheless  parafac1 represents a good step forward from lsa in addressing this problem. we intend to carry out further experiments to determine whether further adaptations can be made to parafac1 to allow for multilingual document clustering to be carried out successfully. it remains to be seen what these adaptations might look like and to what extent we can streamline the method to maximize multilingual precision  but given the fact that our research with parafac1 is still in a relatively initial stage  we are extremely optimistic that parafac1 offers a promising way forward for truly languageindependent clustering of documents by topic. 
 
1. acknowledgements 
we are grateful to steve verzi  stephen helmreich  and brad mancke for the many constructive comments they have given us as we have worked on the material for this paper. 
sandia is a multiprogram laboratory operated by sandia corporation  a lockheed martin company  for the united states department 	of 	energy's 	national 	nuclear 	security administration under contract de-ac1al1. 
 
