we present and empirically analyze a machine-learning approach for detecting intrusions on individual computers.  our winnowbased algorithm continually monitors user and system behavior  recording such properties as the number of bytes transferred over the last 1 seconds  the programs that currently are running  and the load on the cpu.  in all  hundreds of measurements are made and analyzed each second.  using this data  our algorithm creates a model that represents each particular computer's range of normal behavior.  parameters that determine when an alarm should be raised  due to abnormal activity  are set on a percomputer basis  based on an analysis of training data.  a major issue in intrusion-detection systems is the need for very low falsealarm rates.  our empirical results suggest that it is possible to obtain high intrusion-detection rates  1%  and low false-alarm rates  less than one per day per computer   without  stealing  too many cpu cycles  less than 1% .  we also report which system measurements are the most valuable in terms of detecting intrusions.  a surprisingly large number of different measurements prove significantly useful. 
categories and subject descriptors 
d.1  security and protection    
i.1   artificial intelligence : learning  
general terms 
algorithms  experimentation  security 
keywords 
intrusion detection  anomaly detection  machine learning  user modeling  windows 1  feature selection  winnow algorithm  
 
 
1. introduction 
in an increasingly computerized and networked world  it is crucial to develop defenses against malicious activity in information systems.  one promising approach is to develop computer algorithms that detect when someone is inappropriately intruding on the computer of another person.  however  intrusion detection is a difficult problem to solve .  system performance cannot be adversely affected  false positives must be minimized  and intrusions must be caught  i.e.  false negatives must be very low .  the current state of the art in intrusion-detection systems is not good; false positives are much too high and successful detection is unfortunately too rare.  we report on an approach where we have made significant advances toward creating an intrusion-detection system that requires few cpu cycles  less than 1%   produces few false alarms  less than one per day   and detects most intrusions quickly  about 1% within five minutes . 
intrusion-detection systems  ids's  can either  a  look for known attack patterns or  b  be  adaptive software  that is smart enough to monitor and learn how the system is supposed to work under normal operation versus how it works when misuse is occurring .  we address approach  b  in this article.  specifically  we are empirically determining which sets of fine-grained system measurements are the most effective at distinguishing usage by the assigned user of a given computer from misusage by others  who may well be  insiders   1; 1  within an organization. 
we have created a prototype anomaly-detection system that creates statistical profiles of the normal usage for a given computer running windows 1.  significant deviations from normal behavior indicate that an intrusion is likely occurring. for example  if the probability that a specific computer receives 1 mbytes/sec during evenings is measured to be very low  then 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
kdd'1  august 1  1  seattle  washington  usa. 
copyright 1 acm 1-1/1...$1. 
 
when our monitoring program detects such a high transfer rate during evening hours  it can suggest that an intrusion may be occurring. 
the algorithm we have developed measures over two-hundred windows 1 properties every second  and creates about 1  features  out of them.  during a machine-learning  training  phase  it learns how to weight these 1 features in order to accurately characterize the particular behavior of each user - each user gets his or her own set of feature weights.  following training  every second all of the features  vote  as to whether or not it seems like an intrusion is occurring. the weighted votes  for  and  against  an intrusion are compared  and if there is enough evidence  an alarm is raised.   section 1 presents additional details about our ids algorithm that are being glossed over at this point.  
this ability to create statistical models of individual computer's normal usage means that each computer's unique characteristics serve a protective role.  similar to how each person's antibodies can distinguish one's own cells from invading organisms  these statistical-profile programs can  as they gather data during the normal operation of a computer  learn to distinguish  self  behavior from  foreign  behavior.  for instance  some people use notepad to view small text files  while others prefer wordpad.  should someone leave their computer unattended and someone else try to inappropriately access their files  the individual differences between people's computer usage will mean that our statistical-modeling program will quickly recognize this illegal access. 
we evaluate the ability to detect computer misuse by collecting data from multiple employees of shavlik technologies  creating user profiles by analyzing  training  subsets of this data  and then experimentally judging the accuracy of our approach by predicting whether or not data in  testing sets  is from the normal user of a given computer or from an intruder.  the key hypothesis investigated is whether or not creating statistical models of user behavior can be used to accurately detect computer misuse.  we focus our algorithmic development on methods that produce very low false-alarm rates  since a major reason system administrators ignore ids systems is that they produce too many false alarms.  
our empirical results suggest that it is possible to detect about 1% of the intrusions with less than one false alarm per  1 hr  day per user.  it should be noted  though  that these results are based on our model of an  insider intruder   which assumes that when insider y uses user x's computer  that y is not able to alter his or her behavior to explicitly mimic x's normal behavior.  the training phase our approach can be computationally intensive due to some parameter tuning  but this parameter tuning could be done on a central server or during the evenings when users are not at work.  the cpu load of our ids is negligible during ordinary operation; it requires less than 1% of the cpu cycles of a standard personal computer.  our approach is also robust to the fact that users' normal behavior constantly changes over time. 
our approach can also be used to detect abnormal behavior in computers operating as specialized http  ftp  or email servers.  similarly  these techniques could be used to monitor  say  the behavior of autonomous intelligent software agents in order to detect rogue agents whose behavior is not consistent with the normal range of agent behavior for a given family of tasks.   however  the experiments reported herein only involve computers used by humans doing the normal everyday business tasks.    while we employ the word  user  throughout this article  the reader should keep in mind that our approach applies equally well to the monitoring of servers and autonomous intelligent agents.  all that would be needed to apply our approach to a different scenario would be to define a set of potentially distinctive properties to measure and to write code that measured these properties periodically. 
previous empirical studies have investigated the value of creating intrusion-detection systems by monitoring properties of computer systems  an idea that goes back at least 1 years .  however  prior work has focused on unix systems  whereas over 1% of the world's computers run some variant of microsoft windows.  in addition  prior studies have not looked at as large a collection of system measurements as we use.   for example  warrender et al.   ghosh et al.   and lane and brodley  only look at unix system calls  whereas lee et al.  only look at audit data  mainly from the tcp program.  lazarevic et al.  provide a summary of some of the recent research on the application of data mining to network-based anomaly detection. 
in section 1 we describe the algorithm we developed that analyzes the windows 1 properties that we measure each second  creating a profile of normal usage for each user.  section 1 presents and discusses empirical studies that evaluate the strengths and weaknesses of this algorithm  stressing it along various dimensions such as the amount of data used for training.  this section also lists which windows 1 properties end up with the highest weights in our weighted-voting scheme.  section 1 describes possible future follow-up research tasks  and section 1 concludes this article. 
1. algorithm developed  
in this section we describe the algorithm that we developed.  our key finding is that a machine-learning algorithm called winnow   a weighted-majority type of algorithm  works very well as the core component of an ids. 
this algorithm operates by taking weighted votes from a pool of individual prediction methods  continually adjusting these weights in order to improve accuracy.  in our case  the individual predictors are the windows 1 properties that we measure  where we look at the probability of obtaining the current value and comparing it to a threshold.  that is  each individual measurement suggests that an intrusion may be occurring if: 
 
    prob  measured property has value v         p                    eq. 1  
each property we measure votes as to whether or not an intrusion is currently occurring.  when the weighted sum of votes leads to the wrong prediction  intrusion vs. no intrusion   then the weights of all those properties that voted incorrectly are halved.  exponentially quickly  those properties that are not informative end up with very small weights.  besides leading to a highly accurate ids  the winnow algorithm allows us to see which windows 1 properties are the most useful for intrusion detection  namely those properties with the highest weights following training  as we shall see  when viewed across several users  a surprisingly high number of properties end up with high weights . 
actually  rather than using equation 1  we found  it slightly better to compare probabilities relative to those in general  i.e.  computed over all our experimental subjects  and use: 
    prob value | user x   /  p value | general public      r       eq. 1  
an alarm is sounded if this ratio is less than some constant  r.  this way we look for relatively rare events for a specific user rather than rare events in general  it may well make sense to use both equations. 1 and 1  and in the one experiment  where we did so - using w = 1sec in table 1's algorithm - we increased the detection rate from 1% to 1% while still meeting our target of less than one false-alarm per day . 
the idea behind using the above ratio is that it focuses on feature values that are rare for this user relative to their probability of occurrence in the general population.  for example  feature values that are rare for user x but also occur rarely across the general population may not produce low ratios  while feature values that are rare for user x but are not rare in general will.  that is  this ratio distinguishes between  rare for user x and for other computer users as well  and  rare for user x but not rare in general.  
we estimate prob feature=value for the general population  by simply pooling all the training data from our experimental subjects  and then creating a discrete probability distribution using ten bins  using the technique explained below.  doing this in a fielded system would be reasonable  since in our ids design one requires a pool of users for the training and tuning phases. 
because our experimental setup only involves measurements from normal computer users  the use of our ratio of probabilities makes sense in our experiments  since it defines  rare for user x  relative to the baseline of other computer users operating normally.  however  it is likely that the behavior of intruders  even insiders working at the same facility  may be quite different from normal computer usage  unfortunately we do not yet have such data to analyze .  for example  an intruder might do something that is rare in general  and hence equation 1 above might not produce a value less than the setting for the threshold r.   
before presenting our algorithm that calls as a subroutine the winnow algorithm  we discuss how we make  features  out of the over two-hundred windows properties that we measure.  
technically  it is these 1 or so features that do the weighted voting. 
1 features used  
space limitations preclude describing here all of the 1+ properties measured.  appendix a of our full project report  lists and briefly describes all of the windows 1 properties that we measure; some relate to network activity  some to file accesses  and others to the cpu load of the programs currently running.  most come from windows' perfmon   performance monitor   program.  several features we measure appear in tables 1 and 1 of this article.  for each of these measurements  we also derive additional measurements: 
   actual value measured 
   average of the previous  1 values 
   average of the previous 1 values 
   difference between current value and previous value 
   difference between current value and average of last 1 
   difference between current value and ave of last 1 
   difference between averages of previous 1 and previous 1 
as we discovered in our experiments  these additional  derived  features play an important role; without them intrusion-detection rates are significantly lower.  for the remainder of this article  we will use the term  feature  to refer the combination of a measured 
windows 1 property and one of the seven above transformations.  in other words  each windows 1 property that we measure produces seven features.   the first item in the above list is not actually a derived feature; it is the  raw  measurement  but we include it in the above list for 
completeness.   
1 our ids algorithm 
table 1 contains our main algorithm.  we take a machine-learning  approach to creating an ids  and as is typical we divide the learning process into three phases.  first  we use some training data to create a model; here is where we make use of the winnow algorithm  see table 1   which we further explain below.  next  we use some more data  the tuning set  to  tune  some additional parameters in our ids.  finally  we evaluate how well our learned ids works be measuring its performance on some testing data.  
we repeat this process for multiple users and report the average test-set performance in our experiments. 
the windows 1 properties that we measure are continuousvalued  and in step 1b of table 1 we first decide how to discretize each measurement into 1 bins; we then use these bins to create a discrete probability distribution for the values for this feature.   importantly  we do this discretization separately for each user  since this way we can accurately approximate each user's probability distribution with our 1 bins.   we did not experiment with values other than 1 for the number of bins.  we chose 1 arbitrarily  though it does make sense that this number be small to reduce storage demands and to  smooth  our measurements.  
we always place the value 1 in its own bin  since it occurs so frequently.  we then choose the  cut points  that define the remaining bins by fitting a sample of the measurements produced by each user to each of several standard probability distributions:  uniform  gaussian  and erlang  for k ranging from 1 to 1 .   when k = 1 the erlang is equivalent to the better known   decaying   exponential distribution  and as k increases the distribution looks more and more like a gaussian.  we then select the probability distribution that best fits the sample data  i.e.  has the lowest root-mean-squared error   and create our 1 bins as follows: 
  for the uniform probability distribution  we uniformly divide the interval  minimum value  maximum value  into seven bins  and use the two remaining bins for values less than the minimum and greater than the maximum  since values encountered in the future might exceed those we have seen so far . 
  for the gaussian probability distribution  we place the lowest 1% of the probability mass in the first bin  the next 1% in the second bin  1% in the next bin  and 1% in the bin after that.  we do the same working down from the highest value  which leaves about 1% for the middle bin  1% is roughly one standard deviation around the mean of a gaussian . 
  for the exponential probability distribution  we put half the probability mass in the first bin  and then half of the remaining probability mass in each successive bin  except for the last bin .  
  for the erlang probability distribution  we execute a combination of what we do for the gaussian and the exponential  depending on the value of k. 
we did not investigate alternate design choices in our discretization process; we developed the above approach and then used it unchanged during our subsequent learning-algorithm development and evaluation. 
                                                     table 1.   creating and maintaining an ids for user x 
step 1: initial training 
step 1a:  collect measurements from user x and place in trainset. 
step 1b:  using trainset  choose good  cut points   for user x  to discretize continuous values.  see text. step 1c:  select weights for user x's measured features by applying the winnow algorithm  see table 1 and accompanying text  using trainset and an equal number of  archived  sample measurements from other users. however  be sure to discretize the measurements from the other users by applying user x's cut points  since we will be pretending that the other users are inappropriately using x's computer. 
step 1: parameter tuning 
step 1a:  using new measurements collected from user x and other users  the tuneset   perform steps 1b and 1c  calculating false-alarm and intrusion-detection rates in conceptually independent runs for as many as possible combinations of the parameters being tuned: w  threshmini and threshfull. step 1b:  use the weighted features to  vote  on  minialarms  each second;  
if  wgtedvotesfor / wgtedvotesagainst      threshmini  then raise a mini-alarm.  see steps 1a and 1b of  table 1. 
step 1c:  if the fraction of mini-alarms in the last w seconds ¡Ý threshfull then raise an alarm signaling that an intrusion might be occurring. after each  call   wait another w seconds  i.e  the windows do no overlap . step 1d:  given the specified maximum false-alarm rate per  1-hour  day  choose the parameter settings that produce the highest intrusion-detection rate on the set of sample  other  users  while not producing more than the desired number of false alarms for user x. 
step 1: continual operation 
using step 1d's chosen settings for w  threshmini and threshfull  repeat steps 1b and 1c on the testset.  it might make sense to periodically retrain and retune in order to adjust to changing user behavior - see text.  
                                                     most of our features turned out to be best modeled by gaussians  with the exponential distribution being the second most common selection.  one final point about converting to a discrete probability distribution needs to be mentioned: for those windows 1 measurements that vary over orders of magnitude  e. g.  bytes sent per second ; we use the log of their values. 
after we have discretized our features  we simply count how often in the training data did a feature value fall into a given bin  thereby producing a probability distribution  after normalizing by the total number of counts .  following standard practice  we initialize all bins with a count of 1; this ensures that we will never estimate from our finite samples a probability of zero for any bin. we are now able to estimate the prob feature = measured value  that was mentioned earlier in equations 1 and 1. 
                                                     table 1.   variant of winnow that is used 
step 1:    initialize user x's weights on each measured                feature  wgtf   to 1. 
step 1:    for each training example do: 
step 1a:  zero wgtedvotesfor and wgtedvotesagainst. step 1b:  if then relative probability  eq. 1  of the current measured value for feature f   r  then add wgtf  to wgtedvotesfor  otherwise add wgtf  to wgtedvotesagainst. 
i.e.  if the relative probability of the current value of feature f is  low   then this is evidence that something anomalous is occurring. in our experiments  we found that r = 1 worked well; however  overall performance was robust in regards to the value of r  and eq 1's p .  various values for r that we tried in the range  1  1  all worked well. 
step 1c:  if   wgtedvotesfor   wgtedvotesagainst  then call the current measurements anomalous. step 1d:  if user x produced the current measurements and they are considered anomalous  then a false-alarm error has been made. multiply by 1 all those features that incorrectly voted for raising an alarm. 
otherwise if some other user produced the current measurements and they were not considered anomalous  then an intrusion has been missed.  multiply by 1 all those features that incorrectly voted against raising an alarm.  when neither a false-alarm nor a missedintrusion error occurred  leave the current weights unchanged. 
                                                     
we next turn to discuss using these probabilities to learn models for distinguishing the normal user of a given computer from an intruder.  ideally we would use training data where some user x provided the examples of normal  i. e.  non-intrusion  data and we had another sample of data measured during a wide range of intrusions on this user's computer.  however  we do not have such data  this is a problem that plagues ids research in general   and so we use what is a standard approach  namely we collect data from several users  in our case  1   and we then simulate intrusions by replaying user y's measurements on user x's computer.  we say that a false alarm occurs when user y's recent measurements are viewed as anomalous - that is  suggestive of an intrusion - when replayed on his or her own computer.  a detected intrusion occurs when we view user y's measurements as being anomalous when evaluated using x's feature discretization and feature weighting.   notice that we need to use x's discretization  rather than y's  since we are assuming that y is operating on x's computer.   figure 1 abstractly illustrates how we define false alarms and detected intrusions in our experimental setting. 

  
figure 1.  false alarms and detected intrusions 
as mentioned  we use table 1's version of littlestone's winnow algorithm  to choose weights on the features we measure.  this algorithm is quite simple  yet has impressive theoretical properties and practical success on real-world tasks  especially those that have a very large number of features  which is the case for our task.  as already discussed  this algorithm sums weighted votes  for  and  against  the possibility that an intrusion is currently occurring.  when the winning choice  i. e.   for  or  against   is wrong  then all those features that voted for the wrong choice have their weights halved.  we perform the winnow algorithm for each user  in each case using a 1 mixture of examples  with half drawn from this user's measured behavior  the  against an intrusion  examples  and half randomly drawn from some other user in the experiment  the  for an intrusion  examples . 
in order to raise an alarm after the training phase  step 1 in table 1  has set the feature weights  our algorithm does not simply use the current weighted vote.  instead  the current weighted vote can raise what we call a mini alarm  and we require that there be at least n mini alarms in the last w seconds in order to raise an actual alarm.  in other words  our intrusion detector works as follows  steps 1b and 1c in table 1 : 
if weighted vote current measurements      threshmini  then raise  mini  alarm 
if fraction of  mini  alarms in last w sec  ¡Ý threshfull   then predict intrusion 
as will be seen in section 1  w needs to be on the order of 1 to get good detection rates with few false alarms. 
we choose the settings for our parameters on a per-user basis by evaluating performance on a set of tuning data - see step 1 of table 1.  one significant advantage of a data-driven approach like ours is that we do not have to pre-select parameter values.  
instead  the learning algorithm selects for each user his or her personal set of parameter values  based on the performance of these parameters on a substantial sample of  tuning set  data. 
the only computationally demanding portion of our algorithm is the parameter-tuning phase  which depends on how many parameter combinations are considered and on how much tuning data each combination is evaluated.  in a fielded system  it might make sense to do this step on a central server or during the evenings.  the other tasks of measuring features  computing weighted sums  and using winnow to adjust weights can all be done very rapidly.  outside of the parameter tuning  table 1's algorithm requires less than 1% of a desktop computer's cpu cycles. 
notice that even during the testing phase  e. g.  step 1 in table 1   we find it necessary to still execute the winnow algorithm  to adjust the weights on the features after our algorithm decides whether or not an intrusion occurred.  if we do not do this  we get too many false alarms when the user's behavior switches  and the intrusion-detection rates drastically drops to 1% from about 1%.  on the other hand continually adjusting weights means that if we miss an intrusion we will start learning the behavior of the intruder  which is a weakness of our approach  and a weakness of statistics-based approaches for intrusion detection in general .  this also means that the empirical results reported in the next section should properly be interpreted as estimating the probability that we will detect an intruder after his or her first w seconds of activity.  a subject for future work is to empirically evaluate how likely our approach will detect an intruder in the second  and successive  w seconds of activity  given we did not detect the intruder in the first w seconds.  on the other hand  the fact that we continually are adjusting the weights means that after the legitimate user reauthenticates himself or herself after a false alarm  our algorithm will adapt to the change in the user's behavior. 
obviously there is a delicate balance between adapting quickly to changes in the legitimate user's behavior  and thus reducing false alarms  and adapting too quickly to the activity of an intruder and thus thinking the intruder's behavior is simply a change in the behavior of the normal user of the given computer and thereby missing actual intrusions.  it is a simple fact of life that most users' behavior is wide ranging and changing over time.  the more consistent a user's behavior is  and the more accurately we can capture his or her idiosyncrasies  the better our approach will work. 
1. experimental evaluation 
this section reports some experimental evaluation of our ids algorithm.  additional experiments are reported in detail in shavlik and shavlik   with some of their results mentioned in this article. 
1 methodology 
we collected about 1 gb of data from 1 employees of shavlik technologies who volunteered to be experimental subjects.  we only collected data between 1am and 1pm on weekdays. 
of these 1 experimental subjects  we use 1 during training  steps 1 and 1 of table 1 ; for each one  we train our ids to recognize the differences in behavior of that user from the other 1 users.  we call these 1 users  insiders  and view them as members of a small group of co-workers.  the remaining 1 subjects  for whom we have a total of about 1 work days of measurements  serve as simulated  external  intruders  i.e.  users whose computer-usage behavior is not seen during training  including computing the denominator in eq. 1  - these 1 experimental subjects are only used during the testing phase  step 1 of table 1  and are never used during the training and tuning phases.  hence  one expects that these 1  outsiders  would be harder to recognize as intruders on user x's computer since their behavior is not observed while the ids's are still learning. 
1 primary results and discussion 
figure 1 shows  as a function of w  see table 1  the detection and false-alarm rates for the scenario where the training lasts 1 work days  1 seconds   and the tuning  and testing periods each last 1 work days  1 seconds .  the train  tune  and test sets are temporally disjoint from one another.  this scenario involves a five-week-long training process  but as presented in shavlik and shavlik  shorter training periods produce results nearly as good. 
the results are averages over the 1  insiders;  that is  each of these 1 experimental subjects is evaluated using the other 1 subjects as  insider intruders  and the above-described 1  outsider intruders   and the 1 resulting sets of false-alarm and detection rates are averaged to produce figure 1.  during the tuning phase of table 1  the specified false-alarm rate of step 1e was set to 1; such a extreme false-alarm rate could always be produced on the tuning set  though due to the fact we are able to explicitly fit our parameters only to the tuning data  a false-alarm rate of zero did not result during the testing rate  as one expects .  over fitting  getting much higher accuracies on the tuning data than on the testing data due to having too many  degrees of freedom  during the tuning phase  is arguably the key issue in machine learning and is central to adaptive ids's. 
as can be seen in figure 1  for a wide range of window widths  from 1 to 1 minutes   the false-alarm rates are very low - always less than one per eight-hour work day per user - and the intrusion-detection rates are impressively high  nearly 1%.  interestingly  the detection rate for  outsiders   whose behavior is never seen during training  is approximately the same as for  insiders.   this suggests that our learning algorithm is doing a good job of learning what is characteristic about user x  rather than just exploiting idiosyncratic differences between user x and the other nine  insiders.  
based on figure 1  1 seconds is a reasonable setting for w in a fielded system  and in most of the subsequent experiments in this section use that value. 
 it should be noted that going down to w = 1 sec in figure 1 is not completely appropriate.  some of the features we use are averages of a given measurement over the last 1 seconds  as explained earlier in this article.  in all of our experiments  we do not use any examples where the user's computer has not been turned on for at least 1 seconds.  hence  when we replay a 1second window of activity from user y on user x's computer  there is some  leakage  of user y's data going back 1 seconds.  in a fielded system  1 seconds worth of the data would actually be from user x and 1 seconds from user y.  however  our experimental setup does not currently support such  mixing  of user behavior.  should a fielded system wish to use w=1 sec  a simple solution would be to average over the last 1 seconds  rather than the last 1 seconds as done in our experiments.  we do not expect the impact of such a change to be significant.  the data point for w = 1 sec in figure 1 only uses features that involve no more than the last 1 seconds of measurements  as a reference point - the issue of using less or more than the last 1 seconds of measurements is visited in more depth in the next section.  
                                                    

figure 1.  false alarm and detection rates
                                                    
one potentially confusing technical point needs to be clarified at this point.  in an eight-hour work day  there are 1 sixty-secondwide  non-overlapping windows  i. e.  w = 1  but only 1 sixhundred-second-wide  w = 1  ones.  so one false alarm per day for w = 1 sec corresponds to a false-alarm rate of 1%  whereas for w = 1 sec a false-alarm rate of 1% produces one falsealarm per day on average.  the  lower  dotted line in figure 1 shows the false-alarm rate that produces one false alarm per day per user.  although it cannot be seen in figure 1  as w increases the actual number of false alarms per day decreases.  making a call every second leads to too many false alarms   so we use non-overlapping windows.  conversely  as w increases an intruder is able to use someone else's computer longer before being detected. 
to produce figure 1's results  table 1's tuning step considered 1 possible settings for thresholdmini  1  1  1  1  1  
1  1  1  1  1  and 1  and 1 for thresholdfull   1   
1  1  1  1  1  1  1  1  1  1  1  1  1  
1  1  1  1  1  1  1  1  1  1  1  and 1   that is 1=1 different combinations of these two parameters.  we did not experiment with different choices for the particular values and number of the candidate parameter settings  except we found it necessary to restrict thresholdmini = 1 in the cases in figure 1 where w = 1 sec and w = 1 sec.  
table 1 shows the highest-weighted features at the end of figure 1's experiment  where the weights are averaged over all ten of our experimental subjects and over those values for w   1 used to create figure 1; for each experimental subject and setting for w  we normalize the weights so that they sum to 1  thus insuring that each configuration contributes equally.  remember that the weights are always changing  so this table should be viewed as a representation  snapshot.    appendix a of shavlik and shavlik  contains additional explanations of several of these features . 
                                                     
table 1.  features with the 1 highest weights averaged across the experiments that produced figure 1. 
   print jobs  average of previous 1 values  ranked #1     print jobs  average of previous 1 values 
   system driver total bytes  actual value measured 
   logon total  actual value measured 
   print jobs  actual value measured 
   lsass: working set  average of previous 1 values 
   number of semaphores  average of previous 1 values 
   calc: elapsed time  
difference between averages of prev 1 and prev 1 
   number of semaphores  actual value measured 
   lsass: working set  average of previous 1 values 
   cmd: handle count  
difference between current and average of last 1 
   cmd: handle count  average of previous 1 values 
   write bytes cache/sec  
difference between current and average of last 1 
   excel: working set  
difference between current and average of last 1 
   number of semaphores  average of previous 1 values 
   cmd: % processor time  
difference between averages of prev 1 and prev 1 
   lsass: working set  actual value measured 
   system driver total bytes  average of previous 1 values 
   cmd: % processor time  
difference between current and average of last 1 
   cmd: % processor time  
difference between current and average of last 1 
   system driver resident bytes  actual value measured 
   excel: handle count  average of previous 1 values 
   errors access permissions  
difference between current and average of last 1 
   file write operations/sec  average of previous 1 values    system driver resident bytes  average of previous 1 values 
                                                                                                          
table 1.  the 1 measurements with the highest number of occurrences in the top 1 weights  including ties  in the experiments that produced figure 1  the numbers in parentheses are the percentages of top 1 appearances  
number of semaphores  1%  
logon total  1%  
print jobs  1%  
	system driver total bytes  1%  	 
cmd: handle count  1%  
system driver resident bytes  1%  
excel: handle count  1%  
number of mutexes  1%  
errors access permissions  1%  
files opened total  1%  
tcp connections passive  1%  
lsass: working set  1%  
lsass: % processor time  1%  
system: working set  1%  
notepad: % processor time  1%  
cmd: working set  1%  
packets/sec  1%  
datagrams received address errors  1%  
excel: working set  1%  
msdev: working set  1%  
udp datagrams no port / sec  1%  
winword: working set  1%  
file write operations / sec  1%  
bytes received / sec  1%  
bytes transmitted / sec  1%  
                                                     
observe that a wide range of features appear in table 1: some relate to network traffic  some measure file accesses  others refer to which programs are being used  while others relate to the overall load on the computer.  it is also interesting to notice that for some features their average values over 1 seconds are important  whereas for others their instantaneous values matter  and for still others what is important is the change in the feature's value. 
a weakness of table 1 is that a measured windows 1 property that is important for only one or two subjects might not have a very high average weight.  table 1 provides a different way to see which features play important roles.  to produce this table we count how often each measured property appears in the top 1 weights  including ties  which are common  following training.  surprisingly  over half of the windows 1 properties we measure appear at least once in some top 1 list!  this supports our thesis that one should monitor a large number of system properties in order to best create a behavioral model that is well tailored to each individual computer user.  our project's final report  displays longer and additional lists of the highestweighted features  including those for one specific user. 
most of the  derived  calculations  see section 1  are used regularly in the highly weighted features  with the exception of  difference from previous value   which appears in the top 1 weighted features only about 1th as often as the others.  presumably because it is too noisy of an estimate and needs to be smoothed.   difference between current and average of last 1  is the most used  but the difference between the most used and the 1th-most used is only a factor of two. 
1 additional results 
tables 1 and 1 show that the features that use the last n 
measurements of a windows 1 property play an important role.  figure 1 illustrates the performance of table 1's algorithm when we restrict features to use at most the last 1  1  1  or 1 measurements  respectively  of the window 1 properties that we monitor.  the y-axis is the test-set detection rate and in all cases the false-alarm rate meets our goal of no more than one per user per workday.   figure 1's data is from the case where w = 1 seconds; 1 days of training data  1 of tuning  and 1 of testing are used for each experimental subject. 
figure 1 shows that there is an advantage in considering features that have longer  histories.    however  the cost of a longer history is that more data needs to be collected to define a feature value.  that is  if histories can go back as far as 1 seconds  a little over 1 minutes   then it will take 1 seconds after an intrusion until all of the feature values are due solely to the intruder's behavior.  it appears that limiting features to at most the last 1 seconds of measurements is a good choice. 
so far we have reported results average over our pool of 1 insiders and 1 outsiders.  it is interesting to look at results from individual experimental subjects.  table 1 reports how often user y was not detected when  intruding  on user x's computer.  for example  the cell  row=user1  column=user1  says that the probability of detection is 1 when user 1 operates on user 1's computer for 1 seconds.   the rightmost column is the detection rate when outsiders operate on each insider's computer.   
given that the overall detection rate is about 1%  i.e.  only 1% of 1-sec intrusions do not sound alarms   one might expect that most of the individual penetration rates would range from  say  1% to 1%.  however  the results are much more skewed.  in most cases  all  or nearly all  the attempted intrusions are detected - the majority of cells in table 1 contain 1's  in fact we report  penetration  rates rather than detection rates in this table because otherwise all of the 1%'s would be visually overwhelming .  but in several cases a user is frequently not detected when operating on another user's computer. 
one implication of the results in table 1 is that for a fielded system one could run experiments like these on some group of users  and then identify for which ones their computer behavior is sufficiently distinctive that table 1's algorithm provides them effective protection. 
                                                     

	1	1
number of previous values used
  
figure 1.   detection rate as function of number of           previous values used  w = 1 sec  

table 1.  percentage  %  of times that user y successfully intruded on user x's machine  using w = 1 sec . the columns range over y and the rows over x.  the rightmost column  o  reports the rate of successful intrusions by the set of six outsiders.  cells with values less than 1% are left blank. 
 y x  1 1 1 1 1 1 1 1 1 1 o 1     1  1    1  1 1   1    1  1       1  1     1       1  1 1           1   1    1         1  1           1  1 1         1 1  1 1            1 1        1    1 1   1 1 1 1  1 1  1   1 1 comparison to na ve bayes 
a successful algorithm on many tasks is the na ve bayes algorithm   in which one assumes all the features  i.e.  measurements in our case  are conditionally independent of each other given the category  and estimates the probability of obtaining the current set of measurements given each of the possible categories  intrusion versus normal behavior in our case . 
we applied the na ve bayes algorithm in the same experimental setup as used to evaluate table 1's algorithm.  however  the best results we have been able to obtain  for w = 1 seconds  are a 1% detection rate with an average of 1 false alarms per day per user  which compares poorly to table 1's algorithm's results  in the identical scenario  of a 1% detection with an average of 
1 false alarms per day per user.   in fact  we started out this project using the na ve bayes algorithm  and then switched to our winnow-based approach when we realized that na ve bayes' independence assumption was too severely violated for us to create an effective anomaly detector.  
1. future work 
we discuss a few possible extensions to the work reported above that have not yet been fully discussed.  an obvious extension is to obtain and analyze data from a larger number of users  as well as data from a collection of server machines.  and of course it would be greatly beneficial to have data gathered during actual intrusions  rather than simulating them by replaying one user's measurements on another user's computer.  among other advantages  having data from a larger pool of experimental subjects would allow  scaling up  issues to be addressed  statistically justified confidence intervals on results to be produced  and parameters to be better tuned  including many for which we have  hard-wired in  values in our current experiments . 
when we apply the winnow algorithm during the training phase  step 1 in table 1   we get remarkable accuracies.  for example  out of 1 1 seconds of examples  half that should be called an intrusion and half that should not   we consistently obtain numbers on the order of only 1 missed intrusions and 1 false alarms  and that is from starting with all features weighted equally.  clearly the winnow algorithm can quickly pick out what is characteristic about each user and can quickly adjust to changes in the user's behavior.  in fact  this rapid adaptation is also somewhat of a curse  as previously discussed in section 1   since an intruder who is not immediately detected may soon be seen as the normal user of a given computer.  this is why we look for n mini-alarms in the last w seconds before either sounding an alarm or calling the recent measurements normal and then applying winnow to these measurements; our assumption is that when the normal user changes behavior  only a few mini-alarms will occur  whereas for intruders the number of mini-alarms produced will exceed n.  nevertheless  we still feel that we are not close to fully exploiting the power of the winnow algorithm on the intrusiondetection task.  with more tinkering and algorithmic variations  it seems possible to get closer to 1% detection rates with very few false alarms. 
in section 1's winnow-based algorithm we estimate the probability of the current value for a feature and then make a simple  yes-no  call  see eq. 1 and 1   regardless of how close the estimated probability is to the threshold.  however  it seems that an extremely low probability should have more impact than a value just below the threshold.  in the often-successful na ve bayes algorithm  for example  actual probabilities appear in the calculations  and it seems worthwhile to consider ways of somehow combining the weights of winnow and the actual 
 rather than thresholded  probabilities. 
in our main algorithm  table 1  we did not  condition  the probabilities of any of the features we measured.   doing so might lead to more informative probabilities and  hence  better performance.  for example  instead of simply considering  prob file write operations/sec   it might be more valuable to use prob file write operations/sec | ms word is using most of the recent cycles   where '|' is read  given.   similarly  one could use the winnow algorithm to select good pairs of features.  however these alternatives might be too computationally expensive unless domain expertise was somehow used to choose only a small subset of all the possible combinations. 
in none of the experiments of this article did we mix the behavior of the normal user of a computer and an intruder  though that is likely to be the case in practice.  it is not trivial to combine two sets of windows 1 measurements in a semantically meaningful way  e. g.  one cannot simply add the two values for each feature or  for example  cpu utilizations of 1% might result .  however  with some thought it seems possible to devise a plausible way to mix normal and intruder behavior.  an alternate approach would be to run our data-gathering software while someone is trying to intrude on a computer that is simultaneously being used by another person. 
in the results reported in section 1  we tune parameters to get zero false alarms on the tuning data  and we found that on the testing data we were able to meet our goal of less than one false alarm per user per day  often we obtained test-set results more like one per week .  if one wanted to obtain even fewer false alarms  then some new techniques would be needed  since our approach already is getting no false alarms on the tuning set.  one solution we have explored is to tune the parameters to zero false alarms  and then to increase the stringency of our parameters - e. g.  require 1% of the number of mini-alarms as needed to get zero tuning-set false alarms.  more evaluation of this and similar approaches is needed.  
we have also collected windows 1 event-log data from our set of 1 shavlik technologies employees.  however we decided not to use that data in our experiments since it seems one would need to be using data from people actually trying to intrude on someone else's computer for interesting event-log data to be generated.  our approach for simulating  intruders  does not result in then generation of meaningful event-log entries like failed logins. 
another type of measurement that seems promising to monitor are the specific ip addresses involved in traffic to and from a given computer.  possibly interesting variables to compute include the number of different ip addresses visited in the last n seconds   the number of  first time visited  ip addresses in the last n seconds  and differences between incoming and outgoing ip addresses. 
a final possible future research topic is to extend the approaches in this article to local networks of computers  where the statistics of behavior across the set of computers is monitored.  some intrusion attempts that might not seem anomalous on any one computer may appear highly anomalous when looking at the behavior of a set of machines. 
1. conclusion 
our approach to creating an effective intrusion-detection system  ids  is to continually gather and analyze hundreds of finegrained measurements about windows 1.  the hypothesis that we successfully tested is that a properly  and automatically  chosen set of measurements can provide a  fingerprint  that is unique to each user  serving to accurately recognize abnormal usage of a given computer.  we also provide some insights into which system measurements play the most valuable roles in creating statistical profiles of users  tables 1 and 1 .  our experiments indicate that we may get high intrusion-detection rates and low false-alarm rates  without  stealing  too many cpu cycles.  we believe it is of particular importance to have very low false-alarm rates; otherwise the warnings from ids will soon be disregarded. 
specific key lessons learned are that it is valuable to: 
  consider a large number of different properties to measure  since many different features play an important role in capturing the idiosyncratic behavior of at least some user  see table 1 and 1  
  continually reweight the importance of each feature measured  since users' behavior changes   which can be efficiently accomplished by the winnow algorithm  
  look at features that involve more than just the instantaneous measurements  e. g.  difference between the current measurement and the average over the last 1 seconds  
  tune parameters on a per-user basis  e. g.  the number of  mini alarms  in the last n seconds that are needed to trigger an actual alarm  
  tune parameters on  tuning  datasets and then estimate  future  performance by measuring detection and falsealarm rates on a separate  testing  set  if one only looks at performance on the data used to train and tune the learner  one will get unrealistically high estimates of future performance; for example  we are always able to tune to zero false alarms  
  look at the variance in the detection rates across users; for some  there are no or very few missed intrusions  while for others many more intrusions are missed - this suggests that for at least some users  or servers  our approach can be particularly highly effective 
an anomaly-based ids  such as the one we present  should not be expected to play the sole intrusion-detection role  but such systems nicely complement ids that look for known patterns of abuse.  new misuse strategies will always be arising  and anomaly-based approaches provide an excellent opportunity to detect them even before the internal details of the latest intrusion strategy are fully understood. 
1. acknowledgments 
we wish to thank the employees of shavlik technologies who volunteered to have data gathered on their personal computers.  we also wish to thank michael skroch for encouraging us to undertake this project and michael fahland for programming support for the data-collection process.  finally we also wish to thank the anonymous reviewers for their insightful comments.  
 
this research was supported by darpa's insider threat active profiling  itap  program within the atias program. 
