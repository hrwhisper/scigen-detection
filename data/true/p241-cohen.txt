a bloom filter is a space-efficient randomized data structure allowing membership queries over sets with certain allowable errors. it is widely used in many applications which take advantage of its ability to compactly represent a set  and filter out effectively any element that does not belong to the set  with small error probability. this paper introduces the spectral bloom filter  sbf   an extension of the original bloom filter to multi-sets  allowing the filtering of elements whose multiplicities are below a threshold given at query time. using memory only slightly larger than that of the original bloom filter  the sbf supports queries on the multiplicities of individual keys with a guaranteed  small error probability. the sbf also supports insertions and deletions over the data set. we present novel methods for reducing the probability and magnitude of errors. we also present an efficient data structure and algorithms to build it incrementally and maintain it over streaming data  as well as over materialized data with arbitrary insertions and deletions. the sbf does not assume any a priori filtering threshold and effectively and efficiently maintains information over the entire data-set  allowing for ad-hoc queries with arbitrary parameters and enabling a range of new applications.
1. introduction
　bloom filters are space efficient data structures which allow for membership queries over a given set . the bloom filter uses k hash functions  h1 h1 ... hk to hash elements into an array of size m. for each element s  the bits at positions h1 s  h1 s  ... hk s  in the array are set to 1. given an item q  we check its membership in the data-set by examining the bits at positions h1 q  h1 q  ... hk q . the item q is reported to be contained in the data-set if  and only if  all the bits are set to 1. this method allows a small probability of producing a false positive error  it may return a positive result for an item which actually is

 research supported in part by the israel science foundation.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
not contained in the set   but no false-negative error  while gaining substantial space savings. bloom filters are widely used in many applications.
　this paper introduces the spectral bloom filter  sbf   an extension of the original bloom filter to multi-sets  allowing estimates of the multiplicities of individual keys with a small error probability. this expansion of the bloom filter is spectral in the sense that it allows filtering of elements whose multiplicities are within a requested spectrum. the sbf extends the functionality of the bloom filter and thus makes it usable in a variety of new applications  while requiring only a slight increase in memory compared to the original bloom filter. we present efficient algorithms to build an sbf  and maintain it for streaming data  as well as arbitrary insertions and deletions. the sbf can be considered as a high-granularity histogram. it is considerably larger than regular histograms  but unlike such histograms it supports queries at high granularity  and in fact at the single item level  and it is substantially smaller than the original data set.
1 previous work
　as the size of data sets encountered in databases  in communication  and in other applications keeps on growing  it becomes increasingly important to handle massive data sets using compact data structures. indeed  there is extensive research in recent years on data synopses  and data streams .
　the applicability of bloom filters as an effective  compact data representation is well recognized. bloom filters are often used in distributed environments to store an inventory of items stored at every node. in   it is proposed to be used within a hierarchy of proxy servers to maintain a summary of the data stored in the cache of each proxy. this allows for a scalable caching scheme utilizing several servers. the summary cache algorithm proposed in the same paper was implemented in the squid web proxy cache software  1  1   with a variation of this algorithm called cache digest implemented in a later version of squid.
　in peer-to-peer systems  an efficient algorithm is needed to establish the nearest node holding a copy of a requested file  and the route to reach it. in   a structure called  attenuated bloom filter  is described. this structure is basically an array of simple bloom filters in which component filters are labeled with their level in the array. each filter summarizes the items that can be reached by performing a number of hops from the originating node that is equal to the level of that filter. the paper proposes an algorithm for efficient location of information using this structure.
　the use of bloom filters was proposed in handling joins  especially in distributed environments. bloomjoin is a scheme for performing distributed joins   in which a join between relations r and s over the attribute x is handled by building a bloom filter over r.x and transmitting it to s. this bloom filter is used to filter tuples in s which will not contribute to the join result  and the remaining tuples are sent back to r for completion of the join. the compactness of the bloom filter saves significant transmission size.
　bloom filters were also proposed in order to improve performance of working with differential files . a differential file stores changes in a database until they are executed as a batch. this reduces overheads caused by sporadic updates and deletions to large tables. when using a differential file  its contents must be taken into account when performing queries over the database  with as little overhead as possible. a bloom filter is used to identify data items which have entries within the differential file  thus saving unnecessary access to the file. another area in which bloom filters can be used is checking validity of proposed passwords  against previous passwords used and a dictionary. recently  broder et al  used bloom filters in conjunction with hot list techniques presented in  to efficiently identify popular search queries in the alta-vista search engine.
　several improvements have been proposed over the original bloom filter. in  the data structure was optimized with respect to its compressed size  rather than its normal size  to allow for efficient transmission of the bloom filter between servers  as proposed in . another improvement proposed in  is imposing a locality restriction on the hash functions  to allow for faster performance when using external storage. in  a counter has been attached to each bit in the array to count the number of items mapped to that location. this provides the means to allow deletions in a set  but still does not support multi-sets. to maintain the compactness of the structure  these counters were limited to 1 bits  which is shown statistically to be enough to encode the number of items mapped to the same location  based on the maximum occupancy in a probabilistic urn model  even for very large sets. however this approach is not adequate when dealing with frequencies of multi-sets  in which items may appear hundreds and thousands of times.
　the concept of multiple hashing  while not precisely in the form of bloom filters  was used in several recent works  such as supporting iceberg queries  and tracking large flows in network traffic . both handle queries which correspond to a very small subset of the data  the tip of the iceberg  defined by a threshold  while having to efficiently explore the entire data. these implementations assume a prior knowledge of the threshold and avoid maintaining a
　synopsis over the full data set. a recent survey describes several applications and extensions of the bloom filter  with emphasis on network applications .
　current implementations of bloom filters do not address the issue of deletions over multi-sets. an insert-only approach is not enough when using widely used data warehouse techniques  such as maintaining a sliding window over the data. in this method  while new data is inserted into the data structure  the oldest data is constantly removed. when tracking streaming data  often we would be interested in the data that arrived in the last hour or day  for example. in this paper we show that the sbf provides this functionality as a built-in ability  under the assumption that the data leaving the sliding window is available for deletion  while allowing  approximate  membership and multiplicity queries for individual items. an earlier version of this work appears in .
1 contributions
　this paper presents the spectral bloom filter  sbf   a synopsis which represents multisets that may change dynamically in a compact and efficient manner. queries regarding the multiplicities of individual items can be answered with high accuracy and confidence  allowing a range of new applications. the main contributions of this paper are:
  the spectral bloom filter synopsis  which provides a compact representation of data sets while supporting queries on the multiplicities of individual items. for a multiset s consisting of n distinct elements from u with multiplicities {fx : x （ s}  an sbf of n + o n  + o n  bits can be built in o n  time  where
n = k x（s dlogfxe. for any given q （ u  the sbf f q  so that f q − fq    occurs with low proba-
bility  exponentially small in k . this allows effective filtering of elements whose multiplicities in the data set are below a threshold given at query time  with a small fraction of false positives  and no false negatives. the sbf can be maintained in o 1  expected amortized time for inserts  updates and deletions  and can be effectively built incrementally for streaming data. we present experiments testing various aspects of the sbf structure.
  we show how the sbf can be used to enable new applications and extend and improve existing applications. performing ad-hoc iceberg queries is an example where one performs a query expected to return only a
small fraction of the data  depending on a threshold given only on query time. another example is spectral bloomjoins  where the sbf reduces the number of communication rounds among remote database sites when performing joins  decreasing complexity and network usage. it can also be used to provide a fast aggregative index over an attribute  which can be used in algorithms such as bifocal sampling.
　the following novel approaches and algorithms are used within the sbf structure:
  we show two algorithms for sbf maintenance and
lookup  which result with substantially improved lookup accuracy. the first  minimal increase  is simple  efficient and has very low error rates. however  it is only suitable for handling inserts. this technique was independently proposed in  for handling streaming data. the second method  recurring minimum  also improves error rates dramatically while supporting the full insert  delete and update capabilities. experiments show favorable accuracy for both algorithms. for a sequence of insertions only  both recurring minimum and minimal increase significantly improve over the basic algorithm  with advantage for minimal increase. for sequences that include deletions  recurring minimum is significantly better than the other algorithms.
  one of the challenges in having a compact representation of the sbf is to allow effective lookup into the i'th string in an array of variable length strings  representing counters in the sbf . we address this challenge by presenting the string-array index data structure which is of independent interest. for a string-array of m strings with an overall length of n bits  a string-array index of o n +o m  bits can be built in o m  time  and support access to any requested string in o 1  time.
1 paper outline
　the rest of this paper is structured as follows. in section 1 we describe the basic ideas of the spectral bloom filter as an extension of the bloom filter. in section 1  we describe two heuristics which improve the performance of the sbf with regards to error ratio and size. section 1 deals with the problem of efficiently encoding the data in the sbf  and presents the string-array index data structure which provides fast access while maintaining the compactness of the data structure. section 1 presents several applications which use the sbf. experimental results are presented in section 1  followed by our conclusions.
1. spectral bloom filters
　this section reviews the bloom filter structure  as proposed by bloom in . we present the basic implementation of the spectral bloom filter which relies on this structure  and present the minimum selection method for querying the sbf. we briefly discuss the way the sbf deals with insertions  deletions  updates and sliding window scenarios.
1 the bloom filter
　a bloom filter is a method for representing a set s = {s1 s1 ... sn} of keys from a universe u  by using a bitvector v of m = o n  bits. it was invented by burton bloom in 1 .
　all the bits in the vector v are initially set to 1. the bloom filter uses k hash functions  h1 h1 ... hk mapping keys from u to the range {1...m}. for each element in s （ s  the bits at positions h1 s  h1 s  ... hk s  in v are set to 1. given an item q （ u  we check its membership in s by examining the bits at positions h1 q  h1 q  ... hk q . if one  or more  of the bits is equal to 1  then q is certainly not in s. otherwise  we report that q is in s  but there may be false positive error: the bits hi q  may be all one even though q 1（ s  if other keys from s were mapped into these positions. we call this bloom error and denote it by eb.
　the probability for a false positive error is dependent on the selection of the parameters m k. after the insertion of n keys at random to the array of size m  the probability that a particular bit is 1 is exactly  1   1/m kn. hence the probability for a bloom error in this situation is
.
the right-hand expression is minimized for    in which case the error rate is  1 k =  1 m/n. thus  the bloom filter is highly effective even for m = cn using a small constant c. for c = 1  for example  the false positive error rate is slightly larger than 1%. let γ = nk/m; i.e  γ is the ratio between the number of items hashed into the filter and the number of counters. note that in the optimal case  γ = ln 1  「 1.
1 the spectral bloom filter
　the spectral bloom filter  sbf  replaces the bit vector v with a vector of m counters  c. the counters in c roughly represent multiplicities of items  all the counters in c are initially set to 1. in the basic implementation  when inserting an item s  we increase the counters ch1 s  ch1 s  ... chk s  by 1. the sbf stores the frequency of each item  and it also allows for deletions  by decreasing the same counters. consequently  updates are also allowed  by performing a delete and then an insert .
sbf basic construction and maintenance
let s be a multi-set of keys taken from a universe u. for x （ u let fx be the frequency of x in s. let
vx = {ch1 x  ch1 x  ... chk x }
be the sequence of values stored in the k counters representing x's value  and  vx = {v x1 v x1 ... v xk} be a sequence consisting of the same items of vx  sorted in non-decreasing order; i.e. mx = v x1 is the minimal value observed in those k counters.
to add a new item x （ u to the sbf  the counters
{ch1 x  ch1 x  ... chk x } are increased by 1. the spectral bloom filter for a multi-set s can be computed by repeatedly inserting all the items from s. the same logic is applied when dealing with streaming data. while the data flows  it is hashed into the sbf by a series of insertions.
querying the sbf
a basic query for the sbf on an item x （ u returns an estimate on fx. we define the sbf error  denoted esbf  to be the probability that for an arbitrary element z  not necessarily a member of s   f z 1= fz. the basic estimator  denoted as the minimum selection  ms  estimator is f x = mx. the proof of the following claim as well as of other claims are omitted due to space limitation  and are given in the full paper .
claim 1. for all x （ u  fx ＋ mx. furthermore  fx =1
                                                                 k mx with probability esbf = eb 「	1   e kn/m	.
the above claim shows that the error of the estimator is onesided  and that the probability of error is the bloom error. hence  when testing whether fx   1 for an item x （ u  we obtain identical functionality to that of a simple bloom filter. however  an sbf enables more general tests of fx   t for an arbitrary threshold t − 1  for which possible errors are only false-positives. for any such query the error probability is esbf.
deletions and sliding window maintenance
deleting an item x （ u from the sbf is achieved simply by reversing the actions taken for inserting x  namely decreasing by 1 the counters {ch1 x  ch1 x  ... chk x }. in sliding windows scenarios  in cases data within the current window is available  as is the case in data warehouse applications   the sliding window can be maintained simply by preforming deletions of the out-of-date data.
distributed processing
the sbf is easily extended to distributed environment. it allows simple and fast union of multi-sets  for example when a query is required over several sets. once a query is required upon the entire collection of sets  sbfs can be united simply by addition of their counter vectors. this property can be useful for partitioning a relation into several tables covering parts of the relation. other features of the sbf relevant to distributed execution of joins are presented in section 1.
sbf multiplication
several applications  such as bloomjoins  see section 1   can be implemented efficiently by multiplying sbf. the multiplication requires the sbf to be identical in their parameters and hash functions. the counter vectors are linearly multiplied to generate an sbf representing the join of the two relations. the number of distinct items in a join is bounded by the maximal number of distinct items in the relations  resulting in an sbf with fewer values  and hence better accuracy.
external memory sbf
while bloom filters are relatively compact  they may still be too large to fit in main memory. however  their random nature prevents them from being readily adapted to external memory usage because of the multiple  up to k  external memory accesses required for a single lookup. in   a multi-level hashing scheme was proposed  in which a first hash function hashes each value to a specific block  and the hash functions of the bloom filter hash within that block. the analysis in  showed that the accuracy of the bloom filter is affected by the segmentation of the available hashing domain  but for large enough segments  the difference is negligible. the same analysis applies in the sbf case  since the basic mechanism remains the same.
sbf implementation
the major issues that need to be resolved for this data structure are maintaining the array of counters  where we must consider the total size of the array  along with the computational complexity of random access  inserts and deletions from the array  and query performance  with respect to two error metrics: the error rate  similar to the original bloom filter   and the size of the error.
1. optimizations
　in this section we present two methods that significantly improve the query performance that is provided by the sbf when the threshold is greater than 1; both in terms of reducing the probability of error esbf  as well as reducing the magnitude of error  in case there is one. for membership queries  i.e.  threshold equals 1   the error remains unchanged.
1 minimal increase
　the minimal increase  mi  algorithm uses a pretty simple logic: since we know for sure that the minimal counter is the most accurate one  if other counters are larger it is clear that they have some extra data because of other items hashed to them. knowing that  we don't increase them on insertion until the minimal counter catches up with them. this way we minimize redundant insertions and in fact  we perform the minimal number of increases needed to maintain the property of  x （ u  mx − fx  hence its name.
minimal increase when performing an insert of an item x  increase only the counters that equal mx  its minimal counter . when performing lookup query  return mx. for insertion of r occurrences of x this method can be executed iteratively  or instead increase the smallest counter s  by r  and set every other counter to the maximum of their old value and mx + r.
　a similar method appeared in   referred to as conservative update. we develop this method further and set some claims as to its performance and abilities. the performance of the minimal increase algorithm is quite powerful:
　claim 1  minimal increase performance . for every item x （ u  the error probability in estimating fx using the mi algorithm  esbf  is at most eb  and the error size is at most that of the ms algorithm. its counters hold the minimal values which maintains mx − fx.
　the minimal increase algorithm is rather complex to analyze  as it is dependent upon the distribution of the data and the order of its introduction. for the simple uniform case we can quantify the error rate reduction:
　claim 1. when the items are drawn at random from a uniform distribution over u  the mi algorithm decreases the error esbf by a factor of k.
thus  the mi algorithm is strictly better than the ms algorithm for any given item  and can result with significantly better performance. this is indeed demonstrated in the experimental studies. note that no increase in space is required here.
minimal increase and deletions.. along with the obvious strength of this method  it is important to note that even though this approach provides very good results while using a very simple operation scheme  it does not allow deletions. in fact  when allowing deletions the minimal increase algorithm introduces a new kind of errors - false-negative errors. this result is salient in the experiments dealing with deletions and sliding-window approaches  where the minimal increase method becomes unattractive because of its poor performance  mostly because of false negative errors.
1 recurring minimum
　the main idea of the next heuristics is to identify the events in which bloom errors occur  and handle them separately. we observe that for multi-sets  an item which is subject to bloom error is typically less likely to have recurring minimum among its counters. for item x with recurring minimum  we report mx as an estimate for fx  with error probability typically considerably smaller than eb. for the set consisting of all items with a single minimum  we use a secondary sbf. since the number of items kept in the secondary sbf is only a small fraction of the original number of items  we have improved sbf parameters  compared to the primary sbf   resulting with overall effective error that can be considerably smaller than eb.
　let ex be the event of an estimation error for item x: mx 1= fx  i.e.  mx   fx . let sx be the event where x has a single minimum  and rx be the event in which x has a
　recurring minimum  over two or more counters .
　table 1 shows experimental results when using a filter with k = 1 n = 1  secondary sbf size of ms = m/1  various γ values and zipfian data with skew 1. values shown are γ  usual bloom error eb  fraction of cases with recurring minimum  p rx    fraction of estimation errors in those cases  p ex|rx    the γ parameter for the secondary sbf γs = n 1   p rx  k/ms  ebs - the calculated bloom error for the secondary sbf. the next column shows the expected error ratio which is calculated by
erm = p rx p ex|rx  +  1   p rx  ebs
the last column is the ratio between the original error ratio and the new error ratio. note that for the  recommended  case of γ = 1  the sbf error  erm  is over 1 times smaller than the bloom error.
　note that the recurring minimum method requires additional space for the secondary sbf. this space could be used  instead  to reduce the bloom error within the basic  minimum selection method. table 1 compares the error obtained by using additional memory  presented as a fraction of the original memory m  to increase the size of the primary sbf within the minimum selection method  vs. using it as a secondary sbf within the recurring minimum method. the error ratio row shows the ratio between the error of minimum selection and the error of the recurring minimum methods. in the minimum selection method  when we increased the primary sbf  we increased k from its original value k = 1  maintaining γ at about 1  so as to have maximum impact of the additional space . the new value for k is shown in the table. a ratio over 1 shows advantage to the recurring minimum method. for instance  when having additional 1% in space  recurring minimum performs about 1 times better than minimum selection  note that as per table 1 the total improvement is by a factor of about 1 .
the algorithm. the algorithm works by identifying potential errors during insertions and trying to neutralize them. it has no impact over  classic  bloom error  false-positive errors  since it can only address items which appear in the data; it reduces the size of error for items which appear in the data and are  stepped over  by other items. the algorithm is as follows:
　when adding an item x  increase the counters of x in the primary sbf. then check if x has a recurring minimum. if so  continue normally. otherwise  if x has a single minimum   look for x in the secondary sbf. if found  increase its counters  otherwise add x to the secondary sbf  with an initial value that equals its minimal value from the primary sbf.
　when performing lookup for x  check if x has a recurring minimum in the primary sbf. if so return the minimum. otherwise  perform lookup for x in secondary sbf. if returned value is greater than 1  return it. otherwise  return minimum from primary sbf.
　a refinement of this algorithm which improves its accuracy but requires more storage uses a bloom filter bf of size m to mark items which were moved to secondary sbf. when an item x is moved to the secondary sbf  x is inserted into bf as well  and this marks that x should be handled in the secondary sbf from now on. when inserting an item and it exists in bf it is handled in the secondary sbf  otherwise it is handled as in the original algorithm. when performing lookup for x  bf is checked to determine which sbf should be examined for x's frequency.
　the additional bloom filter might have errors in it  but since only about 1% of the items have a single minimum  as seen in the tables   the actual γ of bf is about a fifth of the original γ. for γ = 1 k = 1  this implies a bloom error ratio of  1  e 1/1 = 1，1  which is negligible when compared with other errors of the algorithm.
deletions and sliding window maintenance
deleting x when using recurring minimum is essentially reversing the increase operation: first decrease its counters in the primary sbf  then if it has a single minimum  or if it exists in bf  decrease its counters in the secondary sbf  unless at least one of them is 1. since we perform insertions both to the primary and secondary sbf  there can be no false negative situations when deleting items. sliding window is easily implemented as a series of deletions  assuming that the out-of-scope data is available.
analysis. since the primary sbf is always updated  in case the estimate is taken from the primary sbf  the error is at most that of the ms algorithm. in many cases it will be considerably better  as potential bloom error are expected to be identified in most cases. when the secondary sbf provides the estimate  errors can happen because of bloom errors in the secondary sbf  which is less probable than bloom errors in the primary sbf   or due to late detection of single minimum events  in which case the magnitude of error is expected to be much smaller than in the ms algorithm . a full analysis is given in the full paper.
1 methods comparison
we compare the three methods.
error rates. the ms algorithm provides the same error rates as the original bloom filter. both rm and mi methods perform better over various configurations  with mi being the most accurate of them. these results are consistent in the experimental results  taken over data with various skews and using several γ values. for example  with optimal γ and various skews  mi performs about 1 times better in terms of error ratio than the ms algorithm. the rm algorithm is not as good  but is consistently better than the ms algorithm.
memory overhead. the rm algorithm requires an additional memory for storing the secondary sbf  so it is not always cost-effective to use this method. the mi algorithm is the most economical  since it needs the minimal number of insertions. note that  as seen in the experiments  when using the same overall amount of memory for each method  the rm algorithm still performed better than the ms algorithm  but mi outperforms it .
complexity. the rm algorithm is the most complex method  because of the hashing into two sbfs  but this happens only for items with non-recurring minimum. as shown above  this happens for about 1% of the cases  which accounts for 1% increase in the average complexity of the algorithm. when using the flags array in the rm algorithm  the complexity naturally increases.the ms method is the simplest. updates/deletions. both the ms and rm methods support these actions. the mi algorithm does not  and may produce false-negative errors if used. experiments show that in these cases  the mi algorithm becomes practically unusable. for example  using sliding window  the additive error
γebp rx p ex|rx γsebermeb/erm1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.111 ， 11 ， 11s
table 1: error rates with recurring minimum and without it. eb is the usual bloom error  p rx  is the ratio of recurring minimum  p ex|rx  is the ratio of errors given recurring minimum  γs ebs are the secondary bf parameters  with size m/1   erm is esbf for recurring minimum  and the last column is the gain.
memory increase1.1.1.1.1.1error ratio111111modified k111table 1: effect of increased memory for primary sbf and secondary sbf  with original k = 1.of the mi algorithm is 1 to 1 orders of magnitude larger than that of the rm algorithms  for various skews.
1. data structures
　while the data structure implementation of the  original  bloom filter is a simple bit-vector  the implementation of the sbf presents a different challenge. the sbf of a multiset of m items  consists of a sequence of counters
c  c  ... c   where c is the number of items hashed into
; then 
knlog m/n   wherep n is the number
of distinct items in the set. the goal is to have a compact encoding of the sbf which is as close to n as possible. clearly  a straight-forward implementation of allocating logm bits per counter is excluded. in this section we show:
　theorem 1. an sbf of size n+o n +o m  bits can be constructed in o n  time  supporting lookup in o 1  time. furthermore  the sbf can be maintained so that insertions  deletions and updates take each o 1  expected amortized time.
　the basic representation of the sbf consists of embedding the counters ci in their dlogcie-bit binary representation  consecutively in a base array of size n bits.  for simplicity of exposition  we will omit below the ceiling operator.  in the static case the counters are placed without any gap between them  totaling n bits  whereas to support dynamic changes we add 1m slack bits between counters  where 1   1 is a small constant. this representation introduces a challenge in executing the lookup operations  since locations of various strings are not known due to their variable sizes.
　in section 1 we address this challenge  presenting a data structure that enables effective  random access  to the i'th substring  for any i  in a sequence consisting of arbitrary variable length substrings. section 1 shows how to handle the dynamic problem  supporting inserts and deletes over the data set represented by the sbf. the proposed sbf implementation is general  with no assumption made on the distribution of the data. finally  in section 1  we show an alternative method which requires only o m  bits in addition to the base array  rather than o n +o m    but which is less efficient when performing lookups.
1 the string-array index
　we first define a general access problem related to the one encountered in the context of the sbf.
variable length access problem. let {s1 s1 ... sm} be binary strings of arbitrary lengths. let s = s1 ...sm be the concatenation of those substrings  with length |s| = n. given an arbitrary i  1 ＋ i ＋ m  return the position of si in s  and optionally  si itself.
　note that the lookup problem for the sbf compact basearray representation is the variable length access problem with two additional constraints:  i   i |si| ＋ logm; and  ii  the strings roughly represent the frequencies of items in the given data set  and the order between them is determined at random using the hash functions of the sbf. we describe a data structure  the string-array index  that addresses the general  unconstrained variable length access problem.
　the string-array index uses a combination of various instances of three types of simple data structures  which hold offset data for given sequences of some σ items  totaling some t bits:
1. coarse vector - this is the backbone of the string-array index  and its role is to effectively reduce a given problem into a set of smaller sub-problems. it partitions the given sequence into σ/σ1 subsequences of σ1 items each  and provides offset information for the beginning of each subsequence  using an array of fixed-sized offsets. the coarse vector requires  σ/σ1 logt bits  and reduces the access problem  for a given i  into a problem with σ1 items and some t1   t length.
1. offset vector - provides a straightforward representation of the σ offsets in an array  requiring σ logt bits  and supports o 1  lookup time. it is used when σ is small relative to t; in particular when σ logt   t  and it can therefore be stored for such subsequences within the required space bounds. if t ┐ σ logn then the offsets are with respect to the base array.
1. lookup table - a global array  whose indices represent all possible sequences and queries over those sequences  for a sufficiently small t. it requires 1o t  bits  which is o n  for t = o logn . a problem with a sufficiently small t can use it for o 1  lookup time  by storing additional appropriate encoding information that maps

figure 1: the string-array index data structure.
it into its appropriate array index.
　for a given variable length access problem consisting of m strings totaling n bits  a string-array index can be constructed as follows.
　lemma 1  string-array index . the string-array index data structure of size o n  + o m  bits can be built in o m  time  and subsequently support access to any requested item si in o 1  time.
　the string-array index is depicted in figure 1; it consists of two levels of arrays of pointers to sub-sequences of s. the first level consists of a coarse offset array c1  which holds m/logn offsets of the positions of logn-size groups of items in the sbf base array. since offsets are at most n  they can be represented using logn bits  for a total size of m bits. the offset in cj1 points to the  j logn 'th item in s  i.e.  to sr where r =  j logn . thus  for any i  one access to c1 can provide us with the pointer to a subsequence s1 of logn items in s  that includes si.
　the second level enables effective access within such subsequences s1. if a subsequence is of size larger than log1 n bits  then it is supported by a simple offset vector  consisting of the logn offsets of the individual items of the subsequence  in the sbf base array; each offset is of logn bits  totaling log1 n bits for the entire offset vector. the total size of all such offset vectors is at most n/logn bits.
　each subsequence s1 whose size is at most log1 n bits is supported by a level-1 coarse offset array cj1  which partitions s1 to chunks of loglogn items. it holds logn/loglogn offsets of the loglogn-size chunks s1 inside s1. since offsets are at most log1 n  each can be represented using 1loglogn bits  totaling 1logn bits per a subarray cj1. the total size of all such subarrays is hence at most 1m.
　a lookup using the string-array index requires 1 lookups through the coarse offset arrays  which provides with either the exact position of the requested item in the sbf base array  or a pointer to the beginning of a subsequence s1 of loglogn items  which includes the requested item. the items within each subsequence s1 are accessed either through an offset vector built for s1  or using a global lookup table shared by all subsequences  depending on the size of s1. we use a threshold t1 =  loglogn 1  to determine which method is used. let s1 be of size t = t s1  bits.
　if t   t1  we keep for s1 an offset vector; since t ＋ log1 n  each offset can be represented using 1loglogn bits  and the offset vector for s1 will consist of such loglogn offsets  totaling size 1 loglogn 1   t s1 . hence  the total size of all such offset vectors is o n .
　it remains to deal with s1 such that t ＋ t1. we keep a single global lookup table  that will serve all such subproblems. an entry to the lookup table consists of a string representing a subsequence s1 and an index i  1 ＋ i ＋ loglogn. for each such entry  the lookup table will return the offset from the beginning of s1 in the sbf base array  of the i'th item in s1.
　the lookup table consists of a simple array lt  whose indices represent all binary combinations representing the entries hs1 ii  and for each entry the result is precomputed and stored in the array lt. each entry can be represented as the t-bit subarray of the sbf base array representing s1  and a secondary subarray l s1  consisting of an encoding of the lengths of the items in s1  so as to allow unique interpretation of the t-bit subarray representing s1. the encoding in l s1  has the property that the size of each code word is proportional to the encoding length of the value it represents. this is obtained using  e.g.  elias encoding  see section 1 . the length of l s1  is either o loglogn  or o t . in addition to the representation of s1  including l s1   the entry includes the index i  consisting of logloglogn bits .
　it is easy to see that since t ＋ t1  the total size of lt is o n  bits  and that all its entries can be computed in o n  time. the subarray l s1  is stored for each s1 whose size t is less than t1 as part of the sbf. the offset of the ith item in such s1 is obtained by looking up at lt the value corresponding to the entry consisting of the hs1 ii  as determined using l s1 .
　in summary  the string-array index consists of the following components: the coarse offset array c1  an array c1 consisting of all level-1 coarse offset arrays cj1  the offset vectors of first level and second level sequences  the global lookup table lt  and the length arrays l s1 . the total size of the string-array index is o n  + o m   its construction takes o m  time  and it can be used as discussed to solve the variable length access problem in o 1  time. the lemma follows.
　note that when actually implementing a string-array index  several of the structures could be eliminated or altered due to practical considerations. in particular  even for relatively large values of n  one should not be concerned with paying o loglogn  factor overhead for a fraction of the data structure.
　the sbf can now be constructed as stated in theorem 1: the base-array is built in o n  time by updating the counters ci as the input data set items are hashed one by one. subsequently  building the string-array index over the base array. this requires using during construction time a temporary array of o mlogm  bits. next subsection shows how to construct the sbf incrementally  as well as how to support update operations  without using any temporary array  and within the storage bounds of n + o n  + o m  bits.
1 handling updates
　we show how to extend the string-array index data structure described above  to allow dynamic changes in the dataset  for a base array of an sbf. when one of the counters increases its bit-size in the base array  additional space needs to be allocated within the base array to accommodate the enlarged substring. it is also necessary to update the string-array index structure to reflect the changes made in the base-array. delete operations only affect individual counters  and do not affect their positions  and hence the string-array index. to remain within storage bounds  after a long sequence of deletions the entire data structure is rebuilt  with amortized constant time per deletion.
　to support inserts  we allocate a slack of extra bits in the base array. in particular  we add 1m slack bits  one every 1 items  for some 1   1. a counter which needs to expand  pushes  the item next to it  which in turn pushes the next item  until a slack is encountered. for each item  the nearest slack is initially allocated within a distance of at most 1 items. however  upon expansion  the nearest slack may not be available  in case at least one of the items between the expanded item and the slack was already expanded. in such case  farther slack will need to be used. the cost of expansion is linear in the number of items that need to be pushed  assuming that each item fits into machine word.
　the next lemma bounds the expected distance from an expanded item to the nearest available slack  using the fact that items location is determined at random by the hash functions of the sbf. for purpose of simplicity  we assume full randomness. it is assumed that the number of inserts is at most 1m  for some 1   1. after 1m inserts  the base array is refreshed by moving counters so that that slacks are again placed in 1 intervals  and the string-array index is updated accordingly.
　lemma 1. suppose that the size of some counter cj increases  and that the total number of insertions is at most 1m  for 1 = 1e. then  the number of items between cj and the first available slack  denoted `j  satisfies e `j  = o 1 .
　proof. suppose first that cj increases for the first time. a slack is available within the sub-array of i/1 items following cj  if the number di of expansions of items within this sub-array is less than i. since items are hashed into the base array at random  then for any sequence of 1m insertions  di is bounded by a binomial with parameters  1m i/ 1m  . hence  e di  ＋ i1m/ 1m  = i1. the probability that items within i chunks will need to move upon an insertion is bounded by  with the last inequality due to chernoff bounds. hence 
1. p	p	p
e
　it remains to account for repeated expansions of particular counters. suppose that a counter cj has a sequence of x expansions. for the last expansion  it is guaranteed that the nearest x 1 slack bits are not available. further  items within the nearest x   1 chunks of size 1 might also have been expanded resulting with additional slack unavailability. on the other hand  the additional expected cost can be amortized against the 1x updates to cj which are required to facilitate x expansions. the expected amortized cost per repeated expansion remains o 1 . 
　the string-array index is updated when items are moved. the update of the structure has the same computational complexity as that of updating the base array itself  since essentially only offset information about items that are pushed needs to be changed in the string-array index. the expected amortized cost per update therefore remains o 1 . since refreshing the entire base array and updating the string-array index takes o m  time  the amortized cost of such refresh and update is o 1  per update.
1 an alternative approach
　the data structure can be made more compact  while sacrificing lookup performance  by using the c1 and c1 indexes and not building any further structures. once the problem is reduced to loglogn items  we allow a serial scan of the sub-group in order to access the requested item. to allow that  we need a compact prefix-free encoding that can be read sequentially. for this purpose we use a combination of elias encoding and a method which is more compact for small counters.
　in this scenario  a sub-group consists of loglogn items. using the encodings presented in this section  each counter with value c can be encoded with close to logc bits. therefore  this approach requires n bits to encode the actual counters in the original vector  with additional o m  bits for the structures of c1 and c1  while on average a lookup costs loglogn. the same approach described in section 1 can be used to allow dynamic maintenance of the structure.
elias encoding
the elias encoding  consists of the following method: let b n  be the binary representation of the integer  with length l n . a binary prefix code b1 n  is created by adding a prefix of l n    1 zeroes to the beginning of b n . now we create the sequence representing n by encoding b1 l n   followed by b n  with its leading 1 removed1. the total length of this representation is
l1 n  = blog1 nc + 1blog1  blog1 nc + 1 c + 1
the steps method
elias encoding is a strong and simple method to create an encoding which is prefix-free while being compact. however  for very small numbers the overhead of loglogn bits is substantial and should be avoided. for example  to encode the number 1  actually encoding the number 1  we need 1 bits. in sets  most counters will be 1  so for an optimal hit ratio of 1  the average is 1 bits per counter.
　to solve that problem  we use compact encoding for small numbers. for example  using 1 to represent 1  1 to represent 1 and 1 means the number is bigger than 1  with the elias encoding of this number following the prefix. this reduces the cost to 1 bits per counter. it is further reduced if we encode longer sequences  reducing the overhead to an 1 as small as we choose. full details are omitted due to space limitations .
1. applications
　in this section we explore a range of applications which may take advantage of the abilities of the sbf. the sbf enables new applications which use its properties to efficiently perform tasks such as ad-hoc iceberg queries. other application  such as bloomjoins or range queries  are extensions of methods or abilities of the regular bloom filter.
1 aggregate queries over specified items
　spectral bloom filters hold mostly accurate information over each and every item of the data set  and therefore can approximately answer any  aggregate  query regarding a given subset of the items  so that the error ratio is expected to be esbf  and the size of the error is expected to be smaller than the average frequency of items in the set  f．. for example  queries of the kind
select count a1  from r where a1 = v
　in performing this query  the sbf acts as an aggregate index built upon the attribute a1 and providing the  mostly  accurate frequency of v in the relation. other aggregates  such as average sum max etc. can be easily implemented using this basic ability. the sbf behaves very much like a histogram where each item has its own bucket. since the sbf keeps the full information  it is very versatile in its uses  while requiring storage relative to the size of the set.
1 ad-hoc iceberg queries
　in some cases we are interested in monitoring insertions  and want to set some triggers that will alert us once an item with a high count is inserted. for example  a company which tracks customers can create a calculation that reports their likeliness to churn. once a customer with a high churning probability contacts the company  the company representative should be alerted  so he can offer him special deals. the threshold for such special treatment is dynamic  and depends on many factors  so the calculation cannot be executed a priori.
　this example presents a sort of an iceberg query  in which the threshold against which items are tested upon insertion is dynamic and possibly changes between queries. other methods  proposed in  1  1  require a certain preprocessing the data given a static threshold. when the threshold changes  the methods of  1  1  require rescanning of the data using the new threshold  or in the case of streaming data   it cannot be done   while the sbf does not require any additional scan of the data  other than one that examines the data against the counts stored in the sbf.
1 spectral bloomjoins
　bloomjoins  are a method for performing a fast distributed join between relations r1 and r1 residing on different database servers - r1 in site 1 and r1 in site 1  based on attribute a. both relations have a bf built on attribute a. the bloomjoin method is executed in the following steps: r1 sends its bf  b1  to r1  r1 is scanned and tuples with a match in b1 are sent back to site 1 as r1 . at site 1  r1 is joined with  to produce final result. this method is economical in network usage  since in the first transmission  only a synopsis is sent  and the second transmission usually contains a small fraction of the tuples  since a filtering stage was executed.
　sbfs can be used to perform distributed aggregative queries  such as the following query  which filter the results using a given threshold:
select r.a count *  from r s
where r.a = s.a group by r.a
having count *     =  t
since in most schemas the join between the relations will
be a one-to-many join  the detail table s can send its sbf to r's site. the bloom filters are multiplied and r is scanned  testing each tuple in sbfrs against the threshold t. results can be reported immediately since no value is repeated more than once in r. when using      or  −   as the filter operator  there is only a small fraction ρ of false positive errors  e ρ  = esbf  and no false negatives. since the errors are one-sided  they can be eliminated by retrieving the accurate frequencies for the items in the result set  resulting in a fraction of ρ extra accesses to the data. the effectiveness of this method increases as the size of the result set decreases. when using the  =  operator  two-sided errors are possible  with recall of 1 esbf  and possibly additional false-alarms.
　the sbf's ability to maintain counters can also be used in queries which perform no filtering  such as the following:
select r.a count *  from r s
where r.a = s.a group by r.a
　to perform this query using a bloomjoin  the full scheme described in  must be executed  with bloom filters and tuple stream sent back and forth between the sites. however  using sbf multiplication  a shorter scheme can be executed  assuming that both s and r have a sbf representing the attribute a present  and r being the primary query site: s sends its sbf  sbfs  to r's site  where sbfs and sbfr are multiplied to create sbfsr. next  r is scanned  and each tuple is checked against sbfsr for existence. if it exists  the item and its frequency are reported.
　this scheme does not guarantee exact results. items which appear in r and not in s may be reported because of errors in sbfs. also  the frequencies reported are subject to bloom error and may be higher than their actual value. to ensure the uniqueness of items in the results  we suggest the use of a validating sbf for that purpose. this method saves the transmission of data back to the main site. if the main site has to be the one reporting the results  the final answer may be sent back to it  with minuscule network usage.
advantages. using sbf for bloomjoins simplifies and shortens the algorithm for performing the distributed joins. while the sbf itself is slightly larger than a bloom filter of the same parameters  this is balanced by the shorter operation scheme  requiring less sbfs to be sent between sites  and therefore saving bandwidth.
1 bifocal sampling
　a spectral bloom filter can be plugged into various schemes that require an index on a relation for count queries. one such application is bifocal sampling   where using an sbf one can get similar join estimations without using an expensive index. the paper deals with joining two relations with unknown properties by dividing each relation to two distinct groups: dense and sparse tuples. the join size is estimated by combining the groups in all ways possible  creating a dense-dense join and sparse-any joins. in the sparseany case  a join of type t-index  is used  meaning for each tuple in a sample of one relation  a query on the other relation is performed to determine the frequency of the join attribute in the second relation. by replacing the t-index with an sbf  the multiplicities used for estimation are replaced by their approximations  resulting with only a small additional error to the overall estimate.
advantages. the sbf provides an efficient approximation to the t-index scheme  and enables a more space-efficient implementation of bifocal sampling.
1. experiments
　we have tested the accuracy of the various sbf algorithms described in section 1  as well as the space efficiency of the encoding methods described in section 1.
algorithms comparisons. we have tested and compared the three lookup schemes from section 1: minimum selection  ms   recurring minimum  rm   and minimal increase  mi . the sbf was implemented using hash functions of modulo/multiply type: given a value v  its hash value h v  
1 ＋ h v    m is computed by h v  = dm αv mod 1 e  where α is taken uniformly at random from  1 . we measured two parameters; the first is the mean squared additive error  which is calculated by
	vuutp	1
the second is the error ratio eratio  computed as the fraction of the queries that return erroneous results. thus  e eratio  = esbf  and for ms  it is eb. each reported result is the average over 1 independent experiments with the same parameters.
　two sets of tests were conducted; in both we used synthetic data produces by a zipfian distribution. we used integers as data values  and the data set was constructed of 1 distinct values  with m = 1. we have also conducted experiments in which m  and hence the average item frequency  was changed  generating smaller  and bigger  data sets. the observed behavior was consistent with the experiments reported here.
　in the first set of tests  the skew of the data was changed  from θ = 1  uniform data  to θ = 1  very skewed data . the results are shown in figure 1a b  solid lines . as can be seen  the mi algorithm has the best performance both in terms of additive error and error ratio  and is very stable with regard to changes in the skew. the rm algorithm outperforms the ms algorithm in both parameters  but in most cases is no match to the mi algorithm.
　in the second set of tests  the storage size m was changed  to produce γ = nk/m ranging from about 1 to about 1. the results are shown in figure 1a b. for a fair comparison between the algorithms  in this and in all other experiments the rm algorithm used m as an overall storage size; that is the sizes of the primary and the secondary sbfs together being m. this causes the actual γ of the rm algorithm in its primary sbf to be larger than that of the ms and mi algorithms. these experiments show that all three algorithms behave similarly  with rm and mi being almost identical in their error ratios. the mi algorithm performs best in terms of additive error when m is small  and γ increases . this is due to the fact that it performs a minimal number of actual insertions into the base array  which becomes critical as the error ratio increases.
　the third experiment tested the behavior of the various schemes when the number of hash functions  k  changes.
the data used was again zipfian with a skew of 1  in all configurations γ was fixed at 1 by increasing m along with k. the results are shown in figure 1c. in the k = 1 case  all the methods perform the same  as they should . the mi method improves dramatically when k increases  while the rm method needs k of at least 1 to become effective  with major improvement when k increases to 1 and more. these experiments show clearly the incredible precision and stability of the minimal increase method  and also the substantial improvement that the recurring minimum method shows over the minimum selection.
deletions and sliding window. next  we tested the sbfs when faced with deletions. the setup consisted of a series of insertions  followed by a series of deletions and so on. in every deletion phase  1% of the items were randomly chosen and were entirely deleted from the sbf. the results  shown in figure 1  compare the error ratio and the additive error of the sbfs when subject to deletions to their performance without deletions. it is evident that the mi algorithm deteriorates dramatically when deletions are performed. the third graph shows the main reason for that false-negative errors. note that almost all of the errors of the mi algorithm are false negatives  ms and rm have no false-negatives . this makes it a poor choice when deletions are considered  since the one-sided nature of the errors is no longer valid.
　the second test shown in figure 1  used a sliding window scenario. in this experiment  a total of m items were inserted  but the sbfs only kept track of the m/1 most recent items as items were inserted  with data leaving the window explicitly deleted. the ms and the rm algorithm are much better that the mi algorithm for this scenario  with advantage to the rm.
encoding methods. we tested the storage needed by the encoding methods described in section 1  comparing the elias method  and several configurations of  steps  for data with varying average frequency of items. the results  shown at figure 1  were compared to the  log counters   which close to 1   almost set   the steps methods are more eco-p is simply . for data sets with average frequency
nomical  due to their low overhead. however  the elias encoding improves as the average frequency increases  and beats the performance of the steps methods.
1. conclusions
　this paper presented spectral bloom filters  extending bloom filters by storing counters instead of bit flags. the structure supports updates and deletions  while preserving storage size of n +o n +o m  bits. we presented several heuristics for insertions and lookups in a sbf. minimum selection uses the same logic as the original bloom filter. minimal increase is a simple yet powerful heuristic with very low error rates  but no support for updates and deletions. recurring minimum uses a secondary storage to take care of  problematic  cases  and it supports deletions and updates with no accuracy loss. we also present the stringarray index  a data structure which provide fast access to variable-length encoded data while being compact enough to be used in the spectral bloom filter. we show its structure and maintenance for static data and during dynamic
　

figure 1: accuracy of ms  mi and rm algorithms for various values of γ  with k = 1  with additive error  a   and log of error ratio  b   dotted line represent optimal γ. additive errors in the three algorithms for various k values  with γ = 1  c . in all experiments  mi and rm are better than ms  with some advantage to mi.

figure 1: performance of ms  rm and mi algorithms for zipfian distribution with varying skew  θ   with deletions  dotted lines  and without deletions  full lines . both additive error  left  and log of error ratio  center  are shown; in all experiments γ = 1 k = 1. the third graph shows the ratio of false negative errors in the mi algorithm out of the total errors  there are no false negatives in ms and rm .

	skew	skew
figure 1: accuracy of ms  rm and mi algorithms for zipfian distribution of varying skew  θ   in a sliding window scenario. both log of additive error and log of error ratio are shown  in all experiments γ = 1 k = 1.
　

figure 1: comparison of various encoding methods. several  steps  configurations were tested along with elias encoding. the results are compared to the optimal log of the counters.
changes in the data-set.
　there are several extensions to the basic functionality of the sbf. one property is the ability to union sets effectively  provided that the same parameters are used  hash functions and array size . for such bloom filters  a union of two data sets only requires an addition of the counter vectors representing them. the sbf can support both streaming data and sliding window data sets   given that old data is available for deletion.
　the sbf enables new applications  and enables more effective execution of existing applications. sbfs can be used for maintaining demographics of a multiset or set  and allow data profiling and filtering using an arbitrary threshold. it can be used for ad-hoc iceberg-queries  where the threshold defining the query is not known in construction time  or changes as the data is queried. bifocal sampling  can use sbf as an index data structure in the sparse-any procedure  in fact  sbf can be used in any join of type t-index . the sbf can also be plugged into many applications currently using bloom filters. for example  bloomjoins  can be extended using sbf  with better efficiency for many types of queries.
