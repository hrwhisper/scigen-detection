schema matching is a critical problem for integrating heterogeneous information sources. traditionally  the problem of matching multiple schemas has essentially relied on finding pairwise-attribute correspondence. this paper proposes a different approach  motivated by integrating large numbers of data sources on the internet. on this  deep web   we observe two distinguishing characteristics that offer a new view for considering schema matching: first  as the web scales  there are ample sources that provide structured information in the same domains  e.g.  books and automobiles . second  while sources proliferate  their aggregate schema vocabulary tends to converge at a relatively small size. motivated by these observations  we propose a new paradigm  statistical schema matching: unlike traditional approaches using pairwise-attribute correspondence  we take a holistic approach to match all input schemas by finding an underlying generative schema model. we propose a general statistical framework mgs for such hidden model discovery  which consists of hypothesis modeling  generation  and selection. further  we specialize the general framework to develop algorithm mgssd  targeting at synonym discovery  a canonical problem of schema matching  by designing and discovering a model that specifically captures synonym attributes. we demonstrate our approach over hundreds of real web sources in four domains and the results show good accuracy.
1.	introduction
　schema matching is fundamental for enabling query mediation and data exchange across information sources. this paper attempts to consider the schema matching problem with a new paradigm: traditionally  such matching has been approached mainly by finding pairwise-attribute correspondence  to construct an integrated schema for two or some  small number of  n sources. we observe that there are often challenges  and certainly also opportunities  to deal

  this material is based upon work partially supported by nsf grant iis-1. any opinions  findings  and conclusions or recommendations expressed in this publication are those of the author s  and do not necessarily reflect the views of the funding agencies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
with large numbers of sources. as a different approach  we take a holistic view of all the attributes in input sources and attempt to find an underlying unified model which captures their matchings.
　such scenarios arise  in particular  for integrating databases across the internet. on this  deep web   numerous data sources provide structured information  e.g.  amazon.com for books; cars.com for automobiles  accessible only via dynamic queries instead of static url links. each source accepts queries over its schema attributes  e.g.  author and title for amazon.com  through its query interface. thus  schema matching across query interfaces is clearly essential for mediating related sources of the same domains  which this paper specifically focuses on.
　on the deep web  we observe two distinguishing characteristics that offer a new view for schema matching: on one hand  we observe proliferating sources: as the web scales  many data sources provide structured information in the same domains  e.g.  there are numerous other  book  sources  such as bn.com . on the other hand  we also observe converging vocabularies: the aggregate schema vocabulary of these sources tends to converge at a relatively small size  e.g.  these books sources share 1 frequent attributes  which account for 1% of all attribute occurrences .
　these observations lead us to hypothesize that  underlying sources of the same domain  there exists a hidden schema model  which is a unified generative model that describes how schemas are generated from a finite vocabulary of attributes  with some probabilistic behavior. under this hypothesis  it is natural that we have observed proliferating sources with converging vocabularies.
　motivated by this hypothesis  we explore a new paradigm  which we call statistical schema matching. unlike traditional approaches using pairwise-attribute correspondence  given a set of input sources as observed schemas  we will find hidden models that are consistent  in a statistical sense  with the schemas observed. using a scenario of matching
several book sources  figure 1 contrasts the two different approaches. given a set of schemas as input  the traditional schema-matching approaches essentially rely on finding pairwise-attribute correspondence  e.g.  author in source s1 maps to name in s1  for eventually constructing a unified schema for all sources. in contrast  our approach hypothesizes and attempts to find  in the first place  a unified model  which gives the  structure  of the attributes across all sources  e.g.  author  name  and writer are the same concept  and so are subject and category .
　to realize statistical schema matching  we propose a general abstract framework  mgs  with three steps:  1  hypoth-

figure 1: two different matching approaches.
esis modeling: we first specify a parameterized structure of the hypothetical hidden models. such models should capture the target questions of schema matching that we want to address- e.g.  the model in figure 1 targets at synonym discovery.  1  hypothesis generation: we then generate all  consistent  models that instantiate the observed schemas with non-zero probabilities.  1  hypothesis selection: finally  we select models of sufficient statistical consistency with the observed schemas. such an underlying model is likely the one that unifies the input schemas  and answers our target questions. we stress that the discovery of the underlying model can directly address the target questions  e.g.  attribute correspondence . also  the model is itself useful as a mediated schema  e.g.  as a query front-end .
　further  we specialize the mgs framework for synonym discovery- a canonical problem in schema matching- across query interfaces of deep web sources. we view each query interface as a set of attributes  for querying   or a schema  and our goal is to find the synonym attributes. in this algorithm mgssd  we design a simple model to capture the target question of synonym attributes  i.e.  multiple attributes are of the same  concept  . given a set of schemas  mgssd generates hypothetical models  under which it is  possible   with non-zero probabilities  to  observe  these schemas. finally  mgssd adopts χ1 hypothesis testing to select candidate models that are consistent with the observed schemas at a sufficient significance level.
　we performed case studies for over 1 real sources in 1 domains: books  e.g.  amazon.com   movies  e.g.  blockbuster.com   music records  e.g.  towerrecords.com   and automobiles  e.g.  cars.com . our goals are two-fold:  1  verify the phenomenons  of proliferating sources and converging vocabularies  that support our motivating hypotheses  section 1 .  1  validate the performance of mgssdwith two suites of metrics: model accuracy and target accuracy. in either case  we observed good performance  section 1 .
　in our development  we also observed several interesting issues. can we deal with more expressive models  how can our framework benefit from existing techniques  as  surveys   does a hidden model always exist for a collection of schemas  we discuss these open issues in section 1.
　in summary  our approach takes a new view to cope with schema matching:  1  we study and report two distinguishing characteristics of databases on the deep web  an important frontier for information integration.  1  we present a new paradigm  hidden model discovery  for large-scale schema matching  realized by a general statistical framework mgs.  1  we develop algorithm mgssd specifically for synonym discovery across web query interfaces.
we discuss related work in section 1. section 1 explores
our observations of sources on the deep web. section 1 presents the general mgs framework for statistical schema matching  which section 1 specializes to mgssd for synonym discovery for web interfaces. section 1 reports our case studies and evaluation. section 1 discusses some open issues.
1.	related work
　schema matching  which this paper focuses on  is one critical step for schema integration  1  1 . we relate our work to existing works in four aspects: the paradigms  the techniques  the input data  and the focuses.
　first  paradigms: traditionally  schema matching relies on matchings between pairwise attributes before integrating multiple schemas. for instance  traditional binary or n-ary  schema integration methodologies  as  surveys  exploit pairwise-attribute correspondence assertions  mostly manually given  for merging two or some n sources. the more recent development of general model management essentially abstracts schema matching as pairwise similarity mappings between two input sources . in contrast  we propose a new paradigm  statistical schema matching  to holistically match many sources at the same time by discovering their underlying hidden model. our work was motivated by integrating the deep web  where the challenge of large scale matching is pressing. our framework leverages such scale to enable statistical matching.
　the closest idea is probably the recent revere proposal   which suggests to use a separately-built schema corpus as a  knowledge-base  for assisting matching of unseen sources. while sharing the same insights of statistics analysis over corpora  our approach differs in that it leverages input schemas themselves as the corpus and assumes a generative model to unify the corpus.
　second  techniques: based on our observation of the deep web sources  section 1   we develop a statistical framework  which contrasts with existing techniques such as machine learning   constraint-based   and hybrid approaches . the survey  presents a taxonomy of these approaches.
　third  input data: the previous works assume their input as either relational or structured schemas. those schemas are designed internally for developers. as a consequence  the attributes of the schemas may be named in a highly inconsistent manner  imposing many difficulties in schema matching. in contrast  our work  section 1  focuses on matching query interfaces of deep web sources. these interfaces are designed for end users and are likely more meaningful and consistent. thus  we observed this distinguishing characteristic of  converging vocabulary  in our deep web studies  section 1   which motivated our statistical approach.
　fourth  focuses: the previous works focus on different aspects of schema matching. to begin with  for name matching  some focus on simple 1 correspondence  while others complex m:n. further  some works also consider structure matching. this paper targets at synonym discovery to support simple attribute correspondence  which seems sufficient for integrating web interfaces. however  we believe our general framework can deal with a wider class of schema matching problems  section 1 .
　the concept of generative models  as we intend to hypothesize and discover  has been applied in many different contexts. in particular  information retrieval  as well as model-based clustering  both assume an underlying generative model for a collection of objects. similar to us  their
domainsourcesall attributesnon-rarebooks11movies11music records11automobiles11       figure 1: statistics of sources studied. tasks also involve estimating the parameters to construct the right model. however  our search space is significantly larger because we need to select a unified model among multiple model candidates.
1.	motivation: the deep web
　in the last couple of years  the web has been rapidly  deepened  with the prevalence of databases online: a significant amount of information is now hidden on the  deep  web  behind the query forms of searchable databases. such information cannot be accessed directly through static url links; they are only available as responses to dynamic queries submitted through the query interface of a database.
　with massive sources  the deep web is clearly an important frontier for data integration. for any such attempts  e.g.  mediating a specific domain  say  automobiles   it is essential to integrate these query interfaces  since data must be retrieved with queries. this paper focuses on matching the schema aspect of query interfaces by discovering synonym attributes across different sources.
　this  wild  frontier of the deep web is characterized by its unprecedented scale. as a challenge: we often need to match large numbers of sources. as an opportunity: ample sources are usually available to form a useful  context  of matching. intuitively  by holistically unifying many sources in the same domain  our statistical approach intends to leverage the opportunity while addressing the challenge.
1	deep web observations
　to understand their characteristics  we performed informal study of sources on the deep web. from web directories  we drew sources in each of the four domains: books  music records  movies  and automobiles. in particular  we collected all of invisibleweb.com's sources  in these 1 domains  and most of yahoo.com's without any bias  until reaching about 1 sources in each domain  as figure 1 summarizes.
　on one hand  we observe proliferating sources: as the web scales  many data sources exist to provide structured information in the same domains  as figure 1 shows. while many web directories such as invisibleweb.com already list impressive numbers of online sources by manual compilation  there are certainly much more sources out there: by using overlap analysis  a july 1 survey  estimated 1  search cites  and 1 billion hidden pages in the deep web. our survey  in december 1 found 1 deep web sources by exploiting the random ip-sampling approach. as the web continues to expand  it will house virtually unlimited numbers of sources in interesting domains.
　on the other hand  we also observe converging vocabularies: the aggregate schema vocabulary of sources in the same domain tends to converge at relatively small size. figure 1 summarizes  in the middle column  the sizes of the entire vocabularies of all attributes used in any sources  which are about 1 for each domain. figure 1 a  further analyzes the growth of vocabularies as sources increase in numbers. the curves clearly indicate the convergence of vocabularies. for instance  for book domain  1%  1  attributes are observed at 1th sources  and 1%  1  at 1th. since the vocabulary growth rates  i.e.  the slopes of these curves  decrease rapidly  as sources proliferate  their vocabularies will tend to stabilize. note that the sources are sorted in the same order as they were collected without any bias.
　in fact  the vocabularies will converge more rapidly  if we exclude  rare  attributes. to quantify  let the frequency of an attribute be the number of sources in which it occurs. figure 1 b  orders these frequencies for the book domain over their ranks  with attributes detailed in figure 1. it is interesting but perhaps not surprising to observe that the distribution obeys the zipf's law: the frequencies are inversely proportional to their ranks. many low-ranked attributes thus rarely occur; figure 1 b  shows only the top 1 attributes  which account for 1% or 1 of all the attribute occurrences ; most others occur only once. in practice  these rare attributes are likely unimportant in matching since their rareness indicates that very few other sources will find them useful. with such rare attributes  say  below 1% frequencies  excluded  the  useful  vocabularies are much smaller: about 1 attributes per domain  figure 1 .
　note that  while vocabularies tend to converge  schema heterogeneity still persists. that is  although web query interfaces tend to share attributes  they are not universally unified- thus creating the real challenge of schema matching. in particular  among the top  popular  attributes for books in figure 1 b - how many different attributes are  synonyms  for the same concepts  we found 1  {author  last name  first name}  {subject  category}  out of 1  or a significant 1%. we observed similar levels of heterogeneity in other domains as well  see figure 1 .
1	toward hidden model discovery
　these observations lead us to hypothesize the existence of a hidden schema model that probabilistically generates  from a finite vocabulary  the schemas we observed. intuitively  such a model gives the  structure  of the vocabulary to constrain how instances can be generated. we believe this hypothesis reasonable  since it naturally explains our observations in section 1.
example 1  hidden model : referring to figure 1  right   the example model structures vocabulary {author  subject  name  ，，，} as m = { author  writer  name    subject  category   ，，，}. under m  some schemas can be generated  and thus observed   while others cannot. in particular  schema i1 = {author  name  title  ，，，} is unlikely  since it contains redundant attributes  i.e.  author and name  for the same concept  but i1 = {title  author  subject  ，，，} is possible.
　the hypothesis sheds new light on a different way for coping with schema matching: if a hidden model does exist  its discovery would reveal the vocabulary structure  which will in principle answer  any  schema matching questions.  as an analogy  an english dictionary can semantically relate all english words  subsuming the need for their pairwise correspondence.  as figure 1 contrasts  such model-level unification of all attributes in the same domain will subsume their pairwise correspondence  as used in traditional schema matching . we thus propose a new paradigm: statistical schema matching by hidden model discovery.
1.	statistical schema matching:
the mgs framework
	number of sources	ti au is kwpusu ln fmpr cg fn pd
 a  vocabulary growth over proliferating sources.	 b  frequencies over ranks of attributes. figure 1: analyzing schema vocabularies of deep web sources.　as just motivated  we view schema matching as a quest for an underlying model generating the input schemas. that is  our probabilistic approach seeks to treat the schemas as being generated by a random process following a specific distribution. our goal is thus  given the input schemas as  observations   to reconstruct the hidden generative distribution. to emphasize the statistic nature  we refer to this general paradigm as statistical schema matching.
　we believe the statistical approach has several advantages over traditional schema matching: first  scalability: by unifying large number of input schemas holistically rather than matching attributes pairwise  it addresses the scale of matching required in the new frontier of networked databases  such as our motivating goal of the deep web.
　second  solvability: in fact  the large scale can itself be a crucial leverage to make schema matching more solvable- in particular  it enables a principled and effective statistical approach  by taking advantages of sufficient samples. intuitively  we are building upon the  peer context  among schemas. our approach showed good accuracy with relatively small number of sources  section 1 . being statisticsbased  our approach will benefit from the scale: the accuracy will  scale  with the number of sources.
　third  generality: we believe such hidden model discovery can generally deal with many different aspects of schema matching  and present an abstract framework that can be specialized. except synonym discovery  which we will focus on   there are certainly other important questions related to schema matching  such as: what concepts are popular   such popularity can guide the design of a  mediator view  over sources  by exporting commonly supported concepts . how are attributes frequently associated   an interactive mediator query interface may  following author  prompt users with title  or following make with model. 
　to realize this statistical approach  we propose a general framework  mgs  consisting of hypothesis modeling  generation  and selection. we believe the mgs framework

is important in its own rights: in principle  by applicationspecific hypothesis modeling  mgs can be applied to find models addressing different  target questions.  we next present the abstract mgs framework by explaining its three essential steps below. section 1 will develop mgssd as a concrete specialization for finding synonyms.
1.hypotheses modeling: to guide the seeking of a hypothetical model  or a hypothesis  we start by defining the general structure of such models. such modeling should essentially capture specific target questions. for instance  if finding synonyms is the target  a model should explicitly express the grouping of  synonyms.  such modeling will also specify a generative behavior of how schemas can be generated.
such behavior is mainly probabilistic  e.g.  attributes will be drawn randomly by their  popularity    although it can also partially be deterministic  e.g.  no synonyms can be selected together . effectively  the model forms a statistical distribution  which generates a particular schema with some instantiation probability.
1.hypotheses generation: we then enumerate concrete hypotheses  in the specified abstract model  that are consistent with the observed schemas  with non-zero probabilities . note that  even with a parameterized structure  there will be a large space of candidate hypotheses to search  for a vocabulary of reasonable size. this generation step helps to focus the search to only those promising hypotheses that are likely to generate the observed schemas.
1.hypotheses selection: finally  we select hypotheses that are consistent with the observed schemas with sufficient statistical significance. there are various statistical devices for such hypothesis testing . for instance  we use χ1 testing in our mgssd algorithm  section 1 .
　in summary  we propose mgs as a general framework for the hidden model discovery problem: given a set of schemas i as observations  hypothesize and select the schema models with sufficient statistical consistency as the generative distributions of i. we next specialize the abstract framework for synonym discovery.
1.	synonym attributes discovery
　finding corresponding attributes is a central problem for schema matching; in this paper  we pursue this problem as synonym discovery. the challenge is to find  typically without semantics understanding  the synonyms among the input attributes. that is  across different schemas  some attributes  e.g.  author and name  or subject and category  are synonyms for the same concepts  e.g.  for the  author  and  subject  concepts respectively . as section 1 motivated  we focus on matching query interfaces for sources in the same domain on the deep web. thus  given such schemas  our goal is to discover all the synonym attributes.
　guided by the general mgs framework  we develop algorithm mgssd  figure 1   specifically for synonym attributes discovery as the target question. mgssd first defines the hypothetical model structure for capturing synonym attributes  section 1   generates the model candidates with non-zero probabilities  section 1   and selects the sufficiently consistent ones  section 1 . beyond these essential steps  we develop techniques for coping with several real-world issues that complicate our statistical approach  section 1 . finally  we put all the components together to present the complete algorithm  section 1 .
1	hypothesis modeling
　following mgs  we first define the structure of the underlying model. specifically  we aim at answering the target question of synonym attributes for web interfaces.  incidentally  our model can also capture the target question of concept popularity.  we view a query interface as a  flat  schema  or a set of attributes; e.g.  amazon.com has a schema {title  author  ，，，}. this simple view is sufficient for our purpose of synonym discovery. in particular  we do not concern complex matching  e.g.  author as last and first name   which itself is another interesting target question.
　to reasonably define a simple model  we make several assumptions of how our schemas are generated.  imagine a human web developer generates such web interfaces to bring databases online.  first  concept mutual-independence: a query interface contains several different concepts  e.g.   author  or  subject  . we assume that  in generating a schema  which may not contain all concepts   different concepts are selected independently.
　second  synonym mutual-exclusion: when multiple synonyms exist for a concept  e.g.  author and name   we assume that  in generating a schema  no two synonyms will both be selected. such duplicated selections will create redundancy and perhaps confusion; our case studies  of real sources; section 1  in fact have found no such schemas. as section 1 will discuss  this mutual exclusion enables significant pruning of the hypothetical model space.
　third  non-overlapping concepts: we assume that different concepts do not overlap in semantics  i.e.  no distinct concepts will share attributes. this assumption holds in most cases  when synonyms in the same concept are fully equivalent: e.g.  concepts {author  name} and {subject  category} do not overlap. thus this assumption says that all concepts will form an equivalence partition of the vocabulary set. however  as our case studies observed  section 1   sometimes an attribute can be a non-equivalent synonym to others  and thus participate in distinct concepts- e.g.  concepts {author  last name} and {author  first name}  where author corresponds to last name and first name in different  senses.  this assumption excludes such cases: instead of complicating simple synonym equivalence  such cases can be more systematically treated  by first grouping attributes  last name  first name   such grouping can itself be another target question; see section 1  and then finding equivalent synonym {author   last name  first name }  see section 1 .
1.1	model structure
　based on our assumptions  we define a simple model for capturing synonym attributes. essentially  a model describes how to generate schemas from a vocabulary of attributes. figure 1 visualizes mb  an example for book sources  as a two-level tree  for vocabulary vb = {author  title  isbn  subject  category}. to express synonyms  our model partitions all attributes into concepts  or equivalent classes  by the non-overlapping concepts assumption : e.g.  c1: {author}  ，，，  c1: {subject  category} in mb. the model will generate schemas by  first  independently selecting each concept ci with probability αi  by concept mutual-independence . for any selected concept  the model will then choose exactly one of its member attributes aj with probability βj  by synonym mutual-exclusion . the model thus generates a schema with all the chosen attributes.
definition 1: a schema model m is a 1-tuple  v c pc pa :

figure 1: an example of schema model mb.
the vocabulary v is a set of attributes {a1  ，，，  an}. the concept partition c is a set of concepts {c1  ，，，  cm} that partition v  i.e.  v = ci and ci ” ck =   . pc is the concept probability function  which determines the probability αi for including concept ci in schema generation. pa is the attribute probability function  which determines the probability βj for selecting attribute aj  once its concept is included. for every concept ci: aj（ci βj = 1.
　notationally  we will write a model by parenthesizing attributes in concepts with probability annotations  e.g.:  figure 1  mb = { author: β1 : α1   title: β1 : α1   isbn: β1 : α1   subject: β1  category: β1 : α1}. when probabilities are not critical in the context  we will simply write mb =
{ author    title    isbn    subject  category } .
1.1	schema generation and observations
　we now discuss how a model m will generate schemas. by definition 1  m will simply decide  for each concept ci  if ci is included  and if so  select one attribute aj to represent ci. this process will generate a schema as a set of attributes.
example 1: for mb in figure 1: possible schemas  with non-zero probabilities  from mb include: i1 = {author  title  subject  isbn} and i1 = {title  category  isbn}.
　note that a model m  by definition  represents a generative distribution  giving probabilities for any schema that can be generated. we now formalize such probabilities. first  to generate a schema  m selects concepts to include: by definition 1  a concept ci will appear with probability
pr ci|m  = αi or otherwise pr  ci|m  = 1   αi.
　next  we consider the probability of picking some attribute: by definition 1  the probability of selecting an individual attribute aj in schema generation from m is:

　how about selecting a set of attributes a1  a1 ..  am from m in any schema  definition 1 implies this probability as below  where the first condition represents synonym mutualexclusion and the other concept mutual-independence.

　putting together  we can derive the probability that m will generate some schema i  denoted by pr i|m . definition 1 below formalizes this instantiation probability. specifically  pr i|m  is the probability of used attributes times the probability of unselected concepts.
definition 1: for model m =  v c pc pa   the instantiation probability of a schema i = {a1 ... am} is pr i|m  =
pr a1 a1 .. am|m  〜  aj aj（/ci pr  ci|m . we say i can be instantiated from model m if pr i|m    1.
example 1: continuing example 1: we have pr i1|mb  =
α1 〜α1 〜α1 〜α1 〜β1  pr i1|mb  =  1 α1 〜α1 〜α1 〜 α1 〜 β1  where  1   α1  is the probability that the concept c1 is not used. however  for i1 = {author  isbn  subject  category}  we have pr i1|mb  = 1  since subject and category both belong to c1. thus i1 and i1 can be instantiated from mb  but i1 cannot.
　our approach seeks to discover the hidden model from many schemas observed  as input . therefore  we will take a set of schemas i  e.g.  the web sources summarized in figure 1   our input  as schema observations. to emphasize that in our input we may observe the same schema several times  we write i as a set of 1-tuple hii bii. each hii bii denotes the number of occurrences bi for each schema ii.
　to discover the hidden model  it is essential to answer: given model m  how likely will m generate the schemas in i   or  how likely can we observe i  if m is the hidden model   it follows definition 1 that this probability is pr i|m  = pr ii|m bi. note that  if pr i|m  = 1  it is impossible to observe i under m. therefore  we say model m is consistent with observations i  if pr i|m    1. thus  the hypothesis generation finds these  consistent models  as candidate hidden models  section 1 .
example 1: continuing example 1: we may have observations i = {hi1i  hi1i}  i.e.  i1 times and i1 times. thus  pr i|mb  = pr i1|mb 1〜pr i1|mb 1. note mb is consistent with i  since pr i1|mb  and pr i1|mb  are both non-zero  example 1 .
1	hypothesis generation
　guided by the second step of the mgs framework  we now generate candidate models that are likely to be sufficiently consistent  which section 1 will determine  with the input observations i. it is clear that any candidate m has to be at least consistent with i  i.e.  pr i|m    1  so that i is at least possible under m  section 1 . this section focuses on constructing such models.
　intuitively  we want to reconstruct m from our given observations i. using a statistical approach  we assume the observations are unbiased and sufficient. first  by the unbiased assumption  we will observe  or collect  a schema i with a frequency in i proportional to how likely i will be generated under m  i.e.  pr i|m .  say  we will not collect only schemas that contain author - that would be biased.  second  by the sufficient assumption  our observations will be large enough  so that every possible schema is represented in i. we use these assumptions to estimate the probability parameters  pa and pc  of a candidate model. in practice  the sufficient assumption is likely not to be satisfied; we discuss techniques for dealing with  the real world  in section 1.
　our goal in hypothesis generation is  given i  to construct models m =  v c pc pa  so that pr i|m    1. to begin with  we determine v: by our above assumptions  v = ii  since every possible schema occurs in i  and so does every attribute in v. on the other hand  even if the observations are not perfect  for our purpose of matching  we do not care any  phantom  attributes that have not occurred in any input source. thus  our model will capture only attributes that are used by at least one schema  in i .
　next  having determined v  we complete the model  v  c  pc  pa  by constructing first the concept partition c  section 1.1   and then the probabilities pc pa  section 1.1 .
1.1	building concept partitions
　given the vocabulary set v  we first construct a concept partition c for a candidate model. by definition 1  c is a partition of v. it is clear that  given v  there can easily be a large number of possible partitions. the number of partitions for an n-set is called a bell number b n   which has an exponential generating function and satisfies recursive relation  a vocabulary with  say  1 attributes will thus have 1 possible concept partitions  and as many possible models .
　to cope with the large space  it is thus critical to focus on only concept partitions that can lead to consistent models  m  such that pr i|m    1 . these consistent models form the hypothesis space with respect to i. our case studies show that the  consistent  condition can prune the search space to a very small number of models. for instance  in the book domain  we only have 1 models left in the hypothesis space with 1 attributes. to construct the hypothesis space  a naive approach that constructs and test every hypothesis will not work  due to the large number of possible concept partitions  as just discussed .
　however  not all concept partitions are useful for constructing a consistent model: it is important to note that  not every model  with arbitrary concept partitions  can generate a schema observed. in particular  as example 1 showed  i1 cannot be observed under mb  or pr i1|mb  = 1  since subject and category are both synonyms in c1  figure 1 . thus  if i1 is in i as part of our input schema  we will not consider mb  since it will be inconsistent with i  i.e. 
pr i|mb  = 1 as pr i1|mb  = 1.
　we can easily generalize this idea to focus on models that will not  contradict  with any observed schema i. in such models  none of the concepts will contain attributes aj and ak that are used by i.  in example 1  mb is not good for i1  since mb contains c1 with attributes subject and category both from i1.  that is  we will construct consistent models by using only consistent concepts  which do not contain any co-occurring attributes from any schema in i. property 1 formalizes this idea.
property 1: given observations i with vocabulary v  let c = {c1  ，，，  cm} be a concept partition for vocabulary v. any model m constructed from c will be inconsistent with i  or pr i|m  = 1  if for some attributes aj and ak  both of the following hold:
1.   schema i （ i  such that aj （ i and ak （ i.
1.   concept ci （ c  such that aj （ ci and ak （ ci.
　based on property 1  we use a two-step process to build the hypothesis space  of consistent models  using consistent concepts as building blocks.  for instance  mb in figure 1
　is built upon concepts c1  ，，，  c1.  step 1  consistentconceptsconstruction  will first find all consistent concept  and step 1  buildhypothesisspace  will then build consistent models accordingly. these two procedures are used to build the initial hypothesis space in algorithm mgssd.
　in step 1  we can translate the problem of finding consistent concepts into finding all cliques in an attribute  cooccurrence graph  . specifically  we construct a concept network from our observations i: in this graph  a node represents an attribute  and an edge exists between attributes aj and ak if and only if they do not co-occur in any schema i in i. thus  non-cooccurring attributes will be connected

figure 1: an example concept network.
with an edge- precisely such attributes will form consistent concepts. however  a concept can be of any number of attributes. therefore  we look for cliques for any size in the graph to construct consistent concepts.
example 1: consider observations i in example 1. from i  we can derive its concept network in figure 1. in particular  author and title do not have an edge because they co-occur in i1. author and category have an edge since they do not co-occur in any schema.
　further  what can be consistent concepts  there are 1 cliques in figure 1: {author}  {title}  {subject}  {category}  {isbn}  {author  category}  and {subject  category}. any clique represents a cluster of non-cooccurring attributes  and therefore is a consistent concept  by property 1 . in particular  some of these concepts  such as {author} and {subject  category}  are part of mb  which is consistent with i  as example 1 explained .
　in step 1  we use the consistent concepts just obtained as the building blocks for constructing consistent models. since all the concepts in a model partition its vocabulary set v  this step is essentially a classic set cover problem   with the covering subsets being non-overlapping. that is  given some subsets  the consistent concepts  of set v  we want to select some non-overlapping subsets to cover v. below we illustrate the result of constructing all the consistent models as the hypothesis space  which concludes our hypothesis generation step in this section.
example 1: given the consistent concepts in example 1  we can construct a consistent model m1 = { author    title    isbn    subject    category }  since the five concepts partition the vocabulary. we can find all the other consistent models: m1 = { author    title    isbn    subject  category } and m1 = { author  category    title    isbn    subject }. the hypothesis space is therefore {m1  m1  m1}.
1.1	building probability functions
　we have generated all the consistent models  which form the hypothesis space. however  these models are still incomplete: as a 1-tuple  v c pc pa   m has yet to determine the probability functions pc and pa  although v and c are specified. recall our ultimate goal is to discover those hidden models that are sufficiently consistent with input i. so far  for each consistent model m  by building upon only consistent concepts  we guarantee that pr i|m  is not necessarily zero. therefore  there exist pc and pa assignments for m  such that pr i|m    1.
　to complete each of these consistent models  we still need to specify pc and pa- clearly these probabilities should further maximize pr i|m . the reason is that with the assumptions of unbiased and sufficient input data  the values of pc and pa must be the ones that make the model the most consistent with the data. the  consistency  is reflected as the instantiation probability. so the most consistent model is corresponding to the model with the highest probability. thus  we have an optimization problem to find
	max pr i|m v c pc pa   	 1 
　　　　　　　　　pc pa which is essentially the maximum likelihood estimation problem  for given v and c.
example 1: continue example 1  where we showed m1 as one of the consistent models. to completely specify m1  we need to determine pc and pa to maximize pr i|m1   for i given in example 1 .
　as example 1 derives  note m1 and mb are the same model : 
. we apply maximum
likelihood estimation to select those α's and β's that can maximize pr i|m1 . the result is α1 = 1  α1 = 1 α1 = 1 α1 = 1 β1 = 1  and β1 = 1.
　in maximum likelihood estimation of functions pc and pa  we are effectively estimating parameters αi and βj  definition 1 . since concepts are independently selected  the concept mutual independence assumption of section 1   each αi can be estimated independently. we can also derive the solution for βj based on   since βj in a concept ci form a multinomial distribution. therefore  for any schema model  equation 1 has the closed-form solutions:

where oj is the frequency of attribute aj in observations i  i.e.  the number of schemas that contain aj   and |i| is the total number of schemas in i.
1	hypothesis selection
　guided by the third step of the mgs framework  we need to select sufficiently consistent hypotheses. after hypothesis generation  a hypothesis is a determined model  distribution  m =  v c pc pa . we propose to apply χ1 hypothesis testing to quantify how consistent the schema model is with the data. below we briefly introduce χ1 testing .
　suppose we have n independent observations  schemas  and in each observation  precisely one of r events  schemas with non-zero probability   i1 .. ir must happen  and their respective probabilities are p1 .. pr  with pose that p1 .. pr1 are the respective instantiation probabilities of the observed i1 .. ir with respect to the tested model m  with rj=1 pj1 = 1. we want to test the hypothesis p1 = p1 .. pr = pr1 by considering the statistic

where n is essentially |i|. it can be shown that d1 has asymptotically a χ1 distribution with r   1 degrees of freedom. again a test of the null hypothesis h : p1 = p1 .. pr = pr1 at the 1a% significance level is obtained by choosing a number b such that pr{d1   b} = a  where d1 has the χ1 distribution with r 1 degrees  and rejecting the hypothesis if a value of d1 greater than b is actually observed.
example 1: assume we have observations i = {hi1i  hi1i  hi1i}  with i1 = {author  subject}  i1 = {author  category}  and i1 = {subject}. our goal is to select the schema model at the significance level 1. the hypothesis generation step will output two hypotheses models : m1 = { author:1 :1  subject:1 category:1 :1} and m1 = { author:1 :1  subject:1 :1  category:1 :1}.
　we first consider m1. four schemas can be instantiated from m1: {subject}  {category}  {author  subject}  and
{author  category} with instantiation probabilities 1  1  1  and 1 respectively. thus  the computation of d1 is: 
 freedom degree 1. the χ1 distri-
bution table shows pr d1   1  = 1 at that freedom degree. since 1   1  we accept this hypothesis and consider it as a sufficiently consistent schema model.
　m1 is processed in the same way. eight schemas can be instantiated from m1: {}  {author}  {subject}  {category}  {author  subject}  {author  category}  {subject  category}  and
{author  subject  category} with probabilities 1  1 
1  1  1  1  1  and 1 respectively.
	1	.
then we have d  m1  = 1 with freedom degree 1. the χ1 distribution table shows pr d1   1  = 1. since 1   1  we should not select m1. therefore  hypothesis selection will select m1 as the schema model.
1	dealing with the real world
　we presented the overall process of algorithm mgssd  guided by the general principles of the mgs framework. further  there are often  real-world  issues on data observations that can compromise a statistical approach like ours. we find that  specifically for schema matching  the key challenge is the extremely  unbalanced  attribute distribution- we observed a zipf-like distribution  figure 1b  of attributes in our analysis of deep web sources  section 1 .
　challenges arise on the either end of this zipf distribution: on one hand  the head-ranked attributes  e.g.  ti and au in figure 1b  are extremely frequent  occurring in almost every schema: their occurrences tend to dominate any models and thus render these models indiscriminate under hypothesis testing  as section 1 developed . section 1.1 addresses dominating attributes with incremental consensus projection to isolate their effects.
　on the other hand  the tail-ranked attributes  e.g.  those not shown in figure 1b  are extremely rare  often occurring only once in some schema. their occurrences in observations  while rare  tend to  confuse  our statistical approach that asserts sufficient samples. in principle  a rare attribute a can appear in many concepts  by combining with other attributes in schema generation . also  as a being rare  these  a-schemas  are unlikely to be observed in i if it is not arbitrarily large- thus a will compromise a statistical approach for the lack of schemas. section 1.1 addresses rare attributes with attribute selection.
　together  this head-often  tail-rare attribute distribution will imply similar non-uniformness of schemas. thus  some schemas  with rare attributes  will be extremely rare too. our hypothesis testing essentially relies on estimating schema frequencies bj  section 1 . a rare schema i occurring only once in i tends to result in an overestimated frequency  or i needs to be arbitrarily large to justify i's only occurrence being sufficiently rare. section 1.1 addresses rare schemas by  smoothing. 
1.1	attribute selection
　rare attributes can confuse a statistical approach  with their lack of complete schemas in our observations i. such rare attributes will require virtually arbitrarily large i to give them sufficient context. that is  for these rare attributes  i is unlikely to be sufficient to statistically  explain  their properties- thus  our sufficient assumption  section 1  is unlikely to hold for such attributes. to draw valid statistical results  our approach is to systematically remove rare attributes- they are effectively  noises  in our setting.
　fortunately  these rare attributes may indeed be unimportant in schema matching. as section 1 explained  with zipf distribution  most rare attributes occur in only one source. thus  few other sources will find these attributes useful in query mediation or data exchange.  a mediator will not be likely to support such attributes; they are neither  mediatable  nor  exchangeable.   we believe it is naturally justified to remove rare noises in matching.
　we believe systematic attribute selection will be crucial for finding attribute subsets  for which robust statistical results can be achieved. we use a frequency-based pruning to select only frequent attributes into vocabulary v  section 1   as a procedure attributeselection in algorithm mgssd  figure 1 . specifically  we select an attribute aj if its observation frequency oj − f  where f is a given threshold set as 1% in our experiments. while this empirical value works well  section 1   further investigation is clearly necessary to automate threshold selection.
1.1	rare schema smoothing
　our observations i may contain infrequent schemas i that are presumably rare  as explained earlier. in particular  the χ1 testing  section 1  evaluates the difference between the estimated probabilities pr i|m  and the observed frequencies bj. for infrequent schemas  such difference will significantly distort the closeness of d1 to the χ1 distribution  which may influence the result of hypothesis selection.
example 1: suppose our observations i = {hi1i  hi1i  hi1i  hi1i}  with i1 = {author}  i1 = {last name}  i1 = {author  price}  and i1 = {price}. the hypothesis generation will find three hypotheses:
m1={ author:.1 last name:.1 :.1  price:1 :.1} m1={ author:1 :.1  last name:.1 price:.1 :.1} m1={ author:1 :.1  last name:1 :.1  price:1 :.1}.
the probabilities of i1 in m1  m1  and m1 are .1 
.1  and .1 respectively  which indicates i1 a rare schema. the χ1 testing will in fact reject all three models  at the significance level 1 .
　note that even the correct model m1 does not pass the test  simply because the early observation of the rare schema i1 results in an unreliable estimation of its probability. thus the rare schema disturbs the result.
　we cope with this problem by rare schema smoothing: instead of regarding each possible schema ij as an individual event  section 1   we will aggregate infrequent schemas into a conceptual event irare  whose probability is the sum of the probabilities of its members. such aggregation will smooth the overestimation in frequency counting  thus giving more reliable probability indication . we will then take χ1 testing on those frequent events plus irare.
　the key issue is then how to determine whether a schema ij is rare. our basis is its frequency in observations i  with size |i|   since the real probability is hidden to be discovered. we apply two criteria: 1  if not observed in i  ij is rare. 1  if observed  ij is rare if pr ij|m  〜 |i|   tsmooth  where tsmooth is a threshold  dynamically determined .
　we further develop adaptive thresholding of tsmooth in smoothing  as a procedure dynamicselection in algorithm mgssd  figure 1 : during hypothesis selection  section 1   we test the hypotheses with increasing thresholds until reaching at least one qualified hypothesis.  implicitly  we are applying our motivating assertion that there must exist a correct hidden model.  otherwise  it will output all the hypotheses  since they are not distinguishable  and at least one must be correct . empirically  we start the adaptive thresholding at tsmooth = 1 with a step size 1  and stop at 1  which works well  section 1 .
1.1	consensus projection
　straightforward testing cannot always distinguish models that share a dominating  consensus   which makes other differences insignificant . as explained earlier  the head-ranked attributes often dominate the testing and thus all these models may agree on the  structure  of these attributes- such consensus can be recognized  for early conclusion  and projected  for isolating dominating attributes . note that we assume a consensus must be correct  based on our motivating assertion that there exists at least a correct model.
example 1: suppose our observations i = {hi1i  hi1i  hi1i  hi1i} with i1 = {title}  i1 = {title  subject}  i1 = {title  subject  price}  and i1 = {title  category}. hypothesis generation will output three hypotheses:
m1={ title:1 :1  subject:.1 category:.1 :.1  price:1 :.1} m1={ title:1 :1  subject:1 :.1  category:.1 price:.1 :.1} m1={ title:1 :1  subject:1 :.1  category:1 :.1  price:1 :.1}.
　the χ1 hypothesis testing will reject all the three models at the significance level 1. in fact  their d1 values are not distinguishable- due to the highly frequent attribute title  which dominates the χ1 testing. however  it is clear that they all share a  consensus  on title.
　we thus propose consensus projection for recognizing and extracting consensuses  or shared concepts across models   so that hypothesis testing will better focus on models' distinctions. note that  the soundness of such projection  of consensus concepts  follows our concept mutual independence assumption  section 1 .
　specifically  consensus projection will extract the consensus from all the models in the hypothesis space. also it will extract the consensus attributes from the observed schemas and aggregate the projected schemas that become identical. the projection and aggregation will result in a new set of input schemas  which are used for the re-estimation of the parameters of the projected models. such projection can be repeated  since more consensuses will gradually emerge as the algorithm progresses. we can then discover the final models incrementally by projecting consensuses in progressive iterations. we thus structure algorithm mgssd as an iterative framework  as section 1 will discuss.
example 1: continuing example 1: we recognize concept  title  as the consensus. we thus perform consensus projection to extract  title  from all hypotheses and attribute title from all schemas in i.
so we have h  = π{subject category price} h   with
 subject:.1 category:.1 :1  price:1 :.1}
 subject:1 :.1  category:.1 price:.1 :.1}
 subject:1 :.1  category:1 :.1  price:1 :.1} and i  = π{subject category price} i  = {hi1  1i  hi1  1i  hi1  1i} with	 {subject  price}  and {category}.
i1  is empty after projection and thus removed. the new parameters of m  are estimated from i  with maximum likelihood estimation. the χ1 testing will select  and reject others  at the significance level 1.
1	putting it all together: algorithm mgssd
　for solving the target question of synonym attributes  algorithm mgssd  figure 1  consists of two phases: building initial hypothesis space and iteratively discovering the hidden models. the first phase selects the attributes as the vocabulary  section 1.1  and builds the hypothesis space  section 1.1 . the iterative process is based on consensus projection  section 1.1 : in each iteration  it projects the consensus  re-estimates the parameters  section 1.1   and tests the hypotheses  section 1  with the smoothing technique  section 1.1 .
example 1: consider the book domain sources listed in figure 1  the iterative process is illustrated in figure 1. in the first iteration  the consensus consists of concepts
 ti    is    kw    pr    fm   and  pd . the dynamicselection function will select four hypotheses as selectedh with tsmooth as 1  which are listed in the third column of the 1st iteration of figure 1. in the second iteration  the consensus consists of concept  pu . the dynamicselection function will select two hypotheses among the four in the 1st iteration. in the third iteration  the consensus is  su  cg  and dynamicselection cannot find any passing hypothesis with all the tsmooth's. therefore  the algorithm will stop and output two discovered schema models: m1 = { ti    is    kw    pr    fm    pd    pu    su  cg    au  ln    fn } and m1 = { ti    is    kw    pr    fm    pd    pu    su  cg    au  fn    ln }  where the parameters α's and β's are omitted.
　the time complexity of mgssd is exponential with respect to the number of attributes. for instance  the complexity of consistentconceptsconstruction is exponential since the clique problem is np-complete. similarly  the steps of buildhypothesisspace and dynamicselection are both exponential. since schema matching is typically done  offline   such computation time may still be tolerable in most situations. for instance  in our experimental setting  section 1   the running time is typically within one minute. further  our observation in section 1 indicates that in practice the computation is likely to scale to many sources: even with more sources  their aggregate vocabulary tends to converge- the growth of attributes and thus the corresponding computation cost are likely to stop at some point. nevertheless  it is certainly a real issue to explore more efficient algorithms  as section 1 discusses.
1.	case studies
　to evaluate the mgssd framework  we test it with four domains of sources on the deep web. we design two suites of metrics to quantify the accuracy of both the model itself and its ability to answer the target questions. the experimental results show remarkable accuracy for both metrics.
1	experiment setup
kthconsensushypotheses pass kth iterationtsmooth1st ti   is   kw    pr   fm   pd { au:1 ln:1 :1  pu:1 :1  su:1 :1  cg:1 :1  fn:1 :1}1{ au:1 ln:1 :1  pu:1 :1  su:1 cg:1 :1  fn:1 :1}{ au:1 fn:1 :1  pu:1 :1  su:1 :1  cg:1 :1  ln:1 :1}{ au:1 fn:1 :1  pu:1 :1  su:1 cg:1 :1  ln:1 :1}1nd pu { au:1 ln:1 :1  su:1 cg:1 :1  fn:1 :1}1{ au:1 fn:1 :1  su:1 cg:1 :1  ln:1 :1}1rd su cg { au:1 ln:1 :1  fn:1 :1}1{ au:1 fn:1 :1  ln:1 :1}1th figure 1: process of discovering schema model for the book domain.
domainvocabulary  abbreviation bookstitle ti  author au  isbn is  keyword kw  publisher pu  subject su  last name ln   format fm  category cg  price pr  first name fn  publication date pd moviestitle ti  director dr  actor ac  genre gn  format fm  category cg   keyword kw  rating rt  price pr  studio sd  star st  artist at music recordsartist at  song sg  album ab  title ti  label lb  format fm   genre gn  soundtrack sr  catalog # ct  keyword kw  band bn automobilesmake mk  model md  price pr  year yr  type tp  zip code zc   mileage ml  style sy  color cl  state st  category cg figure 1: vocabularies of the four domains.　we collect over 1 sources over four domains as stated in section 1. for each source  we manually extracted attributes from its query interface and did some straightforward preprocessing to merge attributes of slight textual variations  e.g.  author's name and author . thus  we focus on discovering synonym attributes and consider such attribute extraction and preprocessing as independent tasks. in particular  attribute extraction  e.g.  extracting  author's name  from  please input author's name   can be automated with noun-phrase extraction tools  such as linkit require: schemaset i  significancelevel a
1: /* initial hypothesis generation: */
1: v = attributeselection 	ii 
1: c = consistentconceptsconstruction i 
1: h = buildhypothesisspace c 
1: /* iterative framework: */
1: while true do
1:	conattrs = attributes in the consensus of h
1:	if conattrs =   or v = conattrs then
1:	output the initial models of h
1:	else
1:	/* consensus projection */
1:	v = v   conattrs; i  = πv i ; h  = πv h 
1:	/* maximum likelihood estimation */ 1:	for each m in h  do
1:	estimate parameters α β of m using i  1:	end for
1:	/* hypothesis selection: χ1 testing+smoothing */
1:	selectedh = dynamicselection h  
1:	/* new hypothesis space for next iteration*/
1:	h = selectedh 1:	end if
1: end while
figure 1: algorithm mgssd.
 http://www.columbia.edu/cu/cria/sigtops/ . moreover  our preprocessing simply matched entities with obvious textual similarity  e.g.  book title and title   which could be done with more sophisticated techniques such as .
　in the experiments  we select the attributes using the approach proposed in section 1.1 with threshold f = 1%. the attributes passing that threshold are listed in figure 1. also  in the experiments we assume 1 as the significance level of χ1 hypothesis testing. in practice  the threshold and significance level can be specified by users.
1	metrics
　we propose two suites of metrics for different purposes. the first suite is generic since it measures how the hypothesized schema model is close to the correct schema model written by human experts. the second suite of metrics is specific in the sense that it measures how good the hypothesized schema model can answer the target questions.
first  we introduce the notion of correct schema model.
a correct schema model mc is a schema model where attributes are correctly partitioned into concepts. since it is difficult and unreliable  even for human experts  to specify the ideal probability parameters  we assign them using maximum likelihood estimation  which is consistent with the  unbias  and  sufficient  assumptions in section 1.
　the purpose of the first suite of metrics is to compare two models  or distributions . we view each distribution as a set of schemas  instantiated from that distribution   associated with a probability  or member frequency . thus  we adopt precision and recall to measure this  member frequency . we define ins m  as the set of all schemas that can be instantiated from m. precision is designed to measure the portion of the hypothesized set that is correct. in our case  the correct part is the intersection of ins mh  and ins mc   denoted by s. so the model precision is:
pr i
  
where i（ins m  pr i|m  = 1 for any model m. similarly  model recall measures the portion of mc that is contained in mh  which is rm mh mc  = i（s pr i|mc .
example 1: consider example 1  we can see that the correct schema model is actually m1 and thus both model precision and recall of m1 are 1. now consider m1  although it is rejected  we still can measure it as an example. example 1 has shown the schemas and instantiation probabilities of ins m1  and ins mc . so s contains four schemas: {subject}  {category}  {author  subject} and {author  category}. then we can compute the model precision and recall as pm m1 mc  = 1+1+1+1 = 1 and rm m1 mc  = 1 + 1 + 1 + 1 = 1.
domainoutput modelspmrmptrtmoviesmmovie1 = { ti   dr   fm   rt   pr   sd   kw   ac st   gn cg   at }111mmovie1 = { ti   dr   fm   rt   pr   sd   kw   ac st at   gn   cg }111mmovie1 = { ti   dr   fm   rt   pr   sd   kw   ac st at   gn cg }11music recordsmmusic1 = { sg   lb   fm   at bn   ab ti   gn   sr   kw   ct }11mmusic1 = { sg   lb   fm   at bn   ab ti   gn   sr   kw ct }1.1.1mmusic1 = { sg   lb   fm   at bn   ab ti   gn   sr kw   ct }1.1.1mmusic1 = { sg   lb   fm   at bn   ab ti   gn sr   kw   ct }1.1.1mmusic1 = { sg   lb   fm   at bn   ab ti   gn sr   kw ct }1.1.1automobilesmauto = { mk   md   pr   yr   sy tp cg   zc cl   st ml }1.1.1figure 1: experimental results for movies  music records and automobiles.　the second suite of metrics measures how the model is correct in answering the target questions. in our case  the target question is to ask for the synonyms of attributes. specifically  we imagine there is a  random querier  who will ask for the synonyms of each attribute according to the probability of that attribute. the model will answer each question by returning the set of synonyms of the queried attribute in that model. we define syn aj|m  as the set of synonyms of attribute aj in model m. to compare two synonym sets  precision and recall are again applied. given the correct model mc and a hypothesized model mh  the precision and recall of the synonym sets of attribute aj are:
paj mh mc  = |syn a|jsyn|mc  a”jsyn|mh a |j|mh | and
raj mh mc  = |syn aj|syn|mc  a”jsyn|mc a |j|mh |.
　for this  random querier   more frequently observed attributes have higher probabilities to be asked. thus we compute the weighted average of all the paj's and raj's as the target precision and target recall. the weight is assigned as a normalized probability of the attributes. that is  for attribute aj  the weight
 according to the formulae in sec-
tion 1.1 . therefore  target precision and target recall of mh with respect to mc are defined as:
	pt mh mc  =	aj（vh	oojk paj mh mc 
　　　rt mh mc  = aj（vc oojk raj mh mc   where vh and vc are the vocabulary sets of mh and mc.
example 1: still consider example 1  the target precision and recall of m1 are both 1 since m1 is the correct schema model. for m1  we have pauthor m1 mc  = 1 and rauthor m1 mc  = 1 since author is correctly partitioned in m1. however  for subject  we have syn subject |mc  = {category} and syn subject |m1  =  . therefore psubject m1 m1  = 1 and rsubject m1 m1  = 1. we do the same measurement on category and then compute the weighted average. the occurrences of author  subject  and category are 1  1  and 1 respectively. thus  the results are and

1	experimental results
　we report and discuss the experimental results for the book domain. for other domains  we only show the input and output. figure 1 lists all the selected attributes. the result shows two sufficiently consistent models: mbook1 =
{ ti:1 :.1   is:1 :.1   kw:1 :.1   pr:1 :.1   fm:1 :.1   pd:1 :.1   pu:1 :.1   su:.1  cg:.1 :.1   au:.1  ln:.1 :.1   fn:1 :.1} and mbook1 = { ti:1 :.1   is:1  :.1   kw:1 :.1   pr:1 :.1   fm:1 :.1   pd:1 :.1   pu:1 :.1   su:.1  cg:.1 :.1   au:.1  fn:.1 :.1   ln:1 :.1}.
　the result successfully identifies the matchings  au  ln    au  fn  and  su  cg . without attribute grouping techniques  section 1  to merge last name and first name  human experts can only consider that mbook1 and mbook1 both are correct schema models and thus give 1 precision and 1 recall in both model and target metrics. as stated in section 1  attribute grouping is a different target question. assume another specialized framework mgsag have done this task  then the result will be mbook = { ti:1 :.1   is:1 :.1   kw:1 :.1   pr:1 :.1   fm:1 :.1   pd:1 :.1   pu:1 :.1   su:.1  cg:.1 :.1   au:.1   ln fn :.1 :.1 }  which is perfectly accurate in the sense of  equivalent synonym.  in addition  the parameters in the results can be used to answer the question of concept popularity  section 1   which indicates that this model is not limited to synonym discovery. for the other three domains: movies  music records  and automobiles  their output is summarized in figure 1. the results show that our approach can identify most concepts correctly. in movies and music records  the correct schema model is returned in our output models  which are mmovie1 and mmusic1 respectively. however  for automobiles  we did not get the correct model. the incorrect matchings are due to the small number of observations we have. if we observe more sources  we should be able to observe some cooccurrences to remove false synonyms. for example  in the automobile domain  the incorrect matchings  zc  cl  and  st  ml  are because we did not observe the co-occurrences of zip code and color  state and mileage. with larger observation size  we believe the result will be better.
　the measurement results in figure 1 show that we do need two suites of metrics because they evaluate different aspects. for instance  the model recall of mmovie1 = 1 means mmovie1 can generate all correct instances  while the target precision of mmovie1 = 1 denotes the synonyms answered by mmovie1 are all correct ones.
　finally  although in principle the time complexity of mgssd is exponential  in our experiments  the overall execution time is within one minute  on a pentium-iii 1ghz with 1mb memory . therefore  we believe that in practice the computation cost is likely to be acceptable for schemamatching as an off-line process.
1.	discussion
　in our study for statistical schema matching  we also observed some open issues that warrant further research. first  as inherent in statistical methods  our approach handles only frequent attributes with sufficient observations.  however  a more principled attribute selection may help to include rare attributes in popular concepts; see later.  is it still useful then  section 1 observed that these frequent attributes  while small in number  dominate 1% occurrences  because of the zipf-like distribution . thus  in the sense of the classic 1 rule  our technique is appealing in coping with at least the  vital few   although it leaves out the  trivial many   that are likely unimportant . further  as section 1 mentioned  the  solvability  will scale; i.e.  more attributes can be handled with more sources. finally  we stress that  for large-scale integration of sources for numerous domains on the web  automatic techniques like ours  even only for frequent attributes in each domain  will be imperative  as manual matching will not scale.
　second  can this approach deal with more expressive models for harder target questions  e.g.  homonyms  hypernyms  or complex m:n matchings  we are currently studying hypernyms and complex matchings  for which we found the framework promising. however  there is a fundamental limitation: our approach  relying on only attribute syntactic names  but not semantics in data contents   cannot distinguish homonyms. for sources created in the same domains  homonyms may be unlikely; we indeed found none in our study of web query interfaces. for other scenarios when matching different domains  homonyms can be more significant  e.g.  title in books and jobs . we may thus incorporate other techniques that consider more semantics  see below .
　third  can we reduce the exponential complexity of algorithm mgssd  while such computation may be tolerable as schema matching is typically off-line and input attributes will likely converge  section 1   cost reduction is clearly necessary. we believe that  within the mgs framework  cost reduction is possible by interleaving the model generation and selection phases to greedily construct  rather than exhaustively search  the best models.  analogously  cost-based query optimization adopts  say  dynamic programming to speedup search in the plan space. 
　fourth  how can our framework integrate various techniques  for schema matching  we feel the statistical approach can benefit from complementary techniques that explore more  semantics   e.g.  data values for distinguishing homonyms . we believe the framework can integrate other techniques in a principled way  by  importing  their results as an a priori probabilistic structure for vocabulary v  currently structureless . thus  say  a linguistic approach may indicate that author and writer are likely synonyms  with a score 1. such vocabulary structure will then bias the statistical framework toward better results.
　fifth  does a hidden model always exist for a collection of schemas  our approach hypothesizes such models. for sources of the same domains  this hypothesis seems empirically appealing.  our recent study  clusters schemas into domains by such statistical models.  can we relax to more liberal notion of  domains   for instance  some sources only partially overlap in structure  e.g.  automobiles and car rental  - can they be coped with by a statistical approach  sixth  there are several  knobs  in mgssd that currently rely on thresholds. in particular  we use frequency thresholding  by 1%  for attribute selection  which might be too crude: it can prune rare attributes that are actually in frequent concepts  e.g.  for books  binding and media type are pruned  while their synonym format is among the top concepts . clearly  a more principled treatment will help in distinguishing true  outlier  attributes.
　finally  can we leverage massive sources to enhance the statistical approach  in principle  our approach relies on having sufficient observations. in practice  we have developed a suite of techniques  section 1  for alleviating insufficiency. think the other way: can we leverage the virtually unlimited supply of sources on the internet to help establish the required statistical context for schema matching  for instance  we may  automatically  collect many book sources  just to help in integrating similar ones.
1.	conclusion
　this paper explores statistical schema matching  by hypothesizing and discovering hidden models that unify input schemas. our experience indicates high promise for moving the traditional pairwise-attribute correspondence toward a new paradigm of holistic matching of massive sources. this approach is well suited for the new frontier of large-scale networked databases  such as our focus of the deep web. we propose a general statistical framework mgs  and further specialize it to develop algorithm mgssd for finding synonym attributes. our extensive case studies motivated our approach as well as validated its effectiveness. we discussed several open issues  and we are eager to see how the statistical paradigm and framework can be generally applied in other schema matching scenarios.
acknowledgments we are grateful to the illinois statistics office at university of illinois at urbana-champaign  for their discussions on developing the statistical techniques.
