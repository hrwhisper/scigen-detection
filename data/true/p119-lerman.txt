many web sites  especially those that dynamically generate html pages to display the results of a user's query  present information in the form of list or tables. current tools that allow applications to programmatically extract this information rely heavily on user input  often in the form of labeled extracted records. the sheer size and rate of growth of the web make any solution that relies primarily on user input is infeasible in the long term. fortunately  many web sites contain much explicit and implicit structure  both in layout and content  that we can exploit for the purpose of information extraction. this paper describes an approach to automatic extraction and segmentation of records from web tables. automatic methods do not require any user input  but rely solely on the layout and content of the web source. our approach relies on the common structure of many web sites  which present information as a list or a table  with a link in each entry leading to a detail page containing additional information about that item. we describe two algorithms that use redundancies in the content of table and detail pages to aid in information extraction. the first algorithm encodes additional information provided by detail pages as constraints and finds the segmentation by solving a
constraint satisfaction problem. the second algorithm uses probabilistic inference to find the record segmentation. we show how each approach can exploit the web site structure in a general  domain-independent manner  and we demonstrate the effectiveness of each algorithm on a set of twelve web sites.
1. introduction
the world wide web is a vast repository of information. the amount of data stored in electronic databases accessible to users through search forms and dynamically gener-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro¡êt or commercial advantage  and that copies bear this notice and the full citation on the ¡êrst page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci¡êc permission and/or a fee.
sigmod 1 june 1  1  paris  france.
copyright 1 acm 1-1/1 . . . $1.
ated web pages  the so-called hidden web   dwarfs the amount of information available on static web pages. unfortunately  most of this information is presented in a form accessible only to a human user  e.g.  list or tables that visually lay out relational data. although newer technologies  such as xml and the semantic web  address this problem directly  only a small fraction of the information on the web is semantically labeled. the overwhelming majority of the available data has to be accessed in other ways.
web wrappers are popular tools for efficiently extracting information from web pages. much of the research in this area over the last decade has been concerned with quick and robust construction of web wrappers   usually with the help of machine learning techniques. because even the most advanced of such systems learn correct wrappers from examples provided by the user  the focus recently has been on minimizing the number of examples the user has to label  e.g.  through active learning . still  even when user effort is significantly reduced  the amount and the rate of growth of information on the web will quickly overwhelm user resources. maintaining wrappers so that they continue to extract information correctly as web sites change  requires significant effort  although some progress has been made on automating this task . heuristic techniques that may work in one information domain are unlikely to work in another. a domain-independent  fully automatic solution that requires no user intervention is the holy grail of information extraction from the web. despite the inherent difficulty of the problem  there are general principles and algorithms that can be used to automatically extract data from structured web sites. in this paper  we present new novel techniques that are applicable to a broad range of hidden web sources.
extraction of records or tuples of data from lists or tables in html documents is of particular interest  as the majority of web sites that belong to the hidden web are presented in this manner. record extraction is required for a multitude of applications  including web data mining and questionanswering. the main challenge to automatic extraction of data from tables is the great variability in html table styles and layout. a naive approach based on using html  table  tags will not work. only a fraction of html tables are actually created with  table  tags  and these tags are also used to format multi-column text  images  and other non-table applications. the vast majority of html documents use non-standard tags to format tables  including text separators  such as ~  to separate fields and  br  to separate different items as well as fields. more sophisticated automatic approaches to table recognition and information extraction have been suggested which rely on the document object model  or regularities in html tags  1  1 . these approaches are not robust and will fail for some sites. it is our experience that the variability in html tags is often too great to rely on them for table recognition and information extraction.
fortunately  dynamically generated web pages contain much explicit and implicit structure  both in layout and content  that we can exploit for purposes of automatic information extraction. previous approaches have focused on exploiting structure within a page  1  1  1 ; here we use the web site's structure to improve information extraction.
web sites belonging to the hidden web have a surprisingly uniform structure. the entry point is an html form for the user to input her query. the result of the query is a list of items or a collection of records from a database. the results are usually displayed as a list or a table on an automatically generated page. we call such a results page the list page. each item or record often has a link to a detail page that contains additional information about that entry. detail pages are also generated automatically and populated with results of database queries. some of the item attributes are likely to appear on the list page as well as on the detail page.
as others have noted  there is important structure in the layout of the list page: some html tags are used to separate records  and other html tags or text symbols are used to separate columns. in addition to similarities in layout  we expect similarities in the content of data: data in the same column should be of the same type  name or phone number for instance. of course  the underlying structure of real world data may not be immediately clear: an item may have missing columns; column separators may be different  depending on whether the attribute value is present or missing  or attribute values may be formatted in different ways; a data field may have a lot of variability that we are not able to capture if we assume a uniform layout for each row.
in this paper we propose methods that allow us to efficiently segment data on the list page into records using information contained in detail pages. we describe two approaches: one formulates the task as a constraint satisfaction problem  csp  and the other uses a probabilistic inference approach. both techniques exploit the additional information provided by the overlap in the content between list and detail pages. the idea we are trying to leverage is the fact that each detail page represents a separate record. the constraint satisfaction-based technique encodes the relations between data found on the list and detail pages as logical constraints. solving a constraint satisfaction problem yields a record segmentation. in the probabilistic approach  the information provided by detail pages is used to learn parameters of a probabilistic model. record segmentation is the assignment of attributes to records that maximizes the likelihood of the data given the model. in addition to computing the record segmentation  the probabilistic approach produces a mapping of data to columns.
the approaches we describe are novel  in that unlike many other techniques  they rely on the content of web pages rather than their layout  i.e.  tags . the number of text strings on a typical web page is very small compared to the number of html tags; therefore  inference algorithms that rely on content will be much faster than the algorithms that use layout features. both of our approaches are fully automatic  domain independent and unsupervised  i.e.  they do not require any training or user-labeled data. we have validated both methods on 1 sites from diverse information domains  including white pages  property tax and corrections domains. despite the great variability in the appearance  presentation  and data across these sites  both approaches performed quite well.
1. related work
several researchers have addressed the problem of detecting tables in web and plain text documents and segmenting them into records.
1 table extraction from html documents
existing approaches to extracting table data from web documents can be classified as heuristic or machine learning. heuristic approaches to detecting tables and record boundaries in web documents include using the document object model  dom  and other features  to identify tables. domain-specific heuristic rules that rely on features such as percent signs and date/time formats have also been tried successfully .
machine-learning approaches learn a model of data from a set of labeled training examples using hand-selected features. borkar et al.  use multiple heuristic features  including domain-specific controlled vocabularies  to learn a hidden markov-based probabilistic model from a set of training examples. hurst  trained a naive bayes classifier  while wang et al.  describe a domain-independent classifier that uses non-text layout features  average number of columns/rows  average cell length and consistency  and content features  image  form  hyperlink  alphabetic  digit  others . the classifier achieves good performance after being trained on thousands of labeled examples. in a related work  wang et al.  1  1  optimizes whole page segmentation probability over spatial layout features  much as it is done in document image analysis  to find correct tables in web documents. cohen et al.  present another example of an approach that combines alternate representations  text  dom and non-text layout features  with a learning algorithm to improve the wrapper learning process. these methods require many training examples in order to learn a useful model of data.
clearly  the methods listed above suffer from being domainspecific or requiring user-labeled training examples. recently  several unsupervised learning approaches  which require no training examples  have been investigated. yoshida  automatically recognized table structures on the basis of probabilistic models where parameters are estimated using the expectation maximization algorithm. this approach was validated on two domains on the information extraction and integration task with 1% accuracy.
the roadrunner system  1  1  automatically extracts data from web sites by exploiting similarities in page layout. the premise behind the system is that many web pages are generated by a grammar  which can be inferred from example pages. thus  roadrunner can learn the table template and use it to automatically extract data from the web site. roadrunner's authors focus on a subclass of web pages that can be generated by union-free grammar and describe the algorithm to learn the grammar. the learning algorithm is exponential  and further simplifications are necessary to keep it computationally tractable. although the roadrunner system uses an elegant approach and method for automatic extraction of data  its applicability is limited  because union-free grammars do not allow for disjunctions  and disjunctions appear frequently in the grammar of web pages. disjunctions are necessary to represent alternative layout instructions often used by web sites for a same field. our approach  in contrast  is able to handle disjunctions.
chang & lui  present an algorithm based on pat trees for detecting repeated html tag sequences that represented rows of web tables. they then apply the sequences to automatically extract data from web search engine pages. although they show good performance in this domain  search engine pages are much simpler than html pages containing tables that are typically found on the web. we have tried a similar approach and found that it had limited utility when applied to most web pages. other recent layout-based automatic extraction algorithms  such as   have been shown to work well on detail pages  but cannot handle lists.
1 table extraction from plain text
automatic table extraction from plain text documents is a
line of research parallel to the work on html table extraction. there are differences between plain text and html tables that make the two fundamentally different problems. plain text documents use white space and new line for the purpose of formatting tables: new lines are used to separate records and white spaces are used to separate columns  among other purposes. record segmentation from plain text documents is  therefore  a much easier task. closely linking format and content in plain text documents also gives rise to new challenges. in plain text tables  a long attribute value that may not fit in a table cell will be broken between two lines  creating a non-locality in a text stream. an automatic algorithm will have to associate part of a string with another string that will appear arbitrarily later in the text stream. this problem does not usually arise in html documents. on the other hand  html tables vary widely in their layout and formatting conventions  making it difficult to rely on any set of features to be good row or column separators. although one may employ the same set of tools for extracting from html and plain text tables  specialized algorithms that address the conventions of each domain may outperform a general algorithm.
despite these differences  a similar variety of techniques has been used in table extraction from plain text and for html documents  from heuristic rules  to machine learning algorithms that learn a feature or a combination of features that predict a table and its elements  1  1 . pyrredy et al.  propose a unique approach that uses structural features  such as alignment of characters on a page  to identify the tables and its main elements. as we do  pinto et al.  capitalize on recent developments in probabilistic models  and examine their use in extracting tables from plain text documents and identifying their structure. the authors are interested in identifying header and data rows  not in segmenting data into individual records and attributes. their probabilistic models are trained on user-labeled examples using a feature set that includes white space  text and separator features. white spaces are used heavily to format tables in plain text documents. in html documents  on the other hand  white space is almost never used for this purpose. although their methods may  in principle  work for html pages  the authors have not applied them for this purpose. it is likely that they will need a wide variety of training examples to capture the range of variability in web documents. our approach  in contrast  requires no training data.
1. overview of the problem
in this section we give an overview of the problem of record extraction and segmentation using the structure of a web site to aid in extraction. as we discussed above  many web sites that present information contained in databases follow a de facto convention in displaying information to the users and allowing them to navigate it. this convention affects how the web site is organized  and gives us additional information we can leverage for information extraction. such web sites generate list and detail pages dynamically from templates and fill them with results of database queries. figure 1 shows example list and detail pages from the verizon superpages site. the superpages site allows customers to search over 1 million yellow page listings and a national white pages database by name  phone number or business type. as shown in the figure  the results returned for a search include the fields  name  address  city  state  zip and phone. here the text  more info  serves as a link to the detail page. note that list and detail pages present two views of the record. using automatic techniques  we can potentially combine the two views to get a more complete view of the record. for example  maps of the addresses are shown on the detail pages in figure 1  but absent from the list pages.
we envision an application where the user provides a pointer to the top-level page - index page or a form - and the system automatically navigates the site  retrieving all pages  classifying them as list and detail pages  and extracting structured data from these pages. we are already close to this vision . the current paper addresses the technical issues involved in the problem of structured information extraction from list pages.
1 page templates

figure 1: example list and detail pages from the superpages site  identifying information has been removedconsider a typical list page from a web site. as the server constructs the list page in response to a query  it generates a header containing the company logo  followed in most cases by an advertisement  then possibly a summary of the results  such as ''displaying 1 of 1 records.''  table header and footer  followed by some concluding remarks  such as a copyright information or navigation aids. in the to preserve confidentiality .
above example  the header includes results  1 matching listings  search again and the associated html. the footer includes advertisement and navigational links. we call this part of the page the page template. the page template of a list page contains data that is shared by all list pages and is invariant from page to page. a different page template is used to generate detail pages. as the server writes out records to the table  it uses a table template. this template contains layout information for the table  that is the html tags or ascii characters used to separate columns and rows  or format attribute values.
given two  or preferably more  example list pages from a site  we can derive the template used to generate these pages and use it to identify the table and extract data from it. the process starts with a set of list pages from a site and a set of detail pages obtained by following links from one of the list pages. the pages are tokenized - the text is split into individual words  or more accurately tokens  and html escape sequences are converted to ascii text. each token is assigned one or more syntactic types  1  1   based on the characters appearing in it. the three basic syntactic types we consider are: html  punctuation  and alphanumeric. in addition  the alphanumeric type can be either numeric or alphabetic  and the alphabetic can be capitalized  lowercased or allcaps. this gives us a total of eight  non-mutually exclusive  possible token types. a template finding algorithm  e.g.  one described in  1  1   is used to extract the main page template.
slots are sections of the page that are not part of the page template. if any of the tables on the pages contain more than two rows  the tags specifying the structure of the table will not be part of the page template  because they will appear more than once on that page. likewise  table data will also not be part of the page template  since it varies from page to page. therefore  the entire table  data plus separators  will be contained in a single slot. considering that tables usually contain a significant amount of data  we use a heuristic that the table will be found in the slot that contains the largest number of text tokens.
1 data extraction
next  we extract data from the table. we do this simply by extracting  from the slot we believe to contain the table  the contiguous sequences of tokens that do not contain separators. separators are html tags and special punctuation characters  any character that is not in the set  .   -  . practically speaking  we end up with all visible strings in the table. we call these sequences extracts  e = {e1 e1 ... en}. these are the attribute values that the records in the table contain  and possibly some extraneous data  such as  more info    send flowers  as shown in figure 1. our goal is to segment these extracts into individual records.
the csp and probabilistic approaches share the same basic premise: detail and list pages present two views of the same record and each detail page corresponds to a distinct record. the labels for detail page are: {r1 r1 ... rk}.
for each extract ei  we record all detail pages on which it was observed  di. 1 the extracts are checked in the order they appear in the text stream of the list page. if an extract appears in all the list pages or in all the detail pages  it is ignored: such extracts will not contribute useful information to the record segmentation task.
the methods presented below are appropriate for tables that are laid out horizontally  meaning that the records are on separate rows. a table can also be laid out vertically  with records appearing in different columns; fortunately  few web sites lay out their data in this way. in horizontally laid out tables  the order in which records appear in the text stream of the page is the same as the order in which they appear in the table. in other words  any attribute of the second record will appear in the stream after all the attributes of the first record and before any of the attributes of the third record have been observed.
table 1 is an example of a table of observations of extracts on detail pages from the superpages site  see figure 1 .1the columns correspond to extracts and they are displayed in the order they appear on the list page.
as can be seen from the table  the same extract can appear on multiple detail pages. in the superpages site  for example  several people may have the same name or the phone number. note that only the extracts or record attributes that appear in both the list and detail pages are used. there may be other potential attributes  but if they do not appear on any of the details pages  they will not be considered in the analysis. this works to our advantage by reducing the need to precisely identify the table slot.
1 record segmentation
extracts common to list and detail pages give us an additional source of information that we can use to segment data into individual records. an extract  attribute value  belongs to a record only if it appears on the detail page corresponding to that record. the same extract cannot be assigned to more than one record or more than once to the same record. thus  in the table above  we can use these rules to assign e1  e1  e1 and e1 to the first record  and e1  e1  e1 and e1 to the second record  as shown in table 1  even though e1 and e1 appear in both records. these rules can be easily encoded in both the csp and the probabilistic framework. solving this problem yields an assignment of data to records.
1 column extraction
the probabilistic model is more expressive than the csp. in addition to record segmentation  we can learn a model for predicting the column of an extract  based not only on its token type  but also on the neighboring columns. we can learn a probabilistic model of the data given the column label: i.e.  first attribute of the record is of alphabetic type  the second a numeric type  etc. the inference algorithm estimates parameters of the model from the observations of extracts on details pages. the parameters are then used to find a column assignment that maximizes the total probability of the observations given the model. the column labels will be l1 ... lk  we can find k by the longest potential sequence of ri in the data . to provide them with more semantically meaningful labels  we can use other automatic extraction techniques  such as those described in the roadrunner system .
1. a csp approach to record segmentation
csps are stated as logical expressions  or constraints  over a set of variables  each of which can take a value from a finite domain  boolean  integer  etc. . the case where the variables and logical formulas are boolean is known as boolean satisfiability  the most widely studied area of csp. in a pseudo-boolean representation  variables are 1  and the constraints can be inequalities. the csp problem consists of finding the value assignment for each variable such that all constraints are satisfied at the same time. when constraints are inequalities  the resulting problem is an optimization problem.
1 structure constraints
we encode the record segmentation problem into pseudoboolean representation and solve it using integer variable constraint optimization techniques. let xij be the assignment variable  such that xij = 1 when extract ei is assigned to record rj. and xij = 1 when the extract ei is not part of the record rj. the assignment of extracts to records will look something like the table in table 1. blank cells in table correspond to xij = 1. by examining this table  we see that there are two logical constraints it makes sense to impose on the assignment variables. first  there has to be exactly a single  1  in each column of the assignment table. this is the uniqueness constraint.
uniqueness constraint: every extract ei belongs to exactly one record rj.
mathematically  the uniqueness constraint can be stated as
= 1. if necessary  we can make the constraint less rigid by requesting that every extract appear in at most one record. the relaxed constraint can be written as
1.
the assignment table  table 1  suggests a second constraint  what we call the consecutiveness constraint.
consecutiveness constraint: only contiguous blocks of extracts can be assigned to the same record.
these constraints can be expressed mathematically in the following way: xij + xkj ¡Ü 1 when there is n  k   n   i  such that xnj = 1. in other words  we cannot assign extract e1 in table 1 to r1 along with extracts e1  e1  and e1 because neither e1 nor e1 can be assigned to r1. a better choice is to assign e1 e1 e1 e1 to r1 and e1 e1 e1 e1 to r1.
e1e1e1e1e1e1e1e1e1e1e1john1 wanew 1 john1r wawash 1 george w.findlay  1 smithington...holland...1smithshington...ington...1smithoh...1dir1 r1r1r1r1 r1r1 r1r1r1r1 r1r1r1r1table 1: observations of extracts on detail pages di for the superpages site
e1e1e1e1e1e1e1e1e1e1e1john1 wanew 1 john1r wawash 1 george w.findlay  1 smithington...holland...1smithshington...ington...1smithoh...1r111r111r111table 1: assignment of extracts to recordsthe observations and assignment tables are closely linked. if extract ei was not observed on detail page   then xij = 1  in other words  ei cannot be assigned to rj. if ei was observed on detail page rj  then xij is either 1 or 1. the constraints on the assignment variables can be easily written down from the observations data. for the superpages site data shown in table 1 the uniqueness and consecutiveness constraints are written below:
x1 + x1 = 1x1 + x1  = 1x1 = 1x1 + x1  = 1x1 = 1x1 + x1  = 1x1 + x1 = 1x1 + x1  = 1x1 + x1 = 1x1 + x1  = 1x1 = 1x1 + x1  = 1......1 position constraints
detail pages present another source of constraints we can exploit  because in addition to the occurrence of an extract on a page  they provide information about the position of that extract. this information is summarized in table 1 for the superpages site. the horizontal rows are the positions on page j  e.g.  token number of the starting token  where the extracts were observed. note that in table 1 the first four positions are from detail page r1  while the next four are from r1. if extract ei was observed in position posjk ei  on detail page rk  the  i j  cell in table has an entry  1 ; otherwise  it is empty. it is clear that no two extracts assigned to the same record can appear in the same position on that page. the collorary is: if two extracts appear in the same position on the detail page  they must be assigned to different records. we express it more formally as
position constraint: if   then ei and
ek cannot both be assigned to rj
in the example in table 1  the position constraints are x1 + x1 = 1 x1 + x1 = 1 x1 + x1 = 1
...
these constraints can also be relaxed to produce inequalities.
after we have constructed the uniqueness  consecutiveness and position constraints on the assignment variables for a particular web page  we solve them using wsat oip    an integer optimization algorithm . the solution is the assignment of extracts to records. results are presented in section 1.
1. a probabilistic approach to record segmentation
an alternate approach is to frame the record segmentation and extraction task as a probabilistic inference problem. common probabilistic models for information extraction include hidden markov models  hmms    and conditional random fields  crfs   1  1 . in these approaches  a labeled training set is provided and the model is learned using standard probabilistic inference techniques; because the state is hidden  the common approach to learning the models is to use the expectation maximization  em  algorithm. while these approaches have their computational limitations  they have been applied successfully to a wide range of problems beyond information extraction including speech recognition and robot navigation.
unfortunately  here we are faced with a more challenging problem. we do not have a labeled training set to start from. the key to our success will be to:
factor: we will factor the state space and observation set of the hmm to allow for more efficient learning  because fewer parameters will be required .
bootstrap: we will use the information from the detail pages to help bootstrap the learning algorithm. the constraints from the detail extracts will provide useful information that can keep our learning algorithm on track.
structure: we will use a hierarchical model to capture global parameters such as the length of the record  or the period  to make our inference more tractable. note that while there is a global record length  the record lengths of the individual records may vary; for some records not all columns will be presented.

table 1: positions of extracts on detail pages. entry of 1 means extract ei was observed at position k onpage j  poskj .

figure 1: a probabilistic model for record extraction from list and detail pages.
we begin by describing the probablistic model that we will use  section 1   then describe how the model is used to do record segmentation.  section 1 .
1 probabilisticmodelforrecordextraction
figure 1 shows a graphical model representation for our problem. the basic variables in the model are the following:
t = {t1 ... tn}: token types of extract ei. examples of token types are alphanumeric  capitalized  punctuation  as described earlier. we have 1 token types  so each ti is represented as a vector ti1  ti1  ...  ti1.
d = {d1 ... dn}: record numbers of the detail pages di   1 ... k on which ei occurred.
the above variables are observed in our data - given a list and set of detail pages  we can compute the values for the above variables.
in addition  we have the following set of unobserved variables:
r = {r1 ... rn}: the record number of the extract. we assume these range from 1...k  these correspond to detail pages .
c = {c1 ... cn}: the column label of the extract. we assume these range from l1 ... lk  a bound on this is the largest number of extracts found on a detail page .
s = {s1 ... sn}: si is true if ei is the start of a new record  false otherwise.
of course  in addition to the variables  our model describes the dependencies between them. rather than using the standard hmm representation  we use a factored representation  1  1   which allows us to more economically model  and learn  the state transition probabilities. we have defined the factored structure of the model  as shown in figure 1. arrows indicate probabilistic dependencies. in some cases these dependencies are deterministic - for example if an extract appears on only one detail page  then we know the record to which it belongs. in other cases the dependencies are probabilistic - for example  the starting token type of a column might usually be a capitalized string  but occasionally will be an html tag. here  we assume the structure of the dependencies  and where reasonable  the functional form for the dependencies  and we will learn the probabilities for the model using our observed data.
our model assumes the following dependencies:
p ti|ci : the token type for extract i depends on the column label. for example  for the name field  the token type is likely to be capitalized token  but this is a probabilistic relationship.
p ci|ci 1 : the column label for extract i depends on the column label of the previous column. for example  the address field is likely to follow the name field. note that because the token type in turn depends on the column label  it will also provide information to help us determine the column label. for example  if the previous column label is name  and the current token is numeric  we may think the column label address is most likely. however if the token is all-caps  we might think the column label is more likely to be state. note that while we used the values name and address above  we really only have the column labels l1 ... lk. as mentioned earlier  we may be able to automatically create semantically meaningful names for them using other automatic extraction techniques.
p si|ci : si  whether extract i starts a new record  depends on the column label. it turns out that while later columns may be missing from a record  in all of the domains that we have examined the first column  which usually contains the most salient identifier  such as the name  is never missing. this allows us to make a very useful simplification: rather than needing to learn the transition probabilities for the data  it makes sense to assume a deterministic relationship: p si = true|ci =
l1  = 1 and p si = true = 1  = 1. note that since ci is not observed  in order to do segmentation we will still need to do probabilistic inference to compute ci.
p ri|ri 1 di si : the record number for extract i will depend on the record number of the previous extract  whether or not si is the start of a new record  and di  the detail pages on which ei has been observed. in general  this is a deterministic relationship: if si is false  then p ri = ri 1  = 1 and if si is true  then p ri = ri 1 + 1  = 1 however di also constrains ri. ri must take one of the values of di.
the task of record segmentation boils down to finding values for the unobserved r and c variables. as is commonly done in probabilistic models for sequence data  we compute maximum a posteriori  map  probability for r and c and use this as our segmentation:
               argmaxp r c|t d  because we are assuming a markov model  we can write:

and using the structure of the graphical model above  this simplifies to:

1 learning the model
at this point  we could apply a standard off-the-shelf probabilistic inference algorithm to learn the model parameters and to make the appropriate inferences. unfortunately  our model is so unconstrained  we would have little luck inferring anything useful. we will use two ideas: bootstrapping and structure to make successful inferences.
1.1 bootstrapping
the key way in which information from detail pages helps us is it gives us a guide to some of the initial ri assignments. it turns out this little bit of information can be surprisingly informative.
recall that di is the set of detail pages on which extract ei occurs. this provides invaluable evidence for the record assignment for ri. we make use of this information by setting:

and p ri = ri  = 1  for all.
in addition  we make the following initial assumptions for the parameters of the model: p tij = true|ci  = 1  in other words  without observing any of the data  we assume that the probability of a token type occurring  given the column  is 1. note that this does not preclude an extract

figure 1: a probabilistic model for record extraction from list and detail pages which includes a record period model ¦Ð.
being of more than one token type. while we begin with this uniform assumption  we will be updating the model parameters as we go along  so the parameters will quickly be updated in accordance with the frequency that we observe the different tokens and column labels.
we also make use of the di to infer values for si. if di 1 ¡É di =    then p si = true  = 1. as a simple example  if extract i only appears on detail page j and extract i   1 only appears on detail page j   1  then si = true.
1.1 structure
besides the information from the detail pages  another important piece of information we can make use of is the record length  or the period ¦Ð of the table; ¦Ð is the number of columns in the table. recall  however  that not every record will have all ¦Ð columns. instead  for each record rj  there will be an associated ¦Ðj  which is the number of fields in record j. this approach allows for missing fields in a record - a common occurrence in web data.
one way of allowing this is simply to make use of the si. if si = true and ri = j and  is the next record start indicator that is true  then we can simply compute ¦Ðj =
¡¡¡¡. but this fails to capture the correlations among record lengths. in our example  there are 1 fields possible in a full record  perhaps most often we will have all 1 fields  name  address  city  phone  but another common occurrence is to just have the 1 fields name  city and phone. we want to be able to learn this. now  the columns ci will be conditioned on the corresponding ¦Ðj. we can learn for example that ci = city is much more likely if ¦Ðj = 1  and ci 1 = name .
the difficulty here is that until we know the correspondence of extract i to rj; we do not even know which ¦Ðj to depend on. this could make inference even more complex. it turns out however  that there are only a small number of possibilities  so this is in fact feasible. furthermore this more complex model does in fact give us improvements in accuracy. figure 1 shows a graphical model representation of the updated model.
1.1 implementation
we use em to implement the probabilistic inference algorithm. we have developed a variant of the forward-backward algorithm that exploits the hierarchical nature of the record segmentation problem. by explicitly modeling the period probabilities  we can constrain the structure of the model and in turn use this structure to guide the inference process.
the basic components of the algorithm are:
1. compute initial distribution for the global period ¦Ð using the current values for the si. because of the constraints the detail pages offer  our initial si will provide us with some useful information for ¦Ð.
1. now  having updated ¦Ð  we compute the ¦Ðk for each record j.
1. for each potential starting point and record length  weupdate the column start probabilities  p ci|ti ci 1 .
1. next we update p si|ci .
1. and finally we update p ri|ri 1 di si 
in the end we output the most likely assignment to r and c. this gives us the segmentation for the list page. while in theory  the running time of this algorithm is potentially exponential in the length of the extract sequence  by using the period probabilities to structure the inference we get enormous savings. in practice the algorithm is very efficient and took just a few seconds to run on each of our test cases.
1. results
we now present results of automatic segmentation of records using the csp and probabilistic approaches described in the sections above.
1 experimental setup
the data set consisted of list and detail pages from 1 web sites in four different information domains  including book sellers  amazon  bnbooks   property tax sites  buttler  allegheny  lee counties   white pages  superpages  y ahoo  canada1  sprintcanada  and corrections  ohio  minnesotta  michigan  domains. from each site  we randomly selected two list pages and manually downloaded the detail pages. in this work  we were not concerned with the task of automatically determining detail pages. in fact  there are often other links from the list page that point to advertisements and other extraneous data. such data may or may not influence the segmentation results. in future work  we will attempt to automate detail page identification. one approach is to use heuristic rules  such as  follow links in the table  or  follow a link whose text field is more info.  alternatively  one can download all the pages that are linked on the list pages  and then use a classification algorithm  1  1  to find a subset that contains the detail pages only. the detail pages  generated from the same template  will look similar to one another and different from advertisement pages  which probably don't share any common structure.
in addition to displaying different data  the pages varied greatly in their presentation and layout. some used gridlike tables  with or without borders  with easily identifiable columns and rows. others were more free-form  with a block of the page containing information about an item  followed by another block containing information about another item. within the block  attributes could appear in separate columns  rows  or in a formatted table. the entries could be numbered or unnumbered. commercial sites had the greatest complexity and more likely to be free-form than government sites.
the csp and probabilistic algorithms were exceedingly fast  taking only a few seconds to run in all cases.
1 evaluation
table 1 shows results of automatic record segmentation for these 1 sites using two different approaches  probabilistic and constraint satisfaction. both approaches share a common step  the page template finding algorithm. in cases where the template finding algorithm could not find a good page template  we have taken the entire text of the list page for analysis. only the strings that appeared on both list and detail pages were used in record segmentation. the rest of the table data are assumed to belong to the same record as the last assigned extract. the reason for this is that each row of the table usually starts with the most important attribute  such as the name or id number. this attribute will almost always appear on the detail page as well.
we manually checked the results of automatic segmentation and classified them as correctly segmented  cor  and incorrectly segmented  incor  records  unsegmented records  fn  and non-records  fp . precision and recall are defined below. we used the f measure to gauge the accuracy of the task.
p = cor/ cor + incor + fp  r = cor/ cor + fn  f = 1pr/ p + r 
we calculated p = 1  r = 1 and f = 1 for the probabilistic approach and p = 1  r = 1 and f = 1 for the csp approach. this is an exceedingly good performance for automatic algorithms. using heuristics  as described below  we can further improve on the results.
1 discussion
each approach has its benefits and drawbacks  which make them suitable in different situations. the csp approach is very reliable on clean data  but it is sensitive to errors and inconsistencies in the data source. one such source of data inconsistency was observed on the michigan corrections site  where an attribute had one value on the list pages and another value on the detail pages. this by itself is not a problem; however  the list page string appeared on one detail page in an unrelated context. the csp algorithm could not find an assignment of the variables that satisfied all the constraints. the probabilistic approach  on the other hand  tolerates such inconsistencies and is more expressive than the csp representation. its expressiveness gives us the power to potentially assign extracts to individual attributes  and  when combined with a system that automatically extracts column labels  from tables  reconstruct the relational database behind the web site. both techniques  or a
probabilisticcspwrappercorincfnfpcorincfnfpnotesamazon1111a  bbooks1111bn1111a  b  c  dbooks1111allegheny1111county1111butler1111county1111lee1111county1111michigan1111corrections1111c  dminnesota1111a  b  c  dcorrections1111ohio1111corrections1111canada111111111c  dsprint1111canada1111yahoo1111a  b  c  dpeople1111bsuper1111a  bpages1111precision11recall11f11notes
a. page template problem; b. entire page used; c. no solution found;
d. relax constraints
table 1: results of automatic record segmentation of tables in web pages using the probabilistic and cspapproaches
combination of the two  are likely to be required for robust and reliable large-scale information extraction. we stress that the approaches are novel in that they are domain independent  unsupervised  and rely on the content of web pages rather than their layout.
the page template finding algorithm performed poorly on five of the 1 sites: amazon  bnbooks  minnesota corrections  yahoo people and superpages. in the first three sites  the entries were numbered. thus  sequences such as  1.   will be found on every page. if the tables are of different lengths  the shortest table will limit what is to be considered a page template  and the remainder of data on the longer tables will be extracted. when we encountered a problem with the page template algorithm  we use the entire page as the table slot - in other words  we used the entire content of the list page for matching with the detail page. at times  using the entire list page led to problems  amazon  first yahoo people list page   as many strings that were not part of the table found matches on detail pages  although in some cases this approach performed quite well  second list in yahoo people  superpages . there are a number of ways to circumvent this problem which were not explored in this paper. one method is to simply follow the  next  link  and download the next page of results. the entry numbers of the next page will be different from others in the sample. another approach is to build a heuristic into the page template algorithm that finds enumerated entries. we will try this approach in our future work.
the csp approach performed extremely well on clean data; however  it was prone to fail when there were errors and inconsistencies in the data underlying the web sites. for example  on one canada1 page  one of the records had the town attribute missing on the detail page but not on the list page. since the town name was the same as in other records  it was found on every detail page but the one corresponding to the record in question. as a result  wsat oip  could not find a solution satisfying all constraints. relaxing constraints by replacing equalities with inequalities produced a solution  but it was a partial solution  because not every extract was assigned to a record. another common data inconsistency that caused wsat oip  to fail was when the attribute had different values on list and detail pages. for example  on the amazon site  a long list of authors was abbreviated as  firstname lastname  et al  on list pages  while the names appeared in full on the detail page. on the minnesota corrections site  there was a case mismatch between attribute values on list and detail pages. on the michigan corrections site  status of an paroled inmate was listed as  parole  on list pages and  parolee  on detail pages. unfortunately  the string  parole  appeared on another page in a completely different context. as a result  all constraints could not be satisfied. in such cases we relaxed the constraints  for example  by requiring that an extract appear on at most one detail page. wsat oip  was able to find solutions for the relaxed constraint problem  but the solution corresponded to a partial assignment. if we excluded from consideration those web pages for which the csp algorithm could not find a solution  performance metrics on the remaining 1 pages were p = 1  r = 1 and f = 1. this performance is comparable to that obtained with hand-crafted heuristic or domain-specific rules.  1  1  the probabilistic approach was less sensitive to data inconsistencies  but was slightly less accurate overall. on the same 1 pages as above  its performance was p = 1  r = 1 and f = 1.
except for the two book sites  we were able to correctly segment at least one table using either method. the tables on the book site pages presented challenges. the entries in these lists were numbered. as a result  the page template algorithm did not work  and we had to use the text of the entire list page. unfortunately  many of the strings in the list page  that were not part of the list  appeared in detail pages  confounding our algorithms. for the amazon site the problem was further compounded by the fact that we downloaded the pages manually. the site offers the user a useful feature of displaying her browsing history on the pages. this led to title of books from previously downloaded detail pages to appear on unrelated pages  completely derailing the csp algorithm. even relaxing constraints to find a partial assignment did not produce good results for these two sites. these observations do not apply to the performance of the algorithm  only the data collection and preparation steps. adding domain-specific data collection techniques should improve the final segmentation results.
the probabilistic approach allows us to assign extracts to attributes  not only records. this remains an open problem for future research. it may also be possible to obtain the attribute assignment in the csp approach  by using the observation that different values of the same attribute should be similar in content  e.g.  start with the same token type. we may be able to express this observation as a set of constraints.
as discussed earlier in this paper  the roadrunner system uses an elegant approach to automatically extract data from data-rich web sites. roadrunner assumes that the results pages from a site were generated by a union-free grammar and induces this grammar from example list pages. this approach  however  fails for web sites that use alternate formatting instructions for the same field. such alternate instructions are syntactically equivalent to disjunctions  which are disallowed by union-free grammars. consider superpages list page in fig. 1. if an address field is missing  the text  street address not available  is displayed in gray font; otherwise  an address is displayed in black. in each alternative  different html tags are used to format the address field. there is still an underlying grammar that gave rise to this superpages page  but this grammar admits unions. inferring such grammars from examples is an even more computationally complex task than inferring unionfree grammars. our approach  on the other hand  is more computationally efficient  because rather than using a page's layout  it relies on the content of the page  which is often much less than the layout. both of our methods handle the superpages site very effectively  as shown in the results.
1. conclusion
there are multiple ways to represent and leverage the additional information contained in the structure of web sites.
in this work we investigated two of them: 1  a logic-based approach in which we encode the information provided by detail pages as constraints and solve them to obtain the record segmentation  and 1  a probabilistic inference approach in which we represent the observations and structure of the table as a probabilistic model and use an inference algorithm to find appropriate segmentation. both approaches have widely used  efficient algorithms for solving problems. each has its benefits and drawbacks  that make them preferable in different situations. the constraint-satisfaction approach is very reliable on clean data  but it is sensitive to errors and inconsistencies in the data source. the probabilistic approach on the other hand  tolerates inconsistencies and is more expressive than the constraint-based approach  and  beyond record segmentation  it can perform record extraction. both techniques  or a combination of the two  are likely to be required for large-scale robust and reliable information extraction.
1. acknowledgements
