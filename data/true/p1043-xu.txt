parallel processing continues to be important in large data warehouses. the processing requirements continue to expand in multiple dimensions. these include greater volumes  increasing number of concurrent users  more complex queries  and more applications which define complex logical  semantic  and physical data models. shared nothing parallel database management systems  can scale up horizontally by adding more nodes. most parallel algorithms  however  do not take into account data skew. data skew occurs naturally in many applications. a query processing skewed data not only slows down its response time  but generates hot nodes  which become a bottleneck throttling the overall system performance. motivated by real business problems  we propose a new join geography called prpd  partial redistribution & partial duplication  to improve the performance and scalability of parallel joins in the presence of data skew in a shared-nothing system. our experimental results show that prpd significantly speeds up query elapsed time in the presence of data skew. our experience shows that eliminating system bottlenecks caused by data skew improves the throughput of the whole system which is important in parallel data warehouses that often run high concurrency workloads.
categories and subject descriptors
h.1  information systems : database management-systems
general terms
algorithms
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  vancouver  bc  canada.
copyright 1 acm 1-1-1/1 ...$1.
keywords
data skew  parallel joins  shared nothing
1. introduction
　parallel processing continues to be important in large data warehouses. the processing requirements continue to expand in multiple dimensions. these include greater volume  increasing number of concurrent users  more complex queries  and more applications which define complex logical  semantic  and physical data models.
　in a shared nothing architecture  multiple nodes communicate via high-speed interconnect network and each node has its own private memory and disk s . in current systems  there are usually multiple virtual processors  collections of software processes  running on each node to take advantage of the multiple cpus and disks available on each node for further parallelism. these virtual processors  responsible for doing the scans  joins  locking  transaction management  and other data management work  are called parallel units  pus  in this paper.
　relations are usually horizontally partitioned across all pus which allows the system to exploit the i/o bandwidth of multiple disks by reading and writing them in parallel. hash partitioning is commonly used to partition relations across all pus. tuples of a relation are assigned to a pu by applying a hash function to their partitioning column. this partitioning column is one or more attributes from the relation  specified by the user or automatically chosen by the system.
　as an example  figure 1 shows the partitioning of two relations r x a  and s y b  on a three-pu system  assuming that the partitioning columns are r.x and s.y respectively  and that the hash function h is h i  = i mod 1 + 1. the hash function h places any tuple with the value i in the partitioning column on the h i -th pu. for example  a tuple  x = 1 a = 1  of r is placed on the first pu since h 1  = 1.
the fragment of r  or s  on the i-th pu is denoted as ri
 or si .
　conventionally  in a shared nothing parallel system  there are two join geographies to evaluate. the first join geography is called the redistribution plan and the second is called the duplication plan. both plans consist of two stages.
　in the first stage of the redistribution plan  when neither r.a or s.b is the partitioning column  both r and s are redistributed based on the hash values of their join attributes so that matching rows are sent to the same pus1. this redistribution in the first stage of the redistribution plan is called hash redistribution. for example  figure 1 shows the result of the first stage after hash redistributing both r and s on r.a and s.b respectively. rredisi  or sredisi   denotes the spool on the i-th pu that contains all rows of r  or s  hash redistributed to the i-th pu from all pus. these include rows from ri  si . obviously  if a relation's join attribute is its partitioning column  there is no need to hash redistribute that relation. when both relations' join attributes are their partitioning columns  the first stage of the redistribution is not needed.
　in the first stage of the duplication plan  tuples of the smaller relation on each pu is duplicated  broadcast  to all pus so that each pu has a complete copy of the smaller relation. as an example  figure 1 shows the result of duplicating s in figure 1 to every pu.
　in the second stage of both plans  the join operation is performed on each pu in parallel. this can be done since the first stage has put all matching rows from the join relations on the same pus.
　research has shown that the redistribution plan has nearlinear speed-up on shared nothing systems under even balancing conditions . however  if we use the redistribution plan to evaluateand if one relation  r  has many rows with the same value v in the join attribute  r.a   one pu will receive all these rows. this pu will become hot and can be the performance bottleneck in the whole system. the value v is called a skewed value of r in r.a. any row of r containing a skewed value v is called a skewed row. figure 1 shows that the number of rows of r the second pu receives is 1 times that of any other pu. obviously the more skew in the data  the hotter the hot pu will become.
　adding more nodes to the system will not solve the skew problem because all skewed rows will still be sent to a single pu. this will only reduce the parallel efficiency  since adding more nodes will make each non-hot pu colder  having fewer rows  and make the hot pu comparatively even hotter. the type of data skew demonstrated in figure 1 is categorized as redistribution skew in  and is what we study in this paper.
　redistribution skew could be the result of a poorly designed hash function. however  theoretical research on hashing offers a class of good universal hash functions  that perform well with a high probability. the more fundamental problem comes from naturally occurring skewed values in the join attributes.
　we have seen redistribution skew in many types of industrial applications. for example  in the travel booking industry  a big customer often makes a large number of reservations on behalf of all of its end users. in online e-commerce  a few professionals make millions of transactions a year while the vast majority of the other customers only do a few a year. in telecommunication  some phone numbers used for telemarketing make a huge number of phone calls while most customers make a few calls daily. in retail  some items sell far more than other items. usually the relations in these

1
 the base relations are not changed  only copies of the projected rows are redistributed for the evaluation of this query.

figure 1: two relations r and s on 1 parallel units. the partitioning columns are r.x and s.y respectively. the hash function  h i  = i mod 1  places a tuple with value i in the partitioning column on the h i -th pu.

figure 1: the first stage of the redistribution plan for  : data placement after hash redistributing r and s on the join attributes  r.a and s.b  based on the hash function h i  = i mod 1 + 1. rredisi  or sredisi   denotes the spool on the i-th pu that contains all rows of r  or s  hash redistributed to the i-th pu.
applications are almost evenly partitioned by their unique transaction ids. however  when the join attribute is a nonpartitioning column attribute  like customer id  phone number or item id   severe redistribution skew happens. this can have a disastrous effect on system performance. out of spool space errors  which cause queries to abort  often after hours of operation in large data warehouses   can also happen in the presence of severe redistribution skew. this is because although disk continues to become larger and cheaper  parallel dbmses still maintain spool space quotas for users on each pu for the purpose of workload management and concurrency control. sometimes  a system can choose the duplication plan instead of the redistribution plan to alleviate the skew problem. this works in very limited cases when one join relation is fairly small. though many algorithms have been proposed in the research community  to our best knowledge  no effective skew handling mechanism has been implemented by major parallel dbms vendors  either because of their high implementation complexity or communication cost  or the significant changes required to the current systems.
we make the following contributions in the paper:   we propose a practical and efficient join geography called partial redistribution & partial duplication  prpd  to handle data skew in parallel joins motivated by real business problems.
  the prpd join geography does not require major changes to the current implementations of a shared-nothing architecture.

figure 1: the first stage of the duplication plan for
: data placement after duplicating s to every pu. the spool sdup on each pu has a complete copy of s.
  our experiments show the efficiency of the proposed prpd join geography.
　the rest of the paper is organized as follows. in section 1 we present the prpd join geography. section 1 discusses how the prpd geography is applied to multiple joins. section 1 shows the experimental results. section 1 discusses related work. section 1 concludes the paper and discusses future work.
1. partial redistribution partial duplication  prpd 
　in this section we present the prpd join geography  starting with an observation and an intuition behind the prpd approach. we notice that parallel system dbas and application writers are well educated on the importance of choosing good partitioning columns to evenly partition their data for efficient parallel processing  usually through required certification or training. parallel dbmses provide tools to show how data is partitioned and  if necessary  to help users change the partitioning columns. in the absence of other good choices  tables can be defined using identity columns  which automatically generate unique values  as partitioning columns. thus  in practice  large relations are most likely to be evenly partitioned.
　consider the join . assume r is evenly partitioned  r.a is not the partitioning column of r  and r has many skewed rows in r.a. the key observation is that the skewed rows of r tend to be evenly partitioned on each pu. the intuition behind the prpd join geography is to deal with the skewed rows and non-skewed rows of r differently. prpd keeps the skewed rows of r locally on each pu  instead of hash redistributing them all to a single pu   and duplicates those matching rows from s to all pus. prpd uses the redistribution plan for other rows of r and s. in section 1 we discuss the prpd join geography. section 1 compares prpd with the redistribution plan and the duplication plan. section 1 discusses how prpd handles unusual cases where skewed rows are not evenly partitioned on all pus.

 figure 1: the prpd plan for  and  are hash redistributed on r.a and s.b respectively;	 and	 are duplicated;  and  are kept locally.
1 prpd description
　assume the optimizer chooses the prpd join geography to join . for simplicity  we will use the terms prpd join geography and the prpd plan interchangeably. we first assume that the system knows the set of skewed values l1 in r.a and the set of skewed values l1 in s.b and that neither r.a nor s.b is the partitioning column. we will show later how prpd works when only one relation is skewed or when one relation's partitioning column is the join attribute.
　there are three steps in the prpd plan. assume there are n pus in the system.
  step 1
- 1  on each pui  scan ri once and split the rows into three sets. the first set  named   contains all skewed rows of ri for any value in l1.
 is kept locally. the second set  named r1 dup  contains every row of ri whose r.a value matches any value in l1 and is duplicated to all pus. the third set  named contains all other rows in ri and is hash redistributed on r.a.
note that  and disjointly partition ri on each pu.
on each pui  three receiving spool files rredisi   rdupi   and rloci are created. the first spool rredisi receives all rows of r redistributed to the i-th pu from any pu including pui itself. that is  rredisi contains any row from any r1j redis  1 ＋ j ＋ n  which is redistributed to the i-th pu by the system's hash function  h.

the second spool rdupi receives all rows of r duplicated to the i-th pu from any pu including
pui itself. that is  . notice that for any i and j. the third spool rloci receives all rows from. mathematically  .
- 1  similarly  on each pui  scan si once and split the rows into three sets. the first set  named
  contains all skewed rows of si in any value in l1. s1 loc is kept locally. the second set si whose
named  contains every row of
s.b value matches any value in l1 and is duplicated to all pus. the third set  named
contains all other rows in si and is hash redistributed on s.b. note that loc si and disjointly partition si on each pu.
on each pui  three receiving spools sredisi   sdupi   and sloci are also created  similarly to rredisi   rdupi   and rloci . the definitions are shown below.
 
note that in implementation  when a row of r or s is read  the system immediately determines whether to keep it locally  redistribute it or duplicate it based on the value in the join attribute.
the six sets   and are not materialized and are used only for presentation purpose. only the six receiving spools rredisi   rdupi   rloci   sredisi   sdupi and sloci are materialized and used in the join operations shown in the next step.
  step 1 on each pui do the following three joins 
- 1  .
- 1  .
- 1  .
  step 1
the final result is the union of the three joins from step 1 on all pus.
　notice that all sub-steps in each step can run in parallel and they usually do. for example  the sub-steps 1 and 1 in the first step can run in parallel  and so can the three joins in the second step.
　in step 1  the skewed values in r.a and s.b together determine how to split r and s into different spools. when a value v appears in both the skewed values in r.a and s.b  we can not duplicate them in both relations. our solution dealing with overlapping skewed values is described below. when invoking the prpd plan  the system always invokes the prpd join plan with two non-overlapping sets l1  of skewed values in r  and l1  of skewed values in s . the systems chooses to include the skewed value v  in both relations  in only one of l1 and l1  instead of in both. by doing this  it is guaranteed that r1i loc  r1i dup and r1i redis
　　　　　ri on each pu  andand disjointly partition disjointly partition si on each pu. the question is to decide which set  l1 or l1  should include v. assume the number of skewed rows of r  whose r.a value is v  times the projected row size of r is bigger than the number of skewed rows of s  whose s.b value is v  times the projected row size of s  then the system chooses to include v in l1 to keep these skewed rows of r locally in rloci   and those rows of s whose join attribute value is v are duplicated to sdupi .
　when only one relation is skewed  the first step produces two receiving spools for each relation instead of three. assume only r is skewed. the first step will produce only two receiving spools rloci and rredisi for r  and sdupi and sredisi for s. rdupi and sloci are not created since there is no skewed value in s. consequently  the second step will only need to perform the first two joins. in step 1  we allow the optimizer to choose the best physical join method for each of the three joins. the optimizer may choose different join methods for each of the three joins.
　when one relation's  assume it is s  join attribute is also its partitioning column  the prpd approach works logically in the same way. the one difference is that each pu will keep locally those rows of s whose join attribute values are not skewed in r  rather than redistribute them to any other pus since s.b is the partitioning column. although the redistribution plan does not need to redistribute s  skewed rows in r will be still sent to a single pu  causing system bottleneck. in this case  prpd can outperform the redistribution plan just as in the case where neither relation's join attribute is the partitioning column.
　overall  when r is skewed  the skewed rows of r are kept locally  the matching rows in s for these skewed rows are duplicated to all pus  and all other rows from both relations are redistributed. part of s is redistributed and part of s is duplicated. when s is also skewed  symmetric actions are taken. thus we named the proposed approach prpd
 partial redistribution and partial duplication .
　the basic idea of the prpd plan is illustrated in figure 1. the new operator split reads each row and either keeps it locally  redistributes it or duplicates it according to its value in the join attribute and the set of skewed values in r and s. as an example  figure 1 a  shows how the prpd plan is going to place each row of r and s at each pu for the example data shown in figure 1. figure 1 b  shows the data placement after applying the prpd plan. comparing figure 1 b  with figure 1  we can see the figures visually show that the processing at each pu in the prpd plan is even and there is no hot pu.
　appendix a provides the correctness proof of the prpd plan.
1 prpd  redistribution plan and duplication plan comparison
　the partial duplication part of the prpd plan is not in the redistribution plan  therefore causes prpd to use more total spool space than the redistribution plan. on the other hand  the partial redistribution part of the prpd plan keeps the skewed rows locally and has less networking communication cost than the redistribution plan. a more important difference between the prpd plan and the redistribution plan is that prpd does not send all skewed rows to a single pu and does not cause a system bottleneck.

figure 1: the prpd join geography for. assume the hash function is h i  = i mod 1 + 1  1 is the skewed value in r and 1 is the skewed value in s.  a  illustration of each tuple's future placement.ir1i iredis andare to be hash redistributed on r.a and s.b. r1 dup andto be duplicated;and s1 loc to be kept locally.  b  illustration of each tuple's placement after applying the prpd plan.　the partial duplication part of the prpd plan may make prpd use less total spool space than the duplication plan since not every row of a relation is duplicated. on the other hand  the partial redistribution part of the prpd plan may make the prpd plan have more networking communication cost than the duplication plan when data skew is not significant and the prpd plan needs to redistribute a large relation. another difference is that the duplication plan always joins a complete copy of the duplicated relation to the fragment of the other relation on every pu while the prpd plan reduces the sizes of the spools to be joined on each pu. the prpd join plan is a hybrid of the redistribution plan and the duplication plan. consider the join the optimizer invokes the prpd plan with an empty set l1  of skewed values in r.a  and an empty set l1  of skewed values in s.b   prpd will behave exactly the same way as the redistribution plan  since no rows are kept locally or duplicated. if l1  the set of skewed values in r.a  is equal to uniq r.a   the set of unique values in r.a  and is a superset of uniq s.b   the set of unique values in s.b  then prpd will behave exactly the same way as the duplication plan since every row of r will be kept locally  because every value in r.a is a skewed value  and every row of s will be duplicated. if l1 is equal to uniq r.a   but is not a superset of uniq s.b   then prpd will keep every row of r locally  duplicate some rows of s and hash redistribute some rows of s. in this case those hash redistributed rows of s will not match any rows of r. if l1 is a superset of uniq s.b  and a subset of uniq r.a   then prpd will duplicate every row of s  keep some rows of r locally  and hash redistribute some rows of r. in this case those hash redistributed rows of r will not match any rows of s. neither hash redistribution in the last two cases is necessary. however unless the optimizer knows in advance the relationships among l1  uniq r.a  and uniq s.b   the system has no way to eliminate it.
　assume the number of pus in the system is n  x is the percentage of the skewed rows  skewness  in a relation r. the number of rows of r in the hot pu after redistribution in the redistribution plan is roughly while the number of rows of r on a non-hot pu is roughly . thus  the ratio of skewed rows of r on the hot pu over the
number of rows ofin the prpd plan  after the first stage  the number of rowsr on a non-hot pu is roughly 1 + 1nx x.
of r on each pu is roughly . thus  the ratio of the number of rows of r on the hot pu in the redistribution plan over the number of rows of r on any pu in the pprd plan after the first stage is roughly 1+ n 1 x. the ratio depends on the skewness of r and the size of the system  the number of pus in the system   but does not depend on the size of the relation r itself. the ratio is nearly linear to the size of the system and the skewness of r. the ratio is one important factor determining the speedup ratio that prpd can achieve over the redistribution plan.
　as an example  for a small system where n = 1  1 nodes each running 1 pus   the ratio is 1 for x = 1%  1 for x = 1%. for a medium size system where n = 1  1 nodes each running 1 pus   the ratio is 1 for x = 1%  1 for x = 1%. for a large system where n = 1  1 nodes each running 1 pus   the ratio is 1 for x = 1% and 1 for x = 1%. we include in section 1 experiments exploring the impact of the size of the system and data skewness on the speedup ratio of prpd over the redistribution plan.
　one important issue is how the optimizer chooses prpd over other plans. as usual  a cost based optimizer will choose the prpd plan over other plans only when it determines the cost of applying prpd is smaller. we will leave out the discussion on how the optimizer computes and compares the costs of prpd  redistribution plan and duplication plan  which is beyond the scope of this paper.
　in addition to using statistics  the optimizer can also use sampling to find skewed values as sampling is a negligible cost in query response time  with or without data skew  as shown in  1  1 .
1 handling unevenly partitioned skewed rows in prpd
　the prpd join plan presented so far works on the assumption that skewed rows are roughly evenly partitioned on all pus. this section discusses how the prpd plan handles unusual cases where the skewed rows are only on one or a few pus.
　consider the join and r is skewed in its join attribute. consider a case where all the skewed rows of r are on a small number of pus  called skewed pus in this section. though the prpd plan will not cause redistribution skew itself since all skewed rows are kept locally  these skewed pus will become hot pus since in addition to the skewed rows they already have  they will receive redistributed rows from other pus. consider the skewed pu p that contains the most number of skewed rows among all pus. assume r is evenly partitioned  the number of pus is n and the skewness of r is x. with prpd  p will keep all of its skewed rows and receive some non-skewed rows sent from other pus. the number of rows on p after the first step in prpd is roughly where the function min a b  returns the smaller number from a and b. the prpd plan is slightly modified to handle skewed pus cases. instead of keeping the skewed rows of r locally on each pu  we randomly redistribute them to all pus so that skewed rows are evenly redistributed to all pus. there is no change on handling s. those rows of s that contain skewed values in r are still duplicated to every pu. with this change in prpd  the skewed pu p will now have |rn| rows of r after the first step in the modified prpd plan. the ratio of the number of rows on the skewed pu p using the original prpd plan over the number of rows on the same pu p using the modified prpd plan is
the ratio is 1 when x = 1 or x = 1. when nx − 1  the ratio is 1 x. notice thatis the minimal number of
pus needed to contain all skewed rows in r   assuming r is evenly partitioned. the ratio shows that redistributing the skewed rows to all pus in the case where skewed rows are on only one or a few pus can partition the skewed relation in preparation for the joins in the second stage better than keeping all skewed rows locally.
1. applying prpd in multiple joins
　in this section we discuss how the prpd join geography is applied in multiple joins. intuitively  the fact that not all rows in the join result from applying the prpd plan are partitioned by the join attributes may be an issue in multiple joins.
　in this section we show how the prpd plan can be mixed with other conventional plans in multi-table join queries. we will not discuss join ordering and planning with the introduction of prpd  which is beyond the scope of this paper. we focus on how the join result from applying the prpd plan can be joined with other relations using different join geographies.
　consider a query that joins three relations r  s and y and the join condition on r and s is r.a = s.b. assume the optimizer chooses to first join r and s. let x be the result of joining r and s. there are 1 possible cases in terms of join geography that may be used to join x and y before the introduction of prpd. for each of x and y   we can keep it locally  redistribute it or duplicate it. thus  there are 1 combinations for x and y . however  when one relation is duplicated  the other relation does not need to be duplicated or redistributed. the 1 possible combinations are shown in figure 1 where each line denotes one possible case described below.
  case 1: duplicate x; keep y locally.
  case 1: keep x locally; duplicate y .
  case 1: redistribute x on its join attribute that is not
r.a or s.b; redistribute y .
  case 1: redistribute x on its join attribute that is not r.a or s.b; keep y locally when y is a base table and the join attribute of y is y 's partitioning column or when y is an intermediate result and has been redistributed on its join attribute.
  case 1: redistribute y   keep x locally when the join attribute of x is r.a or s.b.
  case 1: keep both x and y locally when y is a base table and the join attribute of y is y 's partitioning column or when y is an intermediate result and has been redistributed on its join attribute; and when the join attribute of x is r.a or s.b.
　now let us consider the impact of joining r and s using prpd in the context of multi-table joins. assume the optimizer has chosen prpd to join r and s. the skew handling using prpd has no impact in the first four cases  in the sense that the system can execute the specified join geography in each case as usual. in cases 1 and 1  we cannot simply keep x locally anymore when prpd has been applied. certainly we can redistribute x again on r.a or s.b to join x and y. however we do not need to redistribute every row in x since some rows have been already redistributed to the right pus in prpd. we just need to redistribute the results from joining those rows that were kept locally in prpd.
　prpd is modified to work in multi-table joins as follows. when the optimizer identifies that the join geography of joining x and y is either case 1 or 1  it uses the following modified prpd step with the changes highlighted. step 1
  1  join rredisi	and sredisi	.
  1  join rloci and sdupi . hash redistribute the result on r.a
  1  join rdupi and sloci . hash redistribute the result on r.a
　when considering the impact of prpd in multiple joins  the optimizer takes into account of the cost of the extra redistribution step described above that is necessary for cases 1 and 1.
　note that these six cases are all based on traditional join geographies. prpd introduces one more choice for the optimizer in joining x and y in multi-table joins. with prpd  another choice is to use the prpd plan again to join x and y . indeed  this is a likely choice for the optimizer for cases 1 and 1.
　consider the case where r.a has skewed values and s is not skewed  called  single-skew  in   which is also the case most previous work focuses on . if the tuples in r with a skewed value v1 match some tuples in s and either r.a or s.b is the join attribute in the second join  as in case 1 or case 1   v1 tends to be a skewed value in x as well  as shown below. assume the set of unique values that appear in both r.a and s.b is {v1 ... vk}. let mi and ni be the number of tuples containing vi in r.a  in r  and in s.b  in s  respectively  1 ＋ i ＋ k . assume v1 is a skewed value in r.a. the number of tuples in the result of joining r and s with the value v1 in the join attribute is: m1. the total number of tuples in the join result is: k . thus  the percentage of tuples in the join result containing v1 in the join attribute is:

let δ1 be the skewness of v1 in r  the percentage of rows in
r containing v1 in r.a  and nmax be the maximal value in

figure 1: join geographies for
{n1 ... nk}. we have

the above equation shows that when v1 is a skewed value in r  v1 tends to be a skewed value in the join result as well  especially when s is not skewed and n1 is close to nmax.
1. experimental evaluation
　in this section  we are mainly interested in comparing the performances of the prpd plan and the redistribution plan because the redistribution plan is more widely used in large data warehousing systems than the duplication plan. since duplicating rows is expensive in terms of both i/o and cpu cost on all pus and the network bandwidth  the larger the number of pus in the system and the larger the join relations  the less likely the duplication plan can outperform the redistribution plan or the prpd plan. however  when one join relation is fairly small  the duplication plan can outperform the other two plans  and we have seen this in our experiments. in this section  we only report the experiments comparing prpd and the redistribution plan.
　we use the tpc-h  and the following query in our experiments.
select *
from customer  supplier q1 where c nationkey = snationkey
the schema of the customer and supplier relations are shown in figure 1.
　originally there are only 1 unique nations in the tpch database. we have increased the number of unique nations to 1 to facilitate our skew experiments. to control data skewness in the customer relation's join attribute  c nationkey   we randomly choose a portion of the data  and change their c nationkey values to one value. for example  in our experiments  if the skewness is 1% in the customer relation's join attribute  this means that we have used the following sql statement to change 1% of the rows of the customer relation to have the same value  1  in the join attribute.
update customer
	set	c nationkey = 1	q1
where random 1  ＋ 1

figure	1:	two	relations	customer	and
supplier from tpc-h. the partitioning columns

figure 1: query execution time  a single skewed value in the customer relation
1 query execution time
　the test system we use for the experiments has 1 nodes and 1 pus. each node has four pentium iv 1 ghz cpus  1 gb memory and 1 pus. we generate 1 million rows for the supplier relation  and 1 million rows for the customer relation and we vary the data skewness in the customer relation. the size of the query result is around 1 billion rows. figure 1 shows the execution times using the prpd plan and the redistribution plan. when the skewness increases  the execution time of the redistribution plan grows almost linearly because all skewed values are redistributed to one pu and that pu becomes the system bottleneck while the execution time of the prpd plan essentially stays the same. we consider the case where there are two skewed values and they cause two hot pus in the redistribution plan  rows with different skewed values in the join attribute are hash redistributed to two different pus . we report the results in figure 1 where both skewed values have the same skewness shown on x-axis. figures 1 shows the same performance relationship between the two plans as in figure 1. we also consider the case where both relations have skewed values. the results are similar to what are shown in figures 1 and
1.
1 speedup ratio with different number of pus
　in this experiment  we calculate the speedup ratio of prpd over the redistribution plan on three systems  1 pus  1 pus and 1 pus respectively. the result is shown in figure 1. as can been from figure 1  when the skewness increases  the speedup ratio of prpd over the redistribu-

figure 1: query execution time  two skewed values with the same skewness shown on x-axis in the customer relation causing two hot pus in the redistribution plan

figure 1: speedup ratio of the prpd plan over the redistribution plan
tion plan increases. also the larger the system  the larger the speedup ratio is.
　we have also done experiments on joining two relations where one relation's partitioning column is the join attribute and we have seen similar performance relationships shown in figures 1  1 and 1. we note here that our experimental results on using prpd in multi-table joins also show significant query performance improvement.
1. related work
　extensive research has been done on handling data skew in parallel dbmses.  describes four types of skew : tuple placement skew  tps   selectivity skew  ss   redistribution skew  rs  and join product skew  jps . tps can be avoided with a good hash function. in the theoretical literature  hash functions have been extensively studied  and several techniques have been proposed to design a well-performed hash function . furthermore  in practice partitioning columns are carefully chosen by dbas so that tuples of large relations are most likely evenly partitioned on all pus  parallel units . thus  the possibility of tps is extremely low and nearly no work focuses on tps. ss is caused by different selectivity of selection predicates and is query-dependent. ss becomes a problem only when it causes rs or jps. most prior work does not consider ss either.
as discussed in section 1  rs occurs when pus receive different numbers of tuples when they are redistributed in preparation for the actual joins. jps occurs when the join selectivity at each node differs  causing imbalance in the number of tuples produced at each pu. rs and jps are closely related problems and have been heavily studied. previous algorithms can be roughly classified into the following three categories.
　the first category of algorithms models the skew problem as the conventional task scheduling problem. for parallel joins  one or both join relations are partitioned into smaller units  ranges  or hash buckets  1  1  1  1  1  1  1   which are distributed to all pus. joins are computed in parallel locally. since the task scheduling problem is npcomplete  various well-known heuristics including lpt  and multifit  are used in this type of algorithms  1  1  1  1  1  1  1 . based on different cost models  different algorithms in the first category have been proposed.
　the first type of models uses the number of tuples on each pu as the estimation of join cost. the goal is to make every pu receive similar number of tuples of the two join relations so that rs does not occur. a few range-partitioning based algorithms in  handle the rs problem. the algorithms first find the skewed relation by sampling both relations  and then compute a split vector  different algorithms use different strategies to compute the split vector  to split the join attribute values in the skewed relation to a few ranges such that the number of ranges is equal to the number of pus in the system. tuples whose join attribute values fall within one range are all mapped  redistributed  to the same pu. the goal of the split vector is to make each pu receive similar number of tuples. if one value spans more than one range  which means it is a skewed value   the subset-replication algorithm in  replicates  duplicates  the tuples in the other join relation with this join attribute value to all pus that this value maps to. notice that in the subset-replication algorithm skewed rows are usually redistributed to a few pus according to the split vector  not to all pus.
　the second type of algorithms uses more sophisticated models   1  1  1  1  1  1  . the virtual processor scheduling  vps  algorithm in  deals with the jps problem. it usesand s are the two join relations  to estimate the cost of join on each pu. in   two functions  output function and work function are used to estimate join cost.  considers parallel systems whose pus may not have identical configurations.
　unlike the first category of algorithms which try to estimate the execution time before the actual join computation  the core idea of the second category of algorithms is to handle skew dynamically. work load at each pu is monitored at run time. if a pu is doing much more work than others  some work load from this hot pu will be migrated to other pus.  uses shared virtual memory to migrate workload from hot pus to idle pus.  detects the processing rate of the join relations on each pu. if a pu's processing rate is slow  this pu is deemed as hot. two algorithms are introduced: result redistribution and processing task migration. in result redistribution  results generated by hot pus are not stored locally but sent to other pus' disk. in processing task migration  part of unprocessed relations on a hot pu is migrated to other idle pus.  proposes two algorithms: the single-phase and the two-phase scheduling algorithms. the single-phase algorithm is similar to  1  1  1 . in the twophase scheduling algorithm  a central coordinator is used to maintain a task pool. when a pu becomes idle  it picks up one task from the central pool.
　the third category of algorithms uses semi-joins to more accurately estimate the size of joins and to reduce the cost of network communication.  proposes two methods. the first method consists of two stages. in the first stage  it computes the histogram of one join relation  r . based on the histogram  redistribution plan is computed such that each pu receives similar number of r's tuples. in this stage  rows with skewed values are randomly sent to other pus. in the second stage  the other relation  s  is semi-joined with the histogram of r. the result of the semi-join is duplicated to all pus and joined with r. the second method in  is a hybrid approach. it uses the first method only for skewed values and uses other conventional algorithms for non-skewed values.
　compared with previous algorithms  prpd has the following characteristics. skewed values are kept locally rather than redistributed to all  or some  pus. this saves redistribution cost and decreases execution time. unlike most previous work  prpd not only deals with cases where only one relation is skewed  but also cases where both relations are skewed. prpd also solves the type of jps problems that result from redistribution skew  though it is mainly designed to solve the redistribution skew problem that we have seen in various industrial applications discussed in section 1. one big advantage of the prpd algorithm is that it does not require major changes to the implementations of the sharednothing architecture since it does not require any new type of central coordination or communication among pus.
1. conclusions and future work
　one of the important challenges in parallel dbmses is to effectively handle data skew in joins. based on our observations of data warehouses in practice at various industries  we notice that skewed rows tend to be evenly partitioned on all parallel units. motivated by the redistribution skew problem that arises naturally in business applications in the various industries  we propose an approach called prpd  partial redistribution & partial duplication . to identify the skewed values  prpd uses either collected or sampled statistics. instead of redistributing these skewed rows as is done for the non-skewed rows  they are kept locally. the matching skewed value rows from the joined table are duplicated to each parallel unit to complete the join. the prpd algorithm also handles the extreme cases where skewed rows are clustered in only one or a few parallel units by redistributing the skewed rows to all parallel units.
　eliminating skewed processing eliminates system bottlenecks created by more conventional algorithms. our experimental results have demonstrated the effectiveness of the prpd algorithm in improving query execution time. we have also shown that the prpd algorithm can be used in multiple joins. in addition  we are looking at handling skew dynamically to avoid reliance on collected or sampled statistics.
