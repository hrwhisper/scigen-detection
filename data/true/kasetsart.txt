this paper describes our first large-scale retrieval attempt in trec-1 using dsir. dsir is a vector space based retrieval system in which semantic similarity between words  documents and queries  is interpreted in terms of geometric proximity of vectors in a multi-dimensional space. a co-occurrence matrix computed directly from the collection is used to build the underlying semantic space. we have implemented dsir on a cluster of lowcost pc pentium-class machines  and chosen the pvm message-passing library to manage our distributed dsir version. although our first adhoc retrieval results are quite poor in terms of recall-precision measure  we believe that more work and experiments have to be explored in order to obtain more promising retrieval performance.
1 introduction
for our first large-scale text retrieval attempt in trec1 adhoc experiments  we use our own retrieval artifact  called  dsir   a full-text retrieval system developed on a cluster of pc pentiums at the department of computer science  kasetsart university1. dsir stands for  distributional semantics based information retrieval   a retrieval model based on vector space. in this model  the contents of retrievable objects  such as words  phrases  sentences  documents  are represented in a unified way by multi-dimensional vectors. these vectors are derived

from a co-occurrence matrix computed on textual collection being indexed. semantic proximity among objects is then simply interpreted in terms of geometric proximity between corresponding vectors in the multi-dimensional space  called the  meaning space .
former source codes of dsir algorithm are written in c  and perl programming languages  running on a standard unix machine. to achieve trec-1 experiments  we reexport the distributed dsir to a cluster of lowcost pc pentiums  running linux operating system. each pc is hooked together through a low-cost ethernet local area network. new distributed version is developed using pvm1   a widely used message-passing software package.
we organize this paper in the following ways. section 1 gives a brief overview of fundamental concepts constituting dsir model. section 1 provides more detail about distributed dsir implementation. section 1 presents trec-1 retrieval experiments and gives the results. finally  section 1 concludes this paper.
1 dsir model
1 basic concept
research in distributional semantics concerns with the utilization of distributional information extracted from textual collections to represent the meaning of linguistic

entities  e.g. words  phrases  sentences  documents. we assume that there exists a correlation between meaning of a word and its observable distributional characteristics within particular contexts in a given language . these distributional characteristics can either be  occurrences  of that word itself  or its  co-occurrences  with the other words appearing within the documents.
in this retrieval approach  we are especially interested in using word contexts to characterize the meaning of a word  1  1  1  1 . in general  every word has meaning. each contributes its own meaning  according to its occurrence  to the whole content of the document in which it appears. here  we choose  word  as an elementary entity that holds the meaning. we consider tokens of length at least two characters  beginning with an alphabet  excluding those words in a pre-defined non-significant word list  as words  i.e. keywords or index terms  that constitute a set of vocabulary chosen for indexing a document collection. following the  distributional structure  definition of harris  1  1 :
 the distribution of an element will be understood as the sum of all its environments.  
we denote the  context  of a word as a knowledge concerning its usage  i.e. how that word is used with the other words in order to compose the content of a document. we characterize word contexts on the basis of  co-occurrence statistic . this choice is made because it is a source of distributional information that is easily extracted from a document collection.
we then define the co-occurrence statistic of a word as the number of times that word co-occurs with one of its neighbors within a pre-defined boundary. we denote this boundary  the  distributional environment . possible distributional environments can be sentences  paragraphs  sections  whole documents  or windows of k words.
the definition of this distributional environment is essential in our retrieval model. it is used to delimit the scope of the contexts which are of interest. co-occurrences measured within distributional environment defined by a sentence will let the  local  context information of words written in the documents to be observed. on the other hand  co-occurrencesmeasured within the environmentof a paragraph or the whole document will let the  global  context information of words to be examined. a window of k words can be used to extract the information between local and global contexts.
a specialized case of representing a word based on its contexts is that true synonyms will have identical contexts. near-synonyms or related words will have just similar contexts. on the other hand  in case of a polysemy  its contexts are different because its meanings are in general invoked with different sets of words in different contexts. representing the contents of documents on the basis of word contexts rather than just word occurrences thereby makes this retrieval model different from other standard keyword-based approaches. documents in the collections should be retrieved without difficulty even if a query is composed of synonyms or related terms.
in our computational model  we use a co-occurrence matrix illustrated in figure 1 to represent distributional information extracted from a document collection. each row in this matrix represents the distribution of a word   while each column represents the distribution of another word which appears close to . the intersection between row and column   i.e. the   records the cooccurrence frequency between and extracted from a document collection.

figure 1: co-occurrence matrix.
to represent meaning of a word according to its contexts by a vector  we depict each word distribution corresponding to row in the co-occurrence matrix by a vector using the sequence of j as its coordinate. each dimension of this vector is associated to word representing the column of the matrix. we hereafter
call this vector   co-occurrence vector .
therefore  if a co-occurrence matrix built from a document collection consists of i rows representing i word distributions  and j columns representing j word distributions  the meaning of these i words can  by this way  be projected onto a vector space of j dimensions by i corresponding co-occurrence vectors. we name hereafter this vector space   meaning space . figure 1 is supposed to illustrate the first three vector representations corresponding to words and in the first three-dimensional meaning space associated with words and   derived from a document collection.

figure 1: vector representation of words in a meaning space.
1 document representation
a full-text document consists of words. since we have already represented the meanings of words as vectors in the meaning space  our problem now is limited to define the vector representation of a documenton the basis of the co-occurrence vectors of words of which that document is composed. we propose to define the vector representation of a document using the weighted vector sum of the cooccurrence vectors corresponding to words occurring in that document. formally  if we choose i words  and j features  to index a collection of n documents  a document vector is written by:
	i	i	i
i
j
 1 
where w    is the weighting function addressing the importance of the word in document . since a query can be considered as a specific document  its vector representation is derived in the same way as those of documents. figure 1 below illustrates our document and query vector representations.

figure 1: document representation.
the document vector representation defined in equation  1  can be seen as an approximationofsemantic content of a document  because the  weighted  vector sum averages the direction of a set of vectors corresponding to words constituting that document. the intuition underlying this proposition is that a given document is composed of several words corresponding to different topics. if at least some of the words in a document are frequently used to described what the current topic is about then their corresponding co-occurrence vectors will pull the final vector sum towards the direction of that topic.
we also include the document vector components derived from the conventional vector space retrieval model  in our retrieval model. if we define ds as the component vector written in equation  1   and vs as the component vector conventionally derived from the standard vector space method  our final document vector representation can be written as follows:
	dsir	ds	vs	 1 
the parameter  which we call  hybrid parameter   takes the real value between 1 and 1. when is defined = 1  each document vector just takes the ds component vector. on the other hand  when is defined = 1  each document vector is derived from conventional vector space retrieval model.
1 document retrieval
since words  documents  and queries are represented as vectors in the same vector space  the basic retrieval operation in this retrieval model is then very simple; the query vector is compared to every document vector  and the documents whose vectors locate close to that query vector in the meaning space are presented to the user as relevant answer. these documents are returned in decreasingorder of their closeness.
in addition  other similarity comparison can be obtained as well. for example  it is easy to combine traditional keyword matching method during any retrieval operation since each keyword also has a corresponding cooccurrence vector in the same meaning space. the user can first use his keyword s  as query to filter for ranked documents which locate close to that keywords  and then select one  or several  of them as his new query to find the closest remaining documents of interest. in the same way  traditional relevance feedback  can easily be integrated into this retrieval model as well. during a retrieval process  the user can choose certain terms or documents  with weights  so that their vectors can simply be added up to the query vector to search more documents in the collection.
if we assume that there are chosen j distinct features for representing documents in a collection  a given document can then be written as a j-dimensional vector of the form:
	j	 1 
where represents the element of document vector . in the same way  a vector representation of a given query can be written as a j-dimensional vector of the form:
	j	 1 
a typical vector similarity measure that we use in this retrieval model is the cosine similarity function. this function represents the cosine of the angle between vectors of query and document in a j-dimensional vector space  which is written by:
j
	sim		 1 
	j	j
1 dsir implementation
dsir system consists of 1 parts; document preprocessing  co-occurrence matrix computation  document vector derivation  and document retrieval. we use a small perl routine to arrange trec adhoc collections to be ready for document preprocessing. document preprocessing in dsir integrates both porter and lovin stemmers  including standard stopword elimination.
distributed-dsir implementation uses master/slave or pool of tasks programming style on pvm platform . during co-occurrencematrix computation a big word-byword co-occurrence matrix is partitioned into small portions  each can be fit in physical memory of pc pentium machines  see the machine configuration in figure 1 . due to the fact that we have only one big local harddisk  each machine reads and performs co-occurrence computation on trec collections via nfs1. a scheduler  or master   the machine at which the trec collections is located  has responsible to manage the messages between machines in the pool. finally  that scheduler accumulates all co-occurrent matrix portions from other machines and writes them out to its local disk.

figure 1: distributed-dsir machine configuration during co-occurrence matrix computation and document vector derivation.

during document vector derivation  we still use the same machine configuration illustrated in figure 1. the big cooccurrence matrix is partitioned  each small portion is distributed through other machine. the main scheduler then reads each trec-1 document from the collections to see which co-occurrence vectors are needed to be retrieved from other machines to compose that document vector. a careful design of caching strategy during document vector derivation is necessary in order to reduce message passing between scheduler and other machine in the pool in order not to saturate the network bus.
distributed document retrieval algorithm in dsir is quite straightforward  see the machine configuration in figure 1 . each machine in the pool  called  retrieval engine   reads portion of document vector into its main memory  and waits for retrieval command from the central scheduler. the central scheduler reads each query vector  and distributes it to every retrieval engine. each retrieval engine retrieves and ranks its document vectors  the ones which are close to the query vector are ranked first  and send its ranking back to scheduler. note that during this phase  retrieval engines perform their retrieval tasks in parallel. then scheduler accumulates all rank lists from retrieval engines  and performs the final ranking scores.

figure 1: distributed-dsir machine configuration during document retrieval.
1 experimental results and discussion
we participate quite lately our trec-1 adhoc experiments  i.e. in may 1. that means we have only 1 months before the official deadline to prepare and scaleup dsir on our pc cluster to perform this large-scale text retrieval task. dsir adopts standard smart stoplist  and uses lovin stemmer to pre-process all adhoc documents. for each run  two term sets are chosen by document and occurrence frequency criterion to build a cooccurrence matrix. we also applied the chi-square correcting weight  which we call  spatial transformation    to every entry in the co-occurrence matrix. that means  each co-occurrence entry is transformed to
          where is defined as row total  and is defined as column total.
document vectors are computed using formulae explained earlier in equations  1  and  1 . dsir uses 'aaa.bbb'smart weighting style  during indexing the documents. query vectors are calculated in the same way  using words found in  title  and  description  field of the topics 1. we submitted two adhoc run; dsir1 a1 and dsir1 a1. table 1 gives the values of differentdsir parameters  and results.
rectly from the whole trec-1 collections is confused by several domains that are specific to those trec-1 collections themselves. dsir should work better when derived word contexts are learned from a specific domain of interest. thus  we plan to use dsir to index and search trec-1 collections separately so that each semantic space  i.e. meaning space  has been derived from word co-occurrences more correctly with respect to a certain domain.
1 conclusion
in this paper  we introduce our dsir retrieval system that has been tuned to perform large-scale text retrieval in trec-1 adhoc track. dsir is a distributional semantics based retrieval model in which semantic proximity is derived from a co-occurrence matrix calculated from the textual collection being indexed. words  documents  and users' queries  are represented in a unified way by vectors in a multi-dimensional space. retrieval is performed on the basis of the geometric proximity between vectors representing documents and the user's query; documents whose corresponding vectors are closed to that of query are returned as relevant answer.
run	index	features	weighting	av. p ported our distributed dsir on a cluster of low-cost pc
dsir1 1 1 ntc.atc 1pentium machines using pvm framework. since the dsir1 1 1 ntc.atc 1results of our first large-scale retrieval attempt in thisto achieve trec-1 adhoc experiments  we have reex-

table 1: indexing parameters used in trec-1  and results.
considering our first official testing of dsir over the trec-1 collection set  we found that the recall/precision results were very bad. this level of performance is far below dsir's typical performance tested over the old standard  very small size collections such as cranfield and time. we look forward to getting a more complete set of experiments and to spending more time understanding the situation in which dsir had difficulty in identifying relevant documents. we believe that the word contexts that dsir derived from the co-occurrence matrix built ditrec-1 are not quite successful  in terms of recallprecision measure  more work and experiments must be continued.
