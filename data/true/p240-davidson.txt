clustering with constraints is an emerging area of data mining research. however  most work assumes that the constraints are given as one large batch. in this paper we explore the situation where the constraints are incrementally given. in this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution. we consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than re-clustering the entire data set. we show that the problem of incremental clustering under constraints is np-hard in general  but identify several sufficient conditions which lead to efficiently solvable versions. these translate into a set of rules on the types of constraints that can be added and constraint set properties that must be maintained. we demonstrate that this approach is more efficient than re-clustering the entire data set and has several other advantages.
categories and subject descriptors
h.1  information systems : database management. data mining.
general terms
algorithms  experimentation  theory
keywords
clustering  constraints  algorithms
1. introduction and motivation
　the last five years have seen extensive work on incorporating instance-level constraints into clustering methods  1  1  1 . constraints provide guidance about the desired partition and make it possible for clustering algorithms to perform better  sometimes dramatically. instance-level constraints specify that two items must be placed in the same cluster
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  san jose  california  usa.
copyright 1 acm 1-1-1/1 ...$1.
 must-link  ml  or in different clusters  cannot-link  cl . this semi-supervised approach has led to improved performance for several uci data sets as well as for real-world applications  such as person identification from surveillance camera clips   noun phrase coreference resolution  gpsbased map refinement  and landscape detection from hyperspectral data .
　however  most constrained clustering work involves batch style specification of the constraints and the subsequent running of the clustering algorithm. in many applications  it may be reasonable for constraints to be incrementally added or removed based on feedback from a user. this can be viewed as the user critiquing a clustering and can be repeated often until the user is happy with the resulting clustering. furthermore  it is known that this approach of selectively choosing constraints produces significantly better results than randomly choosing constraints. consider the situation of document clustering as described by cohn  caruana and mccallum . after the clustering is performed  the user has the option of critiquing the clustering by implicit feedback   a document does not belong in this cluster   and explicit feedback   a document belongs here  or  these documents should be together/apart  . this incorporation of a user into the clustering loop is analogous to active learning  except that the constraints are chosen by a user rather than by a machine. cohn et al.  explore the use of this type of active constraint generation in clustering. in the area of document clustering using the reuter's newsgroup data  they simulate actively chosen constraints  by picking two instances placed in the same cluster but with known different categories/labels . ten constraints chosen using this approach produces as good results as using between 1% to 1% more constraints that are randomly chosen. furthermore  davidson  wagstaff and basu  have recently shown that there is a significant variation in the benefits  when measured by purity on the class labels  provided by different sets of constraints. this was shown to be the case despite each constraint set having the same number of constraints from the same source  set of labels . this provides further reason to believe that user-specified constraints are better than randomly chosen constraints.
　the works of cohn et al.  and davidson et al.  illustrate that incremental constrained clustering offers tremendous potential improvement over batch style constraint processing and that not all constraint sets are equal in terms of their effect on the final solution. however  the work of cohn et al. required the clustering algorithm to be re-run after each set of constraints is added  an approach we refer to as non-incremental constrained clustering or total reclustering. this is an extremely time consuming exercise if the data set is large. in addition  the user may have liked the existing clustering and there is no guarantee that a similar clustering would be found after reapplying the algorithm. we provide experimental evidence to support of this claim.
contributions of this paper. the purpose of this work is to investigate the conditions under which incremental clustering with constraints can be carried out efficiently with large data sets. to do this  we allow the addition and removal of constraints and try to find clusterings that can be obtained by making a small number of changes to a given clustering. we begin in section 1 by defining several versions of the incremental constrained clustering problem. these problems encompass reasonable methods of specifying feedback in the form of constraints. in general  incrementally adding constraints is intractable; further  incrementally removing constraints is intractable when a reasonable secondary objective such as minimizing the partition diameter is specified. these results are in shown in section 1. because of these general intractability results  our focus is on finding sufficient conditions which give rise to restricted versions of the problem that can be solved efficiently. these sufficient conditions can be used to provide instructions to a user regarding the properties to be satisfied by constraints so that a given partition can be efficiently updated to satisfy the new constraints. our sufficient conditions are described in sections 1 and 1  and a greedy update algorithm that uses these conditions is described in section 1. section 1 presents experimental results to show that our incremental constrained algorithm is more efficient than re-running the batch style constrained clustering algorithm and even produces better quality results compared to batch style constrained clustering algorithms that attempt to satisfy all constraints.
1. problem definitions
assumptions. to provide a proper perspective for our contributions  we mention the assumptions used in this work. firstly  our earlier work on the feasibility problem  1  1  1  addressed the question of determining whether there is a partition that satisfies a given set of constraints. in this paper  we assume that a clustering satisfying all the initial constraints  c  is given and that the addition of new constraints  c  may give rise to a combined set of constraints for which no feasible clustering exists for the given value of k  the number of clusters . we also assume that the value of k cannot be changed when constraints are added or removed. without loss of generality  we study the addition of only one constraint at a time. the addition of multiple constraints can be carried out by repeating the procedure for each constraint. finally  we assume that adding a new constraint does not cause an obvious contradiction such as ml x y  and cl x y .
　in an ideal situation  the user should be able to specify several different forms of feedback such as:
 a  an instance x does not belong in cluster q.
 b  an instance x should be in cluster q.
 c  two instances x and y must be in the same cluster.
 d  two instances x and y shouldn't be in the same cluster.
feedback types  a  and  b  provide label information for an instance while  c  and  d  provide constraints on two instances. it should be noted that it is possible to unambiguously translate a set of labels on instances to a set of constraints but the converse is not true. for this reason  we assume that all feedback from the user is in the form of must-link and cannot-link constraints. while these constraints will not be able to efficiently encode all types of feedback  e.g. limits on cluster sizes   they can model many useful forms of feedback.
　we now formally define the incremental clustering problems addressed in this paper. the incremental nature of the problems are with respect to a  adding constraints and b  removing constraints. when a change  i.e.  the addition or deletion of constraints  is specified  the goal is to efficiently update a given clustering  without running the base clustering algorithm. in updating a partition  one may also be interested in optimizing a suitable objective function  e.g. the objective function used by the algorithm that produced the original clustering  or the number of changes to the given partition.
　throughout this paper  a clustering is a partition of the given set of points. we use the term  k-clustering  to mean a partition with k subsets. several objective functions are known  e.g. vector quantization error  partition diameter  partition purity  for measuring the quality of a partition. given a partition Π  we use f Π  to denote the quality of the partition as given by the objective function. some objective functions represent minimization objectives  e.g. vector quantization error  partition diameter  while others  e.g. partition purity  are maximization objectives. we will formulate the incremental clustering problems assuming that f is a minimization objective. we will also indicate the changes to the formulation when f is a maximization objective.
　we begin by formulating the incremental clustering problem under the addition of new constraints. these formulations assume that the new clustering  when it exists  should also improve the objective function. we will comment on this issue after the problem specification.
　problem 1. incremental clustering under constraint addition. let s be a set of points and let c be a set of constraints. let Π be a k-clustering of s satisfying all the constraints in c. let c be a new set of constraints such that . is there a k-clustering  such that  i  Π satisfies all the constraints in and  ii 
  if so  find a partition Π  for which f Π   is a minimum among all partitions that satisfy conditions  i  and  ii .
　when constraints are added  the most desirable outcome is that there is a feasible partition for the new set of constraints and the partition improves the value of the objective function. however  this may not be the case in general.
when there is no feasible partition for the constraint set
  a system can ask the user to choose a different set of constraints. another possibility is that there are feasible partitions for the new constraint set  but none of these partitions improves the objective function value. in such a case  an algorithm would try to produce a partition Π such that the increase in the value of the objective function is as small as possible. it is again up to the user to decide whether or not the new constraints should be used.
　in the statement of problem 1 if f is a maximization objective  the condition    Π   would be replaced by    Π   and one would seek an optimal partition Π  for which f Π   is a maximum among all partitions that satisfy the conditions.
　we now define the natural analog of the constraint addition problem  namely the constraint removal problem.
　problem 1. incremental clustering under constraint removal. let s be a set of points and let c be a set of constraints. let Π be a k-clustering of s satisfying all the constraints in c. let be a subset constraints to be removed. find a k-clustering Π  of s such that f Π   is a minimum among all the k-clusterings that satisfy the constraints in.
　the formulation of the problem under constraint removal is slightly different from that of constraint addition since feasibility is not an issue in the former case; the original partition will itself satisfy the new set of constraints. so  the focus is on optimizing the objective function value.
1. worst case complexity results
this section presents the following results.
 a  the problem of determining whether there is a feasiblesolution under the addition of a single ml or cl constraint is computationally intractable. these results do not depend on optimizing the objective function. they are discussed in sections 1 and 1.
 b  as mentioned earlier  the feasibility problem is trivial under constraint removal. however  we show that if the goal is to delete a constraint and minimize the partition diameter  the problem is computationally intractable. this result is presented in section 1.
　we have also obtained complexity results for other forms of incremental clustering such as removing points  not constraints  to reduce cluster diameter. these results are not shown here due to space reasons. we believe that these are somewhat specialized versions of the incremental clustering problem and that practitioners are likely to be more interested in addition and removal of constraints.
　the remainder of this section  which proves the results mentioned in  a  and  b  above  can be skipped on first reading of this paper without loss of flow.
1 incremental clustering under the addition of a must-link constraint
　given a set of points  a set of constraints and a partition of the point set into k subsets such that all the given constraints are satisfied  we want to determine whether the addition of a new ml constraint can cause infeasibility. in this section  we examine the complexity of this problem. we begin with a precise formulation of the problem.
incremental feasibilitytesting: adding an ml constraint  ift-ml 
instance: a set s = {s1 s1 ... sn} of n points; an integer k  1 ＋ k ＋ n; a set c of constraints; a partition Π of s into k subsets such that Π satisfies all the constraints in c; a constraint ml si sj  which is not in c.
1. let	g v e 	be	the	given	graph 	with	v	=
{v1 v1 ... vn}.
1. construct graph g1 v1 e1  as follows.
 a  let va = {a1 a1 ... an} and vb = {b1 b1 ... bn}. let v1 = v “ va “ vb.
 b  let
　ai vi   i vi : 1 i n . let denote the set of edges {{ai bi} : 1 ＋ i ＋ n}.
let.
figure 1: a graph construction procedure

required: determine whether there is a partition of s into

k subsets such that all the constraints in c “ {ml si sj } are satisfied. if so  output one such partition Π.
　the main result of this section is that unless p = np  there is no efficient algorithm for the ift-ml problem. before we can prove this result  we need to introduce some preliminary definitions and results.
1.1 preliminary definitions and results
　recall that the graph 1-colorability problem is np-complete . in fact  the problem remains np-complete even when restricted to graphs in which the maximum node degree is 1 . we refer to this restricted version of the graph 1-colorability problem as the restricted 1-colorability problem  r1cp . the following result is proven in .
　theorem 1. the restricted 1-colorability problem is np-complete.	
　we also need another result from graph theory. this well known result  called brooks's theorem  gives an upper bound on the number of colors needed for a graph in terms of the maximum node degree. the proof of the following theorem can be found in .
　theorem 1.  brooks's theorem  any undirected graph with a maximum node degree of Δ can be colored using at most Δ+1 colors. moreover  such a coloring can be obtained in polynomial time.	
　let g v e  be an undirected graph with v = {v1 v1 ... vn}. figure 1 describes a procedure for constructing another graph g1 from g. informally  this construction adds two new nodes for each node of g and adds three edges that connect the three nodes into a complete graph. an example of this construction is shown in figure 1. some key properties of this construction are shown in the following lemma.
　lemma 1. suppose g is an undirected graph with n nodes and a maximum node degree of 1. let g1 be the graph obtained from g using the construction described in figure 1. the following properties hold.
 a  g1 is 1-colorable. moreover  a 1-coloring of g1 can be constructed in polynomial time.
 b  there is a 1-coloring of g1 in which all the nodes in va = {a1 a1 ... an} have the same color and all the
	v1	v1	
	v1	v1	a1	v1	v1
	 i 	 ii 
figure 1: an example for the graph construction procedure described in figure 1.  the given graph g is shown in  i  and the graph g1 resulting from the construction is shown in  ii . the new edges added to produce g1 are shown as dashed lines. 

nodes in vb = {b1 b1 ... bn} have the same color if and only if g is 1-colorable.
proof:
part  a : note that g has a maximum node degree of 1. thus  by brooks's theorem  g is 1-colorable and one such 1coloring can be obtained in polynomial time. a 1-coloring of g1 can be constructed efficiently from that of g as follows. consider any node vi of g and let j denote the color assigned to vi. choose two different colors other than j and assign them to ai and bi respectively. by repeating this process for each vertex of g  we obtain a 1-coloring of g1.
part  b :
if part: suppose g is 1-colorable  and let the colors used be
. use color 1 for all the nodes in va and color 1 for all the nodes in vb. clearly  this is a 1-coloring of g1 in which all the nodes in va have the same color and all the nodes in vb have the same color.
only if part: suppose there is a 1-coloring of g1  using colors

{1 1 1}  in which all the nodes in va have the same color and all the nodes in vb have the same color. note that the color assigned to the nodes in va must be different from the one assigned to the nodes in vb since g1 has the edge {a1 b1}. so  let 1 and 1 denote the colors assigned to the nodes in va and vb respectively. now  consider any node vi of g. since vi is adjacent to both ai and bi  colors 1 and 1 cannot be used for vi. in other words  each node of g must be colored 1  1 or 1; that is  g is 1-colorable. 
1.1 complexity of the ift-ml problem
　theorem 1. unless p = np  there is no polynomial time algorithm for ift-ml problem.
proof: suppose there is a polynomial time algorithm a for the ift-ml problem. we will show that a can be used to solve the restricted 1-coloring problem  r1cp  in polynomial time. since r1cp is np-complete  theorem 1   this would contradict the assumption that p = np.
　consider any instance of the r1cp problem with g v e  being a graph in which the maximum node degree is 1.

1. let g v e  be the given graph with maximum node degree 1  with v = {v1 v1 ... vn}.
1. construct graph g1 v1 e1  from g as described in figure 1. let va = {a1 a1 ... an}  vb =
{b1 b1 ... bn} and e1 = {e1 e1 ... em}.
1. create a set of points s = x “ y “ z  where x = {x1 x1 ... xn}  y = {y1 y1 ... yn} and z = {z1 z1 ... zn}.  the sets x  y and z are in oneto-one correspondence with sets v   va and vb respectively. coordinates are not specified for the points in s since they play no role in the algorithm. 
1. for each edge ej （ e1  create the constraint cj = cl x y   where x and y are the points corresponding to the two nodes joined by ej  1 ＋ j ＋ m. initialize the constraint set c to {c1 c1 ... cm}.
1. find a 1-coloring of g1 as explained in the proof of lemma 1. let wr be the set of nodes of g1 such that all nodes in wr have color r  1 ＋ r ＋ 1. create a partition Π of s into 1 subsets s1  s1  s1  s1 and s1  where sr has the points corresponding to the nodes in wr  1 ＋ r ＋ 1.  note that Π satisfies all the constraints in c. 
1. let l denote the list of the following 1n   1 ml constraints:    ml y1 y1   ...  ml yn 1 yn   ml z1 z1 th  ml z1 z1   ...  ml zn . let li denote the i constraint in l  1 ＋ i ＋ 1n   1.
1. for i = 1 to 1n   1 do
 a  execute algorithm a on s  with constraint set c and partition Π to determine whether there is a feasible solution when constraint li is added.
 c  if algorithm a returns  yes  along with new partition Π then
let c = c “ {li} and Π = Π.
else
　　　　　　　　print  g is not 1-colorable  and stop. 1. print  g is 1-colorable .
figure 1: algorithm b used to prove theorem 1

we refer to the algorithm in figure 1 as b. this algorithm constructs the graph g1 from g  as described in figure 1   creates a set s of points corresponding to the nodes of g1  and uses the edges of g1 to create the initial set c consisting of cl constraints. the algorithm produces an initial partition of s into 1 subsets using the result of lemma 1 a . the algorithm then successively adds the ml constraints ml y1 y1   ml y1 y1   ... ml yn 1 yn 1   ml z1 z1   ml z1 z1   ... ml zn 1 zn 1   one at a time  and invokes algorithm a to determine whether there is a feasible solution after each addition. in terms of g1  it can be seen that the effect of above collection of ml constraints is to force all nodes in va to have the same color and all nodes in vb to have the same color. thus  from lemma 1 b   it follows that algorithm a can produce a feasible solution satisfying all the original constraints and the new ml constraints if and only if g is 1-colorable.
　thus  algorithm b correctly decides whether g is 1-colorable. in figure 1  it is easy to see that all steps except step 1 can be carried out in polynomial time. in step 1  algorithm b makes at most 1n   1 calls to algorithm a. thus  if algorithm a runs in polynomial time  then so does algorithm b. in other words  we have a polynomial time algorithm for the r1cp problem. this contradicts our assumption that p = np and completes the proof of theorem 1. 
1 incremental clustering under the addition of a cannot-link constraint
　in this section  we present our complexity result for incremental clustering when a cl constraint is added. the problem formulation is as follows.
incremental feasibility testing: adding a cl constraint  ift-cl 
instance: a set s = {s1 s1 ... sn} of n points; an integer k  1 ＋ k ＋ n; a set c of constraints; a partition Π of s into k subsets such that Π satisfies all the constraints in c; a constraint cl si sj  which is not in c.
required: determine whether there is a partition of s into k

subsets which satisfies all the constraints in c“{cl si sj }. if so  output one such partition Π.
　the following theorem points out the difficulty of obtaining an efficient algorithm for the ift-cl problem.
　theorem 1. unless p = np  there is no polynomial time algorithm for the ift-cl problem.
proof: suppose there is a polynomial time algorithm a1 for the ift-cl problem. we will show that a1 can be used to devise a polynomial time algorithm for the k-coloring problem. this would contradict the assumption that p = np.
　let g v e  and integer k represent the given instance of the k-coloring problem. we refer to the algorithm shown in figure 1 as b1. algorithm b1 starts with an empty set c of constraints and an arbitrary partition of the set s into k subsets.  since the constraint set c is empty  any partition of s is a feasible solution.  for each edge of the graph  the algorithm creates a new cl constraint and invokes algorithm a1 to determine whether there is a feasible solution when the new constraint is added. we now show that b1 correctly decides whether g is k-colorable.
　suppose algorithm b1 outputs the message g is k-colorable . then  from the description in figure 1  algorithm a1 produced a feasible partition into k subsets after all the cl constraints corresponding to the edges of g were added. let s1  s1  ...  sk denote the resulting subsets. consider the coloring of g obtained by assigning color j to all the nodes corresponding to the points in sj  1 ＋ j ＋ k. this coloring uses k colors. further  for any edge {vx vy} of g  the constraint cl sx sy  ensures that sx and sy are in different subsets of the partition. in other words  nodes vx and vy have different colors. thus  we have a valid k-coloring of g. suppose algorithm b1 outputs the message  g is not kcolorable . thus  at some stage  algorithm a1 must have returned no in step 1 c  of figure 1. we prove by contradiction that there is no valid k-coloring of g. suppose g is k-colorable  and let 1  1  ...  k denote the colors used. thus  there is a partition of the node set v into k subsets v1  v1  ...  vk such that vj is the set of all nodes assigned color j  1 ＋ j ＋ k. consider the partition of s into k subsets s1  s1  ...  sk  where sj contains all the points corresponding to the nodes in vj  1 ＋ j ＋ k. clearly  this partition satisfies all the cl constraints added by algorithm b1. thus  algorithm a1 cannot return  no  at any stage. this contradiction shows that g has no valid k-coloring.
　thus  algorithm b1 correctly decides whether g is kcolorable. in figure 1  it is easy to see that all steps except step 1 can be carried out in polynomial time. in step 1  algorithm b1 makes at most m = |e| calls to algorithm a1. thus  if algorithm a1 runs in polynomial time  then so does algorithm b1. in other words  we have a polynomial time algorithm for the k-coloring problem. this contradicts our assumption that p = np and completes the proof of theorem 1.	
1 removing constraints to reduce partition diameter
　we now discuss the flip-side problem of removing constraints. since we are given a partition that already satisfies a given set of constraints  such a partition will trivially also satisfy any subset of these constraints. instead we shall explore whether there exists a new partition which minimizes the clustering diameter. we begin by first defining the clustering  partition  diameter.
　definition 1. suppose s = {s1 s1 ... sn} is a set of n points with a distance d si sj  for each pair of pointe si and sj in s. let Π be a partition of s into k subsets  clusters  s1  s1  ...  sk. the diameter of cluster si  denoted by dia si   1 ＋ i ＋ k  is given by dia si  = max{d sx sy  : sx sy （ si}.
the diameter of the partition Π  denoted by dia Π   is given by
dia Π  = max{dia si  : 1 ＋ i ＋ k}.
pruning constraints to decrease diameter  pcdd 
instance: a set s = {s1 s1 ... sn} of n points; distance d si sj  for each pair of pointe si and sj in s; an integer k  1 ＋ k ＋ n; a set c of constraints; a partition Π of s into k subsets such that Π satisfies all the constraints in c and dia Π  = σ1; a subset of constraints and a number σ1   σ1.
1. let	g v e 	be	the	given	graph 	with	v	=
{v1 v1 ... vn} and e = {e1 e1 ... em}.
1. create an arbitrary set s = {s1 s1 ... sn} of n points.  note that s is one-to-one correspondence with the node set v . coordinates for the points in s are not specified since they play no role in the algorithm.  initially  the constraint set c is empty. create an arbitrary partition of s into k subsets. let Π denote this partition.
1. for i = 1 to m do
 a  let edge ei join nodes vx and vy. create a new constraint c = cl sx sy .
 b  execute algorithm a1 on s  with constraint set c  partition Π and new ml constraint c to determine whether there is a feasible solution.
 c  if algorithm a1 returns  yes  along with new partition Π then
let c = c “ {c} and Π = Π.
else
print  g is not k-colorable  and stop.
1. print  g is k-colorable .
figure 1: algorithm used to prove theorem 1

question: is there a partition Π blocks such that Π satisfies all the constraints in and dia  
　theorem 1. the pcdd problem is np-complete. moreover  the result holds even when the given constraint set contains only ml constraints and the number of constraints to be removed is 1.
proof idea: the proof involves a reduction from k-coloring and is omitted due to space reasons.
1. easy instances of incremental constrained clustering problems
　in this section we introduce several sufficient conditions that despite our previous worst case results  give rise to situations where there are efficient algorithms to find a feasible clustering to satisfy the new and old set of constraints. we begin by first describing the basic conditions and then move on to non-trivial conditions. in section 1 we put these results together into a greedy algorithm that we experimentally verify in section 1 and compare its performance against non-incremental constrained clustering.
1 basic sufficient conditions
　we present the following basic situations with little explanations and no proofs. without loss of generality we discuss a single constraint on points x and y. recall that a clustering Π that satisfies the constraint set c is given and that the additional constraint is denoted by c.
figure 1: a constraint graph for ml a b   ml a c   ml d e   ml f g   ml h i   ml j k   cl a l   cl l j   cl d i   cl d l 

  the constraint in c is on a pair of points that are not already involved in a constraint in c. in this situation  if the constraint is not already satisfied  we move the constrained points  x and y  together  for ml  or apart  for cl  in such a way to minimize the objective function f. for ml and cl constraints  this would use respectively o k  and o k1  evaluations of the function f.
  the constraint in c involves a pair of points such that only one of the points is involved in an existing constraint in c. since one of the points is unconstrained  it can be easily moved to satisfy the constraint. for an ml constraint  this does not need any evaluation of the function f. for a cl constraint  this uses o k  evaluations of f.
  all of the constraints in c and c are must-link constraints. in such a situation we can efficiently recompute the transitive closure.
1 non-trivial sufficient conditions
　the non-trivial situations occur when both constrained points x and y are already involved in constraints in c. this is precisely the situation that is exploited in proving our worst case results. for these non-trivial situations we make use of several sufficient conditions which make the incremental clustering problem easy. we now describe these sufficient conditions and the implications on the limitations of what constraints can be included in c.
1.1 brooks's theorem
consider clustering under the set of constraints: ml a b  
ml a c   ml d e   ml f g   ml h i   ml j k   cl a l   cl l j   cl d i  and cl d l  as graphically shown in figure 1. the edges represent cl constraints and must-linked points are represented by a single node. we refer to such a graph as a constraint graph. suppose we wish to cluster the data for k = 1. proceeding in the order of the points in the data set  alphabetically   there could be an assignment of abc to cluster 1  de to cluster 1  jk hi and fg to cluster 1  but then point l can't be assigned to any cluster feasibly. it is not that the graph is not 1-colorable  rather the ordering of the nodes presents a problem. this is of pragmatic importance  since the ordering of the points as processed by clustering algorithms is typically fixed apriori and does not change.
　brooks's theorem  see theorem 1  points out that if the maximum node degree of a graph g is Δ and k − Δ + 1  then a k-coloring of the graph is easy to obtain using any linear ordering of the nodes. this means that if the maximum number of cl-constraints involving the same point is at most k   1  then the feasibility problem is guaranteed to be easy; otherwise  the feasibility problem may be difficult.
figure 1: a graphical representation of the situation where the constraint to add is either a ml or cl constraint for k = 1. the number in parentheses is the cluster number the point is assigned to. we discuss single constraints without loss of generality.

　the above discussion points out one way of making the incremental constrained clustering problem easy. we can add constraints without restriction except to make sure that no single point is involved in k or more cl constraints  including given and entailed constraints.
　we can use this sufficient condition to make the incremental addition of a cl constraint easy  but the addition of an ml constraint may still involve a large number of cluster reassignments. why this is the case can be explained by examining figure 1. consider the top graph. both x and y are already constrained and are in the same cluster. the new constraint to add is cl x y . since we want to use brooks's theorem  both x and y will have at least one free  color/cluster-id. in our example  we can assign x to cluster 1 or y to cluster 1  whichever choice optimizes the chosen objective function f. in general  this operation may use o k1  evaluations of the function f. however  if the new constraint to add is ml x y   no such easy reassignment may exist  as can be seen by considering the lower diagram in figure 1. clearly  x cannot be placed in y's cluster and vice-versa. the only solution is to place x and y together in cluster 1  1 or 1 and then assign those points in the chosen cluster to which x and y are cannot-linked to another cluster  while making sure that none of their constraints are violated.
　note that brooks's theorem states that a Δ + 1 coloring is possible regardless of the order in which nodes are considered. however  there may exist a coloring using far less the Δ + 1 colors. the notion of inductiveness of a graph  explored in the next section  provides a particular ordering of the nodes and hence can significantly reduce the number of colors needed.
1.1 inductiveness of a constraint graph
　consider the example in figure 1. according to brooks's theorem  this graph is four-colorable; actually  the graph is two-colorable. here  we examine another graph property which generalizes brooks's result to give a stronger upper bound on the number of colors. more importantly  the property can be used to order the instances in the training data set to make the feasibility problem easy. the value of q  to be defined  along with this ordering gives us another sufficient condition which if satisfied when adding in new constraints will allow for an efficient incremental clustering algorithm.
the following definition is from .
　definition 1. let q be a positive integer. an undirected graph g v e  is q-inductive if the nodes of g can be assigned distinct integer values in such a way that each node has an edge to at most q nodes with higher assigned values.
　to illustrate this definition  consider a star graph g v e  with n nodes. let v1 denote the center node  with degree n   1  of the star and let vi  1 ＋ i ＋ n   1 denote the other n 1 nodes  each of which has degree 1 . assign the integer 1 to node v1  1 to node v1  ...  n   1 to node vn 1 and n to node v1. this creates the following linear ordering of the nodes: . examining the nodes in this order  it can be seen that each node has an edge to at most one node with a higher assigned value  each node = 1  has one edge to v1 .
　the usefulness of q-inductiveness is shown in the following theorem from .
　theorem 1. suppose g v e  is q-inductive. g can be colored using at most q + 1 colors.	
　thus  the star graph is 1-inductive and is two-colorable  whereas brooks's theorem would state that it is n colorable. as a further example for the graph in figure 1  the following is a 1-inductive ordering: fg  abc  jk  l  de and hi; hence the graph is 1-colorable.
　the proof of the above theorem actually provides an algorithm that colors the graph g using at most q+1 colors. in particular  the algorithm colors the nodes in an order that is the reverse of the given q-inductive ordering. this translates into the reasonable heuristic of ordering the instances to cluster from the most constrained to the least  if at all  constrained.
　we can make use of existing instance ordering given by the q-inductiveness of a constraint graph to efficiently recolor  re-assign  instances given additional new constraints in linear time to the number of instances to cluster. indeed the situation will be similar to that of figure 1. since the q-inductiveness ordering ensures that the most constrained points are assigned clusters first  there will always be one free color/cluster-id for both x and y.
　it should be noted that the instances may need to be reordered when new constraints are added to the old set.
1. putting it all together: an efficient incremental constrained clustering algorithm
　it should be noted that the following algorithm is only correct when our basic and non-trivial conditions listed in sections 1 and 1 occur. the algorithm takes an input a single constraint at a time and depending on the properties of the constraint will attempt to greedily optimize the objective function f. if the constraint does not improve f then the constraint is passed over and the user chooses another. furthermore  since finding a clustering to satisfy a must-link constraint between two points that are already constrained by cannot-link constraints cannot be done efficiently  we also inform the user of this situation and pass over the constraints. the algorithm is shown in its entirety in figure 1.
figure 1: graph of the average kl distance  over 1 repetitions  between the initial constrained clustering under 1 randomly chosen constraints and the clustering obtained from these 1 and additional constraints  1 to 1 . series -x- indicates incremental constrained clustering results and series -o- indicates non-incremental total constrained reclustering results. number of clusters k is set to number of extrinsic classes.

1. experimental results
　we now present results from testing the algorithm described in figure 1 under the stronger sufficient condition of k   q + 1. our first set of experimental results shows that the approach of adding in new constraints and then totally reclustering the data  i.e. the non-incremental/totalreclustering approach  using cop-k-means  has several significant limitations. however  if there are substantially many more constraints to be added this is a reasonable approach. the second set of experimental results shows that our incremental clustering algorithm is efficient with respect to the number of instances moved and interestingly the resultant clustering quality is typically better than total reclustering of all the available data under the constraints.
1 limitation of total re-clustering
　we take six uci data sets and randomly choose to keep only 1% of the data point labels. from these labeled points we will generate ml constraints  when the labels agree  and cl constraints  when the labels disagree  by randomly choosing two points at a time. we begin by clustering using cop-k-means  the various uci data sets with twenty initial randomly chosen constraints  c  to produce the initial clustering Π. the cop-k-means algorithm attempts to find a set partition with minimum vector quantization error that satisfies all of the constraints. we randomly choose more constraints which are then given to our greedy incremental clustering algorithm  figure 1  one at a time. the algorithm will accept an additional twenty constraints   . note that these twenty additional constraints are actively chosen in the sense that they reduce the objective function f  which in this case is the average label purity over all clusters over all points.
　in the non-incremental clustering case  each constraint is added one at a time to c  we recluster the data with the initial twenty and progressively added constraints. we then report the difference between the new clustering and the initial clustering with respect to the kullback-leibler  kl  distance of the respective mixture distributions each clustering defines. it should be noted that  for each run  we use the same random number seed and hence the same cluster centroids; so  the only thing that changes from experiment to experiment is the set of additional constraints.
　these results  shown in figure 1  indicate that the approach of total re-clustering can lead to significantly different clusterings compared to the initial clustering  while the results of incremental clustering remain rather similar to the initial clustering even for up to 1 incrementally added constraints. this will often be desirable  since the user may prefer the existing clustering for application-oriented reasons. adding in a single constraint and reclustering can give vastly different results  particularly if the new constraint contains instances that are towards the start of the ordering of how the instances will be assigned. for a fair comparison to see how different the initial and final clusterings would be  we gave the non-incremental constrained clustering algorithm the constraints chosen by our algorithm. we will not do this for the remainder of our experimental section.
　it is tempting to think that since non-incremental clustering can greatly change a clustering solution  then why not just fix all instances not involved in the new constraints. however  due to the transitivity of ml constraints  ml x y   ml y z  ★ml x z   and the entailment property of cl constraints  ml a b  ml c d  cl a c  ★ cl a d  cl b c   cl b d   many points not explicitly in c have additional constraints placed on them and the points that can be fixed may be quite small. furthermore  fixing points to be in a particular cluster may lead to the situation where a partition that satisfies the joint constraint set    may not exist.
1 benefits of greedy incremental algorithms
　in this section we show that incremental clustering as expected is more efficient and also that the results obtained by using an incremental algorithm compare favorably with non-incremental  total-reclustering  cop-k-means.
　we repeat a similar set of experiments as before  except that this time we measure how much  work  is performed by each algorithm. for incremental clustering this is measured as the number of instances moved from one cluster to another. for non-incremental clustering it is the number of changed assignments from iteration to iteration for all iterations until the algorithm converges. results are shown in figure 1.
　when comparing the accuracy of incremental constrained clustering with two styles of non-incremental constrained clustering  cop-k-means and basu  bilkeno and mooney's mkm algorithm  that learns a distance function  we see some interesting results. firstly  when compared to algorithms that attempt to satisfy all constraints  as it does  incremental constrained clustering compares favorably and indeed performs better. this is to be expected as by incrementally specifying the constraints  rather than all at once  the incremental algorithm does not get over-constrained as the non-incremental variant can .
　however  both non-incremental and incremental algorithms that satisfy all the constraints are typically outperformed by distance learning algorithms. the latter class of algorithms interpret ml a b  and cl x y  as indicating that a and b should be close together and x and y should be far apart in some learnt distance function. the reason why these approaches perform better than those satisfying all constraints is that learning a distance metric means not only that a and b are closer together  most likely in the same cluster  but all points surrounding a and b are closer together. in this way  each constraints is helpful beyond those points that are part of the constraint. typical experimental results are shown in figure 1. in all cases  incremental algorithms that satisfy all constraints perform better than non-incremental algorithms that satisfy all constraints; sometimes  the former class of algorithms perform as well as distance metric learning. this result indicates that investigating the problem of incrementally learning distance functions should be profitable.
1. conclusion and future work
　previous work by ourselves and others has established the benefits of clustering under a batch of given instance level constraints. in this paper we look at the problem of incremental constrained clustering. we explore several problem definitions that allow the user to provide feedback by critquing an existing clustering through constraints and to receive feedback on how useful the additional constraints were at further optimizing a given objective function. this approach allows for a two-way feedback: the user presents feedback to the algorithm in the form of a constraint. the algorithm provides feedback to the user by indicating whether the given constraint was useful.
　however  our complexity analysis show that both adding and removing constraints is typically intractable. in this paper  we focus on the constraint addition problem and show that just adding a single constraint  be it ml or cl  is in the worst case intractable. however  we identify two sufficient conditions when the feasibility problem for adding of constraints is easy which translate into restrictions on the types of constraints that can be added. for example  the first condition  namely brooks's theorem  requires a user to choose new cl constraints so that no point is part of k or more cl constraints. the second condition  namely irani's q-inductiveness condition  restricts a user to those cl constraints which ensure that each point x is cannot-linked to at most k   1 points that follow x in the chosen ordering.
　we then developed an efficient algorithm for these sufficient conditions that incrementally allows feedback and tested it using simulated feedback from small amounts of labeled data. our results show that a  just adding one constraint but not performing incremental clustering  rather rerunning the constrained clustering algorithm using the same random number seed and initial centroids can produce quite different clusterings b  incremental clustering is more efficient than re-running the entire clustering algorithm with the additional constraints and c  incremental clustering performs better than non-incremental clustering because the latter can get over-constrained as reported earlier . however  both the incremental and non-incremental algorithms that attempt to satisfy all constraints can sometimes perform significantly worse than algorithms that learn a distance metric from the constraints and then using it to cluster the data.
　for future work we wish to investigate what is the maximum number of additional constraints allowed before complete re-clustering should be performed. furthermore  for the k = 1 case we have preliminary results for efficient algorithms to incrementally add both ml and cl constraints which we wish to further explore. finally  the problem of incrementally modifying a distance function remains an important open question.
1. acknowledgements
　the first author gratefully acknowledges support of this work by nsf grant number 1 titled career: knowledge ehanced clustering using constraints.
