aslam  pavlu  and savell  introduced the hedge algorithm for metasearch which effectively combines the ranked lists of documents returned by multiple retrieval systems in response to a given query and learns which documents are likely to be relevant from a sequence of on-line relevance judgments. it has been demonstrated that the hedge algorithm is an effective technique for metasearch  often significantly exceeding the performance of standard metasearch and ir techniques over small trec collections. in this work  we explore the effectiveness of hedge over the much larger terabyte 1 collection.
1 introduction
aslam  pavlu  and savell introduced a unified framework for simultaneously solving the problems of metasearch  pooling  and system evaluation based on the hedge algorithm for on-line learning . given the ranked lists of documents returned by a collection of ir systems in response to a given query  hedge is capable of matching and often exceeding the performance of the best underlying retrieval system; given relevance feedback  hedge is capable of  learning  how to optimally combine the input systems  yielding a level of performance which often significantly exceeds that of the best underlying system.
　in previous experiments with smaller trec collections   it has been shown that after only a handful of judged feedback documents  hedge is able to significantly outperform the combmnz and condorcet metasearch techniques. it has also been shown that hedge is able to efficiently construct pools containing significant numbers of relevant documents and that these pools are highly effective at evaluating the underlying systems . although the hedge algorithm has been shown to be a strong technique for metasearch  pooling  and system evaluation using the relatively small or moderate trec collections  trecs 1  1  1  1  1   it has yet to be demonstrated that the technique is scalable to corpora whose data size is at the terabyte level. in this work  we assess the performance of hedge on a terabyte scale  summarizing training results using the terabyte 1 queries and data and presenting testing results using the terabyte 1 queries and data.
　finally  we note that in the context of trec  the hedge algorithm is both an automatic and a manual technique: in the absence of feedback  hedge is a fully automatic metasearch algorithm; in the presence of feedback  hedge is a manual technique  capable of  learning  how to optimally combine the underlying systems.
1 metasearch
the problem of metasearch  1  1  1  1  1  1  1  is to combine the ranked lists of documents output by multiple retrieval systems in response to a given query so as to optimize the quality of the combination and hopefully exceed the performance of the best underlying system. aslam  pavlu  and savell  considered two benchmark metasearch techniques for assessing how well their hedge algorithm performed:  1  combmnz  a technique which sums the  appropriately normalized  relevance scores assigned to each document by the underlying retrieval systems and then multiplies that summation by the number of systems that retrieved the document and  1  con-
dorcet  a technique based on a well known method for conducting a multicandidate election  where the documents act as candidates and the retrieval systems act as voters providing preferential rankings among these candidates. in experiments using the trec 1  1  1  1  and 1 collections  aslam et al. demonstrated that  in the absence of feedback  hedge consistently outperforms condorcet and at least equals the performance of combmnz; in the presence of even modest amounts of user feedback  hedge significantly outperforms both combmnz and condorcet  as well as the best underlying system.
　in this work  we discuss our experiments with the hedge algorithm in the terabyte track at trec 1  and we also compare to those results obtained by using the hedge algorithm run over the data from the terabyte track at trec 1. in the sections that follow  we begin by briefly describing our methodology and experimental setup  and we then describe our results and conclude with future work.
1 methodology
we implemented and tested the hedge algorithm for metasearch as described in aslam et al. . while the details of the hedge algorithm can be found in the aforementioned paper  the relevant intuition for this technique  as quoted from this paper  is given below.
consider a user who submits a given query to multiple search engines and receives a collection of ranked lists in response. how would the user select documents to read in order to satisfy his or her information need  in the absence of any knowledge about the quality of the underlying systems  the user would probably begin by selecting some document which is  highly ranked  by  many  systems; such a document has  in effect  the collective weight of the underlying systems behind it. if the selected document were relevant  the user would begin to  trust  systems which retrieved this document highly  i.e.  they would be  rewarded    while the user would begin to  lose faith  in systems which did not retrieve this document highly  i.e.  they would be  punished  . conversely  if the document were non-relevant  the user would punish systems which retrieved the document highly and reward systems which did not. in subsequent rounds  the user would likely select documents according to his or her faith in the various systems in conjunction with how these systems rank the various documents; in other words  the user would likely pick documents which are ranked highly by trusted systems.
　our hedge algorithm for on-line metasearch precisely encodes the above intution using the well studied hedge algorithm for on-line learning  first proposed by freund and schapire . in our generalization of the hedge algorithm  hedge assigns a weight to each system corresponding to hedge's computed  trust  in that system  and each system assigns a weight to each document corresponding to its  trust  in that document; the overall score assigned to a document is the sum  over all systems  of the product of the hedge weight assigned to the system  a quantity which varies given user feedback  and the system's weight assigned to that document  a fixed quantity which is a function of the rank of that document according to the system . the weights hedge assigns to systems are initially uniform  and they are updated given user feedback  in line with the intuition given above   and the document set is dynamically ranked according to the overall document scores which change as the hedge-assigned system weights change.
　initially  hedge assigns a uniform weight to all systems and computes overall scores for the documents as described above; the ranked list of documents ordered by these scores is created  and we refer to this system and corresponding list as  hedge1.  a user would naturally begin by examining the top document in this list  and hedge would seek feedback on the relevance of that document. given this feedback  hedge will assign new system weights  rewarding those systems that performed  well  with respect to this document and punishing those that did not   and it will assign new overall scores to the documents based on these new system weights. the remaining unjudged documents would then be re-ranked according to these updated scores  and this new list would be presented to the user in the next round.
　after k documents have been judged  the performance of  hedge k  can be assessed from at least two perspectives  which we refer to as the  user experience  and the  research librarian  perspectives  respectively.
  user experience: concatenate the list of k judged documents  in the order that they were presented to the user  with ranking of the unjudged documents produced at the end of round k. this concatenated list corresponds to the  user experience   i.e.  the ordered documents that have been examined so far along with those that will be examined if no further feedback is provided.
  research librarian: concatenate the relevant subset of the k judged documents with the ranking of the unjudged documents produced at the end of round k. this concatenated list corresponds to what a research librarian using the hedge system might present to a client: the relevant documents found thus far followed by the ordered list of unjudged documents in the collection.
note that the performance of the  research librarian  is likely to exceed that of the  user experience  by any reasonable measure of retrieval performance since judged non-relevant documents are eliminated from the former concatenated list. in what follows   hedge k  refers to the system  concatenated list  and performance as defined with respect to the  research librarian  perspective.
1 experimental setup and results
we tested the performance of the hedge algorithm by using the queries from trec 1 terabyte track. then we run hedge for terabyte1 track  using real user feedback  we judged 1 documents per query . both terabyte1 and terabyte1 use the gov1 collection of about 1 million documents. we indexed the collection using the lemur toolkit; that process took about 1 days using a 1-processor dual-core opteron machine  1 ghz/core .
1 underlying ir systems
the underlying systems include:  1  two tf-idf retrieval systems;  1  three kl-divergence retrieval models  one with dirichlet prior smoothing  one with jelinek-mercer smoothing  and the last with absolute discounting;  1  a cosine similarity model;  1  the okapi retrieval model;  1  and the inquery retrieval method. all of the above retrieval models are provided as standard ir systems by the lemur
toolkit .
　these models were run against a collection  gov1  of web data crawled from web sites in the .gov domain during early 1 by nist . the collection is 1gb in size and contains 1 million documents . although this collection is not a full terabyte in size  it is still much larger than the collections used at previous trec conferences.
　for each query and retrieval system  we considered the top 1 scored documents for that retrieval system. once all retrieval systems were run against all queries  we ran the hedge algorithm described above to perform metasearch on the ranked lists we obtained.
1 results	using	terabyte	1 queries and qrel
we used the trec 1 qrel files to provide hedge with relevance feedback. if one of our underlying systems retrieved a document that was not included in the qrel file  we assumed the document to be non-relevant.
　hedge was run as follows. in the first round each of the underlying systems all have an equal weight and the underlying lists are fused by ranking documents according to highest weighted average mixture loss . the initial run of hedge  hedge1  will not acquire any relevance judgments and hence can be compared directly to standard metasearch techniques   e.g. combmnz .
　in the following round  the top document from hedge1 is judged. in our case  we obtain the judgment from trec qrel file  1 if document not in the qrel . if the document is relevant  it is put at the top our metasearch list  and if it is not  it is discarded. the judgment is then used to re-weight the underlying systems. as described above  systems are re-weigthed based on the rank of the document just judged. then a new metasearch list is produced  corresponding to hedge1. the next round proceeds in the same manner: the top unjudged document from the last metasearch list is judged and then used to:  1  identify where the document should be placed in the list;  1  update the system weight vector to reward the correct systems and punish the incorrect systems;  1  re-rank the remaining unjudged documents.
　in our experiments we had 1 rounds  relevance judgments  and we note the results of hedge for 1  1  1  1  1  1  and 1 judgments.
　for comparison  we also ran condorcet and combmnz over the ranked lists generated by our underlying systems. we then calculated mean average precision scores for each of the three metasearch systems and compared the performance of the hedge system with the performance of the lists generated by condorcet and combmnz  see figure 1. .

figure 1: terabyte1: hedge-m: metasearch performance as more documents are judged.
　we compare hedge to combmnz  condorcet  and the underlying retrieval systems that were used for our metasearch technique. table 1 shows that hedge  in the absence of any relevance feedback  hedge1   consistently outperforms condorcet. the performance of hedge1 is comparable with the performance of combmnz.
　table 1 illustrates that both hedge1 and combmnz are able to exceed the performance of the best underlying system. this demonstrates that hedge alone  even without any relevance feedback  is a successful metasearch technique.
　after providing the hedge algorithm with only ten relevance judgments  hedge1   hedge significantly outperforms combmnz  condorcet  and the best underlying system in terms of map  table 1 . also hedge1 more than doubles precision at cutoff 1 of the top underlying system. this is in part because
system11combmnz1111condorcet1111hedge 1.1.1.1.1hedge 1.1.1.1.1hedge 1.1.1.1.1table 1: terabyte1: hedge vs. metasearch techniques combmnz and condorcet  combining 1   1  1  1 underlying systems.
systemmapp 1jelinek-mercer11dirichlet11tfidf11okapi11log-tfidf11absolute discounting11cosine similarity11combmnz11condorcet11hedge1.1.1hedge1.1.1hedge1.1.1table 1: results for input and metasearch systems on terabyte1. combmnz  cordorcet  and hedge n were run over all input systems.
documents that have been ranked relevant are placed at the top of the list  whereas the documents that have been judged non-relevant are discarded.
1 results for terabyte 1 queries
for our terabyte submission to trec 1  given the lack of judgments  we manually judged several documents for each query. we choose to run hedge for 1 rounds  for each query  on top of our underlying ir systems  provided by lemur  as described above . therefore  in total  1 rounds x 1 queries = 1 documents were judged for relevance.
　as a function of the amount of relevance feedback utilised  four different runs were submitted to terabyte 1: hedge1  no judgments   which is essentially an automatic metasearch system; hedge1  1 judgments per query ; hedge1  1 judgments per query  and hedge1  1 judgments per query . the performance of all four runs are presented in table 1.
the table reports the mean average precision  map   r-precison  and precision-at-cutoff 1  1  1 and 1. against expectations  hedge1 looks slightly better than hedge1 but this is most likely due to the fact that hedge1 was included as a contributor to the trec pool of judged documents while hedge1 was not.
1 judgment disagreement and impact to hedge performance
hedge works as an on-line metasearch algorithm  using user feedback  judged documents  to weight underlying input systems. it does not have a  search engine  component; i.e.  it does not perform traditional retrieval by analyzing documents for relevance to a given query. therefore the performance is heavily determined by user feedback  i.e.  the quality of he judgments. in what follows  we discuss how well our own judgments  1 per query  match those provided by trec qrel file  released at the conclusion of trec 1. major disagreements could obviously lead to significant changes in performance. first  we note that there are consistent  large disagreements. mismatched relevance judgments for query 1 are shown below:
gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1absenthedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=1gx1-1absenthedgerel=1gx1-1trecrel=1hedgerel=1gx1-1absenthedgerel=1gx1-1trecrel=1hedgerel=1gx1-1trecrel=1hedgerel=11 mismatches　we examined a subset of the mismatched relevance judgments and we believe that there were judgment errors on both sides. nevertheless all judgment disagreements on judges affect measured hedge performance negatively. for comparison we re-run hedge1  1 judgments  using the trec qrel file for relevance feedback. in doing so  we obtained a mean average precision of 1  consistent with performance on terabyte 1. this would place the new hedge1 run second among all manual runs  as ordered by map
 figure 1 .
　we also looked at this new run  hedge1 with trec qrel file instead of user feedback  on a queryby-query basis. figure 1 shows a scatterplot comparison  per query  of the original hedge1 performance and the performance using trec judgments for feedback. note the significant and nearly uniform improvements obtained using trec judgments.
 hedge1 performance on terabyte1

figure 1: terabyte1: hedge1. each dot corresponds to a query; x-axis corresponds to hedge1 ap values obtained with our judgments as user feedback; y-axis corresponds to hedge1 ap values using trec qrel file for feedback. map vaues are denoted by  〜 .
1 conclusions
systemmapr-precp 1p 1p 1p 1hedge1.1.1.1.1.1.1hedge1.1.1.1.1.1.1hedge1.1.1.1.1.1.1hedge1.1.1.1.1.1.1table 1: results for hedge runs on terabyte1 queries.

figure 1: terabyte1: hedge1 with trec qrel judgments. the shell shows trec eval measurements on top of the published trec terabyte1 ranking of manual runs ; it would rank second in terms of map.it has been shown that the hedge algorithm for online learning is highly efficient and effective as a metasearch technique. our experiments show that even without relevance feedback hedge is still able to produce metasearch lists which are directly comparable to the standard metasearch techniques condorcet and combmnz  and which exceed the performance of the best underlying list. with relevance feedback hedge is able to considerably outperform condorcet and combmnz.
　the performance shown when using trec qrels file was consistently very good; when using our judgments the relatively poor performance was due to using a set of judgments for feedback and a different set of judgments for evaluation. ultimately we believe that hedge is somehow immune to judge disagreement  as long us the feedback comes from the same source  or judge or user  as the performance measurement. certainly  in practice  it is possible that two users ask the same query but they are looking for different information; in this case user feedback would be different which would lead to different metasearch lists produced and eventually to a satisfactory performance for each user.
