 
complex queries over high speed data streams often need to rely on approximations to keep up with their input.  the research community has developed a rich literature on approximate streaming algorithms for this application.  many of these algorithms produce samples of the input stream  providing better properties than conventional random sampling.  in this paper  we abstract the stream sampling process and design a new stream sample operator. we show how it can be used to implement a wide variety of algorithms that perform sampling and samplingbased aggregations.  also  we show how to implement the operator in gigascope - a high speed stream database specialized for ip network monitoring applications. as an example study  we apply the operator within such an enhanced gigascope to perform subset-sum sampling which is of great interest for ip network management.  we evaluate this implemention on a live  high speed internet traffic data stream and find that  a  the operator is a flexible  versatile addition to gigascope suitable for tuning and algorithm engineering  and  b  the operator imposes only a small evaluation overhead. this is the first operational implementation we know of  for a wide variety of stream sampling algorithms at line speed within a data stream management system. 
1. introduction 
 
many applications  such as network monitoring  financial monitoring  sensor networks  and the processing of large scale scientific data feeds  produce data in the form of high-speed streams.  a query set which analyzes these streams might and often does resort to approximation algorithms in order to keep up with the worst case load.  the research community has developed a large body of work to approximate expensive functions on data streams. examples include approximation algorithms for quantiles  heavy hitters  set resemblance  count distinct  and so on. 
see  for an overview of data stream research. 
we focus on sampling methods for data streams. a sample is a small-sized representative of the data suitable for different purposes. sampling has a rich history in statistics with several variants: sampling with/without replacement  biased sampling  fixed or variable size sampling etc. there is also extensive use of sampling in databases with many modified methods such as stratified  congressional  outliner or distance-based sampling etc. .   sampling in the context of data streams shares some common aspects with sampling in statistics and databases  but has additional constraints. in stream sampling  typically one is interested in sampling in one pass over a high speed data that can not be stored at its matching rate. as a result  when an item repeats on the stream  it is difficult to sample based on whether or not it has been seen before. so  even uniform sampling of the distinct items in the data stream is tricky. further  one may need to obtain fixed-sized sample when the size of the stream is unknown. finally  stream input has many attributes and items are often ``weighted'' and it is difficult to ensure that the sample has desirable properties - such as it captures the  heavy hitters or subrange aggregates - accurately for various subset combinations of attributes and cumulative weights on these combinations. the past few years have seen the design of many effective stream sampling methods for estimating specific aggregates such as quantiles   heavy hitters   distinct counts   subset-sums   set resemblance and rarity  etc. as well as generic sampling such as fixed-size reservoir  sampling    adaptive geometric  sampling    etc.  
the focus of this paper is not to design new stream sampling methods. instead  we address the problem of how these widely varied and quite sophisticated sampling methods can be implemented within an operational data stream management system and scale in performance to line speeds in ip network monitoring applications. the problem we address in this paper is to incorporate approximate streaming algorithms into a dsms  specifically sampling-based algorithms.   
possible approaches: there are several approaches to doing this integration  which we discuss here.   

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod 1  june 1  1  baltimore  maryland  usa. 
copyright 1 acm 1-1/1   $1. 
 
the first approach is to incorporate the different sampling algorithms directly into the dsms kernel  and make the option of using them available to the user through several keywords. this approach is attractive when the special techniques being incorporated into the database engine are mature  for example data mining keywords in sql server 1   windowing keywords in sql 1   and so on.  however  stream sampling algorithms is an active research area with new techniques being continually developed.  incorporating new techniques into the kernel is cumbersome and does not promote experimentation.  in addition  the query language is burdened with a keyword explosion. aurora incorporates a drop operator which performs random sampling to shed load ; stream also provides operator-level sampling via a sample keyword . 
