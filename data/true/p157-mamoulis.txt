object-oriented and object-relational dbms support setvalued attributes  which are a natural and concise way to model complex information. however  there has been limited research to-date on the evaluation of query operators that apply on sets. in this paper we study the join of two relations on their set-valued attributes. various join types are considered  namely the set containment  set equality  and set overlap joins. we show that the inverted file  a powerful index for selection queries  can also facilitate the efficient evaluation of most join predicates. we propose join algorithms that utilize inverted files and compare them with signature-based methods for several set-comparison predicates.
1.	introduction
commercial object-oriented and object relational dbms  support set-valued attributes in relations  which are a natural and concise way to model complex information. although sets are ubiquitous in many applications  document retrieval  semi-structured data management  data mining  etc.   there has been limited research on the evaluation of database operators that apply on sets.
모on the other hand  there has been significant research in the ir community on the management of set-valued data  triggered by the need for fast content-based retrieval in document databases. research in this area has focused on processing of keyword-based selection queries. a range of indexing methods has been proposed  among which signaturebased techniques  1  1  1  and inverted files  1  1  dominate. these indexes have been extensively revised and evaluated for various selection queries on set-valued attributes  1  1 . an important operator which has received limited attention is the join between two relations on their set-valued attributes. formally  given two relations r and s  with setvalued attributes r.r and s.s  r 1r붿s s returns the subset of their cartesian product r 뫄 s  in which the set-valued attributes qualify the join predicate 붿. two join operators
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1  june 1  1  san diego  ca.
copyright 1 acm 1-1-x/1 ...$1.
that have been studied to date are the set containment join  where 붿 is    1  1   and the all nearest neighbors operator   which retrieves for each set in r its k nearest neighbors in s based on a similarity function. other operators include the set equality join and the set overlap join.
모as an example of a set containment join  consider the join of a job-offers relation r with a persons relation s such that r.required skills   s.skills  where r.required skills stores the required skills for each job and s.skills captures the skills of persons. this query will return qualified personjob pairs. as an example of a set overlap join  consider two bookstore databases with similar book catalogs  which store information about their clients. we may want to find pairs of clients which have bought a large number of common books in order to make recommendations or for classification purposes.
모in this paper we study the efficient processing of set join operators. we assume that the dbms stores the set-valued attribute in a nested representation  i.e.  the set elements are stored together with the other attributes in a single tuple . this representation  as shown in   facilitates efficient query evaluation. we propose join algorithms that utilize inverted files and compare them with signature-based methods for several set-comparison predicates. our contribution is two-fold:
  we propose a new algorithm for the set containment join  which is significantly faster than previous approaches  1  1 . the algorithm builds an inverted file for the  container  relation s and uses it to process the join. we demonstrate the efficiency and robustness of this method under various experimental settings.   we discuss and validate the efficient processing of other join predicates  namely set equality and set overlap joins. for each problem  we consider alternative methods based on both signatures and inverted files. experiments show that using an inverted file to perform the join is also the most appropriate choice for overlap joins  whereas signature-based methods are competitive only for equality joins.
모the rest of the paper is organized as follows. section 1 provides background about set indexing methods. section 1 describes existing signature-based algorithms for the set containment join and presents new techniques that employ inverted files. section 1 discusses how signature-based and inverted file join algorithms can be adapted to process set equality and set overlap joins. the performance of the algorithms is evaluated experimentally in section 1. finally  section 1 concludes the paper with a discussion about future work.
1.	background
in this section we provide a description for various methods used to index set-valued attributes. most of them originate from information retrieval applications and they are used for text databases  yet they can be seamlessly employed for generic set-valued attributes in object-oriented databases.
1	signature-based indexes
the signature is a bitmap used to represent sets  exactly or approximately. let d be the  arbitrarily ordered  domain of the potential elements which can be included in a set  and |d| be its cardinality. then a set x can be represented exactly by a |d|-length signature sig x . for each i  1 뫞 i 뫞 |d|  the i-th bit of sig x  is 1 iff the i-th item of d is present in x. exact signatures are expensive to store if the sets are sparse  therefore approximations are typically used; given a fixed signature length b  a mapping function assigns each element from d to a bit position  or a set of bit positions  in  1 b . then the signature of a set is derived by setting the bits that correspond to its elements.
모queries on signature approximations are processed in two steps  in analogy to using rectangle approximations of objects in spatial query processing  . during the filter step  the signatures of the objects are tested against the query predicate. a signature that qualifies the query is called a drop. during the refinement step  for each drop the actual set is retrieved and validated. if a drop does not pass the refinement step  it is called a false drop.
모assume  for example  that the domain d comprises the first 1 integers and let b = 1. sets x = {1 1 1} and y = {1 1} can then be represented by signatures sig x  = 1 and sig y  = 1 if modulo 1 is used as a mapping function. checking whether x = y is performed in two steps. first we check if sig x  = sig y   and only if this holds we compare the actual sets. the same holds for the subset predicate  x   y   sig x    sig y   and the simple overlap predicate  e.g.  whether x and y overlap or not . the signatures are usually much smaller than the set instances and binary operations on them are very cheap  thus the two-step query processing can save many computations. in the above example  since sig x  =1 sig y   we need not validate the equality on the actual set instances. on the other hand  the object pair hx yi passes the filter step for x   y  but it is a false drop. finally  for x 뫌 y =1   hx yi passes both the filter and refinement steps. naturally  the probability of a false drop decreases as b increases. on the other hand  the storage and comparison costs for the signatures increases  therefore a good trade-off should be found.
모for most query predicates the signatures can serve as a fast preprocessing step  however  they can be inefficient for others. consider for instance the query  called -overlap query and denoted byin the following  asking whether x and y share at least  common items. notice that the signatures can be used to prune only sets for which sig x  뫇 sig y  = 1  the wedge here denotes logical and . thus  if    1 the signatures are not prune-effective. consider the running example instances x and y and assume that  = 1. the fact that sig x  and sig y  have three common bits does not provide a lower bound for the actual overlap  since different elements can map to the same bit. as another example  assume that x = {1 1} and y = {1 1}. the signatures now share just one bit  but they qualify the query. this shows why we cannot prune signatures with overlap smaller than .
모signatures have been used to process selection queries on set-valued attributes. the most typical query is the set containment query  which given a set q and a relation r  asks for all sets r.r 뫍 r such that q   r. as an example application  assume that we are looking for all documents which contain a set of index terms. usually  signatures are organized into indexes to further accelerate search. the simplest form of a signature-based index is the signature file . in this representation the signatures of all sets in the relation are stored sequentially in a file. the file is scanned and each of them is compared with the signature of the query. this is quite efficient if the query selectivity is low  i.e.  if the percentage of the qualifying signatures is large   but it is not an attractive method for typical applications with sparse sets.
모an improved representation of the signature file employs bit-slicing . in this representation  there is a separate bitslice vector stored individually for each bit of the signatures. when applying query q the bit slices where sig q  has 1 are anded to derive a bit slice  representing the record-ids  rids  that pass the filter step  for containment and equality queries . figure 1 shows an example of a bit-sliced signature file. if the query signature is 1  we need to join slices s1  s1  and s1 in order to get the candidate rids. if the partial join of some slices sets only a few rids  it might be beneficial not to retrieve and join the remaining slices  but to validate q directly on the qualifying rids. overlap queries can be processed by taking the logical or of the bit-vectors.
b=1

s1 s1 s1 s1 s1 s1 s1 s1 s1 s1
1
1
1
.
 ..
1
1
1
.
 ..
1
1
1
.
.
.
1
1
1
.
.
.
1
1
1
.
 ..
1
1
1
.
 ..
1
1
1
.
.
.
1
1
1
.
.
.
1
1
1
.
 ..
1
1
1
.
 ..
1
1
1
.
.. 
n
figure 1: a bit-sliced signature file
모the signature-tree  1  1  is a hierarchical index  which clusters signatures based on their similarity. it is a dynamic  balanced tree  similar to the r-tree   where each node entry is a tuple of the form hsig ptri. in a leaf node entry sig is the signature and ptr is the record-id of the indexed tuple. the signature of a directory node entry is the logical or of the signatures in the node pointed by it and ptr is a pointer to that node. in order to process a containment/equality query we start from the root and follow the entry pointers e.ptr  if sig q    e.sig  until the leaf nodes where qualifying signatures are found. for overlap queries all entries which intersect with sig q  have to be followed. this index is efficient when there are high correlations between the data signatures. in this case  the signatures at the directory nodes are sparse enough to facilitate search. on the other hand  if the signatures are random and uncorrelated the number of 1's in the directory node entries is high and search performance degrades. a strength of the index is that it handles updates very efficiently  thus it is especially suitable for highly dynamic environments.
모another method for indexing signatures is based on hashing . given a small k  the first k bits are used as hashkeys and the signatures are split into 1k partitions. the first k bits of the query are then used to select the buckets which may contain qualifying results. to facilitate locating the qualifying buckets  a directory structure which contains hash values and pointers is stored separately  even in memory if the number of buckets is small enough . in order to control the size of the directory and partitions dynamically  extendible hashing can be used. this index is very efficient for equality selection queries  since only one bucket has to be read for each query. the effect is the same as if we sorted the signatures and imposed a b+-tree over them  using a signature prefix as key. however  the performance of this index is not good for set containment queries .
1	the inverted file
the inverted file is an alternative index for set-valued attributes. for each set element el in the domain d  an inverted list is created with the record ids of the sets that contain this element in sorted order. then these lists are stored sequentially on disk and a directory is created on top of the inverted file  pointing for each element to the offset of its inverted list in the file. if the domain cardinality |d| is large  the directory may not fit in memory  so its entries hel offseti are organized in a b+-tree having el as key.
모given a query set q  assume that we want to find all sets that contain it. searching on the inverted file is performed by fetching the inverted lists for all elements in q and merge-joining them. the final list contains the recordids of the results. for example  if the set containment query q = {el1 el1} is applied on the inverted file of figure 1  only tuple with rid=1 qualifies. since the record-ids are sorted in the lists  the join is performed fast. if the search predicate is 'overlap'  then the lists are just merged  eliminating duplicates. notice that the inverted file can also process -overlap queries efficiently since we just need to count the number of occurrences of each record-id in the merged list.
el1 	1  1  1  1  1 el1 	1  1  1  1 
	el1 	1  1  1  1  1  1 
	. .. 	...
	elm 	1  1  1  1  1  1 
figure 1: an inverted file
모fetching the inverted lists can incur many random accesses. in order to reduce the access cost  the elements of q are first sorted and the lists are retrieved in this order. prefetching can be used to avoid random accesses if two lists are close to each other.
updates on the inverted file are expensive. when a new set is inserted  a number of lists equal to its cardinality have to be updated. an update-friendly version of the index allows some space at the end of each list for potential insertions and/or distributes the lists to buckets  so that new disk pages can be allocated at the end of each bucket. the inverted file shares some features with the bit-sliced signature file. both structures merge a set of rid-lists  represented in a different way  and are mostly suitable for static data with batch updates.
모in recent comparison studies  1  1   the inverted file was proven significantly faster than signature-based indexes  for set containment queries. there are several reasons for this. first  it is especially suitable for real-life applications where the sets are sparse and the domain cardinality large. in this case  signature-based methods generate many false drops. second  it applies exact indexing  as opposed to signaturebased methods  which retrieve a superset of the qualifying rids and require a refinement step. finally  as discussed in the next subsection  it achieves higher compression rates  than signature-based approaches and its performance is less sensitive to the decompression overhead.
1	compression
both signature-based indexes and inverted files can use compression to reduce the storage and retrieval costs. dense signatures are already compressed representations of sets  but sparse ones could require more storage than the sets  e.g.  the exact signature representations of small sets . thus  a way to compress the signatures is to reduce their length b at the cost of increasing the false drop probability. another method is to store the element-ids instead of the signature for small sets or to encode the gaps between the 1's instead of storing the actual bitmap. however  this technique increases the cost of query processing; the signature has to be constructed on-the-fly if we want to still use fast bitwise operations. the bit-sliced index can also be compressed by encoding the gaps between the 1's  but again this comes at the expense of extra query processing cost .
모the compression of the inverted file is much more effective. an inverted list is already sorted and can be easily compressed by encoding the gaps  i.e.  differences  between consecutive rids. for example  assume that an inverted list contains the following rids: {1 1 1 ...}. it can be converted to the run-length encoding of the rids: {1 1 1 ...}  i.e.  the series of differences between successive rids . observe that this comes at no cost for query processing  since the algorithm for merging the lists can be easily adapted. although the above list contains small numbers there is no upper bound for them; a fixed-length encoding would yield no compression. thus a variable-length encoding should be used.
모an example of such an encoding is the golomb coding . assume that the integers we want to encode follow a geometric law with a probability parameter p. the golomb code takes a parameter g such that  1   p g +  1   p g+1 뫞 1    1   p g 1 +  1   p g. now assume that we want to encode an integer x  where x   1. in order to save code space  we encode x 1 which is potentially 1. we represent  x   1  using two integers: l = b x   1 /gc and r =  x   1  modulo g. finally  x is encoded using a unary encoding for l + 1 followed by a binary encoding of r. table 1 shows the run-length integers in our running example and their golomb codes for g = 1. for example  to compute the code of 1 we derive l = b 1 c = 1 and r = 1 modulo 1 = 1  and concatenate the unary 1 = 1 with the binary 1 = 1. additional coding techniques are also applicable. see  1  1  for more details on compressing inverted files.
numbercode111111table 1: example of golomb codes for integers
모decoding the rids is quite fast and affects little the performance of the inverted file. moreover  the compression rate increases with the density of the list  as the gaps between the rids decrease. to exemplify this  consider a relation r of 1 tuples  where each set r.r has 1 elements on the average  and the set domain d has 1 elements. the number of rids per inverted list is = 1 on the average. this means that the expected difference between two rids in an inverted list is |r|/1 = 1. if we use golomb coding with g = 1 we can encode each rid with 1 bits on the average. now assume that each set has 1 elements on the average. in this case  the expected difference between two rids is just 1 and we need fewer bits to encode them. thus the size  and retrieval cost  of the inverted file increases sub-linearly with the average set cardinality. moreover  the golomb coding can be applied independently for each inverted list adjusting the compression rate according to the data skew. the two extreme cases are lists containing few uncompressed integers  and lists which reduce to bitmaps.
1.	evaluation of set containment joins
several join operators can be applied on set-valued attributes  including the set containment  the set equality and the set overlap join. in this section we restrict our attention to the set containment join  which is one of the most interesting operators and also has been proven rather hard in past research  1  1 . we first describe existing signature-based join algorithms. then we propose and discuss the optimization of new algorithms that utilize inverted files. throughout this section we will deal with the join r 1r s s  where r and s are set-valued attributes of relations r and s  respectively. unless otherwise specified  we will assume that none of the relations is indexed.
1	the signature nested loops join
in  two main memory join algorithms were proposed for the set containment join. one of them is also applicable for large relations that do not fit in memory. the signature nested loops  snl  join consists of three phases. during the signature construction phase  for each tuple ts 뫍 s a triplet h|ts.s| sig ts.s  ts.ridi is computed  where |ts.s| is the cardinality of ts.s  sig ts.s  is its signature  and ts.rid is the record-id of ts. these triplets are stored in a signature file ssig. during the probe phase  each tuple tr 뫍 r is read  its signature sig tr.r  is computed and the signature file ssig is scanned to find all pairs htr.rid ts.ridi  such that sig tr.r    sig ts.s  and |tr.r| 뫞 |ts.s|. these rid-pairs are written to a temporary relation. during the final verification phase the htr.rid ts.ridi pairs generated from the probe phase are read  the corresponding tuples are fetched  and tr.r   ts.s is verified.
모this method requires a quadratic number  to the relation size  of signature comparisons and is not attractive for large problems. notice that the verification phase could be combined with the match phase if we fetch ts.s immediately after the signature comparison succeeds  however  this would incur many random accesses. in  a hash-based method is also proposed  however  it would result in a large number of random i/os if applied to secondary memory problems.
1	the partitioned set join
ramasamy et al.  introduced a hash-based algorithm that aims at reducing the quadratic cost of snl. the partitioned set join  psj  hashes the tuples from r and s to a number of p buckets r1 r1 ... rp and s1 s1 ... sp and joins rx with sx for all 1 뫞 x 뫞 p.
모the algorithm also works in three phases. during the first  partitioning  phase  the tuples from the two relations are partitioned. each tuple tr 뫍 r is read  and the triplet h|tr.r| sig tr.r  tr.ridi is computed. a random element er from tr.r is picked and the triplet is written to the bucket determined by a hash function h er . then relation s is partitioned as follows. each tuple ts 뫍 s is read  and the triplet h|ts.s| sig ts.s  ts.ridi is computed. for each element es in ts.s  the triplet is written to the partition determined by the same hash function h es . if two elements es 뫍 ts.s have the same hash value the triplet is sent to the corresponding bucket only once. thus  each triplet from r is hashed to exactly one partition  but a triplet from s is in general hashed to many partitions. the algorithm does not miss join results  because if tr.r   ts.s then the random element er 뫍 tr.r will equal some es 뫍 ts.s  and the triplets will co-exist in the partition determined by h er .
모during the join phase  the pairs of partitions hrx sxi corresponding to the same hash value x are loaded and joined. rx should be small enough to fit in memory; its contents are loaded and hashed using a secondary hash function. more specifically  a random set bit in each sig tr.r  뫍 rx is picked and the triplet is assigned to the corresponding memory bucket. then each triplet from sx is compared with all triplets in the memory buckets determined by the set bits in sig ts.s . qualifying rid-pairs are written to a temporary file. finally  the verification phase of psj is the same as that of snl.
모although psj is faster than snl  if suffers from certain drawbacks. first  the replication of the signatures from s can be high if the set cardinality is in the same order as the number of partitions p. thus if c is the cardinality of a ts.s 뫍 s  the tuple will be hashed into c   c 1   1/c p partitions on the average  1  1 . for example if p = 1 and c = 1  the quantity above shows that each tuple is replicated to 1 partitions on the average. thus the size of each sx is comparable to ssig and the algorithm has similar cost to snl  actually to a version of snl that employs in-memory hashing; called psj-1 in  . on the other hand  p should be large enough in order to partition r into buckets that fit in memory. second  psj carries the inherent drawback of signature-based evaluation methods; it introduces false drops  which have to be verified. the verification phase incurs additional i/o and computational overhead which constitutes a significant portion of the overall join cost  as shown in  and validated by our experiments. in the rest of this section we introduce join algorithms based on inverted files that alleviate these problems.
1	block nested loops using an inverted file
as shown in   inverted files are more efficient than signature-based indexes for evaluating selection queries. among their other advantages once compression is taken into account  it is mentioned that they are also relatively cheap to create.
모this motivated us to study alternative methods for the set containment join  based on inverted files. even if the relations are not indexed  it could be more beneficial to build inverted files and perform the join using them  rather than applying signature-based join techniques. hopefully  this would yield two advantages. first  the join will be processed fast. second  we would have built a powerful index that can be used afterwards not only for other join queries  but also for selections on the set-valued attribute  as described in section 1. we will now discuss how to build inverted files fast and use them for evaluating set containment joins.
1.1	constructing an inverted file
the inverted file for a relation r can be constructed fast using sorting and hashing techniques . a simple method is to read each tuple tr 뫍 r and decompose it to a number of binary hel tr.ridi tuples  one for each element-id el that appears in the set tr.r. then these tuples are sorted on el and the index is built. for small relations this is fast enough  however  for larger relations we have to consider techniques that save i/o and computational cost. a better method is to partition the binary tuples using a hash function on el and build a part of the index for each partition individually. then the indexes are concatenated and the same hash function is used to define an order for the elements.
모in order to minimize the size of the partitions  before we write the partial lists of the partitions to disk  we can sort them on tr.rid and compress them. this comes at the additional cost of decompressing them and sort the tuples on el to create the inverted lists for the partition. an alternative method is to sort the partial lists on el in order to save computations when sorting the whole partition in memory.
모the cost of building the inverted file comprises the cost of reading the relations  the cost of writing and reading the partitions and the cost of writing the final inverted file. as we demonstrate in section 1  if compression is used  the overall cost is not much larger than the cost of reading r. this is due to the fact that the compressed partial information is several times smaller than the actual data.
1.1	a simple indexed nested-loops join method
the simplest and most intuitive method to perform the set containment join using inverted files is to build an inverted file sif for s and apply a set containment query for each tuple in r. it is not hard to see that if sif fits in memory  this method has optimal i/o cost.
모however  for large problems sif may not fit in memory and in the worst case it has to be read once for each tuple in r. therefore  we should consider alternative techniques that apply in the general case.
1.1	block nested loops join
since it is not efficient to scan the inverted file for each tuple from r we came up with the idea of splitting the inverted file in large blocks that fit in memory and scan r for each block. our block nested loops  bnl  algorithm reads sif sequentially in blocks b1 b1 ... bn  such that each block fits in memory and there is enough space for loading a page from r. assume that currently block bi is loaded in memory  and let li be the set of elements whose inverted list is in bi. for each tuple tr 뫍 r  three cases may apply:
1. tr.r   li; the lists of all elements in tr.r are in bi. in this case the lists are joined and the qualifying results are output.
1. tr.r 뫌 li =  ; the list of no element in tr.r is in bi. in this case tr is ignored and we go to the next tuple.
1. tr.r 뫌 li =1   뫇 tr.r * li; the lists of some  but not all  elements in tr.r are in bi. this is the hardest case to handle  since we may need information from other blocks in order to verify whether tr has any superset in s. we call tr a dangling tuple.
모the first two cases are trivial; thus we concentrate on handling dangling tuples efficiently. a simple way to manage a dangling tuple tr is to merge the lists in tr.r 뫌 li  these are currently in memory   compute a partial rid list  and  if non-empty  write it to a temporary file ti. the partial lists are of the form htr.rid n ts1.rid ts1.rid ...tsn.ridi  where n is the number of tuples from s that currently match with tr.rid. if n is large  the list is compressed to save space. after we have scanned sif  a second phase begins  which loads the temporary files and merge-joins them. the contents of each ti are already sorted on tr.rid  thus merging them in memory is very efficient.
모figure 1 shows the pseudocode of bnl. lel denotes the inverted list of element el. in the implementation  li is not computed explicitly  but the number m of elements el 뫍 tr.r뫇lel 뫍 bi is used to validate whether tr.r   li; if |tr.r| equals m we know that all elements of tr.r are in li. as a final comment  we assume that the directory for each bi can fit in memory  since it should be much smaller than the block itself. we further compress it  by storing the offsets of each inverted list  with respect to the absolute offset of the first list in bi.
1.1	optimizing bnl
the partial lists are many if the memory blocks bi are not large  and so is the overhead of writing  and reading  temporary results to disk. therefore we have to minimize this temporary information at no additional cost in the join process.
1.1	keeping 뷇s.|s| in memory.
a fact that we have not exploited yet  is that a tuple tr 뫍 r could join with a tuple ts 뫍 s only if the cardinalities of the sets qualify |tr.r| 뫞 |ts.s|. if we use this information  we could shrink or prune many partial lists. the basic version of the inverted file does not include any information about |ts.s|. we can obtain this information in two ways. the first is to store |ts.s| with every appearance of ts.rid in the inverted lists. this method is also used in . however  the size of the inverted file may increase dramatically  it may algorithm bnl r  sif  { i := 1; while there are more lists in sif {
i := i + 1;
bi := read next block of sif that fits in memory;
li := elements whose inverted list is in bi; initialize temporary file ti; for each tuple tr 뫍 r do { list tr  := tlel  el 뫍 tr.r 뫇 el 뫍 li; if list tr  =1   then
if tr.r   li then output results; else write list tr  to ti;
}
}
n := i;
merge-join all ti  1 뫞 i 뫞 n and output results;
}
figure 1: block-nested loops join
even double . in a compressed inverted file  we only need few bits  e.g.  1  to encode each ts.rid  and storing |ts.s| could require that many extra bits. on the other hand  we observe that in typical applications  where sets are small  |ts.s| does not require more than a byte to store. thus we can keep the cardinalities of all ts.s 뫍 s in memory  we denote this table by 뷇s.|s|  without significant overhead. for example  if |s|=1m tuples  we need just a megabyte of memory to accommodate 뷇s.|s|. this information can be computed and stored in memory while reading s at the construction phase of sif. it is then used to  i  accelerate the merging of inverted lists   ii  reduce the number and length of the partial lists that have to be written to temporary files.
1.1	pipelining the partial lists.
even with the availability of 뷇s.|s|  the partial lists could be many and long. if an object's elements are spanned to multiple blocks  multiple lists may have to be written to the temporary files. to make things worse  an object which is pruned while joining block bi with r  because its lists there have empty intersection  may qualify at a later block bj  i   j  although it should never be considered again. for example  assume that a tr 뫍 r has two elements in b1 and their inverted lists have empty intersection. this means that there is no set in s containing both elements  and as a result no set in s joins with tr. when b1 is loaded  we can find some elements in tr with lists in b1  which should not normally be processed  since we already know that t does not join with any tuple from s.
모another observation is that while processing b1  we can merge the partial lists generated by it with the lists in t1  i.e.  the temporary file created by the previous pass. thus during the iteration where we join bi with r  we keep in memory  i  block bi   ii  one  the current page  from r  and  iii  one  the current  page from ti 1. since we scan r sequentially and the lists in ti 1 are sorted on tr.rid  we can merge efficiently the lists produced by the previous passes with the ones produced by the current one. this makes it easier also to discard all tr which disqualified at the previous steps; if there is any element el 뫍 tr.r whose inverted list lel was at a previous bj  j   i and there is no list in ti 1 for tr.rid  then we know that tr has been pruned before and we can ignore it at the current and next blocks. another way to avoid processing tuples which disqualified during a previous block-join is to maintain a memory-resident bitmap indicating the pruned tuples from r.
모the pseudocode of this version of bnl that employs pipelining is shown in figure 1. at each step  the qualifying lists from the current block b 뫍 sif are merged with the temporary file tprev from the previous step. to perform merging efficiently we run a pointer on tprev showing the current list lp. this serves multiple purposes. first  each tr 뫍 r which disqualified in some previous step is easily spotted  condition at line 1 . indeed  if the inverted list of some element from tr has been loaded before  this can be easily checked using the order of the elements in tr.r  and tr.rid does not match the current lp.tr.rid in tprev  then we know that tr.rid has been pruned before  because otherwise there would be an entry for it in tprev. the second use of lp is to join its contents with the lists in b. if the current tr has elements in the current block and matches with the current lp  the lists in b are joined with lp. if the resulting list is not empty  the results are either output  or written to the next temporary file tnext  depending on whether there are elements in tr yet to be seen at a next step  cf. line 1 in figure 1 . lp is updated after the corresponding tuple lp.tr is read from r  conditions at lines 1 and 1 .
algorithm bnl r  sif  {
tprev := null; while there are more lists in sif {
b := read next block of sif that fits in memory;
l := elements whose inverted list is in b; initialize temporary file tnext; lp := get next list from tprev; /*if applicable*/ for each tuple tr 뫍 r do {
 1  if some el 뫍 tr.r was in some previous b
	and lp.tr.rid =1	tr.rid then
 continue; /*prune this tuple*/ if tr.r 뫌 l =   then {
 1  if lp.tr.rid = tr.rid then
lp := get next list from tprev; /*if applicable*/
continue; /*goto next tuple*/
}
list tr  := tlel  el 뫍 tr.r 뫇 el 뫍 li;
 1  if lp.tr.rid = tr.rid then {
list tr  := list tr  뫌 lp;
lp := get next list from tprev; /*if applicable*/
}
if list t  =1   then
 1 	if all inv. lists for tr.r have been seen so far then
output results;
else write list tr  to tnext;
}
tprev := tnext;
}
}
figure 1: block-nested loops with pipelining
1.1	optimizing the computational cost.
모another factor which requires optimization is the join algorithm that merges the inverted lists in memory. we initially implemented a multiway merge join algorithm with a heap  i.e.  priority queue . the algorithm scans all lists synchronously increasing all pointers if all rids match  or the smallest one if they do not. the current positions of all pointers were organized in the heap to facilitate fast update of the minimum pointer. however  we found that this version performed rather slow  whereas most of the joined lists had high selectivity or gave no results.
모therefore  we implemented another version that performs a binary join at a time and joins its results with the next list. we experienced great performance improvement with this version  since since many sets from r with large cardinality were pruned much faster.
1.1	analysis of bnl
the cost of bnl heavily depends on the size of sif. clearly  given a memory buffer m  the smaller sif is  the fewer the number of passes on r required by the algorithm. also the more the inverted lists in a block  the fewer the temporary partial lists generated by each pass. the number of bits to store a compressed inverted file i that uses golomb coding is in the worst case  assuming each inverted list has the same number of rids  given by the following formula :
	 	 1 
where n is the number of indexed tuples  c is the average set cardinality and |d| is the domain size of the set elements. the factor in the parentheses is the average number of bits required to encode a  gap  between two rids in an inverted list. we can use this formula to estimate the size of sif. the i/o cost of bnl can then be estimated as follows:

모the last component in equation 1 is the cost of reading and writing the temporary results ti for each block bi. assuming that the simple version of bnl is applied  without pipelining   all ti will be equal  each containing the partial lists for the dangling tuples in bi. assuming that rids are uniformly distributed in the inverted list  the probability probdng tr  of a tr 뫍 r to become a dangling tuple can be derived by the probability that all elements of tr to be contained in li and the probability that no element in tr is part of li  or else:
		 1 
모li can be estimated by  since we assume that all lists have the same size. the number of dangling tuples can then be found by |r|probdng tr . not all dangling tuples will be written to ti  since some of them are pruned. the expected number of qualifying dangling tuples  as well as the size of their partial lists can be derived by applying probabilistic counting  considering the expected lengths of the lists and other parameters. the analysis is rather long and complex  therefore we skip it from the paper.
모as shown in section 1  the size of the temporary results is small  especially for large c  where the chances to prune a tuple are maximized. moreover  the pipelining heuristic decreases fast the temporary results from one iteration to the next. for our experimental instances  where m is not much smaller than sif  e.g.  m 뫟 sif/1   ti is typically a small fraction of sif  thus the i/o cost of bnl reduces roughly to the cost of scanning r times the number of passes.
1	an algorithm that joins two inverted files
the set containment join can also be evaluated by joining the two inverted files rif and sif. although this may require preprocessing both relations  at a non-negligible cost  the resulting inverted files may accelerate search  and at the same time compress the information from the original relations.
모the join algorithm traverses synchronously both inverted files. at each step  the inverted lists are merged to partial lists htr.rid n ts1.rid ts1.rid ...tsn.ridi  where n is the number of tuples from s that match so far with tr.rid. each inverted list pair generates as many partial lists as the number of tuples in the list of rif. after scanning many inverted list pairs  the number of generated partial lists exceeds the memory limits and the lists are merged. the merging process eliminates lists with empty intersection and outputs results if the number of merged lists for a tuple tr is equal to |tr.r|. the rest of the merged lists are compressed and written to a temporary file. in the final phase  the temporary files are loaded and joined. we call this method inverted file join  ifj .
모ifj can be optimized in several ways. first  the projection 뷇r.|r| can be held in memory  to avoid fetching |tr.r| for verification. alternatively  this information could be embedded in the inverted file but  as discussed above  at the non-trivial space cost. another  more critical optimization is to minimize the size of the temporary files. for this  we use similar techniques as those for the optimization of bnl. we keep 뷇s.|s| in memory  and use it to prune fast rid pairs  where |tr.r|   |ts.s|. another optimization is to use the pipelining technique. before we output the partial lists  the previous temporary file tprev is loaded and merged with them. this may prune many lists. in addition  if we know that tr.r has no elements in inverted lists not seen so far  we can immediately output the htr.rid tsi.ridi pairs  since they qualify the join.
모however  this comes at the cost of maintaining an additional table rcount  which counts the number of times each tr.rid has been seen in the inverted lists we have read so far from rif. since the sets are relatively sparse  we will typically need one character per tuple to encode this information. in other words  the size of this array is at most the size of 뷇r.|r| and can be typically maintained in memory. rcount is initialized with 1s. whenever we read an inverted list l in rif we add 1 for each tr.rid present in l. after a partial list has been finalized  we output the results if rcount tr.rid  = |tr.r|  otherwise we write the list to the temporary file. an additional optimization uses the equivalent table scount for s  which can be used to prune some tsi.rid from the partial lists  as follows. if |tr.r| rcount tr.rid    |tsi.s| scount tsi.rid  we can prune tsi.rid because in future lists the occurrences of tr.rid are more than the occurrences of tsi.rid. in other words  the elements of tr.r we have not seen yet are more than the elements of tsi.s we have not seen yet  thus tr.r * tsi.s.
1.	evaluation of other join predicates
in this section we discuss the evaluation of other join operators on set-valued attributes  namely the set equality join and the set overlap join.
1	the set equality join
adaptations of the signature-based and inverted-file based algorithms discussed in section 1 can also be applied for the set equality join  since it is not hard to see that it is a special case of the set containment join.
모from signature-based techniques  we do not consider snl  since this algorithm compares a quadratic number of signatures and we can do much better for joins with high selectivity. psj on the other hand can be very useful for set equality queries. we propose an adapted version of psj for set equality joins. the main difference from the algorithm described in section 1 is that we use a single method to partition the data for both relations. while computing the signature of each tuple in tr 뫍 r we also find the smallest element er 뫍 tr.r  according to a predefined order in the domain d  and send the triplet h|tr.r| sig tr.r  tr.ridi to the partition determined by a hash function h er . tuples from s are also partitioned in the same way. the rationale is that if two sets are equal  then their smallest elements should also be the same.
모the join phase is also similar; each pair of partitions is loaded in memory and the contents are re-hashed in memory using a prefix of their signature  e.g.  the first byte . then all pairs of memory buckets  one pair for each value of the prefix  are joined by applying a signature equality test. the set cardinality is also used before the signature comparison to prune early disqualifying signature pairs. we expect that this version of psj will do much better than the one for set containment joins  since replication is avoided in both partitioning and join phases.
모the inverted file algorithms bnl and ifj  can also be adapted to process set equality joins. the only change in bnl is that 뷇s.|s| is now used to prune fast inverted list rids of different cardinality than the current tr 뫍 r. in other words  the condition |tr.r| 뫞 |ts.s| is now replaced by |tr.r| = |ts.s| for each |ts.rid| found in the lists of bi. ifj is also changed accordingly  using both 뷇s.|s| and 뷇r.|s| .
1	the set overlap join
the set overlap join retrieves the pairs of tuples which have at least one common element. for the more general -overlap join  the qualifying pairs should share at least  elements. in this paragraph we describe how these operators can be processed by signature-based methods and ones that use inverted files.
모the relaxed nature of the overlap predicate makes inappropriate the application of hashing techniques  like psj  on signature representations. indeed it is hard to define a partitioning scheme which will divide the problem into joining p pairs of buckets much smaller than r and s. an idea we quickly abandoned is to partition both r and s by replicating their signatures to many hash buckets  after applying a hash function on each of their elements. this approach does not miss any solution  but the size of partitions becomes so large that the overall cost becomes higher than applying nested loops on two signature files. another problem with this partitioning approach is that it introduces duplicate drops. thus  for this operator we use the simple snl algorithm. finally  notice that whatever signature-based method we use  the filtering step will be the same for all -overlap queries independent to the value of   as discussed in section 1.
모on the other hand  algorithms that use inverted files could process overlap joins more efficiently. we propose an adaptation of bnl as follows. at each pass bnl merges the inverted lists found in bi for each tr. during merging  the number of occurrences for each ts.rid is counted. if this number is larger or equal to the threshold   i.e.  we already found a matching pair of rids   the pair htr.rid ts.ridi is written to a temporary file qi. the remainder of the list is written to a file ti  as before. notice that if  = 1  there will be no temporary files ti. in this special case we also compress the temporary qi's  since each tr joins with numerous ts. after processing all blocks  bnl merges all ti's to produce a last file qi+1 of qualifying results. the final phase scans all qi's  including the last one  and merges them to eliminate duplicates. notice that each multiway merge is performed with a single scan over the temporary lists or results  since these are produced sorted. this version of bnl is described in figure 1. we comment only on the marked line  1   which avoids writing a partial list to ti if all elements of tr are in the current block. if tr.r   li  tr cannot produce more qualifying pairs  since no more lists for tr will be  or have been  produced.
algorithm { i := 1; while there are more lists in sif {
i := i + 1;
bi := read next block of sif that fits in memory;
li := elements whose inverted list is in bi;
initialize temporary file ti; initialize temporary file qi; for each tuple tr 뫍 r do {
list tr  := slel  el 뫍 tr.r 뫇 el 뫍 li; for each ts.rid in list tr  do
if ts.rid appears at least  times then {
remove all ts.rid from list tr ; append htr.rid ts.ridi to qi; }
 1 	if list tr  =1   and tr.r * li then
write list tr  to ti;
}
}
n := i; merge all ti  1 뫞 i 뫞 n to produce qi+1; merge all qi  1 뫞 i 뫞 n + 1 to eliminate duplicates;
}
figure 1: block-nested loops  set overlap 
모ifj can also be adapted the same way as bnl for overlap joins. a set of temporary files and result files ti and qi are produced as the algorithm merges the inverted lists. a special feature of ifj is that  for  = 1 and if we ignore duplicate elimination  it can output results immediately and at the minimal cost of reading the inverted files.
1.	experiments
in this section we evaluate the performance of join algorithms on set-valued attributes. we first compare the proposed algorithms for set containment joins with psj  the previously proposed signature-based method. afterwards  we evaluate the performance of signature-based and inverted file methods for other join predicates. the experiments were performed on a pc with a pentium iii 1mhz processor and 1mb of memory  running linux 1.1. in the next subsection we describe the generator we used for our test data.
1	data generation
we generated synthetic relations with set-valued data using the same generator as in . the parameters of the generator are the relation cardinality  the average set cardinality in a tuple  the domain size of set elements  and a correlation percentage corr. the domain of size |d| is represented by the set of first |d| integers for simplicity  and without loss of generality  such a mapping is typically used in real applications to reduce the space requirements of the tuples and the comparison costs . the domain is split into 1 equal-sized subdomains. elements which fall in the same subdomain  model elements falling in the same real-world class  i.e.  correlated elements .
모the correlation percentage is used to tune the number of elements in a set which are correlated  i.e.  fall in the same class . thus  the relation is generated as follows. for each tuple  we pick a class according to a distribution  unless otherwise stated  we consider a uniform distribution . then corr% of the set elements are picked from this sub-domain and the rest of them are randomly chosen from the remaining 1 subdomains. for set containment joins the data were generated  such that the size of the join result was controlled to be in the order of |r|. finally  in all experiments  unless otherwise stated  r and s were generated to have the same cardinality  the correlation is set to 1%  and |d|=1. the same settings are used in the experiments of .
1	set containment joins
in this subsection  we compare the performance of three algorithms for set containment joins. the first is the stateof-the-art psj algorithm proposed in . the other two are bnl  described in section 1  and ifj  described in section 1 . we do not consider snl  since it is inferior to psj  as shown in . unless otherwise specified  we used all optimization techniques for our proposed algorithms   i.e.  keeping the set cardinality projections in memory  pipelining  pairwise join implementation . for fairness to psj  in the overall cost we included the construction of the inverted files  wherever applicable.
모in the first experiment  we compare the performance of the three methods for the following setting: |r| = |s|  the average set cardinality c is set to 1  the relation cardinality ranges from 1k to 1k and the memory buffer is set to 1% of the size of s on disk. figure 1 shows the cost of the three methods for various relation cardinalities.
모bnl is clearly the winner. the other algorithms suffer from the drawbacks discussed in section 1. in terms of scalability  bnl is also the best algorithm; its cost is subquadratic to the problem size. this is due to the fact that even with only a small memory buffer  sif is split to a few blocks and r is scanned only a few times. in our experimental settings and for all instances of figure 1  the number of blocks was just 1. on the other hand  both psj and ifj do not scale well with the problem size. ifj  in specific  produces a huge number of intermediate results  once the number of rids in the inverted lists increase. this is somehow expected  because ifj generates a quadratic number of candidate rid pairs for each pair of inverted lists it joins.
모figure 1 shows the number of pages accessed by each algorithm. the page size is set to 1k in all experiments. the figure indicates that the i/o cost is the dominant factor for all algorithms  especially for ifj  which generates a large number of temporary lists. the majority of the page accesses are sequential  thus the i/o cost translates mainly to disk transfer costs.
모figure 1 shows  where the time goes  in the experimental instance |r| = |s| = 1k. notice that the bar for ifj has been truncated for better visualization. starting from the bottom  each bar accumulates the i/o and computational costs for partitioning  psj  or building the inverted files  bnl and ifj   for joining  join phase of psj and overall join costs of bnl and ifj   and for verifying the candidate rid-pairs  this applies only to psj . the burden of each algorithm can be easily spotted.

figure 1: cost breakdown  set containment 
모the cost of psj is split evenly in all three phases. the algorithm generates a large number of replicated triplets for each ts 뫍 s. the triplets have to be written to the temporary files  re-read and joined with the partitions of r. thus  psj spends a lot of time in partitioning  much more than the time needed to construct an inverted file . the join phase of psj is also slow  requiring many computations to derive the candidate pairs. finally  the verification cost is also high due to the large number of false drops. notice that the parameters of psj have been tuned according to the analysis in  and they are optimal for each experimental instance. for example  if we increase the signature length the i/o cost of partitioning and joining the data increases at the same rate  but the number of candidates and the verification cost drop. on the other hand  decreasing the signature length leads to smaller partitions but explodes the number of candidate rid-pairs to be verified.
모the burden of ifs is the i/o cost of writing and reading back the temporary lists. the algorithm is less systematic than bnl  since it generates at a huge number of lists at a time  which cannot be managed efficiently. apart from this  its computational cost is also high  mainly due to sorting a
모

1 1 1 relation cardinality x1  set cardinality = 1 
1%
figure 1: buffer1%	1%	1%	1%	1%	1%
memory buffer  percentage of s 
scalability to memory	1	1	1	1
iteration
figure 1: effect of pipelining in
bnl	1%	1%	1%	1%	1%
memory buffer  percentage of s 
figure 1: effect of compressionfigure 1: scalability to the relation size


 1 1 1 relation cardinality x1  set cardinality = 1 
figure 1: scalability to the relation size  i/o 


1 1 1 set cardinality  relation cardinality = 1k 
figure 1: scalability to set cardinality

모
large number of lists prior to merging them   indicating that this method is clearly inappropriate for the set containment join. thus  we omit it from the remainder of the evaluation. on the other hand  bnl processes set containment joins very fast. notice that sif is constructed quite fast  due to the employment of the compression techniques. the join phase is slightly more expensive that the inverted file construction. in this experimental instance  r is scanned twice  dominating the overall cost of the algorithm  since the generated temporary results and sif are small  sif is just 1% of s   compared to the size of the relation.
모the next experiment  figure 1  compares psj with bnl for |r| = |s| = 1k and various values of the average set cardinality c. the performance gap between the two methods is maintained with the increase of c; their response time increases almost linearly with c. this is expected for bnl  since the lengths of the inverted lists are proportional to c. the i/o cost of the algorithm  not shown  increases with a small sublinear factor  due to the highest compression achieved. psj also hides a small sublinear factor  since the ratio of false drops decreases slightly with the increase of c. finally  we have to mention that the cost of ifj  not shown in the diagram  explodes with the increase of c  because of the quadratic increase of the produced rid-lists; the higher |tr.r| is  the larger the number of inverted lists that contain tr.rid  and the longer the expected time to visit all these lists in order to output the matching pairs for this tuple.
모we also compared the algorithms under different conditions   e.g.  |r| 1= |s|  different correlation values  etc.   obtaining similar results. bnl is typically an order of magnitude faster than psj. notice  however  that the efficiency of bnl depends on the available memory. in the next experiment  we validate the robustness of the algorithm for a range of memory buffer values. the settings for this and the remaining experiments in this section are |r| = |s| = 1k and c = 1. figure 1 plots the performance of psj and bnl as a function of the available memory buffer. notice that even with a small amount of memory  case 1% is around 1kb   bnl is significantly faster than psj. the algorithm converges fast to its optimal performance with the increase of memory. on the other hand  psj joins a large amount of independent partitioned information and exploits little the memory buffer.
모the next experiment demonstrates the effectiveness of the pipelining heuristic in bnl. we ran again the previous experimental instance for the case where the memory buffer is 1% the size of s. the number of blocks sif is divided to is 1 in this case. figure 1 shows the number of intermediate results generated by bnl at each pass  compared to the results generated by the basic version of bnl that does not use pipelining. the results are lists of variable size  some compressed  some not  and we measure them by the number of pages they occupy on disk. observe that pipelining reduces significantly the size of intermediate results as we proceed to subsequent iterations. many lists are pruned in the latter passes  because  i  they are unsuccessfully merged with the ones from the previous pass   ii  the corresponding tuple has already been pruned at a previous pass  or  iii  no more elements of the tuple are found in future lists and the current results are output. on the other hand  the basic version of bnl generates a significant amount of intermediate results  which are only processed at the final stage.
모finally  we demonstrate the effectiveness of compression in bnl. figure 1 shows the performance of bnl and a version of the algorithm that does not use compression  as a function of the available memory. the effects of using compression are two. first  building the inverted file is now less expensive. second  the number of inverted lists from sif that can be loaded in memory becomes significantly smaller. as a result  more passes are required  more temporary results are generated  and the algorithm becomes much slower. the performance of the two versions converges at the point where the uncompressed sif fits in memory. a point also worth mentioning is that the version of bnl that does not use compression is faster than psj  compare figures 1 and 1  even for small memory buffers  e.g.  1% of the size of s .
모to conclude  bnl is a fast and robust algorithm for the set containment join. first  it utilizes compression for better memory management. second  it avoids the extensive data replication of psj. third  it exploits greedily the available memory. fourth  it employs a pipelining technique to shrink the number of intermediate results. finally  it avoids the expensive verification of drops required by signature-based algorithms.
1	other join operators
in this section  we compare signature-based methods with inverted file methods for other join predicates. in the first experiment  we compare the performance of the three methods evaluated in the previous section for set equality joins under the following setting: |r| = |s|  the average set cardinality c is set to 1  the relation cardinality varies from 1k to 1k and the memory buffer is set to 1% of the size of s on disk. figure 1 shows the cost of the three methods for various relation cardinalities.

1 1 1 relation cardinality x1  set cardinality = 1 
figure 1: varying |r|  set equality join 
모observe that psj and bnl perform similarly in all experimental instances. they both manage to process the join fast for different reasons. psj avoids the extensive replication  unlike in the set containment join . it also manages to join the signatures fast in memory  using the prefix hashing heuristic. on the other hand  it still has to verify a significant number of candidate object pairs. the verification cost of the algorithm sometimes exceeds the cost of partitioning and joining the signatures.
모bnl is also fast. its performance improves from the set containment join case  although not dramatically. many partial lists and rids are pruned due to the cardinality check and the temporary results affect little the cost of the algorithm. the decrease of the join cost makes the index construction an important factor of the overall cost. finally  ifj performs bad for set equality joins  as well. the arbitrary order of the generated lists and the ad-hoc nature of the algorithm make it less suitable for this join operation.
모figure 1 shows the performance of the algorithms  when the relation cardinality is fixed to 1k and the set cardinality varies. the conclusion is the same: psj and bnl have similar performance  whereas the cost of ifj explodes with the set cardinality  because of the huge number of inverted lists that need to be merged.

1	1	1 set cardinality  relation cardinality = 1k 
figure 1: varying c  set equality join 
모in the final experiment we compare snl  bnl and ifj for set overlap joins. figure 1 shows the performance of the algorithms for |r| = |s| = 1k and c = 1. the extreme cost of the signature-based method makes it clearly inappropriate for set overlap joins. the selectivity of the signatures is low  even if a large signature length is picked  and the ratio of false drops is huge. in this setting  1% of the pairs qualified the signature comparison  whereas only 1% are actual results for =1. moreover  the increase of   and the selectivity of the join  does not affect the signature selectivity  as discussed in section 1 . bnl is faster than ifj  although it generates many temporary results. there are two reasons for this. first  some results are output immediately because we know that they cannot match with lists in other blocks. second  ifj spends a lot of time in sorting  due to the less systematic production of the temporary lists. notice that for  = 1 the cost of bnl is higher than for other values of  due to the higher overhead of the result size. on the other hand  ifj is less sensitive than bnl to the value of .

figure 1: varying   set overlap join 
모as an overall conclusion  bnl the most appropriate algorithm for set overlap joins  too. in the future  we plan to optimize further this algorithm for this join operator  and also study its adaptation to other related query types  like the set similarity join. arguably  the present implementation is not output sensitive; although the query result size reduces significantly with   this is not reflected to the evaluation cost  due to the a large number of temporary lists. an optimization we have not studied yet is to consider which of r and s should be placed as the  outer  relation in equality and overlap joins  since these operators are symmetric  as opposed to the set containment join .
1.	conclusions
in this paper  we studied the efficient processing of various join predicates on set-valued attributes. focusing on the set containment join  we introduced a new algorithm which creates an inverted file for the  container  relation s and uses it to find in a systematic way for each object in the  contained  relation r its supersets in s. this algorithm is a variation of block nested loops  bnl  that gracefully exploits the available memory and compression to produce fast the join results. bnl consistently outperforms a previously proposed signature-based approach  typically by an order of magnitude. if the relations are already indexed by inverted files1  join processing can be even faster.
모we also devised adaptations of signature-based and inverted file methods for other two join operators; the set equality join and the set overlap join. the conclusion is that signature-based methods are only appropriate for set equality joins. for the other join types  a version of bnl is always the most suitable algorithm. on the other hand  a method that joins two inverted files was found inappropriate for all join types.
모in the future  we plan to study additional  interesting set join operators. the set similarity join  retrieves object pairs which exceed a given similarity threshold. this join type is very similar to the -overlap join  however  the similarity function is usually more complex  depending on both overlap and set cardinality. another variation is the closest pairs query   which retrieves from the cartesian product r뫄s the k pairs with the highest similarity  or overlap . a similar operation is the all nearest neighbor query  which finds for each set in r its nearest neighbor in s. this query has been studied in   where inverted files were used  but the proposed algorithms were not optimized and only the i/o cost was considered for evaluating them.
acknowledgements
this work was supported by grant hku 1e from hong kong rgc.
