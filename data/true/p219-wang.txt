negative relevance feedback is a special case of relevance feedback where we do not have any positive example; this often happens when the topic is difficult and the search results are poor. although in principle any standard relevance feedback technique can be applied to negative relevance feedback  it may not perform well due to the lack of positive examples. in this paper  we conduct a systematic study of methods for negative relevance feedback. we compare a set of representative negative feedback methods  covering vector-space models and language models  as well as several special heuristics for negative feedback. evaluating negative feedback methods requires a test set with sufficient difficult topics  but there are not many naturally difficult topics in the existing test collections. we use two sampling strategies to adapt a test collection with easy topics to evaluate negative feedback. experiment results on several trec collections show that language model based negative feedback methods are generally more effective than those based on vector-space models  and using multiple negative models is an effective heuristic for negative feedback. our results also show that it is feasible to adapt test collections with easy topics for evaluating negative feedback methods through sampling.
categories and subject descriptors: h.1  information search and retrieval : retrieval models
general terms: algorithms
keywords: negative feedback  difficult topics  language models  vector space models
1. introduction
　no retrieval model is able to return satisfactory results for every query. indeed  a query might be so difficult that a large number of top-ranked documents are non-relevant. in such a case  a user would have to either reformulate the query or go far down on the ranked list to examine more documents. thus studying how to improve search results for such difficult topics is both theoretically interesting and practically important.
　a commonly used strategy to improve search accuracy is through feedback techniques  such as relevance feedback  1  1   pseudo-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  singapore.
copyright 1 acm 1-1-1/1 ...$1.
relevance feedback  1  1   and implicit feedback . in the case of a difficult topic  we likely will have only negative  i.e.  nonrelevant  examples  raising the important question of how to perform relevance feedback with only negative examples. we refer to this problem as negative feedback. ideally  if we can perform effective negative feedback  when the user could not find any relevant document on the first page of search results  we would be able to improve the ranking of unseen results in the next a few pages.
　however  whether such negative feedback can indeed improve retrieval accuracy is still largely an open question. indeed  the effectiveness of current feedback methods often rely on relevant documents; negative information  such as non-relevant documents  is mostly ignored in past work  1  1 .
　on the surface  any standard relevance feedback technique can be applied to negative relevance feedback. however  our recent work  has shown that special care and special heuristics are needed to achieve effective negative feedback. specifically  in this work  we have shown that some language model-based feedback methods  although quite effective for exploiting positive feedback information  cannot naturally handle negative feedback  thus several methods were proposed to perform negative feedback in language modeling framework. however  this study is neither comprehensive nor conclusive for several reasons:  1  it is only limited to language models; vector space models have not been evaluated.  1  the results are evaluated over only one collection.  1  the lack of systematic experiment design and result analysis makes it hard to know the advantages or disadvantages of different methods.
　in this paper  we conduct a more systematic study of different methods for negative relevance feedback. our study is on two representative retrieval models: vector space models and language models. we first categorize negative feedback techniques into several general strategies: single query model  single positive model with single negative query model  and single positive model with multiple negative query models. following these strategies  we then develop a set of representative retrieval methods for both retrieval models. systematic comparison and analysis are conducted on two large representative trec data sets. ideally  test sets with sufficient naturally difficult topics are required to evaluate these negative feedback methods  but there are not many naturally difficult topics in the existing trec data collections. to overcome this difficulty  we use two sampling strategies to adapt a test collection with easy topics to evaluate negative feedback. the basic idea of our sampling methods is to simulate difficult queries from easy ones through deleting a set of relevant documents so that the results become poor. the effectiveness of these sampling methods is also verified on the trec data sets.
　our systematic study leads to several interesting conclusions. we find that language model-based negative feedback methods are generally more effective and robust than those based on vector space models possibly due to more accurate learning of negative models. while cluster hypothesis  generally holds for relevant documents  our results show that negative documents do not cluster together. thus adapting standard relevance feedback to learn a single query model is not optimal for negative feedback  and using multiple negative models is more effective than a single negative model since negative documents may distract in different ways. our results also show that it is feasible to adapt test collections with easy topics  through sampling  to evaluate negative feedback methods.
　the rest of the paper is organized as follows. in section 1  we review the related work. in section 1  we describe our problem setup and different techniques for negative feedback. we describe our sampling methods to simulate difficult topics by adapting easy ones in section 1. experiments are analyzed and discussed in section 1. we conclude this paper and discuss our future work in section 1.
1. related work
　the study of difficult queries has attracted much attention recently  partly due to the launching of the robust track in the trec conference  which aims at studying the robustness of a retrieval model and developing effective methods for difficult queries  1  1 . however  the most effective methods developed by the participants of the robust track tend to rely on external resources  notably the web  to perform query expansion  which has in some sense bypassed the difficulty of the problem as in reality  there is often no such external resource to exploit  or otherwise  the user would have directly gone to the external resource to find information. indeed  the web resource would not help improve search accuracy for difficult topics on the web itself. in our work  we aim at exploiting negative feedback information in the target collection from which we want to retrieve information.
　there has been some work on understanding why a query is difficult  1  1  1   on identifying difficult queries   and on predicting query performance . but none of this work has addressed the important question of how to improve search accuracy for difficult queries.
　feedback techniques have been extensively studied and mostly shown to be effective to improve retrieval accuracy  1  1  1  1  1  1  1  1 . in general  most feedback techniques rely on positive documents - documents that are explicitly judged as relevant or implicitly assumed to be relevant - to provide useful related terms for query expansion. in contrast  negative  i.e.  non-relevant  documents have not been found to be very useful. in general  exploiting non-relevant information is largely unexplored; query zone  appears to be the only major heuristic proposed to effectively exploit non-relevant information in document routing tasks. it showed that using non-relevant documents which are close to the original queries is more effective than using all non-relevant documents in the whole collection. however  this problem was studied for document routing tasks and a lot of relevant documents are used. our problem setting is quite different in that we only have non-relevant documents for feedback  and we start with non-relevant documents close to a query to study how to use this negative information optimally in ad hoc retrieval.
　our recent work  is the first study on the problem of negative feedback in language models. it shows that special techniques are needed to handle negative feedback. our current work can be regarded as an extension of this previous work to include additional retrieval models  additional heuristics  and making more conclusive findings. the main differences between our current work and the previous work  include:  1  we extend previous study and propose several general negative feedback strategies that can be applied to both vector space and language models.  1  we study two sampling methods to construct larger collections to evaluate negative feedback methods.  1  our experiments are more systematic and conducted over more collections.
1. negative feedback techniques
1 problem formulation
　we formulate the problem of negative feedback in a similar way as presented in . given a query q and a document collection c  a retrieval system returns a ranked list of documents l. li denotes the i-th ranked document in the ranked list. we assume that q is so difficult that all the top f ranked documents  seen so far by a user  are non-relevant. the goal is to study how to use these negative examples  i.e.  n = {l1 ...  lf}  to rerank the next r unseen documents in the original ranked list: u = {lf+1 ... lf+r}. we set f = 1 to simulate that the first page of search results are irrelevant  and set r = 1. we use the following notations in the rest of the paper:
s q d  is the relevance score of document d for query q. c w d  is the count of word w in document d. c w q  is the count of word w in query q.
|c| is the total number of documents in the collection c. df w  is the document frequency of word w.
|d| is the length of document d. avdl is the average document length.
n is the set of negative feedback documents.
u is the set of unseen documents to be reranked.
1 general strategies for negative feedback
1.1 query modification
　since negative feedback can be regarded as a special case of relevance feedback where no positive example is available  our first general strategy is simply to apply any existing feedback methods  e.g.  rocchio   to use only non-relevant examples. we call this strategy query modification because most existing feedback methods would achieve feedback through modifying the representation of a query based on relevant and non-relevant feedback documents. in effect  they often introduce additional terms to expand a query and assign more weight to a term with more occurrences in relevant documents and less weight or negative weight to a term with more occurrences in non-relevant documents.
　some existing feedback methods  such as rocchio method   already have a component for using negative information  so they can be directly applied to negative feedback. however  other methods  such as model-based feedback methods in language modeling approaches   can not naturally support negative feedback  thus extension has to be made to make them work for negative feedback . later we will further discuss this.
　note that with this strategy  we generally end up with one single query model/representation which combines both positive information from the original query and negative information from the feedback documents.
1.1 score combination
　the query modification strategy mixes both positive and negative information together in a single query model. sometimes it is not natural to mix these two kinds of information as in the case of using generative models for feedback . a more flexible alternative strategy is to maintain a positive query representation and a negative query representation separately  and combine the scores of a document w.r.t. both representations. we call this stategy score combination.
　with this strategy  negative examples can be used to learn a negative query representation which can then be used to score a document based on the likelihood that the document is a distracting non-relevant document; such a score can then be used to adjust the positive relevance score between the original query and the corresponding document.
　intuitively  a document with higher relevance score to the negative query representation can be assumed to be less relevant  thus the final score of this document can be computed as
　　scombined q d  = s q d    β 〜 s qneg d   1  where qneg is a negative query representation and β is a parameter to control the influence of negative feedback. when β = 1  we do not perform negative feedback  and the ranking would be the same as the original ranking according to query q. a larger value of β causes more penalization of documents similar to the negative query representation.
　equation  1  shows that either a high score of s q d  or a low score of s qneg d  would result in a high score of scombined q d . this means that the proposed score combination may favor nonrelevant documents if they have lower similarity to the negative model; this is risky because the negative query representation is only reliable for filtering out highly similar documents. thus a more reasonable approach would be to only penalize documents which are most similar to the negative query model and avoid affecting the relevance scores of other documents. to achieve this goal  instead of penalizing all the documents in u  we need to penalize only a subset of documents that are most similar to the negative query. we propose to use the following two heuristics to select documents for penalization  i.e.  adjusting their scores using equation  1  :
heuristic 1  local neighborhood : rank all the documents in u by the negative query and penalize the top ρ documents. heuristic 1  global neighborhood : rank all the documents in c by the negative query. select  from the top ρ documents of this ranked list  those documents in u to penalize.
　in both cases  ρ is a parameter to control the number of documents to be penalized and would be empirically set. the two heuristics essentially differ in how this ρ value affects the number of documents in u to be penalized. in heuristic 1  the actual number of documents in u to be penalized is fixed  i.e.  ρ  but in heuristic 1  it is dynamic and could be smaller than ρ  because the top ρ documents most similar to the negative query are generally not all in the set of u. if we are to set ρ to a constant for all queries  intuitively heuristic 1 can be more robust than heuristics 1  which is confirmed in our experiments.
　how do we compute the negative query repsentation qneg and the score s qneg d   a simple strategy is to combine all the negative information from n to form a single negative query representation  which would be referred to as  single negative model   singleneg . however  unlike positive information  negative information might be quite diverse. thus  it is more desirable to capture negative information with more than one negative query model. formally  let qineg  where 1 ＋ i ＋ k  be k negative query models  we may compute s qneg d  as follows:
k
s qneg d  = f  {s qineg d } 
i=1
where f is an aggregation function to combine the set of k values.
we call this method  multiple negative models   multineg .
1.1 summary
　we have discussed two general strategies with some variations for negative feedback  which can be summarized as follows:  1  singlequery: query modification strategy;  1  singleneg: score combination with a single negative query model;  1  multineg: score combination with multiple negative query models. for both singleneg and multineg models  we can use either of the two heuristics proposed in the previous subsection to penalize documents selectively. in the next subsection  we discuss some specific ways of implementing these general strategies in both vector space models and language models:
1 negative feedback in vector space model
　in vector space models  documents and queries are represented as vectors in a high-dimensional space spanned by terms. the weight of a term w in document d can be computed in many different ways and typically a similar measure such as dot product is used to score documents.
in our experiments  we use the following bm1 weight :
		 1 
where k1 and b are parameters. the weight of a query term is set to the raw term frequency  i.e.  c w q . we compute the relevance
	★ 	★ 	★ 	★ 
score using the dot product: s q d  = q ， d where d and q represent document vector and query vector  respectively.
1.1 singlequery strategy
　the rocchio method  is a commonly used feedback method in vector space models. the idea is to update a query vector with both relevant and non-relevant documents. when only non-relevant documents n are available  the rocchio method can be written as
		 1 
this gives us an updated query vector  which can be used to rerank documents in u.
1.1 singleneg strategy
　singleneg adjusts the original relevance score of a document with a single negative query. we compute the negative query as the center of negative documents  i.e.  .
using equation  1   the combined score of a document d is
	★ 	★ 	★ 	★ 
	scombined q d  = q ， d   β 〜 qneg ， d.	 1 
it is straightforward to verify that equation  1  and  1  are equivalent. however  singleneg has the advantage of allowing us to penalize negative documents selectively using either of the two heuristics presented earlier.
1.1 multineg strategy
　multineg adjusts the original relevance score of a document with multiple negative queries which can be obtained  e.g.  through clustering. in our experiments  we take each negative document as a negative query and use max as our aggregation function. intuitively  max allows us to penalize any document that is close to at least one negative document. thus
 ★1 ★ 
s qneg d  = max    {q ， d} .
q1（n
this score is then combined with s q d  to rerank the documents in u. again  we have two variants of this method corresponding to applying the two heuristics discussed above.
1 negative feedback for language models
　kl-divergence retrieval model  is one of the most effective retrieval models in the language modeling framework. the relevance score is computed based on the negative kl-divergence between query model θq and document model θd

where v is the set of words in our vocabulary. the document model θd needs to be smoothed and an effective method is dirichlet smoothing  where p w|c  is the collection language model and μ is a smoothing parameter.
　unlike vector space models  it is not natural to directly modify a query model using negative information in language model since no term can have a negative probability. in our recent work   several methods have been proposed for negative feedback in the language model framework. we adopt the methods there and combine them with the two heuristics discussed earlier for document penalization.
1.1 singleneg strategy
　singleneg adjusts the original relevance score of a document with a single negative model. let θq be the estimated query model for query q and θd be the estimated document model for document d. let θn be a negative topic model estimated based on the negative feedback documents n = {l1 ... lf}. in singleneg method  the new score of document d is computed as
         s q d  =  d θq||θd  + β ， d θn||θd .  1  note that we only penalize documents selected by either of the two heuristics.
　we now discuss how to estimate negative model θn given a set of non-relevant documents n = {l1 ...  lf}. we use the same estimation method as discussed in . in particular  we assume that all non-relevant documents are generated from a mixture of a unigram language model θn  to generate non-relevant information  and a background language model  to generate common words . thus  the log-likelihood of the sample n is
l n|θn  = x x c w d log  1   λ p w|θn  + λp w|c  
d（n w（d
where λ is a mixture parameter which controls the weight of the background model and the background model is estimated with
. given a fixed λ  λ = 1 in our experiments   a standard em algorithm can then be used to estimate parameters p w|θn . the result of the em algorithm gives a discriminative negative model θn which eliminates background noise.
1.1 singlequery strategy
　singlequery method is to update original query with negative information. since every term has a non-negative probablity in a query model  there is no natural way to update original queries with negative information. however  given equation  1   a singlequery method can be derived after applying algebra transformation and ignoring constants that do not affect document ranking in the following way
	s q d 	=	 d θq||θd  + β ， d θn||θd 
rank
= pw（v  p w|θq    β ， p w|θn  logp w|θd 
the above equation shows that the weight of term w is  p w|θq   β ， p w|θn  log p w|θd   which penalizes a term that has high probability in the negative topic model θn. in this way   p w|θq   β ， p w|θn   can be regarded as the updated query model  which in some sense is the language modeling version of rocchio. for consistence with vector space model  we use γ to replace β and use  p w|θq    γ ， p w|θn   as the updated query model. for this query model  we use the equation above to rerank all the documents in u. note that for singlequery method  we can not apply the two penalization heuristics.
1.1 multineg strategy
　multineg adjusts the original relevance scores with multiple negative models. we use the same em algorithm as singleneg to estimate a negative model θi for each individual negative document li in n. we then obtain f negative models and combine them as
.
1. create test collections with sampling
　in order to evaluate the effectiveness of negative feedback methods  it is necessary to have test collections with sufficient difficult topics. however  trec collections do not have many naturally difficult queries. in this section  we describe two sampling strategies to construct simulated test collections by converting easy topics to difficult topics.
　in our problem formulation  a query is considered to be difficult if none of the top 1 documents retrieved by a retrieval model is relevant. thus  in order to convert an easy query to a difficult one  our main idea of sampling methods is to delete some relevant documents of an easy query and assume these documents do not exist in the collection so that all top 1 documents are non-relevant. we now discuss two different ways to delete relevant documents:
　minimum deletion method: given a query and a ranked document list for the query  we keep deleting the top ranked relevant document until none of the top 1 ranked documents of the list is relevant. we assume that the deleted relevant documents do not exist in the collection.
　random deletion method: given a query and all of its relevant documents  we randomly delete a relevant document each time until none of the top 1 documents of the ranked list is relevant. again  we assume that the deleted documents do not exist in the collection.
　in both methods  we keep deleting relevant documents until none of top 1 ranked documents is relevant. note that the constructed collections are dependent on retrieval models. after deletion  we obtain a new ranked list whose top 1 documents are irrelevant for a query. we then use these 1 irrelevant documents for negative feedback to rerank the next 1 documents in this new ranked list.
1. experiments
　to evaluate the effectiveness of negative feedback techniques  we construct our test collections based on two representative trec data sets: robust track and web track data sets.
1 data sets
　our first data set is from the robust track of trec 1. it has about 1 news articles . on average  each document has 1 terms. we use all the 1 queries as our base query set.
this data set is denoted by  robust. 
robustgovlmvsmlmvsmμ = 1k1 = 1 b = 1μ = 1k1 = 1 b = 1table 1: optimal parameter values.
query setsrobustgovlmvsmlmvsmqs1: p 111qs1: 1 ＋p 1＋ 111qs1: 1 ＋p 1＋ 111all11table 1: the query sets used in our experiments.
　the second data set is the gov data set used in the web track of trec 1 and 1. it is about 1 gb in size and contains 1 1 web pages crawled from the  .gov  domain in 1. on average  each document has 1 terms. in our experiment  we only use the content of the pages for retrieval. there are 1 types of queries used in web track: homepage finding  named page finding  and topic distillation. we use the queries with topic distillation type in both web track 1 and 1. in total  we have 1 queries in our base set  1 from web track 1 and 1 from web track 1 . we denote this data set by  gov. 
　for both data sets  preprocessing involves only stemming but without removing any stopword. since our goal is to study difficult queries  we construct different types of query sets from our base sets as follows.
1.1 naturally difficult queries
　the first type of query set consists of those naturally difficult queries. in this paper  we say that a query is a naturally difficult query if its p 1  given a retrieval model.
　for both language models  lm  and vector space models  vsm   we use their standard ranking functions to select their naturally difficult queries respectively. we first optimize the parameters of μ for lm and k1 and b for the vsm using the base set of queries on each data set. the optimal parameters are shown in table 1. all these parameters are fixed in all the following experiments. using the optimal parameter setting  we then select those queries whose p 1 as our naturally difficult queries. the row of qs1 in table 1 shows the number of queries in this type of query sets.
1.1 simulated difficult queries
　since there are not many naturally difficult queries  we further used the two deletion-based sampling methods to construct simulated difficult queries from easy ones. in our experiments  we use two types of easy queries. the first type consists of those queries whose p 1 satisfy 1 ＋p 1＋ 1  qs1 in table 1  and the second consists of those queries whose p 1 satisfy 1 ＋p 1＋ 1  qs1 in table 1 . again  all these queries are selected for the two retrieval models on the two data sets respectively.
　the last type of query sets is the all query sets which are the union of the three types of query sets. table 1 gives a summary of all the query sets used in our experiments.
1 retrieval effectiveness
　our experiment setup follows section 1 to rerank the next unseen 1 documents. we use two sets of performance measures:  1  mean average precision  map  and geometric mean average precision  gmap   which serve as good measures of the overall ranking accuracy.  1  mean reciprocal rank  mrr  and precision at 1 documents  p 1   which reflect the utility for users who only read the very top ranked documents.

figure 1: the performance of rocchio feedback under different parameters.
1.1 apply existing relevance feedback
　in this section  we use the rocchio method in vsm to show that existing relevance feedback techniques do not work well if we only have negative information. the standard rocchio method updates a query as

where r is the set of relevant feedback documents. we use the query sets of  all  type. for any query  we first obtain its original ranking list. starting from the top of the ranking list  we search downward until we arrive at a cutting point  before which we just find 1 irrelevant documents. all the documents before the cutting points  including both relevant and non-relevant documents  are used in the rocchio feedback. the updated query vectors are then used to rerank the next 1 documents starting from the cutting points. in our experiments  we set α = 1 and vary β and γ. the results are shown in figure 1. from this figure  we can see that if we have relevant information  i.e.  β   1  the map values can be improved dramatically. however  when we do not have any relevant information  i.e.  β = 1  negative information always hurts map. this means that the existing relevance feedback techniques are not effective if only negative information is available for feedback  although they are very effective for positive feedback. this also shows that special techniques are needed for negative relevance feedback.
1.1 overall accuracy comparison
　using naturally difficult query sets qs1  in this section  we study the effect of different negative feedback techniques. for both lm and vsm on the two data sets  we show their performance of the original ranking  originalrank  and the 1 negative feedback methods: singlequery means the singlequery strategy; singleneg1 and singleneg1 are the singleneg strategy plus heuristic 1 and heuristic 1 respectively; multineg1 and multineg1 are the multineg strategy plus heuristic 1 and heuristic 1 respectively.
　we vary the parameters for each method: γ from 1 to 1 for singlequery  β from 1 to 1 and ρ from 1 to 1 for singleneg and multineg methods. in table 1  we compare the optimal performance  selected according to gmap measure  of all methods. from this table  we have the following observations:
　 1  lm approaches usually work better than vsm approaches. on the robust data  lm can improve the map from 1 to 1  1% relative improvement  but vsm can only improve from 1 to 1  1% relative improvement. on the gov data  lm approaches can significantly improve over both originalrank and singlequery approaches  but vsm approaches can not consistently give improvements.
robust+vsmmapgmapmrrp 1originalrank1111singlequery1111singleneg1.1 +111singleneg1.1 +111multineg1.1 +111multineg1.1 +111robust+lm
	map	gmap	mrr	p 1
	originalrank	1	1	1	1
singlequery 1 1 1 1 singleneg1.1  1 1 1
	singleneg1.1 +	1	1	1
multineg1.1 +	1	1	1 multineg1.1 +	1	1	1
gov+vsmmapgmapmrrp 1originalrank1111singlequery1111singleneg1.1 111singleneg1.1 111multineg1.1.1.1.1multineg1.1.1.1.1gov+lm
	map	gmap	mrr	p 1
	originalrank	1	1	1	1
	singlequery	1	1	1	1
	singleneg1.1 	1	1	1
	singleneg1.1 	1	1	1
	multineg1.1 +	1	1	1
	multineg1.1 +	1	1	1
table 1: optimal results of lm and vsm on the robust and gov data sets. * and + mean improvements over originalrank and singlequery  respectively  are statistically significant. we only show the significance tests on map values. note that the values are not comparable across tables since each table corresponds to a different qs1 query set in table 1.
posnegmneggroup1 relevant111group1 irrelevant111table 1: similarity between pos  neg  mneg learned from group1 and relevant/irrelevant documents in group1.
　 1  for lm approaches  multineg always works better than singlequery and singleneg. this shows that irrelevant documents may distract in different ways and do not form a coherent cluster. to verify this  we use the cutting point defined in section 1.1 to form two groups of documents for each query: all documents before the cutting point form group1 and the next 1 documents after the cutting point form group1. we learn a positive  denoted as pos  and a negative language model  denoted as neg  using the relevant and non-relevant documents in group1. using the exponential transform of negative kl-divergence as the similarity measure  we calculate the average similarity between pos/neg and relevant/irrelevant documents of group1. the average values over all queries are shown in table 1. we can see that pos has a notably higher similarity to relevant documents than to irrelevant documents in group1  but neg does not have a notably higher similarity to irrelevant than relevant documents. in this table  we also show the results of multiple negative models  denoted as mneg . clearly  mneg can distinguish between relevant and irrelevant documents better than neg  confirming that negative documents are more diverse and multineg is more appropriate for negative feedback.
　 1  the results of vsm are mixed  and multineg can not yield notable improvement on the gov data. one possible reason is that the negative query vector generated using one single document in multineg tends to over-emphasize rare terms due their high idf values. in table 1  we show two documents g1-1 and g1-1 in the gov data set and their high-weight terms in the extracted negative query vectors. it is clear that vsm is biased towards those rare words such as  xxxxx  and  1   which makes the computed negative vectors less powerful to push down those similar irrelevant documents. for lm  the extracted terms are much better. this means that lm is more powerful to pick up more meaningful terms from negative documents and thus works better on gov data.
　this may also explain why singleneg in vsm is generally more effective than multineg on the gov data set: singleneg uses mul-
g1-1g1-1vsmlmvsmlmxxxxx 1mine 1pest 1pest 1csic 1industri 1mgmt 1safeti 1quarri 1miner 11.1ds 1naic 1metal 1ds 1mgmt 1bitumin 1or 1ipm 1nih 1table 1: two examples of extracted negative models.
tiple negative documents to compute the negative models. while the rare words may bias the negative model computed from a single negative document in multineg  their weights are small in singleneg since they are not common in all the negative documents.
1.1 results with simulated difficult queries
　we have proposed two deletion-based sampling methods to make an easy query artificially difficult. in this section  we show the retrieval results on simulated difficult queries using all query sets. we only show the gmap values in table 1. for random deletion  we run it 1 times and the average performance values are reported here. in this table  we show the results of both deletion methods on both retrieval models and both data sets. since random deletion deletes more relevant documents for each query than minimum deletion  it is expected that its overall performance is much lower than that of minimum deletion. the relative performance of different negative feedback methods is  however  similar to what we observed on the naturally difficult query sets  further confirming that the effectiveness of lm approaches and the multineg strategy.
1 effectiveness of sampling methods
1.1 evaluation methodology
　to see whether a simulated test set generated using our sampling methods is as good as a test set with naturally difficult queries for evaluating negative feedback  we use both to rank different negative feedback methods based on their retrieval accuracy  e.g.  map  and compare the two rankings; if they are highly correlated  it would indicate that the simulated test set can approximate a  natural  data set well.
robust+lmrobust+vsmgov+lmgov+vsmminimumrandomminimumrandomminimumrandomminimumrandomoriginalrank11111111singlequery11111111singleneg1.1.1.1.1.1.1.1.1singleneg1.1.1.1.1.1.1.1.1multipleneg1.1.1.1.1.1.1.1.1multipleneg1.1.1.1.1.1.1.1.1table 1: the gmap values of different methods on the simulated difficult query sets using minimum and random deletion methods.
robust+lmrobust+vsmgov+lmgov+vsmmapgmapmapgmapmapgmapmapgmapminimum11111111random11111111table 1: kendall's τ coefficients between naturally difficult and simulated difficult queries.　formally  assume that we have n negative retrieval functions. we can rank them based on their performance on the  gold standard  set  i.e.  the naturally difficult queries . this would be our  gold standard ranking.  similarly  we can use the simulated difficult queries to rank all these retrieval functions. we then compute the correlation between these two ranking lists based on kendall's τ rank coefficient. given two ranking lists r1 and r1 of n retrieval functions  the coefficient is defined as
.
the range of the coefficient is between  1 and 1. when τ r1 r1    1  r1 and r1 are positively correlated. the larger the value  the higher the correlation. τ r1 r1  = 1 if r1 and r1 are exactly the same.
1.1 results
　we construct the simulated difficult queries using both minimum and random deletion methods. again we run the random method 1 times and uses the average values to rank retrieval functions.
　our retrieval functions are from the 1 methods. for each method  we vary its parameter setting in a certain range. each parameter setting will give us a different retrieval function. in total we have 1 retrieval functions.
　table 1 shows the kendall's τ correlation coefficients between the naturally difficult queries qs1 and the simulated difficult queries on all query sets using the two deletion methods. from this table  we can see that both deletion methods are positively correlated with the naturally difficult queries. this confirm that our two deletion methods are reasonable to convert an easy query to a difficult one. overall  random deletion is better than minimum deletion. comparing two measures gmap and map  we can see that the simulated difficult queries are more consistent with the naturally difficult queries on the gmap measure. this indicates that gmap is more appropriate as a measure on the simulated difficult queries than map. indeed  gmap has been used in robust track to evaluate difficult queries and this shows the reasonableness of our deletion methods to simulate difficult queries.
1 parameter sensitivity study
　in this section  we study the parameter sensitivity. due to space limit  we only show the results on robust data set with the naturally difficult query set qs1; other results are similar.
　figure 1 shows the impact of γ on the singlequery method for both lm and vsm. we can see that singlequery can not effectively use the negative feedback information and it is quite sensitive if γ is larger. figure 1 shows the impact of score combination parameter β where we set ρ = 1. all methods have the same level of sensitivities to β value. figure 1 shows the impact of the penalization scope parameter ρ. it can be seen that singleneg1 and multineg1 are very sensitive to this parameter  while singleneg1 and

figure 1: the impact of γ for singlequery method.

figure 1: impact of β for singleneg and multineg.
multineg1 are more robust. these results confirm that heuristic 1 is more stable than heuristic 1 in general. eventually  when ρ is large enough  the performance of singleneg1 and multineg1 will drop as we penalize more documents which are not very similar to negative models. finally  we study the impact of the number of feedback documents in multineg. we set f = 1 but we only use a subset of these 1 documents in negative feedback. the result is in figure 1 and it shows that we can get more improvement according to both map and gmap if we use more documents in negative feedback. this means that our method can help more when a user accumulates more negative information.
1. conclusions and future work
　negative feedback is very important because it can help a user when search results are very poor. in this paper  we conducted a

figure 1: impact of ρ in singleneg and multineg.

figure 1: impact of the number of feedback documents in multineg.
systematic study of negative relevance feedback techniques. we proposed a set of general strategies for negative feedback and compared their instantiations in both vector space model and language modeling framework. we also proposed two heuristics to increase the robustness of using negative feedback information. experiment results show that modeling multiple negative models is more effective than a single negative model and language model approaches are more effective than vector space model approaches. studying negative feedback needs a test set with sufficient difficult queries. we further proposed two sampling methods to simulate difficult queries using easy ones. our experiments show that both sampling methods are effective.
　this work inspires several future directions. first  we can study a more principled way to model multiple negative models and use these multiple negative models to conduct constrained query expansion  for example  avoiding terms which are in negative models. second  we are interested in a learning framework which can utilize both a little positive information  original queries  and a certain amount of negative information to learn a ranking function to help difficult queries. third  queries are difficult due to different reasons. identifying these reasons and customizing negative feedback strategies would be much worth studying.
1. acknowledgements
　we thank the anonymous reviewers for their valuable suggestions. this work is in part supported by the national science foundation under award numbers iis-1 and iis-1.
