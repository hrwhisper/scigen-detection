learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. when the imbalance is large  classification accuracy on the smaller class es  tends to be lower. in particular  when a class is of great interest but occurs relatively rarely such as cases of fraud  instances of disease  and regions of interest in largescale simulations  it is important to accurately identify it. it then becomes more costly to misclassify the interesting class. in this paper  we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples  smote  to improve minority class accuracy. the f-value serves as the evaluation function. experimental results show the wrapper approach is effective in optimization of the composite f-value  and reduces the average cost per test example for the datasets considered. we report both average cost per test example and the cost curves in the paper. the true positive rate of the minority class increases significantly without causing a significant change in the f-value. we also obtain the lowest cost per test example  compared to any result we are aware of for the kdd cup-1 intrusion detection data set.
categories and subject descriptors
i.1  artificial intelligence : learning - induction; h.1  database management : applications - data mining
general terms
algorithms  performance  design  experimentation
keywords
cost-sensitive learning and evaluation  imbalanced datasets  wrapper  under-sampling  smote
1. introduction
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
ubdm'1 august 1  1  chicago  illinois  usa.
copyright 1 acm 1-1/1 ...$1.
　in this work  we focus on the problem of learning a classification model from imbalanced data sets. an imbalanced data set is one in which there is a significant difference in the number of examples in a set of classes. the imbalanced datasets pose an economic or utility problem  as there is usually a higher cost in misclassifying the interesting class. a simple consistent guess can become an accurate classifier  by classifying everything as the majority class  but that is not useful for the problem at hand. on the other hand  a simple guess classifying everything as the interesting class will also not work due to the number of false positives. one wants a high number of true positives  while maintaining a low false-positive rate.
　there are many examples of imbalanced data sets where the minority class is of interest. for example  cellular-phone fraud or credit card fraud data are typically comprised of a very small proportion of the the fraudlent cases  minority class   1  1  1 . however  it is quite important to predict a fraudulent transaction. it is also important to minimize the false positives  the nonfraudulent transactions that are predicted to be fraudulent  because these cost time to investigate and can potentially upset the customer. thus  there is a non-zero cost associated with the false positives as well. typically  the cost with the false negatives will be the cost of the transaction. we don't want a system that will strongly target true positives at the expense of a high false positive rate  thereby increasing the total cost of the operation.
　as another example  large-scale simulations can be based on extremely large data sets. some simulations are replacing or augmenting physical experiments. this requires that they be done in great detail  1  1 . however  the process of building very large-scale simulations and examining them for correctness when looking for important  subtle details may prevent all areas of interest from being viewed . in any event  the process of validating a simulation can take weeks to months. a similar amount of time is required to actually utilize and explore the simulation. this is indicative of the great opportunity for building intelligent tools which can help the simulation designers/users find regions of interest and/or anomalies quickly. there is a cost involved not only in correctly displaying the regions of interest but also the costs in time. hence  the intelligent tool should not only be  fast   but also accurately identify the interesting regions  without too many false alarms. having many false alarms for the user to browse through can inadvertently increase the cost in terms of the time spent. as these two examples highlight  there is a  utility  associated with the usage of a technique. that utility is comprised of various costs of errors  time spent  etc.
　we investigate an enhancement to a particular composite approach  combining over-sampling by creating new examples and under-sampling  for dealing with imbalanced data sets. the synthetic minority oversampling technique  smote  creates synthetic examples from minority classes . we also under-sample the majority class es  to obtain higher accuracy on the minority class es  without greatly increasing the number of false positives. however  previous work has not shown how to effectively set the amounts of under-sampling and smote for a given dataset. in this paper  we explore an automated method to do this. we set up the method such that the f-value  is optimized. we chose the f-value as it is a composite measure that incorporates both the false positives and false negatives. hence  if an approach significantly increases the true positives  but also increases the false positives  then the f-value will appropriately reflect that. we evaluate the final performance of the classifiers under a cost-based framework using costcurves and average cost per test example.
　it is important to identify the potentially optimal undersampling and smote percentages. the amount of sampling performed to mitigate the imbalance in class distribution will have an effect on the performance of the classifier. we want to reduce the costs per test example. the utility of the learning algorithm for a particular domain or task is strongly dependent on the right amount of sampling and the examples distribution in the dataset. each dataset and the corresponding class distribution will have its own requirements . the computational time and resources spent deploying the wrapper technique should be mitigated by the reduced cost per test example  and a higher detection of the interesting class or regions in the dataset. there is a tradeoff between the time spent in learning or searching for the parameters  and the relative reduction in the costs or improvement in the true positives on the testing set. we will show that minority class accuracy is improved on several data sets with only small increases in false positive predictions. in addition  we will also show that our approach produces much reduced costs per test example. the approach is shown to be both tractable computationally and effective in choosing the parameters.
1. learning from imbalanced datasets
　researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes  undersampling the majority classes  assigning different costs for different misclassification errors  learning by recognition as opposed to discrimination  etc  1  1  1  1  1  1  1  1  1 . there is a significant body of research comparing the various sampling methods  1  1  1  1  1 . sampling strategies have almost become the de facto standard for countering the imbalance in datasets . with all this there is still no answer on how to do the sampling required for obtaining good classifier accuracies on minority classes.
　there are a number of different approaches that can be applied to build classifiers on imbalanced data sets. in this work  we examined under sampling and over-sampling by creating synthetic examples of minority classes. undersampling the majority class can reduce the bias of the learned classifier towards it and thus improve the accuracy on the minority classes.
　some studies  1  1  have been done which combined under-sampling of majority classes with over sampling by replication of minority classes. while japkowicz  found this approach very effective  ling and li  were not able to get significant improvement in their performance measures. japkowicz experimented with only one-dimensional artificial data of varying complexity whereas ling and li used real data from a direct marketing problem. this might have been the reason for the discrepancy between their results. on the whole  from the body of literature  it was found that under-sampling of majority classes was better than over-sampling with replication of minority classes  1  1  and that the combination of the two did not significantly improve the performance over under sampling alone.
　chawla et al.  introduced a new over-sampling approach for two class problems that over-sampled the minority class by creating synthetic examples rather than replicating examples. they pointed out the limitation of oversampling with replication in terms of the decision regions in feature space for decision trees. they showed that as the minority class was over sampled by increasing amounts  for decision trees  the result was to identify similar but more specific regions in the feature space. a preferable approach is to build generalized regions around minority class examples.
　the synthetic minority over-sampling technique  smote  was introduced to provide synthetic minority class examples which were not identical but came from the same region in feature space. the over-sampling was done by selecting each minority class example and creating a synthetic example along the line segment joining the selected example and any/all of the k minority class nearest neighbors. in the calculations of the nearest neighbors for the minority class examples a euclidean distance for continuous features and the value distance metric  with the euclidean assumption  for nominal features was used. for examples with continuous features  the synthetic examples are generated by taking the difference between the feature vectors of selected examples under consideration and their nearest neighbors. the difference between the feature vectors is multiplied by a random number between 1 and 1 and then added to the feature vector of the example under consideration to get a new synthetic example. for nominal valued features  a majority vote for the feature value is taken between the example under consideration and its k nearest neighbors. this approach effectively selects a random point along the line segment between the two feature vectors. this strategy forces the decision regions of the minority class learned by the classifier to become more general and effectively provides better generalization performance on unseen data.
　however  an investigation into how to choose the number of examples to be added was not done. in addition  the amount of under-sampling also needs to be determined. given the various costs of making errors  it is important to identify potentially optimal values for both smote and under-sampling. this is equivalent to discovering the operating point in the roc space giving the best trade-off between true positives and false positives. in this paper  we develop an approach to automatically set the parameters. we discuss a wrapper framework using cross-validation that performs a step-wise and greedy search for the parameters. note that while the computational aspects of the automated approach induces certain costs  we do not incorporate that into our framework. we optimize based on the different types of errors made. however  we do try to restrict our search space. we show that this approach works on three highly skewed datasets. we also utilized a cost-matrix to indicate the costs per test example based on the different kinds of errors.
1. wrapper
　in this work  a wrapper  approach was utilized to determine the percentage of minority class examples to add to the training set and the percentage to under-sample the majority class examples. the wrapper approach works by doing a guided search of the parameter space. in this case the underlying classifier is used to evaluate the chosen performance function for every considered amount of under-sampling and smote. a particular parameter or set of parameters is chosen and a five-fold cross validation on the train data is done to get the performance average. the parameters are varied in a systematic way such that a set of parameter candidates are generated  training sets are updated  and the classifiers built and evaluated. the candidate associated with the highest performance is chosen to have its parameters systematically modified to create new candidate solutions. this process is a type of best-first search. in order to evaluate the effectiveness of the wrapper approach in selecting the parameters for under-sampling and smote  we need to use a metric other than strict accuracy. with imbalanced data  accuracy can be misleading  because it causes you to favor high prediction accuracy on the majority class which is often uninteresting. hence  the f-value metric was used as the evaluation function . it is made up of two measures: precision which gives us the measure of correctness of the classifier in predicting the actual positive or minority class  whereas recall gives us the measure of the percentage of positive or minority class examples predicted correctly. the precision  recall and f-value were calculated as follows  where β corresponds to the relative importance of precision vs recall.
tp
	precision = 	 1 
tp + fp
tp
	recall = 	 1 
tp + fn
		 1 
　we implemented our wrapper approach as follows. we first do a ten-fold stratified split to separate the original dataset into ten training sets and ten disjoint testing sets. then  for each of the ten training folds  we implement the wrapper approach using five-fold cross-validation to get more robust amount estimates for under-sampling and smote. note that these performance estimates will hold true only when either the training data is a good representative of the actual data distribution or the wrapper strategy does not over-fit the training data. if the training data is not a good representative of the actual data  no strategy can help. so the only thing which remains is to see whether the wrapper approach finds under-sampling and smote levels which when used to build a classifier  do not over-fit the training data. once the wrapper selects the particular amount of smote and under-sampling  we apply those amounts five different random times on the training set  since both smote and under-sampling randomly remove or create new instances. the classifiers learned from the updated training sets are evaluated on the same testing set  and those performances are averaged. this is done for each of the 1 folds. thus  the final ten-fold average reported is essentially over fifty classifiers.
　the two search parameters for the wrapper are the undersampling and smote percentages. the search space becomes large if the search is done simultaneously for both the under-sampling percentage and the smote percentage  creation of new synthetic examples . hence  we chose to first use wrappers to find the best under-sampling percentage. the wrapper starts with no under sampling for all majority classes and obtains baseline results on the training data. then in a step-by-step greedy fashion it traverses through the search space of under-sampling percentages to seek better performance over the minority classes. the search process continues as long as it does not reduce the f-value of the minority classes or reduce the f-value over the majority classes more than some specified amount  generally 1% . note that for under-sampling we look at both the minority and majority class f-values. we also looked at the f-value for the majority class as we only want to remove the redundant examples through undersampling  and not remove some of the important majority class examples. by looking at both the values simultaneously we are maintaining the decision regions for all the classes. also  we wanted to identify the amount of under-sampling before introducing any synthetic minority class examples as that could have inadvertently penalized the f-value for the majority class. we want to first remove the majority class examples  that add no learning value to the base classifier.
　then with the under-sampling percentage fixed  we used the wrapper approach  to find the smote percentage. oversampling by creating synthetic examples is done until no minority class f-value increase is obtained for 1 candidate expansions. now  for smote we are only interested in improving the performance of the minority class. the f-value takes into account the increase in false positives  lowered precision   if any  by smote increments. thus  an overwhelming increase in the precision will stop the smote process. this provides significantly improved computation times at the cost of a potential loss in accuracy. once the best percentages for under-sampling and over-sampling via smote are found  the training folds are updated with the requisite smote and undersampling amounts. a classifier is then learned and evaluated on the unseen test data. we would like to be able to put this in a cost-framework if the time spent in searching for the  optimal  and  best  under-sampling and smote percentages  justifies the performance improvement. we are investigating that line of work  as future work.
1. experiments
we report results on three data sets:
  mammography dataset 
  forest cover dataset  and
dataset# of examples# classes# of majority
class examples# of minority class examples#	of tributesat-# of continuous attributesmammography11  1% 11forest cover11  1% 11modified
kdd	cup
1	 intrusion data 1normal: 1; dos: 1;
probe: 1u1r:	1
 1% ;
r1l:	1
 1% 11table 1: summary of datasets. the percentages indicate the proportion of minority class in the complete dataset.
  kdd-cup 1: network intrusion detection dataset
 two versions .
a brief summary of the datasets is presented in table 1 and further details are given in later subsections. the forest cover dataset is available from the uci repository  and our modifications to it will be described in the proceeding. the network intrusion data set comes from the kdd cup competition in 1  and the mammography data set is one that we locally extracted . it is clear from table 1 that there is significant imbalance between the two classes of each of these data sets. hence  there is an opportunity to improve the minority class recognition accuracy because a typical classifier will be highly accurate but focused on the majority class. we report the f-value for all our experiments. the f-value assumed a β of 1. we introduced a cost-matrix for the mammography dataset  as there can be a large cost associated with misclassification of a potentially malignant calcification  cancer  as non-calcification  non-cancerous . moreover  there is also a slight cost associated with misclassifying the non-calcifications as calcifications. while there wasn't a natural application of costs to the forest cover dataset  we still constructed a cost-matrix for the sake of analysis. the kdd-cup dataset comes with a cost-matrix for each of the relative type of errors.
　however  we did not incorporate the cost-matrix during the validation stage to select the amount of smote and under-sampling. we are going to investigate that as a future line of work. it requires a definite cost matrix to be known for a dataset. it will be interesting to compare the smote and undersampling parameters discovered using cost matrices during validation with the smote and undersampling parameters discovered without using the cost-matrices  assuming the same loss .
1 classifiers
　experiments were done with two types of classifiers  decision trees using software  usfc1  which emulates c1 release 1  and a rule learning technique called ripper . usfc1 was used with the default settings. by default  ripper will build rules first for the smallest class and will not build rules for the largest class. in the case of two class problems with imbalanced classes  such as here  only rules for the minority class are going to be built. hence  one might expect that ripper will be better than a decision tree in accuracy on the minority class.
　the wrapper algorithm that uses five fold cross-validation on the training set finds the undersampling and smote percentages for a particular training fold  one of the ten folds for cross-validating the system . then under-sampling and smote are applied to each fold with wrapper selected percentages  a classifier was built on the updated training data and evaluated on the test data  unseen during the wrapper process. due to the inherent random nature of under-sampling and smote  the process of training and testing with wrapper selected under-sampling and smote percentages is done five times to get an averaged  more stable  performance measure. to summarize  on each of the 1 folds  training and testing for wrapper selected smote and under-sampling percentages was done five times i.e. smote and under-sampling was done for a total of 1 times for cross-validation to get average stable results. all results reported in the proceeding are averages obtained in this way. in the tables  t-stat indicates the results of a significance test at the 1% level. this was a paired t-test. the x% of under-sampling means that x% of majority class examples were retained; and the y% of smote means that many more examples of the minority class were created. for example  1% of smote means that twice as many  than the original number  minority class examples were created.
1. results
　we did a ten-fold cross-validation  for mammography and forest cover datasets  in which the original dataset is stratified into ten disjoint sets or folds from which ten distinct testing sets and ten training sets are created. for the intrusion dataset  we utilized the training and testing sets as provided. we also used the cost-matrix as provided for the intrusion dataset and report the average cost per test example to compare with other published results . for the mammography and forest cover datasets  we report various perfomance metrics  including tprate  fprate  f-values  average cost per test example at different cost ratios  and cost curves. our main goal is to compare the classifiers in terms of reduction in the expected cost across different cost ratios. drummond and holte  introduced the cost space representation that allows for comparing different classifiers in terms of the expected cost. let p +  be the prior probability of the positive class  and p -  be the prior probability of the negative class. c  |+  is cost of misclassifying a positive example as a negative example  false negative ; and c +|   is cost of misclassifying a negative example as a positive example  false positive . the normalized expected cost  ne c   can then be expressed in terms of tprate  fprate  and probability cost function  pcf  as follows:
		 1 
ripperc1ripper as base classifierc1 as base classifier
	1	1	1	1	1	1
	cost ratio	cost ratio
figure 1: average cost per test example at different cost ratios for the mammography dataset.	ne c  =  1   tprate   fprate  〜 pcf +  + fp	 1 
　the performance of classifier using a fixed threshold  as used in this paper  is represented by a pair of  tp  fp . it can thus be represented as a line in the cost space  comprising of the normalized expected cost  ne c   in the y-axis and pcf +  in the x-axis. the range of both the measures is between 1 and 1. given a family of such classifiers  if a classifier is lower in the normalized expected cost across a range of pcf  it dominates the other. one can  thus  choose a classifier that has a minimum cost either over a range of pcf +  or at a particular operating range.
1 mammography data
　the mammography dataset was used in  and consists of 1 total samples with six numeric features and two classes representing calcification  cancerous  and noncalcification  non-cancerous . the minority class which represents calcification contained only 1 examples in the dataset i.e. only 1% of the total examples. the results obtained are shown in table 1. the negative sign before the number in the '% increase' row indicates reduction in the associated value. it can be seen from table 1  that for all four experimental trials  the wrapper algorithm was able to statistically significantly improve tp-rates for the minority class at the expense of a statistically significant reduction in f-values for the majority class. however  the wrapper method also produces significantly higher fp-rates than the baseline methods. but  the correspnding decrease in the f-value was not significant. hence  the goal of a higher true positive rate is attainable without a significant reduction in the overall f-value.
　we then constructed a cost-matrix for the mammography dataset by considering the the following costs of making errors between the positive and negative examples  using the convention  c +|   c  |+  :  1  1 ;  1  1 ;  1  1 ;  1  1 ; and  1  1 . figure 1 shows the results using these varied cost ratios with the different methods presented in table 1. as one would expect  if using the same costs of errors  the baseline method produces the lowest cost per test example  and is indeed the preferred method. however  varying the costs from twice as much for false negative to 1 times  we see that the smote classifier achieves the least cost. under-sampling in conjunction with smote provides very little reduction in the cost  if any. both the c1 and ripper classifiers exhibit similar behavior with smote - significant improvement in performance over baseline. ripper is well-suited for the task  as it is able to produce lower cost estimates per test example. we believe that incorporating a f-value in the wrapper framework maintains the relative importance of false positives and false negatives  as the number of true positives increases. however  one might vary the relative importance of precision and recall in the equation based on the specified costs. we assumed uniform costs in the f-value.
　we also implemented cost-curves over the range of pcf +  established by varying c +|   and c  |+  . figure 1 shows the result. again over the wide range of pcf +   the ripper-smote classifier achieves the lowest expected costs. until a pcf +  of 1 all the classifiers achieve similar performances  but beyond that the ripper-smote classifier dominates over the others.
1 forest cover data
　originally  the forest cover dataset  consisted of 1 examples with 1 numeric features related to cartographic variables and seven classes representing the type of the forest cover. for our study  the data samples from two classes were extracted while the rest were ignored as done in . the two classes we considered are ponderosa pine with 1 samples and cottonwood/willow with 1 samples. the results obtained on this dataset are tabulated below in table 1.
　for the forest cover dataset  the results for the minority class were as expected  with the wrapper tp rate increasing with statistical significance. but the interesting thing about these results was that  the wrapper f-values obtained on the majority class using ripper in both scenarios actually increased slightly instead of decreasing which was the general trend. for the 'smote only' scenario using ripper  the wrapper f-values were better than baseline f-values with statistical significance. for c1  the drop in the wrapper f-values over the majority class though statistically significant was extremely small. these were almost perfect results which one might always hope for  where the minority ex-

table 1: results for the mammography data.   indicates wrapper is statistically significantly greater than baseline;   indicates wrapper is statistically significantly lower than baseline; and 《 indicates there is no statistically significant difference between the wrapper and baseline methods.
c1rippersmote onlyundersampling
and smotesmote onlyundersampling
and smoteaverage
smote %1%1%1%1%average
under- sampling %1%1%1%1%average nority tp-rate　miclassbaseline1111wrapper1111% increase1%1%1%1%t-stat-1-1-1-1significance    average nority fp-rate　miclassbaseline1111wrapper1111% increase1%1%1%1%t-stat-1-1-1-1significance    average nority f-value　miclassbaseline1111wrapper1111% increase1%-1%-1%-1%t-stat-1111significance《《《《average majority class fvaluebaseline1111wrapper1111% decrease1%1%1%1%t-stat1111significance    
figure 1: cost curve for mammography dataset.
table 1: results for the forest cover data.   indicates wrapper is statistically significantly greater than baseline;   indicates wrapper is statistically significantly lower than baseline; and 《 indicates there is no statistically significant difference between the wrapper and baseline methods.
c1rippersmote onlyundersampling
and smotesmote onlyundersampling
and smoteaverage
smote %1%1%1%1%average
under- sampling %1%1%1%1%average nority tp-rate　miclassbaseline1111wrapper1111% increase1%1%1%1%t-stat-1-1-1-1significance    average nority fp-rate　miclassbaseline1111wrapper1111% increase1%1%1%1%t-stat-1-1-1-1significance    average nority f-value　miclassbaseline1111wrapper1111% increase-1%-1%1%1%t-stat11-1-1significance《《  average majority class fvaluebaseline1111wrapper1111% decrease1%1%-1%-1%t-stat11-1-1significance   《
	ripper	c1

figure 1: average cost per test example at different cost ratios for the forest cover dataset.

amples which were previously misclassified were correctly classified without increasing the number of majority class examples being classified as belonging to the minority class. the reason for these good results might be due to the similar distribution of the minority class examples in training and test data when cross-validation is performed. for example  in the forest cover dataset which contains 1 total minority class examples  the training data will contain approximately 1 examples while test data will contain 1 examples. since there were a fair number of examples in the minority class smote may have been more effective. it is unlike the mammography dataset where the number of minority class examples in the testing set is only 1.
　we also looked at the forest cover dataset under a cost framework. while  there weren't any obvious cost matrices that could be constructed  we simply utilized the following relative costs of  c +|   c  |+  :  1  1 ;  1  1 ;  1  1 ;  1 ; and  1 . as evident in figure 1  ripper and c1 provide different performances as the cost matrices change for this dataset. ripper is helped by undersampling  while c1 is not. this further justifies the use of wrapper techniques for different classifiers when considering sampling as a strategy for imbalanced datasets. moreover  ripper at  1  also benefited by smote. figure 1 shows the cost-curves across the range of pcf + . the wrapper based smote and smote-undersampling for c1 and ripper  respectively  produce the lowest expected for the broad range of pcf + . the choice of the classifier with the sampling methods doesn't seem to make a significant difference in the expected costs  while the individual classifiers are significantly apart in the cost space.
　an interesting addition to our work will be analysis of the behavior of smote and the rules thus constructed with both ripper and c1. we would also investigate combination of the outputs of both the classifiers if they are making different kinds of errors to reduce the overall costs.
1 kdd-1 cup intrusion dataset
　this data set we treat differently. we look at it in a way that allows for comparisons with previous published work. a particular interesting example for comparison is to look at the results from of the kdd-1 cup data. a cost matrix was used in the scoring of the competition as shown in table 1 . it was used to produce the results in table 1 below. there were many duplicate examples in the original 1 million example training set. all duplicate examples were removed. we also under-sampled both the normal and neptune  dos  class by removing examples which occurred only once. for training data 1 as in the table 1  we under-sampled the normal class  and for the training data 1 we under-sampled both the normal and neptune classes. note that for both these set of experiments  the test set remained unchanged. our assumption was that some of them could be mislabeled or they were not very representative. these changes resulted in the training data set used here. only smote was applied to the modified data with the percentages for each minority class shown in table 1.
　it can be seen that our approach with ripper as the classifier produced the lowest cost per example after undersampling both the normal and neptune classes  training data 1   and applying 1% smote to the u1r class  while keeping the r1l class unchanged. this was better than the winner of the contest and better than the succeeding results from the literature. even c1 as the base classifier with smote  1% u1r and 1% for r1l  performed better than the other published techniques.
1. conclusions
　in this work  a wrapper  approach was utilized to determine the percentage of minority examples to add to the training set and the percentage to under-sample the majority class examples. the wrapper approach works by doing a guided search of the parameter space. the evaluation function was applied with a five fold cross validation done on the training set. once the best percentages for under sampling and smote are found it can be used to build a classifier on the updated training set and applied on the unseen testing set.

figure 1: cost curve for forest cover dataset.
　the f-value metric was used as the evaluation function. by using such a composite measure  we are able to control the relative increases in precision and recall  as both are essentially dependent on the different types of errors - false positives and false negatives. to statistically validate the results  we applied a 1-fold cross-validation framework to all but one of the datasets. within each 1-folds  the wrapper utilized 1-fold cross-validation to identify the potentially optimal amounts of under-sampling and smote.
table 1: cost matrix used for scoring entries in kdd cup 1 competition.
actual/predicteddosu1rr1lprobenormaldos111u1r111r1l111probe111normal111table 1: comparison of results obtained on the original kdd cup 1 test data. the numbers beside u1r and r1l indicate the smote percentage utilized for the experiments.
dosu1rr1lprobenormalcost per test examplewinning	strategy
1%1%1%1%1%1decision tree 1%1%1%1%1%1nave bayes 1%1%1%1%1%1multi-classifier 1%1%1%1%-1using c1 on training data 1 u1r  1  - r1l  1 1%1%1%1%1%1using ripper on training data 1 u1r
 1  - r1l  1 1%1%1%1%1%1using c1 on training data 1 u1r  1  - r1l  1 1%1%1%1%1%1using ripper on training data 1 u1r
 1  - r1l  1 1%1%1%1%1%1　we show results from applying this approach to the mammography dataset  the forest cover dataset  and the kddcup 1: network intrusion detection dataset. two learning algorithms were used  ripper a rule learning algorithm and c1 a decision tree learning algorithm. for the experiments  it was shown that it was possible to significantly increase the accuracy on the minority class  and reduce the overall expected costs. our approach for imbalanced datasets significantly outperformed the baselines both in the true positive rate and the average cost per test example. note that the f-values did not differ significantly because of the reduction in precision at the expense of the increase in recall. however  the relative increase in false positives does not impact the costs computation  because it is more costly to err as a false negative than a false positive. we achieved the lowest cost per test example of any approach we know of for the intrusion detection data. we also introduced artificial costs for both the mammography and forest cover data. our approach again produces lowest cost per test example  when compared to the baseline approach. it is very compelling that for the forest cover dataset  our approach produces lower cost per test example even for  1  1 . hence  the wrapper approach for automatically selecting the amounts of smote with under-sampling is very promising. the proposed framework should be applicable to any sampling technique and evaluation measure.
　in this paper  we did not include the costs in the f-value by varying the β parameter to reflect the relative ratios. we believe that will be an interesting addition to our work. if the costs are known then they can expressed within validation framework for selecting the amounts of smote and undersampling quantities. we believe incorporating costs should again reduce the overall costs of the errors. the sampling quantities are also discovered using the same costs for both the classes as they will be used during evaluation. thus  a stronger utilitarian framework can be implemented.
1. acknowledgments
　larry hall was partially supported by the department of energy through the advanced strategic computing initiative  asci  visual interactive environment for weapons simulation  views  data discovery program contract number: deac1do1. nitesh chawla was partially supported by national science foundation grant cns-1  by the central intelligence agency  and department of justice grant 1-dd-bx-1. we would like to thank chris drummond for his helpful input on the cost curves. we would also like to thank the anonymous reviewers for their useful comments.
