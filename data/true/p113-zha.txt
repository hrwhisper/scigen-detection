a novel method for simultaneous keyphrase extraction and generic text summarization is proposed by modeling text documents as weighted undirected and weighted bipartite graphs. spectral graph clustering algorithms are used for partitioning sentences of the documents into topical groups with sentence link priors being exploited to enhance clustering quality. within each topical group  saliency scores for keyphrases and sentences are generated based on a mutual reinforcement principle. the keyphrases and sentences are then ranked according to their saliency scores and selected for inclusion in the top keyphrase list and summaries of the document. the idea of building a hierarchy of summaries for documents capturing different levels of granularity is also briefly discussed. our method is illustrated using several examples from news articles  news broadcast transcripts and web documents.
categories and subject descriptors
h.1  information systems : information search and retrieval-clustering; h.1.m  information systems : miscellaneous-text summarization; g.1  numerical analysis : numerical linear algebra-singular value decomposition; g.1  discrete mathematics : graph theory- graph algorithms
general terms
algorithms  experimentation  theory
keywords
text summarization  keyphrase extraction  mutual reinforcement principle  bipartite graph  graph partitioning  singular value decomposition
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  tampere  finland.
copyright 1 acm 1-1/1 ...$1.
1. introduction
¡¡text summarization is an increasingly pressing practical problem due to the explosion of the amount of textual information available. for example  web search engines have exploited the use of text summarization from the very beginning: starting with the extraction of certain number of bytes from the beginning of each document to the more sophisticated query-focused summaries typified by google's snippets  see also the recent work in  . query-focused summaries provide the users with the useful information for initial relevance judgement so that they can quickly zero in on documents deserving further inspection. in contrast  a generic summary in general distills the most important overall information from a document  or a set of documents   it can be especially useful when the documents are relatively long and contain a variety of topics. with many search engines starting to index documents in postscript and pdf formats  we will see increased availability of long and multi-part documents and the pressing needs for efficiently generating effective generic summaries for those documents. in addition  there is also a great amount of news articles and broadcast transcripts generated daily from various news agencies needing effective summarization.
¡¡automatic text summarization is an extremely active research field making connections with many other research areas such as information retrieval  natural language processing and machine learning . informally  the goal of text summarization is to take a textual document  extract content from it and present the most important content to the user in a condensed form and in a manner sensitive to the user's or application's needs . in this paper we concentrate on the shallow approach of text summarization using sentence and keyphrase extractions. abstract generation utilizing materials not present in the documents to be summarized is a more challenging problem and will not be addressed here  1  1  1 . two basic approaches to sentence extraction can be distinguished on whether they are supervised or unsupervised in nature. supervised approaches need human-generated summary extracts for feature extraction and parameter estimation as is typified by the methods in  1  1  where sentence classifiers are trained using human-generated sentencesummary pairs as training examples. possible drawbacks of the supervised approaches are domain-dependency and the problems caused by the potential inconsistency of humangenerated summaries. in this paper we adopt the unsupervised approach  we explicitly model both keyphrases and the sentences that contain them using weighted undirected and weighted bipartite graphs and generate sentence extracts on the fly without extensive training. our method can also be viewed as representing a more sophisticated and effective approach to exploiting the term co-occurrence relationship in textual documents.
¡¡many text summarization methods are surveyed in  1  1  1 . we mention here several recent approaches that are closest in spirit to our approach. in   documents are modeled using undirected graphs with the vertices representing paragraphs  and edge weights representing similarity between two paragraphs. salient paragraphs are those connected to many other paragraphs with similarity above certain thresholds. in   latent semantic indexing was used as the bases for sentence selection of a given textual document  exploiting the components of multiple singular vectors. however  the singular vectors other than the one corresponding to the largest singular value can have both positive and negative components  making ranking sentences by singular vector component values less meaningful. in   qr decomposition with column pivoting applied to termsentence matrices was used for sentence selection  providing another example of unsupervised summarization methods. to emphasize diversity of topic coverage in a generic summary  in  it was proposed to use a variation of the kmeans method to cluster the sentences of a document into different topical groups  and then apply a sentence weighting model within each topical group for sentence selection.
¡¡the basic idea of our summarization method is to first cluster sentences of a document  or a set of documents  into topical groups and then  within each topical group  to select the keyphrases and sentences by their saliency scores. our major contributions are 1  proposing the use of sentence link priors resulted from the linear ordering of the sentences in a document to enhance sentence clustering quality; and 1  developing the mutual reinforcement principle for simultaneous keyphrase and sentence saliency score computation. we also alluded to the possibility of building a summary hierarchy based on a hierarchical clustering of the sentences of a document. the rest of the paper is organized as follows: in section 1  we develop the mutual reinforcement principle and its connection to computing the largest singular value triplet of the weight matrix of the term-sentence bipartite graph. in section 1  we introduce the sentence link prior and show how we can incorporate it into the sum-of-squares sentence clustering objective function using spectral clustering techniques. we also discuss link strength selection using generalized cross-validation. in section 1  we describe some experimental results using documents from the newswires  broadcasting transcripts and the world wide web. we conclude the paper in section 1 with pointers to future research.
1. themutualreinforcementprinciple
¡¡for each document  we generate two sets of objects: one the set of terms t = {t1 ... tn} and the other the set of sentences s = {s1 ... sm} in the document. 1 we build a weighted bipartite graph from t and s in the following way: if the term ti appears in sentence sj  we then create

1
 the choice is flexible  for example  terms can be words or phrases and sentences can be replaced by paragraphs or other text units.
an edge between ti and sj. we can also specify nonnegative weights on the edges of the weighted bipartite graph with wij indicating the weight on the edge  ti sj . for example  we can simply choose wij to be the number of times ti appears in sj. more sophisticated weighting schemes will be discussed later. we denote the weighted bipartite graph by g t s w  where w =  wij  is the m-by-n weight matrix containing all the pairwise edge weights. for each term ti and each sentence sj we wish to compute their saliency scores u ti  and v sj   respectively. to this end  we state the following mutual reinforcement principle:1
a term should have a high saliency score if it appears in many sentences with high saliency scores while a sentence should have a high saliency score if it contains many terms with high saliency scores.
¡¡in essence the principle dictates that the saliency score of a term is determined by the saliency scores of the sentences it appears in  and the saliency score of a sentence is determined by the saliency scores of the terms it contains. mathematically  the above statement is rendered as
 
where the summations are over the neighbors of the vertices in question  and a ¡« b indicates there is an edge between vertices a and b  i.e.  when computing a term score  the summation is over all sentences that contain the term and when computing a sentence score  the summation is over all terms that appear in the sentence. the symbol ¡Ø stands for  proportional to . now we collect the saliency scores for terms and sentences into two vectors u and v  respectively  the above equation can then be written in the following matrix format

where w is the weight matrix of the bipartite graph of the document in question  wt stands for the matrix transpose of w  and 1/¦Ò is the proportionality constant. it is easy to see that u and v are the left and right singular vectors of w corresponding to the singular value ¦Ò. if we choose ¦Ò to be the largest singular value of w  then its is guaranteed that both u and v have nonnegative components. the corresponding component values of u and v give the term and sentence saliency scores  respectively. we can rank terms and sentences in decreasing order of their saliency scores  and select the top t terms  with the highest saliency scores  to add to the top term list and the top s sentences  with the highest saliency scores  to add to the summary. here t and s are some user-defined values  and s can be estimated from the compression rate of the desired summary.
¡¡remark. for numerical computation of the largest singular value triplet {u ¦Ò v}  we can use a variation of the power method adapted to the case of singular value triplets: choose an initial value for v to be the vector of all ones. alternate between the following two steps until convergence 
1. compute and normalize
 

1
 similar ideas have also been used to find the hub and authority web pages in a link graph .
1. compute and normalize
 
where the vector norm  ¡¤  can be chosen to the euclidean norm  and ¦Ò can be computed as ¦Ò = utwv upon convergence. for a detailed analysis of the singular value decomposition for related types of matrices  the reader is referred to .
¡¡remark. the above general weighted bipartite graph model can be extended further by adding vertex weights to the terms and/or sentences. both types of weights can incorporate certain kind of prior information  for example  the weight of a sentence vertex can be increased if it contains certain bonus words; we can also modify the weight of a sentence vertex based on its position in the document. in general  let dt and ds be two diagonal matrices the diagonal elements of which represent the weights of the term vertices and sentence vertices  respectively. then instead of finding the largest singular value triplet of the edge weight matrix w  we compute the largest singular value triplet {u ¦Ò v} of the scaled matrix dtwds. a specific sentence vertex weighting scheme will be discussed later.
1. clustering sentences into topical groups
¡¡the saliency score computation discussed in section 1 can be more effective if it is applied within each topical group of a document  or a set of documents . to this end we discuss effective algorithms for sentence clustering with the purpose to reveal the latent topical structure of textual documents. the idea of using sentences clustering has also been recently used in  1  1 . for sentence clustering we first build an undirected weighted graph with vertices representing the sentences of a document and two sentences si and sj are linked by an edge if there are terms shared by the two sentences  we also specify an edge weight wij for the edge  si sj   in general wij indicates the similarity between the two sentences si and sj  and there are many different ways for their specification. we denote the resulting graph as g s ws   where s is the set of sentences and ws =  wij  gives the edge weights. it is quite easy to verify that ws has the same sparsity pattern as wtw  where w is the weight matrix for the bipartite graph introduced in section 1.
1 incorporating sentence link priors
¡¡documents consist of sentences arranged in a linear order  and near-by sentences in terms of this intrinsic linear order tend to be about the same topic. the fact that topical groups within a document are usually made of sections of consecutive sentences is a strong prior that needs to be fully exploited during the sentence clustering process. the prior which we call sentence link prior will be especially helpful when dealing with broadcasting transcripts where there are no syntactic cues such as paragraph heading/ending to indicate topic boundaries. to ease discussion  we call si and sj are near-by to each other if si is followed by sj in the linear order of the document  or sj is followed by si  since the graph we consider is undirected anyway .
¡¡the weighted undirected graph framework we are using for sentence clustering naturally lends itself to incorporating the sentence link prior. in general  pairs of vertices  sentences  with large similarity weight tend to be clustered into the same group. a simple approach to taking advantage of this goes as follows: we strengthen the similarity weight between near-by sentences  i.e.  we modify the weights for all the near-by sentence pairs 
	wij + ¦Á	if si and sj are near-by
w
otherwise
we call ¦Á the sentence link strength  and the modified weight matrix is denoted by ws ¦Á . there are n  1 modifications in total for a document with n sentences. there is also the possibility of strengthen the similarity of pairs of sentences that are two  or more  edges away from each other. notice that incorporating sentence link prior is different from the requirement in text segmentation: we do allow several sections of consecutive sentences to form a single topical group. the parameter ¦Á can be considered as a regularization parameter  and we use the idea of generalized cross-validation  gcv  for choosing a good ¦Á . for a fixed ¦Á  we apply the spectral clustering technique discussed in the next section to w ¦Á  to obtain a set of sentence clusters ¦°  ¦Á . for any sentence clustering ¦° we define ¦Ã ¦°  to be the number of consecutive sentence segments it generates which is then used as a measure of model complexity for the clustering ¦°. the idea is to simultaneously minimize the clustering cost function and the model complexity. to this end  we compute a function of ¦Á defined as
¡¡¡¡¡¡gcv ¦Á  =  n   k   j w ¦°  ¦Á   /¦Ã ¦°  ¦Á     1  where k is the desirable number of sentence clusters  w is the weight matrix for the term-sentence bipartite graph and ¦°  ¦Á  is the set of clusters obtained by applying spectral clustering to the modified weight matrix w ¦Á . we then select the ¦Á that maximizes the above function as the estimated optimal ¦Á value.
1 building a summary hierarchy
¡¡summaries can actually capture the essential information of textual documents at different levels of granularity. this issue is closely related to the compression rate and coverage/diversity of summaries . however  the point of view of hierarchical sentence clustering seems to provide a rather natural and fruitful way of tackling the issue. we start with a sentence cluster hierarchy with the lower-level clusters  nodes  representing finer structures of the document. summarization is then done at each node of the hierarchy using all the sentences that belong to this node. two basic variations exist when carry out summarization at a non-root node of the hierarchy.
  the terms used in summarization remain the same for all the nodes;
  the terms used in summarization at a particular node do not include the top terms  i.e.  those terms having high saliency scores  from the summarization of its parent nodes.
the idea for the second approach is that we only retain terms that can distinguish finer structures of the document. another possibility is to use the first approach in the above for all the leave nodes of the hierarchy  and at the next higher level carry out a summary of summaries until the root-node is reached. currently  we have only limited experiences with the ideas proposed above  and we will not elaborate on the issues further.
1 sum-of-squares cost function for sentence clustering and spectral relaxation
¡¡we start with the weight matrix w for the bipartite graph g t s w . here each sentence is represented by a column of w =  w1 ... wn  which we will call the sentence vector. a partition ¦° of the sentence vectors can be written in the following form
	 	 1 
where e is a permutation matrix  and wi is m-by-ni  i.e.  the ith sentence cluster contains the sentence vectors in wi. for a given partition ¦°  the associated sum-of-squares cost function is defined as 

i.e.  mi is the centroid of the sentence vectors in cluster i  and ni is the number of sentences in cluster i.
¡¡the traditional k-means algorithm is iterative in nature and in each iteration  the following is performed :
1  for each sentence vector w  find the center mi that is closest to w  and associate w with this center.
1  compute a new set of centers by taking  for each center  the center of mass of sentence vectors associated with this center.
it can be proved that the above algorithm is equivalent to finding a local minimum of j w ¦°  with respect to ¦° using coordinate descent. despite the popularity of k-means clustering algorithm  one of its major drawbacks is that the coordinate descent search method is prone to local minima giving rise to some clusters with very few data points. moreover  it is also not easy to incorporate the sentence link prior into the above sum-of-squares cost function in order to improve sentence clustering quality. much research has been done on computing refined initial centroids and adding explicit constraints to the sum-of-squares cost function for kmeans clustering so that the search can converge to a better local minimum . it was shown  however  that an equivalent formulation of the sum-of-squares minimization can be derived as a matrix trace maximization problem with special constraints; relaxing the constraints leads to a trace maximization problem that possesses optimal global solutions . this formulation also makes k-means method easily adaptable to utilizing the sentence link priors discused in the above subsection.
¡¡here we give a brief presentation of the spectral relaxation approach for k-means clustering. let e be a vector of appropriate dimension with all elements equal to one  it is easy to see that the centroids can be written as
mi = wie/ni.
now let
x = diag .
it was shown in  that the sum-of-squares cost function can be written as
j w ¦°  = trace wtw    trace xtwtwx  
and its minimization is equivalent to
max{ trace xtwtwx  | x = diag .
ignoring the special structure of x and let it be an arbitrary orthonormal matrix  we obtain a relaxed matrix trace maximization problem
	trace xtwtwx .	 1 
an extension of the rayleigh-ritz characterization of eigenvalues of symmetric matrices shows that the above maximum is achieved by the first k largest eigenvectors of the gram matrix wtw  1  section 1.1 . as a by-product  we also have the following inequality
trace 	trace xtwtwx 
k
 
where ¦Òi w  is the i largest singular value of w. this gives a lower bound for the minimum of the sum-of-squares cost function. the spectral relaxation formulation of the k-means algorithm also makes it easy to consider more general kernel functions: instead of using witwj we can compute the gram matrix using any mercer kernel  to obtain  k wi wj  ni j=1. in particular  we can replace wtw by ws ¦Á  after incorporating the link strength ¦Á. the assignment of the cluster labels to each sentence is done by using qr decomposition with pivoting  see below .
¡¡the sentence clustering algorithm based on the modified weight matrix ws ¦Á  is now summarized as follows:
  compute the k eigenvectors vk =  v1 ... vk  of ws ¦Á  corresponding to the largest k eigenvalues.
  compute the pivoted qr decomposition of vkt as
 vk tp = qr = q r1 r1  
where q is a k-by-k orthogonal matrix  r1 is a kby-k upper triangular matrix  and p is a permutation matrix.
  compute
.
then the cluster membership of each sentence is determined by the row index of the largest element in absolute value of the corresponding column of r . this gives rise to the clustering ¦°  ¦Á  used in  1 .
1. experimental results
¡¡evaluation for generic text summarization is a very challenging task: although it can be done against a set of documents with manual summarizations  human-generated sentence extracts do  however  tend to differ significantly especially for longer documents  1  1 . another approach is to evaluate extrinsically the summarization algorithms based on  for example  their performance on document retrieval or text categorization. here we will provide some preliminary quantitative and qualitative assessment of our summarization algorithm. we collected a set of ten documents consisting of news articles  news broadcast transcripts and web pages.
  news articles
documentoptimal ¦Áaccuracyestimated ¦Áaccuracysentence #cluster #sf11%11%1pge1.1%11 %1flg11%11 %1heart11%11%1enron11%11%1cnn11%11%1cnn1.1.1%11%1cnn1.1.1%11 %1cnn1.1.1%11%1dna11%11%1table 1: clustering accuracy with optimal and estimated ¦Ásf: story.news.yahoo.com/news tmpl=story&u= /nyt/1/ts nyt/conduct of war is  redefined by success of special forces
pge: www1.mercurycenter.com/local/center /dereg1.htm
flg: www1.mercurycenter.com/local/center
/fal1.htm
enron: story.news.yahoo.com/news tmpl= story&cid=1&u=/nyt/1/ts nyt/ multiple safeguards failed to detect
 problems at enron
  cnn broadcast transcripts
cnn : moneyline cnn1: newsroom cnn1: wolf blitzer repots cnn1: science and technology week
  web pages
dna: www.pbs.org/wgbh/nova/neanderthals/mtdna.html
heart: www.pbs.org/wgbh/nova/heart/treating.html
¡¡we manually divide each document into topical groups: for web pages and news articles we rely on the section structure of the documents  and for news broadcast transcripts we rely on the contents of the documents. we notice that the clustering is usually not unique  some clusters can be merged into a bigger cluster and some clusters can be split into several smaller ones to capture the finer structure of the documents. the number of sentences and the number of clusters for each document are listed in the table 1.  other items of the table will be discussed later. 
¡¡we first illustrate quantitatively the difference in sentence clustering resulting from using the sentence link prior. in processing the documents  we deleted words appearing in a stop word list and applied porter's stemming . we used the following sentence similarity measure to construct the weight matrix ws: each sentence is represented by a sentence vector using the weight matrix w of the term-sentence bipartite graph introduced in section 1  i.e.  columns of w correspond to the sentences of the document in question.
the weight matrix ws =  wij  for the sentence graph is computed with wij equal to the dot-product between the sentence vectors of sentences si and sj. the sentence vectors are weighted with tf.idf weighting and normalized to have euclidean length one.
¡¡to measure the quality of sentence clustering  we use a variation of the confusion matrix which is frequently used for measuring classifier accuracy. we assume the manually generated section number gives the true cluster label of each sentence. we then compute the accuracy of our clustering algorithm against the section labels. in particular  for a k cluster case  we compute a k-by-k confusion matrix c =  cij  with cij the number of sentences in cluster i that belong to section j. it is actually quite subtle to compute the accuracy using the confusion matrix because we do not know which cluster matches which section. an optimal way is to solve the following maximization problem max{ trace cp  | p is a permutation matrix} 
and divide the maximum by the total number of sentences to obtain the clustering accuracy. this is equivalent to finding a perfect matching of a complete weighted bipartite graph which can be solved using kuhn-munkres algorithm . in our computation  we used a greedy algorithm to compute a sub-optimal solution.
figure 1:  left  sentence clustering accuracy versus sentence link strength ¦Á. sentence cluster distribution
with link prior  middle  and without link prior  right 	dna	flgcnn	1	1	1	1	
	alpha value	alpha value	alpha value
figure 1: clustering accuracy vs. ¦Á  circles ; gcv ¦Á  vs. ¦Á  diamonds 
can be seen from figure 1.	there are 1 sentences in total  and we number the sentences
¡¡for the computation of keyphrase and sentence saliency consecutively starting from 1. the numbers in parentheses scores  we considered sentence vertex weights when applying in the above indicate sentence sequence range in each secthe mutual reinforcement principle. specifically  for the i-th tion.
sentence we apply the weight	in table 1  for ¦Á = 1  we look at the clusters generated
by listing the top five terms and the top one sentence in
log  number of terms in sentence + 1 
	1	.	each topical groups. we also list the sentence numbers in¡¡as an illustration  for a sequence of ¦Á values  we applied the spectral clustering algorithm to the weight matrix ws ¦Á  of the document dna  and in figure 1 we plot the clustering accuracy against ¦Á. in the same figure  we also contrast the results of sentence clustering with and without link priors  the one without link priors tend to be more fragmented. based on the results for the ten documents  a general conclusion seems to be that the clustering algorithm matches the section structure of the document poorly when there is no near-by sentence constraints  i.e.  ¦Á = 1 . with too large an ¦Á value  sentence similarities are overwhelmed by link strength  the results are also poor. our generalized cross-validation method seems to be quite effective at selecting a good ¦Á that produces clustering quality close to that given by the optimal ¦Á. in table 1  we list the optimal ¦Á and the corresponding sentence clustering accuracy for each document. we also list the estimated ¦Á using gcv ¦Á  discussed in section 1 and the corresponding sentence clustering accuracy. in general  clustering accuracy is a relatively flat function of ¦Á  and the estimated ¦Á even though may differ considerably from the optimal ¦Á still produces clustering accuracy that matchs well the best clustering accuracy  as log1 i + total number of sentences 
the idea is to mitigate the influence of long sentences by scaling each sentence by a factor proportional to the length of the sentence in terms of number of words; at the same time sentences close to the beginning of the document get a small boost based on their positions in the document.
¡¡for a detailed illustration we use document dna which consists of a story lead plus seven sections with sections headings given below
1. tracing ancestry with mtdna  title   1 
1. nuclear dna vs mitochondrial dna  1 
1. inheriting mtdna  1 
1. defining mitochondrial ancestors  1 
1. finding mitochondrial ancestors  1 
1. dating mitochondrial ancestors  1 
1. neanderthals and mtdna  1 
1. final note  1 
the each topical group in parentheses.
¡¡the clustering result matches the section structure quite well except for cluster 1 which actually consists of two sentence sequences. taking a closer look at the sentences in cluster 1  we found that sentences 1 to 1 discuss issues related the common ancestor  eve   and section 1 with section heading defining mitochondrial ancestors is about the same topic. so here sentence similarities win over sentence link strength. we have also applied the mutual reinforcement principle to all the sentences in the document dna  and the first few keywords and sentences extracted are related to the main topic of document. similar analysis has also been applied to the other nine documents with quite similar results  the details of which will not be reported here due to the lack of space  see http://www.cse.psu.edu/ zha/sum.html  .
1. conclusions
¡¡in this paper we presented a novel method for simultaneous extraction of keyphrases and sentences from textual documents. we explore the sentence link priors embedded in the linear ordering of a document to enhance the quality of clustering sentences of documents into topical groups. we also develop the mutual reinforcement principle to compute


cluster 1  1  mtdna matern recent recombin alter
in fact  recent studies show that paternal mtdna can on rare occasions enter an egg during fertilization and alter the maternal mtdna through recombination.cluster 1  1  group tree similar famili comput
later  with the help of a computer program  they put together a sort of family tree  grouping those with the most similar dna together  then grouping the groups  and then grouping the groups of groups.cluster 1  1  ancestor mutat mtdna rate live
for instance  if they took the mutation rate to be one in every 1 years and knew that there was a difference of 1 mutations between the mtdna of people living today and the mtdna of an ancestor who lived long ago  then they could infer that the ancestor lived 1 years ago.cluster 1  1  modern neanderth mtdna european anthropologist
this was an unwelcome finding for anthropologists who believe that there was some interbreeding between neanderthals and early modern humans living in europe  which might have helped to explain why modern europeans possess some neanderthal-like features ;cluster 1  1  mtdna live mutat inherit long
even though everyone on earth living today has inherited his or her mtdna from one person who lived long ago  our mtdna is not exactly alike.cluster 1  1  egg nuclear cell mtdna mothers
whenever an egg cell is fertilized  nuclear chromosomes from a sperm cell enter the egg and combine with the egg's nuclear dna  producing a mixture of both parents' genetic code.cluster 1  1  human mtdna live scientist includ
in recent years  scientists have used mtdna to trace the evolution and migration of human species including when the common ancestor to modern humans and neanderthals lived -- though there has been considerable debate over the validity and value of the findings.cluster 1  1  1  ancestor live common ev mtdna
it also does not mean that the mtdna originated with this  eve ; she and her contemporaries also had their own  most recent common ancestor though matrilineal descent   a woman who lived even further into the past who passed on her mtdna to everyone living during  eve's  time.table 1: clustering and sentence extract results for document dna. for each cluster we list five top words and one sentence.

keyphrase and sentence saliency scores within each topical groups. we have also illustrated our method using a variety of documents with different characteristics. many issues need further investigation: 1  more research needs to be done for the determination of optimal link strength ¦Á; 1  other possible ways for sentence clustering. one promising approach is to use a two-stage method: first segment the sentences and then cluster the segments into topical groups; 1  the issues of replacing the use of simple terms by noun phrases  this will impact how the weight matrix w and ws will be constructed  and in general the construction of sentence similarity using various resources beyond lexical match. we have experimented with using ramshaw and marcus noun phrase chuncker for extracting noun phrases ; 1  more extensive and systematic experimentation of the method for both single and multiple documents ; and 1  extension to translingual summarization .
¡¡acknowledgement. the research was supported in part by nsf grant ccr-1. the author also wants to thank the referees for many constructive comments and suggestions.
