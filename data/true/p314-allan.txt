previous research in novelty detection has focused on the task of finding novel material  given a set or stream of documents on a certain topic. this study investigates the more difficult two-part task defined by the trec 1 novelty track: given a topic and a group of documents relevant to that topic  1  find the relevant sentences from the documents  and 1  find the novel sentences from the collection of relevant sentences. our research shows that the former step appears to be the more difficult part of this task  and that the performance of novelty measures is very sensitive to the presence of non-relevant sentences.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-search process
general terms
experimentation
keywords
trec  novelty  redundancy  relevant sentences
1. introduction
﹛the goal of the trec 1 novelty track was to explore methods for reducing the amount of non-relevant and redundant material presented to the user of a document retrieval system. starting from a ranked list of documents  a system's task is to first filter out all non-relevant information from those documents  reducing it to the essential components of relevance-defined in the track to be sentences that were relevant. as a second task  the system is required to scan those relevant sentences and discard any that do not contain new material. in the end  the user should be presented with a ranked list of sentences that contain all of
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
the relevant information but that do not repeat information unnecessarily.
﹛systems participating in the track used roughly the same techniques. relevant sentences were isolated by comparing them to the query using a vector space or language modeling framework  or something very similar . novel sentences were then identified by comparing each sentence to all of those that occurred before it: if they were sufficiently different  they were considered novel. broadly speaking  there were two clear conclusions of the trec novelty track:  1  isolating relevant sentences is very difficult and  1  the value of finding novel sentences depends greatly on a system's ability to start with relevant sentences.
﹛this paper is largely motivated by one aspect of the trec novelty track  and of much other work on finding novel information  that has troubled us. in order to simplify the problem  researchers generally start with sets of documents that are already known to be relevant. that is  given knownrelevant documents  find the documents that are novel. the assumption is presumably that the process of finding relevant material can be explored separately. oddly  however  these efforts rarely examine what happens if that artificial assumption is lifted: what happens if the input to a system is not guaranteed to be relevant 
﹛the surprising result that we have found is that the simplifying assumption may make the novelty results almost meaningless for applications as long as relevance-finding is of low quality. specifically  if technique a is better than technique b when the input is guaranteed to be relevant  then b is sometimes superior to a when the guarantee is lifted.
﹛in this paper we explore the task of the trec novelty track in much greater depth than was done for the trec workshop  with substantial focus on the problem of how novelty detection degrades as the quality of relevant information drops. in section 1 we review the evaluation model of the trec novelty track and describe the training material that we used to augment the handful of training topics provided in the evaluation. in section 1 we describe the techniques and results obtained for sentence retrieval. in section 1 we discuss the measures that we explored for finding novel sentences. the focus of this paper is in sections 1 and 1 where we present detailed analysis of the results  including an examination of the impact of real retrieval results. we conclude in section 1 by summarizing our findings.
1. related work
﹛a significant body of related work arises from the topic detection and tracking  tdt  research and evaluation project  which is concerned with online event detection and tracking in news stories  1  1  1  1  1  1 . however  the task approached in this work differs from tdt research in several important aspects. most importantly  the tasks of tdt are concerned with what can be called inter-topic or inter-event novelty detection  where the concern is on whether two news stories cover the same event. in contrast  this work looks both at inter- and intra-topic novelty detection; in addition to determining whether two sentences cover the same topic  we are concerned with identifying when a sentence contains new information about that topic. another difference is that many of the techniques developed to tackle the various tasks in tdt rely heavily upon temporal clues and other structural components specific to news reporting  information that is not guaranteed to be present in the trec datasets used in this work. finally  tdt is concerned with story-level online evaluation  where news stories are presented in a particular order and each one must be evaluated before the next is seen. in contrast  the task discussed in this paper is based on a batch evaluation at the sentence level.
﹛the one task within tdt that most closely resembles this work is  new information detection  . in that task  a system is expected to monitor a stream of stories on a particular topic and extract sentences that discuss new developments within the topic. that idea was addressed more satisfactorily in research on temporal summarization  1  1   though the primary focus of their effort was to develop a useful evaluation model.
﹛very little research has focused on how to model intratopic novelty. one exception to this is work on novelty detection for adaptive filtering  which brought together several models that had been used for other related tasks  as well as introducing new models. all five of the novelty models presented in that paper are included here as well.
﹛maximal marginal relevance  mmr  is one of the better known concepts for integrating novelty and relevance . that work focused on tradeoffs between the two rather than finding the specific thresholds that are needed by the trec novelty track's task.
﹛this research builds upon previous work on the trec 1 novelty track  1  1  1  by presenting five additional novelty measures and analyzing the performance of all of the novelty measures on relevance results with varying levels of recall and precision.
1. trec novelty track
﹛the goal of trec's novelty track  is to explore methods that reduce the amount of redundant material shown to a searcher. the task is to adapt a ranked list of documents to extract relevant sentences and then to eliminate any of those sentences that do not contain new information.
﹛for its first year  the track's evaluation used 1 old trec topics  taken from topics 1 . for each topic  nist selected only relevant documents  up to a maximum of 1. the documents were ranked using the output of a good quality
manual run  system unspecified  from a past trec evaluation. this provided a ranked list of 1 documents that had been judged on-topic  relevant .
table 1: statistics about the training and test data used in these experiments. all numbers are averaged over the annotated topics in each group.
statnist trainumass trainnist testcount11docs111sents111rel111%rel111novel111%novel111﹛the documents were algorithmically broken into sentences and then each sentence was judged for relevance with respect to the topic. after that was done  annotators read through the relevant sentences in order and marked sentences that contained new information. once done  some set of sentences were marked relevant  and a subset of those sentences were marked novel.
﹛the task of a system was to replicate that sentence-level annotation by entirely automatic methods. that is  given a topic and the ordered set of relevant documents broken into sentences  a system was to find all relevant sentences and then mark some of those as novel.
﹛to help researchers train their systems  nist provided four training topics that were appropriately annotated. because this was a new task and four topics provided very minimal training data  we developed an additional 1 training topics . we used a method almost identical to that used by nist  except that we hired undergraduate student to do the assessment and the documents were ranked by a simple tf﹞idf system rather than a manual run. table 1 shows some statistics calculated from the training and test collections.
﹛evaluation in the novelty track was based on standard recall and precision measures  but applied at the sentence level  and to either relevant or to novel sentences. to minimize confusion   relevant  sometimes means  relevant  and sometimes  novel    we define the following special cases of recall and precision:
  rs-recall is the recall of relevant sentences in a set of sentences.
  rs-precision is the parallel measure of precision.
  ns-recall is the recall of novel sentences. note that all novel sentences are by construction relevant  but not all relevant sentences are novel.
  ns-precision is the parallel measure of precision.
the official evaluation measures of the novelty track focused on set retrieval  evaluating the quality of the relevant set returned or the quality of the novel set returned. this led the track to adopt recall℅precision as an official measure
.1
﹛in this study  however  we will focus on our ability to rank sentences by likelihood of relevance  or novelty . the trec

1
 the official evaluation measure at the time of the trec conference was recall℅precision. however  it was later changed to the f measure.
modelcutoffrs-recallrs-precisiontfidf1%1%1%1%1%1%qm1%1%1%kld1%1%1%table 1: rs-recall and rs-precision values for chosen cutoff values for the retrieved set of training sentences.
task requires identifying a cutoff threshold in that ranked list  but we generally ignore that issue here. the one exception to this is that in all experiments we had to choose a subset of the sentences  the presumed relevant set  to serve as input to our novelty algorithms. because analysis of the training data revealed that roughly 1% of all sentences were relevant  we decided to double this number  to improve rsrecall  and assumed that the top 1% of the ranked results list for each topic was relevant. as many of our results will show  the ability to rank sentences is so poor at the moment  that it is more important to improve that capability than to find a good threshold-i.e.  all thresholds are bad. because we focus on ranking  we use rs- and ns- versions of the recall/precision tradeoff graphs  and calculate average rs- and ns-precision.
1. finding relevant sentences
﹛for all retrieval experiments  the queries were the extended trec topic descriptions and the items being retrieved were the sentences in the provided documents. all queries and sentences were stopped and stemmed  using the krovetz stemmer  . we tried multiple retrieval models and techniques in an attempt to improve the generally poor performance  and experimented extensively with parameter tuning for each model. we experimented with three different well-known retrieval models: the vector space model with tf﹞idf weighting  tfidf   a language modeling approach with the kullback-leibler divergence  kld  as scoring function  and a two stage smoothing language modeling approach  qm  as described by zhai et al. . for the two language model approaches  dirichlet and jelinekmercer smoothing methods were applied.
﹛statistical analysis of the retrieval results for the models used shows that there is no significant difference in their performance  student's t-test p=1 . however  we decided to use the tfidf technique as it performed consistently  but not significantly  better than the others across different query sets. table 1 shows the performance for chosen cutoffs and different retrieval techniques on the training set of 1 topics. these results include the use of pseudo-relevance feedback adapted to each one of the models.
1 vector space model
﹛in the vector space model  both the query and the sentence are represented as weighted vectors and the sentence is scored based on its distance from the query vector. in our experiments  the sentence weighting function was a form of tf﹞idf  the query weighting function was a variant of raw term frequency  and the distance metric was the dot product. thus  the relevance of sentence s given query q is

where tft q and tft s are the number of times term t occurs in the query and sentence  respectively  sft is the number of sentences in which term t appears  and n is the number of sentences in the collection being scored.
1 trying to improve performance
﹛given that traditional document retrieval techniques have proved unsuccessful in the task of sentence retrieval  we believe that the only way to radically improve performance is through the use of techniques specifically customized to the task. in an attempt to boost performance we tried to use known techniques such as query expansion  manual query manipulation  mixing of multiple score functions  and pseudo-relevance feedback  as well as others. out of all of the methods tried  only pseudo-relevance feedback helped to improve performance significantly and consistently across retrieval models and data sets.
﹛also  extensive data analysis was executed with the objective of discovering features that would be deemed important in future sentence retrieval research. for example we tried to analyze the distribution of relevant sentences according to factors such as their position within a document in numeric or relative value or  the length of the sentence itself. we were unable to leverage that information successfully.
1. novelty/redundancy measures
﹛we present seven different novelty measures. two of them  newwords and trec kl  are the measures used by umass at trec 1  and the other five are from zhang et al. . in all experiments  the presumed or known relevant sentences are considered in the same order in which the relevant documents were originally ranked. multiple sentences from the same document are considered in the order in which they appear in the document. the measures described in this section are used to assign a novelty score n to each of the presumed or known relevant sentences  given the set of previously seen sentences. in keeping with the practices of zhang et al.  we treat novelty and redundancy as opposite ends of a continuous scale. therefore ranking the sentences by increasing redundancy score is equivalent to ranking them by decreasing novelty score.
1 word count measures
1.1 simple new word count  newwords 
﹛the simple new word count novelty measure assigns each sentence a score based on the number of words it contains that have not been seen in any previous sentence. it was one of the best performing novelty measures in the trec 1 novelty track.

wsi is the set of words contained in sentence si.
1.1 set difference  setdif 
﹛the set difference measure can be viewed as a more sophisticated version of the simple new word count that represents each sentence as a smoothed set of words. this allows for different words to carry different weights in determining the novelty score. however  set difference differs from the simple new word count in that the novelty score of sentence si is computed through a pairwise comparison between si and every previously seen relevant sentence. the previously seen sentence that is the most similar to si determines si's novelty score. in contrast  the simple new word count measure considered all of the previously seen sentences as one large set.

where wt ﹋ wsi iff count wt si    k and count wt si  = 汐1 tf sf rsfwt.
﹛﹛﹛﹛is the number of occurrences of word wt in sentence si  sfwt is the number of presumed non-relevant sentences in the documents considered so far that contain word wt  and rsfwt is the number of presumed relevant sentences seen so far that contain word wt. 汐1  汐1  汐1  and k are all parameters  set to different values for different collections based on the best results found in the training data.
1.1 cosine distance  cosdist 
﹛the cosine distance metric is very common in information retrieval and has been a popular similarity measure in tdt evaluations. in the cosine distance novelty metric  each sentence is represented as a vector in m-dimensional space  where m is the number of terms in the vocabulary   and the weights on individual dimensions are determined by some weighting function. the negative of the cosine of the angle between a sentence vector and each previously seen sentence vectors then determines the novelty score for that sentence.

where wk si  is the weight for word wk in sentence si. the weighting function used in our experiments is a tf﹞idf function specified by the following formula
.
asl is the average number of words in a relevant sentence for the topic  sfwk is the number of presumed relevant sentences for the topic that contain word wk  and n is the number of presumed relevant sentences for the topic.
﹛although cosine distance has performed very well as a novelty measure in past research that assigned novelty scores to full documents   its performance is known to degrade substantially on shorter pieces of text. therefore  it is not expected to perform as well at the sentence level.
1 language model measures
﹛all of the language-model-based novelty measures presented here use the kullback-leibler divergence between two language models  成1 and 成1  defined as
 
but they differ in which language models they compare.
1.1 interpolated aggregate smoothing  treckl 
﹛interpolated aggregate smoothing is the only language model-based novelty measure that does not perform pairwise comparisons with every previously seen sentence. instead  a sentence is assigned a novelty score equal to the kl divergence between its language model and a single language model built on all previously scored  and presumed relevant  sentences  which is why it is referred to as an aggregate measure. the language models for both the sentence being scored and all previously seen sentences are maximum likelihood models  smoothed using linear interpolation  also known as jelinek-mercer smoothing .

where
p w|成si =p w|成s1 ... si 1 =竹1p w|成mls1 ... si 1  + 1   竹1 p w|成mls1 ... si 
1.1 dirichlet smoothing  lmdiri 
﹛dirichlet smoothing of a maximum likelihood language model automatically adjusts the amount of reliance on the observed text according to the length of that text. in our experiments  this means that shorter sentences are smoothed more against the language model built on all presumed relevant sentences for that topic  whereas longer sentences are smoothed less.
	 	 1 
where both 成si and 成sj are given by
len si 
	p w|成si 	=	len si  + 米p w|成mlsi  +
米
len si  + 米p w|成mls1 ... sn  .
成mlsi is a maximum likelihood language model built on sentence si  成mls1 ... sn is a maximum likelihood model built on all presumed relevant sentences for the topic  and 米 is a parameter learned from training data.
1.1  shrinkage  smoothing  lmshrink 
﹛shrinkage smoothing models each sentence according to the assumption that it was generated by sampling from three different language models: a sentence model  a topic model  and a model for the english language. again  the novelty score for sentence si is given by equation 1  but now 成si and 成sj are determined by p w|成si  = 竹sp w|成mlsi  + 竹tp w|成mlt  + 竹ep w|成mle 
成mlt is a maximum likelihood language model of the topic and 成mle is a maximum likelihood language model of general english text. in our experiments  the topic model is built from the text of the extended trec topic description.
it could also be built on the text of the presumed relevant sentences. the general english model is built on all of the sentences in the collection.
1.1 sentence core mixture model  lmmix 
﹛one of the interesting properties of now popular language modeling smoothing techniques for text retrieval such as those described earlier is that they increase the probability of words that occur more in the background model s  than in the sentence and they decrease the probability of words that occur less in the background model s  than in the sentence. this means that some of what is different about that sentence is smoothed away  which could be an undesirable property when trying to model novelty. it may also give some indication of why such measures perform better on certain relevance results when the background model coefficient was set close to 1.
﹛the sentence core mixture model  introduced by zhang et al.  is based on an opposite assumption that words that occur more in a sentence than in the background text should have higher probability in the sentence model. the observed text is assumed to be sampled from a mixture of a  core  sentence model  a topic model  and a model of english text. however  the task of the algorithm here is to deduce the maximum likelihood sentence core model  which is then compared pairwise to each previously seen sentence core model. as with the last two measures  the novelty score is determined by equation 1 but 成si and 成sj are given as p w|成mlsi  = 竹sp w|成si  + 竹tp w|成mlt  + 竹ep w|成mle .
﹛the language model 成si that maximizes the likelihood of the observed sentence si  given fixed parameters  was computed using the technique described in zhang et al. .
1. novelty results
1 perfect relevance results
﹛prior studies have focused on how various novelty measures perform given a collection of relevant documents. these results show how the novelty measures described in the previous section perform on the set of sentences known to be relevant to each topic.
﹛table 1 shows the performance of each novelty measure on the known relevant sentences for the training set of 1 trec topics.1 table 1 shows the performance of each novelty measure on the known relevant sentences in the test set
of 1 trec topics.1
﹛sign tests at the 1% confidence level reveal that for both the training and testing set  there is no one novelty measure that consistently outperforms the others. however  in both cases  the set difference measure consistently performs worse than all other measures.
1 best relevance results
﹛table 1 shows the performance of each novelty measure on the top 1% of the sentences in each topic from our best

1
 all random average ns-precision values presented are an average over 1 runs.
1
 for 1 of the 1 topics in the test dataset  all of the relevant sentences were also judged novel which means that average ns-precision is 1 for all of these topics  no matter what the novelty measure - therefore these topics have no meaningful impact on the results for the known relevant testing set.

figure 1: rs-recall vs. rs-precision for synthetic results with the top 1% of the total number of sentences for each topic.
relevance results for the training set  1% rs-recall  1% rsprecision  and table 1 shows the performance of each novelty measure on the top 1% of our best relevance results for the testing set  1% rs-recall  1% rs-precision . what is most interesting is how the rankings in tables 1 and 1 are near flip-flops of the rankings in tables 1 and 1. this flip-flop effect is investigated further in section 1.
﹛we were curious about whether our choice of retrieval method for finding relevant sentences would affect the relative performance of the novelty measures  so we tried running our novelty measures on the top 1% of the ranked results list produced by using a two-stage language modeling method   rather than tf﹞idf  for retrieval. we found that the ranking of the novelty measures remained very similar for our training data.
1 synthetic relevance results
﹛because we were intrigued by the observation that the ranking of the various novelty measures changes a great deal between the perfect relevance results and our best relevance results  we decided to construct synthetic results in order to simulate how our novelty measures would perform at different rs-recall and rs-precision levels.
﹛we created synthetic relevance results based on the ranked list for our best relevance results. we held the number of results for each topic constant at 1% of the total number of sentences for each topic. we included the number of relevant sentences from the top of the ranked results list necessary to achieve the desired level of recall  and then filled in the remainder of the results with non-relevant sentences from the top of the ranked list. for example  suppose a topic had 1 relevant sentences out of 1 total. to build a synthetic  1% relevant  set  we select the 1 top-ranked relevant sentences. we then add the top-ranked 1 non-relevant sentences to yield a 1%  of 1  sample with 1%  of 1  recall. in this case the precision would be  = 1%.
﹛we present results from 1 different rs-recall levels: 1%  1%  1%  1%  and max.1 the max rs-recall level results represent the best performance we could have achieved using our methodology of taking the top 1% of sentences in

1
 for 1 of the topics in the training set  more than 1% of the sentences were relevant which means that 1% rs-recall was not possible if only 1% of the sentences were to be chosen.
ranknovelty measureaverage ns-precision1lmmix11trec kl11lmdiri11cosdist11lmshrink11newwords11setdif11random1table 1: performance of novelty measures on known relevant sentences in the training set.
ranknovelty measureaverage ns-precision1lmdiri11cosdist11trec kl11lmshrink11lmmix11newwords11setdif11random1ranknovelty measureaverage ns-precision1setdif11lmmix11newwords11trec kl11lmshrink11lmdiri11cosdist11random1table 1: performance of novelty measures on best relevance results in the training set.
ranknovelty measureaverage ns-precision1newwords11setdif11trec kl11lmdiri11lmmix11lmshrink11cosdist11random1﹛
table 1: performance of novelty measures on known relevant sentences in the testing set.
table 1: performance of novelty measures on best relevance results in the testing set.
﹛
the trec task. in figure 1 a rs-recall/rs-precision graph is shown for these synthetic results  indicating how rs-precision also changes as rs-recall increases. the larger circle on the top line and the larger box on the lower line indicate the points where our best relevance results fall.
﹛figures 1 and 1 show the final ranking for the chosen novelty measures across the two different topic sets  the training and testing set . note that the points near the bottom of the graph show better performing novelty measures. for the training set  setdif goes from rank 1  best  at 1% rs-recall to rank 1  worst  at 1% rs-recall. note that the rankings at 1% rs-recall are similar  but not identical  to the rankings seen on the best relevance results in table 1 where rs-recall was 1%. for the testing set  figure 1   there is a noticeable swap in rankings between setdif and lmshrink. here  note that the rankings at 1% rs-recall are similar to the rankings seen for the best relevance results in table 1 where rs-recall was 1%.
1 summary
﹛when the novelty detection component is handed sentences that are all relevant  the language modeling and cosine distance similarity functions work best  tables 1 and 1 . however  it is the set difference measures that excel when the density of relevant sentences drops because of errors in the relevance-finding step  tables 1 and 1 . the results of the experiments with synthetic data suggest that the ordering changes dramatically when recall is in the 1% range and precision is in the 1% range  see figure 1 . the cosine measure is the only measure that degrades smoothly as recall and precision decrease.
﹛the difference between the two groups of measures is that one just counts words and the other looks at the distribution of words. when non-relevant sentences are added  the probability distribution of vocabulary shifts so that arriving

figure 1: ranking of novelty measures for the training set at different levels of rs-recall and rs-precision.
sentences have more and more dissimilar distributions  suggesting that they are novel-that is  they look new because they are different from the non-relevant sentences.
﹛on the other hand  word counting approaches are less distracted by the new words. relevant sentences that are not novel will generally reuse vocabulary from earlier relevant sentences  and will not be sidetracked by the random vocabulary introduced by the non-relevant sentences. if the sentences are all relevant  the confusion caused by non-relevant vocabulary will disappear and all approaches should perform similarly. that is indeed what happens.
﹛the implication of these results is that we expect that as the density of relevant documents drops even further  we anticipate that the word counting measures will continue to perform the best.

figure 1: ranking of novelty measures for the testing set at different levels of rs-recall and rs-precision.

figure 1: interpolated rs-recall vs. rs-precision for the top 1 documents for each topic in the training set retrieved automatically.
1. real ir results
﹛to explore the hypothesis of the previous section  we examine what happens when we lift the assumption that the documents with which we started are relevant. this time  instead of taking the top ranked relevant documents returned by a retrieval engine-the procedure used to construct the training and test sets-we took the top 1 documents returned by a retrieval system. figure 1 shows the substantial drop in performance for finding relevant sentences when this change is made.
﹛because several very long documents ended up in the top 1 for the 1 topics  a total of 1 1 sentences were retrieved. knowing that only 1 of these were relevant  it seemed unreasonable to pass the top 1% of the relevance rankings on to our novelty detection system. instead we decided to take the same number of sentences from each topic that we used previously.
﹛table 1 shows the results from the novelty runs on the top-ranked sentences where rs-precision was 1% and the rs-recall was 1%. somewhat surprisingly  newwords and setdif  which were formerly the best performers on low rsprecision and low rs-recall sets of sentences are the worst performers here.
this result is totally counter to the intuition expressed
ranknovelty measureaverage ns-precision1trec kl11lmmix11cosdist11lmdiri11lmshrink11newwords11setdif1table 1: performance of novelty measures on tf﹞idf relevance results using an information retrieval system to find the original documents.
previously. we hypothesize that the problem occurs because the proportion of relevant sentences is now so low  1% rsrecall vs. 1% rs-recall for the best relevance run . most of those non-relevant sentences contain new words  so end up highly ranked  incorrectly  in terms of novelty. that is  the score is now dominated by non-novel sentences that are ranked high rather than by novel sentences being ranked low. we hope eventually to extend our synthetic results analysis to much smaller proportions of relevant sentences to understand the issue better.
1. conclusion
﹛we have presented the results of our attempts to identify relevant and novel sentences in a ranked list of relevant documents using many different methods. in our collections  finding relevant sentences has proved very difficult given the very low prior probability of relevance. this presents an interesting quandary in trying to find novel sentences because our preliminary finding from system results and synthetic results is that many novelty measures are very sensitive to the quality of the relevance results. this may be the case because certain novelty measures are more likely to flag nonrelevant sentences as novel.
﹛few efforts have been made to model novelty explicitly- most attempts to measure novelty tend to fall back on established document retrieval techniques. although these measures seem to work well at times  one of the more consistent novelty measures we saw here  the sentence core mixture model  also happens to be one of the measures that was not developed originally for document retrieval. however  clearly the largest hurdle remains the challenge of retrieving relevant sentences.
1. acknowledgements
﹛this work was supported in part by the center for intelligent information retrieval  the advanced research and development activity under contract number mda1-c1 and spawarsyscen-sd grant numbers n1-1 and n1-1. any opinions  findings and conclusions or recommendations expressed in this material are the authors and do not necessarily reflect those of the sponsor.
