defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set. recently  much work has been conducted with the goal of finding fast algorithms for this task. we show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used. we test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude. our average case analysis suggests that much of the efficiency is because the time to process non-outliers  which are the majority of examples  does not depend on the size of the data set.
categories and subject descriptors
h.1  database management : database applications- data mining
keywords
outliers  distance-based operations  anomaly detection  diskbased algorithms
1. introduction
detecting outliers  examples in a database with unusual properties  is an important data mining task. recently researchers have begun focusing on this problem and have attempted to apply algorithms for finding outliers to tasks such as fraud detection   identifying computer network intrusions  1  1   data cleaning   and detecting employers with poor injury histories.
outlier detection has a long history in statistics  1  1   but has largely focussed on data that is univariate  and data with a known  or parametric  distribution. these two limitations have restricted the ability to apply these types of methods to large real-world databases which typically have
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigkdd '1  august 1  1  washington  dc  usa.
copyright 1 acm 1-1/1 ...$1.
many different fields and have no easy way of characterizing the multivariate distribution of examples. other researchers  beginning with the work by knorr and ng   have taken a non-parametric approach and proposed using an example's distance to its nearest neighbors as a measure of unusualness  1  1  1  1 .
although distance is an effective non-parametric approach to detecting outliers  the drawback is the amount of computation time required. straightforward algorithms  such as those based on nested loops  typically require o n1  distance computations. this quadratic scaling means that it will be very difficult to mine outliers as we tackle increasingly larger data sets. this is a major problem for many real databases where there are often millions of records.
recently  researchers have presented many different algorithms for efficiently finding distance-based outliers. these approaches vary from spatial indexing trees to partitioning of the feature space with clustering algorithms . the common goal is developing algorithms that scale to large real data sets.
in this paper  we show that one can modify a simple algorithm based on nested loops  which would normally have quadratic scaling behavior  to yield near linear time mining on real  large  and high-dimensional data sets. specifically  our contributions are:
  we show that an algorithm based on nested loops in conjunction with randomization and a simple pruning rule has near linear time performance on many large real data sets. previous work reported quadratic performance for algorithms based on nested loops  1  1 
1 .
  we demonstrate that our algorithm scales to real data sets with millions of examples and many features  both continuous and discrete. to our knowledge we have run our algorithm on the largest reported data sets to date and obtained among the best scaling results for distance-based outliers on real data sets. other work has reported algorithms with linear time mining of distance-based outliers but only for low-dimensional problems  less than 1   1  1  or have only tested the scaling properties on simple synthetic domains.
  we analyze why our algorithm performs so well. the result of an average case analysis suggests that under certain conditions  the time to process non-outliers  which are the large majority of points  does not depend on the size of the data set.
the remainder of this paper is organized as follows. in the next section  we review the notion of distance-based outliers and present a simple nested loop algorithm that will be the focus of this paper. in section 1  we show that although our simple algorithm has poor worst case scaling properties  for many large  high-dimensional  real data sets the actual performance is extremely good and is close to linear. in section 1  we analyze our algorithm and attempt to explain the performance with an average case analysis. in section 1  we present examples of discovered outliers to give the readers a qualitative feel for how the algorithm works on real data. finally  we conclude this paper by discussing limitations and directions for future work.
1. distance-based outliers
a popular method of identifying outliers is by examining the distance to an example's nearest neighbors  1  1  1  1 . in this approach  one looks at the local neighborhood of points for an example typically defined by the k nearest examples  also known as neighbors . if the neighboring points are relatively close  then the example is considered normal; if the neighboring points are far away  then the example is considered unusual. the advantages of distance-based outliers are that no explicit distribution needs to be defined to determine unusualness  and that it can be applied to any feature space for which we can define a distance measure.
given a distance measure on a feature space  there are many different definitions of distance-based outliers. three popular definitions are
1. outliers are the examples for which there are fewer than p other examples within distance d  1  1 .
1. outliers are the top n examples whose distance to the kth nearest neighbor is greatest .
1. outliers are the top n examples whose average distance to the k nearest neighbors is greatest  1  1 .
there are several minor differences between these definitions. the first definition does not provide a ranking and requires specifying a distance parameter d. ramaswamy et al.  argue that this parameter could be difficult to determine and may involve trial and error to guess an appropriate value. the second definition only considers the distance to the kth neighbor and ignores information about closer points. finally  the last definition accounts for the distance to each neighbor but is slower to calculate than definition 1 or 1. however  all of these definitions are based on a nearest neighbor density estimate  to determine the points in low probability regions which are considered outliers.
researchers have tried a variety of approaches to find these outliers efficiently. the simplest are those using nested loops  1  1  1 . in the basic version one compares each example with every other example to determine its k nearest neighbors. given the neighbors for each example in the data set  simply select the top n candidates according to the outlier definition. this approach has quadratic complexity as we must make all pairwise distance computations between examples.
another method for finding outliers is to use a spatial indexing structure such as a kd-tree   r-tree   or x-tree  to find the nearest neighbors of each candidate point  1  1  1 . one queries the index structure for the closest k points to each example  and as before one simply selects the top candidates according to the outlier definition. for low-dimensional data sets this approach can work extremely well and potentially scales as n logn if the index tree can find an example's nearest neighbors in logn time. however  index structures break down as the dimensionality increases. for example  breunig et al.  used a variant of the x-tree to do nearest neighbor search and found that the index only worked well for low dimensions  less than 1  and performance dramatically worsened for just 1 or 1 dimensions. in fact  for high-dimensional data they recommended sequential scanning over the index tree.
a few researchers have proposed partitioning the space into regions and thus allowing faster determination of the nearest neighbors. for each region  one stores summary statistics such as the minimum bounding rectangle. during nearest neighbor search  one compares the example to the bounding rectangle to determine if it is possible for a nearest neighbor to come from that region. if it is not possible  all points in the region are eliminated as possible neighbors. knorr and ng  partition the space into cells that are hyperrectangles. this yields a complexity linear in n but exponential in the number of dimensions. they found that this cell based approach outperformed a nested loop algorithm  which is quadratic in n  only for four or fewer dimensions. others use a linear time clustering algorithm to partition the data set  1  1 . with this approach  ramaswamy et al. demonstrated much better performance compared with the nested loop and indexing approaches on a low-dimensional synthetic data set. however  their experiments did not test how it would scale on larger and higher-dimensional data.
finally  a few researchers have advocated projections to find outliers. aggrawal and yu  suggest that because of the curse of dimensionality one should focus on finding outliers in low-dimensional projections. angiulli and pizzuti  project the data in the full feature space multiple times onto the interval  1  with hilbert space filling curves. each successive projection improves the estimate of an example's outlier score in the full-dimensional space. their initial scaling results are promising  and appear to be close to linear  however they have reported results on only two synthetic domains.
in this paper  we show that the simplest type of algorithm based on nested loops in conjunction with randomization and a pruning rule gives state-of-the-art performance. table 1 shows our variation of the nested loop algorithm in more detail. the function distance computes the distance between any two examples using  for example  euclidean distance for continuous features and hamming distance for
table 1: a simple algorithm for finding distance-based outliers. lowercase variables represent scalar values and uppercase variables represents sets.

procedure: find outliers
input: k  the number of nearest neighbors; n  the number of outliers to return; d  a set of examples in random order.
output: o  a set of outliers.
let maxdist x  y   return the maximum distance between x and an example in y . let closest x  y   k  return the k closest examples in y to x.
begin
1. c ¡û 1	// set the cutoff for pruning to 1
1. o ¡û  	// initialize to the empty set
1. while b ¡û get-next-block d  {	// load a block of examples from d
1. neighbors b  ¡û   for all b in b
1. for each d in d {
1. for each b in b  b =1	d {
1. if |neighbors b |   k or distance b d    maxdist b neighbors b   {
1. neighbors b  ¡û closest b neighbors b  ¡È d  k 
1. if score neighbors b  b    c {
1. remove b from b
1. } } } }
1. o ¡û top b ¡È o n 	// keep only the top n outliers
1. c ¡û min score o   for all o in o	// the cutoff is the score of the weakest outlier
1. }
1. return o end
discrete features. the score function can be any monotonically decreasing function of the nearest neighbor distances such as the distance to the kth nearest neighbor  or the average distance to the k neighbors.
the main idea in our nested loop algorithm is that for each example in d we keep track of the closest neighbors found so far. when an example's closest neighbors achieve a score lower than the cutoff we remove the example because it can no longer be an outlier. as we process more examples  the algorithm finds more extreme outliers and the cutoff increases along with pruning efficiency.
note that we assume that the examples in the data set are in random order. the examples can be put into random order in linear time and constant main memory with a disk-based algorithm. one repeatedly shuffles the data set into random piles and then concatenates them in random order.
in the worst case  the performance of the algorithm is very poor. because of the nested loops  it could require o n1  distance computations and o n/blocksize   n  data accesses.
1. experimentsonscalingperformance
in this section  we examine the empirical performance of the simple algorithm on several large real data sets. the primary question we are interested in answering is  how does the running time scale with the number of data points for large data sets   in addition  we are also interested in understanding how the running time scales with k  the number of nearest neighbors.
to test our algorithm we selected the five real and one synthetic data sets summarized in table 1. these data sets span a range of problems and have very different types of features. we describe each in more detail.
  corel histogram. each example in this data set encodes the color histogram of an image in a collection of photographs. the histogram has 1 bins corresponding to eight levels of hue and four levels of saturation.
  covertype. this data set represents the type of forest coverings for 1 ¡Á 1 meter cells in the rocky mountain region. for each cell  the data contains the cover type  which is the dominant tree species  and additional attributes such as elevation  slope  and soil type.
  kddcup 1. the kddcup data contains a set of records that represent connections to a military computer network where there have been multiple intrusions by unauthorized users. the raw binary tcp data from the network has been processed into features such as the connection duration  protocol type  number of failed logins  and so forth.
  census. this data set contains the responses from the 1 decennial census in the united states. the data has information on both households and individuals. we divided the responses into two tables  one that stores household records and another that stores person records  and treated each table as its own data set. both the household and person data sets have a variety of geographic  economic  and demographic variables. our data comes from the 1% state public use microdata samples and we used the short variable
list . in total  the 1% state sample contains about 1 million household and 1 million person records. for our experiments we used a maximum of 1 million records for each data set.
  normal 1d. this is a synthetic data set generated from a 1-dimensional normal distribution centered on the origin with a covariance matrix equal to the identity matrix.
we obtained the data sets corel histogram  covertype  and kddcup 1 from the uci kdd archive  and the census data from the ipums repository .
table 1: description of data sets
data setfeaturescontinuousexamplescorel histogram11covertype11kddcup 11 1household 11 1person 11 1normal 1d11 1we processed the data by normalizing all continuous variables to the range  1  and converting all categorical variables to an integer representation. we then randomized the order of examples in the data sets. randomizing a file can be done in o n  time and constant main memory with a disk-based shuffling algorithm as follows: sequentially process each example in the data set by randomly placing it into one of n different piles. recombine the piles in random order and repeat this process a fixed number of times.
we ran our experiments on a lightly loaded pentium 1 computer with a 1 ghz processor and 1gb ram running linux. we report the wall clock time  the time a user would have to wait for the output  in order to measure both cpu and i/o time. the reported times do not include the time needed for the initial randomization of the data set and represent one trial. preliminary experiments indicated that alternate randomizations did not have a major effect on the running time. to measure scaling  we generated smaller data sets by taking the first n samples of the randomized set. unless otherwise noted  we ran experiments to return the top 1 anomalies with k = 1  a block size  |b|  of 1 examples  and we used the average distance to the nearest k neighbors as the score function.
our implementation of the algorithm was written in c++ and compiled with gcc version 1 with the -o1 optimization flag. we accessed examples in the data set sequentially using standard iostream functions and we did not write any special routines to perform caching. the total memory footprint of the executing program was typically less than 1 mb.
figure 1 shows the total time taken to mine outliers on the six data sets as the number of examples varied. note that both the x and y axes are in a logarithmic scale. each graph shows three lines. the bottom line represents the theoretical time necessary to mine the data set given a linear algorithm based on the running time for n = 1. the middle line shows the actual running times of our system. finally  the top line shows the theoretical time needed assuming a quadratic algorithm based on scaling the running time for n = 1.
these results show that our simple algorithm gives extremely good scaling performance that is near linear time. the scaling properties hold for data sets with both continuous and discrete features and the properties hold over several orders of magnitude of increasing data set size. the plotted points follow nearly straight lines on the log-log graphs which means that the relationship between the y and x axis variables is of the form y = axb or logy = loga + blogx  where a and b are constants. thus  the algorithm scales with a polynomial complexity with an exponent equal to the slope of the line. table 1 presents for each data set the slope of a regression line fit to the points in figure 1. the algorithm obtained a polynomial scaling complexity with exponent varying from 1 to 1.
table 1: slope b of the regression fit relating logt = loga + blogn  or t = anb  where t is the total time  cpu + i/o   n is the number of data points  and a is a constant factor.
data setslopecorel histogram1covertype1kddcup 1.1household 1.1person 1.1normal 1d1we also examined how the total running time scales with k  the number of neighbors and the results for normal 1d and person  with n = 1 1  are shown in figure 1. in these graphs  both the x and y axes are in a linear scale and the measured times fall approximately on a straight line. this suggests that the running time scales linearly with k.
1. analysis of scaling time
in this section  we explain with an average case analysis why randomization in conjunction with pruning performs well  especially when much of the past literature reported that nested loop designs were extremely slow because of the o n1  distance computations. in particular  both knorr and ng  and ramaswamy et al.  implemented versions of the nested loop algorithm and reported quadratic performance. however  knorr and ng did not use pruning or randomization in their algorithm  and ramaswamy et al. only incorporated pruning.
consider the number of distance computations needed to process an example x. for now we assume that we are using outlier definition 1  rather than definition 1 which we used in our experiments  for ease of analysis. with this definition an outlier is determined by the distance to its kth nearest neighbor. in order to process x we compare it with examples in the data set until we have either  1  found k neighbors within the cutoff distance d  in which case we eliminate it as it cannot be an outlier  or  1  we have compared it with all n examples in the data set and failed to find k neighbors within distance d  in which case it is classified as an outlier.




figure 1: total time  cpu and i/o  taken to mine outliers as n  the number of points  increases. the top and bottom lines represent the theoretical time taken by a quadratic and linear algorithm based on scaling the observed time at n = 1.

figure 1: total time  cpu and i/o  taken to mine outliers as k increases for the data sets normal 1d and

person  with n = 1 1 .
we can think of this problem as a set of independent bernoulli trials where we keep drawing instances until we have found k successes  k examples within distance d  or we have exhausted the data set. let ¦Ð x  be the probability that a randomly drawn example lies within distance d of point x  let y be a random variable representing the number of trials until we have k successes  and let p y = y  be the probability of obtaining the kth success on trial y. the probability p y = y  follows a negative binomial distribution:
		 1 
the number of expected samples we need to draw to process one example x is:

the first term is the expectation of concluding a negative binomial series within n trials. that is  as we are processing an example  we keep drawing more examples until we have seen k that are within distance d  at which point we eliminate it because it cannot be an outlier. the second term is the expected cost of failing to conclude the negative binomial series within n trials  in which case we have examined all n data points because the example is an outlier  less than k successes in n trials .
the expectation of a negative binomial series with an infinite number of trials is 
		 1 
this is greater than the first term in equation 1. combining equations 1 and 1 yields 
	 1  surprisingly  the first term which represents the number of distance computations to eliminate non-outliers does not depend on n. the second term  which represents the expected cost of outliers  i.e  we must compare with everything in the database and then conclude that nothing is close  does depend on n  yielding an overall quadratic dependency to process n examples in total. however  note that we typically set the program parameters to return a small and possibly fixed number of outliers. thus the first term dominates and we obtain near linear performance.
one assumption of this analysis is that the cutoff distance is fixed. in practice  the cutoff distance varies during program execution  and the final cutoff required to return the top n outliers changes with n. however  the relationship between cutoff value and percentage of the data set processed often stays the same for different values of n. for example  figure 1 shows the plot of cutoff value against the percentage of the data set processed for different values of n.
in general  we expect that if the final cutoff distance increases with larger n  then scaling will be better as ¦Ð x  is larger and any randomly selected example is more likely to be a success  neighbor . conversely  if the cutoff distance decreases  the scaling will be worse. in figure 1 we plotted the relationship between b  the empirical scaling factor  and c1k/c1k  the ratio of the final cutoffs for n = 1 and n = 1 for the six data sets used in the previous section. we also plotted results for two additional data sets  uniform 1d and mixed 1d  which we believed would be respectively extremely difficult and easy. uniform 1d is a three-dimensional data set generated from a uniform distribution between  -1 1  on each dimension. mixed 1d is a mixture of the uniform data set  1%  combined with a gaussian  1%  centered on the origin with covariance matrix equal to the identity matrix.
the results indicate that for many data sets the cutoff ratio is near or greater than 1. the only data set with an extremely low cutoff ratio was uniform1d. the graph also
person

percent of data set processed
figure 1: value of the cutoff versus the percentage of the data set processed for n = 1k  1k  1m  and 1m.
indicates that higher values of the cutoff ratio are associated with better scaling scores  lower b . this supports our theory that the primary factor determining the scaling is how the cutoff changes as n increases.

figure 1: empirical scaling factor b versus c1k/c1k  the ratio of cutoff scores for n = 1 and n =
1.
figure 1 shows the running time plot for uniform 1d and mixed 1d. we expected uniform 1d to have extremely bad scaling performance because it has no true outliers as the probability density is constant across the entire space. increasing n simply increases the density of points and drops the cutoff score but does not reveal rare outliers. in contrast  the results for mixed1d were extremely good  b = 1 . in this data set  as we increase n we find more extreme outliers from the gaussian distribution and the cutoff distance increases  thus improving pruning efficiency. finally  we note that data sets with a true uniform distribution are probably rare in real domains.
1. outliers in census data
although the use of distance-based outliers is well established  in this section  we show results from the census data to give the readers a qualitative idea of the types of outliers found when large data sets are mined. we also compare the discovered outliers with examples flagged as unusual by gritbot  a commercial program from rulequest research that was designed to find anomalies in data .
as we have limited space in this paper  we present only selected results. the full list of outliers on the household and person data sets for both our algorithm and gritbot are available online1 and we encourage the readers to view this list directly.
we emphasize that we are not claiming that one set of results is better than another  but rather we feel these results show that distance-based outlier detection finds unusual examples of a qualitatively different nature than gritbot.
1 distance-based outliers
we report selected results from running our outlier detection algorithm on the full set of 1 million examples to return the top 1 outliers with k = 1.
the top outlier in the household database is a single family living in san diego with 1 married couples  1 mothers  and 1 fathers. in the census data  a family is defined as a group of persons related by blood  adoption  or marriage. to be considered a mother or father  the person's child or children must be present in the household. the house had a reported value of $1k and was mortgaged. the total reported income of the household was approximately $1k for the previous year.
another outlier is a single-family rural farm household in florence  south carolina. the house is owned free and clear by a married couple with no children. this example is unusual because the value of the house is greater than $1k  not including the land   and they reported a household income of over $1k.
in the person data set one of the most extreme outliers was a 1+ year old black male with italian ancestry who does not speak english  was enrolled in school1  has a doctorate degree  is employed as a baker  reported $1k income of which $1k was from wages  $1k from business  $1k from farming  $1k from welfare  and $1k from investments  has a disability which limits but does not prevent work  was a veteran of the u.s. armed forces  takes public transportation  ferry boat  to work  and immigrated to the u.s. 1 years ago but moved into his current dwelling 1 years ago. clearly  there are inconsistencies in this record and we believe that this record represents an improperly completed form.

figure 1: total time  cpu and i/o  taken to mine outliers on the data sets uniform 1d  b = 1  and mixed1d  b = 1 .
a second outlier was a 1 year old  white  widowed female living with 1 family members  two of which are her own children. she has a disability that limits but does not prevent her work as a bookkeeper or accounting clerk in the theater and motion picture industry. she takes public transportation to work  bus or trolley  and it takes her longer than 1 minutes to go from home to work.
a third outlier was a 1 year old  white  female with asian ancestry and mexican hispanic origin with a disability that limits but does not prevent work. she earned $1k in business income  and $1k in retirement income  which may include payments for disabilities   and is also enrolled in school.
1 gritbot
gritbot finds records that have a surprising value on one attribute given the values of other attributes. for example  an outlier gritbot found on the person data set was
case 1:
raced = white  1 cases  1% 'black' 
ancest1d = african american languagd = english
this means that 1% of people who have african american ancestry and who speak english  listed their race as black. case 1 is unusual because the race listed was white.
we were not able to run gritbot on the household and person data sets with five million examples because of memory limitations. gritbot's requirements exceeded the available main memory as it loaded the entire data set and then allocated additional memory during program execution. however  we were able to run gritbot on smaller data sets  and specifically  we ran gritbot using the default settings on approximately one million household records and one half million person records.
since gritbot and our algorithm compute two different sets of outliers  precise comparisons of their running times are not very meaningful. however  to give the reader a rough idea of their performance  gritbot took approximately 1 minutes to process one million household records and 1 minutes to process one half million person records on a 1 mhz mips r1 with 1 gb of memory. in comparison  our algorithm took 1 and 1 minutes respectively to process similar amounts of data on a 1 ghz pentium 1 with 1 gb of memory.1
in contrast to the results from distance-based outliers  gritbot found qualitatively different outliers. for example  on the household data gritbot found a total of 1 anomalies. these anomalies could be divided into roughly three groups:
  1 records for which the household was listed as  rural  although another field indicated that the household was urban  e.g.  metro = in metro area - central city or citypop   1 
  1 records for which the household was listed as  urban  although another field indicated that the household was rural.
  1 records with a total family income  ftotinc  greater than the household income  hhincome . by definition the household income should be greater than or equal to the family income.
on the person data set  gritbot found a total of 1 anomalies. unlike the household data  we could not place the examples into neat categories  but as before gritbot found records with unusual combinations of attributes which included
  people with unusual combinations of ancestry  hispanic origin  and race. for example  gritbot found records for people who are white and african-american  black and italian  black and swedish  black and german  black and polish  hispanic and scotch-irish.
  people who live in the same house where they lived 1 years ago  but also claimed to live in a different country five years ago.
  people who don't work  but have a place of work.
  a person whose ancestry is mexican  but the language spoken at home is chinese.
  a 1 year old person who last worked more than 1 years ago.
  a 1 year old female veteran.
in general  gritbot tended to find examples in which a small number of attributes made the example unusual. this is not surprising as by default gritbot is set to examine four or less conditions. however  gritbot often did not use all four conditions and many outliers had only one or two terms.
1. limitations and future work
the main goal of our experimental study was to show that our algorithm could scale to very large data sets. we showed that on large  real  high-dimensional data sets the algorithm had near linear scaling performance. however  the algorithm depends on a number of assumptions  violations of which can lead to poor performance.
first  our algorithm assumes that the data is in random order. if the data is not in random order and is sorted then the performance can be poor. for example  the census data as retrieved from the ipums repository  came with the examples sorted by state. this can cause problems when our algorithm considers a person from wyoming. it will try to eliminate it by finding the k nearest neighbors who are also likely to be from wyoming. to find these neighbors  the algorithm will first scan all examples from states alabama to wisconsin given the sequential manner in which it accesses the data.
second  our algorithm depends on the independence of examples. if examples are dependent in such a way that they have similar values  and will likely be in the set of k nearest neighbors  this can cause performance to be poor as the algorithm may have to scan the entire data set to find the dependent examples.
an extreme version of this problem can occur when the data set originates from a flattened relational database for example  if there are two tables x and y   with each example in x pointing to several different objects in y   our flattened database will have examples with form  x1 y1    x1 y1    x1 y1    x1 y1   ... and so forth. as it is likely that the closest neighbors of  x1 y1  will be the examples  x1 y1  and  x1 y1  our algorithm may have to scan the entire data set until it finds them to obtain a low score.
however  our algorithm may still perform acceptably on data sets with less severe violations. for example  the examples in the person data set are not completely independent as they are tied together by a common household.1 however  the performance on this data set  b = 1  was still very good.
the third situation when our algorithm can perform poorly occurs when the data does not contain outliers. for example  our experiment with the examples drawn from a uniform distribution had very poor scaling. however  we believe data sets of this type are likely to be rare as most physical quantities one can measure have distributions with tails.
we are interested in extending our work in this paper in several ways. first  we are interested in speeding up the algorithm even further. in section 1 we showed that the scaling performance depended on how the cutoff changes as we process increasingly larger data sets. the algorithm starts with a cutoff threshold of zero which increases as better outliers are found. one modification is to start the algorithm with a pre-defined cutoff threshold below which we would consider any example to be uninteresting. in preliminary experiments  a good initial guess could cut time to a third. there may also be automatic ways to get a good cutoff early. for example  we could first process the examples with a small data set to get an idea of the examples that are most unusual. we then place these examples at the beginning of the data file.
another pressing limitation is that our work has only addressed finding outliers in the data sets that can be represented with a vector space or equivalently a single table in a database. many real data sources will be in the form of relational databases with multiple tables that relate different types of information to each other.
to address relational data  the simplest solution is to flatten the database with join operators to form a single table. while this is a convenient solution it loses much of the information available. for instance  a flattened database cannot easily represent households that have a variable number of individuals. we also found that flattening a database could create dependencies between examples and  as we explained above  this can reduce the effectiveness of randomization and pruning.
we are currently investigating how we can extend our algorithm to handle relational data natively. there are two research questions that arise. first  how does one define a distance metric to compare objects which may have a variable number of linked objects  there has been some work on defining metrics for relational data  1  1  1 . the central idea is to apply a recursive distance measure. that is  to compare two objects one starts by comparing their features directly  and then moves on to compare linked objects and so on. second  how does one efficiently retrieve an object and its related objects to compare them in the context of searching for outliers  retrieving related objects may involve extracting records in a non-sequential order and this can greatly slow database access.
finally  there are many practical issues with algorithms for mining distance-based outliers that we did not investigate such as determining how to set algorithm parameters such as k  the block size  the distance measure  and the score function. each of these parameters can have a large effect on the discovered outliers  or running time for the block size . in supervised classification tasks one can set these parameters to maximize predictive performance by using a hold out set or cross-validation to estimate out of sample performance. however  outlier detection is unsupervised and no such training signal exists.
1. conclusions
in our work applying outlier detection algorithms to large  real databases a major limitation has been scaling the algorithms to handle the volume of data. in this paper  we addressed the scaling problem with an algorithm based on randomization and pruning which finds outliers on many real data sets in near linear time. this efficient scaling allowed us to mine data sets with millions of examples and many features.
1. acknowledgments
we thank thomas hinke and david roland of nasa ames for reviewing a draft of this paper. this work was supported by the cict program at nasa ames research center under grant ncc 1.
