this paper presents work done at cambridge university for the trec-1 spoken document retrieval  sdr  track. the cuhtk transcriptions from trec-1 with word error rate  wer  of 1% were used in conjunction with stopping  porter stemming  okapi-style weighting and query expansion using a contemporaneous corpus of newswire. a windowing/recombination strategy was applied for the case where story boundaries were unknown  su  obtaining a final result of 1% and 1% average precision for the trec-1 short and terse queries respectively. the corresponding results for the story boundaries known runs  sk  were 1% and 1%. document expansion was used in the sk runs and shown to also be beneficial for su under certain circumstances. non-lexical information was generated  which although not used within the evaluation  should prove useful to enrich the transcriptions in real-world applications. finally  cross recogniser experiments again showed there is little performance degradation as wer increases and thus sdr now needs new challenges such as integration with video data.
1. introduction
with the ever-increasing amount of digital audio data being produced  it is becoming increasingly important to be able to access the information contained within this data efficiently. spoken document retrieval  sdr  addresses this problem by requiring systems to automatically produce pointers to passages in a large audio database which are potentially relevant to text-based queries. the systems are formally evaluated within trec using relevance assessments produced by humans who have listened to the audio between previously established manually-defined  story  boundaries. a transcription generated manually is also providedfor a reference run to give an approximate upper-bound on expected performance.
the natural way to allow easy indexing and hence retrieval of audio information is to represent the audio in a text format which can subsequently be searched. one such method is to represent the speech present in the audio as a sequence of sub-word

now sue tranter  dept. of engineering science  oxford  ox1pj  uk :
sue.tranter eng.ox.ac.uk
　　now	at	laboratoire	d'informatique	de	l'universite	d'avignon	: pierre.jourlin lia.univ-avignon.fr
units such as phones; generate a phone sequence for the textbased query; and then perform fuzzy matching between the two.  see e.g.  1  1   the fuzzy phone-level matching allows flexibility in the presence of recognition errors and out of vocabulary  oov  query words can potentially find matches. however  this approach still requires a method of generating phone sequences from the query words  usually a dictionary ; it cannot easily use many standard text-based approaches  such as stopping and stemming; and performance on large scale broadcast news databases  such as those used within the trec-sdr evaluations is generally poor.
with the recent improvements in the performance and speed of large vocabulary continuous speech recognition  lvcsr  systems  it is possible to produce reasonably accurate word based transcriptions of the speech within very large audio databases. this allows standard text-based approaches to be applied in retrieval  and means that a real user could easily browse the transcripts to get an idea of their topic and hence potential relevance without needing to listen to the audio.  see e.g.  . the inclusion of a language model in the recogniser greatly increases the quality of the transcriptions over the phone-based approach  and the overall performance of word-based systems has outperformed other approaches in all previous trec-sdr evaluations . oov words do not currently seem to present a significant problem provided that suitable compensatory measures are employed  and rolling language models have been investigated  see e.g.   as a way to adapt to changing vocabularies as the audio evolves.
several methods to compensate for the errors in the automatically generated transcriptions have been devised. most of these use a contemporaneous text-based news-wire corpus to try to add relevant non-erroneous words to the query  e.g.  1  1   or documents  e.g.  1  1  1   although other approaches are also possible  e.g. the machine-translation approach in  . these methods have proven very successful even for high error rate transcriptions   so the focus of sdr has generally switched to trying to cope with continuous audio streams  in which no  document  boundaries are given1. this story-boundary-unknown
 su  task is the main focus of the trec-1 sdr evaluation.

our overallapproach involves generating a word-leveltranscription and dividing it into overlapping 1 second long windows. standard stopping  stemming and okapi-weighting are used during retrievalwith query expansion from a contemporaneous newswire collection  before merging temporally close windows to reduce the number of duplicates retrieved.
this paper describes the cambridge universitysdr system used in the trec-1 sdr evaluation. sections 1 and 1 describe the tasks and data for the evaluation in more detail. the problem of extracting non-lexical information from the audio which may be helpful for retrieval and/or browsing is addressed in section 1 and the transcriptions used are described in section 1. development for the su runs is given in section 1  with results from the final system on all transcriptions and query sets given in section 1. the effects of using non-lexical information in retrieval are investigated in section 1 and a contrast for the case where story boundary information is known  sk  is given in section 1. finally conclusions are offered in section 1.
1. description of trec-1 sdr tasks
the trec-1 sdr evaluation  consisted of two tasks. for the main story-boundary-unknown  su  task  the system was given just the audio for each news episode  e.g. entire hour-long newscasts  and had to produce a ranked list of episode:time stamps for each text-based query. the scoring procedure involved mapping these stamps to manually defined story-ids  with duplicate hits being scored as irrelevant  and then calculating precision/recall in the usual way1.
the two differences from this task to the trec-1 sdr su evaluation task  1  1  are firstly that for trec-1  all the audio was judged for relevance  including e.g. commercials  and secondly that non-lexical information  such as the bandwidth/ gender /speaker-id  or the presence of music etc.  that was automatically detected by the speech recognition system could be used in addition to the word-level output at retrieval time. a contrast run  sn  was required without the use of the non-lexical information  if it had been used within the su run  to allow the effect of this additional information to be seen.
another contrast run where manually-defined story boundaries were provided  sk  allowed the degradation from losing the story boundary information to be evaluated. this is the same as the primary task in the trec-1 sdr evaluation. sites had to run on their own transcriptions  s1   a baseline provided by nist  b1  and the manually-generated reference  r1.
1. description of data
the audio data for the document collection was the same as that used in the trec-1 sdr evaluation  namely 1 hours   1m words   from 1 episodes of american news broadcast between

february and june 1 inclusive. the sk runs took a subset of  1m words divided into 1 manually defined  stories  to give an average document length of  1 words.
the queries used for development  trec-1  and evaluation  trec-1  are described in table 1. two sets of queries were used  namely short  corresponding to a single sentence  and terse  approximately 1 key words . the query sets corresponded to the same original information needs and thus the same relevance judgements were used in both cases. the introduction of terse queries was new for trec-1  and was intended to model the keyword-type query used in many www search engines. since there were no existing terse development queries  terse forms of the trec-1 queries were developed in house and thus are not the same as those used by other sites.
dev  trec-1 eval  trec-1 num. queries1ave. # words in query1  s  1  t 1  s  1  t ave. # distinct terms per q.1  s  1  t 1  s  1  t ave. # rel docs11table 1: properties of query and relevance sets. s=short t=terse 
the contemporaneous parallel text corpus used for query and document expansion consisted of 1k newswire articles   1m words  from january to june 1. although significantly smaller than that used by some other sites  e.g. 1k articles in    in previous work we found that increasing the parallel corpus size to approximately 1k articles did not help performance . the corpus  summarised in table 1  consisted of the  unique  new york times  nyt  and 1% of the associated press  apw  articles from the trec-1 sdr newswire data enhanced with some la times/washington post  latwp  stories and was evenly distributed over the whole time period.
sourcelatwpnytapwtotalnum. stories11ave. # words in doc.11table 1: description of the parallel corpus.
1. generating non-lexical information
audio contains much more information than is captured simply by transcribing the words spoken. for example  the way things are said  or who said them can be critical in understanding dialogue  and many non-speech events  such as music  applause  sudden noises  silence etc.  may also help the listener follow what was recorded. current speech recognisers can automatically recognise many of these things  such as the speaker id or gender  e.g.   and the presence of music  noise and silence etc.  e.g.    but the speech-recognition-transcription  srt  format used in the sdr evaluations does not support the inclusion of such additional information. for trec-1 a new segmentation detection table  sdt  file was allowed   which represented various audio phenomena found during recognition in a text-based format which could be used at retrieval time.
there are two main uses for such non-lexicalinformation  namely to increase retrievalperformance and to help navigation/browsing in real sdr applications. the trec-1 sdr evaluation only allowed the former to be properly evaluated  but the latter is equally important in real world applications  and tags should not be thought to be irrelevant just because they were not used in the retrieval stage of the system .
non-lexical information can be used to help su retrieval in two main ways. firstly some information about broadcast structure including potential locations of commercials and story boundaries can be postulated from audio cues such as directly-repeated audio sections  changes in bandwidth/speaker or the mean energy in the signal. secondly properties such as the presence of music  background noise or narrowband speech can be used to identify portions of transcription which are potentially less reliable than normal.
table 1 shows the tags generated  whilst the next section explains how these were produced and section 1 discusses their effect on retrieval performance.
tag high -energyrepeatcommercialnumber111segmentgenderbandwidthnospeech1111table 1: non-lexical tags generated for trec-1.
1. segment  gender  bandwidth and nospeech
the first stage of our speech recognition system consists of an audio segmenter. initially the data is classified into wideband speech  narrowband speech or pure music/noise  giving the bandwidth and nospeech tags respectively. the labelling process uses gaussian mixture models and incorporates mllr adaptation. a gender-dependent phone recogniser is then run on the data  and the smoothed gender change points and silence points are used in the final segmentation  hence generating the segment tags. more details can be found in  and .
1. energy
signal energy can help to indicate the presence of commercials. the average normalised log energy  nle 1 for the trec-1 and january tdt-1 data  given in table 1  shows that in general commercials have a higher mean energy content than news.
trec-1 datajanuary tdt-1 databr.storyfillercomm.newscomm.abc-1-1-1-1-1cnn-1-1-1-1-1pri-1-1-1-1-1table 1: average normalised log-energy for trec-1 and january tdt-1 data for stories  fillers and commercials.

by windowing the audio and comparing the nle for each 1s window to a threshold  it is possible to generate a crude indicator of where commercials might be occurring. imposing a minimum length restriction on the postulated commercials can be used to reduce the false alarm rate. table 1 shows the results of applying such a system on the development  january tdt1  and test  trec-1  data. whilst the method does pick out relatively more commercials than news stories  it is not accurate enough in itself to be used during retrieval  and would need to be combined with other cues for more reliable commercial identification. tags were generated using a threshold of 1db  nle=-1   but these were not used in the retrieval system for the reason mentioned.
mlabcpricnn-1-1 11 11 1-1-1 11  11  1-11s1 11  11  1a  development data  january tdt-1 voa-1-1 11 11 11 1-1-1 11 11  11 1-11s1 11  11  11 1b  trec-1 test data
table 1: percentage non-story   story rejection when using a threshold    on the normalised log energy for 1s windows  including restricting the minimum length  ml.
1. repeat and commercial
direct audio repeats  i.e. re-broadcasts  were found using the technique described in   by comparing all the audio  across the entire 1 months  from each broadcaster. commercials were postulated in a similar way to that described in   by assuming that segments which had been repeated several times were commercials and that no news portion of less than some smoothing length could exist between them. table 1 shows the results from applying the parameter set used in the evaluation  c-e  and a less conservative run  c-1  as a contrast. the numbers for our trec-1 commercial detection system are given for comparison.
time  h trec-1trec-1br.n-stst.c-ec-ec-1abc111 11 11 1cnn11.1.1.1.1.1.1pri111 11 11 1voa111 11 11 1all11 11 11 1table 1: overall time and percentage of non-stories   stories rejected using both the trec-1 and trec-1 commercial detection systems with a less conservative c-1 run for comparison.
detection performance with this strategy is very impressive  with over half the adverts being identified for negligible loss of news content. removing these postulated commercials automatically before retrieval was earlier shown not only to reduce the amount
of processing necessary but also to significantly improve performance on the trec-1 data . the improvement from the trec-1 to the trec-1 commercial detection system is due to the change in rules which allows both segments for any given match to be noted within the sdt file1.
1. transcriptions
1. s1 transcriptions
the transcriptions used for our s1 runs were those we generated for the 1 trec-1 sdr evaluation. a summary of the system is shown in figure 1 and a detailed description can be found in . the system ran in 1xrt1 and gave a word error rate  wer  of 1% on the november 1 hub1 eval data and
1% on the 1-hour scoring subset of the trec-1 data.

figure 1: system used to generate transcriptions.
1. other available transcriptions
manually generated closed-caption transcriptions1 were available for the stories within the sk part of the evaluation from trec-1 . word-level time stamps for these portions were produced by limsi using forced alignment after some text normalisation. reference transcriptions were also made for the remaining untranscribed portions of the data by nist using rover on the available trec-1 asr transcriptions . the subsequent reference r1 was thus considerably different to the corresponding set of reference transcriptions for trec-1.
additional transcriptions were made available for the trec1 sdr runs. the baseline cases from trec-1 sdr produced by nist using the bbn rough'n'ready recogniser  were re-released with b1 from trec-1 becoming cr-nist1 

whilst b1 from trec-1 became the baseline b1 for trec-1. the trec-1 transcriptions from sheffield  and limsi  were re-released as cr-shef-s1 and cr-limsi-s1  whilst both sites provided new  higher quality  transcriptions named cr-shef-s1  and cr-limsi-s1  respectively. the wer for these sets of transcriptions on the 1hr trec-1 scoring subset of the corpus are shown in table 1.
recognisercorr.sub.del.ins.werr1.1.1.1.1.1 cuhtk- s1.1.1.1.1.1cr-limsi-s1.1.1.1.1.1cr-limsi-s1.1.1.1.1.1cr-cuhtk1-p1.1.1.1.1.1b1.1.1.1.1.1cr-nist111111cr-shef-s1.1.1.1.1.1cr-shef-s1.1.1.1.1.1table 1: wer on trec-1 hour scoring subset of eval. data.
1. su development
1. the basic system
the basic framework for the su system  shown in figure 1  is similar to our trec-1 system ; but it does not enforce boundaries at proposed commercial breaks  it uses a different method of performing query expansion and is simpler in not having part-of-speech query weighting  semantic poset indexing or parallel collection frequency weighting.

figure 1: framework for the su system.
the transcriptions were first filtered  removing all words which
occurred within periods labelled as commercial in the nonlexical file  see section 1 . windows of 1s length with an inter-window shift of 1s were then generated to divide up the continuous stream of transcriptions.
text-normalisation was applied to the query and parallel corpus to minimise the mismatch between the asr transcriptions and the text-based sources. preprocessing including mapping phrases and some stemming exceptions  punctuation removal  stop word removal and stemming using porter's algorithm  for all documents and queries. the stoplist included numbers since some development experiments suggested this increased performance slightly.
the retrieval engine was similar to that employed in trec1   using the sum of the combined-weights  cw   for each query term to give the score for any given document. for all runs  the value of used in the cw formula was 1  whilst was set to 1 when story boundary information was present  e.g. when using the parallel corpus  or 1 when no documentlength normalisation was necessary  e.g. on the windowed test collection . the inclusion of both query and document expansion before the final retrieval stage is discussed in section 1.
the final recombination stage pooled all windows which were retrieved for a given query which originated within 1 minutes of each other in the same episode. only the highest scoring window was retained  with the others being placed in descending order of score at the bottom of the ranked list. although this means that temporally close stories cannot be distinguished  we assume that the probability that two neighbouring stories are distinct but are both relevant to the same query is less than the probability they are from the same story which drifts in and out of relevance. although alternative  more conservative strategies are also in use  see e.g.    this strategy proved effective in development experiments .
1. document and query expansion
1.1. query expansion
blind relevance feedback  brf  was used to expand the queries prior to the final retrieval stage within our trec-1 system . the implementation of query expansion used for trec-1 differs from this in two main ways. the first concerns which index files to use for the expansion  and the second how to weight the query terms after the expansion stage.
in previous work we ran blind relevance feedback first on the parallel corpus only  pbrf   followed by another run on the test corpus alone  brf  before the final retrieval stage  e.g.  . the idea behind this 'double' expansion was to use the larger parallel corpus  which contained knowledge of story boundaries and had no transcription errors  to add robustly related terms to the query before running the standard brf technique on the test collection. including both stages of brf was found to be helpful to performance . however  we have found it very sensitive to the number of terms added    and number of documents assumed relevant    for each stage. recent work has used a single stage of query expansion on the union of the parallel and test collections  ubrf  before the final retrieval stage . this gives similar results but is less sensitive to the values of and chosen and hence was used in the trec-1 system.
the method of adding and re-weighting terms during query expansion was changed from trec-1 to follow the specifications given in  and  more strictly. all terms were ranked using their offer weights  ow   but only those which did not occur in the original query were then considered as potential terms for expansion. the final matching score was obtained by using the ms-rw formula as described on page 1 of . unlike in previous years  both the original terms and the new expanded terms were reweighted using their relevance weight  rw .
1.1. document expansion
whilst document expansion has been shown to be beneficial for the case where story boundaries are known  1  1  1   it does not seem to have been explored for the su case. we therefore implemented a document expansion stage for our su windowing system based on that used in our trec-1 sk system   namely:
1. form a pseudo-query for each window containing more than 1 different terms  consisting of each distinct term
1. run this pseudo-query on the parallel collection  giving equal weight to all terms
1. find the top expansion terms with the highest offer weight from the top documents
1. add each expansion term to the window once  i.e. increase the term frequency for each expansion term by 1 
experiments varying the values of and showed that the best performance was obtained for for the trec-1 queries. this document-expanded index file was then used for the final retrieval stage along with the queries generated before document expansion.
1.1. results
the results from including query and document expansion within the su system on trec-1 queries are summarised in table 1
and graphically illustrated in figures 1 and 1.
when there is no query expansion  document expansion increases mean average precision by 1% and 1% relative for short and terse queries respectively. for moderate query expansion  e.g.
　　   document expansion is beneficial for both short and terse queries  but this advantage disappears as the level of query expansion increases. although the best result for the short queries is obtained when including document expansion  1% vs 1%   the best performance for the terse queries is considerably worse when including document expansion  1% vs 1%  and thus it was not included in the final system.
the values of were chosen for the ubrf stage despite the fact that they were not optimal for either the short or the terse queries  since they provided more consistent performance across the different query sets.
1
figure 1: effect of query and document expansion on trec-1 short queries for su task on s1 transcriptions.

figure 1: effect of query and document expansion on trec-1
terse queries for su task on s1 transcriptions.
docexpqryexpshort qterse qtrtravepr-pavepr-p----1111--11111--11111-
--
-1
1
1.1
11
111
111--1111111111111111111111111111table 1: interaction of query and document expansion on su task on s1 transcriptions.
1. changing the window skip
recent work at sheffield  suggested that increasing the over-
lap between windows by decreasing the skip during window generation could help improve performance. a contrast run with their lower skip time was thus made to see if this would have helped our system. the results  given in table 1  show that this would not have been beneficial to our system  which uses a significantly different method of final window recombination to that used in sheffield's system.
short queriesterse querieswindowing systemavepr-pavepr-plength 1s  skip 1s1111length 1s  skip 1s1111table 1: effect of reducing the skip size in window generation for s1 transcriptions for su trec-1 queries.
1. summary
thus to summarise  after our trials with the trec-1 queries  our trec-1 su evaluation system used windowing  filtering of potential commercials  relatively simple indexing  query but not document expansion  standard okapi weighting and postretrieval merging. the query expansion was performed on the union of the test and the parallel text collections.
1. the final trec-1 su system
the results using the trec-1 evaluation su system on all transcriptions are given in tables 1 and 1 for the  development  trec-1 and  evaluation trec-1 query sets respectively whilst the relationship between performance and wer is illustrated in figure 1.
transcriptionsshort q.terse q.idweravepr-pavepr-pr1.1.1.1.1.1 cuhtk -s1.1.1.1.1.1cr-limsi1.1.1.1.1.1cr-limsi1.1.1.1.1.1cr-cuhtk111111b1.1.1.1.1.1cr-nist111111cr-shef1.1.1.1.1.1cr-shef1.1.1.1.1.1table 1: cross-recogniser results for  development  trec-1 queries using the trec-1 su evaluation system.
the results confirm the conclusions from earlier work in sdr   that the decline in performance as wer increases is fairly gentle  -1%avep/%wer on average here . the relative degradation with wer for the trec-1 and trec-1 short queries is almost identical  -1 vs -1 %avep/%wer   showing that this fall-off is not query-set specific1.

transcriptionsshort q.terse q.idweravepr-pavepr-pr1.1.1.1.1.1 cuhtk -s1.1.1.1.1.1cr-limsi1.1.1.1.1.1cr-limsi1.1.1.1.1.1cr-cuhtk111111b1.1.1.1.1.1cr-nist111111cr-shef1.1.1.1.1.1cr-shef1.1.1.1.1.1table 1: cross-recogniser results for the trec-1 su eval.

figure 1: relationship between wer and avep for the trec-1
system on trec-1 and trec-1 queries. the ellipses represent 1 standard-deviation points.
the performance on the trec-1  development  queries is significantly higher than that on the trec-1  evaluation  queries. this may be in part due to three reasons  namely
1. the parameters were tuned for the trec-1 queries  and may thus be sub-optimal for the trec-1 queries.
1. all commercials and  filler  portions  e.g. those which summarise stories coming up  were also evaluated for relevance in trec-1  whereas they were assumed irrelevant for trec-1. over the 1 trec-1 queries  there were 1 instances of these portions being scored as relevant. since our system tries to remove portions such as these by automatically removing commercials before retrieval and biasing the post-processing towards removing fillers1  the new relevance assessment procedure may have detrimentally affected our score.
1. natural variation in query difficulty may have meant the trec-1 queries were  harder  than the trec-1 ones1.

to investigate point 1 further  the trec-1 runs were re-scored using the trec-1 procedure  which assumed all non-news portions were irrelevant. this increased average precision by 1% on average for the b1  s1 and r1 runs for both query sets. this is partly because our su system tries to filter out the non-news portions before retrieval.
the number of relevant stories from each episode for each query was counted to investigate the validity of the assumption made during post-processing  that the probability of a given episode containing more than one relevant story for a given query was small. the results illustrated in figure 1 show that 1% of all the relevant stories are unique to their episode and query  but there remains the potential to increase performance by altering the post-processing strategy to allow more temporally close distinct hits1.

figure 1: number of relevant  stories  from each episode for each trec-1 query .
the expansion parameters were chosen so that the results for the terse and short trec-1 queries were similar  meaning suboptimal values were chosen when considering the short queries alone. when compared to the  similar  system from sheffield  whose parameters were chosen based solely on the short queries  we do more poorly on average for the short-query runs  but our results are better for all terse query runs .
in addition  the parameters which gave the best performance on the short development queries  give better performance on the trec-1 short queries  avep=1% on s1   but worse on the terse queries  avep=1% on s1 . this suggests that the choice of parameters should take the expected test query length into account and that performance over a wide range of queries might be increased if the expansion parameters were made to be functions of query length.


1. the effects of using non-lexical information
as mentioned in section 1  non-lexical information automatically derived from the audio could be used within retrieval in the trec-1 evaluation. thus  as discussed in section 1  we generated information for segment  gender  bandwidth  nospeech   high- energy  repeat and commercial tags directly from the audio.
for the su system we used the commercial tags to filter out words thought to have originated in commercial breaks  but we made no use of the other tags. thus for our required sn contrast run  we ran the su system without filtering out the commercials1. as can be seen from table 1  as well as reducing the amount of data processing by around 1%  filtering out commercials improved performance by a small  but statistically significant1 amount on both sets of development queries across all 1 transcriptions  r1 s1 b1 . for the trec-1 evaluation queries only the s1-terse and r1-terse comparisons were statistically significant1.
queryruntimeshort q.terse q.setidreject.avepr-pavepr-psn-r11111trec-1sn-s1.1h1111sn-b11111su-r1.1h1111trec-1su-s1.1h1111su-b1.1h1111sn-r11111trec-1sn-s1.1h1111sn-b11111su-r1.1h1111trec-1su-s1.1h1111su-b1.1h1111table 1: effect of automatically removing commercials  su .
contrast runs were also performed on the development queries using the less conservativecomm1 system and the manual boundaries derived from the sk case. as can be seen from table 1 using either of these would have resulted in little difference in performance for our own transcriptions.  none significant at the 1% level. 
other experiments were run for fun on the trec-1 queries to see the effect of removing various parts of the audio using the non-lexical information  such as high-energy regions  or particular bandwidth/gender segments. the results are given in table 1 for the s1 transcriptions  and plotted in figure 1. the

transcriptionsshort q.terse q.comm.rejectidavepr-pavepr-ptrec-1.1hr1.1.1.1.1comm1.1hs1.1.1.1.11hb1.1.1.1.1manual1hr1.1.1.1.1comms1hs1.1.1.1.1 ndx file 1hb1.1.1.1.1no loud1hs1.1.1.1.1no nb1hs1.1.1.1.1no wb1hs1.1.1.1.1no male1hs1.1.1.1.1no female1hs1.1.1.1.1table 1: effect of including non-lexical information for trec1 queries.  s1 reject times include time removed in trec-1 commercial detection and segmentation stages. 
trend is roughly linear  with the best avep to time-retained ratio being 1%avep/hr when removing all male speakers  whilst the worst is 1%avep/hr when removing female speakers.

figure 1: effect of removing data using non-lexical information on trec-1 queries for s1 transcriptions.
1. the story-known  sk  contrast run
the sk system was similar to the su system described in section 1. the commercial-removal  window-generation and postmerging stages were no longer necessary  since the known story boundaries defined the documents in the collection  but the rest of the system remained practically unaltered.
document expansion was performed in the same way as described in section 1.1 except that the pseudo-query for each document was defined as the 1 terms from the document with the lowest collection frequency. different values of and were investigated for the document expansion stage  but there proved to be little difference between the results  so the values of were chosen to be compatible with .
ubrf was performed as described in section 1.1  using the un-expanded document file to expand the query which was then run on the expanded document file  and the valuesof
   were retained for all retrieval stages. results for varying the expansion parameters in the ubrf stage for the sk system are illustrated in figures 1 and 1 for the short and terse trec-1 queries and are summarised in table 1.

figure 1: effect of query and document expansion on trec-1 short queries  sk case  s1 transcriptions.

figure 1: effect of query and document expansion on trec-1
terse queries  sk case  s1 transcriptions.
the inclusion of document expansion improved performance across both development query sets and all 1 transcriptions  with the largest improvements when the level of query expansion was low to moderate. this consistent improvementwas not found for the su case. the difference is thought to be because the pseudoqueries from windowingfor the su case may be multi-topic  and cannot be as long as for the sk case  since the windows must be kept small  e.g. around 1s  to obtain acceptable performance.
the values of were chosen for the ubrf stage for the sk run to give good performance across both development query sets when used in conjunction with document expansion. the amount of query expansion for the sk case was thus chosen to be less than that used for the su case because of the interaction between the query and document expansion devices.
the sk results on the trec-1 evaluation queries are given in table 1. since this used a subset of the data and hence also
docexpqryexpshort qterse qtr.trtravepr-pavepr-ps1----1111s1--11111s1 s1--111111--1111s111.1.1.1.1s111.1.1.1.1r1----1111r1--11111r11--1111r111.1.1.1.1b1----1111b1--11111b11--1111b111.1.1.1.1table 1: interaction of query and document expansion on sk task for trec-1 queries.
short queriesterse queriessksksusksksuidavepr-pavepavepr-pavepr1 a 11-11-s1 a 11-11-b1 a 11-11-r1 b 111111s1 b 111111b1 b 111111table 1: comparison of trec-1 sk and su results.  a  is on the 1 story subset  whilst  b  is on all the data  to allow a fairer comparison with the su case.
a different relevance file to the su case  another sk run across all the data was performed to allow a more direct comparison between sk and su cases.
although our su-sdr system has been improved by around 1% relative1 since the trec-1 evaluation   and the gap between sk and su has been reduced from 1% avep to 1%  there still remains a considerable performance gap between the sk and su cases.
1. conclusions
this paper has described work carried out at cambridge university for the trec-1 sdr evaluation. the experiments confirmed that the relative degradation of average precision with increasing recogniser error rate is gentle  and performance on high-quality asr transcriptions can be as good as that on a manually transcribed reference.
standard indexing techniques and okapi-weighting provide a good baseline system and adding query expansion using the union

of the test and a contemporaneous parallel newswire collection increases performance further. including a windowing and postretrieval recombination strategy allows good performance even when no story boundaries are known in advance. document expansion  which previously has been found to work well for the sk case  was extended to the su framework and shown to improve performance for small to moderate levels of query expansion.
non-lexical information derived directly from the audio  which would not normally be transcribed  can be used to improve real sdr systems. audio repeats can accurately predict the presence of commercials  which can be filtered out before retrieval  and some broadcast structure information can be recovered by analysing cues such as bandwidth  signal energy and the presence of music in the audio. browsing and understanding could also be improved by including tags such as sentence boundaries and speaker turns. optimally integrating non-lexical information within real sdr systems  using larger databases and including other information such as video data provide interesting challenges for the future.
acknowledgements
this work is in part funded by an epsrc grant reference gr/l1.
