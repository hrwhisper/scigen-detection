we suggest a way for locating duplicates and plagiarisms in a text collection using an r-measure  which is the normalized sum of the lengths of all suffixes of the text repeated in other documents of the collection. the r-measure can be effectively computed using the suffix array data structure. additionally  the computation procedure can be improved to locate the sets of duplicate or plagiarised documents. we applied the technique to several standard text collections and found that they contained a significant number of duplicate and plagiarised documents. another reformulation of the method leads to an algorithm that can be applied to supervised multi-class categorization. we illustrate the approach using the recently available reuters corpus volume 1  rcv1 . the results show that the method outperforms svm at multi-class categorization  and interestingly  that results correlate strongly with compression-based methods.
keywords
text categorization  text compression  language modeling  cross-entropy
general terms
verification
categories and subject descriptors
e.1  coding and information theory : data compaction and compression; h.1  systems : textual databases; h.1  database application : data mining; h.1  content analysis and indexing : indexing methods
1. motivation
　the number of texts in digital form increases rapidly and tremendously. several profound collections are now available: for example  project gutenberg with more than 1
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
documents; the moshkov library www.lib.ru for russianlanguage texts  1 gb as of july 1 ; the reuters corpus volume 1  rcv1   over 1k news articles ; and trec. text collections are used intensively in scientific research for many purposes such as text categorization  text mining  natural language processing  information retrieval and so on. every creator of a text collection is faced at some stage with the task of verifying its contents. for example  it might be undesirable to have duplicate documents in the collection since they can influence the text statistics  correctness of the results obtained  consume disk space and so on. in practice  the situation with duplicates can be even more complicated  with plagiarised  expanded  corrected or template documents occurring  see sanderson  and examples below in section 1 .
　we suggest a way for verifying the collection which seems to us natural  intuitively appealing and computationally effective. more precisely  we define an r-measure which is a number between 1 and 1 characterizing the  repeatedness  of the document. the r-measure is a normalized sum of lengths of all substrings of the document that are repeated in other documents of the collection. the r-measure can be computed effectively using the suffix array data structure. additionally  the computation procedure can be improved to locate the sets of the duplicate or plagiarised documents  and to identify  non-typical  documents  such as documents in a foreign language  these documents can then be subsequently removed to ensure the collection is valid . another reformulation leads to an algorithm that can be applied to supervised classification.
　we stress that the suggested techniques are characterbased and do not require a-priori knowledge about the representation of the documents  which can be  for example  utf-encoded unicode symbols  but we assume  of course that all documents are encoded in the same way  .
　the structure of the paper is as follows. in the next section we present the definition of the r-measure. we suggest several applications in section 1  among which is supervised classification. in section 1  we apply the techniques to several text collections  and in section 1  we apply the supervised categorization method to rcv1. we make some concluding remarks in section 1.
1. r-measure
　suppose that the collection consists of m documents  each document being a string ti = ti 1..|ti|   where |ti| is the length of the string ti. a squared r1-measure of document t with respect to documents t1  ...  tm is defined as
　　　　　　　　　　　　　　　　　　　l 1
r  t t1 ...tm  =  q t i..l  | t1 ...tm    1  l l + 1  i=1
where l = |t| is the length of document t  t i..l  is the ith suffix of document t and q s | t1 ... tm  is the length of the longest prefix of s  repeated in one of documents t1  ...  tm. for example  let us take t =  catsaton  with
t1 =  thecatonamat  and t1 =  thecatsat . then

	+  1 + 1 + 1 + 1 + 1 	「 1	 1 

with r t | t1 t1  = r1 t | t1 t1  「 1. notice that in  1   the sum consists of two parts   1+1+1  from the repetition of  catsat  = t 1..1  and  1+ 1+ 1+ 1 + 1  from  at on  = t 1..1 .
　in principle  we could replace the sum in  1  with the maximum function to get the alternative l-measure  which is the length of the longest common substring of t repeated in t1 or t1 or ... tm:
 
where l = |t|. for the example above  we have
.
　however  we feel that r-measure is a more  intuitive  measure  reflecting perhaps that a human would assign a higher repetition rank to t than 1  since substrings other than  catsat  = t 1..1  are also repeated. in   we present several lemmas to show that the r-measure is wellbehaved in many situations. in particular  we show that r − l and the maximum value for r and l is 1-this will occur when a document is completely duplicated elsewhere in the collection.
　r t | t1 ... tm  can be computed effectively using a suffix array  a full-text indexing structure  introduced in . it is conceptually easier to include t into the collection as t1 = t and to compute for all j = 1  ...  m the r-measure for document tj with respect to the rest of collection
 
where r．j1 is a non-normalized repetition measure. then  r t | t1 ... tm  = r1. the essential idea is to build a single string sc = t1$t1$...tm$ by concatenating all documents together separated by a special sentinel symbol $ to mark each document boundary. a suffix array is then constructed using a standard suffix array construction algorithm . the r．j1 values are calculated simultaneously by reading the suffix array sequentially and adding the lengths of the longest common prefixes between adjacent suffixes for the corresponding r．j1  see appendix for more details . the computation of all rj1-values has an o |sc|  time complexity with o |sc| +o m +o m  memory consumption  where m = maxj=1 ... m |tj|. this assumes that the average length of repeated substrings is significantly less than |sc| which is typical for text collections containing many documents. a heuristic approach based on so-called resilience  can reduce the worst case behaviour to o |sc|  for most highly-repetitive sc.
1. applications of r-measure
　statistical estimate for r．j1. notice that in a suffix array s1  ...  sn  constructed for sc  where n = |sc| and sc sj..n    sc sj+1..n  lexicographically for all j  all the suffixes are mixed  in the sense that the suffix array is a transposition of suffixes which is essentially random  this is reflected by suffix arrays being incompressible . if all the texts have approximately similar frequency distributions for letters  pairs of letters etc.  then notice that any sequence of
suffixescontribute approximatelysn1  ...  sn1 whereαr．j1ninto the non-normalized1  n1 「 αn  α   1 shouldr．j1.
hence  we can introduce a statistical estimate for r．j1:
	 	 1 
where r．j1 n1 n1  is computed using sn1  ...  sn1. a proper discussion of the properties of the estimate  1   as well as numerical evidences for its effectiveness  is beyond the scope of this paper  but we conjecture that it can be extremely useful  especially for applying to extra large data sets and for the purpose of plagiarism detection of a single document t in a large text collection.
　locating the duplicate sets: a heuristic pruning algorithm. duplicate documents are easily detected by determining which have r values = 1. note  however  that the quantity rj1 above does not provide information on which document is a duplicate of another  i.e. even if we know that document tj is completely repeated somewhere else in the text collection t1  ...  tm   i.e.  rj1 = 1   additional work is required to locate the document ti that contains substring tj. a very slight addition to the algorithm of computing r．j1 can help to identify the location of other repetitions. the idea is very simple: the list is organized with values r．jk1   containing contributions to r．j1 from suffixes of sc starting in document k.
　clearly the memory demands could be too large to keep all r．jk1 in memory. we suggest a heuristic to compute approximate values for the essentially large r．jk1 . while scanning the suffix array  a list list j  is maintained of  say  the 1 largest current contributors r．jk1 to r．j1. for each new contribution from suffix si to r．j1  we determine which document k． gives the contribution and if r．j1k． exists in the list j   we simply increase it by the necessary value. otherwise we add the newly found incomplete sum r．j1k． to the list j  and if the size of the list is   1  we exclude the entry with the smallest sum r．jk1 . after we have finished the scanning there is a very good chance that the list j  contains the candidates k with largest r．jk1 and this is born out in our experiments.
　supervised classification. notice that there exists two distinct types of classification. by topic categorization  the first type of problem  one usually means assigning several possible topics to the document. the other type of problem is called multi-class categorization  where the document has to fall into one of several predefined classes. of course  these types of problems are closely related  but they are definitely not equivalent. the first type of problem calls for construction of a binary classifier  which distinguishes only two classes and should be applied afterwards for each category. the second type of problem requires construction of a collection	ref.	r = 1	r − 1	r − 1
rcv1111reuters-11111news-11111news-1111russian-1 1 111
table 1: % of documents that pass the specified rmeasure conditions for various text collections.
multi-class classifier. r-measure can be used in this context: if one needs to select the correct class for the the document t among m classes represented by texts s1  ...  sm  we suggest the source be guessed using the following estimate
	θ  t  = argmaxi r t | si .	 1 
　identifying foreign and/or non-typical documents. non-typical documents can be located simply by examining those documents which have the lowest r-measures  since they are the ones which are least repeated elsewhere in the collection and therefore candidates for rejection during a verification phase. conversely  the articles with the highest measures will represent the ones that in same way  typify  the collection  but are also candidates for rejection because they may be duplicated or highly plagiarised . we also suggest the following method for identifying foreign language documents. in this context  we have a predominant  usually highly domain specific  language associated with the collection as a whole  and we are attempting to identify documents that have a different language to the predominant one. the method is as follows. construct several texts  one for the language typical of the collection  sl  and several for the target foreign languages  sf1  sf1  ...  sfn  that are anticipated to appear in the collection using text that is a representative sample for each language. for example  to identify the presence of french  german  dutch and belgian articles in rcv1  sl is constructed from some sample of english text  we can use english-language documents from rcv1 for this purpose   and sf1  ...  sf1 are constructed from samples of french  german  dutch and belgian text respectively. the identification proceeds by using  1 .
1. experimental results
1 analysis of various text collections
