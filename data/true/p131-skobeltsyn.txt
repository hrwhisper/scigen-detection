results caching is an efficient technique for reducing the query processing load  hence it is commonly used in real search engines. this technique  however  bounds the maximum hit rate due to the large fraction of singleton queries  which is an important limitation. in this paper we propose resin - an architecture that uses a combination of results caching and index pruning to overcome this limitation.
　we argue that results caching is an inexpensive and efficient way to reduce the query processing load and show that it is cheaper to implement compared to a pruned index. at the same time  we show that index pruning performance is fundamentally affected by the changes in the query traffic that the results cache induces. we experiment with real query logs and a large document collection  and show that the combination of both techniques enables efficient reduction of the query processing costs and thus is practical to use in web search engines.
categories and subject descriptors: h.1  information storage and retrieval : information search and retrieval - search process; h.1  information storage and retrieval : systems and software; general terms: algorithms  experimentation
keywords: web search  caching  pruning  query logs
1. introduction
　web search engines rely upon large complex systems to deliver query results to users. such systems are large because they use thousands of servers  interconnected through different networks  and often spanning multiple data centers. servers in these systems are often grouped according to some functionality  e.g.  front-end servers  back-end servers  brokers   and requiring that the groups of servers interact increases the amount of time and resources needed to process each query. it is therefore crucial for high-performance web search engines to design mechanisms that enable a reduction of the amount of time and computational resources involved in query processing to support high query rates and ensure low latencies.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  singapore.
copyright 1 acm 1-1-1/1 ...$1.
　typically  search engines use caching to store previously computed query results  or to speed up the access to posting lists of popular terms . results caching is an attractive technique because there are efficient implementations  it enables good hit rates  and it can process queries in constant time. moreover  as the query results are available after a query is processed by the index  having a results cache is simply a matter of adding more memory to the system to hold such results temporarily.
　one important issue with caches of query results is that their hit rates are bounded. due to the large fraction of infrequent and singleton queries  even very large caches cannot achieve hit rates beyond 1%  independent of the cache size. to overcome this limitation  a system can make use of posting list caching or/and employ a pruned version of the index  which is typically much smaller than the full index and therefore requires fewer resources to be implemented. without affecting the quality of query results  such a static pruned index is capable of processing a certain fraction of queries thus further decreasing the query rate that reaches the main index .
　in this paper  we describe an architecture  resin  that uses both a results cache and a static pruned index. we show that there are several benefits of using them together  including higher hit rates and reduced resource utilization. although index pruning has been studied in the literature  to our knowledge  we are the first to present results on the impact of results caching on index pruning in an architecture that is highly relevant for practical web search systems. in such an architecture  the stream of queries that has to be processed by the pruned index differs significantly from the the original query stream because many queries are filtered out by the results cache. this difference affects the performance of index pruning and the way to optimize it.
　in this paper we consider several index pruning techniques such as term pruning - a complete removal of posting lists of certain terms  document pruning - removing certain portions of the posting lists  and the combination of both.
　apart from introducing the resin architecture  we make the following contributions:
  we show that results caching has important advantages compared to index pruning. in particular  results caching guarantees high cache hit rates with a constant cache capacity independently of the document collection size;   we compare the properties of the original query stream and the query stream after a results cache  and show how the differences affect the applicability of index pruning;   we compare the efficiency of various static index pruning techniques when a pruned index is used separately or in combination with a results cache;
  we propose a different method of combining term and document pruning that outperforms the one of ntoulas and cho .
1. related work
　a number of solutions that aim at reducing query processing costs for web search engines have been proposed in the literature  including various caching and index pruning techniques.
　eviction policies that maximize the hit rate for a given cache capacity have been studied in a number of different domains  including search engine architectures. in the context of results caching  fagni et al.  introduce static dynamic cache  sdc . sdc achieves higher cache-hit rates than the baseline lru by devoting a fraction of the storage for a static cache containing frequent queries precomputed in advance. further improvements are considered in . however  when the cache capacity increases  the hit rate approaches its upper bound determined by the fraction of unique queries in the query log.
　the authors of  explore users' repeat search behavior and categorize different reformulations of similar queries in the query logs. baeza-yates et al.  investigate the impact of results caching and static caching of posting lists in the context of web search engine architecture. the impact of compression on caching efficiency is addressed in . finally  long and suel  introduce a 1-level caching architecture that includes on-disk caching of the posting lists for popular term combinations.
　index pruning reduces significantly the fraction of the index needed for query processing. with static pruning  the system generates a pruned index beforehand  whereas dynamic pruning proceeds on a per-query basis saving resources and reducing latency by dynamically skipping nonrelevant parts of the index.
　in the context of this paper  we call term pruning a complete removal of posting lists of certain terms  e.g.  stop words removal or the approach described in    whereas document pruning refers to ignoring only certain portions of the posting lists  e.g.   .
　in particular  some document pruning techniques were inspired by the algorithms of fagin et al.   known as threshold algorithms. the intuition behind these algorithms is that there is no need to scan complete inverted lists if only a top-k fraction of the intersection is requested. instead  the lists are sorted according to some score-dependent value and it is likely that the top-k query results can be found in the top-portions of the posting lists.
　in the context of text search engines this idea was first exploited by carmel et al. . they introduced static document pruning that is able to reduce the size of the index by up to 1% but at the price of a certain precision loss. the authors of  extend carmel's lossy pruning by taking into account co-occurrences of words that appear close to each other in documents. long and suel also employ a similar pruning technique .
　anh and moffat propose impact-ordered inverted lists and a lossy dynamic pruning scheme tailored for such an index organization . tsegay et al.  extend this approach by considering in-memory caching of pruned posting lists.
　alternatively  lossy index pruning based on removal of collection-specific stop words is discussed in . buttcher： and clarke  1  1  use a compact pruned index that can fit in the main memory of back-end servers. while in  they combine term and document pruning  the approach described in  advocates pruning the least important terms for each document individually. query processing with the full index maintained in main memory is discussed in .
　the approach of ntoulas and cho  is close to ours as it investigates static index pruning with the aim of reducing the amount of resources needed to handle a given queryrate without sacrificing the result quality  lossless pruning . it also employs term pruning  document pruning  and the combination of both. for a real query log and a large document collection the authors report relatively good hit rates  1%  achieved with the pruned index of 1% of the original index size.
　notice that contrary to our architecture  none of the index pruning techniques mentioned above use results caching while evaluating their performance.
1. resin architecture
　in a web search engine  users submit queries through a front-end server. upon receiving a new query  such a server forwards it to back-end servers for processing. each of the back-end servers maintains an index for a subset of the document collection and resolves the query against this subset. the index comprises posting lists for all terms in the sub-collection  where each posting list contains  document reference  term frequency  pairs. once the servers finish processing the query  they return results to the front-end server that displays them to the user. a broker machine is usually responsible for aggregating the results from a number of back-end servers  and returning these results to the front-end server.
　it is a natural design choice to place at least one other server in between the front-end and the broker to cache final top-k query results as the broker has to send them to the front-end server in any case.
　definition 1. results cache is a fixed-capacity temporary storage of previously computed top-k query results. it returns the stored top-k results if a query is a hit or reports a miss otherwise.
　an important advantage of implementing the results cache is decreasing the number of queries that hit the back-end servers  and thus reducing the number of servers needed to handle the query traffic.
　however  the hit rate of the results cache cannot increase beyond the limit imposed by singletons  which often constitute a large fraction of the query traffic. hence  we look into techniques that enable increasing the hit rate further.
　definition 1. pruned index is a smaller version of the main index  which is stored on a separate set of servers. such a static pruned index resolves a query and returns the response that is equivalent to what the full index would produce or reports a miss otherwise.
　we consider pruned index organizations that contain either shorter lists  document pruning   fewer lists  term pruning   or combine both techniques. thus  the pruned index is typically much smaller than the main index and requires fewer servers to maintain it.
　figure 1 shows the resin architecture where the results cache and the pruned index are placed between the frontend and the broker. in such an architecture  a query is forwarded to the main index only if both the results cache and the pruned index could not answer it  thus substantially reducing the load on the back-end serves.
　having a pruned index in a different network rather than in the one connecting the main index servers is not a good

figure 1: query processing scheme with the resin architecture.
design choice because ensuring the pruned index is up-todate requires transferring possibly large portions of the index. thus  we place the pruned index along with the backend servers holding the main index.
　a recent approach from ntoulas and cho  employs a similar architecture to reduce the query-rate at the back-end servers without sacrificing the result quality. the authors  however  do not include a results cache  which is a crucial element in our architecture  and employ only a pruned index for this purpose. note that in reality none of these architectures are really novel as architectures based on clusters for web search have been proposed before . the importance of the approach proposed by ntoulas and cho as well as ours comes from the evaluation of techniques such as caching and pruning.
　a typical experimental setup that is used in the literature to investigate the efficiency of an index pruning approach usually considers an original query log or a small set of trec queries  e.g.   1  1  1  1  1  1  1  . in section 1 we will show that some statistical properties of query logs change significantly when results caching is used.
1. experimental setup
　test queries. we used a large query log of the yahoo! search engine. the log contains more than 1m queries submitted at the .uk front-end of the search engine. we use this query-log to simulate query processing with a results cache and generate a  miss-log  of queries that are not filtered out by the results cache. henceforth  we use all queries to denote the queries from the original query log  and misses to denote the queries from the miss log. throughout the rest of the paper we will compare properties of both logs and use them to test various pruning techniques.
　document collection. to investigate the efficiency of index pruning techniques we also used the uk document collection  1  1 . it contains 1m web-pages crawled from the .uk domain in may 1. we used the terrier platform1 to index the collection on 1 commodity machines  which resulted in a distributed compressed index of approximately 1gb without positional information.
1. results caching
　results cache is a temporary storage for previously computed partial query results. we assume here dynamic caching: when the system processes the query response containing top-k results and returns it to the user  it stores these results in the cache  such that  for future requests of the same query it returns these results instead of asking the back-end servers to process the same query again. results

1 http://ir.dcs.gla.ac.uk/terrier cache uses an eviction policy to ensure that it never exceeds a maximum capacity. a query produces a hit if it was found in the cache and a miss otherwise.
　in this section we analyze the performance of results caching using a real query log and study the differences between the original query stream and the stream of misses after the results cache.
1 results cache performance
　to evaluate the performance of the results cache we implemented the lru policy and tested it with our query log. we varied the cache capacity from 1m to 1m entries  where each entry contains a query and its top-k results.
　figure 1 shows cache-hit rates obtained with different cache capacities for our query log. for each point we warm up the cache with the first 1m queries and then measure the average cache hit for the remaining 1m queries.

figure 1: cache hit achieved with a large results cache using the lru eviction policy.
　we also normalized each query by converting it to lower case  removing special symbols and sorting the terms in each query in alphabetical order. despite the simplicity of this normalization1  it performs similarly to normalization procedures in current search engines. figure 1 shows that the normalization increases the cache hit by roughly 1   1%. this happens because semantically similar  but lexicographically different queries have the same normalized versions  e.g.  queries written in a different case  containing symbols ignored by search engines  etc.
　due to the presence of singleton queries the performance of any results cache is bounded by the fraction of potential hits hitsmax =  1   distinctall queriesqueries . for the normalized version of our query log this value is equal to 1%.
　according to our experiments  sophisticated caching policies such as lfu  sdc  or ac  substantially outperform lru when the cache capacity is small  due to the optimized space usage. however  the improvement is marginal when the cache capacity is big enough such that the hit rate approaches its upper bound. in such a case  lru becomes the best option due to its simplicity.
　let us assume that one entry in the results cache requires around 1 kb in order to store top-1 results including document urls  titles and snippets. this number can be further reduced by using compression. in this case one machine with 1gb of memory can host a results cache with the capacity of approximately 1m queries. thus  we simulated a 1m results cache with the lru eviction policy and used

1
 e.g.  it ignores phrase queries and treats urls as sets of terms.
it to process all 1m normalized queries. we warm up the cache with the first 1m queries  and then record all query misses for the remaining 1m queries. as a result  we obtained a miss log containing 1m  mostly singleton  queries. notice that the miss log contains queries that have to be processed by the back-end servers  and that query processing has to be optimized for such queries.
　a very important property of the results cache is that its hit rate remains constant as the size of the document collection grows because it depends only on the query log properties  in particular on the number of unique queries. this property makes results caching a very efficient technique to reduce the query load on the back-end servers  as it requires a relatively small amount of storage and processing. the size of a pruned index  however  typically grows with the size of the collection as we discuss further in section 1.
1 all queries vs. misses
　first  we confirm that the average number of terms in a query increases from 1 for all queries to 1 for misses. fractions of queries with different number of terms are shown in figure 1. in particular  it shows that the majority of single term queries become hits in the results cache and thus very few of them have to be processed by the index.

figure 1: fraction of queries with a given number of terms among all queries and misses.
　another crucial difference between all queries and misses is the distribution of the sizes of query results. we use the yahoo! search engine to collect the  estimated  results set sizes for 1 randomly chosen queries in each set. figure 1 shows that the result set sizes for query misses are approximately two orders of magnitude smaller than for all queries. in particular  almost half of the misses return less than 1 results  which is extremely small compared to the total number of documents on the web indexed by the yahoo! search engine. figure 1 also shows similar results for the  much smaller  uk document collection of 1m documents described in section 1.
　we say that a query is discriminative if it returns relatively few results. usually it happens because the query contains at least one rare term or the number of terms in the query is large. both cases suggest that such a query is unlikely to be popular in the query log and therefore results caching performs poorly for discriminative queries. furthermore  query misses contain a considerable fraction of misspells as they are typically not filtered out by the results cache. in our test set of misses  we detected about 1% of misspelled queries.
　we characterize each query term with two properties: popularity and frequency. a term is considered popular if it is likely to appear often in queries. that is  term popularity is proportional to the number of occurrences of the term in

 
figure 1: query result size distributions for: 1  the yahoo! web search engine  and 1  the uk document collection  1m documents .
the query log. a term is considered frequent if it is likely to appear in documents. frequency  or document frequency  is hence proportional to the number of occurrences of the term in the document collection.
　figure 1 shows that terms that are popular in the original query log remain popular in the miss log as well. we extracted all terms from the original query log and sorted them by popularity. for each term we also computed its popularity in the miss log  1 if the term does not appear there . each point on the plot shows the average popularity of 1 consecutive terms normalized by the size of the query log  1m for the original query log and 1m for the miss log . notice that averaging over 1 consecutive terms generates a smoother curve for misses while having one point per term would compromise visualization.

	1	1	1k
terms taken from all queries  each point for 1 terms  
figure 1: term popularity distribution for all queries and for misses.
　from the figure we conclude that popular terms remain popular in the miss log as well. unpopular terms typically come from queries that are not hits in the results cache  so their absolute popularity is similar in both logs  but the normalized popularity is higher for the miss log because the miss log size is smaller.
　this result indicates that  despite significant changes to the query stream after the results cache  techniques that are based on term popularity such as term caching and term pruning work similarly for all queries and for misses. we investigate this observation in detail in the following sections.
1. index pruning
　in this section we consider term and document pruning techniques as well as the combination of both in the context of the architecture of figure 1.
　to perform index pruning experiments we used the original query log containing 1m normalized queries and the miss log containing 1m normalized queries. since it is computationally expensive to run experiments with such workloads  we randomly selected a test set of 1 queries from the last 1m queries in the original query log. similarly  we used the last 1m queries in the miss log to generate a test set of 1 misses. we will refer to these test sets as all queries and misses in the following sections. the remaining portions of the logs  first 1m queries and 1m misses  were used to compute term popularities.
1 term pruning
　term pruning selects the most profitable terms and includes their full posting lists into the pruned index. thus  in order to verify whether a query can be processed by such a pruned index  we only need to check that all terms from the query are included in the pruned index. notice that such a pruning technique assumes that each document in the result set of a query has to contain all terms from the query  conjunctive query processing .
　we implemented a term pruning algorithm that estimates a term profit as:   where pop t  is the popularity of the term t and df t  is its document frequency1. hence  the profit of a term is proportional to its popularity and inversely proportional to the space required to store its posting list. we extracted the list of all terms found in the original query log and computed their profits.
　the optimization problem of selecting the terms that maximize the hit rate for a given pruned index size constraint is known to be np-complete  as it can be reduced to the well known knapsack problem. thus  we follow the standard heuristics that have been shown to perform well in this scenario  1  1 . we sort the list of terms by profits in descending order and pick the top terms as long as the sum of the posting list sizes for selected terms remains below the given size constraint. figure 1 shows the performance of such a pruning strategy for all queries and for misses.

figure 1: hit rate with the term pruned index.
　the hit rate for all queries grows quickly in the beginning because of single term queries and queries that contain few popular terms  typical hits in the results cache . in case of misses though  the growth is nearly linear in the beginning  still being able to handle nearly half of the queries with only a quarter of the index. thus  even with misses  we can reduce hardware costs by considering a pruned version of the index. for example  suppose that one full cluster processes

1
 in fact  the denominator df t  in the profit formula implies a non-zero storage cost for t's posting list.
half of the query load  for some design of index cluster  thus requiring two clusters to process all queries. by using a pruned index  we can have instead one full cluster and one other cluster for the pruned index that is one-fourth of the original cluster.
　figure 1-a compares the efficiency of the results cache and the term pruned index when they are used separately. it shows that with a fixed amount of storage available  the hit rate of the pruned index decreases linearly with the growth of the document collection size  a collection of size x corresponds to the set of 1m documents from the .uk domain described in section 1 . the storage required for the results cache  however  depends only on the query log properties and thus remains constant. therefore  for a web-scale document collection  the relative results cache size would amount to a small fraction of the full index. results caching hence is the preferable solution when the amount of available storage is limited and the document collection is large.

figure 1: comparison of results caching and term pruning used separately  a   cumulative hit rate with both techniques used together  b .
　however  the hit rate of the results cache is bounded and to avoid this limitation we study the combination of results caching and index pruning. figure 1-b shows the cumulative hit rate obtained by the results cache and the pruned index together following the architecture of figure 1. the top dotted line in figure 1-b shows the maximum cumulative hit rate in such a scenario assuming a sufficiently large index size such that the results cache storage cost can be neglected. we also measured the cumulative hit rate with the real  and relatively small  uk document collection. in this case the results cache size was limited to maximum 1% of the full index size  approx. 1 gb . the dashed portion of the bottom line in figure 1-b until 1% of the index size corresponds to the hit rate produced by the results cache and the rest is obtained by the pruned index additionally to the results cache.
　figure 1 proves the importance of results caching in our architecture and shows that the combination of results caching and index pruning delivers very good hit rates.
　recall that term popularities in the original query log and in the miss log are very similar  see figure 1 . hence  the term pruned index has to include a significant number of popular terms associated with large posting lists. indeed  with the next experiment we confirm that a large fraction of misses contains at least one frequent term  although misses typically return few results.
　in figure 1 we use the test set of misses and the uk document collection to plot the correlation between the query result set size and the document frequency of the most and the least frequent term in a query. the former one is denoted as maxdf  top plot   while the latter one is mindf  bottom plot . on both plots  we sort queries by the size of their result set in increasing order  and the dashed line shows the size in log scale. the remaining lines reflect the probability of maxdf  mindf  being above one of the thresholds: 1  1k  1k and 1m elements. each point is computed for 1 consecutive queries with about the same result set size  which can be estimated from the dashed line.

 
figure 1: fraction of misses with df of the most frequent  top  and the least frequent  bottom  term in a query above a given threshold.
　figure 1 suggests that mindf correlates with the result set size of the query  bottom plot   whereas maxdf is constantly high for most of the misses  top plot . hence  the term pruned index has to include large posting lists in order to guarantee a reasonable hit rate.
1 document pruning
　including full  and often large  posting lists in the pruned index might seem redundant  so we studied the document pruning option. document pruning removes the least important entries from the posting lists. it is based on the observation that if posting lists are sorted such that more relevant documents are stored first  the top-k results for a query are likely to be computed without traversing the whole lists  but by examining the top portions of them only  1  1 . thus  posting lists are usually sorted by an attribute that reflects the probability of the document to be included in the top results  such as score  term frequency or impact .
　when static document pruning is used  only top-portions of posting lists are included in the pruned version of the index and are used for query processing. a query can be answered from such a pruned index only if it produces exactly the same top-k results as the full index would. it is in general difficult to test this condition  and the only acceptable solution we are aware of is starting processing the query using the pruned index - if the top-k correct results are identified  then the query is a hit. otherwise  the query is a miss and it has to be forwarded to and processed again by the main index  thereby affecting latency. to determine the correctness of the results  we compute a maximum score threshold for all potentially relevant documents whose scores cannot be computed exactly due to truncation. then  we verify whether all top-k results have scores above the threshold  1  1 .
　document pruning depends on the ranking function as it significantly influences how quickly the top-k results for a query can be found while examining the top portions of the posting lists. following craswell et al.   we used the following weighting formula:
 
where bm1 t d  is the non-normalized bm1 score of the document d for a term t and pr d  is the query independent score of the document d  pagerank computed from the graph of the uk collection  . we do not have relevance judgments for our document collection to estimate the values of k and ω  so we fix k = 1 and vary the value of ω to study the impact of the pagerank weight on the document pruning performance. intuitively  the higher the weight ω is  the better document pruning performs. in the following experiments we set ω to 1  no pagerank   1 and 1.
　notice that the way the query independent rank is included in the final score affects the index construction algorithm. our formula guarantees that the top-k results obtained from the pruned index and having scores above the maximum score threshold are exactly the same as if they were computed from the full index. the ranking function presented in  requires a slightly modified index construction so that this condition holds.

fraction of all queries top  / misses bottom  sorted by result set size 
 
figure 1: fraction of all queries  top  and misses  bottom  that can be resolved from posting lists truncated to top pllmax = 1k entries.
　figure 1 shows the fraction of queries whose correct top1 results are found in all pruned posting lists relevant to the query. the maximum posting list length  pllmax  specifies the pruning threshold and is set to 1k entries. from the figure  we can see that document pruning works better with non-discriminative queries  i.e.  queries with a large result set size . furthermore  the fraction of queries that require no more than top-1k entries in the posting lists grows with the increase of the pagerank weight ω.
　the same plot shows the fraction of queries with the least popular term having more than 1 occurrences in the query log. we could see that non-discriminative queries contain popular terms only  while unpopular terms appear in queries that return few results. this observation indicates that popular terms tend to be frequent as well.
figure 1 suggests that document pruning performs much better for all queries than for misses. indeed  figure 1 shows that while working well for all queries  document pruning requires very high values of ω to outperform term pruning with misses. each point of figure 1 is obtained by computing the hit rate1 of the document pruned index with a number of different pllmax values and selecting the one that delivers the maximal hit rate. the dashed lines correspond to term pruning for comparison.

 	 
 
figure 1: document pruning for all queries  left  and misses  right  compared with term pruning.
　in fact  the efficiency of document pruning correlates to the size of the query result. when the query result is small  it is likely that we have to scan large parts of the posting lists  or even whole lists  to obtain enough documents. it can be very inefficient because frequent terms with long posting lists are common in misses  see figure 1 . thus  the worse performance of document pruning with misses can be explained by the fact that the result set sizes are approximately two orders of magnitude smaller compared to all queries  see figure 1 .
1 term+document pruning
　following ntoulas and cho  we study the combination of term and document pruning in our scenario. the intuition behind combining the two is the following: while including profitable terms  only at most top-pllmax entries from each posting list should be copied to the pruned index. if the pllmax constant is chosen such that the hit rate is maximal for a given pruned index size  term+document pruning would deliver at least as good hit rate as term pruning. indeed  in the case when pllmax is set to ± the hit rate with such a pruned index will be exactly the same as for term pruning.
　to select terms for such a pruned index we adopt the same profit function as for term pruning profit because it was used in . however  taking into account the observations made in the previous section  we introduce a new profit function profit specifically tailored to term+document pruning. the rationale behind it is that the storage cost of including a term t in the pruned index is proportional to df t  when df t    pllmax  but depends only on the pllmax parameter if df t  − pllmax.
1
 in fact  we compute an upper bound for the hit rate - the fraction of queries for which the correct top-1 results can be found in each pruned posting list relevant to the query.
　figure 1 shows the hit rate1 of term+document pruning with the two profit functions defined above for all queries and for misses. each point is obtained by computing the performance of the term+document pruned index with a number of different pllmax values and selecting the one that delivers the maximal hit rate. notice that when the maximum hit rate is achieved with pllmax = ±  the corresponding point coincides with the dashed line  which means that there is no improvement over term pruning.
　figure 1 suggests that the new profit1 function  left plot  substantially outperforms profit1  right plot   especially with high pagerank weights. for example  the table below shows the hit rate values obtained with the pruned index 1 times smaller than the full index for both profit functions and different values of ω:
 term pruningterm+document pruningprofit1profit1onlyω=1ω=1 ω=1ω=1ω=1 ω=1all queries111	111	1misses111	111	1　figure 1 also shows that for all queries term+document pruning brings substantial benefits compared to term pruning only. however  with misses it yields only a small increase of hit rate when the pagerank weight is very high. for example  the table above shows that the hit rates of term+document pruning with misses noticeably outperform the baseline 1% of term pruning with ω = 1 only. furthermore  term+document pruning induces high processing costs and latencies compared to term pruning  therefore becoming rather unusable with misses.

 	 
 
figure 1: term+document pruning for all queries and for misses with both profit functions.
　to perform the last experiment  we fixed the size of the pruned index to 1% of the full index  i.e.  the size of the pruned index grows linearly with the size of the document collection  and the pagerank weight ω to 1  the maximum value in our experiments . we measure the hit rate of the pruned index for fractions of the document collection indexed on 1 and 1 machines respectively  as well as for the full index on 1 machines.
figure 1 shows that the hit rate is approximately the
same for the term pruned index when its size is 1% of the full index size  dashed lines  because the length of the posting lists increases linearly with the collection size.
allqueries  profit1= pop/min df pllmax 
allqueries  profit1=pop/df
allqueries  term pruning only
misses  profit1= pop/min df pllmax 
misses      profit1=pop/df
misses  term     pruning onlypruned index size is     
1% of the full index
size of the index  number of machines  
figure 1: index pruning efficiency for different sizes of the document collection.
　the hit rate for term+document pruning grows with the size of the document collection. this happens because document pruning works better with non-discriminative queries  which match a large number of documents. however  in practice  if such a pruned index had to be distributed among several servers following the standard document partitioning scheme  each server would have to compute the top-k results for each query from its  small  fraction of the index. thus  the resulting hit rate suffers because each server matches a small set of documents in its local index.
1 discussion
　the original stream of queries contains a large fraction of non-discriminative queries that usually consist of few frequent terms  e.g.  navigational queries . since frequent terms tend to be popular  those queries are likely to repeat in the query log and therefore are typical hits in the results cache. at the same time  these queries would be good candidates to be processed with the document pruned index due to their large result set size. therefore  document pruning does not perform well anymore when the results cache is included in the architecture. the same conclusion can be drawn from the fact that misses return much fewer results than original queries on average.
　however  individual term popularites are similar for all queries and for misses. therefore the contents of the term pruned index does not change much. a larger fraction of short queries explain higher hit rates with the term pruned index for all queries than for misses.
1. conclusion
　in this paper we have presented the resin architecture for web search engines that combines results caching and index pruning to reduce the query workload of back-end servers. with resin  we showed that such a combination is more effective than previously proposed approaches  for example being capable of handling up to 1% of queries with four times less resources than the full index requires.
　results caching is an effective way to reduce the number of queries processed by the back-end servers  because its performance is bounded by the amount of singleton queries in the query stream  and not by the size of the document collection. we observed that the results cache significantly changes the characteristics of the query stream: the queries that are misses in the results cache match approximately two orders of magnitude fewer documents on average than the original queries. however  the results cache has little effect on the distribution of query terms.
　these observations have important implications for implementations of index pruning: the results cache only slightly affects the performance of term pruning  while document pruning becomes less effective  because it targets the same queries that are already handled by the results cache.
　when combining term and document pruning  we substantially increased the hit rates by considering a better term profit function. we have also found that document pruning is more effective when query independent scores  such as pagerank  have high weights in the final scoring formula.
　overall  resin has allowed us to gain significant insight into the dependence between different techniques for reducing the load on the back-end servers in a web search engine  and it has allowed us to improve existing techniques by testing them in a more realistic setting.
