hummingbird submitted ranked result sets for the topic relevance task of the trec 1 web track  1gb of web data  and for the monolingual arabic task of the trec 1 cross-language track  1mb of arabic news data . searchserver's intuitive searching tm  tied or exceeded the median precision 1 score in 1 of the 1 web queries. for the web queries  enabling searchserver's document length normalization increased precision 1 by 1% and average precision by 1%.  searchserver's option to square the importance of inverse document frequency  v1 vs. v1  increased precision 1 by 1% and average precision by 1%.  searchserver's stemming increased precision 1 by 1% and average precision by 1%.  for the arabic queries  a combination of experimental arabic morphological normalizations  arabic stop words and pseudo-relevance feedback increased average precision by 1% and precision 1 by 1%.
1 introduction
hummingbird searchserver1 is an indexing  search and retrieval engine for embedding in windows and unix information applications.   searchserver  originally a product of fulcrum technologies  was acquired by hummingbird in 1.  founded in 1 in ottawa  canada  fulcrum produced the first commercial application program interface  api  for writing information retrieval applications  fulcrum  ful/text tm .  the searchserver kernel is embedded in many hummingbird products  including searchserver  an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.
searchserver supports a variation of the structured query language  sql   called searchsql tm   which has extensions for text retrieval.  searchserver conforms to subsets of the open database connectivity  odbc  interface for c programming language applications and the java database connectivity  jdbc  interface for java applications.   almost 1 document formats are supported  such as word  wordperfect  excel  powerpoint  pdf and html.  many character sets and languages are supported.  searchserver's intuitive searching algorithms were updated for version 1 which shipped in fall 1  and in subsequent releases of other products.  searchserver 1  which shipped in spring 1  works in unicode internally  and contains improved natural language processing technology  particularly for languages with many compound words  such as german  dutch and finnish.
1 system description
all experiments were conducted on a single-cpu desktop system  otwebtrec  with a 1mhz pentium iii cpu  1mb ram  1gb of external disk space on one e: partition  and running windows nt 1 service pack 1.  an internal development build of searchserver 1 was used for the official trec runs in july 1  build
1.1   which for the web and arabic tasks should give essentially the same rankings as the commercial release version.
1 setup
we describe how searchserver was used to handle the topic relevance task of the trec 1 web track  1gb of web data  and the monolingual arabic task of the trec 1 cross-language track  1mb of arabic news data . 1 data
the wt1g collection of the web track consists of pages downloaded from the world wide web in 1.  it was distributed on 1 cds.  we copied the contents of each cd onto the otwebtrec e: drive  e: data wt1g cd1 e: data wt1g cd1 .  the cd1 info subdirectory  containing supporting information not considered part of wt1g  was removed to ensure it wasn't indexed.  the 1 .gz files comprising wt1g were uncompressed.  no further pre-processing was done on the data.  uncompressed  the 1 files consist of 1 1 bytes  1gb   about 1mb each.  each file contains on average 1  documents   for a total of 1 1 documents.  the average document size is 1 bytes.  for more information on this collection  see .
arabic newswire a corpus of the cross-language track consists of articles from the agence france presse  afp  arabic newswire from 1.  it was distributed on 1 cd.  we copied the contents of its transcripts directory to e: data arabic.  the 1 .gz files comprising the corpus were uncompressed.  no further preprocessing was done on the data.  uncompressed the 1 files consist of  1 1 bytes  1 mb   about 1kb each.  each file contains on average 1  documents   for a total of 1 documents.  the average document size is 1 bytes.  for more information on this collection  see .
1 text reader
to index and retrieve data  searchserver requires the data to be in fulcrum technologies document format  ftdf .  searchserver includes  text readers  for converting most popular formats  e.g. word  wordperfect  etc.  to ftdf.   a special class of text readers   expansion  text readers  can insert a row into a searchserver table for each logical document inside a container  such as directory or library file.  users can also write their own text readers in c for expanding proprietary container formats and converting proprietary data formats to ftdf.
last year  for trec-1  we wrote a custom text reader called ctrec to handle expansion of the library files of wt1g collection and to make a few conversions to the html format  described in .  we used ctrec again this year and made no significant changes regarding wt1g.  this year  we will just describe how ctrec was extended for the arabic collection.
the library files of the arabic collection  like wt1g  consist of several logical documents  each starting with a  doc  tag and ending with a  /doc  tag.  after the  doc  tag  the unique id of the document  e.g. 1 afp arb.1  is included inside  docno .. /docno  tags.  the ctrec /e switch handles expansion of the arabic library files into logical documents identically as for wt1g.
the arabic documents contain sgml tags describing its structure  e.g. the headline is preceded by a  headline  tag and followed by a  /headline  tag .  the document type definition  dtd  which specified the tags and entities used in the documents was provided in the ldc arabic.dtd file on the cd.  when invoked without the /w or /e switch  ctrec by default inserts control sequences to turn off indexing around all tags listed in the arabic collection dtd  and additionally tags listed in the trec disk 1 dtds  as described last year   and converts all entities listed in the dtds  e.g.  &amp;  is converted to the ampersand  &  .  by default  ctrec also turns off indexing for data delineated by certain tags because its content isn't considered helpful  for the arabic collection  data delineated by header  footer and trailer tags is not indexed .  ctrec looks ahead at most 1 bytes for an end tag when it encounters a tag indicating indexing should be turned off; if the end tag is not found  indexing is not turned off.
the arabic documents are in the utf-1 character set  a variable-width encoding of unicode  for which ascii characters are represented with 1 byte  e.g. the latin letter a  which is hexadecimal value 1 in the utf-1 encoding of unicode  is 1 byte in utf-1  hexadecimal 1 or decimal 1    and non-ascii characters are represented with 1 to 1 bytes  e.g. the arabic letter alef  which is 1 in utf-1  is 1 bytes in utf-1  1xd1xa1  .  ctrec passes through the bytes of the documents unchanged  aside from the control sequences inserted and entities converted as described previously .  searchserver's translation text reader  nti   was chained on top of ctrec and the utf1 ucs1 translation was specified via its /t option to translate from utf-1 to the utf-1 encoding desired by searchserver's parser.
1 indexing
we created a searchserver table called wt1gw for the web collection and two different searchserver tables called arab1 and arab1a for the arabic collection.  for example  the searchsql statement to create the arab1a table was as follows:
create schema arab1a create table arab1a
 docno varchar 1  1  periodic
basepath 'e: data' stopfile 'myarab.stp';
the stopfile differed for each table.  for wt1gw  we used the same mytrec.stp stopfile as last year  which contained 1 stopwords to not index  including all letters and single-digit numbers.  for arab1  we did not use a stopfile.  for arab1a  the stopfile myarab.stp did not actually contain any stopwords  but specified a nondefault option to the parser to apply experimental arabic morphological normalizations to the words before indexing:
parser= unicode/a=1 
the parser line of the stopfile specified the built-in unicode parser with the non-default option of a=1 which enables the experimental arabic morphological normalizations.  a powerful new feature of searchserver 1 is the ability to  plug-in  a custom parser to extend or replace the default parser.
into each table  we just inserted one row  specifying the top directory of the data set.  e.g. for the arab1a table  we used this insert statement:
insert into arab1a   ft sfname  ft flist  
values   'arabic'  'ctrec/e/d=1:s!nti/t=utf1 ucs1:ctrec/ :s' ;
to index each table  we just executed a validate index statement such as the following:
validate index arab1a validate table
temp file size 1 buffer 1;
the validate table option of the validate index statement causes searchserver to review whether the contents of container rows  such as directory rows and library files  are correctly reflected in the table.  in this particular case  searchserver initially validated the directory row by inserting each of its sub-directories and files into the table.  then searchserver validated each of those directory and library file rows in turn  etc.  validating library file rows invoked the ctrec text reader in expansion mode to insert a row for each logical document in the library file  including its document id.
after validating the table  searchserver indexed the table  in this case using up to 1mb of memory for sorting  as per the buffer parameter  and using temporary sort files of up to 1gb  as per the temp file size parameter . the index includes a dictionary of the distinct words  after some unicode-based normalizations  such as converting to upper-case and decomposed form  and in the case of the arab1a table  arabic-specific normalizations as previously described  and a reference file with the locations of the word occurrences.  additionally  by default  each distinct word is stemmed and enough information saved so that searchserver can efficiently find all occurrences of any word which has a particular stem.  by default  the stemming is done with an english lexicon which has no effect on arabic words.
1 search techniques
for the topic relevance task of the web track  the 1  topics  were in a file called  topics.1 .  the topics were numbered from 1  and each contained a title  which was an actual web query taken from a search engine log   a description  nist's interpretation of the query   and a narrative  a more detailed set of guidelines for what a relevant document should or should not contain .  we assumed the topics were in the latin-1 character set  the default on north american windows systems.
for the cross-language track  the 1 topics were in 1 different languages  english  french and arabic .  we just used the arabic topics in a file called  arabic topics.txt .  the arabic topics were numbered ar1 to ar1.  they were encoded in the iso 1  latin-1  character set.
we created an odbc application  called querytorankings.c  based on the example stsample.c program included with searchserver  to parse the topics files  construct and execute corresponding searchsql queries  fetch the top 1 rows  and write out the rows in the results format requested by nist.  select statements were issued with the sqlexecdirect api call.  fetches were done with sqlfetch  typically 1 sqlfetch calls per query .
1 character set
searchserver easily handled the issue of the arabic queries and documents being in different character sets.  before running the arabic queries  the searchsql statement  set character set 'iso 1'  was executed so that searchserver would transcode the queries from latin-1 to unicode.  the web queries were assumed to be in the latin-1 character set  the default.
1 intuitive searching
for all runs  we used searchserver's intuitive searching  i.e. the is about predicate of searchsql  which accepts unstructured text.  for example  for topic 1 of the web track  the title was  dodge recalls  .  a corresponding searchsql query would be:
select relevance 'v1'  as rel  docno
from wt1gw
where ft text is about 'dodge recalls '
order by rel desc;
this query would create a working table with the 1 columns named in the select clause  a rel column containing the relevance value of the row for the query  and a docno column containing the document's identifier. the order by clause specifies that the most relevant rows should be listed first.   the statement  set max search rows 1  was previously executed so that the working table would contain at most 1 rows.
1 stemming
searchserver  stems  each distinct word to one or more base forms  called stems  using lexicon-based natural language processing technology.  for example  in english   baby    babied    babies    baby's  and  babying  all have  baby  as a stem.  compound words in languages such as german  dutch and finnish produce multiple stems; e.g.  in german   babykost  has  baby  and  kost  as stems.
by default  intuitive searching stems each word in the query  counts the number of occurrences of each stem  and creates a vector.  optionally some stems are discarded  secondary term selection  if they have a high document frequency or to enforce a maximum number of stems  but we didn't discard any stems for our trec runs this year. the index is searched for documents containing terms which stem to any of the stems of the vector.
the vector generator set option controls which stemming operations are performed by intuitive searching. for most web track runs  we used the default vector generator setting  'word!ftelp/base/single | * | word!ftelp/inflect'  which assumes the english language  but for one submitted run we disabled stemming  using set vector generator '' .   by default  searchserver's index supports both exact matching  after some unicode-based normalizations  such as converting to upper-case and decomposed form  and matching on stems.  for the arabic runs  we always disabled stemming.  when searching searchserver table arab1a  for which arabic morphological normalizations were applied to each word at index-time  the same normalizations were automatically applied to each query term.
besides linguistic expansion from stemming  we did not do any other kinds of query expansion this year.  for example  we did not use approximate text searching for spell-correction because the queries were known to be spelled correctly this year.  we did not use row expansion or any other kind of blind feedback technique for the official runs.
1 statistical relevance ranking
searchserver calculates a relevance value for a row of a table with respect to a vector of stems based on several statistics.  the inverse document frequency of the stem is estimated from information in the dictionary.  the term frequency  number of occurrences of the stem in the row  including any term that stems to it   is determined from the reference file.  the length of the row  based on the number of indexed characters in all columns of the row  which is typically dominated by the external document   is optionally incorporated.  the already-mentioned count of the stem in the vector is also used.  to synthesize this information into a relevance value  searchserver dampens the term frequency and adjusts for document length in a manner similar to okapi  and dampens the inverse document frequency in a manner similar to .  searchserver's relevance values are always an integer in the range 1 to 1.
searchserver's relevance method setting can be used to optionally square the importance of the inverse document frequency  by choosing a relevance method of 'v1' instead of 'v1' . searchserver's relevance dlen imp parameter controls the importance of document length  scale of 1 to 1  to the ranking.
1 query stop words
our querytorankings program removed words such as  find    relevant  and   document  from the topics before presenting them to searchserver  i.e. words which are not stop words in general but were commonly used in the topics as general instructions.  for our clef runs this year   we expanded the list for several languages based on examining the clef 1 topics  not this year's trec topics .  the full list for english is now as follows:  item    items    find    documents    document    relevant    report    what    identify    about    discussing .   some of these words  such as  about   were also in the mytrec.stp stopfile  so removing them was redundant.   although they were unlikely to appear  corresponding words for other languages  e.g. the german word  dokumente   were removed if encountered.  no arabic words were in the list.  this step was found to be only minor benefit for clef .
if a query returned no results  based on our experience with the trec-1 large web task last year  the reason was often that the queries consisted entirely of stop words.  the most famous stopword query   to be or not to be   is a philosophical question  so for the web track this year we pre-selected document wtx1-b1  the yahoo philosophy page  to be returned if otherwise the query would return no results.   it turned out the situation came up this year for topic 1  who and whom   which was judged to be a grammar question  and hence the philosophy page was properly judged non-relevant.   as a more general technique  we may just return the results of the query  philosophy  for this situation in future years.
1 results
the evaluation measures are explained in an appendix of the conference proceedings.  briefly: precision is the percentage of retrieved documents which are relevant.  precision n is the precision after n documents have been retrieved.  average precision for a topic is the average of the precision after each relevant document is retrieved  using zero as the precision for relevant documents which are not retrieved .  recall is the percentage of relevant documents which have been retrieved.  interpolated precision at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level.  for a set of topics  the measure is the average of the measure for each topic  i.e. all topics are weighted equally .
we use the following abbreviations for the evaluation measures in this paper:
avgp: average precision
p 1  p 1  p 1  p 1  p 1: precision after 1  1  1  1 and 1 documents retrieved  respectively
rec1  rec1: interpolated precision at 1% and 1% recall  respectively
avgh: average precision just counting highly relevants as relevant
h 1  h 1  h 1  h 1  h 1: p 1  p 1  p 1  p 1 and p 1 just counting highly relevants as relevant  respectively
h1  h1: rec1 and rec1 just counting highly relevants as relevant  respectively
we refer to the scores with a fixed cutoff  p 1  p 1  p 1  p 1  p 1  as early precision scores.  the other scores  avgp  rec1  rec1   which can be influenced by results later in the result list  we call recall-oriented scores.
1 web track
the topic relevance task of the web track was to run 1 web queries against 1gb of web data and submit a list of the top-1 ranked documents to nist for judging.
nist produced a  qrels  file: a list of documents judged to be highly relevant  relevant or not relevant for each topic.  from these  the scores were calculated with chris buckley's trec eval program  which counts all relevants the same  including highly relevants.  to produce scores which just counted highly relevants as relevant  we ran trec eval a 1nd time on a modified version of the qrels file which had the ordinary relevants filtered out  then multiplied by 1  in 1 of the 1 topics  there were no highly relevants .  hence the scores focused on highly relevants are averaged over just 1 topics.
we submitted 1 runs for the topic relevance task of the web track: hum1t  hum1tl  hum1tlx and hum1tdlx. the run codes we used are as follows:
hum: hummingbird
1: trec 1
t: title field used d: description field used n: narrative field used
l: linguistic expansion  stemming  enabled x: weighting scheme squared importance of inverse document frequency and increased importance of document length normalization  i.e. relevance method 'v1'  relevance dlen imp 1  instead of 'v1' and 1 respectively 
tables 1 and 1 show various scores of our submitted title-only runs  i.e. runs which just used the original web query.  additionally  for some measures  nist reported the median scores of the 1 submitted title-only runs from all groups for each of the 1 topics; we show the average of the median scores:
runavgpp 1p 1p 1p 1p 1rec1rec1a: hum1tlx11%1%1%1%1%111b: hum1tl11%1%1%1%1%111c: hum1t11%1%1%1%1%11median  1 runs 1n/a1%n/a1%1%n/an/atable 1: precision of submitted title-only runs counting all relevants the same
runavghh 1h 1h 1h 1h 1h1h1a: hum1tlx11%1%1%1%1%111b: hum1tl11%1%1%1%1%111c: hum1t11%1%1%1%1%11table 1: precision of submitted title-only runs just counting highly relevants as relevant
impact of stemming  compare hum1t to hum1tl : when counting just highly relevants as relevant  the early precision scores  p 1  p 1  p 1  p 1  p 1  were higher with stemming disabled  and the recall-oriented scores  avgp  rec1  rec1  were higher with stemming enabled.  this result fits intuition: stemming ought to increase recall because more word variants are allowed to match  but sometimes the variants may not reflect the query as accurately  hurting precision.  when counting all relevants the same  the earliest precision score  p 1  was again higher with stemming disabled; the other early precision scores were modestly higher with linguistic expansion enabled  though by a smaller margin than for the recall-oriented scores.  the difference from the result for highly relevants may be from precision suffering less when the quality of the match is not required to be as high. none of these differences were statistically significant at the 1% level by the two-sided wilcoxon signed rank test .
if stemming helps recall but hurts early precision  it may be better to give a higher weight to the original query word than the generated variants.  we haven't yet run any experiments with this approach.
note that all topics were english.  in our clef 1 experiments this year  we found that the impact of
searchserver's stemming was normally larger in other european languages  particularly in german and dutch.  there are two differences in the weighting scheme between the submitted hum1tl and hum1tlx runs.  to isolate the impact of each change  tables 1 and 1 show diagnostic runs whose settings are the same as for hum1tlx except for the document length importance setting  relevance dlen imp .  rows 1d and 1d are the same as rows 1a and 1b  respectively  the hum1tlx run .  rows 1c and 1c differ from hum1tl in just the relevance method  v1 vs. v1 :
dlen importanceavgpp 1p 1p 1p 1p 1rec1rec1a: 1.1.1%1%1%1%1%111b: 1.1.1%1%1%1%1%111c: 1.1.1%1%1%1%1%111d: 1.1.1%1%1%1%1%111e: 1.1.1%1%1%1%1%11table 1: impact of document length normalization  title-only runs 
dlen importanceavghh 1h 1h 1h 1h 1h1h1a: 1.1.1%1%1%1%1%111b: 1.1.1%1%1%1%1%111c: 1.1.1%1%1%1%1%111d: 1.1.1%1%1%1%1%111e: 1.1.1%1%1%1%1%11table 1: impact of document length normalization  title-only runs  on highly relevants
impact of document length normalization: ignoring document length  rows 1a and 1a  hurt all scores; average precision was 1% higher in the other rows  and precision 1 was 1% higher in the other rows.  the impact on highly relevants was even larger; average precision was up to 1% higher in the other rows.  for most measures  a setting of 1 produced the highest scores of the settings investigated.  when comparing the document length importance setting of 1 with 1  i.e. compare 1c to 1a  and 1c to 1a   the differences in all of the shown measures  i.e. from avgp through h1  are statistically significant at the 1% level by the two-sided wilcoxon signed rank test.
impact of squaring the importance of inverse document frequency  compare 1b to 1c  and 1b to 1c  which are the same except for the relevance method  v1 vs v1  : all measures were higher with the importance of inverse document frequency squared  relevance method v1 .  the differences were statistically significant at the 1% level for avgp  p 1 and p 1  and at the 1% level for p 1  p 1  rec1  avgh  h 1 and h1  by the two-sided wilcoxon signed rank test.  note that on some other test collections  such as the clef news collections  we have seen v1 receive higher scores.
for the benefit of the relevance assessment pools  we donated one run with the description field included
 hum1tdlx  and assigned it highest judging priority.  tables 1 and 1 show scores for hum1tdlx and a diagnostic run which is the same as hum1tdlx except that the narrative field was also included.  table 1 also shows averages of the medians reported by nist which were based on a group including all submitted non-title-only runs  including 1 manual runs and some runs using the narrative.
impact of including the description field  compare hum1tlx to hum1tdlx  i.e. 1a to 1a  and 1a to 1a : all scores were higher when including the description.  the differences were statistically significant at the 1% level for avgp  p 1  rec1  h 1 and h1 by the two-sided wilcoxon signed rank test.  none of the differences were statistically significant at the 1% level.
runavgpp 1p 1p 1p 1p 1rec1rec1a: hum1tdlx11%1%1%1%1%111b: 1a + narr11%1%1%1%1%11median  1 runs 1n/a1%n/a1%1%n/an/atable 1: precision of non-title-only runs counting all relevants the same
runavghh 1h 1h 1h 1h 1h1h1a: hum1tdlx11%1%1%1%1%111b: 1a + narr11%1%1%1%1%11table 1: precision of non-title-only runs just counting highly relevants as relevant
impact of including the narrative field  compare 1a to 1b  and 1a to 1b : when counting all relevants the same  all investigated scores were higher when including the narrative.  when just counting highly relevants as relevant  most of the scores were lower when including the narrative.  none of the differences were statistically significant at the 1% level by the two-sided wilcoxon signed rank test.
table 1 shows per-topic comparisons of our submitted runs with the medians in their category for the measures reported by nist: average precision  precison 1  precision 1 and precision 1  respectively.  in each comparison  we show the number of topics on which the run scored higher than the median  lower than the median and tied with the median  higher-lower-tied .  differences statistically significant at the 1% level by the two-sided wilcoxon signed rank test are marked with two asterisks  **   and differences just significant at the 1% level are marked with a single asterisk  * :
runavgpp 1p 1p 1hum1tlx1-1 **1-1 **1-1 **1-1 **hum1tl1-1 **1-1 **1-1 **1-1 **hum1t1-1 *1-1 *1-1 *1-1hum1tdlx1-1 **1-1-11-1table 1: per-topic comparison of submitted runs with medians
the per-topic comparisons show a lot of ties in the early precision scores  particularly p 1  because of the small number of documents considered.  still  in each measure  the difference of the hum1tlx and hum1tl runs with the medians is statistically significant at the 1% level by the two-sided wilcoxon signed rank test  the calculation of the significance level discards the ties  following  .
the significance level  p-value  for the two-sided wilcoxon signed rank test defined in  was computed by our own implemented algorithm.  the computation is exact  aside from double-precision roundoff errors  even in the case of tied absolute differences.  for the wilcoxon signed rank test we assume that the differences on differing topics are independent  and that the differences are from a distribution which is symmetric about a median difference.  the test tests the hypothesis that the median difference is zero.  for more details  see .
1 cross-language track
table 1 shows our submitted arabic runs  which were all monolingual runs  i.e. used the arabic versions of the topics.  the baseline run was humar1td  a title+description run which applied some experimental arabic morphological normalizations to the words before indexing.  the other runs were the same as the baseline except for one factor.  run humar1tdm disabled the arabic-specific normalizations  but not general ones such as case normalization .  run humar1tdx used a different weighting scheme which squared the importance of inverse document frequency and also increased the adjustment for document length.  run humar1t was title-only.  run humar1tdn was title+description+narrative.  no stop words were applied:
runavgpp 1p 1p 1p 1p 1rec1rec1a: humar1td11%1%1%1%1%111b: humar1tdm11%1%1%1%1%111c: humar1tdx11%1%1%1%1%111d: humar1t11%1%1%1%1%111e: humar1tdn11%1%1%1%1%11table 1: precision of submitted monolingual arabic runs
impact of arabic morphological normalizations  compare humar1tdm to humar1td : all investigated scores except p 1 were tied or higher when the arabic morphological normalizations were applied.  none of the differences were statistically significant at the 1% level by the two-sided wilcoxon signed rank test.   the arabic test collection just contained 1 topics  making it harder to detect significant differences than for the web collection  which had 1 topics. 
impact of changing the weighting scheme  compare humar1td to humar1tdx : it made little difference.
impact of excluding the description field  compare humar1td to humar1t : some scores were higher when just using the title field  avgp  p 1  p 1  rec1  rec1 .  others were higher when the description was included  p 1  p 1  p 1 .  it was noted at the conference that the titles for these topics were a little longer than usual.
impact of including the narrative field  compare humar1td to humar1tdn : some scores were higher when including the narrative  p 1  p 1  p 1  p 1  rec1  but a few were lower  avgp  p 1  rec1 .
after the conference  we added approximately 1 arabic stop words based on the isi list .  table 1 shows the new scores when redoing runs humar1td  humar1t and humar1tdn with the stop words  note: an experimental version of searchserver 1  pre-release  was used for the arabic diagnostic runs  but its document ranking with respect to arabic was the same as searchserver 1's except for the experimental differences described in this section :
runavgpp 1p 1p 1p 1p 1rec1rec1td: 1a + stp11%1%1%1%1%111t: 1d + stp11%1%1%1%1%111tdn: 1e + stp11%1%1%1%1%11table 1: precision of runs using arabic stop words
impact of arabic stop words  compare 1a to 1td  1d to 1t  and 1e to 1tdn : while the scores just increased modestly  the increases were consistent across topics.  for example  in the title+description case  runs 1a vs 1td   1 topics had a higher score in average precision  just 1 lower and 1 tied  when using the stop words.  in the title+description case  the difference in average precision was statistically significant at the 1% level  and the differences in p 1 and p 1 were statistically significant at the 1% level  by the two-sided wilcoxon signed ranked test.
as another experiment  we added some morphological normalizations mentioned by other groups which we weren't already using  particularly the orthographic variation mentioned by bbn   yeh vs. alef maksura at end of word  and removing waw prefixes as mentioned by berkeley .  table 1 shows the scores when redoing the runs of table 1 with the additional rules:
runavgpp 1p 1p 1p 1p 1rec1rec1td: 1td +rules11%1%1%1%1%111t: 1t +rules11%1%1%1%1%111tdn: 1tdn+rules11%1%1%1%1%11table 1: precision of runs with additional arabic morphological normalizations
impact of additional arabic morphological normalizations  compare 1td to 1td  1t to 1t  and 1tdn to 1tdn : most of the scores modestly increased from the new rules.  focusing on the title+description case  the difference in p 1 was statistically significant at the 1% level by the two-sided wilcoxon signed rank test  but the other differences were not.
combined impact of  updated  arabic morphological normalizations and stop words  compare 1b to 1td : the combination of arabic morphological normalizations  including the experimental updates  and the stop words increased average precision by 1% and the difference in average precision was statistically significant at the 1% level by the two-sided wilcoxon signed rank test.  none of the other differences were statistically significant even at the 1% level.  precision 1 just increased 1%.  early precision may benefit less from normalization rules because there may be enough exact matches to find.
some groups found that query expansion worked well on this collection  so we applied the  row expansion  technique described in last year's paper .  roughly speaking  row expansion is a pseudo-relevance feedback technique in which it is assumed that the top rows of the initial query are relevant and searchserver's intuitive searching uses them to generate new  broader queries.   in practice  a searchserver user would specify which rows are relevant  which should produce better results than the  blind  automatic technique applied here.   like last year  we used the top-1 rows; a minor difference is that for the row expansion queries  we used a document frequency parameter of 1%  i.e. relevance method 'v1:1'  instead of the experimental secondary term selection approach used last year.  table 1 shows the scores after applying row expansion to the runs of table 1:
runavgpp 1p 1p 1p 1p 1rec1rec1td: 1td +exp11%1%1%1%1%111t: 1t +exp11%1%1%1%1%111tdn: 1tdn+exp11%1%1%1%1%11table 1: precision of arabic runs after row expansion
impact of row expansion  compare 1td to 1td  1t to 1t  and 1tdn to 1tdn : most of the scores modestly increased from row expansion; average precision was up 1%.  focusing on the title+description case  the differences in average precision and rec1 were statistically significant at the 1% level by the two-sided wilcoxon signed rank test; the other differences were not statistically significant at even the 1% level.  query expansion techniques such as row expansion may help recall-oriented measures by contributing terms from the top documents which are not automatically generated from the initial query.
combined impact of  updated  arabic morphological normalizations  stop words and row expansion  compare 1b to
1td : applying all the techniques described above increased average precision by 1%  but increased precison 1 by just 1%.  the differences in average precision and rec1 were statistically significant at the 1% level  and the difference in precision 1 was statistically significant at the 1% level  by the two-sided wilcoxon signed rank test. none of the other differences were statistically significant at the 1% level.
