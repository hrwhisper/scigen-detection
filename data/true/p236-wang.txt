co-occurrence data is quite common in many real applications. latent semantic analysis  lsa  has been successfully used to identify semantic relations in such data. however  lsa can only handle a single co-occurrence relationship between two types of objects. in practical applications  there are many cases where multiple types of objects exist and any pair of these objects could have a pairwise co-occurrence relation. all these co-occurrence relations can be exploited to alleviate data sparseness or to represent objects more meaningfully. in this paper  we propose a novel algorithm  m-lsa  which conducts latent semantic analysis by incorporating all pairwise co-occurrences among multiple types of objects. based on the mutual reinforcement principle  m-lsa identifies the most salient concepts among the co-occurrence data and represents all the objects in a unified semantic space. m-lsa is general and we show that several variants of lsa are special cases of our algorithm. experiment results show that m-lsa outperforms lsa on multiple applications  including collaborative filtering  text clustering  and text categorization.
categories and subject descriptors: h.1  information search and retrieval:  indexing methods
general terms: algorithms
keywords: m-lsa  lsa  mutual reinforcement principle  multipletype
1.	introduction
¡¡co-occurrence data arises naturally and frequently in a variety of applications such as information retrieval and text mining. in most existing work on analysis of co-occurrence data  only a single pairwise co-occurrence relationship between two types of objects is considered. for example  in information retrieval  the cooccurrence information between documents and words is used to rank documents with respect to queries . in collaborative filter-
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.

figure 1: example of multiple-type interrelated data objects. each edge denotes a single co-occurrence relationship.
ing  items are recommended to an active user based on historical co-occurrence data between users and items .
¡¡however  in most applications  there exist multiple types of data objects and each pair of them could have a pairwise co-occurrence relationship. for example  in the web domain as shown in figure 1  users co-occur with web pages by viewing  queries co-occur with web pages by referencing  web pages co-occur with words by containing  and so on. with each kind of objects containing thousands of instances  each single co-occurrence relationship could be quite sparse. in the example above  using a single relationship  say   user  web page   to represent users may not be meaningful since there are millions of web pages and each user may only view a tiny portion of them. latent semantic analysis  lsa   was proposed to alleviate the data sparseness problem by representing objects in a low-dimensional semantic space. in this space  semantically related objects are expected to be near to each other. such a dimension reduction technique has been shown to improve performance in a variety of applications  e.g.   1  1  1  . however  the application of lsa is rather limited since it can only consider the co-occurrence relationship between two types of objects. with multiple co-occurrence relationships available  it is beneficial to exploit all of them to identify the semantic relations and alleviate the data sparseness problem. in the example above  we can also exploit other relations such as  user  query  and  query  web page  to better represent users  since users with similar interests tend to issue similar queries  and similar queries could refer to similar web pages. all these co-occurrence relations could be complementary  thus it is desirable to incorporate all of them so as to represent each type of objects more meaningfully.
¡¡though promising  exploiting the co-occurrence relations among multiple types of objects is challenging: 1  it is not clear how to effectively utilize all the co-occurrence relations among multiple types of objects to overcome data sparseness. 1  there may exist hidden relations between any two types of objects and these relations are complex since the information can also propagate through any co-occurrence path. take figure 1 as an example. there is no direct co-occurrence between users and words. but similar words can induce similar queries and web pages  thus in turn  similar users. the similarity between words can be propagated to users through the path  words ¡ú queries ¡ú users  or  words ¡ú web pages ¡ú users . even more complex propagations are also possible.
¡¡to effectively utilize all the information among heterogeneous objects  we propose a novel and unified latent semantic analysis algorithm  m-lsa  to model all the objects in a unified framework and identify the latent semantic relations underneath all the co-occurrence data. by exploiting all the pairwise co-occurrence data simultaneously  m-lsa identifies the most salient or important concepts among them. these concepts span a unified lowdimensional semantic space  where each object is represented by a vector which reflects the strengths of its association with these concepts.
¡¡specifically  to identify important concepts  a natural belief is that important concepts are related to important objects. based on this assumption  we utilize the mutual reinforcement principle  which is underlying the traditional lsa  to identify the important objects in each type leveraging all the co-occurrence relations quantitatively. this principle leads to an eigenvector problem and the obtained eigenvectors are regarded as the latent concepts. we show that the m-lsa algorithm is a natural generalization of the traditional lsa from two to multiple types of objects.
¡¡the rest of this paper is organized as follows. section 1 is to discuss previous work. we define our problem in section 1. to solve this problem  we identify the mutual reinforcement principle underlying lsa and extend it to multiple types of objects in section 1. this principle naturally leads to a solution to our problem: m-lsa. section 1 is to present our experimental results for different applications. finally  we conclude our paper in section 1.
1.	related work
¡¡lsa was first introduced to address the synonym and polysemy problems in information retrieval . since then  lsa has attracted much attention and several researches analyzed it theoretically  1  1  1  1 . for example   and  used probabilistic model to study the effectiveness of lsa. more recently   argued that spectral algorithms such as lsa can expand the documents implicitly to improve retrieval accuracy.
¡¡some variants of lsa have also been proposed recently. probabilistic lsa  plsa   applies a probabilistic aspect model to the co-occurrence data. iterative residual rescaling  irr   is proposed to counteract lsa's tendency to ignore the minor-class documents. unlike lsa  nonnegative matrix factorization  nmf   1  1  decomposes the  document  word  matrix into two matrices with no negative values.
¡¡most work above only considers co-occurrence relations between two types of objects. high-order co-occurrence data or high-order tensor is studied in multilinear algebra . in   the highorder singular value decomposition  hosvd  is proposed to factor high-order tensors; in contrast  we consider the pairwise cooccurrence relations between different types of objects in this paper  which is less computationally expensive than hosvd.
¡¡several recent studies utilize pairwise co-occurrence data for different specific purposes such as object clustering  and similarity measuring  1  1 . in   a unified approach is proposed to analyze both link and text information. compared with these studies  our approach is more general and fundamental in that we provide a general principled method for analyzing any multiple types of objects.
¡¡our work is related to the hits algorithm  and a key-phrase extraction algorithm  in the sense of sharing the mutual reinforcement principle. hits uses this principle to find good pages from a web subgraph and  uses it to identify salient key-phrases from a document. in this paper  we use this principle for multipletype latent semantic analysis.
1.	the problem
¡¡the problem we study is to analyze the co-occurrence relationship among multiple types of objects. suppose we have n types of objects {x1 x1 ... xn} and each pair of them could have a pairwise co-occurrence relation. formally  we construct an undirected graph g v e . v consists of n vertices with each corresponding to a type of objects. if there is a pairwise co-occurrence relation between two types of objects  we have an edge in e which connects the corresponding vertices. we name graph g as  multiple-type graph . in g  each type corresponds to a set of objects and we use |xi| to denote the number of objects of this type. for each edge eij ¡Ê e  we have a |xi| ¡Á |xj| co-occurrence matrix mij. each edge could have a weight ¦Áij to measure the importance of the co-occurrence relation between xi and xj. note that g is not necessary to be a complete graph. an edge eij is absent if the corresponding co-occurrence data is unavailable or not meaningful for an application.
¡¡for example  in figure 1  the corresponding graph g contains 1 types of objects: users  queries  web pages  and words. we have 1 co-occurrence relations in g and each of them is denoted by an edge in figure 1  thus we have 1 co-occurrence matrices.
¡¡intuitively  based on graph g  objects of any type  e.g.  users  can be represented by objects of the other types to which it is directly connected  e.g.  web pages and queries . however  this method is not effective to exploit all the information on a multiple-type graph. a more general method is to represent objects of any type by all the types of objects which have paths to them  e.g.  representing users by words . however  due to the complex relations among data objects  there may be many paths between two types of objects  e.g.  users and words   thus this method is difficult to be implemented directly.
¡¡to effectively utilize the information on a multiple-type graph g  our goal is to find the latent semantic representations for each type of objects. specifically  based on the co-occurrence data of g  we first identify the most salient concepts based on the mutual reinforcement principle. these concepts span a semantic space. we then represent each object in this unified low-dimensional space.
1.	m-lsa
¡¡in this section  we first describe the mutual reinforcement principle based on the analysis of the traditional lsa. we then extend it to multiple types of objects and present the m-lsa algorithm. finally  we show that two variants of the traditional lsa are special cases of m-lsa. in the following  we denote matrices by uppercase letters  e.g.  a  b   scalars by lower-case letters  e.g.  a  b   and vectors by bold lower-case letters  e.g.  a  b .
1	the mutual reinforcement principle of lsa
1.1	brief review of lsa
¡¡lsa is based on a mathematical operation  singular value decomposition  svd   which is akin to factor analysis. consider the analysis of document-word co-occurrence data  if there are a total of n documents and m words in a document collection  the process starts with the creation of the co-occurrence matrix between the documents and words a =  aij   with each entry aij representing the co-occurrence frequency of the i-th word in the j-th document. for the m ¡Á n matrix a  where without loss of generality m ¡Ü n and rank a  = r  the svd is defined as :
a = u¦²v t
where u =  u1 u1 ... ur  is an m¡Ár column-orthonormal matrix whose columns are called left singular vectors; ¦² = diag ¦Ò1 ¦Ò1  ... ¦Òr  is an r¡Ár diagonal matrix whose diagonal elements are positive singular values sorted in descending order. v =  v1 v1 ... vr  is an n ¡Á r column-orthonormal matrix whose columns are called right singular vectors.
¡¡given an integer  lsa uses the first k singular vectors to represent the documents and words in a k-dimensional space . each singular vector is regarded as a latent concept which captures a salient recurrent word combination pattern in the document collection; a document has a large index value for a concept if it contains the corresponding word pattern . more precisely  lsa represents each document by a row of  ¦Ò1 ... ¦Òkvk  and each word by a row of  ¦Ò1 ... ¦Òkuk .
1.1	the mutual reinforcement principle
¡¡in lsa  the first k singular vectors of a represent the most important k concepts in the document collection. alternatively  we can assume that important concepts are related to both important documents and important words. to identify the most important concept from a  we associate importance property for documents and words respectively and utilize the mutual reinforcement principle1 
an important document co-occurs with important words; an important word co-occurs with important documents.
numerically  if we associate an importance value vi with the ith document and an importance value uj with the j-th word  the mutual reinforcement principle is expressed as:
	vi =	j:j¡«i uj;uj =	i:j¡«i vi.
where j ¡« i means that the j-th word co-occurs with the i-th document. if we denote the importance of all documents by a vector v and the importance of all words by a vector u  we can express the mutual reinforcement principle as:
	v =	atu
 1 
	u =	av
¡¡it is easy to see that v = atav and u = aatu. therefore  u and v are the principal left and right singular vectors of a respectively  please refer to  for proof .
¡¡based on the co-occurrence data  the mutual reinforcement principle provides a reasonable solution to factor the most important concept out. however  a collection generally contains multiple topics. the most important concept represents well the most salient topic  but not other topics. fortunately  the mutual reinforcement principle can also be extended to the non-principal singular vectors of a easily . specifically we can get the first k singular vectors. each vector is then considered as a latent concept  and the magnitude of the corresponding singular value represents the importance of the concept. lsa projects the documents and words to a low-dimensional semantic space spanned by these concepts.

1
similar principles are used in hits  and .
1.1	unified importance and concept vectors
¡¡in lsa  the latent concepts are represented by v in document space and u in word space. conceptually  however  they are associated with the same concept. thus  our key idea for generalizing lsa to handle multiple co-occurrence matrices is to represent each concept as a single vector. we now describe the unified importance and concept vectors.
¡¡recall in equation  1   we have v and u. if we concatenate them as a unified importance vector  u v t  equation  1  can be rewritten as:
		 1 
where b is defined as:
		 1 
¡¡it is easy to see that  u v t is the eigenvector of b. mathematically   shows that the eigenvectors of b are closely related to the singular vectors of a: u and v in equation  1  are the left and right singular vectors of a respectively. thus  computationally the svd of a is equal to finding the eigenvectors of b  while b gives us a  more preferred  unified view of importance.
¡¡with the notion of a unified concept vector  we can now explain lsa in a unified view. let c =  u v t be the unified concept vector. we denote the first k eigenvectors of b as {c1 ... ck}. each of them is a unified concept vector and has the corresponding importance precisely represented by the eigenvalue of b  ¦Òi  which is the same as the singular value of a . therefore  with this unified view  lsa represents each object by a row of the matrix:
		.
the upper part of the matrix is for words and the lower part of the matrix is for documents.
1	the m-lsa algorithm
¡¡to factor the latent semantic concepts out from multiple co-occurrence relations represented by a multiple-type graph g  we first extend the mutual reinforcement principle of lsa to multiple-type graph.
on a multiple-type graph g with n vertices and a number of pairwise co-occurrence relationships  important objects of a type co-occur with important objects of other types.
¡¡the mutual reinforcement principle on a multiple-type graph is a natural generalization of that of two types of objects. this principle also provides a reasonable solution to finding the important latent concepts among the multiple co-occurrence data as we will describe in the following.
¡¡formally  recall that we have n types of objects on graph g: {x1 x1 ... xn}. for any two types of objects: xi and xj  we have the co-occurrence matrix mij  mij = 1 if the edge eij is absent on g . it is easy to see that mij = mjit.  for brevity  we only consider the co-occurrence data between different types of objects. the co-occurrence relationship within a single type of objects can be incorporated similarly.  let us associate an importance value with each object. for the i-th type of objects in xi  we have one weight vector wi to denote their importance. the mutual reinforcement principle can be expressed as:
.
	wi =	mijwj	 1 
j j i
¡¡taking unified view of latent concepts  we use w =  w1 ... wn t as the concatenated importance vector and define
r =1
m1
...
mn1m1
1
...
mn1¡¤¡¤¡¤
¡¤¡¤¡¤
...
¡¤¡¤¡¤m1n
m1n
...
1 1 as the unified co-occurrence matrix. we can rewrite equation  1  in a matrix format:
.
	w = r ¡¤ w	 1 
it is easy to show that w will converge to the eigenvector of the co-occurrence matrix r.
¡¡similar to lsa  since important objects will have high weights in w  we regard w as the most important latent concept vector across all the co-occurrence relations. each entry in w corresponds to an object and its value can be regarded as the association weight between the object and this latent concept. similarly  the first k eigenvectors of r represent the top k important concepts  which span a k-dimensional semantic space to represent all the objects.
specifically  let
¦Ë1 ¡Ý ¦Ë1 ¡Ý ... ¡Ý ¦Ëk
be the top k eigenvalues of r and the corresponding vectors are respectively
c1 c1 ... ck.
the symmetry of matrix r guarantees that all eigenvalues are real numbers and ¦Ël gives precisely the importance of the corresponding concept vector cl  1 ¡Ü l ¡Ü k . therefore  the i-th object can be represented by
 ¦Ë1i ¦Ë1i ... ¦Ëkcki 
where cli is the i-th entry in cl  i.e.  the association weight between the i-th object and the l-th concept. all the objects are represented in the matrix:
 ¦Ë1 ¡¤ c1 ¦Ë1 ¡¤ c1 ... ¦Ëk ¡¤ ck 
with each row representing an object in the k-dimensional space.
the above analysis treats all the co-occurrence relationships equally 
i.e.  g is unweighted. it is not the best choice in many cases. for example  different co-occurrences might not be equally reliable on the same scale; some may contain more noises than others. on the other hand  the entries in different co-occurrence matrices may have different scales  thus need to be normalized. therefore  in general  we want to give different weights to different cooccurrence matrices  i.e.  g is weighted. to incorporate these weights  we can directly replace mij by ¦Áij ¡¤ mij in matrix r in equation  1  and then conduct semantic analysis on this new matrix. these weights can be used as normalization factors or to reflect the importance of different matrices for a specific application. since ¦Áij's only represent the relative importance of the matrices and their scales do not change the eigenvectors of matrix r  without loss of generality  we could add a constraint i j ¦Áij = 1 or set a specific ¦Áij = 1 and adjust others.
¡¡we use the name m-lsa to denote our algorithm since its analysis is based on multiple-type graphs.
1	relation with lsa
¡¡it is trivial to show that the traditional lsa is a special case in our framework. in particular  since lsa only deals with two types of objects  say  x1 and x1  there is only one co-occurrence matrix ¦Á1 ¡¤ m1. in the m-lsa framework  we thus have

it is easy to see that the eigenvectors of r is the same as the eigenvectors of b in equation  1  when m1 = a . furthermore  by setting ¦Á1 = 1  the result of m-lsa for r is the same as the result of lsa for a. therefore  the traditional lsa is a special case in our m-lsa algorithm.
¡¡previous work has also applied lsato the objects with two types of features. for example  in   lsa was applied to images incorporating both keyword features and low-level image features  e.g.  colors of images . by concatenating two kinds of features as longer feature vectors   conducted lsa on a larger matrix. in our mlsa framework  if we use x1 as images  x1 as keywords  and x1 as low-level features  we have 

where m =  ¦Á1 ¦Á1   which concatenates the keyword and low-level features of images together. thus  this variant of lsa is also a special case in our m-lsa algorithm.
1.	experiments
¡¡in this section  we evaluate m-lsa on different data sets and for different tasks  including collaborative filtering  text clustering  and text categorization. in these applications  we design the cooccurrence matrix r from the available data and study the effectiveness of our algorithm. in our experiments  we use two benchmark data sets  movielens1 and 1-newsgroup1.
1	application i: collaborative filtering
¡¡collaborative filtering  cf   is to recommend items to an active user based on the historical data of like-minded users. based on the current ratings of the active user  memory-based algorithms find its nearest neighbors and recommend items based on the ratings of the neighbors. in this paper  we use the pearson method  a popular memory-based method  as one of our baselines. pearson method uses pearson correlation coefficient to find the nearest neighbors for an active user v:

where m is the number of items  rv i  ru i  is the rating of user v  u  for item i  and r¡¥v  r¡¥u  is the average rating of user v  u . wv u is the similarity score between the two users. we use nv as the selected set of the nearest neighbors of v  then the prediction of v's rating for an unseen item i is calculated as
		 1 
¡¡for the memory-based cf algorithm  the prediction accuracy depends on the accuracy of the nearest neighbors. in our experiments  we show that m-lsa can improve the prediction accuracy by representing the users more meaningfully  thus finding more accurate nearest neighbors.

1
 http://www.cs.umn.edu/research/grouplens 1 http://people.csail.mit.edu/jrennie/1newsgroups

figure 1: cf result comparison of different methods on movielens data
1.1	experiment design
¡¡we use the benchmark data set movielens  which includes 1 ratings  1  from 1 users on 1 movies. each movie has the title keyword information. therefore  we have three types of objects: users  x1   items  x1  and keywords  x1 . the co-occurrence matrices include user-item  m1   user-word  m1  and item-word  m1 . m1 is constructed with the ratings of users on movies; m1 is constructed with the movie titles. we construct m1 as follows: if a keyword appears in a title of any movie rated by a user  this keyword and this user has a co-occurrence. their cooccurrence frequency is calculated by the user's rating on movies and the term-frequency of this keyword in the corresponding movie titles. we weigh m1 and m1 by the standard tf-idf scheme which is commonly used in retrieval retrieval . finally  we can construct the relation matrix r based on the three matrices as:
	1	¦Ám1	¦Âm1
	r	=	¦Ám1t	1	¦Ãm1	 1 
	¦Âm1t	¦Ãm1t	1
where the weight parameters ¦Á ¦Â ¦Ã ¡Ý 1  and ¦Á + ¦Â + ¦Ã = 1.
¡¡we use two baseline methods. one is the pearson method which is based on user-item matrix m1. the other is based on userword matrix m1 to calculate the nearest neighbors. we also apply the traditional lsa on these two matrices and then use the lowdimensional representation to calculate the nearest neighbors. our m-lsa will be compared with all the four methods. for all the five methods  after calculating the nearest neighbors  the predictions are based on equation  1  and the comparison is based on the half-time utility metric defined in . for a user v  the expected utility of a ranked list of items is:

where ¦Ó is the half-time parameter  ¦Ó = 1 in our experiments  and rv j is v's rating for the item which is at the j-th position in the current rank list. the final score over all users in the test set is:

where rmaxv is the maximum possible utility obtained where all the test items are ranked at top according to user v's rating. in the following figures  we use  rank gain  to denote r score.
1.1	results
¡¡all the results are averaged over a randomly split of 1 folds on the movielens data. we fix the number of neighbors to 1 and

figure 1: impact of number of neighbors on baseline methods

figure 1: impact of the dimensionality on lsa and m-lsa
the number of dimensions for lsa and m-lsa to 1. we set ¦Á = 1  ¦Â = 1  and ¦Ã = 1 in matrix r. figure 1 gives the comparison results of different methods. the figure shows that m-lsa achieves the best result. compared with the other methods  m-lsa achieves a relative improvement of 1% over user-item  1% over user-word  1% over lsa user-item   and 1% over lsa user-word  with respect to the r score measure. our results are consistent with that of   where the authors found that userword matrix could get better result because it is less sparse than user-item matrix. lsa based methods could only achieve marginal improvement compared with baselines. our method can improve the utility over lsa significantly. we also calculate the standard deviation of the five methods over the 1 folds. the results in figure 1 show that m-lsa is more stable. this confirms that m-lsa can effectively explore all the co-occurrence relations to represent objects more meaningfully.
¡¡we also study the parameters for different methods. in figure 1  we plot the results along with the number of neighbors for the two baselines. we can see that the best results are obtained when the number is 1. thus we fix the number of neighbors as 1 in all the experiments.
¡¡in figure 1  we study the impact of number of dimensions for lsa and m-lsa. for m-lsa  we set ¦Á = 1  ¦Â = 1  and ¦Ã = 1 in matrix r. it can be seen that m-lsa consistently outperforms lsa based methods and all attain their best results when the dimensionality is 1.
¡¡finally  we study the influence of ¦Á  ¦Â  and ¦Ã in matrix r for mlsa. since ¦Á + ¦Â + ¦Ã = 1  we only vary ¦Á and ¦Â and report our results in table 1. we vary ¦Á  ¦Â respectively  from 1 to 1 by step 1 and we retain the top 1 eigenvectors for m-lsa. in this table  we get the best result when ¦Á = 1  ¦Â = 1  thus ¦Ã = 1. furthermore  when ¦Á ¡Ý 1  most of the results  bold font  are better than the baselines and lsa based methods  thus m-lsa is effective for a wide range of parameters.
table 1: impact of weight parameters of m-lsa on cf results
¦Â11111¦Á1111111111*1-1111--111---11----¡¡it is worth noting that when ¦Ã = 1  i.e.  ¦Á+¦Â = 1 in table 1   we only consider the user-item matrix m1 and user-word matrix m1 but not m1  which means we concatenate both features with appropriate weights to represent the users. however  we obtained the best result when ¦Á = 1  ¦Â = 1  and ¦Ã = 1. this means all the co-occurrence information is useful and can be incorporated by m-lsa effectively.
1	application ii: text clustering
¡¡text clustering is one of the fundamental problems and has received much attention recently. when the vector space model  vsm  is used  each document is represented as a term vector and the similarity score between two documents is calculated as the cosine value  or dot product  of corresponding term vectors. in this section  we show experimentally that our m-lsa method can improve the document representation and boost text clustering result significantly. we use k-means  one of the most popular clustering algorithms  to compare different document representation methods.
1.1	experiment design
¡¡to obtain multiple co-occurrence data  we use the 1-newsgroup data set. in this newsgroup data  different emails or posts may have the same subject. thus  besides the email-word relation  we have email-subject and subject-word relations. therefore  the objects we have are: emails  x1   subjects  x1   and words  x1 . each pair of them has a co-occurrence matrix and the obtained matrix r is similar as in equation  1 . we select the five  comp.*  out of the 1 categories as our data set. each of the five categories has 1 emails  thus  we have 1 in total. after preprocessing the subjects by removing  re:  and  fwd:   we obtain 1 subjects in total. we use the f measure defined in  as our evaluation metric. for each cluster  we calculate its precision and recall with respect to each given category. the f measure is defined by combining the precision and recall together. specifically  for cluster j and category i:
             nij recall i j  = ni
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡nij precision i j  = 
nj
	1	precision i j 	recall i j 
	f i j  =	¡Á	¡Á
precision i j  + recall i j 
where nij is the number of members of category i in cluster j  ni is the number of members in category i  nj is the number of members in cluster j and f i j  is the f measure of cluster j and category i. the f measure of the whole clustering result is defined as a weighted sum over all categories as follows:

where the max is taken over all clusters.

figure 1: results on text clustering. we compare the result on different dimensions
¡¡in our experiments  we first represent each email  body + subject  using the tf-idf method. this gives us the baseline result. lsa is applied to this tf-idf representation and our m-lsa method is based on the matrix r defined above. k-means is then run on the low-dimensional representations for lsa and m-lsa. we randomly select the initial centroids 1 times and have 1 runs. the results for each method are averaged over these 1 runs.
1.1	results
¡¡we set ¦Á = 1  ¦Â = 1  and ¦Ã = 1 for m-lsa. figure 1 is the primary results for clustering. in this figure  we vary the number of dimensions for both lsa and m-lsa. it is clear that m-lsa and lsa outperform the baseline method  1  substantially. by comparing m-lsa with lsa  we can see that m-lsa always achieves better result. for example  when we set the number of dimensions to 1  the f measures of m-lsa and lsa are 1 and 1 respectively. thus  m-lsa achieves 1% relative improvement over lsa. the t-test over the 1 runs indicates the improvement is statistically significant  p-value=1 .
¡¡for m-lsa  we also study the parameters ¦Á  ¦Â  and ¦Ã in a similar way as in collaborative filtering. we set the number of dimensions to 1 and the results are reported in table 1. the best result  1  is obtained when ¦Á = 1  ¦Â = 1  and ¦Ã = 1. a difference here is that when ¦Á is set to a large value  the f measure is lowered a lot. this is because ¦Á is the weight for the  email  subject  co-occurrence matrix m1 and each email has only one subject. on average  1 emails share a subject. thus  the cooccurrence between email and subject is extremely sparse and only two emails that have the same subject will have a nonzero similarity score if we set ¦Á = 1. thus  the clustering result is biased when ¦Á is set too large. however  given an appropriate weight to this co-occurrence matrix  m-lsa can improve the clustering performance substantially. this again confirms the effectiveness of m-lsa to incorporate the meaningful co-occurrence information.
¡¡in table 1  we show the first five eigenvectors of r when we set ¦Á = 1  ¦Â = 1  and ¦Ã = 1. the most important words
and their weights associated with each eigenvector are given in this table. it is clear that each eigenvector is a focused concept. for example  eigenvector 1 is about  operating system   eigenvector 1 is about  hardware   and eigenvector 1 is about  monitor . this shows the effectiveness of m-lsa to identify latent semantic concepts by the mutual reinforcement principle.
1	application iii: text categorization
¡¡finally  we test our algorithm on text categorization problem. this experiment is to show that m-lsa is much flexible to model
table 1: the first five concepts of on comp.* newsgroup data
	eigenvector 1	eigenvector 1	eigenvector 1	eigenvector 1	eigenvector 1
window 1microsoft 1scsi 1card 1drive 1card 1os 1drive 1monitor 1appl 1drive 1ms 1id 1video 1mac 1file 1challeng 1controll 1driver 1power 1system 1window 1mb 1ati 1disk 1do 1product 1bu 11price 1run 1duke 1hard 1diamond 1monitor 1edu 1innov 1isa 1simm 1simm 1com 1market 1card 1ultra 1hard 1help 1do 1disk 1mhz 1centri 1table 1: the representative words for comp.* categories
os.ms-windows.miscgraphicssys.mac.hardwaresys.ibm.pc.hardwarewindows.xcentroidm-lsacentroidm-lsacentroidm-lsacentroidm-lsacentroidm-lsawindowwindowgraphicimagmacmacdrivedrivewindowwindowfilefileimaggraphicapplapplcardcardmotifserverdriverdofilefiledrivemonitorcontrollscsiserverapplicdodriverprogramformatmonitordrivescsicontrollwidgetdisplaiwinrunlookgifsimmsimmbubudisplairunedumsformatconvertedupriceididapplicmotifprogramwingifprogrampriceedupcmbsunwidgetrunprogramconvertlookquadracomputdxsystemrunsunmseduthankftpcentripowercomdiskcomprogramcardcardbitcolorlccentrimbhardprogramsettable 1: impact of weight parameters for m-lsa on clustering results
¦Â11111¦Á1111111111*1-1111--111---11----a variety of co-occurrence relations. in particular  we show that m-lsa can incorporate category information.
1.1	experiment design
¡¡the data set we used in this experiment is the same as above: 1 categories of  comp.*  newsgroup data. for each category  we randomly select 1 documents as training and use the remaining 1 as test data.
¡¡to incorporate the category information  in this experiment  we use a different co-occurrence relation matrix. the objects are: emails
 x1   categories  x1   and words  x1 . m1 is based on the tfidf weighting of email-word matrix. since each training example has a word vector in m1  for each category  we calculate the centroid vector of the corresponding training examples. the category-word matrix  m1  is composed of all these centroid vectors. we construct the binary email-category matrix m1 as follows: for each training example  it has a co-occurrence with its labeled category. the test data is not used in the construction of the co-occurrence matrix.
¡¡we apply the m-lsa algorithm to the above co-occurrence matrix and obtain the first k eigenvectors. we then project both the training and test examples along these k eigenvectors  thus represent all the examples in a k-dimensional space. classification experiments are conducted in this space.
¡¡since m1 and m1 are standard tf-idf matrices  we set their weights to 1. we thus only vary the weight ¦Á for matrix m1 and study its impact. the co-occurrence matrix is:

figure 1: results on text categorization. we compare the results on different dimensions
	1	¦Ám1	m1
	r	=	¦Ám1t	1	m1	 1 
	m1t	m1t	1
¡¡for the classifier  we use the sv mlight software1 and all the results reported below are based on the micro-averaging f1  microf1  measure defined in . f1 measure is a tradeoff between precision and recall. another commonly-used f1 measure is macroaveraging f1  macro-f1 . macro-f1 is the arithmetic average of f1 measure over all the categories and micro-f1 is the weighted average that emphasizes on categories with more examples. since all the categories in our data set have the same size  macro-f1 is similar to micro-f1 measure. thus we only report micro-f1.
1.1	results
¡¡we compare our result with the standard svm on the email-word matrix and lsa based method. we set ¦Á = 1 and figure 1 shows the results along with different dimensions. it is clear that m-lsa can outperform the baseline  while lsa can not. when we set the number of dimensions to 1  m-lsa achieves 1 on micro-f1 

1 http://svmlight.joachims.org

figure 1: impact of weight for text categorization
which is higher than the baseline 1 and the best result of lsa 1. since m-lsa incorporates the category supervised information into the co-occurrence matrix  it can achieve better result even when the dimension is set to a smaller value  e.g.  1   as shown in figure 1.
¡¡figure 1 shows the impact of the weight ¦Á for the matrix m1. we set the number of dimensions to 1 and vary ¦Á from 1 to 1 with step 1. when ¦Á = 1  m-lsa does not consider the category information thus the result is the same as lsa. from this figure  we can see that m-lsa is insensitive to the change of ¦Á when ¦Á ¡Ý 1 and its micro-f1 is always higher than the baseline. we obtain the best result when the weight is 1. the result is really encouraging since it indicates: although m-lsa is unsupervised in spirit  it can also incorporate the supervised information by appropriately introducing the co-occurrence relationship.
¡¡finally  we show that m-lsa can associate meaningful words to categories. in table 1  we show the most salient words in the centroid vector of each category  denoted by centroid . for mlsa  we do not use category-word matrix m1 in equation  1  and set the weight of email-category m1 to ¦Á = 1. we use the first 1 eigenvectors for m-lsa. in m-lsa  all the categories and words are projected into a unified semantic space  we thus calculate the similarities between words and categories by their dot products in this space. for each category  the most similar words are reported in table 1  denoted by m-lsa . it is clear that there is a big overlap between centroid and m-lsa based methods. this again confirms the effectiveness of m-lsa in identifying the latent concepts by utilizing all the co-occurrence information and representing each type of objects meaningfully in a unified semantic space.
1.	conclusions
¡¡it is important to exploit the co-occurrence relations among different types of objects in many applications. in this paper  we modelled all the pairwise co-occurrence relations with a multipletype graph and proposed a general algorithm  m-lsa  which conducts latent semantic analysis by incorporating all pairwise cooccurrences among multiple types of objects. based on the mutual reinforcement principle as used in the traditional lsa  m-lsa identifies the most salient concepts among all the co-occurrence data and represents each object in a unified semantic space. m-lsa is general and covers several variants of lsa as special cases. we evaluated m-lsa on three applications and obtained very encouraging results. all the experiments showed that m-lsa is effective in utilizing all the information on a multiple-type graph. m-lsa can be applied to any co-occurrence data involving multiple types of objects  thus has potentially many applications in multiple domains.
