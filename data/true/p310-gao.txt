in this paper  we introduce and study the minimum consistent subset cover  mcsc  problem. given a finite ground set x and a constraint t  find the minimum number of consistent subsets that cover x  where a subset of x is consistent if it satisfies t. the mcsc problem generalizes the traditional set covering problem and has minimum clique partition  a dual problem of graph coloring  as an instance. many practical data mining problems in the areas of rule learning  clustering  and frequent pattern mining can be formulated as mcsc instances. in particular  we discuss the minimum rule set problem that minimizes model complexity of decision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain distance constraints. we also show how the mcsc problem can find applications in frequent pattern summarization. for any of these mcsc formulations  our proposed novel graphbased generic algorithm cag can be directly applicable. cag starts by constructing a maximal optimal partial solution  then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set. our experiments on benchmark datasets show that cag achieves good results compared to existing popular heuristics.
categories and subject descriptors: h.1  database
management : database applications - data mining
general terms: algorithms  theory  performance
keywords: minimum consistent subset cover  minimum rule set  converse k-clustering  pattern summarization
1. introduction
모combinatorial optimization problems such as set covering and graph coloring have been extensively studied. by making connections to these classical problems  we can gain important insights into practical data mining applications.
모in this paper  we introduce and study the minimum consistent subset cover  mcsc  problem. given a finite ground
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  san jose  california  usa.
copyright 1 acm 1-1-1/1 ...$1.
set x and a constraint t  find the minimum number of consistent subsets that cover x  where a subset of x is consistent if it satisfies t. the mcsc problem provides one way of generalizing the traditional set covering problem   where a subset of x is consistent if it is a given subset. different from set covering  in typical mcsc instances the consistent subsets are not explicitly given and they need to be generated. for example  minimum clique partition  mcp   a dual problem of graph coloring  can be considered as an mcsc instance  where a subset is consistent if it forms a clique and the cliques are not given as input.
모as a practical application of the mcsc problem in rule learning  the minimum rule set  mrs  problem finds a complete and consistent set of rules with the minimum cardinality for a given set of labeled examples. the completeness and consistency constraints require correct classifications of all the given examples. with the goal of minimizing model complexity  the mrs problem can be motivated from both data classification and data description applications. the mrs problem is a typical mcsc instance  where a subset is consistent if it forms a consistent rule  i.e.  the bounding box of the subset contains no examples of other classes.
모as a prominent clustering model  k-clustering generates k clusters minimizing some objective  such as maximum radius as in the k-center problem  or maximum diameter as in the pairwise clustering problem . the radius of a cluster is the maximum distance between a fixed point  center  and any point in the cluster  and the diameter is the maximum distance between any two points in the cluster. since the number of clusters is often hard to determine in advance  converse k-clustering can be a more appropriate clustering model  where a maximum radius or diameter threshold is given and the number of clusters k is to be minimized. the converse k-center and converse pairwise clustering problems are both mcsc instances  where a subset is consistent if it forms a cluster satisfying a given distance constraint.
모frequent pattern mining has been a trademark of data mining. while the mining efficiency has been greatly improved  interpretability instead became a bottleneck to its successful application. as a known problem  the overwhelmingly large number of generated frequent patterns containing redundant information are in fact  inaccessible knowledge  that need to be further mined and explored. thus  summarization of large collections of patterns in the pursuit of usability has emerged as an important research problem. the converse k-clustering models discussed above as well as some other mcsc formulations appear to be a very reasonable and promising approach towards this problem.
모these formulated mcsc instances generally feature antimonotonic constraints  under which any subset of a consistent subset is also consistent. such instances allow the design of efficient and effective heuristic algorithms. stimulated by a theoretical study  our graph-based generic algorithm cag starts by constructing a maximal optimal partial solution  and then performs an example-driven specific-togeneral search on a dynamically maintained bipartite assignment graph to simultaneously learn a small consistent subset cover. the construction of initial optimal partial solution  the use of assignment graph allowing good choices of both the element and the consistent subset in an assignment  and the example-driven simultaneous learning  in contrast to the most-seen separate-and-conquer  strategy are the three novel design ideas that help to well guide the search leading to good performance of cag  as demonstrated by our experiments on rule learning and graph coloring benchmarks in comparison with existing popular heuristics.
모contributions.  1  we introduce and study the minimum consistent subset cover  mcsc  problem  which generalizes the set covering problem and has minimum clique
partition  a dual problem of graph coloring  as an instance.
모 1  the mcsc problem has many practical applications in data mining. in particular  we study the minimum rule set problem for rule learning. we also discuss applications in converse k-clustering and frequent pattern summarization.
모 1  to solve mcsc instances  we present a graph-based generic algorithm cag with several novel design ideas  whose performance is justified by our experimental evaluation.
1. preliminaries and related work
모in this section we provide preliminaries on graph theory and a review of related work in a very concise manner.
모preliminaries. a graph is complete if all of its vertices are pairwise adjacent. a clique of a graph g =  v e  is a subset of v such that the induced subgraph is complete. an independent set of g is a subset of v such that no two vertices in the subset are connected in g. the independence number of g  usually denoted by 붸 g   is the cardinality of the largest independent set. independent sets and cliques are opposite in the sense that every independent set in g corresponds to a clique in the complementary graph g.
모we say v 1   v is a dominating set if for all u 뫍 v   v 1  there is some v 뫍 v 1 such that  u v  뫍 e. a maximal independent set  say v 1  is also a dominating set. if it is not  there must be some u 뫍 v   v 1 that is not adjacent to any v 뫍 v 1  then u can be added to v 1 to form a larger independent set  but then v 1 is not maximal.
모the minimum clique partition  mcp  problem is to find a partitioning of the vertices of a graph g into the minimum number of disjoint vertex sets  each of which must be a clique in g. that minimum number is called the clique partition number of g and usually denoted by 뷌 g .
모the well-known graph coloring problem is to use the minimum number of colors to color the vertices of a graph g such that no adjacent vertices receive the same color. that minimum number is called the chromatic number of g and usually denoted by 뷌 g . since the vertices of an independent set can be safely colored with the same color  the graph coloring problem precisely minimizes the number of disjoint independent sets of g that form a partition of v .
apparently  mcp is a dual problem of graph coloring. an instance of the former on g is an instance of the latter on g. thus  we have 뷌 g  = 뷌 g .
observation 1. 붸 g  뫞 뷌 g .
모the observation states that the clique partition number of a graph is lower-bounded by its independence number. it is rather straightforward since two vertices in an independent set cannot appear together in a clique.
모related work. the traditional set covering problem  finds the minimum number of subsets from a given collection of subsets that cover a given ground set. it is one of the most fundamental algorithmic problems that has many variants  settings  and applications. the problem is np-hard and there is no constant factor approximation. it is approximable within 1 + logn by a simple greedy algorithm   which iteratively selects the subset that covers the largest number of uncovered elements until the ground set is covered. this greedy algorithm essentially adopts a separateand-conquer approach as seen in most rule learners.
모in the mcsc problem we study  the subsets are not explicitly given. instead  a constraint is given and used to qualify the subsets that can be used in a cover. the mcp problem and many practical data mining applications can be formulated as mcsc instances.
모graph coloring heuristics can be applied to complementary graphs to solve mcp instances. dsatur  is one of the most popular construction heuristics. in dsatur  a vertex with the largest number of different colors assigned to adjacent vertices is chosen and assigned with the first feasible color  where colors are pre-ordered. ties are broken favoring the vertex with the largest number of uncolored adjacent vertices. dsatur has a cubic runtime complexity.
모in the past few decades  numerous rule learners have been developed  such as the famous aq family  cn1 and ripper . most of them follow a separate-and-conquer approach  which originated from aq and still enjoys popularity. the approach searches for a rule that covers a part of the given  positive  examples  removes them  and recursively conquers the remaining examples by learning more rules until no examples remain. most rule learners minimize model complexity assuming the validity of occam's razor.
모previous work related to converse k-clustering and frequent pattern summarization will be discussed in section 1.
1. the minimum consistent subset cover problem
모in this section  we introduce the minimum consistent subset cover  mcsc  problem and study its properties  which provide important insights for our algorithm design.
1 definition
모the mcsc problem finds the minimum number of consistent subsets that cover a given set of elements  where a subset is consistent if it satisfies a given constraint.
모definition 1.  minimum consistent subset cover  given a finite ground set x and a constraints t  find a collection c of consistent subsets of x with s뫍c s = x such that |c| is minimized  where a subset is consistent if it satisfies t.
모we use a tuple  x t  to denote an mcsc instance with ground set x and constraint t. the consistent subset cover number for  x t   denoted by 붺 x t   is the minimum number of consistent subsets with respect to t that cover x.
모given  x t   we say the constraint t is granular if {x} is consistent with respect to t for any x 뫍 x. apparently  with a granular constraint   x t  always has a non-empty feasible region. most reasonable mcsc formulations feature granular constraints. for example  in the mcp problem  a single vertex also forms a clique.
모the set covering problem can be considered as an mcsc instance where a subset is consistent if it is given. consistent subsets are not explicitly given in typical mcsc instances. if we take a pre-processing step to generate all the consistent subsets  then an mcsc instance becomes a set covering instance. unfortunately  as argued in   the generated collection of subsets would be prohibitively large and this is not a feasible approach to solve mcsc instances.
1 properties
모covering problems have corresponding partitioning problems as special cases. disallowing overlapping  partitioning problems are easier in the sense that they have smaller search spaces. algorithms for partitioning problems usually work for the associated covering problems as well but typically generate solutions of larger sizes. however  finding partitions can be an advantageous way of finding covers for mcsc instances under certain conditions.
모definition 1.  anti-monotonic constraint  given  x t   we say t is anti-monotonic if for any subset s   x that is consistent  any s1   s is also consistent.
모for example  the mcp problem has an anti-monotonic constraint since a subset of a clique is still a clique. as to be shown  many practical data mining problems are also mcsc instances with anti-monotonic constraints  such as the minimum rule set and converse pairwise clustering problems.
모theorem 1. given  x t  where t is anti-monotonic  any solution to  x t  can be transformed into a solution to the associated partitioning problem with the same cardinality.
모proof. we give a constructive proof of the theorem. suppose we have a solution to  x t  at hand which is a set of overlapping consistent subsets. for each pair of consistent subsets that overlap  we can simply assign the overlap to any of the two and remove it from the other. since t is antimonotonic  the consistent subset with the overlap removed remains consistent. then  we obtain a set of consistent subsets of the same cardinality that form a partition of x. 
모theorem 1 implies that for  x t  where t is anti-monotonic  optimal or good partitions are also optimal or good covers. by targeting the simpler partitioning problem  we may design well-guided yet efficient search heuristics for  x t .
모in the following  we define the so-called consistency graph for  x t   based on which we connect the mcsc problem to the mcp problem and derive lower-bounds for 붺 x t .
모definition 1.  consistency graph  given  x t   a consistency graph for  x t   gc =  vc ec   is a simple graph where vc = x  and there is an edge  u v  뫍 ec for a pair of vertices u v 뫍 vc if and only if {u v} is consistent.
모observation 1. given  x t  where t is anti-monotonic  a consistent subset forms a clique in gc  where gc is the consistency graph for  x t .
모the observation is rather straightforward. since t is antimonotonic  any pair of elements in a consistent subset must also constitute a consistent subset and the two corresponding vertices in gc will share an edge connection.
모the observation implies that a feasible solution to  x t  is also a feasible solution to the mcp instance on gc  thus the feasible region for  x t  is a subset of the feasible region for the mcp instance  which implies that an optimal solution to the mcp instance must be an optimal solution to  x t . therefore  the consistent subset cover number 붺 x t  is lower-bounded by the clique partition number 뷌 gc   which is further lower-bounded by the independence number 붸 gc  as we have discussed in the preliminaries.
모theorem 1. 붸 gc  뫞 뷌 gc  뫞 붺 x t   where gc is the consistency graph for  x t  and t is anti-monotonic.
모based on theorem 1  since 붸 gc  is the size of a maximum independent set in gc  the size of any independent set must be less than or equal to any feasible solution to the mcsc  or mcp  problem. if the two are equal  then the solution is optimal for both. this implication has been used to confirm the optimality of some of our experimental results.
1 extensions
모in the following  we relax the conditions specified in the properties established above so that they can be extended to more applications. for example  converse k-center does not come with an anti-monotonic constraint. the removal of cluster centers may corrupt the consistency of clusters.
모definition 1.  pivot-anti-monotonic constraint  given  x t   we say t is pivot-anti-monotonic if for any subset s   x that is consistent  there exists a pivot element p 뫍 s such that s1 뫋 {p} is consistent for any s1   s.
모example 1. let us consider  x t  for the converse k-center problem  where t requires each cluster to have a radius no larger than a given threshold. t is pivot-anti-monotonic with cluster centers as pivots. as long as the center remains  any sub-cluster of a consistent cluster remains consistent.
모obviously  if t is anti-monotonic  t must be pivot-antimonotonic as well. a consistent subset could have multiple pivots. a pivot of a consistent subset may be present in another as a non-pivot element. the concept of pivot can be extended from a single element to a set of elements  however  we keep the case simple in this study.
모theorem 1. given  x t  where t is pivot-anti-monotonic  and given that s1뫋s1 is consistent if s1 and s1 are consistent subsets sharing the same pivot  any solution to  x t  can be transformed into a solution to the associated partitioning problem with the same or smaller cardinality.
모proof. we give a constructive proof of the theorem. suppose we have a solution to  x t  at hand which is a set of overlapping consistent subsets. we first merge all the consistent subsets sharing the same pivot resulting in a set of consistent subsets such that no two of them share the same pivot. now  even if a pivot may still appear in an overlap of two consistent subsets  it is a pivot for one of them but not both. we just need to make sure the pivot is assigned to the one that needs it. we assign all the non-pivot elements in an overlap to either of the two overlapping consistent subsets and remove them from the other. since t is pivot-anti-monotonic  the consistent subset with the nonpivot elements removed remains consistent. then  we get a set of consistent subsets of  x t  of the same or smaller cardinality that form a partition of x. 
모we also define the so-called pseudo consistency graph for  x t   based on which a similar observation as in observation 1 and a similar conclusion as in theorem 1 can be obtained by following similar arguments.
모definition 1.  pseudo consistency graph  given  x t   a pseudo consistency graph for     is a simple graph where vc1 = x  and there is an edge  u v  뫍 ec1 for a pair of vertices u v 뫍 vc1 if and only if there exists p 뫍 x such that {u v p} is consistent.
모observation 1. given  x t  where t is pivot-anti-monotonic  a consistent subset forms a clique in g1c  where g1c is the pseudo consistency graph for  x t .
모theorem 1. 붸 g1c  뫞 뷌 g1c  뫞 붺 x t   where g1c is the pseudo consistency graph for  x t  and t is pivot-antimonotonic.
모in this paper we focus on mcsc instances with antimonotonic  or pivot-anti-monotonic  constraints. the theoretical results established above as well as our proposed algorithm cag can be applied to such instances. in the following  we introduce several practical data mining problems that have mcsc formulations of this kind.
1. data mining applications
모in this section  we discuss several practical data mining applications that can be formulated as minimum consistent subset cover  mcsc  instances  in particular  the minimum rule set  mrs  problem for rule learning  converse k-clustering  and frequent pattern summarization.
1 the minimum rule set problem
모the mrs problem finds a disjunctive set of if-then rules with the minimum cardinality that cover a given set of labeled examples completely and consistently. a rule covers an example if the attribute values of the example satisfy the conditions specified in the antecedent  if-part  of the rule.
모definition 1.  minimum rule set  given a set x of labeled examples of multiple classes  find a complete and consistent set r of propositional rules for x  i.e.  for each e 뫍 x  there exists some r 뫍 rs that covers e and for each r 뫍 rs  all examples covered by r must have the same class label  such that |rs| is minimized.
모as the most human-comprehensible classification tool  decision rules play a unique role in both research and application domains. in addition to data classification  rules can also be used for the purpose of data description as decision trees . data description focuses on existing data instead of unseen data as in classification  seeking to reduce the volume of data by transforming it into a more compact and interpretable form while preserving accuracy. for both applications  simpler models are preferred. by the widely applied principle of occam's razor  simpler models tend to generalize better to unseen data. from the understandability point of view  simpler models provide more compact and concise descriptions that are easier to comprehend.
모decision trees can be used to extract a set of mutually exclusive rules . the optimal decision tree problem has been studied to induct a perfect tree that correctly classifies all the given examples with some objective optimized  e.g.   . while various objectives have been investigated  a common one is to minimize tree complexity  which can be measured by the number of leaf nodes. the problem we study  mrs  can be accordingly referred to as an optimal rule set problem with an objective of the same kind.
모another popular measure for tree complexity is the total number of tests  internal nodes   which corresponds to the total number of conditions in a rule set. the two measures tend to agree to each other. in the rule set case  fewer number of rules usually lead to fewer number of conditions  as demonstrated in our experimental study.
모most rule learners  e.g.  the aq family  cn1 and ripper   also implicitly reduce the complexity of rule sets in order to achieve good generalization accuracy. however  as pointed out by   the minimality of rule sets has not been a  seriously enforced bias   which is also evident in our experimental comparison study.
모the mcsc formulation. the mrs problem can be formulated as an mcsc instance  x t   where x is the given example set and t requires a subset s   x to form a consistent rule. in particular  s is consistent if its bounding box contains no examples of other classes. since any single example forms a consistent rule  t is granular and  x t  has a non-empty feasible region. in addition  any rule that covers a subset of the examples covered by a consistent rule is also consistent  thus t is also anti-monotonic  and the results in theorem 1 and theorem 1 are directly applicable. recall that theorem 1 gives 붸 gc  뫞 뷌 gc  뫞 붺 x t . in the following  we present an additional mrs-specific result.
모lemma 1. given  x t  for the mrs problem  where x is in d-dimensional space with d 뫞 1  a clique in gc forms a consistent subset of  x t .
모to prove the lemma  it suffices to prove the combination of the bounding boxes of all pairs of vertices in a clique covers the bounding box for the clique  which can be proved by induction on the number of vertices. due to the page limit  we omit the full proof  which can be found in . based on lemma 1  the following theorem immediately follows.
모theorem 1. given  x t  for the mrs problem  where x is in d-dimensional space with d 뫞 1  뷌 gc  = 붺 x t .
모lemma 1 and theorem 1 do not hold for d   1. we construct a counter example for d = 1. let x = {a b c e} where a =  1 1   b =  1 1   c =  1 1   and e =  1 1 . let e be the only negative example. since e is not covered by the bounding box of any pair of examples in s = {a b c}  s is a clique in gc. however  e is covered by the bounding box of s  thus s is not consistent.
모approximate models are useful for both data classification  to avoid overfitting  and data description  to improve interpretability  applications of decision rules. for efficiency  it is desirable for rule learners to generate approximate models during the induction process  instead of post-processing  while keeping its ability to generate perfect models. the mcsc formulation of the mrs problem provides this flexibility  where we can simply adjust t and allow each rule to contain a maximum number of examples of other classes. this constraint is anti-monotonic.
모the rgb rule learner. by solving  x t   we obtain a solution to the mrs problem containing a set of bounding boxes. the bounding boxes  which we call rectangles in the following  are a special type of rules with all the attributional conditions specified. to learn a set of compact rules  of the same cardinality  for the mrs problem  we propose the

input: x  t  and b: x is the example set  a set of multiclass examples. t is a constraint requiring rules to be consistent. b is a user-specified beam search width.
output: r: a compact rule set with redundancy removed.
1: r 뫹 cag x t ; //r stores a set of rectangle rules
1: for each r 뫍 r
1:	initialize b empty rules;
1:	initialize b sets of examples e1 e1 ... eb;
1:	while  e1=   뫇 e1 =1	  ... 뫇 eb =1	  
1: for each ri1  choose the top b conditions from r  each being added to ri1  to form b candidates that eliminate the most examples from ei;
1:	choose the top b rules from the b 뫄 b candidates as
and update e1 e1 ... eb accordingly;
1:	end while
1:	r 뫹 rj1 suppose ej =   caused the loop to terminate;
1: end for

rectangle-based and graph-based algorithm rgb that takes two steps. first  we focus on rectangle rules only and solve the formulated mcsc instance. then  we remove redundant conditions in the rectangle rules. a condition is considered redundant for a rule if its removal will not cause the inclusion of any new examples of other classes or the exclusion of any examples of the same class previously covered by the rule.
모as shown in algorithm 1  rgb first calls cag  fully explained in section 1  for the mcsc instance  x t  and stores the set of learned rectangle rules in r. then for each r 뫍 r  a general-to-specific beam search is performed to remove redundant conditions. to explain the idea  we consider a beam search of width 1. we initialize and maintain a set of examples of other classes  the elimination set for r  that need to be eliminated to achieve consistency. the search starts with an empty rule  i.e.  true  the most general rule  and the conditions to add are chosen from the original conditions in r. for the choice of condition to add  a greedy approach is adopted favoring the condition excluding the largest number of examples from the elimination set of r. the search stops when the elimination set is empty.
모since the conditions to add are chosen from the original conditions in the input rules  the extracted rules are more general than their original ones. since the consistency of each rule is also guaranteed  the resulting compact rule set r is complete and consistent. the runtime of rgb is dominated by cag  which we will discuss in section 1.
모as a rule learner  rgb has many unique and/or desirable properties. unlike most existing methods  rgb does not follow a separate-and-conquer approach  instead  all rules are learned simultaneously. unlike most existing bottom-up methods that start the initial rule set with the example set  rgb starts with a subset containing a maximal number of examples such that no pair of them can co-exist in a single consistent rule  which constitutes a maximal optimal partial solution. unlike many existing methods that learn a set of rules for one class at a time  rgb naturally learns a set of rules for all classes simultaneously. unlike many existing methods that can only learn either perfect models such as early members of the aq family  or approximate models such as cn1 and ripper  rgb has the flexibility to learn both without resorting to post-processing.
1 converse k-clustering
모k-clustering methods generate k clusters that optimize a certain compactness measure  typically distance-based  that varies among different clustering models. while some measures use the sum or average of  squared  distances as in k-means and k-medoid   some measures use a single distance value  radius or diameter  as in k-center  and pairwise clustering . the radius of a cluster is the maximum distance between a fixed point  center  and any point in the cluster  and the diameter is the maximum distance between any two points in the cluster.
모a known limitation of k-clustering is that the appropriate number of clusters k is often hard to specify in advance  and methods that try to automatically determine this number have been investigated . as another approach to address this limitation  alternative clustering models have been explored. in converse k-clustering  a compactness measure is given as threshold  and the task is to minimize the
number of clusters k. converse k-clustering models have not received much attention in the data mining community. however  in many applications a distance-based constraint is easier to provide  based on domain knowledge  than the number of clusters. for example  molecular biologists have the knowledge that how similar a pair of sequences should be so that the two proteins can be assumed sharing the same functionality with high probability  or businessmen have the knowledge that how similar two customers should be so that they would have similar purchasing behaviors. the facility location problem   extensively studied in the operations research community  has the general goal of minimizing the total cost of serving all the customers by facilities. one of the problem formulations minimizes the number of located facilities based on the pairwise distance knowledge of customers  which is precisely a converse k-clustering model.
모the converse k-center and converse pairwise clustering models can both be formulated as mcsc instances. in such an instance  x t   x is the input data points for clustering and t requires a cluster to satisfy the maximum radius or diameter threshold constraint. as single distance measures  radius and diameter are more intuitive for domain experts to specify constraints than the more complex ones.
모both mcsc instances have granular constraints since singleton clusters satisfy any distance threshold. as explained in example 1  converse k-center has a pivot-anti-monotonic constraint with cluster centers as pivots. for converse pairwise clustering  the constraint is anti-monotonic since any sub-cluster of a cluster can only have an equal or smaller diameter than the cluster. the results established in section 1 apply to the two mcsc instances. note that in converse kcenter  a union of two consistent clusters sharing the same center is also consistent  thus the additional condition required by theorem 1 is satisfied and the theorem applies. in addition  we have the following observation.
모observation 1. given  x t  for the converse pairwise clustering problem  we have 뷌 gc  = 붺 x t .
모based on observation 1  a consistent subset of  x t  forms a clique in the consistency graph gc. on the other hand  a clique in gc must form a consistent subset  thus converse pairwise clustering is equivalent to the mcp problem on gc. the same observation does not hold for the converse k-center problem.
1 pattern summarization
모frequent pattern mining has been studied extensively for various kinds of patterns including itemsets  sequences  and graphs. while great progress has been made in terms of efficiency improvements  interpretability of the results has become a bottleneck to successful application of pattern mining due to the huge number of patterns typically generated. a closely related problem is that there is a lot of redundancy among the generated patterns. interpretable and representative summarization of large collections of patterns has emerged as an important research direction.
모as a first approach  maximal frequent patterns  and closed frequent patterns  have been introduced. these subsets of frequent patterns are more concise and allow to derive all the frequent patterns. however  the patterns thus generated are still too many to handle. as an alternative   finds the top-k frequent closed patterns of length no less than a given threshold. while this approach effectively limits the output size  the returned patterns are often not representative of the entire collection of frequent patterns. moreover  these approaches fail to address the redundancy problem.
모some recent work aims at finding a fixed number of patterns representing the whole set of frequent patterns as well as possible. in   the objective is to maximize the size of the part of the input collection covered by the selected k sets.  presents an approach that takes into account not only the similarity between frequent itemsets  but also between their supports. using a similarity measure based on kullback-leibler divergence  they group highly correlated patterns together into k groups.
모similar to the scenario for k-clustering  the appropriate number k of patterns to summarize the set of frequent patterns is often hard to specify in advance. however  the users may have the domain knowledge that how similar a group of patterns should be so that they can be represented as a whole without losing too much information. in light of this  some converse k-clustering models that can be formulated as mcsc instances  converse k-center or converse pairwise clustering  appear to be very reasonable and promising to provide concise and informative pattern summarizations. such clustering models generate clusters  i.e.  groups of patterns  with certain quality guarantee. in such a formulation  the objective is to minimize the number of pattern groups necessary to cover the entire collection of frequent patterns  which is natural for the purpose of summarization since it maximizes interpretability of the result representing the entire collection and at the same time reduces redundancy. for the radius or diameter threshold  standard distance functions can be employed  such as the jaccard's coefficient for itemsets or edit distance for sequential patterns.
1. the generic cag algorithm
모in this section  we introduce a generic algorithm cag that works with consistency graphs and assignment graphs to solve minimum consistent subset cover  mcsc  instances featuring anti-monotonic constraints.
1 overview
모given an mcsc instance  x t  where the constraint t is anti-monotonic  cag starts by constructing a maximal optimal partial solution from the consistency graph gc  then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph ga to learn a set of consistent subsets with small cardinality.
모the design of cag has closely followed the insights provided in section 1. from the definition of gc and theorem 1  we know that any pair of vertices in an independent set of gc cannot appear in the same consistent subset. thus a maximal independent set of gc  denoted by is  constitutes a maximal optimal partial solution to  x t .
모each vertex in is forms a singleton consistent subset  represented by a so-called condensed vertex in an assignment graph ga. the rest of the vertices in ga are called element vertices. ga is defined such that there is an edge between an element vertex u and a condensed vertex v if and only if u can be assigned to v while maintaining the consistency of v. since a maximal independent set is also a dominating set as discussed in the preliminaries of section 1  there is an edge for each u connecting to some v in the initial ga.
모element vertices are processed in a sequential manner and assigned to condensed vertices. with the growth of condensed vertices  some element vertices would get isolated and new condensed vertices have to be created for them. upon completion  ga becomes edge-free with all vertices assigned  and the set of condensed vertices  each representing a consistent subset  constitute a solution for  x t .
모the dynamically maintained ga provides the necessary information based on which effective evaluation measures can be designed to guide the search by deciding which element vertex is the next to be assigned to which condensed vertex. for example  with the least degree first criterion  we can process first the element vertex with the least degree since it is the most likely one to get isolated.
모note that cag actually returns a partition of x. as argued in section 1  a solution to a partitioning problem is also a feasible solution to its corresponding covering problem. also due to theorem 1 and theorem 1  partitioning does not cause the increase of solution size compared to covering under certain conditions. in addition  partitioning problems have much smaller search spaces and the search can be better guided.
모also note that under anti-monotonic constraints  if a subset s   x is not consistent  s1   s cannot be consistent. thus such inconsistent s does not need to be considered and the search space can be significantly pruned. our assignment graph ga maintains consistency of all condensed vertices after each assignment transaction allowing cag to work in a pruned search space.
모in contrast to the most-seen separate-and-conquer approach  e.g.  the greedy algorithm  for set covering and most existing rule learners  cag adopts a less greedy exampledriven strategy to learn consistent subsets simultaneously. in summary  the construction of initial optimal partial solution  the use of assignment graph  and the example-driven simultaneous learning strategy are the three novel design ideas that account for the good performance of cag.
1 assignment graph
모in the following  we define and explain assignment graphs  a unique element of cag that helps to guide the search.
모definition 1.  assignment graph  in a bipartite assignment graph ga =  ua 뫋 va ea   ua contains a set of element vertices and va contains a set of condensed vertices each representing a consistent subset.  u v  뫍 ea if and only if u 뫍 ua  v 뫍 va  and v 뫋 {u} is consistent.
모in cag  ga is dynamically maintained showing all the feasible choices for the next assignment. each condensed vertex is essentially a set of vertices condensed together. in order to maintain the consistency of subsets  an element vertex u can be assigned to some condensed vertex v only via an edge connection between them. however  each assignment may cause some edges to disappear in ga. for those isolated element vertices  with degree of 1   new condensed vertices have to be created. each creation may introduce some new edges to ga.
모figure 1  c  shows an initial ga and  d - f  show its dynamic evolvement along the assignment process  for which we will provide further explanations later in this section.
모observation 1. while a vertex assignment may cause multiple edge gains or losses in ga  it can cause at most one edge gain or loss for any single element vertex.
모the observation holds because all the gained or lost edges have to be incident to the condensed vertex involved in an assignment  and any element vertex shares at most one edge with a condensed vertex. the implication of the observation is that if an element vertex has a degree of more than two  it will not become isolated after the next assignment.
모evaluation measures. in the following  we show how the information embedded in ga can help to guide the search by answering important questions in the assignment process: which element vertex is the next to be assigned  which condensed vertex should it be assigned to  in principle  the element vertex with the least degree should be considered first since it is most likely to get isolated. also based on observation 1  element vertices with degree more than two will stay  safe  after such an assignment. thus  a least degree first criterion can be used to choose the next element vertex to be assigned.
모in addition  we want to keep as many edges as possible in ga since they are the relational resources for the assignment of element vertices. edges have different degrees of importance. in general  a gained or lost edge is more important if it is incident to an element vertex with a smaller degree in ga. we design a measure  weighted edge gain  that reflects the influence of an assignment transaction on ga.
모in the following  we define weg uu  and weg uv . weg uu  is the weighted number of edges that can be added to ea if u is chosen to become a new condensed vertex. in such cases  u is assigned to itself  which happens only when d  u  = 1  where d  u  denotes the degree of u in ga. weg uv  is the weighted number of edges that would get lost  an edge loss is just a negative gain  if u is assigned to v via edge  u v  뫍 ea.
for weg uu   the weight for each possibly added edge is  where we use d  u1  + 1 because d  u1  can be 1 and for that case  the weight should be the largest  i.e.  1. for both measures  different weighting schemes can be used.

모once u  the next element vertex to be assigned  is chosen  a largest weg uv  first criterion can be used to decide which condensed vertex v that should take u so as to keep as many weighted edges as possible.
모we also define the following measure weg u  that measures the influence on ga if u is chosen as the next element vertex to be assigned. the associated largest weg u  first criterion can be used alone for the choice of u. it can also be used as a tie-breaking mechanism for the least degree first criterion in choosing .
 	weg uu 	if d  u  = 1 where v
otherwise
모in the formula  for d  u  = 1  weg u  = weg uu . for other cases  weg u  is the largest weg uv  value considering all the condensed vertices adjacent to u in ga  indicating the best assignment choice for u.
1 generic cag
모we have explained the working principles of cag in the overview. in the following we present the algorithm and explain it in more details.
algorithm 1 generic cag
input:  x t : an mcsc instance.
output: va: a set of condensed vertices representing a set
of consistent subsets that cover x.
1: initialize ga =  ua 뫋 va ea ; 1: while  ua 1=   1:choose u 뫍 ua;1:if  d  u  = 1  then1:va 뫹 va 뫋 {u};1:ua 뫹 ua {u};1:ea 뫹 ea 뫋 { u u1 } for each u1 with u1 뫍 ua and
{u u1} is consistent;1:else1:choose v 뫍 va;1:v 뫹 v 뫋 {u};1:ua 뫹 ua {u};1:ea 뫹 ea { u u1 } for each u1 with  u1 v  뫍 ea and v 뫋 {u u1} is inconsistent;1:end if1: end while
모cag starts by initializing the assignment graph ga using a maximal independent set of gc  line 1 . then  the element vertices are processed in a sequential manner  line 1 . for each assignment  a decision is made on which u 뫍 ua is the next element vertex to be assigned  line 1 . after u is chosen  if it has a degree of 1  line 1   a new condensed vertex has to be created for u  line 1  and u will be removed from ua  line 1 . at this moment  ea needs to be updated  line 1 . some element vertices  that are connected to u in ec  may be able to connect to the newly added condensed vertex u resulting in creation of some new edges. if u is not an isolated vertex  line 1   another decision is to be made on which v 뫍 va should take u  line 1 . after the assignment of u to v  lines 1  1   ea also needs to be updated  line 1 . because of the assignment of u to v  some element vertices  say u1  that previously connected to v would lose the connection if v 뫋 {u u1} is inconsistent. upon completion  ua =   and ea =  . the set of condensed vertices va corresponds to a set of consistent subsets together covering x.
모initializing ga. one way of initializing ga is to derive the consistency graph gc for  x t  first then find a maximal independent set is of gc using some known heuristic  e.g.  greedy-is introduced in . with is obtained  we let va = is  ua = vc is  and  u v  뫍 ea if and only if u 뫍 ua  v 뫍 va and  u v  뫍 ec.
모in some cases  e.g  for the mcp problem  gc is identical to the input graph. in other cases  gc needs to be derived and this takes o |x|1tp  time where tp denotes the consistency checking time for a pair of vertices in x. this is because each pair in x needs to be checked for consistency. to improve efficiency  we provide another way of initializing ga without actually deriving gc. initially  va and ea are empty and ua = x. for each u 뫍 ua  we check each v 뫍 va and if v뫋{u} is consistent  we add edge  u v  into ea. if no edge can be added  we add u into va. upon completion  va also contains a maximal independent set is of gc. this procedure takes o |is||x|tp . note that usually |is|   |x|.
모choosing u and v. by varying the decision making schemes  i.e.  how to choose u  line 1  and v  line 1   the generic cag can have different instantiations that are suitable for different applications. by default  we use the least degree first criterion to choose u with the largest weg u  first criterion for tie-breaking. also  we use the largest weg uv  first criterion to choose v.
모with the default scheme  the time spent in choosing u would be on calculating weg u  for tie-breaking. as an approximation  we adopt a sampling technique. we only consider a constant number of element vertices with the tied least degree and for each of them  say u  we only consider a constant number of element vertices for consistency checking  each of which  say u1  connects to some condensed vertex v that is adjacent to u in ga. this sampling technique works well because the least degree first criterion greatly reduces the number of vertices in consideration in calculating weg u . therefore  the runtime for the choice of u is o tc   where tc is the consistency checking time for v뫋{u u1} given that v 뫋 {u} and v 뫋 {u1} are consistent.
모for the choice of v  the calculation of weg uv  is part of the calculation of weg u  and thus it takes o tc  as well.
모updating ga. by varying the ga updating mechanisms  lines 1 and 1   the generic cag can be adapted to different applications. these applications have different constraints requiring different consistency checking mechanisms.
모in line 1  the edge set ea of ga is updated after an isolated element vertex u becomes a new condensed vertex. this may introduce some new edges to ea. note that line 1 will be executed no more than |va|   |is| times where va here represents its final stage after cag terminates  thus the total time spent on line 1 is at most o  |va|   |is| |x|tp .
모in line 1  the edge set ea of ga is updated after an element vertex u is assigned to a condensed vertex v. this may cause the loss of some edges incident to v. each such update could take a worst case runtime of o |x|tc . to reduce the runtime  we adopt a late update strategy that significantly improves efficiency without sacrificing much performance. this is based on the observation that the edge update operations do not have an impact on the assignment process if the corresponding element vertices are not considered in the next assignment. given the least degree first criterion  we only need to perform edge update for those element vertices with the least  or small  degrees since they are likely to be considered in the next assignment. this late update strategy would leave some extra edges in ga that should have been removed. however  these edges will be removed sooner or later when their corresponding member vertices get closer

	a.	a.
	{e d f}	{e d f}
	..	.
{c b a}
{ }c . { }c . .  d   e   f 
figure 1: running example.
and closer to be chosen along the assignment process. therefore  by checking consistency for a constant number of vertices connected to v  we only spend o tc  time for this update without sacrificing much performance.
모time complexity. as we have explained previously  initializing ga  line 1  takes o |is||x|tp . the update of ga for the d  u  = 1 case  line 1  takes in total o  |va|   |is| |x|tp  time. together they take o |va||x|tp  time.
모in each iteration  the choice of u  line 1   the choice of v  line 1  and the update of ga for the d  u  = 1 case each takes tc time. there are o |x|  iterations and thus together they take o |x|tc  time.
모overall  the worst case runtime is o |va||x|tp + |x|tc . in general  |va|   |x| as can be observed from our experimental results in table 1.
모the consistency checking time tp and tc are different for different mechanisms and different applications. for the mcp problem and the converse pairwise clustering problem  both tp and tc are constant. thus  the worst case runtime for the two problems is o |va||x| .
모for the mrs problem  both tp and tc are o |x|  for the brute-force consistency checking mechanism  which can be improved to log time on average by employing indexing techniques. the worst case runtime for the mrs problem is o |va||x|1 . with the help of indexing  the average case runtime is o |va||x|log|x| .
모running example. in figure 1  we provide a running example demonstrating how cag works on a set covering instance  x t . by definition   x t  does not have an antimonotonic constraint since we are allowed to use the given subsets  not their subsets  in covering x. however  if s1 is a subset of some given subset s   x  s1 can be considered  consistent  since it can always be replaced by s. thus  our generic algorithm cag is perfectly applicable.
모in the figure  the  x t  instance is given in  a . we can see that the greedy algorithm will select all the three given subsets. for clarity  we also show gc for  x t  in  b   where we can identify a maximal is containing c and e.  c  is the initial ga with c and e as the condensed vertices.
모now we start the assignment process. both d and f have the least degree of 1  but assigning d to {e} would not cause any edge loss  thus d is chosen and assigned to {e}  and  d  shows the updated ga. next  f is chosen and assigned to {e d}  and  e  shows the updated ga. we can see that ga lost two edges since neither a nor b can join {e d f} while maintaining its consistency  i.e.  {e d f a} and {e d f b} are inconsistent. afterwards  a and b are assigned to {c} and  f  shows the final ga  where both ua and ea are empty and va contains the condensed vertices corresponding to the consistent subsets that cover x.
모solving converse k-center. given  x t  for the converse k-center problem  we have explained previously that t is pivot-anti-monotonic. also   x t  satisfies the additional condition specified in theorem 1  that is  two consistent subsets of  x t  sharing the same pivot can merge into a single consistent subset. based on the theoretical study  we can use the following approach to solve  x t . first  we apply cag where we use the pseudo consistency graph g1c instead of gc to derive ga. then we assign the needed pivots to the learned condensed vertices to obtain consistent subsets. then we can obtain a partition of x by following the constructive method described in the proof of theorem 1.
1. experimental evaluation
모to experimentally evaluate the performance of cag  we have considered two mcsc applications  the minimum rule set  mrs  and minimum clique partition  mcp  problems. for the mrs problem  we compared cag with a popular rule learner aq1   the latest release of the famous aq family  on uci datasets . in this series of experiments  rgb achieved about 1% reduction in the number of rules and 1% reduction in the number of conditions.
모observation 1 and theorem 1 show that the mcp problem is related to all the mcsc instances within the scope of this study. the converse pairwise clustering problem  which can also be applied to frequent pattern summarization  is equivalent to the mcp problem on the consistency graph. since mcp is a dual problem of graph coloring  many wellknown graph coloring heuristics and benchmarks set up an ideal platform to evaluate the performance of cag. the comparison partner we chose  dsatur   is among the most popular ones with top performance. experiments on over 1 dimacs benchmarks  showed that cag outperformed dsatur in general  and in particular for the 1 hard datasets  cag used 1% fewer cliques on average.
1 mrs results
모rgb is our proposed rule learner that first calls cag to learn a set of rectangle rules and then extracts compact rules with redundant conditions removed. our comparison partner  aq1   is the latest release of the famous aq family. other popular rule learners such as cn1 and ripper do not return perfect rule sets. since most rule learners follow a separate-and-conquer approach originated from aq  the performance of aq1 can well represent the performance of most rule learners.
모although rgb can work on both numerical and categorical data in principle  we have focused on numerical data in this implementation since rectangle rules provide good generalization on numerical attributes only. thus  1 numerical datasets without missing values were chosen from the uci repository  for this series of experiments.
모table 1 presents the results. the columns in the table are dataset  ins  number of instances   dim  dimensionality   cla  number of classes   aq1  aq1 results   rgb  rgb results   and reduction  reduction achieved by rgb compared
to aq1  where reduction = aq1 resultaq1 resultrgb result .
모for the columns aq1 and rgb  rul and con indicate the total number of rules and conditions respectively. the rul and con reductions are given under the reduction column. in addition  the columns |ec|  |is| and cliques indicate the edge set size  the maximal independent set size and the number of cliques of gc discovered by rgb respectively.
모from table 1 we see that rgb achieved 1% and 1% averaged reductions over aq1 in terms of the total number of rules and conditions respectively. in the last row of the table  the reductions were calculated for the averaged number of rules and conditions  where we see that rgb achieved 1% and 1% reductions over aq1 in terms of the averaged number of rules and conditions respectively. consistent to theorem 1  although many results are not optimal  each dataset in table 1 exhibits the same trend  that is  |is| 뫞 cliques 뫞 rgb rul 뫞 aq1 rul. as discussed in section 1  theorem 1 can help to confirm the optimality of mcsc results. in table 1 we can see that the is  cliques and rgb results for datasets balance-scale  glass  and iris are optimal. also  the is and cliques results for ionosphere  new-thyroid  sonar  vowel  waveform  and wine are optimal. in addition  we studied the trend of reduction by varying data complexity measured by ins  |ec| and dim. while the detailed results can be found in   in summary  the reductions in rul and con both increase with the increase of data complexity  which also confirms that the two model complexity measures for rule sets  number of rules and number of conditions  are consistent with each other.
1 mcp results
모the mcp problem is a dual problem of graph coloring. algorithms for graph coloring  mcp  can be applied to complementary graphs to return solutions to the mcp  graph coloring  problem. in this series of experiments  we have used dsatur  as our comparison partner. dsatur is among the most popular construction heuristics for graph coloring with top performance reported in the literature.
모the experiments were performed on dimacs benchmarks   for some of which the optimal solutions are known. we divide the benchmarks into two categories: easy and hard. as the rule of thumb for the division  those datasets with optimal solutions relatively easy to obtain are considered easy; otherwise hard. we used 1 easy and 1 hard datasets for our experiments. due to the page limit  we only provide a summary of the results in table 1. from the table we can see that out of the 1 easy datasets  dsatur found 1 while cag found all the 1 optimal solutions.
table 1: mcp results
category# datasetsdsaturcageasy1  optimal 1  optimal hard1  1 tied 1  1 tied 모the optimal solutions for many hard datasets are not known. table 1 reports the winning times of the two methods. from the table we can see that out of the 1 hard datasets  dsatur won 1 times while cag won 1 times where they tied for 1 datasets. to more accurately capture their performance difference  for each dataset  we calculated the reduction of cliques achieved by cag using the formula reduction = dsatur resultdsatur result  cag result. then  we calculated the averaged reduction over the 1 hard datasets to obtain a value of 1%  meaning that cag used about 1% fewer cliques than dsatur on average for each dataset.
datasetinsdimclaaq1|ec||is|cliquesrgbreductionrul	conrul	conrul	contable 1: mrs results
balance-scale11	1111 -1bupa11	1111.1.1car11	1111.1.1diabetes11	1111 1ecoli11	1111.1 -1glass11	1111.1 -1haberman11	1111.1 -1ionosphere11	1111.1.1iris11	1111.1.1letter11 1111 1new-thyroid11	1111.1.1page-block11	1111 1satimage11	1111 1segment11	1111.1.1sonar11	1111.1.1spambase11	1111 1vehicle11	1111 1vowel11	1111.1.1waveform11	1111 1wine11	1111.1.1yeast11	1111 1average1 1111.1.1.1.1reduction11 1모we did not compare the runtime partly because cag and dsatur work on different graphs  g and g  and the complexities of the two graphs are opposite to each other. however  we note that while dsatur has a cubic runtime   for the mcp problem  cag has a subquadratic runtime of o |va||x|   where |va|   |x|  |va| = cliques and |x| = ins  as can be observed from table 1.
1. conclusion
모in this paper  we introduced and studied the minimum consistent subset cover  mcsc  problem  which generalizes the set covering problem and has many practical data mining applications in the areas of rule learning  converse kclustering  and frequent pattern summarization. for mcsc instances with anti-monotonic constraints  our generic algorithm cag was provided in accordance with the theoretical insights featuring many novel design ideas  e.g.  the construction of maximal optimal partial solution  the use of dynamically maintained bipartite assignment graph  and the example-driven simultaneous learning strategy. our experimental evaluation on benchmark datasets justified the performance of cag.
모for future work  we would like to further explore and experimentally evaluate the mcsc applications on converse k-clustering and frequent pattern summarization. we are also interested in seeking other data mining applications that can be formulated as mcsc instances and solved by our proposed cag search framework.
