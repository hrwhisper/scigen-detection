a central goal of robotics and ai is to be able to deploy an agent to act autonomously in the real world over an extended period of time. it is commonly asserted that in order to do so  the agent must be able to learn to deal with unexpectedenvironmental conditions. however an ability to learn is not sufficient. for true extended autonomy  an agent must also be able to recognize when to abandon its current model in favor of learning a new one; and how to learn in its current situation. this paper presents a fully implemented example of such autonomy in the context of color map learning on a vision-based mobile robot for the purpose of image segmentation. past research established the ability of a robot to learn a color map in a single fixed lighting condition when manually given a  curriculum   an action sequence designed to facilitate learning. this paper introduces algorithms that enable a robot to i  devise its own curriculum; and ii  recognize when the lighting conditions have changed sufficiently to warrant learning a new color map.
1 motivation
mobile robotic systems have recently been used in fields as diverse as medicine  rescue  and surveillance  1; 1 . one key enabler to such applications has been the development of powerful sensors such as color cameras and lasers. however  with these rich sensors has come the need for extensive sensor calibration  often performed manually  and usually repeated whenever environmental conditions change significantly.
¡¡here  we focus on the visual sensor  camera   arguably the richest source of sensory information. one important subtask of visual processing is color segmentation: mapping each image pixel to a color label. though significant advances have been made in this field  1; 1   most of the algorithms are computationally expensive to implement on a mobile robot and/or involve a time consuming off-line preprocessing phase. furthermore  the resulting segmentation is typically quite sensitive to illumination variations. a change in illumination could require a repetition of the entire training phase.
¡¡past research established that a robot can efficiently train its own color map based on knowledge of the locations of colored objects in the environment  but only when manually given a sequence of actions to execute while learning  a curriculum  . separately  it has been shown that a robot can recognize illumination changes and switch among color maps at appropriate times  given a fixed set of pre-trained color maps . the prior work was also limited to controlled environments with only solid-colored objects.
¡¡this paper significantly extends these results by enabling a robot i  to recognize when the illumination has changed sufficiently to require a completely new color map rather than using one of the existing ones; and ii  to plan its own action sequence for learning the new color map on-line. furthermore  we introduce a hybrid color-map representation that enables the robot to learn in less controlled environments  including those with textured surfaces. all algorithms run in real-time on the physical robot enabling it to operate autonomously in an uncontrolled environmentwith changing illumination over an extended period of time.
1 problem specification
here  we formulate the problem and describe our solution. section 1 presents the hybrid color-maprepresentation used for autonomous color learning. section 1 describes our approach to detecting significant illumination changes.
1 what to learn: color model
to be able to recognize objects and operate in a color-coded world  a robot typically needs to recognize a discrete number of colors  ¦Ø ¡Ê  1 n   1  . a complete mapping identifies a color label for each point in the color space:

where c1 c1 c1 are the color channels  e.g. rgb  ycbcr   with the corresponding values ranging from 1   1.
¡¡we start out modeling each color as a three-dimensional  1d  gaussian with mutually independent color channels. using empirical data and the statistical technique of bootstrapping   we determined that this representation closely approximates reality. the gaussian model simplifies calculations and stores just the mean and variance as the statistics for each color  thereby reducing the memory requirements and makingthe learningprocess feasible to executeon mobile robots with constrained resources.
¡¡the a priori probability density functions  color ¦Ø ¡Ê  1 n   1   are then given by:
 1 
where  ci ¡Ê  cimin = 1 cimax = 1 represents the value at a pixel along a color channel ci while ¦Ìci and ¦Òci represent the corresponding means and standard deviations.
¡¡assuming equal priors  p ¦Ø  = 1/n   ¦Ø ¡Ê  1 n   1    each color's a posteriori probability is then given by:
	p ¦Ø|c1 c1 c1  ¡Ø p c1 c1 c1|¦Ø 	 1 
the gaussian model for color distributions  as described in our previous work   performs well inside the lab. in addition  it generalizes well with limited samples when the color distributions are actually unimodal; it is able to handle minor illumination changes. however  in settings outside the lab  factors such as shadows and illumination variations cause the color distributions to be multi-modal; the robot is now unable to model colors properly using gaussians.
¡¡in order to extend the previous work to less controlled settings  we propose a hybrid color representation that uses gaussians and color histograms. histograms provide an excellent alternative when colors have multi-modal distributions . here  the possible color values  1 along each channel  are discretized into bins that store the count of pixels that map into that bin. a 1d histogram can be normalized to provide the probability density function:
		 1 
where b1  b1  b1 represent the histogram bin indices corresponding to the color channel values c1  c1  c1  and sumhistv als¦Ø is the sum of the values in all the bins of the histogram for color ¦Ø. the a posteriori probabilities are then given by equation 1.
¡¡unfortunately  histograms do not generalize well with limited training data  especially for samples not observed in the training set  such as with minor illumination changes. resource constraints prevent the implementation of operations more sophisticated than smoothing. also  histograms require more storage  wasteful for colors that can be modeled as gaussians. we combine the two representations such that they complement each other: colors for which a 1d gaussian is not a good fit are modeled using 1d histograms. the goodness-of-fit decision is made online  for each color.
¡¡samples for which a 1d gaussian is a bad fit can still be modeled analytically using other distributions  e.g. mixture of gaussians  weibull  through methods such as expectationmaximization . but most of these methods involve parameter estimation schemes that are computationally expensive to perform on mobile robots. hence  we use a hybrid representation with gaussians and histograms that works well and requires inexpensive computation. in addition  the robot automatically generates the curriculum  action sequence  based on the object configuration  as described in section 1.
1 when to learn: detecting illumination changes
to detect significant changes in illumination  we need a mechanism for representing illumination conditions and for differentiating between them.
¡¡we hypothesizedthat images from the same lighting conditions would have measurably similar distributions of pixels in color space. the original image is in the ycbcr format  with values ranging from  1 along each dimension. to reduce storage  but still retain the useful information we transformed the image to the normalized rgb space   r g b :
r+1	g+1	b+1
since r + g + b = 1  any two of the three features are a sufficient statistic for the pixel values. we represent a particular illumination condition with a set of distributions in  r g  space  quantized into n bins in each dimension  corresponding to several images captured by the robot.
¡¡next  we need a well-defined measure capable of detecting the correlation between discrete distributions. based on experimental validation  we use kl-divergence  an entropybased measure. for two distributions a and b in the 1d  r g  space  n being the number of bins along each dimension:
		 1 
the more similar two distributions are  the smaller is the kldivergence between them. since kl-divergence is a function of the log of the observed color distributions  it is reasonably robust to large peaks in the observed color distributions and is hence less affected by images with large amounts of a single color. the lack of symmetry in kl-divergence is eliminated using the resistor-average kl-divergence  ra-kld  .
¡¡given a set of distributionscorrespondingto m differentillumination conditions  we have previously shown  that it is possible to effectively classify the distribution corresponding to a test image into one of the illumination classes. a major limitation was that we had to know the illumination conditions in advance and also had to provide manually trained color maps for each illumination. here  we make a significant extension in that we do not need to know the different illumination conditions ahead of time.
¡¡for every illumination condition i  in addition to a set of  r g  distributions  rgsamp i    we calculate the ra-kl distances between every pair of  r g  distributions to get a distribution of distances   di   which we model as a gaussian. when the illumination changes significantly  the average rakl distance between a test  r g  distribution and rgsamp i  maps to a point well outside the 1% range of the intraillumination distances  di . this feature is used as a measure of detecting a change in illumination.
1 algorithms: when  what  how to learn
our algorithms for color learning and adaptation to illumination change are summarized in algorithm 1 and algorithm 1.
¡¡algorithm 1 enables the robot to decide when to learn. the robot first learns the color map for the current illumination by generating a curriculum using the world model  as described in algorithm 1. next  it represents this illumination condition algorithm 1 adapting to illumination change - when to learn 

require: for each illumination i ¡Ê  1 m   1   color map and distribution of ra-kld distances di.
1: begin: m = 1  current = m.
1: generate curriculum and learn all colors - algorithm 1.
1: generate rgsamp current   n  r g  space distributions  and distribution of ra-kld distances  dcurrent. 1: save color map and image statistics  m = m + 1.
1: if currenttime   testtime ¡Ý timeth then
1:	rgtest = sample  r g  test distribution.
1:	for
1:	test    = n	 rgtest rgsamp       
1:	end for
1: if davgtest current  lies within the threshold range of dcurrent then
1:	continue with current color map.
1: else if davgtest i  lies within the range of current then
1:	use corresponding color map  current = i.
1: else if  i ¡Ê  1 m   1  davgtest i  lies outside the range of di then
1:	re-learn color map autonomously: algorithm 1.
1:	save  r g  distributions for new illumination.
1:	transition to the new color map for subsequent operations.
1:	current = m  m = m + 1.
1:	end if
1:	testtime = currenttime.
1: end if

by collecting sample image distributions in  r g  and computing the distribution of ra-kl distances  dcurrill.
¡¡periodically  timeth = 1   the robot generates a test distribution  rgtest  and computes its average distance to each set of previously stored distributions  rgsamp i . if davgtest i  lies within the threshold range  1%  of the corresponding di  the robot transitions to the corresponding illumination condition. but  if it lies outside the threshold range of all known distribution of distances  the robot learns a new color map and collects image statistics  which are used in subsequent comparisons. changing the threshold changes the resolution at which the illumination changes are detected but the robot is able to handle minor illumination changes using the color map corresponding to the closest illumination condition  see section 1 . with transition thresholds to ensure that a change in illumination is accepted iff it occurs over a few frames  it also smoothly transitions between the learned maps. the algorithm requires no manual supervision.
¡¡next  we briefly describe the planned color learning algorithm  algorithm 1  used in lines 1 and 1 of algorithm 1. our previous algorithm   lines 1 1   1  had the robot move along a prespecified motion sequence  and model each color as a 1d gaussian. but  outside the controlled lab setting  some color distributions are multi-modal and cannot be modeled effectively as gaussians. the current algorithm significantly extends the previous approach in two ways. it automatically chooses between two representations for each algorithm 1 autonomous color learning - how to learn 

require: known initial pose and color-coded model of the robot's world - objects at known positions. these can change between trials.
require: empty color map; list of colors to be learned.
require: arrays of colored regions  rectangular shapes in
1d; a list for each color  consisting of the properties  size  shape  of the regions of that color.
require: ability to approximately navigate to a target pose  x y ¦È .
1: i = 1 n = maxcolors
1: timest = currtime  time - the maximum time allowed to learn each color.
1: while i   n do
1:	color = bestcolortolearn  i  ;
1:	targetpose = besttargetpose  color  ;
1:	motion	targetpose  
1:	perform motion {monitored using visual input and localization}
1:	if  then
1:	collect	samples	from	the	candidate	region 
observed.1:if possiblegaussianfit observed  then1:learngaussparams  colors i   1:learn mean and variance from samples1: 1:	elselearnhistvals{ 1d gaussian not a good fit to samples  colors i   	}1: update the color's 1d histogram using the samples1:end if1:updatecolormap  
1:if !valid  color   then1:removefrommap  color  
1:end if1:else1:rotate at target position.1:end if1: 1:if	currtime   timest	¡Ý	time color  rotationangle ¡Ý angth then i = i + 1or1:timest = currtime1:end if1: end while
1: write out the color statistics and the color map.
color to facilitate color learning outside the lab: it decides what to learn. it also automatically determines how to learn  i.e. it generates the curriculum for learning colors  for any robot starting pose and object configuration.
¡¡the robot starts off at a known location without any color knowledge. it has a list of colors to be learned and a list of object descriptions corresponding to each color  size  shape  location of regions . though this approach does require some human input  in many applications  particularly when object locations change less frequently than illumination  it is more efficient than hand-labeling several images. to generate the curriculum  the robot has to decide the order in which the colors are to be learned and the best candidate object for learning a particular color. the algorithm currently makes these decisions greedily and heuristically  i.e. it makes these choices one step at a time without actually planning for the subsequent steps. the aim is to get to a large enough target object while moving as little as possible  especially when not many colors are known. the robot computes three weights for each object-color combination  c i :
w1 = fd  d c i     w1 = fs  s c i     w1 = fu  o c i   
 1 
where the functions d c i   s c i  and o c i  represent the distance  size and object description for each color-object combination. function fd  d c i    assigns larger weights to smaller distances  fs  s c i    assigns larger weights to larger candidate objects  and fu  o c i    assigns larger weights iff the object i can be used to learn color c without having to wait for any other color to be learned or object i consists of color c and other colors that have already been learned.
the bestcolortolearn    line 1  is then given by: 
where the robot parses through the different objects available for each color  nc  and calculates the weights. once a color is chosen  the robot determines the best target for the color  using the minimum motion and maximum size constraints:

for a chosen color  the best candidate object is the one with the maximum weight for the given heuristic functions. the robot chooses the besttargetpose    line 1  to learn color from this object and moves there  lines 1 . it searches for candidate image regions satisfying a set of constraints based on current robot location and target object description. if a suitable image region is found  targetregionfound   - line 1   the pixels in the region are used as samples  observed  to verify goodness-of-fit with a 1d gaussian  line 1 . the test is done using bootstrapping  using kl-divergence as the distance measure  as described in algorithm 1.
¡¡if the samples generate a good gaussian fit  they are used to determine the mean and variance of the color distribution  learngaussparams   - line 1 . if not  they are used to populate a 1d histogram  learnhistvals   - line 1 . the learned distributions are used to generate the color map  the mapping from the pixel values to color labels  line 1 . the robot uses the map to segment subsequent images and find objects. the objects help the robot localize to positions suitable for learning other colors  and to validate the learned colors and remove spurious samples  lines 1 .
¡¡to account for slippage and motion model errors  if a suitable image region is not found  the robot turns in place to find it. if it has rotated in place for more than a threshold angle  angth = 1o  and/or has spent more than a threshold amount of time on a color  time color  ¡Ö 1sec   it transitions to the next color in the list. instead of providing algorithm 1 possiblegaussianfit    line 1 algorithm 1 - what to learn 

1: determine maximum-likelihood estimate of gaussian parameters from samples  observed.
1: draw n samples from gaussian - estimated  n = size of observed.
1: dist = kldist observed estimated .
1: mix observed and estimated - data  1n items.
1: for i = 1 to numtrials do
1: sample n items with replacement from data - set1  remaining items - set1.
1:	disti = kldist set1 set1 
1: end for
1: goodness-of-fit by p-value: where dist lies in the distribution of disti.

a color map and/or the action sequence each time the environment or the illumination changes  we now just provide the positions of objects in the robot's world and have it plan its curriculum and learn colors autonomously. the adaptation to illumination changes makes the entire process autonomous. a video of the robot learning colors can be seen online: www.cs.utexas.edu/¡«austinvilla/ p=research/auto vis.
1 experiments
we first provide a brief overviewof the robotic platform used  followed by the experimental results.
1 experimental platform
the sony ers-1 aibo is a four legged robot whose primary sensor is a cmos camera located at the tip of its nose  with a limited field-of-view  1o horz.  1o vert. . the images  captured in the ycbcr format at 1hz with a resolution of 1 ¡Á 1 pixels  possess common defects such as noise and distortion. the robot has 1 degrees-of-freedom three in each leg  three in its head  and a total of five in its tail  mouth  and ears. it has noisy ir sensors and wireless lan for interrobot communication. the legged as opposed to wheeled locomotion results in jerky camera motion. all processing for vision  localization  motion and action selection is performed on-board using a 1mhz processor.
¡¡one major application domain for the aibos is the robocup legged league   a research initiative in which teams of four robots play a competitive game of soccer on an indoor field ¡Ö 1m¡Á 1m. but applications on aibos and mobile robots with cameras typically involve an initial calibration phase  where the color map is produced by hand-labeling images over a period of an hour or more  section 1 . our approach has the robot autonomously learning colors in less than five minutes and adapting to illumination changes.
1 experimental results
we tested our algorithm's ability to answer three main questions: when to learn - the ability to detect illumination changes  how to learn - the ability to plan the action sequence to learn the colors  and how good is the learning the segmentation and localization accuracy in comparison to the standard human-supervised scheme.
when to learn 
first  we tested the ability to accurately detect changes in illumination. the robot learned colors and  r g  distributions corresponding to an illumination condition and then moved around in its environment chasing a ball. we changed the lighting by controlling the intensity of specific lamps and the robot identified significant illumination changes.
 % changechangecchange11changec11table 1 presents results averaged over 1 trials with the rows and columns
table 1: illumination change detection: representing the few errors in 1 trials. ground truth and observed values respectively. there are very few false positives or false negatives. the errors due to highlights and shadows are removed by not accepting a change in illumination unless it is observed over a few consecutive frames.
¡¡to test the ability to transition between known illuminations  the robot learned colormaps and statistics for threeconditions: bright 1lux   dark 1lux   interim 1lux .
illum.transition accuracycorrect  % errorsbright11dark1interim11the intensity of the overhead lamps was changed to one of the three conditions once every ¡Ö 1 sec. table 1 shows results av-
eraged over ¡Ö 1 table 1: illumination transition accu¡Ö trials each. the few racy: few errors in 1 trials. false transitions  due to shadows or highlights  are quickly corrected in the subsequent tests. when tested in conditions in between the known ones  the robot finds the closest illumination condition and is able to work in the entire range.
how to learn 
in previous work   fixed object locations resulted in a single curriculum to learn colors. to test the robot's ability to generate curricula for different object and robot starting positions  we invited a group of seven graduate students with experience working with the aibos to suggest challenging configurations. it is difficult to define challenging situations ahead of time but some examples that came up include having the robot move a large distance in the initial stages of the color learning process  and to put the target objects close to each other  making it difficult to distinguish between them. the success ratio and the correspondinglocalization accuracy over 1 trials are shown in table 1.
configsuccess  % worst1best1avg1.1mate and the actual target positions  measured by a human using a tape measure. we observe that the robot is mostly able totable 1: planning accuracy in challenging configurations.a trial is a success if all colors are learned successfully. the localization error is the difference between the robot's esti-
¡À
plan a suitable motion sequence and learn colors. in the cases where it fails  the main problem is that the robot has to move long distances with very little color knowledge. this  coupled with slippage  puts it in places far away from the target location and it is unable to learn the colors. the motion planning works well and we are working on making the algorithm more robust to such failure conditions. the localization accuracy with the learned map is comparable to that with a hand-labeled color map  ¡Ö 1cm 1cm 1deg in comparison to 1cm 1cm 1deg in x  y   and ¦È .
how good is the learning 
to test the accuracy of learning under different illuminations  we had the robot learn colors under controlled lab conditions and in indoor corridors outside the lab  where the overhead fluorescent lamps provided non-uniform illumination  between 1lux  and some of the colors  floor  wall etc  could not be modeled well with 1d gaussians. we observed that the robot automatically selected the gaussian or histogram model for each color and successfully learned all the colors.
configlocalization errordist  cm ¦È  deg lab1 ¡À 11 ¡À 1indoor1	11	1tion conditions  lab  indoor corridor  based on the learnedtable 1: localization accuracy: comparable to that with a hand-labeled map.table 1 shows the localization accuracies under two different illumina-
	¡À	¡À
color maps. we had the robot walk to 1 different points and averaged the results over 1 trials. the differences were not statistically significant. the corresponding segmentation accuracies were 1% and 1% respectively  calculated over 1 images  as against the 1% and 1% obtained with a hand-labeled color map  differences not statistically significant . the learned maps are as good as the hand-labeled maps for object recognition and high-level task competence. but  our technique takes 1 minutes of robot time instead of an hour or more of human effort. sample images under different testing conditions and a video of the robot localizing in a corridor can be seen online: www.cs.utexas.edu/¡«austinvilla/ p=research/gen color.
¡¡to summarize  our algorithm enables the robot to plan its motion sequence to learn colors autonomously for any given object configuration. it is able to detect and adapt to illumination changes without manual training.
1 related work
color segmentation is a well-researched field in computer vision with several effective algorithms  1; 1 . attempts to learn colors or make them robust to illuminationchangeshave produced reasonable success  1; 1 . but they are computationally expensive to perform on mobile robots which typically have constrained resources.
¡¡on aibos  the standard approaches for creating mappings from the ycbcr values to the color labels  1; 1; 1  require hand-labeling of images  ¡Ö 1  over an hour or more. there have been a few attempts to automatically learn the color map on mobile robots. in one approach  closed figures are constructed corresponding to known environmental features and the color information from these regions is used to build color classifiers . the algorithm is time consuming even with the use of offline processing and requires human supervision. in another approach  three layers of color maps  with increasing precision levels are maintained; colors being represented as cuboids . the generated map is not as accurate as the hand-labeled one. schulz and fox  estimate colors using a hierarchical bayesian model with gaussian priors and a joint posterior on robot position and environmental illumination. ulrich and nourbakhsh  model the ground using color histograms and assume non-ground regions to represent obstacles. anzani et. al  model colors using mixture of gaussians and compensate for illumination changes by modifying the parameters. but  prior knowledge of color distributions and suitable initialization of parameters are required. our approach does not require prior knowledge of color distributions. instead  it uses the world model to automatically learns colors by generating a suitable curriculum  and adapts to illumination changes.
1 conclusions
robotic systems typically require significant amount of manual sensor calibration before they can be deployed. we aim to make the process more autonomous. we propose a scheme that achieves this goal with regard to color segmentation  an important subtask for visual sensors.
¡¡in our previous work   the robot learned colors within the controlled lab setting using a pre-specified motion sequence. in other work   we demonstrated the ability to transition between discrete illumination conditions when appropriate color maps and image statistics were trained offline. but the robot was given a lot of information manually  including the object positions  the action sequence for learning colors  and color maps for each illumination condition.
¡¡with the current method only the object locations need to be specified. a hybrid representation for color enables the robot to generate a curriculum to learn colors and localize both inside the lab and in much more uncontrolled environments with non-uniform overhead illumination and background clutter that can be confused with the objects of interest. other robots may use cameras of higher quality but color maps are still needed. for full autonomy there are always computational constraints at some level  irrespective of the robot platform being used. this paper lays the groundwork for the next step of testing the same algorithm on other robot platforms that work outdoors.
¡¡in the end  the robot is able to detect changes in illumination robustly and efficiently  without prior knowledge of the different illumination conditions. when the robot detects an illumination condition that it had already learned before  it smoothly transitions to using the corresponding color map. currently  we have the robot re-learn the colors when a significant change from known illumination s  is detected. one direction of future work is to have the robot adapt to minor illumination changes by suitably modifying specific color distributions. ultimately  we aim to develop efficient algorithms for a mobile robot to function autonomously under uncontrolled natural lighting conditions.
acknowledgment
special thanks to suresh venkat for discussions on the color learning experiments  and to the ut austinvilla team. this work was supported in part by nsf career award iis1 and onr yip award n1-1.
