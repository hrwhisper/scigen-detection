we present a novel framework for answering complex questions that relies on question decomposition. complex questions are decomposed by a procedure that operates on a markov chain  by following a random walk on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. decomposed questions discovered during this random walk are then submitted to a state-of-the-art question answering  q/a  system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a multi-document summarization  mds  system. in our evaluations  we show that access to the decompositions generated using this method can significantly enhance the relevance and comprehensiveness of summarylength answers to complex questions.
categories and subject descriptors
h.1.m  information storage and retrieval : miscellaneous; i.1  artificial intelligence : natural language processing
general terms
algorithms  performance  experimentation
keywords
question answering  summarization
1. introduction
¡¡complex questions cannot be answered using the same techniques that apply to  factoid  questions. complex questions refer to relations between entities or events; they refer to complex processes and model scenarios that involve deep knowledge of the topic under investigation. for example  a question like q1:  what are the key activities in the research and development phase of creating new drugs   looks for information on two distinct phases of creating drugs. typically  relevant information for these kinds
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
of questions can be found in multiple documents and needs to be fused together into a final answer. in the document understanding conferences  duc   the answer to complex questions like q1 is considered to be a multi-sentence multiple document summary  mds  that meets the information need of the question. we introduce a new paradigm for processing complex questions that relies on a combination of  a  question decompositions  of the complex question ;  b  factoid question answering  q/a  techniques  to process decomposed questions ; and  c  multi-document summarization techniques  to fuse together the answers provided for each decomposed question . central to this process is a question decomposition model that enables the selection of the textual information aggregated in the final answer.
¡¡we present a novel question decomposition procedure that operates on a markov chain model inspired from the markov chains used for expanding language models introduced in . we propose that question decomposition depends on the successive recognition  and exploitation  of the relations that exist between words and concepts extracted from topic-relevant sentences.  for the purposes of this paper  we will define a relation as any semantic property that can exists between two or more entities or events in texts.  for example  if a topic-relation r1 between  develop  and  drugs  is recognized in question q1  we assume that this sentence  and all other sentences containing this particular relation  will contain relevant information that can be used to decompose of q1. furthermore  we expect that sentences containing topic-relevant relations will also contain other relevant relations that should be leveraged in question decomposition. for example  if r1 is identified in the sentence s1  the challenge for glaxo was to develop a drug that was pleasant to swallow.   we expect that a new relation r1 between the concept company   glaxo   and  develop  should be extracted and used to identify still other sentences that could potentially provide relevant information. as new relations are discovered  we expect that sentences containing the most relevant relations  or combinations of relations  can be used to generate questions that can represent possible decompositions of the original complex question. for example  given r1 and r1 in s1  a question like  what companies develop new drugs   can be created which could be used to obtain a set of answers which could represent a partial response to q1. relevant answers to each newly-decomposed question can be used to discover more relevant relations  that in turn  prompt still more question decompositions. this process ends when either no new relations are discovered  or the random walk is stabilizing within a threshold.

figure 1: the architecture of our framework for processing complex questions.¡¡we evaluate question decompositions in three ways. first  we compare them against decompositions produced by humans. second  we conduct several evaluations of the quality of the mds answers they enable. third  we use every sentence from the mds answer and generate questions with the same procedure employed when creating question decompositions from relevant sentences. the questions that have answers in the summaries are evaluated against questions generated by human linguists. they are also used for measuring the similarity to the decomposed questions. our studies indicate that these comparisons correlate with the relevance of the answers. we claim that this is an important finding since current mds evaluation methods typically rely on  a  human produced answers  or  b  human judgments. the automatic scoring of the mds answers based on comparisons of decomposed questions allows a framework in which researchers can test multiple q/a techniques or multiple mds techniques that best operate for finding answers.
¡¡the remainder of the paper is organized as follows. section 1 presents the framework we have designed for processing complex questions. section 1 details the question decomposition procedure. section 1 describes the random walk models employed for decomposing questions. section 1 details the evaluation results while section 1 summarizes the conclusions.
1. processing complex questions
¡¡in this section  we outline three methods for producing answers to complex questions from based on the output of a question decomposition system. by decomposing a complex question into a set of simpler subquestions that each represent a different dimension of the complex question's information need  we expect to be able to identify answers that are both informative and responsive.
¡¡in this paper  we introduce a new technique for question decomposition that uses a random walk in order to generate possible decompositions of a complex question. figure 1 illustrates the system described in this paper.
¡¡figure 1 includes two types of question decomposition modules: a syntactic question decomposition module and a random walkbased question decomposition module. with syntactic question decomposition  overtly-mentioned subquestions are extracted from a complex question by separating conjoined phrases and recognizing embedded questions. while syntactic decomposition is an important part of any question decomposition algorithm  we will not be discussing techniques for this type of decomposition in this paper. 1

1
for more information on syntactic decomposition  see .
¡¡after complex questions are decomposed syntactically  as illustrated in figure 1  keywords are extracted from each sub-question and are expanded with keyword alternations. the keywords are expanded by  1  identifying the semantic class to which they belong  and  1  using other terms from the lexicons associated with such semantic classes. to identify the semantic class  the keyword is matched against the lexicon of the class. the keyword alternations are selected from the first 1 scored words from the lexicon. the semantic classes are acquired off-line with a co-training method reported in .
research: trial  effort  step  study  work  activity  area  business  cause  field  function  issue  program  project  sector  service  site  education  information  sciencedrug: amphetamine  cocaine  ecstasy  epo  heroin  lsd  marijuana  medication  morphine  opium  measure  prozac  ritalin  steroid  treatment  viagra  alcohol  cost  disease  issuefigure 1: keyword alternations.
¡¡figure 1 illustrates the keyword alternations resulting for the keywords  research  and  drug  that were extracted from the subquestion  what are the key activities in the research phase of creating new drugs  .
¡¡additionally  we use two different models of topic signatures to identify  a  the most representative relations for the topic referred by the complex question and evidence by the document collection. the first topic signature  ts1  we have implemented was reported in . ts1 is defined by a set of terms ti  where each term is highly correlated with the topic with an associated weight
.	the se-
lection of the terms for ts1 as well as the assignment of the association weights is determined by the use of the likelihood ratio. the second topic signature  ts1  was introduced in . it takes into account the fact that topics are not characterized only by terms  there are also relations between topic concepts that need to be identified. if only nouns and verbs from ts1 are selected as topic concepts  the topic signature ts1 is defined as ts1 =
  where ri is a binary re-
lation between two topic concepts. the procedure of generating ts1 was detailed in   and it identifies two forms of relations:  a  syntax-based relations  and  b  salience-based context relations. the arguments of these relations may be  1  nouns or nominalizations;  1  named entity types that a named entity recognizer  ner  identifies; and  1  verbs.
¡¡when topic signatures are available  each sentence from the document collection receives a score based on  a  the presence of a term from ts1;  b  the presence of a relation from ts1; and  c  the presence of any of the keywords extracted from the sub-question or their alternations. the sentence scores determine a ranking of the sentences from the collection for each sub-question. finally  the answer is produced by selecting for each decomposition only the corresponding highest ranked sentences. redundancy is eliminated by checking that each new added sentence does not contain any predicate-argument relation that was already present in a previously selected sentence. predicate-argument relations are discovered by processing sentences with a semantic parser trained on propbank . additionally  each predicate and argument is mapped into every wordnet synonym to enable paraphrase identification. in this way answer summary 1 from figure 1 is produced.
¡¡in addition to the method described above  complex questions can also be decomposed by another method that is described in sections 1 and 1. due to this  in our framework we can produce two additional answers as summaries. the second form of question decomposition discovers relations relevant to the complex question and sentences in which they are present. for each such sentence  one or multiple questions are generated  representing additional question decompositions. when these decompositions are ignored and only the sentences are considered  the topic signatures can be used to score them and to produce a second answer as summary  answer summary 1 illustrated in figure 1 .
¡¡when complex questions are decomposed using random walks  subquestions are submitted a state-of-the-art question-answering  q/a  system  described in    which returns sets of ranked relevant answers for each such decomposition. all these answers are considered separate documents  which are used to produce a multidocument summary as the third answer  mds   answer summary 1 illustrated in figure 1 . the mds system that was used has been reported in . furthermore  for each sentence in the third answer  we generate one or several questions with the same technique that is used for decomposing questions with random walks. since questions produced from the complex question  and questions produced from the answer are available  we argue that the answer is relevant if the two sets of questions are very similar. question comparison is produced by a battery of four question similarity measures  previously reported in . in section 1 of this paper we detail the similarity measures we used in the experiments. the selection of only the most similar questions improves the quality of the answer. instead of submitting all questions generated by the random walks  only the most similar questions are processed again by the q/a system  thus closing a feedback loop. using a hill-climbing technique  if the aggregate similarity of the new set of questions derived from the new answer is improved significantly  the feedback loop starts again. the aggregate similarity is also described in section 1 of this paper.
¡¡the feedback loop ends either when new improvements are not obtained  or when the number of loops is larger than a threshold  in our case lt = 1. with this framework   we were able to study the effects of different forms of question decompositions on the quality of the answers.
1. decomposingcomplexquestions
¡¡in order to process complex questions like q1:  what are the key activities in the research and development phase of creating new drugs    current q/a systems need to decompose the question in a series of simpler questions  that can be tackled by the factoid-based techniques that have emerged from the trec q/a evaluations. table 1 illustrates some of the questions that represent decompositions of q1 and can be generated automatically by the technique we present in this section.
q1:	whatcompaniesdevelopnewdrugs q1:	whatdiseasesarenewdrugsbeingdevelopedfor q1:	howlongdoesittaketodevelopanewdrug 1
table 1: examples of question decompositions.
¡¡the main feature of the decomposed question is related to the ability to easily detect their expected answer type  eat   which represents the semantic class to which their answers should belong. for example  the eat of is organization  the eat of  is disease  whereas the eat of  is duration. our main assumption is that the question decomposition model should be based on several types of relations between words or concepts used in  a  the complex question   b  in sentences that contain relevant information for the complex question  or  c  in other question decompositions that have been produced before for the same complex question.
¡¡in order to produce question decompositions  we follow four steps. in the first step we process the complex question for deriving the relations that are meaningful. in the second step we generate questions based on the relations selected. in the third step we enhance the meaningful set of relations with relations discovered when generating a question decomposition and then we select a new relation based on the latest decomposition. in the fourth step  we loop back to step 1 unless the probability to continue is not above a certain threshold. the detailed operations in each step are:
¡¡step 1: the complex question is lexically  syntactically and semantically analyzed with the goal of identifying the relationships between words that may lead to the generations of simpler questions. the three forms of knowledge are marked up in each of the phases of the analysis:
1.a.  lexical  the determination of the part-of-speech of each word  generated by the brill tagger .
1.b.  syntactic  a full parse of the question is generated by the probabilistic parser reported in . the result of the parse renders information about the syntactic constituents of the question and about their relations. for example  for the complex question q1  we derive the following constituents: v p1: {are}; v p1: {containing}; np1: {the key activities}; np1: {the research}; np1: {development phase}; np1: {np1 and np1}; np1: {new drugs}; pp1: {np1 of np1}; pp1: {np1 in pp1} 1.
1.c.  lexical  for each base np  e.g. np1  np1  np1  np1  we determine whether the head is a nominalization of some verb  by accessing the wordnet database . for example  the noun  development  is a morphological derivation of the verb  develop . the nps having heads which are nominalizations are not considered in step 1.d.
1.d.  lexical/semantic  the generality of the heads of each np is assessed in one of the two categories: abstract  or concrete. the assessment is based on a large answer type taxonomy that was developed for the trec evaluations of q/a systems. the taxonomy  which was described in  comprises 1 synsets from wordnet  and their hyponyms  and 1 semantic classes of names that are recognized by the named entity recognizers we have available. if any of the heads of an np is found in the answer taxonomy  it is assigned the attribute concrete  otherwise it is labeled abstract. for the question q1  only the head of np1 is categorized as concrete. np1 is labeled abstract. the question processing techniques ap-

1
 np stands for noun phrase  vp for verb phrase  and pp for prepositional phrase
plied for factoid q/a identify np1 as being the constituent that indicates the eat for the question. since no eat can be established for q1  it is considered a complex question.
1.e.  syntactic  relations between concrete nps and other constituents are sought. the syntactic relationship from the constituent pp1 indicates a prepositional attachment relation between np1 and np1  which is a coordination between np1 and np1. the syntactic decomposition of the coordination entails two relations between the verbs related to np1 and np1 and np1. the output of step 1 for q1 is: relations: {r1 =  develop - new drugs ; r1 =  research - new drugs }
¡¡step 1: for a relation discovered at step 1 we generate questions that involve that relation. in order to generate questions automatically  we employ a method that was first reported in . in order to generate the question  we first find a sentence that constitutes an answer for that question. this is done by the following sub-steps:
1.a. query formulation. in order to find sentences in which el-

ements from the relations list are discovered  we formulated two kinds of queries:  a  queries involving the lexical arguments of the relation  e.g.   develop  and  drug   as well as  b  queries that involved semantic extensions. four forms of extensions were considered:  1  extensions based on the semantic class of names that represent the nominal category  e.g. names of drugs    1  extensions based on verbs which are semantically related to the verb in the wordnet database  e.g. develop v  -sem.relation¡ú create v ; develop v  -sem.relation¡ú produce v  ;  1  extensions that allow the nominal to be anaphoric  therefore replaced by a pronoun  e.g.  develop - it ; and  1  extensions that allow the nominalizations  as well as the verbal conjuncts  to be considered.
rj shares a predicate with rirj shares an argument with rirj specializes the predicate of rirj specializes the argument of rithe predicates of rj and ri can be composedtable 1: properties between relations ri and rj.
1.b. sentence retrieval. we built an index based on the processing of relations in the text collection1. a sentence is added to the inverted list of a relation ri when it may be composed with another relation rj in the same sentence and  a  relations ri and rj meet one of the conditions listed in table 1  or the predicates of relations ri and rj may be composed with the predicate composition procedure described as a special case in 1.c; and  b  the argument of the relation rj can be mapped in one of the eat categories of the q/a system. examples of such sentences are illustrated in table 1. sentence s1 is retrieved because it contains relation ri =  develop - drugs  and also a relation rj =  develop - glaxo  that shares the same predicate   develop   and  glaxo  is mapped into the eat = company. similarly  sentences s1  s1  and s1 are retrieved because they contain three different expansions of ri and new relations that are compatible with it.
1.c. question generation. every sentence retrieved at 1.b. contains

additional relations  besides those that were expressed by the query. among those relations  some share arguments with the queried relations  some do not. the first group of relations may serve to point to eats that the decomposed questions should refer to. for example  in sentence s1  the new relation  glaxo - develop  can be generalized into  organization - develop  in which organization can be selected as the eat of the question that shall be generated. our named entity recognizer  ner  is able to distinguish between different types of organizations  tagging  glaxo 

1
 we process the text collection and discover all syntactic and salient relations when we build the topic signature ts1 described in section 1.
 from sentence s1 as company  and  medical school  from s1 as university. when the eat is established  the question stem that is associated with it is known  e.g.  what companies   and it substitutes the name from the sentence  to generate the question  from table 1  in which relation r1 is fully specified with all the argument adjuncts it had in the complex question q1. sentence s1 generates the question  whereas sentence s1 generates the question  what universities develop drugs  . sentence s1 illustrated in table 1 enables the generation of  whereas sentence s1 is used for generating. starting from relation r1 in relations  three new relations are discovered: = develop - drug  - company   r1 =   r1 = develop - drug  - disease   and = develop - drug  - duration . each of these new relations enable the generation of the decomposed questions listed in table 1.
s1: thechallengeforglaxowastodevelopadrugthatwaspleasant toswallow.s1: theremaining1percentofroyaltieswillbepaidto ... charing crossandwestminstermedicalschoolwhichdevelopedit.s1: fewcompaniesadmitsettingouttocreateme-toodrugs.s1: cancerresearchfundedresearchanddevelopmentofthedrug whichwasoriginallydiscoveredbyastonuniversityinbirmingham.s1: atbristol-myers whichheleftin1tojoinsmithkline crooke helpeddevelopanarrayofchemotherapydrugsforcancerpatients thatputbristolattheforefrontofcancertreatment.s1: sinceatypicaldrugtakes1yearsandy1bntodevelop onlythose companieslargeenoughtoabsorbthecostswillbeabletosurvive.table 1: sentences retrieved for relation r1:  develop - drug .
 special cases. the properties between relations ri and rj that

are used in the index cover three more cases that need to be addressed by question generation. they are:
1.c.	argument specialization.	in order to inquire about the at-

tributes of arguments  three forms of questions are generated:  i  questions that inquire about instances of entities that are referred by the argument of a relation in which the semantic class of the argument is the eat of the question  and the question becomes a list question;  ii  questions that specialize the argument of the relation by using a modifier which becomes the eat of the question; and  iii  questions that inquire about the characteristics of the arguments by using the question stem  what types . an example of the first form of questions is:  what new drugs have been developed    generated from the sentence:  zinnat is a new drug which was developed because other drugs in its class needed to be injected and were therefore of little use outside the hospital environment. . an example of the second form of questions is:  how many medicines are launched per year     in which the eat is number  it modifies the argument  medicines  in sentence  the number of medicines launched during the early 1s averaged about 1 per year. .
howarenewdrugsresearched howaredrugsmanufactured whattypesofactivitiesareincludedinthedevelopmentofnewdrugs table 1: questions based on predicate specialization.
1.d. predicate specialization. there are three ways of specializing

the predicates from the relations:  i  by selecting the eat of the question as a manner  and associating the question stem  how ;  ii  by using adjuncts of the predicates in the question to produce either a specialized manner eat or a yes/no question; and  iii  by considering that the predicates represent complex events that have structure  and thus this structure can be inquired by using special constructs of the form  what steps are included in   or  what
types of activities are included in . the first form of predicate specialization is the most productive one  and it can be generated based on the recognition of manner relations that was reported in .
examples of questions that were generated for predicate specialization are listed in table 1.
1.e. predicate composition. some questions need to capture rela-

tions between predicates. such relations may be determined by the discovery of  a  causal relations  as it was reported in ;  b  temporal relations; or  c  because the predicates share an argument. table 1 illustrates such questions  their type of relations  and the sentences that enabled them. in our implementation  we have used a set of cue phrases and causal verbs to detect causal relations between predicates. for the temporal relations  we relied on the temporal signals annotated in timebank  e.g.  before    after    during  .
causal:
howdotraderestrictionsaffectnewdrugdevelopment temporal:
howmanytimesmustadrugbetestedbeforeitcanbesold argument sharing:
howdocompaniesdecidewhichnewdrugstoresearch table 1: questions based on relations between predicates.
¡¡step 1: the selection of a new relation is performed after newly discovered relations are added to the relations list.
1.a. relations that specialize arguments or predicates are not added to relations  but all the other three types of relations rj are appended. for example  the relations = develop - drug  - company   = develop - drug  - disease   and
=   r1 = develop - drug  - duration  are added. 1.b. a new relation is selected to maximize the probability estimation that it will lead to another question decomposition of the complex question. the probability estimations are detailed in section 1.
¡¡step 1: the decision to continue or stop the process of generating question decompositions depends on our formalization of the process. we have formalized the process of generating question decompositions which lead to the discovery of new meaningful relations as a random walk on a bipartite graph of questions and relations. for a given relation  a sentence that contains the relation is selected. that sentence is considered to be the answer to a question decomposition  which is generated by identifying a new relation  which in turn  when selected will lead to a new question decomposition. thus the random walk continues with a probability ¦Á  generating a new decomposition and selecting a new relation  or it stops with a probability  1 ¦Á . section 1 describes the formalisms that allow us to estimate the probability that the random walk ends after k steps  corresponding to k loops of the steps 1 and 1 of this procedure.
1. markov chains for question decomposition
¡¡in this section  we describe how we employ two different types of random walks to decompose complex questions for questionanswering and/or multi-document summarization applications. we begin by describing how a random walk can be used to populate a network with potential decompositions of a complex question. later  in section 1  we show how another random walk can then be used to select a set of generated decomposed questions that best represents the information need of the complex question.
¡¡the question decomposition procedure detailed in section 1 can be cast as a markov chain  mc . a mc over a set of states s is specified by an initial distribution p1 s  over s  and a set of state transition probabilities p st|st 1 . in the case of question decomposition  the initial state is represented by one relation r1 selected from the list relations  time = 1   which is the set of relations generated when processing the complex question. the probability of the initial state is set as n1   where n=|relations time = 1 |. after selecting a relation ri at step i  the index is consulted to find sentences where ri and other relations rj having the proper-
ties listed in table 1 are present. if the argument of relation rj can be categorized in the eat hierarchy as an expected answer type ej  then it can enable the generation of a question decomposition qdi+1 with eat = ej. the probability that a question decomposition qdi+1  with eat = ej  is generated from a relation ri is given by p qdi+1|ri  = p ej|ri . the new relation rj is placed in the relations list. if the index of ri had only one sentence and only one relationcould be found in that sentence  then ri is removed from the list relations.
¡¡a new relation ri+1 is selected from relations based on the probability p ri+1|qdi+1 . since question qdi+1 was generated based on the eat discovered with the help of relation rj which led to the eat ej  we can evaluate p ri+1|qdi+1  = p ri+1|ej . in this way we have defined the transition probabilities of the markov chain  mc  illustrated in figure 1. the mc alternates from selecting relations from relations and generating a new question decomposition. in this way  the decomposition process is  surfing  the set of relations meaningful for the complex question  and also the decomposed questions that are generated based on these relations. after each step there is some chance that the question decomposition process will stop. the process continues the random walk with probability ¦Á  generating a new set of question decompositions. with probability  1   ¦Á   the walk stops after step k  after producing the question decomposition qdk+1 .
step 1 step 1 step 1 r1 r1 r1

	qd1	qd1	qd1
figure 1: the markov chain alternates between relations and question decompositions.
¡¡since our goal is to estimate the probability that the mc stops after k steps  we produce a matrix formulation of the problem which is similar to the formulation reported in . this formulation is described in section 1. we also want to test our hypothesis that the decomposed questions are relevant for the complex question. since these question decompositions have been generated by relations that we have discovered in the text to be associated with relations originating in the complex question  we want to test if our assumption that they are valid decompositions can be quantified by a measure of relatedness to the complex question. for this purpose  in section 1 we define a mixture model which generates a different random walk that evaluates the relevance of the decomposed questions.
1 matrix formulation
¡¡the operation of the random walk can be cleanly described by using a matrix notation. let n be the size of the index. the number n corresponds to the relations that we have discovered in the text  having the properties that for every relation ri there is also a relation rj sharing with ri the properties from table 1  and rj has the argument mapped in one of the semantic categories of the eat classes. let m be the number of eat classes. we consider a to be a n¡Ám stochastic matrix with entries aij = p ri|ej  representing the probability that the relation ri will be composed in a sentence with a relation rj that has an argument of semantic type ej  which will become the eat of the question decomposition that is generated. similarly  a stochastic matrix b of dimensions m ¡Án can be defined  in which the elements bij = p ei|rj  represent the probability that a sentence that contains the relation rj can be the answer to a question with the eat = ei. then  the n¡Án stochastic matrix c is defined as c = a¡Áb. the probability that the mc stops after k steps is given by  1   ¦Á ¦Ákcr ek   if the last relation it discovered is r and the last question decompositions it has generated had the eat = e.
to estimate p ei|rj  we consider

where p ei  is the prior distribution of the semantic type ei in the corpus  and p rj|ei  is given by the maximum likelihood estimate of the relation distribution in the text collection. let j1 be the number of instances of the relation rj composed with a relation ri in the same sentence such that the argument of ri has the semantic type ei. then  pmle rj|ei  = # instances of rj1 j 
1 random walks with mixture models
¡¡recently   introduced a random walk model for finding answers to complex questions. this model is based on the idea that answers can be found by scoring each sentence against a complex question and selecting only the first top-ranked sentences. the sentence rank is produced by a mixture model that combines an approximation of a sentence's relevance to a question with similarity measures that can be used to select answer sentences that are not similar to one another. using the same idea  we devised a similar mixture model for measuring the relevance of a question decomposition qdi to the complex question cq. the relevance measure is
defined as:sima qdi cq relevance qdi cq 	=	d +
pqdj sima qdj|cq 
	+	 1   d xqdj pqdsimk simb qdb iqd qdk qdj  j 
¡¡the similarities sima and simb are selected from the four similarity measures defined in section 1. if k is the number of question decompositions that we consider  a stochastic matrix a of dimensions k ¡Á k is considered such that aij = ¦Á   relevance qdi cq .
in order for matrix a to be stochastic  pi aij = 1  thus ¦Á =
. similarly  a stochastic matrix b of
dimension k ¡Á k is defined such that bij = ¦Âj   simb qdi cq   where. next  the relevance vector
r for all question decompositions is defined by r =  da +  1   d b i ¡Ár. the square matrix e =  da+ 1 d b  defines a mc where each element eij from e specifies the transition probability from state i to state j in the corresponding markov chain. the relevance vector r is the stationary distribution of the markov chain. with probability d  a transition is made from the current question decomposition qdi to new question decompositions that are similar to the complex question cq. with a probability  1   d   a transition is made to question decompositions that are similar to the last question generated  qdi. we have used several values for d in our experiments.
1. evaluation results
¡¡our experiments have targeted  1  the evaluation of the decomposed questions;  1  the evaluation of the three forms of answers produced by the framework illustrated in figure 1; and  1  the evaluation of the impact of the decomposed questions on the quality of answer summaries.
evaluation of decomposed questions
¡¡the evaluation of the decomposed questions was performed in two ways. first  the decomposed questions were evaluated against decompositions created by humans. second  question decompositions were evaluated against questions generated from the answer summaries. the second evaluation was also compared against an evaluation involving only human-generated questions  both from the complex question and from the answer summaries. the evaluation was performed against 1 complex questions that were asked as part of the duc 1 question-directed summarization task. the questions correspond to the topics listed in table 1.
¡¡four human annotators performed manual question decomposition based solely on the complex questions themselves. annotators were asked to decompose each complex question into the set of subquestions they felt needed to be answered in order to assemble a satisfactory answer to the question.  for ease of reference  we will refer to this set of question decompositions as qdhuman. 
¡¡in order to evaluate the quality of the automatic question decompositions produced by our system  we generated three different types of question decompositions for a total of 1 complex questions that were asked as part of the 1 duc question-directed summarization task. first  we had 1 human annotators perform manual question decomposition based solely on the complex questions themselves. annotators were asked to decompose each complex question into the set of subquestions they felt needed to be answered in order to assemble a satisfactory answer to the question.  for ease of reference  we will refer to this set of question decompositions as qdhuman.  the subquestions generated by the annotators were then compiled into a  pyramid  structure similar to the ones proposed in  nenkova and passonneau  1 . in order to create pyramids  humans first identified subquestions that sought the same information  or were reasonable paraphrases of each other 
and then assigned each unique question a score equal to the number of times it appeared in the question decompositions produced by all annotators. second  we utilized our random walk model to generate a set of question decompositions  qdauto1  for each complex questions. third  as shown in figure 1  the subquestions in qdauto1 were used to generate multi-document summaries which were used to automatically generate a fourth set of question decompositions  qdauto1 . as with qdhuman  the subquestions generated for qdauto1 and qdauto1 were combined into pyramid structures by human annotators.
¡¡each of these three sets of question decompositions were then compared against a set of  gold standard  decompositions created by another team of 1 human annotators from from the 1  model summaries  prepared by nist annotators as  gold standard  answers to the 1 complex questions. each of the three question decompositions described above  i.e. qdhuman  qdauto1  and qdauto1  were then scored against the corresponding  model  question decomposition pyramid using the technique outlined in . table 1 illustrates the pyramid coverage for qdauto1  qdauto1  and qdhuman it is to be noted that although the qdhuman captured 1% of the questions contained in the  model  pyramids  the high average pyramid score  1  suggests that human question decompositions typically included questions that corresponded to the most vital information identified by the authors of the  model  summaries.
¡¡another important observation is that the coverage and the pyramid score of qdauto1 are almost 1% of the same measures obtained for qdhuman  whereas the pyramid score of the question decompositions qdauto1 is only 1% of the pyramid score and coverage obtained for qdhuman. in fact  these scores vary based on the number of feedback loops allowed for the answer summary 1 from figure 1. figure 1 illustrates the average average pyramid scores that were obtained at each step of the feedback loop for all eight questions  both for qdauto1  and qdauto1. the figure
topic descriptionpyramid score for question decompositionsqdauto1qdauto1qdhumanfalkland islands111tourist attacks111drug development111amazon rainforest111welsh government111robot technology111u.k. tourism111czechoslovakia111average111table 1: pyramid coverage of question decompositions.
shows that the pyramid scores improve for qdauto1. the improvement for qdauto1 is less dramatic. this means that the comparison and selection of new question decompositions at each feedback loop determines better questions and better answers.
topic descriptionresponsiveness scoresummary 1summary 1summary 1human sumfalkland islands1111tourist attacks1111drug development1111amazon rainforest1111welsh government1111robot technology1111u.k. tourism1111czechoslovakia1111average1111table 1: responsiveness score for the human summaries and answer summaries 1  1 and 1.
¡¡four different similarity metrics are responsible for the comparisons. they are listed in figure 1. pairs of these similarity metrics were also used for defining the relevance of question decompositions to each complex question. the aggregate similarity between qi ¡Ê qdauto1 and qdauto1 is defined as a sim qi qdauto1  =
. the similarity scores play an impor-
tant role in the selections of questions from qdauto1 for the next loop. in our experiments  we observed that if we take only similarity 1 we obtain the best results.
similarity metric 1 weights content terms in qdauto1 and qdauto1 usingis the number of questions in qdauto1 and qdauto1  while dfi is equal to the number of questions in containing ti and tfi is the number of times ti appears in qdauto1 and qdauto1. any question in qdauto1 and qdauto1 can be transformed into two vectors 
and ; the similarity between qdauto1 and qdauto1 is measured as the cosine measure between their corresponding vec1i 1 ¡Á  pi wu1i 1  . tors:: cos vq vu  =  pi wqiwui /  pi wq  similarity metric 1 is based on the percent of terms in qdauto1 that appear in the qdauto1. it is obtained by finding the intersection of the terms in the term vectors of the two questions.similarity metric 1 utilizes semantic information from wordnet. it involves:
 a  finding the minimum path between wordnet concepts. given two terms t1 and t1  each with n and m wordnet senses s1 = {s1 ... sn} and s1 = {r1 ... rm}. the semantic distance between the terms ¦Ä t1 t1  is defined by the minimum of all the possible pair-wise semantic distances between s1 and s1: ¦Ä t1 t1  = minsi¡Ês1 rj¡Ês1 d si rj   where d si rj  is the path length between si and rj.
 b  the semantic similarity between the vector transformations vq and vu from qdauto1 and qdauto1 respectively is defined as sem vq vu  =
i vq vu +i vu vq 
|vq|+|vu|	  wheresimilarity metric 1 is based on question type similarity  using a question-type similarity matrix similar to the one introduced in .figure 1: the four similarity metrics.

figure 1: pyramid scores at each step of the feedback loop.
evaluation of answers.
¡¡answers were evaluated by the  responsiveness score  designed by the nist assessors. the score provides a coarse ranking of the summaries for each topic  according to the amount of information in the summary that helps to satisfy the information need expressed in the topic statement. four linguist assigned these scores for all three forms of answer summaries. table 1 illustrates the responsiveness scores for answer summary 1  answer summary 1  answer summary 1  from figure 1 and the human generated summaries. the responsiveness score is measured on a scale from 1 to 1 and it quantifies how well does a summary answer the complex question. a score of 1 is the least responsive to the question. a score of 1 means that the summary answers completely the question.
¡¡evaluation of the impact of the decomposed questions on answer summaries.
¡¡we were also interested to evaluate the impact the question decompositions would have when we select different values for the parameter ¦Á which stops the markov chain. figure 1 illustrates the average responsiveness score obtained when ¦Á = 1  ¦Á = 1  ¦Á = 1  ¦Á = 1 and ¦Á = 1. since the question decompositions determine two different answers  as it was illustrated in figure 1  we have measured the responsiveness for both of them and illustrate the results in figure 1.

¦Á
figure 1: responsiveness for different ¦Á values.
¡¡in a separate effort  we evaluated the impact of only the question decompositions that were considered relevant to the complex question by the random walk presented in section 1. since that random walk depends on the parameter d  we have tested the question coverage for d = 1  d = 1  d = 1  d = 1  and d = 1. figure 1 illustrates the average responsiveness obtained in this case. since only answer summary 1 is obtained by considering the relevance of question decompositions to the complex question  unlike figure 1  in figure 1 we illustrate results only for answer summary 1. the best results are obtained for d = 1. this result supports our intuition that the question decompositions should not be necessarily very different  but they must be relevant to the complex question. the difference from the responsiveness of human-generated summaries indicates that relevance takes into account more sophisticated information than the one contained in questions.

d
figure 1: responsiveness for different d values.
1. conclusions
¡¡we have presented a new framework for question decomposition that allows several forms of answers to be returned for complex questions. two forms of random walks were used. the first random walk was used for surfing the space of relations relevant to the complex question  in order to generate question decompositions. the second random walk was used for measuring the relevance of the question decompositions to the complex question.
¡¡the evaluations have shown that the question decompositions lead to more relevant and complete answers. the results have also shown that the coverage of automatically generated question decompositions  when compared with the questions generated from the answer summary are better indicators of answer quality than the relevance score to the complex question. the evaluations have also indicated the question coverage for automatic methods is 1% of the coverage of questions produced by humans.
¡¡in this paper we have also described a q/a architecture which allows feedback loops for improving the quality of answers through the coverage of question decompositions.
1. acknowledgments
¡¡this material is based upon work funded in whole or in part by the u.s. government and any opinions  findings  conclusions  or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the u.s. government.
