association rules have received a lot of attention in the data mining community since their introduction. the classical approach to find rules whose items enjoy high support  appear in a lot of the transactions in the data set  is  however  filled with shortcomings. it has been shown that support can be misleading as an indicator of how interesting the rule is. alternative measures  such as lift  have been proposed. more recently  a paper by dumouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. this approach  however  has its limitations  since it stops short of considering higher order interactions  other than pairwise  among the items. in this paper  we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items. since fitting loglinear models for large data sets can be computationally prohibitive  we apply graph-theoretical results to divide the original set of items into components  sets of items  that are statistically independent from each other. we then apply loglinear modeling to each of the components and find the interesting associations among items in them. the technique is experimentally evaluated with a real data set  insurance data  and a series of synthetic data sets. the results show that the technique is effective in finding interesting associations among the items involved.
categories and subject descriptors
h.1  database management : database applications data mining  statistical database
keywords
association rule  log-linear model  graphical model
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigkdd '1  august 1  1  washington  dc  usa copyright 1 acm 1-1/1 ...$1.
1. introduction
　since their introduction in   association rules have received a lot of attention in the data mining community  having been used in multiple applications. association rules are defined by the support of the set of items  itemset  that are involved in the rule  number of transactions in the database that contain the items   and their confidence  number of times that the right hand side appears in records where the left hand side itemset appears . algorithms to discover association rules usually prune the choices by considering only itemsets whose support exceeds a threshold. a key property  called apriori  states that for an itemset to exhibit high support  all its subsets must have high support. this has given way to a popular algorithm  apriori   that searches for high support itemsets incrementally  beginning from itemsets of size 1  and considering candidates for high support whose size is one unit higher than those considered in the previous iteration. other efficient algorithms have been investigated   .
　in spite of the success of association rules  there are inherent problems with the concept of finding rules based on their support and confidence. in   silverstein et. al show the pitfalls of using support as the guide for pruning rules. it shows that  interest   or lift   the ratio between the actual probability of the itemset divided by the product of the individual probabilities of each item  is a better guide. obviously  the denominator in the interest is simply the estimated probability using independence. so  this ratio simply compares the actual support with the estimation that results from assuming independence among the items. contrary to rules based exclusively on support  those that are found by using lift show that there exists some correlation between the itemset on the right hand side of the rule and the one on the left hand side  as long as the lift value is greater than 1 . as the authors of  show  rules of the type x    y   that have high support for the itemset xy   may be misleading in the sense that the itemset y overall support may be higher when considered by itself than when considering only transactions that also contain the itemset x.
　in   dumouchel and pregibon go further in showing the limitations of support-based algorithms. assume you have a three item set abc with strong support and lift. you really do not know if these are the consequence of a strong show of the triplet abc or because a combination of two attributes is the strong one  e.g.  ab or ac  - they present a meaningful example using two drugs  ab  and kidney failure  c : given that you find association between the three is it due to the combined effect of the two drugs  abc   or is it simply the effect of one  ac   the authors propose to select the multi-item associations that can not be explained by the pairwise associations in the item set by using the standard statistical theory of log-linear models.
　however  dumouchel and pregibon stop short of fully analyzing the interestingness of multi-item associations. the interpretation of  interesting  large item sets can be confusing since it is often unclear whether the item set is interesting because it contains all the items  or if it is interesting because it consists of interesting subsets of items.
　in this paper  we will analyze and interpret the associations among items by using loglinear modeling. loglinear models describe association patterns among categorical variables. with the loglinear approach  we model cell counts in a contingency table in terms of associations among the variables. there are several problems that need to be addressed in order to apply loglinear models to market basket data. first  loglinear modeling is usually applied to domains with low or medium dimensionality    1 . in a typical market basket application  the number of dimensions may be much larger than that. the number of transactions may be very large as well   as opposed to the typical data sets for which loglinear modeling is applied. also  with loglinear models  we need to have at least 1 times the number of cases as cells in our data  a requirement that is not commonly met by market basket data  as the contingency table is very sparse . lastly  the complexity of algorithms for computing the maximum likelihood estimates  mle  in loglinear models is exponential in the dimension of the table thus computationally expensive for large tables. hence building loglinear models directly over all items is prohibitive. fortunately for us  not all combinations of items exhibit associations: some itemsets may be independent from other itemsets. we apply graph-theoretical results to divide the problem into smaller components of items and fit each component using a loglinear model.
　our work is different from dumouchel's work  in the following aspects. first  we aim to get only one optimal loglinear model to describe all the possible associations among the items in the component instead of building many alltwo-factor models. for example  in the component composed by five variables  item abcde   they need to build 1 all-two-factor models  one for each multi-item set  i.e  abc abd ，，，  abcd ，，，  abcde  and compare with shrinkage estimates. second  we interpret the associations among the items by using standardized parameters of fitted loglinear model instead of the excess1 measure used in  1. the large excess1 value indicates complex relationships involving more than pairwise association among the items of the item set. however  from excess1 we can not always infer what causes the support of the itemset to be a large value. for example  if we know that the excess1 measure for abcd is large  is it due to abc abd or abcd  by analyzing the parameters of the fitted loglinear model  we can interpret the interestingness of asso-
denotes an estimate of the
number of transactions containing the item set over and above those that can be explained by the pairwise associations of the items in the item set  Λ〜e is shrinkage estimates which is a substitute of raw data  eall1f is predicted count of all-two-factor model based on all two-way distribution. ciations among items. the γ-term included in the fitted loglinear model  γabc γabcd etc.  precisely describes the interactions of items. third  by analyzing residuals  we can automatically pick out the multi-item associations that can not be explained by all the  not just pairwise  associations included in our fitted loglinear model in the item set. as our model fits better than the assumed all-two-factor model  the number of residuals generated by our method is far less than that generated by all-two-factor model.
　the rest of the paper is organized as follows. in section 1 we review the loglinear model. section 1 presents our method. experimental results are discussed in section 1. in section 1 we draw conclusions and describe directions for future work.
1. loglinear models revisited
　loglinear modeling is a methodology for approximating discrete multidimensional probability distributions. the multiway table of joint probabilities is approximated by a product of lower-order tables. in the database area  loglinear modeling techniques have been successfully applied to high dimensional data compression  1  1   histogram synopses   query approximation   and exploratory data cube analysis . here we should note that loglinear models use only categorical attributes and continuous attributes must be converted to discrete values first.
　for a value yi1，，，in at position ir of the rth dimension dr  1 ＋ r ＋ n   we define the log of anticipated value  yi1，，，in as a linear additive function of contributions from various higher level group-bys as:
	 li1，，，in = log  yi1，，，in =	x	γ gir|dr（g 	 1 
g {d1 d1 ，，， dn}
　we will refer to the γ terms as the coefficients of the model. the coefficients corresponding to any group-by g are obtained by subtracting from the average l value at group-by g all the coefficients from higher level group-by-s.
　for instance  in a 1-dimensional table with dimensions a b c d  we use  i j k l yijkl  to denote the cell in a 1-d cube space  where i = 1 ，，，  i   1 j = 1 ，，，  j   1 k = 1 ，，，  k   1 l = 1 ，，，  l   1. equation 1 shows the saturated loglinear model which contains all the possible k-factor effects  all the possible k   1-factor effects  and so on up to the 1-factor effects and the mean γ. for example  γia is one-factor effect  γijab is two-factor effect which shows the dependency within the distributions of the associated attributes a b. the singly-subscripted terms are analogous to main effects  and the doubly-subscripted terms are analogous to two-factor interactions.
log  yijkl	=	γ + γia + γjb + γkc + γld
+ γijab + γikac + γilad + γjkbc + γjlbd + γklcd
+ γijkabc + γijlabd + γiklacd + γjklbcd
	+ γijklabcd	 1 
　equation 1 shows the linear constraints among coefficients  where a dot  .  means that the parameter has been summed over the index  for example  . in short  the constraints specify that the loglinear parameters sum to 1 over all indices.
                 γ.a = γ.b = γ.c = γ.d = 1 γi.ab = γ.jab = γi.ac = γ.kac = ，，， = γ.lcd = 1
，，，
	γijk.abcd = γij.labcd = γi.klabcd = γ.jklabcd = 1	 1 
　equation 1 shows how to compute the coefficients in a 1-dimensional table.
　　　γ = l.... γia = li...   γ
，，，
γijab = lij..   γia   γjb   γ
abc	ab	ac	bc	a	b	c
γijk	= lijk.   γij	  γik	  γjk   γi   γj   γk   γ
，，，
 1 
　in  a fast computation technique called the updown method that makes this approach feasible for large sets is described. in the up-phase  all the l parameters shown before are computed. for each group-by in equation 1  the corresponding l value from the parameters in the previous group-by-s is computed. for example  in order to compute lij..  we could use the values of lijk.  aggregating for all k.  in general  there is more than one way of computing the parameters  since there is a lattice of group-by aggregations; a benefit analysis approach like the one in  can be used to select the best choice.  we need to start from the most detailed group-by: in general this is the one defined by the raw data.
　in the down-phase  for each group-by starting from the least detailed  for instance  l.... in equation 1   we can compute the corresponding effect  i.e.  γ  at g by subtracting from the corresponding l value the parameters from all the group-by-s h where h   g.  for instance  to compute γijab  we need to subtract from lij.. the values of γia γjb and γ. 
　it is obvious that a large number of models can be used to fit a given data set. for an k-dimensional loglinear model  there are a total 1k possible models  determined by which parameters of the saturated model are set to zero . there are several possible strategies of model selection  see  for more discussion . one approach consists of fitting the model having only single-factor terms  then the model having only single-factor and two-factor terms  then the model having only three-factor and lower order terms  and so forth. fitting such models often reveals a restricted range of goodfitting models. in our earlier work   we apply this strategy to compress data cubes where the objective is to achieve a good compression ratio instead of interpreting the associations.
　brown et. al.  in  1  1   suggested model fitting by using two tests to screen the importance of each possible term. in one test the term is the most complex parameter in a simple model  whereas in the other test all parameters of its level of complexity are included. this strategy works well when our main objective is to test whether a particular interaction is present or significant. however  this strategy involves a large computational cost to build loglinear model as it needs to evaluate the importance of all possible terms.
1. our method
　in this section we describe in detail how we screen and interpret associations by means of building loglinear models and examining their parameters and residuals using market basket data. for market basket data  we define each transaction  such as list of items purchased  as a subset of all possible items.
　definition 1. let i1 ，，，  ik be a set of k boolean variables called attributes. then a set of baskets b = {b1 ，，，  bn} is a collection of n k-tuples from {true false}k which represent a collection of value assignments to the k attributes.
　our method involves decomposing the initial set of items into groups that are mutually independent  building loglinear models for these components  interpreting associations and examining residuals. the method can be sketched as follows:
  step 1. decompose k items into m groups s = {s1 ，，，   sm}  where ksik = ki.  section 1. 
  step 1. transform market basket data into m contingency tables with dimension size ki respectively.
  step 1. for each contingency table 
- step 1. apply the updown method to compute the parameters of saturated model over each derived 1ki contingency table.
- step 1. order and partition the parameters into bins according to their magnitude. fit and compare two models iteratively by including in the first model those parameters from the first j bins and in the second model those parameters from the first j + 1 bins. if the second model fits well while the first one does not  go to step 1. otherwise  increase j by one and repeat step 1.
- step 1. examine iteratively each interaction from the j + 1-th bin by comparing the current model with the new model including one new parameter. the likelihood estimation is used to test the significance of each interaction.
- step 1. examine the parameters of the fitted model to derive the interestingness patterns of associations.
- step 1. examine residuals computed from the fitted model.
　as we stated in the introduction  to effectively process a data set with large number of items  we need to decompose the items into components and build a loglinear model for each component separately. all the significant interactions of a loglinear model built over the original data set must remain unchanged in the loglinear models built over components. in other words  the mles for each parameter of the original model should equal to the mles of models for components. we apply graph-theoretical results to decompose the items into components while keeping the mles of parameters unchanged. we leave the discussion of this part in section 1 and assume the number of dimensions is low or medium  i.e.  k   1  in sections 1 and 1. in section 1 table 1: coil 1 data set with four dimensions denoted by a b c d respectively
b trueb falsea truea falsea truea falsed truec true11c false11d falsec true11c false11
figure 1: lattice for the data set with four dimensions denoted by a b c d respectively. the value in    denotes the value of γ-term of saturated loglinear model
we focus on how to fit loglinear model for each component. in section 1 we present how to interpret the interesting patterns of associations and how to screen interesting itemsets by examining the parameters and residuals of the fitted loglinear model.
1 loglinear model fitting
　the first step of loglinear model fitting is to compute the parameters of the saturated model by applying the updown method step 1 .
　we present our strategy by using one example. table 1 shows a contingency table with four attributes. this table is derived from coil real data set . the original data set contains 1 attributes and we present our experiment with full 1 attributes in section 1. table 1 shows the meaning of the four attributes  a b c d  and the other six attributes  e-j . these ten attributes are the most significant attributes after applying univariate analysis . we use these ten attributes to illustrate our method  including decomposition and loglinear model fitting .
　figure 1 shows the parameter values from the saturated model computed by using the updown method. each of the γ-term in the saturated loglinear model describes the interaction of item variables. for example  γab represents the interaction between item a and b. notice that in market basket data  each item variable can only have two categories: presence  absence. hence  each of the γ-term has only one absolute value due to linear constraints of coefficients  see equation 1  and the positive  negative  value implies positive  negative  associations. for example  γab =  1 in figure 1 implies γ1ab =  1  γ1ab = 1  γ1ab = 1  and γ1ab =  1. it can be interpreted that the presence  absence  of a implies the absence  presence  of b with interaction effect 1. note in general contingency table cases  the γ-term for a particular interaction  i.e.  γab  has more than one absolute value due to variables with more than two categories.
   furthermore  we can compare the interactions according to their magnitude of γ-terms derived from the saturated models in market basket case. for example  the comparison of γac  1  and γcd  1  implies the interaction of ac is more significant than that of cd. it is important to point out that  in general  we cannot compare the magnitude of γ-terms directly. this is due to several reasons. first  the degree of freedom  d.f.  for each particular interaction varies  however  in market basket data  the d.f. for each particular interaction is always 1 . secondly  the variance for each interaction varies  however  in market basket data  the variances for all γ-terms equal to the same value -see appendix a for proof details . the values and 1 do not necessarily imply that the interaction of ac is greater than that of cd  since the variances of  can be different. so in the general case  we have to compute the standardized parameter value  γ/σ γ   for each γ-term in order to compare the significance of each interaction. the cost to standardize each γ-term is very high. thirdly  in general  there can be more than one absolute value for each γ-term and we have to combine the estimates in some way to form an overall test statistic  this is usually hard and subjective  . however  for market basket data  we do not need to do this since each γ term exactly has one absolute value.
　our modeling strategy consists in ordering the γ-terms based on their magnitude and including those γ-terms exceeding some threshold  we can do this since the γ-terms are comparable . the idea of fitting the saturated model and noting which estimates of association and interaction parameters are large compared to their estimated standard errors was first proposed by goodman  in . however  it is not widely used because the high computational cost of standardizing parameters. for market basket data  this idea is very attractive as we can drastically decrease the cost of modeling without computing the variance of each γ-term.
table 1: coil significant attributes used in example. the column  mapping  shows how to map each original variable to binary variable.
attributei th attributenamedescriptionmappinga1mopllaaglower level education  1 ★ 1b1minkm1income   1k  1 ★ 1c1minkgemaverage income  1 ★ 1d1mkoopklapurchasing power class  1 ★ 1e1pwapartcontribution private third party insurance  1 ★ 1f1ppersautcontribution car policies  1 ★ 1g1pbrandcontribution fire policies  1 ★ 1h1awapartnumber of private third party insurance  1 ★ 1i1apersautnumber of car policies  1 ★ 1j1caravannumber of mobile home policies  1★　to determine which interactions should be included in the fitted model  we need a threshold. however  there is no good way to determine the threshold for a given data set. it is unknown what the distribution of all γ-terms estimates is  although each γ-term estimate follows an approximate normal distribution with mean γ and variance σ γ  . we apply a heuristic strategy here. we order γ-terms according to their magnitude and divide them into bins  equi-width . we first include in the starting model those terms in the first bin. when that model fits well  it may be possible to simplify it and remove some terms with small absolute values. when it does not fit well  we need to include additional parameters in the second bin. in other words  we keep comparing models built with parameters up to the j-th and j + 1-th bin until the latter fits well. during step 1  we apply the likelihood ratio l1  see equation 1  to assess the importance of terms in j+1-th bin. the likelihood ratio is minimized and follows a chi-square distribution with the d.f. equal to the number of γ-terms set equal to zero. for a given d.f.  larger l1 values give smaller right-tail probabilities  p-values   and represent poor fits. equation 1 shows how the l1 statistics is used for comparison of two models. the d.f. is calculated by subtracting the d.f. of model1 from the d.f. of model1. in this step  the difference of d.f. is always 1 as the two models we compared are same except the tested γ-term.
	l1 = 1xyilog yi/y i 	 1 

1 interpreting and screening associations
　as we stated in the introduction  we are departing from the majority of published approaches to the market basket problem by going beyond the examination of frequent itemsets. the idea of using measures other than itemset frequency has been explored a few times. for example  in   they propose measuring significance of dependence via the chi-squared test for independence from classical statistics. in   they only distinguish between multi-item associations that can be explained by all pairwise associations  and item sets that are significantly more frequent than their pairwise associations would suggest. in our framework  we interpret associations by examining the γ-terms of fitted loglinear models instead of by examining the differences between observed frequencies of itemsets and expected frequencies computed from assumed models.
log  ylift=γ + γa + γb + γc + γd	 1 log  ypairwise=γ + γa + γb + γc + γd + γab
+ γac + γad + γbc + γbd + γcd  1 log  yfitted=γ + γa + γb + γc + γd + γac + γbc	+ γbd + γcd + γabc + γabd	 1 
　now we illustrate the difference of our work with previous approaches using an example. equation 1 assumes the independence model and includes all-one-factor  main  effects and grand mean. equation 1 includes all-two-factor effects apart from all-one-factor effects and grand mean. the comparison between the observed value y with either  ylift or y pairwise is used to screen interesting itemsets in  or  respectively. the assumed independence model  shown in equation 1  or pairwise model  shown as equation 1  may be inaccurate. by comparing with an inaccurate model  false interpretations may be introduced when we examine itemsets. in our framework  we fit the market basket data to derive the fitted loglinear model  as shown in equation 1  instead of just assuming some specific model  independence or pairwise model .
　as our model really fits the underlying data and includes significant interactions at all possible levels  we can derive the association patterns by examining the γ-terms of our fitted model directly. for example  from γac = 1  γbc =  1  and γabc = 1  we can see the positive two-factor interaction  i.e.  the presence of one item implies the presence of the other one  between item a and c  the negative two-factor interaction between item b and c  no significant two-factor interaction between item a and b  and positive three-factor interaction among abc. from γbd =  1 γabd =  1  we can see the negative two-factor interaction between item b and d  the threefactor negative interaction among abd  however no significant two-factor interaction for item sets ab or ad.
　we would like to point out that we apply a non-hierarchical modeling strategy  step 1 and 1 . hierarchical models are nested models in which when an interaction of d factors is present  all the interactions of lower order between the variables of that interaction are also present. for example  if a three-way interaction  γabc  is present  the model must also include all two-way effects  γab γac γbc  as well as the single variable effects  γa γb γc  and the grand mean  γ . non-hierarchical modeling can better interpret the associations for market basket data. consider one example for the concept of synergism1 where each item from a  b  c is sold independently and the third item is free if custable 1: comparison of three models. residuals is the number of cells by comparing standardized residuals to standard normal percentage points 1
modellikelihood ratio l1d.f.residualsindependence11all-two-factor11our model11tomer buys any other two items. clearly there exists a three-way interaction effect  γabc  but no two-way interaction between any pairs among a  b  c  
clearly only non-hierarchical model  i.e.  log  yijk = γ+γi + γjb + γkc + γijkabc  can explain this case correctly. notice in the non-hierarchical model  the two-way effects are not included in the model therefore violating the hierarchical requirement.
　the parameters of the loglinear model provide the interactions between item variables. further analysis of residuals may reveal in cell-by-cell comparisons of observed and fitted frequencies. note here our loglinear model is built at the finest level  containing all variables  and it is easy to compute expected frequencies of itemsets at any upper level by simply summing those cells from the finest level. equation 1 shows the standardized residual form used in our framework.
		 1 
　when the model holds  ei is asymptotically normal with mean 1. in comparing standardized residuals to standard normal percentage points  we obtain conservative indications of cells having lack of fit. table 1 shows the comparison of independence model  pairwise model  and our fitted model for the coil data set. the likelihood ratio and the size of residuals from table 1 clearly show that our fitted model is better than the independence and pairwise models as it includes significant high-factor effects and excludes those non-significant 1-factor effects  even the main effect .
1 graphical decompositions for large contingency tables
　as we stated in the introduction  we cannot build loglinear models over the very sparse and large contingency table that results from market basket data. besides  even if the data set is dense  the complexity of algorithms for computing the mles in loglinear models is generally exponential in the dimension of the item variables and thus computationally expensive for large tables. in this section  we discuss how to decompose the problem into subsets and build loglinear models for each subset without losing any significant interaction. we do this by using graph-theoretical results. the procedure involves two steps: 1  we build one independence graph for all item variables; 1  we apply graph-theoretical results to decompose the graph into non-decomposable irreducible components.
　the independence graph is defined by making every vertex of the graph correspond to a discrete random variable  and the edges denoting the dependency of the two variables linked. a missing edge in the graph represents the conditional independence of the two variables associated with the two vertices. models with the maximal permissible higherorder interactions corresponding to a given independence graph are called graphical models.  see  1  1  for comprehensive treatment of graphical models.  figure 1 a  shows the independence graph  two disconnected subgraphs fi  abcdjgeh  for our coil data set. from this graph  we can infer for instance that variables i and f are independent with respect to the remaining variables. we can also derive that variables e and h are conditionally independent with respect to the set acdej given the variable g. intuitively  there is no interaction between any variable from set eh and any variable from the set acdej given variable g.
　the second step is to decompose the graph into basic  irreducible components. graph-theoretical results show that if a graph corresponding to a graphical model for a contingency table is decomposable into subgraphs by a clique separator 1  the mles for the parameters of the model can easily be derived by combining the estimates of the models on the lower dimensional tables represented by the simpler subgraphs. hence  applying a divide-and-conquer approach based on the decompositions will make the procedure applicable to much larger tables.
　the theory may be interpreted by the following way: if two disjoint subsets of vertices sa and sb are separated by a subset sc in the sense that all paths from sa to sb go through sc  then the variables in sa are conditionally independent of those in sb given the variables in sc. the subgraphs may be further decomposed into subgraphs. the requirement that the subgraph on sc is complete implies that there is no further independence constraints on the elements of sc  so that this factorization contains all the information about the joint distribution.
　figure 1 b  shows the components  abcd  acj  ehg  ag  if . we can see the interactions among abcd are independent with respect to other variables. the interactions among abcd  i.e.  γab  γac  γabc etc.  can be derived directly from the condensed 1-dimensional contingency table  i.e.  abcd  instead from the original 1-dimensional contingency table  i.e.  abcdefghij . the mles of the interactions for each component are the same as those for the original graphs. in our experiments  we apply coco  within xlisp-stat with a complexity of o nm1   with m the number of generators and n the number of variables  to perform decomposition for large contingency tables. to find the clique separators of a graph or to find the vertex-sets of the irreducible components of the graphs  an algorithm with a complexity of o ne +n1  can be used   where n is the number of vertices and e is the number of edges.
                   c  a  independence graphd	c
 b  decomposed component	e	h	e	h
f
fa                  bb
a                 b
figure 1: composition of coil data set with 1 variables　to build the independence graph  we need to test conditional independence for every pair of variables  controlling for the other variables. there are several approaches to test conditional independence  see  . in our paper  we build the independence graph by applying the cochran-mantelhasenzel test. for any pair of two items ii ij from item set i = {i1 ，，，  ik}  we derive one partial 1 〜 1 contingency table  stratum  for each possible value from set i   {ii ij}. hence we can have l  l = 1k 1  strata. for each stratum l  we need to compute the marginal totals.
table 1 a  shows the stratum form for item variable a and b while table 1 b  shows one stratum  c = 1 d = 1  derived from table 1. equation 1 shows the summary statistics where and v  n1  is mean and variance respectively.
		 1 
　the summary statistics m1 has approximately a chi-squared distribution with d.f. = 1 under the null hypothesis of conditional independence. hence  if m1   pα  we can reject the null hypothesis of conditional independence and include the edge of ii and ij in the interaction graph. in our experiments  we choose α = 1 and pα = 1. however  the cochran-mantel-hasenzel test does not work well for very sparse data sets because the marginal totals for a given partial table usually equal zero. to deal with very sparse market basket data sets  we may test marginal independence for each pair of variables by applying log odds ratio over one marginal 1 〜 1 table  shown in table 1 c   which contains summary frequencies and ignores the other controlling variables.
1. experimental results
　in this section we show the results of experimenting with one real data set and some synthetic data sets. the experiments were conducted in a dell dimension 1  with one 1g processor  and 1 mbytes of ram.
1 coil data
　the coil challenge 1  provides data from a real insurance business. the competition consisted of two tasks: 1  predict which customers are potentially interested in a caravan insurance policy; 1  describe the actual or potential customers; and possibly explain why these customers buy a caravan policy. information about customers consists of 1 attributes and includes product usage data and socio-demographic data derived from zip area codes. the training set consists 1 descriptions of customers  including the information of whether or not they have a caravan insurance policy. a test data set contains 1 tuples table 1: a 1〜1 contingency table for variable a and
b
bbbban1n1en1.a1e1an1n1n1.a11n.1n.1n..11	e	e
 a  stratum form  b  stratum for c = 1 d = 1
bba1e1a1111e
 c  marginal table
which only the organizers know if they have a caravan insurance policy. here our aim is to identify interaction patterns among 1 attributes varying from product usage to socio-demographic. our data is formed by collapsing nonbinary categorical attributes into binary form  the data can be found at www.cs.uncc.edu/ xwu/classify/b1.dat   with n = 1 baskets and k = 1 binary items.
　we successfully decomposed the data set with 1 variables into components with much less variables  the largest one with 1 variables and most components with less than 1 variables . after decomposition  we got 1 components  figure 1 shows 1 components which contain 1 or more variables  and we then fit each component by using loglinear model.
　we also did the experiment over this data set by using the apriori algorithm. the algorithm generated 1 large item sets and 1 rules under support 1 and confidence 1. we found it was much harder to draw interesting conclusions about data from support-confidence results. we compared the significant interactions discovered by our algorithm with the large item sets discovered by apriori algorithm and found the percentage of overlap is very low. table 1 shows several significant interactions discovered by our loglinear fitting algorithm and the actual support valtable 1: component after decomposition for coil data set  we omit 1 components which contain less than 1 variables.
componentvariables1  1 1  1 1 1  1 1 1 1 11 1 1 1 1 11  1  1 11 11  11  11 11 11 11  1table 1: several significant interactions
terminteractionactual support % γ1 1-11γ1 1.1.1γ1 1-11γ1 111γ1 1.1ues for those subsets. the lower support value for all subsets  except for γ1 1  definitely prevent them to be discovered by traditional support-confidence framework. for instance  the association γ1 1 reveals that people are inclined to buy delivery van policies  1   agricultural machines policies  1  and disability insurance policies  1  together.
1 synthetic data set
　the coil data set is too sparse to study the performance  running time  of our algorithm. in order evaluate the performance our algorithm properly  we turn to synthetic data  the same market basket data generator used in   from ibm's quest group.
　we generated two data sets  one with 1 items and the other with 1 items . we have not done the experiments over data sets with more than 1 variables as we have used coco   an environment for graphical models  which can not deal with more than 1 variables. we are currently im-
table 1: parameter description
parametervaluemeaningntrans1k-1mnumber of transactionsnitems1number of different itemstlen1average items per transactionnpats1number of patterns  large item sets patlen1average length of maximal patterncorr1correlation between patternsconf1average confidence in a rule
figure 1: execution time by varying ntrans from
 1k  1k  1k  1k  1k  1k  1m 
plementing the decomposition algorithm proposed by  to be able to handle data sets with larger number of variables. we set the average basket size to be 1  the average of large itemsets to be 1  the correlation between large itemsets to be 1  the confidence in a rule to be 1  the number of transactions varying from 1k to 1m. we ran some experiments with the tlen set to 1 or the correlation level set to 1 but did not find significant difference in the nature of our performance results.
　figure 1 shows our execution time. note that decomposition step is determined by the size of independence graph  i.e.  the number of variables k  the number of edges e or the number of generators m . we observe the decomposition time is small compared with the preprocessing time because the size of independence graph in our experiment is usually small  with 1 nodes and several hundreds edges . as the number of items contained in each component is comparably small  most less than 1   the time of loglinear model fitting for each component is trivial. in figure 1  we also include the execution time of apriori algorithm  with mininum support 1% and minimum confidence 1% . we can see the execution time of our algorithm is comparable to that of apriori algorithm for medium dimension size  1  1 . figure 1 shows the number of components generated in our experiment. when we fix the other parameters of market basket generater and increase the number of transactions  the number of components decreases because the number of edges in independence graph increases.
1. conclusions and future work
　in this paper we presented how to interpret associations among items by fitting loglinear models and examining those magnitude of parameters for market basket data. our work departed from earlier work that just aims to find large or interesting itemsets and leaves those itemsets to domain expert directly. on the contrary  we build loglinear model and apply the values of γ-terms as measures of associations among item variables directly. we believe those values provided by our loglinear model are very helpful for domain expert to make judgments about cause and effect relations among items. to deal with large number of variables  we

figure 1: number of components by varying ntrans from  1k  1k  1k  1k  1k  1k  1m 
applied graph-theoretical results to decompose items into subsets without losing any significant interaction.
　there are some aspects of this work that merit further research. among them  we are trying to automatically derive rules from the γ-terms included in fitted loglinear model. for components with more than 1 variables  it is hard for user to grasp all the association patterns. we will be exploring how to combine visualization techniques and association graph for this issue.
　another aspect that merits further research is that of interactive analysis of associations among items. for example  the user may want to examine a given subset  say abc . clearly collapsing into contingency table of abc directly will lose information as item a  b  or c may have interactions with other items. to find the smallest set containing a given set  i.e.  abc  and onto which the model is collapsible was studied in . we will investigate this problem for online association analysis.
　finally  we will study how to better deal with sparse data when either structural zero cells present or it contains many small cell values. it is known that loglinear model can still work for small incomplete table with structural or sampling zeros . we will investigate other techniques such as shrinkage estimates  for large incomplete market basket data.
1. acknowledgments
　the authors would like to thank christian borgelt for providing his implementation of the apriori algorithm. we would like to thank jens henrik badsberg for his coco program which makes our experiments possible. we would also like to thank ibm quest group for providing the market basket data generator.
