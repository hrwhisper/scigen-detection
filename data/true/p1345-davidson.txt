provenance in the context of workflows  both for the data they derive and for their specification  is an essential component to allow for result reproducibility  sharing  and knowledge re-use in the scientific community. several workshops have been held on the topic  and it has been the focus of many research projects and prototype systems. this tutorial provides an overview of research issues in provenance for scientific workflows  with a focus on recent literature and technology in this area. it is aimed at a general database research audience and at people who work with scientific data and workflows. we will  1  provide a general overview of scientific workflows   1  describe research on provenance for scientific workflows and show in detail how provenance is supported in existing systems;  1  discuss emerging applications that are enabled by provenance; and  1  outline open problems and new directions for database-related research.
categories and subject descriptors
h.1  database management : general
general terms
documentation experimentation
keywords
provenance  scientific workflows
1. importance of provenance for workflows
　computing has been an enormous accelerator to science and has led to an information explosion in many different fields. to analyze and understand scientific data  complex computational processes must be assembled  often requiring the combination of looselycoupled resources  specialized libraries  and grid and web services. these processes may generate many final and intermediate data products  adding to the overflow of information scientists need to deal with. ad-hoc approaches to data exploration  e.g.  perl scripts 
copyright is held by the author/owner s .
sigmod'1  june 1  1  vancouver  bc  canada.
acm 1-1-1/1.
have been widely used in the scientific community  but have serious limitations. in particular  scientists and engineers need to expend substantial effort managing data  e.g.  scripts that encode computational tasks  raw data  data products  and notes  and recording provenance information so that basic questions can be answered  such as: who created this data product and when  when was it modified and by whom  what was the process used to create the data product  were two data products derived from the same raw data  not only is the process time-consuming  but also error-prone.
　workflow systems have therefore grown in popularity within the scientific community  1  1  1  1  1  1  1  1  1  1 . not only do they support the automation of repetitive tasks  but they can also capture complex analysis processes at various levels of detail and systematically capture provenance information for the derived data products . the provenance  also referred to as the audit trail  lineage  and pedigree  of a data product contains information about the process and data used to derive the data product. it provides important documentation that is key to preserving the data  to determining the data's quality and authorship  and to reproduce as well as validate the results. these are all important requirements of the scientific process.
　provenance in scientific workflows is thus of paramount and increasing importance  as evidenced by recent specialized workshops  1  1  1  1  1  and surveys  1  1  1  1 . while provenance in workflows bears some similarity to that of provenance in databases  which was the topic of a tutorial in sigmod'1  and a recent survey    there are important differences and new challenges for the database community to consider.
　our objective in this tutorial is to give an overview of the problem of managing provenance data for scientific workflows  illustrate some of the techniques that have been developed to address different aspects of the problem  and outline interesting directions for future work in the area. in particular  we will present techniques for reducing provenance overload as well as making provenance information more  fine-grained.  we will examine uses of provenance that go beyond the ability to reproduce and share results  and will demonstrate how workflow evolution provenance can be leveraged to explain difference in data products  streamline exploratory computational tasks  and enable knowledge re-use. we will also discuss a new applications that are enabled by provenance  such as social data analysis   which have the potential to change the way people explore data and do science.
1. tutorial outline
1 overview of scientific workflows
　we motivate the need for scientific workflows using real applications as examples  in particular within genomics  medical imaging  environmental observatories and forecasting systems. we also introduce basic concepts for scientific workflows that are related to provenance.
　workflow and workflow-based systems have emerged as an alternative to ad-hoc approaches for constructing computational scientific experiments  1  1  1  1  1 . workflow systems help scientists conceptualize and manage the analysis process  support scientists by allowing the creation and reuse of analysis tasks  aid in the discovery process by managing the data used and generated at each step  and  more recently  systematically record provenance information for later use. workflows are rapidly replacing primitive shell scripts as evidenced by the release of apple's mac os x automator  microsoft's workflow foundation  and yahoo! pipes.
　scientific workflows systems often adopt simple computational models  in particular a dataflow model  where the execution order of workflow modules is determined by the flow of data through the workflow. this is in contrast to business workflows which provide expressive languages  such as the business process execution language  bpel   to specify complex control flows . in addition  unlike business workflows  scientific workflows are often used to perform data intensive tasks.
　workflow systems have a number of advantages for constructing and managing computational tasks compared to programs and scripts. they provide a simple programming model whereby a sequence of tasks is composed by connecting the outputs of one task to the inputs of another. furthermore  workflow systems often provide intuitive visual programming interfaces  which make them more suitable for users who do not have substantial programming expertise. workflows also have an explicit structure. they can be viewed as graphs  where nodes represent processes  or modules  and edges capture the flow of data between the processes. the benefits of structure are well-known when it comes to exploring data. a program  or script  is to a workflow what an unstructured document is to a  structured  database.
1 managing provenance
　we first describe different kinds of provenance that can be captured for scientific workflows. then  we discuss the three key components of a provenance management solution: the capture mechanism; the data model for representing provenance information; and the infrastructure for storing  accessing  and querying provenance. last  but not least  we present different approaches used for each of these components and classify the different workflow systems based on a set of dimensions along which their treatments of the issues differ.
information represented in provenance. in the context of scientific workflows  provenance is a record of the derivation of a set of results. there are two distinct forms of provenance : prospective and retrospective. prospective provenance captures the specification of a computational task  i.e.  a workflow -it corresponds to the steps that need to be followed  or a recipe  to generate a data product or class of data products. retrospective provenance captures the steps that were executed as well as information about the execution environment used to derive a specific data product- a detailed log of the execution of a computational task. figure 1 illustrates these two kinds of provenance.
an important piece of information present in workflow prove-

figure 1: prospective versus retrospective provenance. the workflow generates two data products: a histogram of the scalar values of a structured grid data set; and a visualization of an isosurface of the data set. the workflow definition provides prospective provenance  a recipe to derive these two kinds of data products. on the left  we show some of the retrospective provenance that was collected during a run of this workflow. this figure also illustrates user-defined provenance in the form of annotations  shown in yellow boxes.
nance is information about causality: the dependency relationships among data products and the processes that generate them. causality can be inferred from both prospective and retrospective provenance and it captures the sequence of steps which  together with input data and parameters  caused the creation of a data product. causality consists of different types of dependencies. data-process dependencies  e.g.  the fact that head-hist.png was derived by the sub-workflow on the left in figure 1  are useful for documenting data generation process  and they can also be used to reproduce or validate the process. for example  it would allow new histograms to be derived for different input data sets. data dependencies are also useful. for example  in the event that the ct scanner used to generate the input file head.1.vtk is found to be defective  results that depend on the scan can be invalidated by examining data dependencies.
　another key component of provenance is user-defined information. this includes documentation that cannot be automatically captured but records important decisions and notes. this data is often captured in the form of annotations. as figure 1 illustrates  annotations can be added at different levels of granularity and associated with different components of both prospective and retrospective provenance  e.g.  for modules  data products  execution log records .
capturing  modeling  storing and querying provenance. one of the major advantages to using workflow systems is that they can be easily instrumented to automatically capture provenance - this information can be accessed directly through system apis. while early workflow systems  e.g.  taverna  and kepler   have been extended to capture provenance  newer systems  such as vistrails  have been designed to support provenance.

figure 1: usable interface to refine workflows by analogy. the user chooses a pair of data products to serve as an analogy template. in this case  the pair represents a change to a workflow that downloads a file from the web and creates a simple visualization  into a new workflow where the resulting visualization is smoothed. then  the user chooses a set of other workflows to apply the same change automatically. the workflow on the left reflects the original changes  and the one on the right reflects the changes when translated to the workflow used to derive the last visualization on the right. the workflow components to be removed are shown in orange  and the ones to be added  in blue. note that the surrounding modules do not match exactly: the system identifies out themost likely match. image from .
　several provenance models have been proposed in the literature  1  1  1  1  1  1  1  1  1 . all of these models support some form of retrospective provenance and many also provide the means to capture prospective provenance as well as annotations. although these models differ in many ways  including the use of different structures and storage strategies  they all share an essential type of information: process and data dependencies. in fact  a recent exercise to explore interoperability issues among provenance models has shown that it is possible to integrate different provenance models .
　while several approaches have been proposed to capture and model provenance  only recently has the problem of storing  accessing  and querying provenance started to receive attention. besides allowing users to explore and better understand results  the ability to query the provenance of workflows enables knowledge re-use. for example  users can identify workflows that are suitable and can be re-used for a given task; compare and understand differences between workflows; and refine workflows by analogy  see figure 1 . provenance information can also be associated with data products  e.g.  images  graphs   allowing structured queries to be posed over these unstructured data.
　a common feature across many of the approaches to querying provenance is that their solutions are closely tied to the storage models used. a wide variety of data models and storage systems have been used ranging from specialized semantic web languages  e.g.  rdf and owl  and xml dialects that are stored as files and to tuples stored in relational database tables. hence  they require users to write queries in languages like sql   prolog  and sparql  1  1  1 . while such standard languages can be useful if users are already familiar with their syntax  none of them have been designed for provenance. for that reason  simple queries can be awkward and complex. we will discuss these approaches and contrast them to recent work on intuitive visual interfaces to query workflows  1  1 .
provenance systems. we survey approaches to provenance adopted
by scientific workflow systems. we present and compare different proposals for capturing  modeling  storing and querying provenance  e.g.   1  1  1  1  1  1  1  1  1  .
1 using provenance for reproducibility and beyond
　we will also discuss a number of emerging applications for workflow provenance and discuss the challenges they pose to database research. some of these applications are described below.
provenance and scientific publications. a key benefit for maintaining provenance of computational results is reproducibility: a detailed record of the steps followed to produce a result allows others to reproduce and validate these results. recently  the issue of publishing reproducible research has started to receive attention in the scientific community. in 1  sigmod has introduced the  experimental repeatability requirement  to  help published papers achieve an impact and stand as reliable reference-able works for future research .1 a number of journals are also encouraging authors to make their publications reproducible  including  for example the ieee transactions on signal processing1 and the computing in science and engineering  cise  magazine1. provenance management infrastructure and tools will have the potential to transform scientific publications as we know them today. however  for these to be widely adopted  they need to be usable and within reach for scientists that do not have computer science training.
provenance and data exploration. provenance can also be used to simplify exploratory processes. in particular  we present mechanisms that allow the flexible re-use of workflows; scalable exploration of large parameter spaces; and comparison of data products as well as their corresponding workflows  1  1 . in addition  we show that useful knowledge is embedded in provenance which can be re-used to simplify the construction of workflows .
social analysis of scientific data. social web sites and web-based communities  e.g.  flickr  facebook  yahoo! pipes   which facilitate collaboration and sharing between users  are becoming increasingly popular. an important benefit of these sites is that they enable users to leverage the wisdom of the crowds. in the  very  recent past  a new class of web site has emerged that enables users to upload and collectively analyze many types of data  e.g.  many eyes  . these are part of a broad phenomenon that has been called  social data analysis . this trend is expanding to the scientific domain where a number of collaboratories are under development. as the cost of hardware decreases over time  the cost of people goes up as analyses get more involved  larger groups need to collaborate  and the volume of data manipulated increases. science collaboratories aim to bridge this gap by allowing scientists to share  re-use and refine their workflows. we discuss the challenges and key components that are needed to enable the development of effective social data analysis  sda  sites for the scientific domain . for example  usable interfaces that allow users to query and re-use the information in these collaboratories are key to their success. we will present recent work that has addressed usability issues in the context of workflow systems and provenance  see figure 1 .
provenance in education. teaching is one of the killer applications of provenance-enabled workflow systems  in particular  for courses which have a strong data exploration component such as data mining and visualization. provenance can help instructors to be more effective and improve the students' learning experience. by using a provenance-enabled tool in class  an instructor can keep detailed record of all the steps she tried while while responding to students questions; and after the class  all these results and their provenance can be made available to students. for assignments  students can turn the detailed provenance of their work  showing all the steps they followed to solve a problem.
1 open problems
　we discuss a number of open problems and outline possible directions for future research  including:
  information management infrastructure. with the growing volume of raw data  workflows and provenance information  there is a need for efficient and effective techniques to manage these data. besides the need to handle large volumes of heterogeneous and distributed data  an important challenge that needs to be addressed is usability: information management systems are notoriously hard to use  1  1 . as the need for these systems grows in a wide range of applications  notably in the scientific domain  usability is of paramount importance. the growth in the volume of provenance data also calls for techniques that deal with information overload .
  provenance analytics and visualization. the problem of mining and extracting knowledge from provenance data has been largely unexplored. by analyzing and creating insightful visualizations of provenance data  scientists can debug their tasks and obtain a better understanding of their results. mining this data may also lead to the discovery of patterns that can potentially simplify the notoriously hard  time-consuming process of designing and refining scientific workflows.
  interoperability. complex data products may result from long processing chains that require multiples tools  e.g.  scientific workflows and visualization tools . in order to provide detailed provenance for such data products  it becomes necessary to integrate provenance derived from different systems and represented using different models. this was the goal of the second provenance challenge   which brought together several research groups with the goal of integrating provenance across their independently developed workflow systems. although the preliminary results are promising and indicate that such an integration is possible  there needs to be more principled approaches to this problem. one direction currently being investigated is the creation of a standard for representing provenance .
  connecting database and workflow provenance. in many scientific applications  database manipulations co-exist with the execution of workflow modules: data is selected from a database  potentially joined with data from other databases  reformatted  and used in an analysis. the results of the analysis may then be put into a database and potentially used in other analyses. to understand the provenance of a result  it is therefore important to be able to connect provenance information across databases and workflows. combining these disparate forms of provenance information will require a framework in which database operators and workflow modules can be treated uniformly  and a model in which the interaction between the structure of data and the structure of workflows can be captured.
1. about the presenters
　susan b. davidson received a b.a. degree in mathematics from cornell university in 1  and a ph.d. degree in electrical engineering and computer science from princeton university in 1. dr. davidson joined the university of pennsylvania in 1  and is now the weiss professor and department chair of computer and information science. she is an acm fellow  a fulbright scholar  and a founding co-director of the center for bioinformatics at upenn  pcbi . dr. davidson's research interests include database systems  database modeling  distributed systems  and bioinformatics. within bioinformatics she is best known for her work in data integration  xml query and update technologies  and more recently provenance in workflow systems.
　juliana freire joined the faculty of the school of computing at the university of utah in july 1. before  she was member of technical staff at the database systems research department at bell laboratories  lucent technologies  and an assistant professor at ogi/ohsu. she received ph.d. and m.s. degrees in computer science from the state university of new york at stony brook  and a b.s. degree in computer science from universidade federal do cear│  brazil. dr. freire's research has focused on extending traditional database technology and developing techniques to address new data management problems introduced by the web and scien-
tific applications. she is a co-creator of vistrails  www.vistrails.org   an open-source scientific workflow and provenance management system.
1. acknowledgments
　this work was partially supported by the national science foundation and the department of energy.
