this paper describes the setup and results of our contribution to the trec 1 terabyte track. our implementation was based on the algorithms proposed in   iotop-k: index-access optimized top-k query processing  vldb'1   with a main focus on the efficiency track.
1. introduction
　io-top-k  extends the family of threshold algorithms  ta   1  1  1  with a suite of new strategies. to retrieve the best-scoring  so-called top-k  answers to a multi-keyword query under a monotonic aggregation of per-keyword scores  ta-style algorithms perform index scans  so-called sorted accesses  over precomputed index lists  one for each keyword in the query  which are sorted in descending order of perkeyword scores. the key point of ta is that it aggregates scores on the fly  thus computes a lower bound for the total score of the current rank-k result document and an upper bound for the total scores of all other candidate documents  and is thus often able to terminate the index scans long before it reaches the bottom of the index lists  namely  when the lower bound for the rank-k result  the threshold  is at least as high as the upper bound for all other candidates. additional  carefully selected random accesses to reveal the score of a candidate document in a list where it has not yet been seen so far can further speed up the computation. the goal of such algorithms is to minimize the sum of the access costs  assuming a fixed cost cs for each sorted access and a fixed cost cr for each random access. in a realistic scenario  random accesses are a factor of 1 to 1 more expensive than sorted accesses. we aim at accelerating queries and  at the same time  limit or even aim to reducing the memory consumption for candidate queues and other auxiliary data structures.
　for our participation in trec 1  we selected two strategies from the suite of algorithms presented in :
  a scheduling strategy for random accesses that postpones all random accesses to the end of the execution  switching from scans to random accesses when the estimated cost for them is the same.
  a heuristics for early termination that scans only a configurable fraction of the lists  regardless of the score bounds.
1. computationalmodelandscoring
　we associate with each document-term pair a numeric score that reflects the  goodness  or relevance of the data item with regard to the term. as effectiveness was not in the focus of our experiments  we chose the well-known probabilistic okapi bm1 score derived from term frequencies  tf  and inverse document frequencies  idf  . we boost the frequency of terms within important tags  like title  h1  or caption  by an additional  tag-specific weight. denoting the score of document dj for the ith dimension by sij  we get

where tfi dj  is the term frequency of term i in document dj  dfi is the document frequency of term i  and

　for our experiments  we chose k1 = 1 and b = 1. all scores are normalized to the interval  1   with 1 being the best possible score.
　a top-k query asks for those k documents with the highest score sum. note that such a document must not necessarily contain all query words  because a document containing just some of the query words but with a high score each can have a larger score sum than a document which contains all query words  but with only a relatively low score each.
1. inverted block-index
　the documents that contain specific terms and their corresponding scores are precomputed and stored in inverted index lists li  i = 1..m . there is one such index list per term. the entries in a list are  docid  score  pairs. the lists may be very long  millions of entries  and reside on disk  with a b+-tree or similar data structure for efficiently locating the keys of the lists  i.e.  the attribute values or terms . we partition each index list into blocks and use score-descending order among blocks but keep the index entries within each block in docid order. this special ordering  which is halfway between an ordering by score and an ordering by doc id  is key to efficiently manage the substantial bookkeeping required in ta-style query processing.
　the block size is a configuration parameter that is chosen in a way that balances disk seek time and transfer rate; a typical block size would be 1.
1. query processing
　our query processing model is based on the nra and ca variants of the ta family of algorithms . an mdimensional top-k query  with m search conditions  is primarily processed by scanning the corresponding m index lists in descending score orders in an interleaved  roundrobin manner  and by making judicious random accesses to look up index entries of specific documents . without loss of generality  we assume that these are the index lists numbered l1 through lm.
　when scanning the m index lists  the query processor collects candidates for the query result and maintains them in two priority queues  one for the current top-k items and another one for all other candidates that could still make it into the final top-k. for simpler presentation  we assume that the score aggregation function is simple summation  but it is easy to extend this to other monotonic functions . the query processor maintains the following state information:
  the current cursor position posi for each list li 
  the score values highi at the current cursor positions  which serve as upper bounds for the unknown scores
in the lists' tails 
  a set of current top-k items  d1 through dk  renumbered to reflect their current ranks  and a set of data items dj  j = k + 1..k + q  in the current candidate queue q  each with
- a set of evaluated dimensions e dj  in which dj has already been seen during the scans or by random lookups 
- a set of remainder dimensions e． dj  for which the score of dj is still unknown 
- a lower bound worstscore dj  for the total score of dj which is the sum of the scores from e dj  
- an upper bound bestscore dj  for the total score of dj which is equal to
x
	worstscore dj  +	highν
ν（e． dj 
 and not actually stored but rather computed from worstscore dj  and the current highν values whenever needed .
in addition  the following information is derived at each step:
  the minimum worstscore min-k of the current top-k docs  which serves as the stopping threshold 
  the bestscore that any currently unseen document can get  which is computed as the sum of the current highi values  and
  and for each candidate  a score deficit δj = min-k   worstscore dj  that dj would have to reach in order to qualify for the current top-k.
the top-k queue is sorted by worstscore values  and the candidate queue is sorted by descending bestscore values. ties among scores may be broken by using the concatenation of  score  docid  for sorting. the invariant that separates the two is that the rank-k worstscore of the top-k queue is at least as high as the best worstscore in the candidate queue. the algorithm can safely terminate  yielding the correct top-k results  when the maximum bestscore of the candidate queue is not larger than the rank-k worstscore of the current top-k  i.e.  when
 min {worstscore d } =: min-k − max{bestscore c } d（top-k	c（q
more generally  whenever a candidate in the queue q has a bestscore that is not higher than min-k  this candidate can be pruned from the queue. early termination  i.e.  the point when the queue becomes empty  is one goal of efficient top-k processing  but early pruning to keep the queue and its memory consumption small is an equally important goal  and is not necessarily implied by early termination . the candidate bookkeeping is illustrated in fig. 1.

figure 1: top-k and candidate bookkeeping.
　the state information we have to maintain for each document  from the point it is first encountered to the point where it is surely known that either the document is one of the top k or that it cannot be  adds a noticeable amount of overhead to the algorithm. this has to be contrasted with a simple full merge  of the lists sorted by document ids   which can compute the full scores document by document  and then determine the top-k items by a  partial  sort. it is not at all obvious  and indeed put forward as an open problem in   whether the state maintenance of any of the sophisticated ta-style algorithms can be implemented efficiently enough so that the gains in the abstract cost indeed show in faster running times.
　in our first implementation we maintained all state information in a hash data structure; indeed  this is the approach taken in all top-k implementations that we are aware of . however  despite their strong advantage in theoretical cost  none of our sophisticated algorithms could beat the simple full-merge baseline in this implementation. we then switched to the inverted block-index described in section 1. an essential ingredient of our implementation is to keep the state information in-place  i.e.  in a contiguous memory segment together with the document id. the process of merging two or more document lists  and updating all state information then has almost optimal locality properties.
　the most time-critical step in the merge is the computation of the bestscore  which we do not store explicitly but rather compute from the worstscore and the set of lists in which the documents have been seen so far. we store this seen information by a simple m-bit vector  where m is the number of lists  and for each round precompute all 1m partial sums of the high-scores highi of each list  see section 1 . for any document  the bestscore can then be computed from the worstscore by a simple table lookup with the seen-bitvector serving as a direct index into that table.
　to keep the merges as fast as those of the baseline fullmerge  we also do not maintain the set of top-k items as we merge  and not even the min-k score. we rather do the merge twice  outputting only the scores in the first round  doing a partial sort of these to obtain the min-k score  and then repeat the merge  but this time with an on-the-fly pruning of all documents with a bestscore below that min-k score. by these  and a bag of other tricks  we managed to keep the overhead for maintaining the state-information a small fraction of the essential operations of reading and merging blocks of pairs of document ids and score  sorted by document id.
1. schedulingofrandomaccesses
　random-access  ra  scheduling is crucial both in the early and the late stages of top-k query processing. in the early stage  it is important to ensure that the min-k threshold moves up quickly so as to make the candidate pruning more effective as the scans proceed and collect large amounts of candidates. later  it is important to avoid that the algorithm cannot terminate merely because of a few pieces of information missing about a few borderline candidates. in   we analyzed various strategies for deciding when to issue ras and for which candidates in which lists; for our experiments  we focused on one of these strategies that was both efficient to compute and resulted in a good performance. following the literature  1  1   we refer to score lookups by ra as probing. as in   we denote by cs the cost of a sorted access  and by cr the cost of a random access.
　our scheduling strategy for random accesses  coined lastprobing in   does a balanced number of random accesses just as fagin's ca algorithm  that is  the total cost of the random accesses is about the same as the total cost of all sorted accesses. in ca  this is trivially achieved by doing one random access after each round of dcr/cse sorted accesses. in last-probing  we perform random accesses only after the last round  that is  we have a phase of only sorted accesses  followed by a phase of only random accesses.
　we do this by estimating  after each round  the number of random accesses that would have to be done if this were the last round of sorted accesses. two criteria must be met for this round of sorted accesses being the last. first  the estimated number of random accesses must be less than dcr/cse times the number of all sorted accesses done up to this point
second  we must have  since only then we can be sure that we have encountered all the top-k items already. we remark that in all our applications  the second criterion is typically fulfilled long before  that is  after much fewer rounds than  the first criterion. a simple estimate for the number of random lookups that would have to be done if we stopped doing sorted accesses at a certain point  is the number of candidate documents which are then in our queue. when the distribution is very skewed  it is in fact quite a good estimate  because then each document in the queue has a positive but only very tiny probability of becoming one of the top-k items.
　when doing the random accesses  it plays a role in which order we process the documents for which we do random lookups. in our algorithm  we first schedule ras for any items in the current top-k that are not yet completely evaluated  in decreasing order of their worst scores. then  we schedule ras for items in the candidate queue  ordered by decreasing bestscore  last-best . this is similar to ca  which after each round of sorted accesses does a random access for the candidate document with the highest bestscore. note that unlike more aggressive pruning strategies proposed in the literature  1  1  1  that provide approximate top-k results  our method is non-approximative and achieves major runtime gains with no loss in result precision.
1. early stopping heuristic
　we used an early stopping heuristic for two of our efficiency runs. for these runs  we ignored all the blocks after the first 1-th of the blocks in every list  e.g. if there are 1 blocks in a list  we only considered the first 1 blocks . note that since the blocksize of our inverted block index was as large as 1  all the small lists were almost fully scanned. only for the very long lists  the tails are ignored. as we find from the results  see section 1   this heuristic works quite well in practice.
1. test platform
　we parsed the collection on a small cluster of three servers  each with two intel xeon processors running windows at 1 ghz. our trec runs were performed on a single machine having two 1 mhz amd opteron cpus and 1 gb of memory. the index files were stored on a local 1 gb scsi disk with 1 rpm rotational speed. the operating system used was linux.
1. indexing
　we indexed the collection using okapi bm1 scoring function with standard parameters after removing stopwords  but without stemming. prior to computing the bm1 scores  the term frequencies  tfs  of the terms were computed using a weighted sum of term-occurrences  with the weights being one for occurrences in standard text  and between 1 and 1 for occurrences inside special html tags  see table 1 for details . the term scores were initially stored in a relational schema of the form  docid term score  in an oracle database. after the parsing  the index lists were created from the database and stored in two files  namely one for sorted access and one for random access.
html tagfactortitle  in url1h1 - h1.1h1 - h1  strong  b  caption  th1em  i  u  dl  ol  ul  a  meta1table 1: term weights for different tags
　the inverted block-index for sorted access stored  for each term  a list of pairs of the form  docid score   together with a two level b-tree to store the offsets of the lists corresponding to every term. the index for random access stored list of pairs  termid score  for every document and the offsets for the lists were stored in a single array. the total size of our index files on disk was about 1 gb  each index contributing to little more than 1 gb. at runtime  only the offsets of the lists for random access  about 1 mb  resided in memory  all other data were read from disk. we did not use any caching other than some automatic filesystem caching over which we did not have any control.
1. runs and results
　we submitted four runs for the efficiency task and four runs for the adhoc task. however  this year  our main focus was in the efficiency task.
1 efficiency task
　for all of the efficiency runs  the queries were parsed automatically from the query streams  and all the words present in each query line were taken as keywords. since stopwords were removed at the time of parsing  such words automatically did not play any role in retrieval.
　the four runs submitted for the efficiency task were based on two versions of our algorithm  as follows:
  mpiiotopk - computation of exact top-1 documents  as defined in section 1 and section 1  for each query. the 1 queries were processed sequentially from a single stream. the average running time was 1 sec. this run essentially used a single processor.
  mpiiotopkpar - computation of exact top-1 documents using the same scheme as in mpiiotopk. however  for this run the queries were processed from four streams in parallel. note that the documents returned by this run were the same as the documents returned by mpiiotopk. in spite of processing four streams parallally  this run is only about twice faster  average running time: 1 sec  than the previous run  because the machine we used had only two processors.
  mpiiotopk1 - avoids scanning deep into long lists using the early stopping heuristic as described in section 1  with all 1 queries being processed sequentially from a single stream. the documents returned by this run are not the exact top-1. using the early stopping heuristic  the average runtime improves by more than 1 times  average running time: 1 sec  from the exact run.
  mpiiotopk1p - same as mpiiotopk1  but four query streams were processed in parallel. again  the parallelization improves the running time only by a factor of two  average running time: 1 sec   because the program was run on a machine with two processors. a proper parallelization of the process with at least 1 processors could boost the efficiency of these runs  but we did not have such a setup at the time of performing these experiments.
the documents returned by the first two runs  mpiiotopk and mpiiotopkpar  are precisely those that any retrieval model using standard bm1 scoring function would return. the precisons of these runs turned out to be decent  namely 1 on average for topics 1 and 1 on average for topics 1. interestingly  the precisions of the runs mpiiotopk1 and mpiiotopk1p using the early stopping heuristic were not much worse for topics 1  1  and equally good for topics 1  1 . since the blocksize of our inverted block index was large  1   the first block of every list was always scanned and ignoring the tail of long lists did not affect the retrieval quality much  instead we gained a factor of more than 1 in running time. the details of the runs are given1 in table 1.
run#cpuavg queryp 1 topicsp 1 topicstime11mpiiotopk1.1.1.1mpiiotopkpar1.1.1.1mpiiotopk1111mpiiotopk1p1.1.1.1
table 1: performances of runs in the efficiency task: the number of cpus used  average running time per query and precision at top-1 for our runs. the median of the average running time taken over all 1 submitted runs by all groups turns out to be exactly the same as our slowest run  1 sec .
1 adhoc task
　our adhoc runs were based on simple methods for constructing queries from the topics provided. for this task also we used the bm1 scoring model default parameters. for all runs  queries were processed automatically using the exact top-k algorithm same as our efficiency runs  e.g. as in mpiiotopk . for the four runs  the queries were constructed as follows:
  mpiirtitle - words in the title fields were taken as keywords.
  mpiirdesc - words in the description fields were taken as keywords.
  mpiircomb - words in the title as well as in the description fields  with possible repetition  were taken as keywords.
  mpiirmanual - only the construction of the queries were manual  as the keywords were chosen manually by only looking at the title  description and narrative fields.
among these runs  the precisions of mpiircomb and mpiirmanual are better than the other two runs  as we see in table 1.
1. conclusions
　our focus this year was on the efficiency track. telling from the statistics posted by the trec organizers  our runs performed very well. our slowest  single-processor  run was the median of all runs and our fastest run  processing 1 streams  but with only two processors  average query time:
1 sec  was close to the best run  average query time: 1 sec . in all our experiments  most of the data was read from disk  as opposed to from main memory . the
runp 1bprefmapinfapmpiirtitle1111mpiirdesc1111mpiircomb1111mpiirmanual1111
table 1: performances of runs in the adhoc task by p 1  bpref  map and infap measures  averaged over topics 1.
precisions of our runs were decent  but could be improved by more advanced scoring models  without compromising efficiency.
