the research in information extraction  ie  regards the generation of wrappers that can extract particular information from semistructured web documents. similar to compiler generation  the extractor is actually a driver program  which is accompanied with the generated extraction rule. previous work in this field aims to learn extraction rules from users' training example. in this paper  we propose iepad  a system that automatically discovers extraction rules from web pages. the system can automatically identify record boundary by repeated pattern mining and multiple sequence alignment. the discovery of repeated patterns are realized through a data structure call pat trees. additionally  repeated patterns are further extended by pattern alignment to comprehend all record instances. this new track to ie involves no human effort and content-dependent heuristics. experimental results show that the constructed extraction rules can achieve 1 percent extraction over fourteen popular search engines. 
keywords 
information extraction  extraction rule  pat tree  multiple string alignment 
1. introduction 
current web sites present information on various topics in various formats.  a great amount of effort is often required for a user to manually locate and extract useful data from the web sites. therefore  there is a great need for value-added service that integrates information from multiple sources. for example  customizable web information gathering robots/crawlers  comparison-shopping agents  meta-search engines and newsbots  etc. to facilitate the development of these information integration systems  we need good tools for information gathering and extraction. suppose the data has been collected from different web sites  a conventional approach for extracting data from various web pages would have to write programs  called  wrappers  or  extractors   to extract the contents of the web pages based on a priori knowledge of their format. in other words  we have to observe the extraction rules in person and write programs for each web site. however  programming wrappers require manual coding which generally entails extensive debugging and is  therefore  labor-intensive. in addition  since the format of web pages is often subject to change  maintaining the wrapper can be expensive and impractical. 
fortunately  researchers have built tools that can generate wrappers automatically. for example  wien   softmealy   stalker  etc. are three famous works in this field. similar to scanner or parser generator for compilers where users provide the syntax grammar and get the transition tables for scanner or parser drivers  these wrapper construction systems actually output extraction rules from training examples provided by the designer of the wrapper. the common idea involved is the machine learning techniques to summarize extraction rules  while the difference is the extractor architectures presumed in each system. for example  the single-pass  lr structure in wien and the multi-pass  hierarchical structure in stalker. nevertheless  the designer must manually label the beginning and the end of the training examples for generating the rules. manual labeling  in general  is time-consuming and not efficient enough.   
recently  researchers are exploring new approaches to fully automate wrapper construction. that is  without users' training examples. for example  embley et al. describe a heuristic approach to discover record boundaries in web documents by identifying candidate separator tags using five independent heuristics and selecting a consensus separator tag based on a heuristic combination . however  one serious problem in this one-tag separator approach arises when the separator tag is used elsewhere among a record other than the boundary. 
on the other hand  our work here attempts to eliminate human intervention by pattern mining . the motivation is from the observation that useful information in a web page is often placed in a structure having a particular alignment and order. particularly  web pages produced by search engines generally present search results in regular and repetitive patterns. mining repetitive patterns  therefore  may discover the extraction rules for wrappers.  
in this paper  we introduce iepad  an information extraction system applying pattern discovery techniques. in section 1  we present the system overview of iepad  including a pattern viewer  a rule generator and an extractor module. in section 1  we present the details of rule generator  followed by the embodiment of extractor in section 1. finally  we present experimental results in section 1 and make the conclusion in section 1. 
1. system overview 
the system iepad includes three components  an extraction rule 

 
copyright is held by the author/owner. 
www1 '1  may 1  1  hong kong.  
copyright 1 acm 1-1/1...$1. 
 
generator which accepts an input web page  a graphical user interface  called pattern viewer  which shows repetitive patterns discovered  and an extractor module which extracts desired information from similar web pages according to the extraction rule chosen by the user.  
html page

extraction rules
figure 1. extraction rule generator
the core techniques of pattern mining are implemented in the rule generator. the extraction rule generator includes a translator  a pat tree constructor  a pattern discoverer  a pattern validator  and an extraction rule composer. the results of rule extractor are extraction rules discovered in a web page. the graphical user interface can then enable users to view the information extracted by each extraction rule. once the user selects a target extraction rule conforming to his information desire  the extractor module can use it to extract information from other pages having similar structure with the input page.  
referring to figure 1  a flowchart of the rule extraction process is shown. the flowchart presents an overview of the mining process; more details will follow as to how a target pattern for extraction is generated. when a user submits an html page to iepad  the translator will receive the html page and translate it into a string of abstract representations  referred to here as tokens.  each token is represented by a binary code of fixed length l.  the pat tree constructor receives the binary file to construct a pat tree.  the pattern discoverer then uses the pat tree to discover repetitive patterns  called maximal repeats. the maximal repeats are forwarded to validator  which filters out undesired patterns and produces candidate patterns. finally  the rule composer revises each candidate pattern to form an extraction rule in regular expression. 
once the extraction rules are discovered  the user may select from these candidate patterns his target pattern that contains desired information. the extractor receives a target pattern and a web page as input and applies a pattern-matching algorithm to recognize and extract all occurrences of the target pattern in the token sequence of the encoded web page. the extractor is a c++ module that can be linked to other information integration systems for information extraction. a typical pattern-matching algorithm  for example  is the knuth-morris-pratt's algorithm. each occurrence of the target pattern may represent a desired data record  and all of the desired data records form a useful information block.  
1. extraction rule generator 
the motivation of iepad is based on the observation that desired information in a web page is often placed in a structure having a particular alignment and forms repetitive patterns. the repetitive patterns  therefore  may constitute the extraction rules for wrappers. for a program to discover such repetitive patterns in a web page  we need some abstraction mechanism to translate the html source to distinguish the display format  translator  and a pattern discovery algorithm to find out all repetitive patterns. by repetitive patterns  we generally mean any substring that occurs at least twice in the encoded token string. however  such a definition will definitely include too many patterns fitting this requisite. therefore  we define maximal repeats to uniquely identify the longest pattern as follows. 
definition: given an input string s  we define maximal repeat ¦Á as a substring of s that occurs in k distinct positions p1  p1  ...   pk in s  such that the  pi-1 th token in s is different from the  pj-1 th token for at least one i  j pair  1¡Üi j¡Ük  called left maximal   and the  pi'+|¦Á| th token is different from the  pj'+|¦Á| th token for at least one i'  j¡ä pair  1¡Üi' j'¡Ük  called right maximal . 
the definition of maximal repeats is necessary for identifying the well used and popular term repeats. besides  it also captures all interesting repetitive structures in a clear way and avoids generating overwhelming outputs. however  not every maximal repeat represents a good one. maximal repeats have to be further verified by the validator to filter interesting ones.  
1 translator 
since html tags are the basic components for document presentation and the tags themselves carry a certain structure information  it is intuitive to examine the tag token string formed by html tags and regard other non-tag text content between two tags as one single token called text. tokens often seen in an html page include tag tokens and text tokens  denoted as html  tag name   and text     respectively. for example  html  /a   is a tag token  where  /a  is the tag. text     is a text token  which includes a contiguous text string located between two html tags.   
tags tokens can be classified in many ways.  the user can choose a classification depending on the desired level of information to be extracted. for example  tags in the body section of a document can be divided into two distinct groups: block-level tags and text-level tags. the former defines the structure of a document  and the latter defines the characteristics  such as format and style  of the contents of the text.   
in figure 1  a classification of block-level tags and text-level tags is shown. block level tags are divided into different categories including headings  text containers  lists  and other classifications  such as tables and forms. text-level tags are similarly divided into categories including logical markups  physical markups  and special markups for marking up texts in a text block. 
block-level tags	text-level tags
headingslogical markup
h1~h1em  strong  dfn  code 
¡¡¡¡¡¡samp  kbd  var  cite text containers
p  pre  blockquote physical markup
addresstt  i  b  u  strike  big 
small  sub  sup  font
lists
ul  ol  li  dl  dir 
menuspecial markup
	others	a  basefont  img  applet 
	div  center  form 	param  map  area
hr  table  br
figure 1.  tag classification
the many different tag classifications allow different html translations to be generated. with these different abstraction mechanisms  different patterns can be produced. for example  skipping all text-level tags will result in higher abstraction from the input web page than when all tags are included. in addition  different patterns can be discovered and extracted when different encoding schemes are translated. the experiment results also indicate that block-level tags  while being a small percentage of the input size  contain a significant amount of useful information as shown in section 1.   
for example  the translation process will be described using a simple web page that contains only two lines of html source code  i.e.  the congo code :  
	 	 b congo /b  i 1 /i  br  
	 	 b egypt /b  i 1 /i  br $ 
the corresponding translation produced by the translator is a token string: 
 html  b  text   html  /b  html  i  text   html  /i    
html  br  html  b  text   html  /b  html  i  text   html  /i  
html  br  $  where each token is encoded as a binary string of   1 s and  1 s with length l. for example  suppose three bits encode the tokens in the congo code: 
html  b     1 html  /b    1 html  i    1 html  /i    1 html  br    1 text     1 the encoded binary string for the token string of the congo code is  1$  of 
1 bits  where  $  represents the ending of the encoded string.  
1 pat tree construction 
our approach for pattern discovery uses a pat tree to discover repeated patterns in the encoded token string. a pat tree is a patricia tree  practical algorithm to retrieve information coded in alphanumeric   constructed over all the possible suffix strings . a patricia tree is a particular implementation of a compressed binary  1  digital tree such that each internal node in the tree has two branches: zero goes to left and one goes to right. like a suffix tree   the patricia tree stores all its data at the external nodes and keeps one integer  the bit-index  in each internal node as an indication of which bit is to be used for branching. for a character string with n indexing point  or n suffix   there will be n external nodes in the pat tree and n-1 internal nodes. this makes the tree o n  in size. 
referring to figure 1  a pat tree is constructed from the encoded binary string of the congo example. the tree is constructed from fourteen sequences of bits  with each sequence of bits starting from each of the encoded tokens and extending to the end of the token string. each sequence is called a  semi-infinite string  or  sistring  in short. each leaf  or external node  is represented by a square labeled by a number that indicates the starting position of a sistring. for example  leaf 1 corresponds to sistring 1 that starts from the second token in the token string. each internal node is represented by a circle  which is labeled by a bit position in the encoded bit string. the bit position is used when locating a given 

figure 1.  the pat tree for the congo code
sistring in pat tree. as shown in figure 1  all suffix strings with the same prefix will be located in the same subtree. 
each edge in the pat tree has a virtual edge label  which is the substring between two bit positions of the two nodes. for example  the edge label between node k and m is ¦Õ =1  i.e. the substring from the fifth bit to the ninth bit of sistring  external node  1 and 1. edges that are visited when traversing downward from root to a leaf form a path that leads to a sistring corresponding to the leaf. in fact  we can uniquely identify a sistring by a prefix  which is formed by concatenating all the edge labels along the path from root to the corresponding leaf. the concatenated edge labels along the path form a virtual path label.  for example  the edge labels  1    1   and  1...  on the path that leads from root to leaf 1 form a path label  1...   which is a unique prefix for sistring 1.  
the pat tree shown here is actually depicted for understanding. in implementation  there are only two pointers at each internal node. the index bit alone can distinguish the branches  zero goes left and one goes right. the edges labeled with a dollar sign are an indication of the end of the string. this edge uses the right link and pointed to the internal node itself. at the construction phase  sistrings are inserted one by one. to insert a sistring  we first search the sistring in the pat tree and find the proper node that can distinguish the sistrings with others. such an implementation requires more time than the optimal algorithm described in gusfield's book  chapter 1 to 1  linear   since we are using binary tree to store character string and only the character suffixes need to be indexed instead of all bit suffixes. however  binary trees have the advantage of simpler implementation. 
1 pattern discoverer 
all the leaves in a subtree share a common prefix  which is the path label for the path that leads from root to the root of the subtree. each path label of an internal node represents a repeated sequence in the input.  therefore  to discover repetitive patterns  the discoverer only needs to examine path labels to determine whether or not they are maximal repeats. since every internal node in a pat tree indicates a branch  it implies a different bit following immediately the common prefix between two suffixes. hence  every path label is right maximal. for a path label of an internal node v to be left maximal  at least two leaves in the v's subtree should have different left characters. let's call such a node left diverse. followed by definition  the property of being left diverse propagates upward in t. therefore all maximal repeats in s can be found in linear time based on the following lemma . 
lemma  the path labels of an internal node v in a pat tree t is a maximal repeat if and only if v is left diverse. 
the essence of a pat tree is a binary suffix tree  which has also been applied in several research fields for pattern discovery. for example  kurtz and schleiermacher have used suffix trees in bioinformatics for finding repeated substring in genome . as for pat trees  they have been applied for indexing in the field of information retrieval since a long time ago . it has also been used in chinese keyword extraction  for its simpler implementation than suffix trees and its great power for pattern discovery. however  in the application of information extraction  we are not only interested in repeats but also repeats that appear regularly in vicinity.  
by recording the occurrence counts and the reference positions in the leaf nodes of a pat tree  we can easily know how many times a pattern is repeated and where it occurs. thus  given the pattern length  occurrence count  we can traverse the pat tree to enumerate all maximal repeats. for example  to find all patterns with the length greater than three tokens. since each token is encoded with three bits  discoverer only needs to consider the internal nodes with index bit greater than 1.  therefore  only node  d    e    g    l   and  m  are qualified.   
the path labels and their respective token representations are candidates for useful repetitive patterns. applying the definition of maximal repeat to the congo example  only node  d  qualifies for a left diverse  since the left tokens for other leaves rooted at the same node all have the same token. for example  sistrings 1 and 1 which are rooted at node  e  have the same token text   . the token representation for the corresponding maximal repeat is  therefore  html  b  text   html  /b  html  i  text   html  /i   html  br  . 
the use of maximal repeats reduces the amount of unnecessary output produced by the discoverer. besides  during the process of discovering the maximal repeats  the number of occurrences of the repeat and their respective positions in the input sequence can be easily derived for the validator to use. 
in the above example  where the path label of node  d  is a maximal repeat  the subtree having node d as its root contains two leaves: leaf 1 and leaf 1. because leaves 1 and 1 represent the only two sistrings that share the maximal repeat as their common prefix  the number of occurrences is two  and the positions of the maximal repeat in the input are one and eight in the input token sequence. 
1 pattern validator 
the above congo source code is a simple example that shows how a maximal repeat can be discovered for a given input.  however  a typical web page usually contains a large number of maximal repeats  not all of which contain useful information.  to eliminate undesired maximal repeats  iepad uses the validator to determine whether or not the maximal repeats contain useful information.  in addition to the occurrence frequency and pattern length of a maximal repeat  the validator employs a number of criteria  including regularity  compactness  and coverage.  a user of iepad may choose to use only one of the criteria  or to use multiples of them in combination.  each of the criteria has a threshold that can either have a default value  or can be determined by the user.  suppose a maximal repeat ¦Á are ordered by its position such that suffix p1   p1   p1...   pk  where pi denotes the position of each suffix in the encoded token sequence  we define regularity  compactness  and coverage as follows. 
regularity of a pattern is measured by computing the standard deviation of the interval between two adjacent occurrences  pi+1-pi . that is  the sequence of spacing between two adjacent occurrences is  p1-p1    p1-p1   ...   pk-pk-1 .  regularity of the maximal repeat ¦Á is equal to the standard derivation of the sequence divided by the mean of the sequence.  a commonly used bound for regularity is 1. 
compactness is a measure of the density of a maximal repeat. it can be used to eliminate maximal repeats that are scattered far apart beyond a given bound.  using the example of maximal repeat ¦Á in the previous paragraph  density  is defined as  k- 1 *|¦Á|/{pk-p1}  where |¦Á| is the length of ¦Á in number of tokens. the criterion of compactness requires that only maximal repeats with a density greater than a given bound  default 1  be qualified for extraction.  
coverage measures the volume of content in a maximal repeat.  suppose the function p i  returns the position of the ith sistring in the input web page in number of bytes.  coverage is defined as  p pk+|¦Á| -p p1  /|webpage|  where |webpage| is the number of bytes of the input web page. 
these three measures are proposed because most information we would like to extract is presented in regular and contiguous format. ideally  the extraction pattern should have regularity equal to zero  density equal to one and large coverage. to filter potentially good patterns  a simple approach will be to use a threshold for each of these measures above. only patterns with regularity less than the regularity threshold and density between the density thresholds are considered candidate extraction patterns. additionally  we can see that not every candidate maximal repeat is useful. for example  patterns that do not contain any text tokens are of no avail to users.  
in this paper  we have utilized regularity and compactness to filter the discovered patterns. these two measures  together with coverage  can further be used to measure patterns' fitness for pattern ranking. the validation proceeds as follows: all discovered maximal repeats with regularity less than the regularity threshold ¦Ã  = 1  are considered potential and are forwarded to rule composer if they have density greater than ¦Ä  = 1   while others  patterns with density less than ¦Ä  are discarded. as for patterns with regularity greater than ¦Ã  special care is taken by the partition module described below. 
1 occurrence partition 
the regularity threshold 1 can extract information that is placed in a structure having a particular alignment and order. however  in examples such as lycos  as will be discussed later   where the pattern of the target information occurs as three blocks in the web pages  and forms three information blocks   the regularity can be large since it is measured over all instances of the pattern. nonetheless  the regularity of the occurrences in each single block is still small. therefore  the idea here is to partition the occurrences into segments so that we can analyze each partition respectively.  
to handle such web pages  these occurrences of such a pattern are carefully segmented to see if any partition of the pattern's occurrences satisfies the requirement for regularity. generally  the regularity of a pattern is computed through all occurrences of the pattern. for example  if there are k occurrences  the k-1 intervals  between two adjacent instances  are the statistics we use to compute the standard deviation and the mean. to partition those occurrences  the occurrences are first sorted by their position. let gi j denote the ordered occurrences pi  pi+1  ...  pj and initialize variables s=1  j=1. for each instance pj+1  if the regularity of gs j+1 is greater than ¦Ã  then we further consider whether cs j is a good partition and assign j+1 to s. the pseudo code is as follows: 
if a partition includes occurrences more than the minimum count and has regularity less than threshold ¦Ãm  the pattern as well as the occurrences in this partition are outputted. note that the threshold ¦Ãm is set to a smaller value close to zero to control the number of outputted patterns. with this modification  more web pages such as lycos can easily be handled.  
1 rule composer 
the main application of pat tree is on the domain of exact match. to allow inexact  or approximate  matching  matching with some errors permitted   the technique for multiple string alignment is borrowed to extend the discovered exact repeats. the idea is to find a good presentation of the critical common features of multiple strings. for example  suppose  adc  is the discovered pattern for token string  adcwbdadcxbadcxbdadcb . if we have the following multiple alignment for strings  adcwbd    adcxb  and  adcxbd :  
a  d  c  w b  d a  d  c  x  b  - a  d  c  x  b  d 
the extraction pattern can be generalized as  adc w|x b d|-   to cover these three examples where  -  represents a missing character. specifically  for a pattern which has k+1 occurrence  p1  p1  ...  pk+1 in the encoded token string. let string pi denote the string starting at pi and ending at pi+1. the problem is transformed to find the multiple alignment of the k strings s=p1  p1  ...  pk   so that the generalized pattern can be used to extract all records we need. 
multiple string comparison is a natural generalization of alignment for two strings which can be solved in o n*m  by dynamic programming where n and m are string lengths. extending dynamic programming to multiple string alignment yields an o nk  algorithm. instead  an approximation algorithm is used such that the score of the multiple alignment is no greater than twice the score of optimal multiple alignment  see   chapter 1 . the approximation algorithm starts by computing the center string sc in k strings s that minimizes consensus error ¡Æpi¡Ês d sc  pi  . once the center string is found  each string is then iteratively aligned to the center string to construct multiple alignment  which is in turn used to construct the extraction pattern.  
for each pattern with density less than 1  the center star approximation algorithm for multiple string alignment is applied to generalize the extraction pattern. note that the success of this 
s=¦Õ  s=1; 
for j=1 to k-1 do 
	 	if  regularity gs j+1    ¦Ã then 
	 	     if regularity gs j    ¦Ãm then 
 	 	s= s ¡È{gs j};  	     endif 
 	     s= j+1;  	endif endf 
approach lies in the assumption of contiguous repeats. if alignment of substrings between two adjacent occurrences results in extraction rules with alternatives at more than ten positions  such an alignment is ignored. in such cases  the original maximal repeats are kept instead of the aligned patterns. 
another problem regarding the aligned extraction pattern is that the pattern is not necessarily located at the boundary of the information record. suppose the generalized extraction pattern is expressed as  c1c1...cn   where each ci is either a symbol x ¡Ê ¦² or a symbol set indicating more than one symbols can appear at position i  e.g. subset of ¦²¡È{-} . if the actual starting position of a record is at cj then the correct extraction pattern should be  cjcj+1cj+1...cnc1...cj-1 . therefore  an additional step is taken to consider whether  cjcj+1cj+1... cnc1...cj-1  is the pattern we need. generally  such position j often has only one symbol. besides  since extraction patterns often begin with tags like  dl    dt    tr  etc. and end up with tags such as  p    br    hr  etc.  we therefore use this guide1 to generate possible extraction rules. however  the disadvantage here is that a large number of patterns can be produced during this rule generation step. 
1. the extractor  
after the extraction rules are constructed  the user may select from the pattern viewer one or more target patterns that contain desired information. figure 1 is a demonstration of the pattern viewer where two patterns are discovered for savvysearch. the upper frame shows the patterns discovered and the lower frame shows the detail measures of the selected pattern. the selected pattern is then forwarded to the extractor for pattern recognition and extraction. a screen shot of the extracted data when double click the pattern is shown in figure 1. 
the extractor is implemented as a module that can be linked into other information integration systems. there are two ways to complete the extraction work depending on whether a pat tree is available for the input web page. the extractor can search the pat tree to find all occurrences of the extraction pattern. or  the extractor can also apply a pattern-matching algorithm to recognize and extract all occurrences of the target pattern in the translated token string of unseen web pages. searching in a pat is fast  since every subtree of a pat tree has all its sistrings with a common prefix. therefore  it allows efficient  linear-time solutions to complex string search problem. for example  string prefix searching  proximity searching  range searching  regular expression search etc.  
  
figure 1. pattern viewer 

figure 1. screen shot of the data extracted 
on the other hand  if the pattern is used to extract unseen web pages where no pat tree has been constructed  pattern-matching algorithms can be applied. typical pattern matching algorithms  for example  are the knuth-morris-pratt's algorithm or boyermoore's algorithm . alternatively  since the extraction rule is expressed as a regular expression  with concatenation and alternative only   it is easier to construct a finite-state machine for such an extraction rule. 
note that each extraction rule composed by multiple string alignment actually represents several patterns. that is  there are alternatives at each state of the non-deterministic finite-state machine. therefore  several patterns can apply when matching the rule against the translated token sequence. in such cases  the longest match is considered. for example  extracting occurrences of the rule  ab a|-  b|-   in the string  abababab  will find only two occurrences of   abab . 
the input to the extractor module is the web pages to be extracted and the selected rule together with the encoding scheme used to discover the extraction rule. the extractor module first translates the web pages using the encoding scheme specified. in addition to the encoded token string  we also record the starting positions in the source web page for each token. once the occurrences of the pattern in the encoded token string are found  the corresponding positions in the web pages can be counted.  
1. experiments 
extraction results of a rule are often evaluated by retrieval rate and accuracy rate. retrieval rate is defined as the ratio of the number of desired data records enumerated by a maximal repeat to the number of desired data records contained in the input text. accuracy rate is defined as the ratio of the number of desired data records enumerated by a maximal repeat to the number of occurrences of the maximal repeat. a data record is said to be enumerated by a maximal repeat if the matching percentage is greater than a bound determined by the user. the matching percentage is used because the pattern may contain only a portion of the data record. for example  suppose an input text contains 1 desired data records  and a maximal repeat that occurs 1 times enumerates 1 of them. then the retrieval rate is 1 and the accuracy rate is 1. a higher retrieval rate indicates that more records are extracted.  
we first show the number of maximal repeats validated by our system using fourteen state-of-the-art search engines  each with ten web pages. the number of validated maximal repeats is sometimes an indicator of the extraction result. if the number of validated maximal repeats is 1  the maximal repeat is very likely the target pattern. when the number of the maximal repeats is increased  it implies that there is a great variance in the records and each discovered maximal repeat only captures part of the target information block. 
table 1. number of maximal repeats validated 
search 
engine maximal repeats regularity   1 density  1  1 altavista       
cora 
excite 
galaxy 
hotbot 
infoseek 
lycos 
magellan 
metacrawler 
northernlight 
openfind 
savvysearch 
stpt.com 
webcrawler 1 
1 
1 
1 
1 
1 
1 1 
1 
1 1 
1 
1 
1 1 
1 
1 
1 1 1 1 1 1 1 1 1 1 
1 1 
1 1 
1 
1 
1 1 
1 
1 
1 1 
1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 average 1 1 1 1   
there are several control parameters which can affect the number of maximal repeats validated  including the regularity threshold  the density thresholds  the minimum length and frequency of a pattern  and the encoding scheme  etc. the effect of different encoding schemes will be discussed in the next section. we first experiment with the all-tag-encoding scheme. table 1 shows the number of maximal repeats validated with regularity smaller than 1  density between 1 and 1. the thresholds are decided empirically to include as many good patterns as possible. the effect of various regularity and density thresholds is shown in figure 1 with fixed minimum length and minimum frequency requirements set to 1 and 1  respectively. 

	1.1.1.1
figure 1. number of patterns validated
1 encoding scheme 
the experiments in the previous section present the encoding scheme when all tag classes are involved in the translation  each tag is translated to their corresponding token class . in this section  four additional encoding schemes are conducted  which skip logical  physical  special and all text-level tags respectively  figure 1 . for example  the second encoding scheme skips physical markups  including  tt    i    b   and  u   etc. because of space limitation  we show only the average performance for these encoding schemes. in table 1  we show the comparison on the length of translated token string. basically  the higher the abstraction level  the shorter the length. table 1 shows the performance comparison of the last three encoding schemes. note that the patterns evaluated here are maximal repeats past the validation criteria. the effects of occurrence partition and multiple string alignment are discussed later. the results of the first encoding scheme are not shown because logical markups are less used in html files  only 1%  and the difference is not obvious from all-tag encoding schemes. for the second encoding scheme  the performance is increased for cora  metacrawler and savvysearch; while for the third encoding scheme  the matching percentage is increased for cora  excite  and metacrawler. that is  though the encoding scheme of skipping some markups/tags enables the display of extraction pattern for some search engines  it may disable the patterns of other web sites  especially when the pattern gets shorter than three tokens. however  high-level abstraction has better performance on average.  
the experimental results indicate that block-level tags  skipping all text-level tags   while being a small percentage of the input size  contain a significant amount of useful information. in our experiment  the size of the token string having only block-level tags is 1 percent of the input size  table 1 ; however  the extraction result obtained from the token string can extract 1 percent of the desired information block  table 1 . note that for the block-level encoding scheme  the performance is increased for almost every search engine except for lycos. lycos has a decreased performance because the maximal repeats occur in three blocks and has regularity greater than 1 in block-level encoding scheme. we conclude that the block-level encoding scheme performs best among others. in addition  the token string for the block-level encoding scheme is only two percent  1 bytes  of the original html file  1kb in average . 
table 1. size of translated sequences and number of patterns 
encoding scheme size of translated sequences no. of patterns discovered all tag 1 1 no physical  1 1 no special 1 1 block-level 1 1 table 1. comparison of different encoding schemes 
encoding scheme retrieval rate accuracy rate matching percentage all tag 1 1 1 no physical  1 1 1 no special 1 1 1 block-level 1 1 1 1 occurrence partition and multiple string alignment 
since the block-level encoding scheme has better abstraction of the input web page  the following experiments use this encoding to show the effect of multiple string alignment and segmentation. as discussed above  the block-level encoding scheme discovers no patterns for lycos because its regularity is greater than the default threshold 1. though larger threshold can keep such patterns  it may also include too many irregular patterns. occurrence partition  on the other hand  provides the opportunity to resume those patterns with larger threshold while reserve only information blocks with really small threshold. indeed  with the additional step of occurrence segmentation  we can successfully partition instances into several blocks and discover such patterns. therefore  the performance is greatly increased to 1% retrieval rate and accuracy rate  table 1 . however  the number of patterns increased a lot for web sites like lycos.  
table 1 also shows that with the help of multiple string alignment the performance is improved to 1% retrieval rate  1% accuracy rate and 1 matching percentage. the high percentage of retrieval rate is pretty encouraging. the 1% of matching percentage is actually higher in terms of the text content retrieved. for those web sites with matching percentage greater than 1%  the text contents are all successfully extracted. what bothers is the accuracy rate. since the extraction pattern generalized from multiple alignment may comprehend more than the information we need  the extractor may extract more than the desired information. the detail performance of each web site is shown in table 1 for reference. 
table 1. effect of segmentation 
method retrieval rate accuracy rate matching percentage block-level encoding 1 1 1 occurrence segmentation 1 1 1 multiple string alignment 1 1 1 table 1. the performance of multiple string alignment 
search 
engine retrieval rate accuracy rate matching percentage altavista       
cora 
excite 
galaxy 
hotbot 
infoseek 
lycos 
magellan 
metacrawler 
northernlight 
openfind 
savvysearch 
stpt.com 
webcrawler 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 average 1 1 1 1. summary and future work 
the core technique of information extraction is the discovery of extraction rules. in earlier work  extraction rules are learned from training examples. in this paper  we presented an unsupervised approach to pattern discovery. we propose the application of pat trees for pattern discovery in the encoded token string of web pages. the discovered maximal repeats are further filtered by the measures: regularity and compactness. the basic pattern discovery module  without occurrence partition and multiple string alignment  can extract 1% records with block-level encoding scheme. the parameters of the thresholds for regularity and compactness have been controlled to keep as many good patterns while to filter out useless patterns.  
for patterns with regularity greater than the default threshold  occurrence partition is applied to segment the occurrences into blocks. this additional step help to solve cases like lycos whose pattern scatters among several blocks and has large regularity. though higher threshold can reserve such patterns  it may also include too many irregular patterns. occurrence partition  on the other hand  provides the opportunity to resume those patterns with greater threshold than default while reserve only information blocks with really small threshold.  
despite pat trees' efficiency in  exact  string problems   inexact  string problems are ubiquitous. therefore  multiple string alignment is applied to patterns to generalize multiple records and express the extraction rule in regular expressions. in our experiments  the extraction rule generalized from multiple string alignment can achieve 1% retrieval rate and 1% accuracy rate with high matching percentage. the whole process requires no human intervention and training example. it takes only three minutes to extract 1 web pages. comparing our algorithm to others  our approach is quick and effective.  
1. acknowledgement 
this work is sponsored by national science council  taiwan  under grant nsc 1-e-1. also  we would like to thank lee-feng chien  ming-jer lee and jung-liang chen for providing their pat tree code for us. 
 
