test collections for the filtering track in trec have typically used either past sets of relevance judgments  or categorized collections such as reuters corpus volume 1 or ohsumed  because filtering systems need relevance judgments during the experiment for training and adaptation. for trec 1  we constructed an entirely new set of search topics for the reuters corpus for measuring filtering systems. our method for building the topics involved multiple iterations of feedback from assessors  and fusion of results from multiple search systems using different search algorithms. we also developed a second set of  inexpensive  topics based on categories in the document collection. we found that the initial judgments made for the experiment were sufficient; subsequent pooled judging changed system rankings very little. we also found that systems performed very differently on the category topics than on the assessorbuilt topics.
categories & subject descriptors: h.1  information storage and retrieval : information search and retrieval - information filtering  relevance feedback; h.1.m  information storage and retrieval : miscellaneous - test collections
general terms: experimentation  measurement
1. introduction
　filtering is a special kind of retrieval task where someone with a long-term information need is monitoring a stream of documents  and the system selects documents from the stream by learning a profile of the users' interests. a number of experiments have been conducted in the trec conferences on various aspects of this process  including routing  batch filtering  and adaptive filtering. a routing system learns a static profile from training documents  and ranks all documents in the test set according to the profile. filtering systems examine the test set one document at a time and must make a decision at each document whether to show it
copyright 1 association for computing machinery. acm acknowledges that this contribution was authored or co-authored by a contractor or affiliate of the u.s. government. as such  the government retains a nonexclusive  royalty-free right to publish or reproduce this article  or to allow others to do so  for government purposes only.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
to the user or not. if an adaptive system decides to show a document  it receives any available relevance information for that document  which it can use to update its profile or decision thresholds.
　the filtering task presents unique challenges in building test collections  because filtering systems require relevance judgments during a run for training and adaptation  whereas in trec ad hoc collections the relevance of documents is not determined until after the experiment. where do these relevance judgments come from  typically  older search topics  such as from a previous year's ad hoc task  with their corresponding documents and relevance assessments were used for training  and a new document collection was provided for testing. however  obtaining new test data  as well as finding collections appropriate to the task  has been a challenge for the track since the beginning.
1 trec 1: routing
　in trec-1  topics 1 were developed for routing and 1 for ad hoc. the first two trec cds comprised the collections. for routing  a small number of judgments were made on cd 1 for topics 1 and released as training data. because of the incompleteness of these training judgments  the trec-1 results should be seen as preliminary. participants constructed routing profiles from this data and tested their systems on the documents on the second trec cd. in trec-1 and 1  routing was able to make use of the past year's ad hoc topics  with their judgments serving as training data. it was intended that routing would use new collections each year for test data  and new documents were accordingly promised for trec-1  but in the end were not delivered. as a result  cd 1 was reused as the test set. despite these topics already being judged for cd 1  nist pooled the results and made additional judgments from the trec-1 routing runs .
1 trec 1: early filtering
　after trec-1  a strong argument was made that a more realistic filtering task should be developed in addition to routing. david lewis designed the structure of the filtering track for trec 1  which was cast as a binary classification task rather than a ranking task. this necessitated several methodological departures from the standard trec evaluations  namely the use of set-based measures and statistical sampling for pooling .
yeartaskstrainingtesttopicsjudged trec-1rcd 1cd 1-1  pilot yestrec-1rcd 1cd 1-1  t1 ad hoc yestrec-1rcd 1cd 1-1  t1 ad hoc yestrec-1r ficd 1 1fr 1  ziff  cd 1    net trash 1 oldyestrec-1r fiap from cd 1fbis1 oldyestrec-1r fi afbis cd 1fbis cd 1 old  1 newyestrec-1r b aap 1  r b ap 1  a   1  r b a 1yestrec-1r b aft 1ft 11  t1 ad hoc yestrec-1r b aohsumed  1 ohsumed  remainder 1 ohsu topics
1 and 1 mesh labelsnotrec 1r b arcv1  aug 1 rcv1  remainder 1 reuters categoriesnotrec 1r b arcv1  aug-sep 1 rcv1  remainder 1 new topics
1 category intersectionsyestable 1: trec routing and filtering tasks  their collections and search topics. for tasks  r=routing  fi=filtering  as defined in trec-1   a=adaptive filtering  b=batch filtering. the  judged  column indicates if new judgments were made for that topic set and collection of test documents.　because new test data was hard to come by  the choice of topics was driven by what data could be had. for trec1  topics were selected from past years and comprised two subsets. half of the topics were selected on the basis of having relevant documents in the existing fr collection  and a new set of federal register documents  fr1  was assembled. the other half of the topics were chosen so as to make a  computers  subcollection; training data came from the ziff collections on disks 1  and the test data was ziff from disk 1 and an assortment of usenet articles and issues of the ir digest and virtual worlds mailing lists. the topics were interspersed together so that systems did not necessarily know what kind of topic they were processing . for trec-1 and 1  new documents became available from the foreign broadcast information service  fbis . in trec-1  all 1 topics1 were chosen by relevant document occurrence in ap  assuming that fbis documents would resemble ap. this assumption turned out not to be valid; several topics had few or no relevant documents found in the test set  while others had a very large number . for trec-1  more fbis documents were acquired and as such a better training/test match could be made. there were 1 topics used: 1 trec-1 filtering topics with at least six relevant fbis documents  five additional old topics  1  1  1  1  and 1   and four brand new topics  numbered 1 . for the nine new topics  incomplete training judgments were made by assessing the top 1 fbis documents retrieved by prise.
　in trec-1 adaptive filtering began as a  pilot  task although only one group  umass  attempted it. an adaptive filtering run began with only the topic statement  ran over the entire fbis collection as a test  and could adapt based on an available judgment for a document retrieved by the system. participants were allowed to use any other nontopic-specific training data they wanted  such as idfs from trec collections outside of fbis  or thesauri. because of the incomplete judgments for the nine new topics  the task only used the 1 trec-1 topics .
1 modern filtering
　in trec-1  routing was folded into the filtering track  while the definition of  filtering  was refined into two tasks: batch filtering and adaptive filtering. topics 1 and the ap collections on cds 1 were used as data. these topics had the limited relevance judgments for 1 used as training for routing in trec-1  better judgments for 1  the trec-1 routing test data   and no judgments at all for 1. thus  routing and batch filtering used 1 for training and 1 for test data; adaptive filtering started with the  very long  topic statements and had to filter the whole ap collection . it is likely that the quality of the trec-1 relevance judgments on ap1 and 1  combined with the very large time gap between when the trec-1 judgments on ap1 and trec-1 judgments on ap1 were made  had a large effect on the trec-1 filtering results. to overcome this  trec-1 used the trec-1 ad hoc topics and document collection. although no new judgments were initially planned  nist in the end agreed to do limited pooling . for trec-1  no assessment resources were available for filtering  so the track used the ohsumed collection   which consists of nearly 1 documents labeled with mesh categories  as well as 1 search topics with relevance judgments on a three-point scale. adaptive filtering systems were given the topic statements along with two  definitely relevant  training documents.
　two other  topic  sets were used: 1 mesh headings having four or more definitely relevant documents in 1 and at least one document in the final year  and a subset of 1 mesh headings sampled from this larger set. adaptive systems were given the heading name itself and its scope note  about the length of a trec description field   along with four relevant training documents. apart from dramatically increasing the scale of trec filtering experiments  these topic sets were the first use of categories as filtering user needs .
　in trec 1  the filtering task decided to use the new reuters corpus volume 1  rcv1  as a document collection.1 rcv1 contains about 1 reuters news articles dating from august 1 to 1. each article is classified by hand into topic  country  and/or industry categories . at the time  there were no search topics for rcv1  and so for the trec 1 filtering tasks the track coordinators selected 1 topic categories to serve as filtering
topics. these particular topics were among those containing the fewest relevant documents  but even so many categories had several thousand .
1. developing filtering topics
　we can see that the trec filtering tasks have used a wide variety of document collections  search topics  and approaches for gathering training and test judgments. these collections have not been driven by the task but rather by the  limited  availability of document collections and resources for making new relevance judgments. in some years this resulted in a poor match between training and test collections. furthermore when trec topics were used  training data consisted of small sets of judgments made on the results of a single search system  or past judgments made for those topics at least one year earlier. there was always a time gap of at least a year when new relevance judgments were made for a test set. chronological consistency was best maintained when using categories as topics  but categories were less desirable than true search topics.
　consequently it was decided for trec 1 to have the nist assessors develop a new set of topics specifically for the filtering track. the goals of this process were to build a topic set for rcv1 of comparable quality to a trec ad hoc collection  but making as many of the judgments during topic development as possible  both to provide adaptive systems with full data and to avoid problems of assessment  drift  due to time lag.
　creating trec topics is an expensive process even for ad hoc search tasks  and so the track also decided to experiment with intersections of reuters categories as a cheaper way to build realistic search topics. filtering track participants were given 1 search topics  of which 1 were composed by human searchers and 1 by intersecting two reuters topic categories  and were asked to run their systems on all the topics together. our hope was that system performance on the intersection topics would predict performance on the assessor topics  but this turned out not to be the case.
1 assessor topics
　for trec ad hoc collections  assessors develop topics by exploring the collection using a retrieval system and making minimal relevance assessments to try to determine how easy or hard the topic will turn out to be. final relevance judgments are made by pooling participants' search results after they have submitted their trec runs. for the filtering track  we have to provide the relevance judgments to the participants up front. thus  the problem we faced was how to determine the relevant documents for the topics  without exhaustively searching the collection  or releasing the topics to participants. furthermore  we didn't want to use more assessor time than would typically be used to do relevance assessing for an ad hoc task. in the end we did make additional relevance judgments from pooled runs in order to verify that the test-set judgments were reasonably complete. to allow the assessors to do more exhaustive searching  we augmented the topic development process with multiple iterations of relevance feedback. after their initial searches were complete and the topic definition established  we asked the assessors to judge the top 1 documents as retrieved by prise for their final query for the topic. these judgments were used as relevance feedback  and on the next day the assessors received a new set of 1 documents to judge. the feedback cycles continued until no more relevant documents were found  or for a maximum of five days. due to some glitches in the system  some topics were judged for more than five days.
　an important concern was the quality and diversity of the documents being judged. the quality of a pool depends on the number and variety of systems and searchers contributing to it. for example  manual ad hoc runs often contribute a disproportionately large fraction of relevant documents found by no other run . one reason pooling works in trec to create test collections usable outside the conference itself is that many different systems are contributing to the pools. it was clear to us that if we only used prise for searching  we would very likely miss many documents that other systems would retrieve. to avoid this  we fed the feedback results to four search systems using seven different retrieval strategies:
prise prise is nist's search system and is used to develop topics for many trec tracks. this is actually an internal development version and has not been released publicly. it is a traditional ir system and supports many retrieval and feedback models. for this task  we used bm1 weights for terms  robertsonsparck jones reweighting for feedback terms  and selection of the top ten reweighted terms for use in feedback retrieval. feedback was based on the topic statement and all relevant documents found in previous iterations.
smart cornell's smart system  with some minor modifications not used here. we used ltc.ntc weighting  and rocchio feedback with default settings. feedback input was all relevant documents found so far  and irrelevant documents from the most recent iteration.
yari yari is a language modeling system written by victor lavrenko of the university of massachusetts. the specific language modeling approach is described in . in our setup  yari built its model using all prior positive feedback. we used uniform weighting for documents and linear smoothing.
bow andrew mccallum's bag of words toolkit is designed for experimenting with text classification algorithms . we used the naive-bayes and svm algorithms from bow in a multiclass classification setup where each topic was a separate class. for svm  we used two different input sets  one with just the feedback data for each topic  and one including a sample of around 1 documents as an  unlabeled  class for transduction. we also used bow's k-nearest-neighbors algorithm  but discarded it after two iterations for speed reasons; thus  not all topics have documents from the knn classifier. the classifiers were given all topics running in the current feedback iteration. documents were ranked by their classification score for each topic being run that day.
　we chose systems that represented a variety of approaches that might be used in the trec filtering track  came with source code  and could be scripted easily into our feedback process.
　each system was configured to return the top 1 documents for each topic based on the latest feedback as described above. these result sets were then merged using the

figure 1: new relevant documents found on each day a topic was evaluated.
combmnz fusion algorithm   and the top-ranked 1 documents were chosen as the pool to be judged the next day. unlike trec pools  these pools were judged in descending order of combmnz score.
　the pools were judged by the assessor who originally composed the topic. on each day  an assessor would judge pools for three to five topics  and at the end of the day the assessors' judgments were collected and fed back to the above systems. if no new relevant documents were found on that day for a given topic  that topic was  retired  and no more judgments were made for it. also  if a topic had been judged for five days  we halted that topic. after retiring a topic from feedback  we gave a new topic to that assessor to judge until all topics were judged or we ran out of time.
　figure 1 illustrates the number of relevant documents discovered in each feedback iteration for the fifty topics used in trec 1.  day 1  is the final prise search before the feedback process was started. several different patterns are evident. some topics displayed the expected behavior  with most relevant documents found in the first couple of iterations followed by a dropoff. however  others only  blossomed  after one or two iterations  and others kept turning up new relevant documents even after a week of searching.
　if we consider the full set of documents judged for a topic as an aggregate pool  the average number of documents judged was 1 with 1 relevant  1% . in comparison  the average trec-1 ad hoc pool contained 1 documents  of which 1  1%  were judged relevant . so by using relevance feedback to construct multiple pools in sequence  we were able to find comparable numbers of relevant documents in an overall smaller pool. this does not suggest that the assessors were more lenient judges of these smaller pools  since they are experienced trec assessors and the standards of relevance were the same as for trec ad hoc relevance  and also the general distribution of relevant documents across topics was similar to an ad hoc collection of similar size.
　table 1 illustrates how each system contributed to the pool  considering all the pools for each topic together . the first column shows the percentage of the pool contributed by each system; for example  1% of the documents in the pool were contributed by prise. the numbers add up to more than 1% because of overlap among the systems; on average  1% of the documents in each pool were con-
% judged% relevantsystemtotaluniquetotaluniqueprise1%1%1%1%smart1%1%1%1%yari1%1%1%1%bow-nb1%1%1%1%bow-svm1%1%1%1%bow-svm-trans1%1%1%1%bow-knn1%1%1%1%table 1: contributions of each system to the sets of judged and relevant documents.
tributed by more than one system. this is a much higher degree of overlap than is seen in trec pools . the second column gives the percentage of unique pool documents contributed by each system. we can see that prise and smart contributed the most unique documents to the pools. even though bow-svm had the largest average contribution  the bow systems probably had fewer unique documents in the pool because of similarity to each other.
　the third and fourth columns show the percentage of relevant documents contributed by each system  and also unique relevant documents. yari's percentage of unique relevant documents is actually due to a bug in the yari runs: a coding error caused each document to be included twice at rank n and n + 1. thus  yari's overall contributions to the pool are understated because their runs were effectively only examined to depth 1. however  each of those documents received an undue combmnz score because yari  recommended  them each twice.
　these numbers hide a lot of variation in the contributions to each topic and on each day. not only did the systems vary quite a bit in how they contributed to each topic  but also during each feedback cycle some systems made more of a contribution to the pool than others. figure 1 illustrates relevant contributions by each system to the pools over the multiple feedback cycles. while there is a general decreasing trend as time progresses  different systems found new groups of relevant documents on different topics.
　ninety-eight topics in all were judged through this process  and we selected fifty to use for trec 1. these topics had between 1 and 1 judgments with between 1 and 1 relevant documents each. the selection process was guided by the training requirements for adaptive filtering. furthermore  since some topics are  bursty  in nature and some are more periodic  we wanted topics with a variety of patterns of relevant documents across the collection. during the feedback process  we tracked topics to see where the judged documents are occurring within the collection. in the end  we were able to select a training period cutoff date in the collection  1/1  that included at least three positive training examples per topic.
　the topic creation process took a total of four weeks. if there had been no bugs and we had also better identified topics to  retire   we probably could have saved a week. this is roughly equivalent to the total amount of time needed to develop and judge a set of trec ad hoc topics. note however that trec topic development and assessment are typically separated by 1 months  during which time assessors' notion of relevance can change.

figure 1: relevant documents contributed to the pools on each day  per system. bow-knn has been omitted.
1 intersection topics
　because rcv1 documents are already labeled with category codes by reuters  we wished to see if some use could be made of these. based on a suggestion at trec 1  we decided to explore using intersections of categories as topics. a category intersection is a pair of categories where the relevant documents are those that belong to both categories. category intersections have the same or fewer relevant documents than either parent category. furthermore  the intersection of two categories might be considered as a specialized interest in each of those categories  and thus more similar to trec topics than categories alone.
　the 1 reuters topic categories are divided into five groups and have a wide range of scope. several of these groups have hierarchy  and the  government/social  group has general categories as well as a hierarchy which overlaps somewhat with the general categories. the reuters industry categories are an even richer hierarchy  but have coding problems which would have complicated usage of the corpus . we did not make use of the region codes.
　of the full set of topic categories  1 are represented in the corpus. most documents are labeled with more than one topic category. of all possible pairs of topic categories  we found 1 represented in the collection  that is  for a pair there exists at least one document labeled with the two categories and possibly others as well . 1 of those pairs had three or more documents in the portion of the collection designated for training; this was our starting set of candidate topic intersections. we also briefly looked at category triples but did not use them.
　we selected 1 category pairs that seemed  from the category names  to be meaningful as search topics  and to have an overall number of relevant documents  documents labeled with both categories  within the range of the assessor-built topics. the topics were selected after the assessors had completed their topics but before the topics were used in trec  so the assessor-built topics actually have more relevant documents because of additional assessments made from participants runs  described below.
　since the assessor-built topics also have documents labeled as irrelevant  we created irrelevant sets for the intersections by selecting a random sample of documents belonging to one of the two categories but not their intersection. this ensured that the irrelevant documents were not arbitrary but represented near-misses as might be selected for a pool and marked irrelevant by an assessor.
　trec-style topic statements for the intersection topics were created mechanically from a set of category descriptions obtained from reuters. the descriptions were extended phrases such as  stories relating to deaths of famous persons  for the category gobit: obituaries. the 'title' and 'description' sections were made from the category names  and the 'narrative' was pasted together from the reuters descriptions. the descriptions were cleaned up minimally  by hand  for grammatical consistency. for example  for topic r1   management  obituaries   the narrative reads   relevant documents discuss all management issues and stories relating to deaths of famous persons.  clearly  these descriptions are not as good as manually-created ones  but filtering systems rely more on training documents than the topic description  and these allow systems to process the intersection topics in the same way they do regular ones.
1. trec 1 experiences
　as discussed above  the filtering track in trec 1 had three main tasks: adaptive filtering  batch filtering and routing. in addition  two measures of performance were used for filtering  utility and fbeta   which meant that there were in effect five distinct tasks.
1 tasks
　the trec 1 model of adaptive filtering task follows the general pattern discussed. we assume the user arrives with a small number of known positive examples  relevant documents . for each topic  the last three relevant documents in the training set were made available to the participants for this purpose; no other relevance judgments from the training set could be used. however  statistics such as term frequencies could be taken from the full training set. subsequently  once a document is retrieved  the relevance assessment  when one exists  is immediately made available to the system. judgments for unretrieved documents are never revealed to the system. once the system makes a decision about whether or not to retrieve a document  that decision is final. no back-tracking or temporary caching of documents is allowed.
　again as discussed  in batch filtering  all the training set documents and all relevance judgments on that set are available in advance. once the system is trained  the test set is processed in its entirety. for each topic  the system returns a single retrieved set. for routing  the training data is the same as for batch filtering  but in this case systems return a ranked list of the top 1 retrieved documents from the test set.
1 measures
　fbeta is a variant on the f1 measure commonly used in text categorization  and originally proposed by van rijsbergen . the constant β is set to 1  corresponding to an emphasis on precision. the measure is averaged over topics. the utility measure is a linear utility  with a credit of 1 units for a relevant document retrieved and a debit of 1 unit for a non-relevant retrieved. this measure is scaled before being averaged over topics; the form of normalization used means that a system which retrieves nothing gets a certain positive score  which we treat as a baseline performance level  indicated in the figures below . full details are given in . these two measures are used for the adaptive and batch filtering tasks; each submitted run was declared to be optimized for one of these measures. for the routing task  mean average precision was used.
1 results
　for reasons which will become apparent  these results are separated into assessor and intersection topics.
assessor topics. the graph on the left of figure 1 shows the utility results for the assessor topics and all submissions to the adaptive filtering track.  note that some of the runs were not optimized for this measure.  the systems are ranked by the mean value across topics of the scaled utility measure  t1su. the horizontal line inside a run's box is the median topic score  the box shows interquartile distance  the whiskers extend to the furthest topic within 1 times the interquartile distance  and the circles are outliers. the horizontal line across the whole graph shows the performance that would obtain if a system were to retrieve nothing. unlike in some earlier trec filtering experiments  a substantial number of systems performed well over this level. in fact for quite a number of systems  1% of the topics were over this level.
　there is a certain amount of bunching among the bestperforming systems - a characteristic which is generally taken at trec to mean some degree of maturity among the competing systems for this task. however  there are clearly several systems with scope for improvement. the performance measurement appears to be doing a reasonable job of distinguishing between systems.
intersection topics. the graph on the right of figure 1 shows the equivalent results from the intersection topics. the story told by this graph is very different. first  the absolute levels of performance are terrible - no system did better on average than our hypothetical baseline which retrieves nothing. indeed  the best upper quartile is exactly on the baseline - no system succeeded in getting even 1% of the intersection topics above this level.
　one might be tempted to think that the intersection topics are simply much harder  but nevertheless represent a realistic task. however  the magnitude of the difference makes this explanation difficult to maintain. in fact it seems that the only solution open to the systems was to shut down most topics as soon as possible  to cut their losses.
　some ongoing analysis of individual topics is suggesting some possible reasons for this discrepancy. but we are forced to the conclusion  at least for the present  that the intersection topics do not constitute a useful set of topics for filtering experiments. again  these impressions are not confined to the adaptive filtering utility results - the results for batch filtering and routing are equally bad  despite the additional training material available.
	t1u	t1f

adaptive 1 1 batch 1 1
	routing	1  map 
table 1: correlation of the official trec results to a system ranking measured using the first-round relevance judgments only.
newjudgments. although more than 1 relevance judgments were made during topic creation and released with the topics  we were concerned that participants would still find more relevant documents. in order to make sure systems were measured fairly  nist pooled participants' runs and judged any previously unjudged documents in the pool. pooling was done as follows. each participating group  who may have submitted up to four adaptive  two batch  and two routing runs  was allotted a fixed budget of documents to be pooled from their runs. if the group had any routing runs  we added unjudged documents from the top 1 ranks to the pool. if the group also had filtering runs  at most half the budget was expended on routing documents. we then merged all batch and adaptive filtering runs from that group and took a random sample of documents from the combined runs to fill out the pool budget. in all  another 1 documents were judged during this second round of assessment.
　figure 1 shows the numbers of relevant documents found for each topic in the first and second rounds of judging. note that overall the topics have between 1 and 1 relevant documents apiece  much fewer than the trec 1 categories and closer to trec ad hoc scale. for most topics only a few new relevant documents were found in the second round  median = 1   but seven topics had more than fifty new. four of these topics had more than twenty new relevant documents found in their last feedback iteration during the creation phase. although our pooling process is radically different  these findings agree with harman's analysis of the trec-1 relevance judgments   as well as those of zobel  that the  largest  topics  those with the most relevant documents  tend to yield even more relevant documents upon further searching. we have seen that such topics tend to have a greater number of relevant documents found in the last round of judging. in retrospect it probably would have been a good idea to discard these topics.
　another important factor is that five topics were judged by a different assessor in the second round than the one who had created it. although as a general rule assessors always judges their own topics  due to time constraints we were forced to move these topics to different assessors. in these cases  the assessor was shown all of the relevant documents found in the first round as orientation to the topic. four of these  moved  topics were also topics with more than fifty new relevant found  suggesting that these topics were not judged as well as the others.
　despite all the additional judgments and newfound relevant documents  the performance of the systems participating in the trec 1 filtering track was largely unchanged when measured with the full set of relevance judgments. kendall's tau correlations  shown in table 1  between the official trec scores and measurements made using the first round of judgments only show that the rankings are virtu-
　

figure 1: adaptive filtering - utility
runt1u  utility t1f  fbeta trecrerundifftrecrerundiff1a11111-11b1111111c1111111d1111111a11111-11b11111-11c11111-1table 1: two adaptive systems performed slightly differently when using the final set of relevance judgments for adaptation during their run.
　　　　ally identical. this means that the evaluation did not penalize systems because of relevant retrieved documents which were not judged. for adaptive systems  the story is a bit more complicated  since we don't know if the systems would have adapted differently because there were more judgments available. we asked participants to do adaptive runs using the final relevance judgments as input  but otherwise keeping their systems identical to what was submitted to trec. two groups were able to provide a total of seven runs. the results  shown in table 1  are not conclusive since they only come from two groups  but seem to indicate that adaptive systems would not have performed very differently on the main measures if they had been given the additional relefigure 1: relevant documents found in the first and vance judgments. the true effect depends on how systems second rounds of judging. adapt when they retrieve an unjudged document as opposed to a judged one. a more detailed look at the results suggests that systems achieved noticeably higher recall in the new runs  but with a balancing loss of precision.
　
1. conclusions
　we believe that the 1 new assessor topics  together with the relevance judgments on the reuters rcv1 corpus  constitute a good and valuable addition to the resource represented by the collective trec collections.
　by and large  the method of generating relevance judgments by successive feedback iterations on four different systems has proved valid and useful. the resulting judgments are likely to be sufficient for both feedback and evaluation purposes. the discovery of additional relevant documents in the second round does not appear to invalidate this conclusion; however  if we were running the experiment again  we might be inclined to reject topics which are continuing to generate significant numbers in the final feedback iteration. until we better understand the problems of the intersection topic set  this method of construction cannot be recommended.
