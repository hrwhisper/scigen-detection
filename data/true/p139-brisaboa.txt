recent research has demonstrated beyond doubts the benefits of compressing natural language texts using word-based statistical semistatic compression. not only it achieves extremely competitive compression rates  but also direct search on the compressed text can be carried out faster than on the original text; indexing based on inverted lists benefits from compression as well.
　such compression methods assign a variable-length codeword to each different text word. some coding methods  plain huffman and restricted prefix byte codes  do not clearly mark codeword boundaries  and hence cannot be accessed at random positions nor searched with the fastest text search algorithms. other coding methods  tagged huffman  end-tagged dense code  or  s c -dense code  do mark codeword boundaries  achieving a self-synchronization property that enables fast search and random access  in exchange for some loss in compression effectiveness.
　in this paper  we show that by just performing a simple reordering of the target symbols in the compressed text  more precisely  reorganizing the bytes into a wavelet-treelike shape  and using little additional space  searching capabilities are greatly improved without a drastic impact in compression and decompression times. with this approach  all the codes achieve synchronism and can be searched fast and accessed at arbitrary points. moreover  the reordered compressed text becomes an implicitly indexed representation of the text  which can be searched for words in time independent of the text length. that is  we achieve not only fast sequential search time  but indexed search time  for almost no extra space cost.
　we experiment with three well-known word-based compression techniques with different characteristics  plain huffman  end-tagged dense code and restricted prefix byte

 funded in part by acei grant a/1  spain ; for the spanish group by mec grant tin1-c1  spain ; and for the fourth author by yahoo! research grant  compact data structures .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  singapore.
copyright 1 acm 1-1-1/1 ...$1.
codes   and show the searching capabilities achieved by reordering the compressed representation on several corpora. we show that the reordered versions are not only much more efficient than their classical counterparts  but also more efficient than explicit inverted indexes built on the collection  when using the same amount of space.
categories and subject descriptors
e.1  coding and information theory : data compaction and compression; h.1  information storage and retrieval : information search and retrieval-search process
general terms
algorithms
keywords
word-based compression  searching compressed text  compressed indexing.
1. introduction
　text compression is useful not only to save disk space  but more importantly  to save processing  transmission and disk transfer time. compression techniques especially designed for natural language texts permit searching the compressed text much faster  up to 1 times  than the original text  1  1   in addition to their proven effectiveness  with compression ratios around 1%-1% .
　those ratios are obtained using a word-based model   where words are encoded instead of characters. words present a more biased distribution of frequencies than characters  following a zipf law  1  1 . thus the text  regarded as a sequence of words  is highly compressible with a zero-order encoder such as huffman code . with the optimal huffman coding  compression ratios approach 1%.
　although necessarily inferior to huffman code in compression effectiveness  different coding methods  such as plain huffman  or restricted prefix byte codes   try to approach the performance of classical huffman while encoding the source symbols as sequences of bytes instead of bits. this degrades compression ratios to around 1%  yet allows much faster decompression.
　still other encoding methods  such as tagged huffman codes   end-tagged dense codes  and  s c -dense codes   worsen the compression ratios a bit more  up to 1%  in exchange for being self-synchronized. this means that codeword boundaries can be distinguished starting from anywhere in the encoded sequence  which enables random access to the compressed text  as well as very fast boyer-moore-like direct search of the compressed text.
　in this paper  we propose a reordering of the bytes in the codewords of the compressed text following a wavelet-treelike strategy. we show that this simple variation obtains a compressed text that is always self-synchronized  despite building on encodings which are not. that is  the reorganized compressed text can be accessed at any point  even if plain huffman coding is used  for example. this encourages using the most efficient bytewise encodings with no penalty. what is even more striking is that the reorganized text turns out to have some implicit indexing properties. that is  with very little extra space  it is possible to search it in time that is not proportional to the text length  as any sequential search method  but logarithmic on it  as typical indexed techniques . indeed  we compare our reorganized codes against the original techniques armed with an explicit inverted index  and show that the former are more efficient when using the same amount of space. within that little allowed space  block-addressing compressed inverted indexes are the best choice as far as we know  1  1 . we implement such a compressed block-addressing index following the most recent algorithms for list intersections . our results demonstrate that it is more convenient to use reorganized codes than trying to use very space-efficient inverted indexes; only if one is willing to pay a significant extra space do inverted indexes pay off.
　we note that our technique is tailored to main memory due to its random access pattern. therefore  it can only compete with inverted indexes in main memory. there has been a lot of recent interest on inverted indexes that operate in main memory  1  1  1   mainly motivated by the possibility of distributing a large collection among the main memories of several interconnected processors. by using less space for those in-memory indexes  as our technique allows  more text could be cached in the main memory of each processor and fewer processors  and less communication  would be required.
　the paper is organized as follows. the next section describes the coding schemes used as the basis for our research. section 1 describes wavelet trees and how they can be used. section 1 presents our reorganizing strategy in detail. finally  sections 1 and 1 present our empirical results  conclusions and future work.
1. bytewise encoders
　we cover the byte-oriented encoding methods we will use in this paper; many others exist  e.g.  1  1 .
　the basic byte-oriented variant of the original huffman code is called plain huffman  ph  . plain huffman does not modify the basic huffman code except by using bytes as the symbols of the target alphabet. this worsens the compression ratios to 1%  compared to the 1% achieved by the original huffman coding on natural language and using words as symbols . in exchange  decompression and searching are much faster with plain huffman code because no bit manipulations are necessary.
　end-tagged dense code  etdc   is also a word-based byte-oriented compression technique where the first bit of each byte is reserved to flag whether the byte is the last one of its codeword. the flag bit is enough to ensure that the code is a prefix code regardless of the content of the other 1 bits of each byte  so there is no need at all to use huffman coding in order to maintain a prefix code. therefore  all possible combinations are used over the remaining 1 bits of each byte  producing a dense encoding. etdc is easier to build and faster in both compression and decompression. while searching plain huffman compressed text requires inspecting all its bytes from the beginning  the tag bit in etdc permits boyer-moore-type searching   that is  skipping bytes  by simply compressing the pattern and then running the string matching algorithm. on plain huffman this does not work  as the pattern could occur in the text not aligned to any codeword . moreover  it is possible to start decompression at any point of the compressed text  because the 1th bit gives etdc the self-synchronization property:
one can easily determine the codeword boundaries.
　in general  etdc can be defined over symbols of b bits  although in this paper we focus on the byte-oriented version where b = 1. given source symbols with decreasing probabilities {pi}1＋i n the corresponding codeword using the etdc is formed by a sequence of symbols of b bits  all of them representing digits in base 1b 1  that is  from 1 to
       1   except the last one which has a value between 1  1 and 1   1  and the assignment is done sequentially.
   note that the code depends on the rank of the words  not on their actual frequency. as a result  only the sorted vocabulary must be stored with the compressed text for the decompressor to rebuild the model. therefore  the vocabulary will be slightly smaller than in the case of huffman codes  where some information about the shape of the huffman tree must be stored  even for canonical huffman trees . as it can be seen  the computation of codes is extremely simple: it is only necessary to sort the source symbols by decreasing frequency and then sequentially assign the codewords. but not only the sequential procedure is available to assign codewords to the words. there are simple encode and decode procedures that can be efficiently implemented  because the codeword corresponding to symbol i is obtained as the number x written in base 1b 1  where   and adding 1b 1 to the last digit.
　in restricted prefix byte codes  rpbc   the first byte of each codeword completely specifies its length. the encoding scheme is determined by a 1-tuple  v1 v1 v1 v1  satisfying v1 + v1 + v1 + v1 ＋ r. the code has v1 one-byte codewords  rv1 two-byte codewords  r1 three-byte codewords and r1 four-byte ones. they require v1 + v1r + v1 + v1 − n where r is the radix  typically 1. this method improves the compression ratio of etdc as it adds more flexibility to the codeword lengths  it maintains the efficiency with simple encode and decode procedures  it is also a dense code  but it loses the self-synchronization property. if we seek to a random position in the text  it is not possible to determine the beginning of the current codeword. it is possible to adapt boyer-moore searching over text compressed with this technique  but it is slower than searching over text compressed with etdc.
1. wavelet trees
　a wavelet tree is a succint data structure. it was proposed in  for solving rank and select queries over sequences on large alphabets. given a sequence of symbols b  rankb b i  = y if the symbol b appears y times in the prefix b1 i  and selectb b j  = x if the jth occurrence of the symbol b in the sequence b appears at position x.
　the original wavelet tree is a balanced binary tree that divides the alphabet into two halves at each node  and stores bitmaps in the nodes to mark which side was chosen by each symbol in the sequence. each child handles recursively the part of the sequence formed by its symbols. solving rank and select queries over bit sequences in constant time is wellknown  1  1 . the wavelet tree reduces rank and select operations on a sequence s to rank and select operations over the bitmaps stored at the nodes. for rank  the tree is traversed top-down  and bottom-up for select.
　multi-ary wavelet trees are introduced in   where symbol rank and select operations are needed within the nodes. huffman shaped wavelet trees have also been considered  1  1 . our wavelet trees in this paper are in some sense inspired by these.
1. reorganization of codewords
　our method can be applied to any word-based  byte-oriented semistatic statistical prefix-free compression technique  as all those mentioned in section 1 . basically the idea is to reorganize the different bytes of each codeword  placing them in different nodes of a tree that we call wavelet tree for its similarity with the wavelet trees used in . that is  instead of representing the compressed text as a concatenated sequence of codewords  composed of one or more bytes   each one replacing the original word at that position in the text  we represent the compressed text as a wavelet tree where the different bytes of each codeword are placed at different nodes.
　the root of the wavelet tree contains the first byte of all the codewords  following the same order as the words in the original text. that is  at position i in the root we place the first byte of the codeword that encodes the ith word in the source text. the root has as many children as different bytes can be the first byte of a codeword. for instance  in etdc the root has always 1 children and in rpbc it will typically have 1   v1. the node x in the second level  taking the root as the first level  stores the second byte of those codewords whose first byte is x. hence each node handles a subset of the text words  in the same order they have in the original text. that is  the byte at position i in node x is the second byte of the ith text codeword that starts with byte x. the same arrangement is done to create the lower levels of the tree. that is  node x has as many children as different second bytes exist in codewords with more than 1 bytes having x as their first byte.
formally  let us represent the text words.
lets call cwi the codeword representing word wi. notice that two codewords cwi and cwj can be the same if the ith and jth words in the text coincide. the bytes of codeword cwi are denoted as were m is the size of codeword cwi. the root node of the tree is formed by the following sequence of bytes. notice that the root has as many bytes as words has the text. as explained  the root has a child for each byte value that can be the first in a codeword. assume there are r words in the source text encoded by codewords  longer than 1 byte  starting with the byte x: cwi1...cwir. then the node x will store the sequence . some of those will be the last byte of their

1
 we speak of words to simplify the discussion. in practice both words and separators are encoded as atomic entities in word-based compression.
codeword  yet others would correspond to codewords with more than two bytes.
　therefore  node x would have in turn children as explained before. assume node xy is a child of node x. it stores the byte sequence of all the third bytes of codewords cwj1...cwjk starting with xy  in their original text order. our wavelet tree is not balanced because some codewords are longer than others. the number of levels will be equal to the number of bytes of the longer codewords.
　figure 1 shows an example of a wavelet tree1  built from the text long time ago in a galaxy far far away  and the alphabet Σ = {a ago away far galaxy in long time}. after obtaining the codewords for all the words in the text  using a known compressor  we reorganize their bytes in the wavelet tree following the arrangement explained. the first byte of each codeword is in the root node. the next bytes are contained in the corresponding child nodes. for example  the second byte of the word 'away' is the third byte of node b1  because it is the third word in the root node having b1 as first byte. its third byte is in node b1 as its two first codeword bytes are b1 and b1.
　assume we want to know which is the 1th word in the text. starting at the root node in figure 1  we read the byte at position 1 of the root node: root = b1. the encoding scheme indicates that the codeword is not complete yet  so we move to the second level of the tree. the second byte is contained in the node b1  which is the child node of the root where the second bytes of all codewords starting by byte b1 are stored. using a byte rank operation we obtain rankb1 root 1  = 1. this means that the second byte of the codeword starting in the byte at position 1 in the root node will be the 1nd byte in the node b1. in the next level  b1 = b1  therefore b1 is the second byte of the codeword we are looking for. again the encoding scheme indicates that the codeword is still not complete  and rankb1 b1  = 1 tells us that the 1rd byte of that word will be in the node b1 at position 1. one level down  we obtain b1 = b1  and now the obtained sequence b1b1 is a complete codeword according to the encoding scheme. it corresponds to 'galaxy'  which therefore is the 1th word in the source text.
　this process can be used to recover any word. notice that this mechanism gives direct access and random decompression capabilities to encoding methods that do not mark boundaries in the codewords. with the proposed arrangement  those boundaries become automatically defined  each byte in the root corresponds to a new codeword .
　if we want to search for the first occurrence of 'away' in the example of figure 1  we start by finding out its codeword  which is b1b1. therefore the search will start at the node b1  which holds all the codewords starting with b1. in this leaf node we find out where the first byte b1 occurs  because b1 is the third byte of the codeword sought. operation selectb1 b1 1  = 1 tell us that the first occurrence of our codeword is the first of all codewords starting with b1  thus in the node b1 the first occurrence of byte b1 is the one encoding the first occurrence of the word 'away' in the text. again  to know where in the node b1 is the first byte b1 we perform selectb1 b1  = 1. now we know that in the root node the 1rd byte b1 will be the one corresponding to the first byte of our codeword. to know where in the root node is that 1rd byte b1 we compute selectb1 root 1  = 1.

1
 note that only the shaded byte sequences are stored in the nodes; the text is shown only for clarity.
figure 1: example
	word: long time ago in	a galaxy far far away
text:  long time ago in a galaxy far far away  position: 1	1	1	1	1
b1b1b1b1b1b1b1b1b1	symbol	freq	code
	far	1	b1	b1
	in	1	b1 b1	time in away	long a	ago galaxy
	a	1	b1 b1
	1	1	1	1
b1	b1b1	b1	b1	b1	b1	long	1	b1 b1
	ago	1	b1 b1
	time	1	b1 b1
	away	1	b1 b1 b1	b1b1
	galaxy	1	b1 b1 b1	away	galaxy
	1
b1b1finally the result is that the word 'away' appears for the
first time as the 1th word of the text. notice that it would be easy to obtain a snippet of an arbitrary number of words around this occurrence  just by using the explained decompression mechanism  on any encoding.
　the sum of the space needed for the byte sequences stored at all nodes of the tree is exactly the same as the size of the compressed text. just a reordering has taken place. yet  a minimum of extra space is necessary in order to maintain the tree shape information with a few pointers. actually  the shape of the tree is determined by the compression technique  so it is not necessary to store those pointers  but only the length of the sequence at each node.
1 algorithms
　we now detail the algorithms for compression  decompression  and searching.
1.1 compression
　the compression algorithm makes two passes on the source text. in the first pass we obtain the vocabulary and the model  frequencies   and then assign codewords using any prefix-free semistatic encoding scheme. in the second pass the source text is processed again and each word is translated into its codeword. instead of storing those codewords sequentially  as a classical compressor  the codeword bytes are spread along the different nodes in the wavelet tree. the node where a byte of a codeword is stored depends on the previous bytes of that codeword  as explained.
　it is possible to precalculate how many nodes will form the tree and the sizes of each node before the second pass starts  so they can be allocated and filled with the codeword bytes as the second pass takes place. we maintain an array of markers that point to the current writing position at each node  so that they can be filled sequentially following the order of the words in the text.
　finally  we generate the compressed text as the concatenation of the sequences of all the nodes in the wavelet tree  and add a header with the words   codewords assignment  plus the length of the sequence at each tree node.
1.1 random decompression
　to decompress from a random text word j  we access the j-th byte of the root node sequence to obtain the first byte of algorithm 1 construction of wtdc

//input: t  source text
//output: compressed text with shape of wavelet tree voc ○ first-pass t 
sort voc  totalnodes ○ calculatenumbernodes   for all node （ totalnodes do
length node  ○ calculateseqlength node 
wt node  ○ allocate length node   marker node  ○ 1
end for for all word （ t do cw ○ code word  currentnode ○ rootnode for	do
currentnode 
wt currentnode  j  ○ cwi marker currentnode  ○ j + 1 currentnode ○ child currentnode cwi 
end for
end for
return concatenation of node sequences  vocabulary  and length of node sequences

the codeword. if the codeword has just one byte  we finish at this point. if the byte read bi is not the last one of a codeword  we have to go down in the tree to obtain the rest of the bytes. as explained  the next byte of the codeword is stored in the child node bi  the one reached from the first byte bi. all the codewords starting with that byte bi are stored in bi  so we count the number of occurrences of the byte bi in the root node before position j by using the rank operation  rankbi root j  = k. thus k is the position in the child node bi of the second byte of the codeword. we repeat this procedure as many times as the length of the codeword. if we need to decompress the previous or the next word we follow the same algorithm starting with the previous or the next entry of the root node.
　the complexity of this algorithm is  l 1  times the complexity of rank operation  where l is the length of the codeword. therefore  its performance depends on the implementation of the rank operation.
algorithm 1 display x

//input: x  position in the compressed text //output: p  word at position x in the compressed text currentnode ○ rootnode
c ○ wt currentnode  x  cw ○  c  while cw is not completed do
x ○ rankc currentnode x  currentnode ○ child currentnode c 
c ○ wt currentnode  x  cw ○ cw||c
end while p ○ decode cw  return p

1.1 full decompression
　after loading the vocabulary and rebuilding the wavelet tree  the full decompression of the compressed text consists of decoding sequentially each entry of the root. all the nodes of the tree will be also processed sequentially  so to gain efficiency we maintain pointers to the current first unprocessed entry of each node. once we obtain the child node where the codeword of the current word continues  we can avoid unnecessary rank operations because that byte will be the next one to process in the corresponding node. except for this improvement  the algorithm is the same as that explained in section 1.1.
1.1 searching
　to count the occurrences of a given word  we compute how many times the last byte of the codeword assigned to that word appears in the corresponding leaf node. that leaf node is the one identified by all the bytes of the codeword except the last one. the pseudocode is presented in algorithm 1.

algorithm 1 count operation

//input: w  a word //output: n  number of occurrences of w cw ○ code w 
○
n ○ rankc currentnode length currentnode   return n

　to locate all the occurrences of a given word  we start looking for the last byte of the corresponding codeword cw in the associated leaf node using operation select. if the last symbol of the codeword  cw|cw|  occurs at position j in the leaf node  then the previous byte cw|cw| 1 of that codeword will be the jth one occurring in the parent node. we proceded in the same way up in the tree until reaching the position x of the first byte cw1 in the root. thus x is the position of the first occurrence of the word searched for. to find all the occurrences of a word we proceed in the same way  yet we can use pointers to the already found positions in the nodes to speed up the select operations  this might be relevant depending on the select algorithm used .
　it is also possible to search a phrase pattern. we locate all the occurrences of the least frequent word in the root node  and then check if all the first bytes of each codeword of the algorithm 1 locate jth occurrence of word w operation

//input: w  word
//input: j  integer
//output: position of the j-th occurrence of w cw ○ code w  c
○
for
cwi
currentnode ○ parent currentnode 
end for return j

pattern match with the previous and next entries of the root node. if those first bytes match  we verify their complete codewords around the candidate occurrence found. our experiments show this is a very efficient method in practice.
1 rank and select over bytes
　as it was mentioned before  the efficiency of the search and random decompression algorithms depends on the implementation of rank and select operations.
　a baseline solution is to carry out those operations by brute force  that is  by sequentially counting all the occurrences of the byte we are interested in  from the beginning of the node sequence. this simple option does not require any extra structure. interestingly enough  it already allows that operations count and locate are carried out more efficiently than in classically compressed files. in both cases we do sequential searches  but in the reorganized version these searches are done over a reduced portion of the file. likewise  it is possible to access the compressed text at random  even using non-synchronized codes such as ph and rpbc  faster than scanning the file from the beginning.
　however  it is possible to drastically improve the performance of rank and select operations at a very moderate extra space cost  by adapting well-known theoretical techniques . given a sequence of bytes b 1 n   we use a two-level directory structure  dividing the sequence into sb superblocks and each superblock into b blocks of size n/ sb b . the first level stores the number of occurrences of each byte from the beginning of the sequence to the start of each superblock. the second level stores the number of occurrences of each byte up to the start of each block from the beginning of the superblock it belongs to. the second-level values cannot be larger than sb   b  and hence can be represented with fewer bits.
　with this approach  rankbi b j  is obtained by counting the number of occurrences of bi from the beginning of the last block before j up to the position j  and adding to that the values stored in the corresponding block and superblock for byte bi. instead of o n   this structure answers rank in time o n/ sb   b  . to compute selectbi b j  we binary search for the first value x such that rankbi b x  = j. we first binary search the stored values in the superblocks  then those in the blocks inside the right superblock  and finally complete the search with a sequential scanning in the right block. the time is o logsb + logb + n/ sb   b  .
　an interesting property is that this structure is parameterizable. that is  there is a space/time tradeoff associated to parameters sb and b. the shorter the blocks  the faster the sequential counting of occurrences of byte bi.
table 1: description of the corpora used.
corpussize  bytes num wordsvoc. sizecr1 1 11ziff1 1 11all1 11 1 1table 1: compression ratio  in % .
phetdcrpbcwphwtdcwrpbccr111111ziff111111all1111111. experimental results
　we used some large text collections from trec-1: ap newswire 1  ap  and ziff data 1  ziff   as well as trec-1  namely congressional record 1  cr  and financial times 1 to 1  ft1 to ft1  to create a large corpora  all  by aggregating them all. we also used cr and ziff corpus individually. table 1 presents the main characteristics of the corpora used. the first column indicates the name of the corpus  the second its size  in bytes . the third column in that table indicates the number of words that compose the corpus and finally the fourth column shows the number of different words in the text.
　we used the spaceless word model  to create the vocabulary  that is  if a word was followed by a space  we just encoded the word  otherwise both the word and the separator were encoded.
　an isolated intelr pentium 1 ghz system  1kb l1 + 1kb l1 cache   with 1 gb dual-channel ddr1mhz ram was used in our tests. it ran debian gnu/linux
 kernel version 1.1 . the compiler used was gcc version 1.1 and -o1 compiler optimizations were set. time results measure cpu user time in seconds.
1 compression properties
　we measure how our reorganization of codeword bytes affects the main compression parameters  such as compression ratio and compression and decompression times.
　we use our reorganization method over the compressed texts obtained using three well-known compression techniques explained in section 1. we call wph  wtdc  and wrpbc to the wavelet-tree reorganization applied over plain huffman  end-tagged dense code  and restricted prefix byte codes  respectively.
　table 1 shows that compression ratio is essentially not affected. there is a very slight loss of compression  close to 1%   due to the storage of the tree shape.
　tables 1 and 1 show the compression and decompression time obtained using the wavelet trees. whereas the compression time is almost the same  1%-1% worse   there are larger differences in decompression time  1%-1% slower . with the wavelet tree  decompression is not just a sequential process. for each word of the text  a top-down traversal is carried out on the tree  so the benefits of cache and spatial locality disappear. this is more noticeable than at compression  where the overhead of parsing the source text blurs those time differences.
table 1: compression time.
phetdcrpbcwphwtdcwrpbccr111111ziff111111all111111table 1: decompression time.
phetdcrpbcwphwtdcwrpbccr111111ziff111111all1111111 searching and displaying
　we show now the efficiency achieved by the reordering technique for pattern searching and random decompression.
　table 1 summarizes the performance  measuring user time  of the main search operations: count all the occurrences of a pattern  in milliseconds   locate the position of the first occurrence  in milliseconds   locate all  in seconds  and extract all the snippets of 1 words centered around the occurrences of a pattern  in seconds . we run our experiments over the largest corpus  all  and show the average time of searching for 1 distinct words randomly chosen from the vocabulary  of frequency up to 1  to avoid searching for stopwords  which is meaningless . we present the results obtained by the compression methods ph  etdc  and rpbc; by the wavelet trees implemented without blocks and superblocks  wph  wtdc  and wrpbc ; and also by the wavelet trees using those structures  covering 1 bytes per block  and 1 blocks per superblock  with a waste of 1% of extra space  to speed up rank and select operations  wph+  wtdc+  and wrpbc+ . table 1 shows the loading time  so that the compressed text becomes ready for querying  and the internal memory usage to solve queries needed for each method. in the case of rank structures  it takes more than 1 second to create the two-level directory. this time is not as important as search times  because this loading is paid only once.
　even without using the extra space for the blocks and superblock structures  wavelet trees improve all searching capabilities except for extracting all the snippets  as shown in table 1. this is because snippets require decompressing several codewords around each occurrence  and random decompression is very slow in the wavelet trees if one has no extra support for the rank operations that track random codewords down.
　by just spending 1% extra space in block and superblock data structures to obtain rank values faster  all the operations are dramatically improved  including the extraction of all the snippets. only the self-synchronized etdc is still faster than its corresponding wavelet tree  wtdc+  for extracting snippets. this is because extracting a snippet around a word in a non self-synchronized code implies extra operations to permit the decompression of the previous words  while etdc can easily move backwards in the compressed text.
　by raising the extra space allocated to blocks and superblocks to 1%  wtdc+ finally takes over etdc in extracting snippets as well. it is important to remark that our proposal improves all searching capabilities when a compression technique is not self-synchronized.
table 1: load time  in seconds  and internal memory usage for queries  % of corpus size .
phetdcrpbcwphwtdcwrpbcwtph+wtdc+wrpbc+load time  s 111111111memory usage1%1%1%1%1%1%1%1%1%table 1: searching capabilities
countfirstlocatesnippet ms  ms  s  s ph1111etdc1111rpbc1111wph1111wtdc1111wrpbc1111wph+1111wtdc+1111wrpbc+11111 implicit indexing versus classical indexes
　as explained  our reorganization brings some  implicit  indexed search capabilities into the compressed file. in this section we compare the search performance of wtdc+ with a block-addressing compressed inverted index  ii  in the style of   over text compressed with etdc  and working completely in main memory1. basically  ii is a blockgrained inverted index: it assumes that the indexed text is partitioned into blocks of size b  and for each term it keeps a list of occurrences that stores all the block-ids in which that term occurs. to reduce its size  the lists of occurrences were compacted using the index+bc strategy in ; that is  an absolute sample is kept every s values  and the remaining values are kept with a d-gap strategy combined with byte-codes  bc . moreover  the text is compressed with etdc. therefore  the list of occurrences of a term t points actually to the blocks that contain the beginning of the codeword etdc t  associated by etdc to t. searches for a word t are done by obtaining the block-ids of the blocks in which t appears and then searching for etdc t   using horspool's algorithm  in the pointed blocks. searches for a phrase t1 ...tm imply intersecting the lists of occurrences of all the terms t1 ...tm and finally applying horspool's algorithm to search for the sequence of their codewords p = etdc t1 ...etdc tm  in such blocks1. as recommended in   the intersection of lists is done using set-vs-set  which begins with the shortest list as a pivot and intersects it against the others in increasing order of length  using binary search on the larger set to carry out each intersection.
　ii uses two parameters: the block size  b  measured in kilobytes  and the sample value  s   such that given a posting list of size absolute samples will be chosen at regular intervals of size s. different space-time trade-offs are obtained depending on such parameters. to make a fair

1
 etdc is used for the ii because it permits the fastest text scanning  which is essential to ii's performance. for fairness with the space etdc costs to ii  we compared to wtdc+ instead of wtph+  which was better in all aspects.
1
 note that we miss phrases that span over consecutive blocks  so a complete ii solution would pay some kind of additional penalty to handle this properly.
comparison between ii and wtdc+ we chose two differ-
ent configurations for ii  iis1 b1 and iis1 b1  by setting b = 1  s = 1; and b = 1  s = 1  respectively. then  we chose two settings of wtdc+  wt1 and wt1  that required almost the same amount of memory. more precisely  wt1 uses one block per each 1 bytes in the compressed text  and 1 superblock per each 1 blocks. in wt1  we used 1 superblock per each 1 blocks and 1 block per 1 bytes. the sizes of the resulting four structures  as well as their compression ratios  are shown in table 1.
table 1: sizes of the compared wtdc+ and ii structures.
index typewavelet treesblock-add inv indexeswt1	wt1iis1 b1	iis1 b1size mb 1	11	1c. ratio % 1	1	1	1　table 1 shows the time  in seconds  needed to locate all the text occurrences of 1 words randomly chosen from the vocabulary  and to extract all the snippets around such occurrences. we show results for 1 groups of words depending on their number of occurrences  frequency f : i  f ＋ 1  ii  1   f ＋ 1  iii  1   f ＋ 1   and iv  f   1. the snippets were obtained by decompressing 1 words  starting at an offset 1 words before an occurrence. both locate and the extraction of snippets are faster in wtdc+ than in ii  as the former directly jumps to the next occurrence whereas the latter has to scan the text. only when we are extracting the snippets of very frequent words can ii beat wtdc+  by profiting from the locality of its text decompression process.
table 1: searching for words: wtdc+ vs blockaddressing inverted index. times in seconds.
freq.wt1 iis1 b1wt1 iis1 b111111locate1 1.1.1.1.11-11111 1111111111snippet1 1.1.1.1.11-11111 11	11	1　table 1 shows the time  in seconds  needed for performing locate and extract-snippet operations over 1 distinct phrase patterns randomly chosen from the text  and discarding those where all are stopwords . results are given depending on the number of words  1 1  in the phrase pattern. in all cases wtdc+ behaves better than ii  and the gap widens as less space is used in both structures. in particular the difference in locate times is always notorious  whereas those for snippet extraction tend to reduce as more snippets have to be extracted  just as for one-word queries.
table 1: searching for phrases: wtdc+ vs blockaddressing inverted index. times in seconds.
#wordswt1iis1 b1wt1iis1 b11.1.1.1.11.1.1.1.1locate1.1.1.1.11.1.1.1.11.1.1.1.11.1.1.1.1snippet1.1.1.1.11.1.1.1.1　we remark that our good results essentially owe to the fact that we are not sequentially scanning any significant portion of the file  whereas a block addressing inverted index must sequentially scan  sometimes a significant number of  blocks. as more space is given to both structures  both improve in time but the ii eventually takes over wtdc+  this occurs when both use around 1% of the text's space and a block size of 1 bytes is set for ii . if sufficient space is given  1%-1%   the ii can directly point to occurrences instead of blocks and needs no scanning. yet  as explained in the introduction  using little space is very relevant for the current trend of maintaining the index distributed among the main memory of several processors. what our experiments show is that the wtdc+ makes better use of the available space when there is not much to spend.
1. conclusions and future work
　it has been long established that semistatic word-based byte-oriented compressors such as those considered in this paper are useful not only to save space and time  but also to speed up sequential search for words and phrases. however  the more efficient compressors such as ph and rpbc are not that fast at searching or random decompression  because they are not self-synchronizing. in this paper we have shown how a simple reorganization of the bytes of the codewords obtained when a text is being compressed  can produce clear codewords boundaries for those compressors. this gives better search capabilities and random access than all the byteoriented compressors  even those that pay some compression degradation to mark codeword boundaries  th  etdc .
　as our reorganization permits carrying out all those operations efficiently over ph  the most space-efficient byteoriented compressor  the usefulness of looking for coding variants that sacrifice compression ratio for search or decoding performance is questioned: a reorganized plain huffman  wph  will do better in almost all aspects.
　this reorganization has also surprising consequences related to implicit indexing of the compressed text. blockaddressing indexes over compressed text have been long considered the best low-space structure to index a text for efficient word and phrase searches. they can trade space for speed by varying the block size. we have shown that the reorganized codewords provide a powerful alternative to these inverted indexes. by adding a small extra structure to the wavelet trees  the search operations are speeded up so sharply that the structure competes successfully with blockaddressing inverted indexes that take the same space on top of the compressed text. especially  our structure is superior when little extra space on top of the compressed text is permitted. more experiments are required to compare more exhaustively our wavelet trees against not only inverted indexes but also other reduced-space structures.
