content-based music genre classification is a fundamental component of music information retrieval systems and has been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the internet. currently little work has been done on automatic music genre classification  and in addition  the reported classification accuracies are relatively low. this paper proposes a new feature extraction method for music genre classification  dwchs1. dwchs capture the local and global information of music signals simultaneously by computing histograms on their daubechies wavelet coefficients. effectiveness of this new feature and of previously studied features are compared using various machine learning classification algorithms  including support vector machines and linear discriminant analysis. it is demonstrated that the use of dwchs significantly improves the accuracy of music genre classification.
categories and subject descriptors
h.1  information storage and retrieval : content analysis and indexing information search and retrieval; i.1  artificial intelligence : learning; i.1  pattern recognition : applications; j.1  arts and humanities : music
general terms
algorithms  measurement  performance  experimentation  verification
keywords
music genre classification  wavelet coefficients histogram  feature extraction  multi-class classification

1
 dwchs stands for daubechies wavelet coefficient histograms
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1-august 1  1  toronto  canada.
copyright 1 acm 1-1/1 ...$1.
1. introduction
　music is not only for entertainment and for pleasure  but has been used for a wide range of purposes due to its social and physiological effects. at the beginning of the 1st century the world is facing ever-increasing growth of the on-line music information  empowered by the permeation of internet into daily life. efficient and accurate automatic music information processing  accessing and retrieval  in particular  will be an extremely important issue  and it has been enjoying a growing amount of attention. music can be classified based on its style and the styles have a hierarchical structure. a currently popular topic in automatic music information retrieval is the problem of organizing  categorizing  and describing music contents on the web. such endeavor can be found in on-line music databases such as mp1.com and napster. one important aspect of the the genre structures in these on-line databases is that the genre is specified by human experts as well as amateurs  such as the user  and that labeling process is time-consuming and expensive. currently music genre classification is done mainly by hand because giving a precise definition of a music genre is extremely difficult and  in addition  many music sounds sit on boundaries between genres. these difficulties are due to the fact that music is an art that evolves  where performers and composers have been influenced by music in other genres. however  it has been observed that audio signals  digital or analog  of music belonging to the same genre share certain characteristics  because they are composed of similar types of instruments  having similar rhythmic patterns  and similar pitch distributions . this suggests feasibility of automatic musical genre classification.
　by automatic musical genre classification we mean here the most strict form of the problem  i.e.  classification of music signals into a single unique class based computational analysis of music feature representations. automatic music genre classification is a fundamental component of music information retrieval systems. we divide the process of genre categorization in music into two steps: feature extraction and multi-class classification. in the feature extraction step  we extract from the music signals information representing the music. the features extract should be comprehensive  representing the music very well   compact  requiring a small amount of storage   and effective  not requiring much computation for extraction . to meet the first requirement the design has to be made so that the both low-level and high-level information of the music is included. in the second step  we build a mechanism  an algorithm and/or a mathematical model  for identifying the labels from the representation of the music sounds with respect to their features.
　there has been a considerable amount of work in extracting features for speech recognition and music-speech discrimination  but much less work has been reported on the development of descriptive features specifically for music signals. to the best of our knowledge  currently the most influential approach to direct modeling of music signals for automatic genre classification is due to tsanetakis and cook   where the timbral texture  rhythm  and pitch content features are explicitly developed. the accuracy of classification based on these features  however  is only 1% is achieved on their ten-genre sound dataset. this raises the question of whether there are different features that are more useful in music classification and whether the use of statistical or machine learning techniques  e.g.  discriminant analysis and support vector machines  can improve the accuracy. the main goal of this paper is to address these two issues.
　in this paper  we propose a new feature extraction method based on wavelet coefficients histogram  which we call dwchs. dwchs represent both local and global information by computing histograms on daubechies wavelet coefficients at different frequency subbands with different resolutions. using dwchs along with advanced machine learning techniques  accuracy of music genre classification has been significantly improved. on the dataset of   the classification has been increased to almost 1%. on genre specific classification  i.e.  distinguish one genre from the rest  the accuracy can be as high as 1%. we also examined which features proposed in  are the most effective. it turns out that the timbral features combined with the mel-frequency cepstral coefficients achieve a high accuracy on each of the multi-class classification algorithms we tested. the rest of the paper is organized as follows: section 1 reviews the related work on automatic genre classification. section 1 gives a brief overview of the previously proposed features. section 1 presents the wavelet coefficients histogram features. section 1 describes the machine learning techniques used for our experiments. section 1 presents and discusses our experimental results. finally  section 1 provides our conclusions.
1. related work
　many different features can be used for music classification  e.g.  reference features including title and composer  content-based acoustic features including tonality  pitch  and beat  symbolic features extracted from the scores  and text-based features extracted from the song lyrics. in this paper we are interested in content-based features1.
　the content-based acoustic features are classified into timbral texture features  rhythmic content features and pitch content features . timbral features are mostly originated from traditional speech recognition techniques. they are usually calculated for every short-time frame of sound based on the short time fourier transform  stft  . typical timbral features include spectral centroid  spectral rolloff  spectral flux  energy  zero crossings  linear prediction coefficients  and mel-frequency cepstral coefficients  mfccs   see  for more detail . among these tim-

1
 our future work will address the issue of mixing different types of futures.
bral features mfccs have been dominantly used in speech recognition. logan  examines mfccs for music modeling and music/speech discrimination. rhythmic content features contains information about the regularity of the rhythm  the beat and tempo information. tempo and beat tracking from acoustic musical signals has been explored in  1  1  1 . foote and uchihashi  use the beat spectrum to represent rhythm. pitch content features deals with the frequency information of the music bands and are obtained using various pitch detection techniques.
　much less work has been reported on music genre classification. tzanetakis and cook  proposes a comprehensive set of features for direct modeling of music signals and explores the use of those features for musical genre classification using k-nearest neighbors and gaussian mixture models. lambrou et al.  uses statistical features in the temporal domain as well as three different wavelet transform domains to classify music into rock  piano and jazz. deshpande et al.  uses gaussian mixtures  support vector machines and nearest neighbors to classify the music into rock  piano  and jazz based on timbral features. pye  investigates the use of gaussian mixture modeling  gmm  and tree-based vector quantization in music genre classification. soltau et al.  propose an approach of representing temporal structures of input signal. they show that this new set of abstract features can be learned via artificial neural networks and can be used for music genre identification.
　the problem of discriminate music and speech has been investigated by saunders   scheier and slaney . zhang and kuo  propose a heuristic rule-based system to segment and classify audio signals from movies or tv programs. in  audio contents are divided into instrument sounds  speech sounds  and environment sounds using automatically extracted features. foote  constructs a learning tree vector quantizer using twelve mfccs plus energy as audio features for retrieval. li and khokhar  propose nearest feature line methods for content based classification audio retrieval.
1. the content-based features
1 timbral textural feature
　timbral textual features are used to differentiate mixture of sounds that are possibly with the same or similar rhythmic and pitch contents. the use of these features originates from speech recognition. to extract timbral features  the sound signals are first divided into frames that are statistically stationary  usually by applying a windowing function at fixed intervals. the window function  typically a hamming window  removes edge effects. timbral textural features are then computed for each frame and the statistical values  such as the mean and the variance  of those features are calculated.
  mel-frequency cesptral coefficients  mfccs  are designed to capture short-term spectral-based features. after taking the logarithm of the amplitude spectrum based on stft for each frame  the frequency bins are grouped and smoothed according to mel-frequency scaling  which is design to agree with perception. mfccs are generated by decorrelating the mel-spectral vectors using discrete cosine transform.
  spectral centroid is the centroid of the magnitude spectrum of stft and it is a measure of spectral brightness.
  spectral rolloff is the frequency below which 1% of the magnitude distribution is concentrated. it measures the spectral shape.
  spectral flux is the squared difference between the normalized magnitudes of successive spectral distributions. it measures the amount of local spectral change.
  zero crossings is the number of time domain zero crossings of the signal. it measures noisiness of the signal.
  low energy is the percentage of frames that have energy less than the average energy over the whole signal. it measures amplitude distribution of the signal.
1 rhythmic content features
　rhythmic content features characterize the movement of music signals over time and contain such information as the regularity of the rhythm  the beat  the tempo  and the time signature. the feature set for representing rhythm structure is based on detecting the most salient periodicities of the signal and it is usually extracted from beat histogram. to construct the beat histogram  the time domain amplitude envelope of each band is first extracted by decomposing the music signal into a number of octave frequency band. then  the envelopes of each band are summed together followed by the computation of the autocorrelation of resulting sum envelop. the dominant peaks of the autocorrelation function  corresponding to the various periodicities of the signal's envelope  are accumulated over the whole sound file into a beat histogram where each bin corresponds to the peak lag. the rhythmic content features are then extracted from the beat histogram  and generally they contain relative amplitude of the first and the second histogram peak  ration of the amplitude of the second peak divided by the amplitude of the first peak  periods of the first and second peak  overall sum of the histogram.
1 pitch content features
　the pitch content features describe the melody and harmony information about music signals and are extracted based on various pitch detection techniques. basically the dominant peaks of the autocorrelation function  calculated via the summation of envelopes for each frequency band obtained by decomposing the signal  are accumulated into pitch histograms and the pitch content features are then extracted from the pitch histograms. the pitch content features typically include: the amplitudes and periods of maximum peaks in the histogram  pitch intervals between the two most prominent peaks  the overall sums of the histograms.
1. dwchs of music signals
　it is not difficult to see that the traditional feature extraction described in section 1 more or less capture incomplete information of music signals. timbral textural features are standard features used in speech recognition and are calculated for every short-time frame of sound while rhythmic and pitch content features are computed over the whole file. in other words  timbral features capture the statistics of local information of music signals from a global perspective  but not enough in representing the global information of the music. moreover  as indicated by our experiments in section 1  the rhythm and pitch content features don't seem to capture enough information content for classification purpose. in this section  we will propose a new feature extraction technique for music genre classification  namely  dwchs  based on wavelet histogram to capture local and information of music signals simultaneously.
1 wavelet basics
　the wavelet transform is a synthesis of ideas emerging over many years from different fields such as mathematics and image and signal processing and has been widely used in information retrieval and data mining  1  1 . a complete survey on wavelet application in data mining can be found in . generally speaking  the wavelet transform  providing good time and frequency resolution  is a tool that divides up data  functions  or operators into different frequency components and then studies each component with a resolution matched to its scale . straightforwardly  a wavelet coefficients histogram is the histogram of the  rounded  wavelet coefficients obtained by convolving a wavelet filter with an input music signal  details on histogram and wavelet filter/analysis can be found in  1  1  respectively .
　several favorable properties of wavelets  such as compact support  vanishing moments and decorrelated coefficients  make them useful tools for signal representation and transformation. generally speaking  wavelets are designed to give good time resolution at high frequencies and good frequency resolution at low frequencies. compact support guarantees the localization of wavelet  vanishing moment property allows wavelet focusing on most important information and discarding noisy signal  and decorrelated coefficients property enables wavelet to reduce temporal correlation so that the correlation of wavelet coefficients are much smaller than that of the corresponding temporal process . hence  after wavelet transform  the complex signal in the time domain can be reduced into a much simpler process in the wavelet domain. by computing the histograms of wavelet coefficients  we then could get a good estimation of the probability distribution over time. the good probability estimation thus leads to a good feature representation.
1 dwchs
　a sound file is a kind of oscillation waveform in timedomain and can be considered as a two dimensional entity of the amplitude over time  in the form of m t  = d a t  where a is the amplitude and generally ranges from   1 . the distinguishing characteristics are contained in the amplitude variation  and in consequence  identifying the amplitude variation would be essential for music categorization.
　on one hand  the histogram technique is an efficient means for the distribution estimation. however  the raw signal in time domain is not good representation particularly for content-based categorization since the most distinguished characteristics are hidden in frequency domain. on the other hand  the sound frequency spectrum is generally divided into octaves with each having a unique quality. an octave is the interval between any frequencies that have a tonal ratio of 1 to 1  a logarithmic-relation in frequency band. the wavelet decomposition scheme matches the models of sound octave division for perceptual scales and provides good time and frequency resolution . in other words  the decomposition of audio signal using wavelets produces a set of subband signals at different frequencies corresponding different characteristics. this motivates the use of wavelet histogram techniques for feature extraction. the wavelet coefficients are distributed in various frequency bands at different resolutions.
　there are many kinds of wavelet filters  including daubechies wavelet filter  gabor filter etc. daubechies wavelet filters are the one commonly in image retrieval  more details on wavelet filter can be found in  . in our work  we use daubechies wavelet filter db1 with seven levels of decomposition. after the decomposition  we construct the histogram of the wavelet coefficients at each subband. the coefficient histogram provides a good approximation of the waveform variations at each subband. from probability theory  a probability distribution is uniquely characterized by its moments. hence  if we interpret the waveform distribution as a probability distribution  then it can be characterized by its moments. to characterize the waveform distribution  the first three moments of a histogram is used . the first three moments are the average  the variance and the skewness of each subband. we also compute the subband energy  defined as the mean of the absolute value of coefficients  for each subband. in addition  our final dwchs feature set includes the tradition timbral features used for speech recognition.
　each music file in our datasets is 1-second signal that will be first converted to an extremely long vector. based on an intuition of  self-similarity  of a music  i.e.  its repeated theme  the dwchs feature can be extracted on a small slice of an input music signal and in our experiments 1 seconds sound clip are used. in summary  our algorithm of dwchs extraction contains the following steps:
1. the wavelet decomposition of the music signals is obtained
1. histogram of each subband is constructed
1. compute the first three moments of all histograms
1. compute the subband energy for each subband
the algorithm is very easy to implement in matlab that contains a complete wavelet package.
1 examples
　concrete examples will make clearer the dwchs features and their putative advantages. in figure 1  from left to right and top to bottom  dwchs features of ten music sounds drawn from the different genres of the ten-genre dataset  the dataset a that we used in our experiments  to be described in section 1  are shown. we observe the distinct difference of the feature representations of different music genres. for example  the features on several subbands  corresponding the features from 1 to 1  are clearly different for country and disco. the differences are quite consistent with the human feeling on music. figure 1 and figure 1 show dwchs features of ten blues and ten ten classical music sounds taken from dataset a respectively. we observe that similar features are present inside a single music genre. if our observations are correct  a unique dwchs feature pattern exists in each music genre  and we can hypothesize that the use of dwchs will improve classification of music genre.
	blues	classical

	1	1	1	1
figure 1: dwchs of 1 music signals in different genre. the feature representations of different genre are mostly different to each other.
1. multi-class learning methods
　once the feature set has been extracted  the music genre classification problem is reduced to a multi-class classification problem. the problem can be formally defined as follows: the input to the problem is a set of training samples of the form of   where xi is a data point and li is its label  chosen from a finite set of labels {c1 c1 ，，，  ck}. in our case the labels are music genres. the goal is to infer a function f that well approximates the mapping the x's to their labels. generally speaking  approaches to multiclass classification problems can be roughly divided into two groups. the first group consists of those binary classification algorithms that can be naturally extended to handle multiclass cases. this group contains such algorithm as discriminant analysis  k-nearest neighbors  regression  and decision trees including c1 and cart. the second group consists of methods that involve decomposition of multi-class classification problems to a collection binary ones. many decomposition technique exists  including such popular methods as the one-versus-the-rest method  pairwise comparison  error-correcting output coding  ecoc   and multi-class objective functions.
　the idea of the one-versus-the-rest method is as follows: to get a k-class classifier  first construct a set of binary classifiers c1 c1 ，，，  ck. each binary classifier is first trained to separate one class from the rest and then the multi-class classification is carried out according to the maximal output of the binary classifiers. in pairwise comparison  a classifier is trained for each possible pair of classes. for k classes  this results in  k   1 k/1 binary classifiers. given a new instance  the multi-class classification is then executed by evaluating all  k   1 k/1 individual classifiers and assigning the instance to the class which gets the highest number of votes. error-correcting output coding  ecoc    roughly speaking  generates a number of binary classification problems by intelligently splitting the original set of classes into two sets. in other words  each class is assigned a unique binary strings of length l  these strings are regarded to codewords . then l classifiers are trained to predict each bit of the string. for new instances  the predicted class is the one whose codeword is the closest  in hamming distance  to the codeword produced by the classifiers. one-versus-therest and pairwise comparison can be regarded as two special cases of ecoc with specific coding schemes. multi-class objective functions aims to directly modify the objective function of binary svms in such a way that it simultaneously allows the computation of a multi-class classifier. in practice  the choice of reduction method from multi-class to binary is problem-dependent and not a trivial task and it is fair to say that there is probably no method generally outperforms the others .
　we conducted experiments with the following additional multi-class classification approaches  see  for more information about the methods :
	blues no.1	blues no.1

	1	1	1	1
figure 1: dwchs of ten blues songs. the feature representations are similar.
  support vector machines support vector machines  svms   have shown superb performance in binary classification tasks. basically  support vector machine aim at searching for a hyperplane that separates the positive data points and the negative data points with maximum margin. to extend svms for multi-class classification  we use one-versus-the-rest  pairwise comparison  and multi-class objective functions.
  k-nearest neighbor  knn  knn is a nonparametric classifier. it is proven that the error of knn is asymptotically at most twice as large as the bayesian error rate. knn has been applied in various musical analysis problems. the basic idea is to allow a small number of neighbors to influence the decision on a point.
  gaussian mixture models  gmm  gmm models the distribution of acoustics and has been widely used in music information retrieval. for each class we assume the existence of a probability density function  pdf  expressible as a mixture of a number of multidimensional gaussian distributions. the iterative em algorithm is then used to estimate the parameters for each gaussian component and the mixture weight.
  linear discriminant analysis  lda  in the statistical pattern recognition literature discriminant analysis approaches are known to learn discriminative feature transformations very well. the approach has been successfully used in many classification tasks . the basic idea of lda is to find a linear transformation that best discriminates among classes and perform classification in the transformed space based on some metric such as euclidean distances. fisher discriminant analysis finds discriminative feature transform as eigenvectors of matrix t = Σ  w1Σ b where Σ w is the intra-class covariance matrix and Σ b is the inter-class covariance matrix. this matrix t captures both compactness of each class and separations between classes. so  the eigenvectors corresponding to the largest eigenvalues of t are expected to constitute a discriminative feature transform.
1. experimental results
	   cla. no.1	   cla. no.1

	1	1	1	1
figure 1: dwchs of ten classical songs.
1 datasets
　we used two datasets for our experiments. the first dataset  dataset a  contains 1 songs over ten genres with 1 songs per genre. this dataset was used in . the ten genres are blues  classical  country  disco  hiphop  jazz  metal  pop  reggae  and rock. the excerpts of the dataset were taken from radio  compact disks  and mp1 compressed audio files. the second dataset  dataset b  contains 1 sounds over five genres: ambient  classical  fusion  jazz  and rock. this dataset was constructed for this paper from the cd collection of the second author. the collection of 1 sound files was created from 1 music albums as follows: from each album the first four music tracks were chosen  three tracks from albums with only three music tracks . then from each music track the sound signals over a period of 1 seconds after the initial 1 seconds were extracted in mp1. the distribution of different genres is: ambient  1 files   classical  1 files   fusion  1 files   jazz  1 files  and rock  1 files 1. for both datasets  the sound files are converted to 1hz  1-bit  mono audio files.
1 experiment setup
　we used marsyas1  a public software framework for computer audition applications  for extracting the features proposed in : mfccs  fft  beat and pitch. melfrequency cepstral coefficients  denoted by mfccs   the timbral texture features excluding mfccs  denoted by fft   the rhythm content features  denoted by beat   and the pitch contents feature  denoted by pitch   see section 1 . the mfccs feature vector consists of the mean and variance of each of the first five mfcc coefficients over the frames  the fft feature vector consists of the mean and variance of spectral centroid  of rolloff  of flux  and of zero crossings  and of low energy  the beat feature vector consists of six features from the rhythm histogram  the pitch feature vector consists of five features from pitch histograms. more information of the feature extraction can be found in . our original dwch feature set contains four features for each of seven frequency subbands along with nineteen traditional timbral features. however  we found that not all the frequency subbands are informative and we only use selective subbands  resulting a feature vector of length 1.
　for classification methods  we use three different reduction methods to extend svm for multi-class: pairwise  oneagainst-the-rest  and multi-class objective functions. for one-against-the-rest and pairwise methods  our svm implementation was based on the libsvm   a library for support vector classification and regression. for multiclass objective functions  our implementation was based on mpsvm . for experiments involving svms  we tested them with linear  polynomial  and radius-based kernels and the best results are reported in the tables. for gaussian mixture models  we used three gaussian mixtures to model each music genre. for k-nearest neighbors  we set k = 1.
1 results and analysis
　table 1 shows the accuracy of the various classification algorithms on dataset a. the bottom four rows show how the classifiers performed on a single set of features proposed in . the experiments verify the fact that each of the tradition features contains useful yet incomplete information characterizing music signals. the classification accuracy on any single feature set is significantly better than random guess the accuracy of random guess on dataset a is 1%. . the performance with either fft or mfcc was significantly higher than that with beat or pitch in each of the methods tested. this naturally raises a question of whether fft and mfcc are each more suitable than beat or pitch for music genre classification. we combined the four sets of features in every possible way to examine the accuracy. the accuracy with only beat and pitch is significantly smaller than the accuracy with any combination that includes either fft or mfcc. indeed  the accuracy with only fft and mfcc is almost the same as that with all four for all methods. this seems to answer positively to our question.

1
 a list of music sources for dataset b is available at http://www.cs.rochester.edu/u/ogihara/music/sigirlist.xls. 1
marsyas can be downloaded from
http://www.cs.princeton.edu/゛gtzan/marsyas.html.
　on the other hand  the use of dwchs further improved the accuracy on all methods. in particular  there is a significant jump in the accuracy when support vector machines are used with either the pairwise or the one-versus-all approach. the accuracy of the one-versus-the-rest svm is 1% on the average in the ten-fold cross validation. for some of the cross validation tests the accuracy went beyond 1%. this is a remarkable improvement over tsanetakis and cook's 1%. perrot and gjerdigen  report a human study in which college students were trained to learn a music company's genre classification on a ten-genre data collection in which about 1% of accuracy is achieved. although these results are not directly comparable due to the different dataset collections  it clearly implies that the automatic content-based genre classification could possibly achieve similar accuracy as human performance. in fact  in comparison the performance of our best method seems to go far beyond that.
　there are papers reporting better accuracy of automatic music genre recognition of smaller datasets. pye  reports 1% on a total set of 1 songs covering six genres  blues  easy listening  classical  opera  dance  and indie rock . soltau et al.  report 1% accuracy on four classes  rock  pop  techno  and classical . just for the sake of comparison  we show in table 1 the performance of the best classifier  dwchs with svm  on one-versus-all tests on each of the ten music genres in dataset a. the performance of these classifiers are extremely good. also  in table 1 we show the performance of the multi-class classification for distinction among smaller numbers of classes. the accuracy gradually decreases as the number of classes increases.
　table 1 presents the results on our own dataset. this dataset was generated with little control by blindly taking 1 seconds after introductory 1 seconds of each music and covers many different albums  so the performance was anticipated to be lower than that for database a. also  there is the genre of ambient  which covers music bridging between classical and jazz. the difficulty in classifying such borderline cases is compensated for the reduction in the number of classes. the overall performance was only 1 to 1% lower than that for database a.
　we observe that svms are always the best classifiers for content-based music genre classification. however  the choice of the reduction method from multi-class to binary seems to be problem-dependent and there is no clear overall winner. it is fair to say that there is probably no reduction method generally outperforms the others. feature extraction is crucial for music genre classification. the choice of features is more important than the choice of classifiers. the variations of classification accuracy on different classification techniques are much smaller than those of different feature extraction techniques.
1. conclusions and future work
　in this paper we proposed dwchs  a new feature extraction method for music genre classification. dwchs represent music signals by computing histograms on daubechies wavelet coefficients at various frequency bands and it has significantly improved the classification accuracy. we also provide a comparative study of various feature extraction and classification methods and investigate the classification performance of various classification methods on different feature sets. to the best of our knowledge  this is the first
　
featuresmethodssvm1svm1mpsvmgmmldaknndwchs1 1 1 1 1 1 1 1 1 1 1 1 beat+fft+mfcc+pitch1 1 1 1 1 1 1 1 1 1 1 1 beat+fft+mfcc1 1 1 1 1 1 1 1 1 1 1 1 beat+fft+pitch1 1 1 1 1 1 1 1 1 1 1 1 beat+mfcc+pitch1 1 1 1 1 1 1 1 1 1 1 1 fft+mfcc+pitch1 1 1 1 1 1 1 1 1 1 1 1 beat+fft1 1 1 1 1 1 1 1 1 1 1 1 beat+mfcc1 1 1 1 1 1 1 1 1 1 1 1 beat+pitch1 1 1 1 1 1 1 1 1 1 1 1 fft+mfcc1 1 1 1 1 1 1 1 1 1 1 1 fft+pitch1 1 1 1 1 1 1 1 1 1 1 1 mfcc+pitch1 1 1 1 1 1 1 1 1 1 1 1 beat1 1 1 1 1 1 1 1 1 1 1 1 fft1 1 1 1 1 1 1 1 1 1 1 1 mfcc1 1 1 1 1 1 1 1 1 1 1 1 pitch1 1 1 1 1 1 1 1 1 1 1 1 table 1: classification accuracy of the learning methods tested on dataset a using various combinations of features. the accuracy values are calculated via ten-fold cross validation. the numbers within parentheses are standard deviations. svm1 and svm1 respectively denote the pairwise svm and the one-versus-the-rest svm.

numbergenreaccuracy1disco1  1 1blues1  1 1jazz1 1 1classical1  1 1metal1  1 1country1  1 1pop1  1 
1hiphop1  1 1reggae1  1 1rock1  1 table 1: genre specific accuracy of svm1 on dwchs. the results are calculated via ten fold cross validation and each entry in the table is in the form of accuracy standard deviation .
classesmethodssvm1svm1mpsvmgmmldaknn1 & 1.1.1 1 1 1 1 1 1 1 1 1 1 1  1 & 1.1.1 1 1 1 1 1 1 1 1 1 1 1 through 1.1.1 1 1 1 1 1 1 1 1 1 1 1 through 1.1.1 1 1 1 1 1 1 1 1 1 1 1 through 1.1.1 1 1 1 1 1 1 1 1 1 1 1 through 1.1.1 1 1 1 1 1 1 1 1 1 1 1 through 1.1.1 1 1 1 1 1 1 1 1 1 1 1 through 1.1.1 1 1 1 1 1 1 1 1 1 1 table 1: accuracy on various subsets of dataset a using dwchs. the class numbers correspond to those of table 1. the accuracy values are calculated via ten-fold cross validation. the numbers in the parentheses are the standard deviations.
featuresmethodssvm1svm1mpsvmgmmldaknndwchs1 1 1 1 1 1 1 1 1 1 1 1 beat+fft+mfcc+pitch1 1 1 1 1 1 1 1 1 1 1 1 fft+mfcc1 1 1 1 1 1 1 1 1 1 1 1 beat1 1 1 1 1 1 1 1 1 1 1 1 fft1 1 1 1 1 1 1 1 1 1 1 1 mfcc1 1 1 1 1 1 1 1 1 1 1 1 pitch1 1 1 1 1 1 1 1 1 1 1 1 table 1: classification accuracy of the learning methods tested on dataset b using various combinations of features calculated via via ten-fold cross validation. the numbers within parentheses are standard deviations.
　
study of such kind in music genre classification.
acknowledgments
the authors thank george tzanetakis for useful discussions and for kindly sharing his data with us. this work is supported in part by nsf grants eia-1  due-1  and eia-1  and in part by nih grants ro1-ag1  1  and p1-ag1.
