random sampling is one of the most fundamental data management tools available. however  most current research involving sampling considers the problem of how to use a sample  and not how to compute one. the implicit assumption is that a  sample  is a small data structure that is easily maintained as new data are encountered  even though simple statistical arguments demonstrate that very large samples of gigabytes or terabytes in size can be necessary to provide high accuracy. no existing work tackles the problem of maintaining very large  disk-based samples from a data management perspective  and no techniques now exist for maintaining very large samples in an online manner from streaming data. in this paper  we present online algorithms for maintaining on-disk samples that are gigabytes or terabytes in size. the algorithms are designed for streaming data  or for any environment where a large sample must be maintained online in a single pass through a data set. the algorithms meet the strict requirement that the sample always be a true  statistically random sample  without replacement  of all of the data processed thus far. our algorithms are also suitable for biased or unequal probability sampling.
1 introduction
