we argue that the ability to identify pairs of related terms is at the heart of what makes spectral retrieval work in practice. schemes such as latent semantic indexing  lsi  and its descendants have this ability in the sense that they can be viewed as computing a matrix of term-term relatedness scores which is then used to expand the given documents  not the queries . for almost all existing spectral retrieval schemes  this matrix of relatedness scores depends on a fixed low-dimensional subspace of the original term space. we instead vary the dimension and study for each term pair the resulting curve of relatedness scores. we find that it is actually the shape of this curve which is indicative for the termpair relatedness  and not any of the individual relatedness scores on the curve. we derive two simple  parameterless algorithms that detect this shape and that consistently outperform previous methods on a number of test collections. our curves also shed light on the effectiveness of three fundamental types of variations of the basic lsi scheme.
categories and subject descriptors
h.1  information search and retrieval : clustering  retrieval models
general terms
algorithms  performance  theory  experimentation
keywords
spectral analysis  latent semantic indexing  document expansion  curve of relatedness scores
1. introduction
　latent semantic indexing  lsi   became famous as one of the first information retrieval techniques for fully automatically and with surprising effectiveness dealing with the problems of synonymy  web  internet  and polysemy  surfing the web  surfing at waikiki beach   which make searching
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
by pure text matching  where only documents are returned that contain all or at least some of the words in the query  so frequently a frustrating experience.
　lsi and its many successors are based on what is usually referred to as the vector space model   where documents as well as queries are represented as vectors  with each dimension corresponding to a word or term that occurs in the document collection; the similarity between a document and a query is then measured by the similarity between the corresponding vectors  typical measures being the dot product or the cosine of the angle  which coincide if the vectors are normalized .
　the key idea of lsi is to map both queries and documents from the m-dimensional space  where m is the number of terms  to a space of significantly lower dimension k  and compute vector similarities in the reduced space instead of in the original one. the hope is that while the dimensions of the original space correspond to the specific terms used  the dimensions of the lower dimensional space correspond more to the  relatively few  concepts underlying this term usage. we will therefore refer to the original m-dimensional space as term space and to the reduced k-dimensional space as concept space.
　lsi derives the low-dimensional representation by a spectral decomposition of the term-document matrix  more precisely  by projecting the high-dimensional vectors on the k most significant left singular vectors  i.e.  pertaining to the k largest singular values. following lsi  many other related schemes have been proposed         which all do  or can be used for  what we call spectral retrieval in this paper: the documents are projected from the original term space to some lower-dimensional space spanned by eigenvectors related to the term-document matrix.1 all of these schemes can be viewed as to augment the basic lsi method by mere normalizations and rescalings  which can have a significant impact on retrieval quality. this will be explained and discussed in section 1.
　there have been many attempts to explain the success of lsi and spectral retrieval in general  most notably the analyzes of       and . the bottom line of all these results is that if the term-document matrix is a slight perturbation of some rank-k matrix - and the results differ in what kind of perturbation is permitted - then spectral retrieval will succeed in recovering that rank-k matrix. on the one hand  this is of course an interesting and non-trivial

property. on the other hand  any statement of this kind is much closer to the mathematics of spectral retrieval  approximating a set of points from a high-dimensional space by a set of points in a lower-dimensional space  than to what real document collections actually look like. for example  any of the many term-document matrices we have seen so far - be they small  medium  or large - can be seen as  approximately rank k  for about any k with equal justification. subtler points are that there is usually more than one reasonable way to divide a set of documents into  a given number of  categories  and that natural categorizations are often not flat but rather hierarchical.
　in this paper  we take a novel approach to understanding and analyzing spectral retrieval  and to making use of its potential. we first work out the details of lsi  the most basic implementation of spectral retrieval  as a document expansion  and not query expansion  process. this leads us to the insight that spectral retrieval essentially works by assigning a relatedness score to each pair of terms. we then introduce our central notion: the curve of relatedness scores  which for a given term pair shows the mentioned relatedness score as the dimension k of the concept space is varied from 1 to the full rank of the term-document matrix. we then provide complementary theoretical and empirical evidence that the relatedness of two terms in a collection corresponds to a certain characteristic shape of the curve of relatedness scores and not to any of the individual scores on the curve. our findings imply a surprising answer to the question of the appropriate choice of dimension  namely that every fixed choice is inappropriate for a significant fraction of the term pairs. but our findings also help us out of this dilemma  by naturally leading to two simple  parameterless algorithms that assess the shape of the curve of relatedness scores for each term pair and thus overcome the limitations of methods committing to a concept space of fixed dimension. we find that on three test collections - one small  one mediumsized  and one large - our algorithms consistently outperform lsi and a variety of its successors.
　our findings also shed new light on the effectiveness of the many variations of spectral retrieval. in section 1  we formulate three fundamental types of variations of the basic lsi scheme  which together capture almost all variants of spectral retrieval which we found in the literature. we comment on the effects of each of these variations from the point of view of our curves of relatedness scores. in particular  we will see that none of these variations can overcome the problems associated with considering only individual scores of the curves  instead of their overall shape.
　parts of our work were inspired by the setup from dupret   who experimentally investigated the influence of the choice of k on particular terms  not term pairs . in a statistician's approach  efron  provided a number of accurate formulas for predicting that k which yields the best retrieval performance  among the possible choices of a fixed k . in a very recent work  dupret  presented a heuristic that avoids committing to a fixed k  with promising  yet inconclusive results and without much emphasis on theoretical foundation and explanation.
1. viewing lsi as document expansion
it has been remarked  but never explored further in much detail  by several authors that lsi is related to a process known as query expansion  where terms related to those initially present are added to a query. as we show next  lsi is in fact doing exactly what could be called document expansion. the most widely used variant of lsi maps vectors from the m-dimensional term space to the k-dimensional concept space by the map x 1★ uktx  where uk is the matrix containing the k left singular vectors pertaining to the k largest singular values. the cosine-similarity of a query q with document ai in the lsi space is then given by the formula
 uktq t ， uktai / |uktq| ， |ukt ai| .
if our goal is to rank the documents by their similarities to a given query  we can drop the division by |ukt q|  it is the same for every document   and obtain the 1 〜 n row vector of all similarities by

where n is an n〜n diagonal matrix  with.
now a simple but important observation is that
 
and
|uktai|1 = ati ukuktai = ati ukuktukuktai = |ukuktai|1.
instead of mapping both queries and documents to the kdimensional concept space via and computing the cosine similarity there  we may therefore as well transform the documents via the m 〜 m matrix ukukt  and compute cosine similarities in the original term space. according to the preceding calculations  both procedures will yield exactly the same ranking.
　now the effect of multiplying a document or any vector in the m-dimensional term space by an m〜m matrix  as ukukt is one  has a simple intuition. let us call the matrix t and the vector d  and first assume that both have entries either zero or one. then if tij = 1 and dj = 1  the ith component of td will also be  at least  1  that is  the effect of tij = 1 is to  add  term i to the document whenever term j is present. more generally  for arbitrary values in t and d  the entry in tij determines to which extent the weight for term i should be increased when term j is present. note that for lsi  t may also have negative values  in which case a term can also effect the  subtraction  of other terms.
1. thecurveofrelatednessscores and its mathematics
　as we said  almost all previous spectral retrieval schemes commit to a particular dimension  and from that viewpoint it appears natural to study the entries of the document expansion matrix t = ukukt for some fixed value of k. the central idea of this paper is now to shift to a different viewpoint  and study how the entry at a fixed position  term pair  changes when the dimension is varied  that is  instead of looking at all the entries for a fixed dimension  we now look at a fixed entry for all dimensions.
　definition 1. given an m〜n term-document matrix of rank r  let uΣv t be its singular value decomposition  where u =  uij  is an m〜r matrix and the singular values in Σ are sorted in descending order. then the curve of relatedness scores for terms i and j is a plot of the function
　　　　k k 1★ xuilujl	for	k = 1 ... r.
l=1
　by looking at the curves of many term pairs  one quickly recognizes three characteristic types of shapes  irrespective of the document collection under consideration.

figure 1: curves of relatedness scores for pairs of related terms  a  +  b   and for a pair of unrelated terms  c   all from a collection of computer science abstracts.
　figure 1 shows a typical curve for each type. curves of type  a  go up relatively steadily from beginning to end. curves of type  b  go up steadily in the beginning  but then from some point on start to go down again. curves of type  c  are quite different in that they do not have a clear direction upward or downward; they are also less smooth. there are a few intermediate cases  but most curves quite naturally fall into one of these three categories. it turns out that curves of type  a  and  b  typically belong to intuitively more or less related terms   voronoi  and  diagram    job  and  scheduling    while most curves of type  c  belong to intuitively unrelated terms   geometry  and  logic  . so far  these are empirical observations  but we will soon see theoretical evidence for what we have described here.
　it is important to note that for our categorization above we did not use the order of magnitude of the relatedness scores. it is true that curves of types  a  and  b  on average reach larger scores than those of type  c . but it will become clear by the theory that follows that individual scores on the curves - and almost all previous schemes are based on these - are much less reliable indicators for term relatedness than the overall shape of the curves.
1 perfectly related terms
　we next explain why intuitively related terms give rise to curves of the types  a  and  b  that we have seen in figure 1. to this end  we introduce a notion of perfectly related terms  which are terms that have identical co-occurrence patterns in the document collection; this definition was inspired by the experimental setup of dupret .
　definition 1. two terms  indexed i and j; without loss of generality assume i = m   1  j = m  in an m 〜 n termdocument matrix a are called perfectly related if  for some permutation of the columns of a 

where a1 is a sub-matrix with dimension  m 1 〜n1  a1 is a sub-matrix with dimension  m 1 〜n1  a1 is a sub-matrix of dimension  m 1 〜n1  a1 and b1 are row vectors of length n1 each and a1 is a row vector of length n1  consequently  1 + n1 + n1 = n .
　the following lemma says that a pair of perfectly related terms gives rise to a very particular substructure in the left singular vectors  which  as we will explain afterwards  implies an equally particular kind of curve of relatedness scores.
lemma 1. for a matrix a as in definition 1 
　 a  the vectora left sin-
gular vector of a;
　 b  the corresponding singular value is |a1   b1|;
　 c  for all other left singular vectors u of a  um 1 = um  that is  the last two entries are equal.
　proof. if a = uΣv t is the singular value decomposition of a  and we define c = aat  we have
c =  uΣv t  uΣv t t = uΣ v tv  Σtut = uΣ1ut 
because v tv is the identity matrix. the left singular vectors of a are therefore just the eigenvectors of c  and the singular values of a are the square roots of the eigenvalues of c  so that we may as well consider c instead of a. now if a is as stated in  1   then
ct1 x yct1   y
x  where c1 = 1at1 +a1at1 +a1t is an  m 1 〜 m 1  matrix  1
  and . then eigenvector of c with
eigenvalue x   y because cv =  x   y v  and

moreover  since all other eigenvectors u of c are orthogonal

to v  the dot product utv =  um 1/〔1+um/〔1 has to be zero  hence um 1 = um. 
　lemma 1 implies the following particular shape of the curve of relatedness scores for two perfectly related terms. if k is the rank of the special singular vector from lemma 1 a   then because of lemma 1 c  the relatedness scores will steadily increase until k  then at dimension k fall off by 1  and then again increase steadily. for an example  see the left curve of figure 1. by lemma 1 b   the dimension k of fall-off is exactly1 the number of singular values which are greater or equal to |a1   b1|.
　the dimension of fall-off depends on the co-occurrence pattern of the term pair in an interesting way. for an intuitive explanation  let us assume that b1 = 1  which  according to definition 1  means that when the terms co-occur  they do so in the same frequency. if then also |a1| = 1  the two terms co-occur whenever they occur. then |a1   b1| is

zero  which means that the relatedness scores will increase on the whole range from 1 to the full rank of the matrix. this corresponds to curves of the type shown in figure 1 a . when |a1| is non-zero  then the larger it is compared to other terms in the collection  the more singular values will be smaller than |a1   b1|  and the earlier the dimension of fall-off will come. this leads to graphs of the type shown in figure 1 b .1
1 adding perturbations
　the following lemma shows that our definition of perfectly related terms is robust under small perturbations of the term-document matrix. in view of another application in section 1  the lemma is formulated slightly more generally than would be necessary for this section. in the following we will write mij for the entries of an arbitrary matrix m  and |m1|f for the frobenius norm  which is the square root of pmij.
　lemma 1. let a be a term-document matrix and let k be an integer such that the matrix u of the k most significant left singular vectors of a has two identical rows i and j. let e be any matrix with |e|f bounded by some fraction f   1 of the gap between the singular values σk and σk+1  and let u1 be the matrix of the k most significant left singular vectors of a + e. then 

remark. in every real term-document matrix we have seen so far  a plot of the sorted singular values shows a very smooth curve  in particular  there is a gap between each pair of neighboring  non-zero  singular values. under these circumstances the lemma above implies that sufficiently small perturbations change the curve of relatedness scores only little at any dimension before it falls off.
　proof. by an adaption of stewart's theorem on the perturbation of symmetric matrices  1  theorem 1.1 on page 1  to arbitrary rectangular matrices  similarly as done in  and   it can be shown that u1 = ur+h  where r is an orthogonal1 k 〜 k matrix  and |h|f ＋1f. 1
　let u = ur and let uti   utj   and ui t  ujt denote the ith and jth row of u and u1  respectively. then u1 shares two properties with u. first  by assumption  rows i and j of u are identical  and this property is invariant under right multiplication by any matrix  so that u1 has this property as well. second  ui1t = uti r and uj1t = ujt r  so that u1ituj1 = uti rrtuj = uti uj  that is  the dot product of rows i and j is the same for u1 as for u.
　let further ui1t  uj1t  and hti   hjt denote the ith and jth row of u1 and h  respectively. then ui1t = ui1t + hti and uj1t = uj1t +htj   and we now show that because h has small norm  the dot product of rows i and j of u1 is close to that for u. for that  first write
	ui1tuj1	=	 ui1t + hti   uj1 + hj 
	=	ui1tuj1 + ui1thj + hti uj1 + hti hj.

with ui1tuj1 = uti uj  as established above  and writing u1 for the identical ui1 and uj1  this becomes uti uj + u1t hi + hj  + hti hj

	＋	uti uj + p|u1||hi + hj| + p|hi||hj| 
where the inequality follows from cauchy's inequality. now |u1| ＋ 1 because u1 is part of a row of an orthogonal matrix  and |hi| and |hj| are each bounded by |h|  and  by the triangle inequality  |hi1+ h1j| is bounded by |h|  too. this  finally  proves that |uituj   uti uj| is bounded by 1|h|f  which  in turn  is at most 1f  as desired. 
　figure 1 gives a typical example of the effect of adding perturbation for two perfectly related terms. it is important to note that the absolute values of the curve can and do actually change quite a lot on the way from two perfectly related terms to a pair as it is found in a real collection  however  the basic shape of the curve remains unchanged: there is a phase of ascend in the beginning  and exactly one intermediate phase of descend  and drawn on its scale the curve looks rather smooth.  whether another phase of ascend follows after the phase of descend or not will not be important for us. 

figure 1: curves of relatedness scores for two perfectly related terms  left   after a relatively small perturbation of the term-document matrix  middle   and after a relatively large such perturbation
 right .
　the three curves from figure 1 were obtained as follows: we took the term-document matrix of the collections of abstracts mentioned in the caption of figure 1  and modified two of its rows  terms  to obey the perfect-relatedness criterion of definition 1. the curve for the in such a way modified matrix is shown on the left of figure 1. for the curves in the middle and on the right  we added to this modified matrix a 1 and a 1 fraction  respectively  of the difference between the original and the modified matrix  adding the whole difference would give the graph for the original matrix .
1 unrelated terms
　in the previous section  we defined an ideal notion of perfectly related terms  found a very characteristic shape of their curves of relatedness scores  and verified that related terms from real collections do actually have curves of a similar shape. it remains to argue that unrelated terms give rise to curves of a  measurably  different kind.
　for that consider the co-occurrence graph of a collection  which is an unweighted undirected graph that has a vertex for each term in the collection  and that has an edge between two vertices if and only if these two terms co-occur in at least one document  that is  the corresponding entry in the term-document matrix is non-zero .
　definition 1. two terms are called completely unrelated in a collection  if and only there is no path from the one term to the other in the co-occurrence graph of that collection.
　lemma 1. the curve of relatedness scores for two completely unrelated terms is all zero. a small perturbation of the underlying term-document matrix affects that curve in a way that can be bounded exactly as stated in lemma 1.
　proof. the first part of the lemma is an easy consequence of theorem 1 in . the second part is implied by lemma 1. 
　indeed  the curves which we observe for pairs of intuitively unrelated terms  the vast majority of all term pairs  on real collections are of that kind: they vary around zero  change their direction many times and look zig-zagged. an example was given in figure 1 c .
1. dimensionless algorithms
　one consequence of our findings in the last section is a surprising answer to one of the fundamental questions of spectral retrieval  as to what the appropriate choice of dimension  number of concepts  is. we can say that every choice is inappropriate for a significant fraction of the term pairs. figure 1 gives an example to illustrate this point.

figure 1: no single fixed dimension can do justice to both term pairs.
　both term pairs are intuitively strongly related  singular and plural of the same word  and both curves have the shape characteristic for related terms. however  the first curve reaches its peak at a very small k  where the relatedness scores of the second curve are still very low  but for larger k  when the scores of the second curve become large  the scores of the first curve actually become negative. we therefore cannot find a single fixed k such that both term pairs get the high relatedness score which they would deserve.
　another problem of any method that  implicitly or explicitly  works with relatedness scores at a fixed dimension is illustrated by an example in figure 1. with respect to the underlying collection  still the cs abstracts   the two terms on the right are not related  which indeed shows in the shape of the curve of relatedness scores. for a significant and interesting range of dimensions  however  that curve reaches scores of the same order as those reached by curves for strongly related terms  like that on the left hand side of figure 1.

	k	k
figure 1: for many fixed dimensions these term pairs look indistinguishable from each other  although the different shapes of the two curves clearly tell them apart.
　but our findings from the previous section also suggest a way to overcome these two problems. algorithms for spectral retrieval should not build on relatedness scores computed for a fixed dimension  but instead assess the overall shape of the curve of relatedness scores for each term pair.
1 algorithms tn
　the following three steps describe one particularly simple such algorithm  where u denotes the matrix of the left singular vectors of the given term-document matrix a.
1. normalize the rows of a to length 1  compute the svd of this normalized matrix  and compute the rank r of the number 1 in the sorted list of the singular values.
1. for each pair of terms i j compute the size zij of the set  that is  the number of dimensions at which the relatedness scores are at or below zero  until the potentially earliest dimension of fall-off.
1. perform document expansion1 with the 1 matrix t  where tij = 1 if and only if zij = 1  that is  if and only if the corresponding curve of relatedness scores is never negative before the potentially earliest dimension of fall-off.
let us quickly verify that this algorithm is in accordance with our theoretical findings from section 1. by lemma 1  we have that for all perfectly related terms zij = 1 and hence tij = 1. by lemma 1  completely unrelated terms have all-zero curves  in which case zij = r   1  and hence tij = 1. by lemma 1  these assignments to t are invariant under small perturbations of the underlying term-document matrix.1

1 algorithm ts
　as pointed out already in the paragraph following figure 1  curves of type  a  and  b  could also be distinguished from those of type  c  by the smoothness of the curves. this idea gives rise to the following alternative algorithm.
1. compute the same matrix u as for tn.
1. for each pair of terms i j  compute the smoothness sij of their curve as  where max and min are the maximum and minimum score of that curve  respectively. observe that sij is 1 if and only if the scores go only up or only down  and that zig-zags push sij towards zero.
1. perform document expansion1 with the 1 matrix t  where tij = 1 if and only if sij − s  for some threshold s. for our experiments we set s such that 1% of the entries in t are 1.
in contrast to tn  ts has a parameter: the smoothness threshold s. this can be seen as an advantage as well as a disadvantage. the disadvantage is that we have to find a good value for this parameter. the advantage is that it is a very intuitive parameter: it specifies how many related terms we want to consider. this is in sharp contrast to the choice of dimension in previous methods  which  as we pointed out already in the introduction  has no intuitive or natural setting.
1 computational complexity
　the computational complexity of our two dimensionless algorithms tn and ts is essentially that of basic lsi. all three require the computation of a singular value decomposition up to a certain dimension k. after that  lsi needs to map the term-document matrix to the k-dimensional latent space  which takes o k ， nz  basic numerical operations  where nz is the number of nonzero entries in the termdocument matrix. the construction of tn/ts requires o k  operations per term pair  and actually expanding the termdocument matrix requires o l，nz  operations  where l is the average number of related terms of a term  giving a total of o k ， m1 + l ， nz  operation. we can save on the m1 by discarding pairs of terms that do not co-occur in at least one document  because  in practice  tn and ts never assign a 1 to such term pairs.
1. experimental evaluation
　we tested our algorithms on three test collections: the small time collection   1 〜 1   the significantly larger reuters collection   1 〜 1   and the still larger ohsumed collection   1 〜 1 ; in parentheses  the dimensions of the respective term-document matrices are given. in all cases we used stemming  porter  and removed common stop-words as well as words that occurred in less than a certain number of documents. we measured average precision for 1 queries for time  1 queries for reuters  and 1 queries for ohsumed. for time and ohsumed  the available relevance rankings were used. reuters comes only with topic labels  and we synthesized a query from each topic by taking the most significant terms of a random sample of documents from that topic  just as done  for example  in .
　as competitors of our algorithms we chose four major spectral retrieval schemes from the literature: the basic latent semantic indexing scheme  lsi  from   the termnormalized variant  lsi-rn  proposed in   the correlation method  corr  from   and iterative residual rescaling  irr  from . these are among the most well-known variants of lsi and moreover  each of the three fundamental types of variation of spectral retrieval  discussed in the next section  is covered by this selection.
　as a baseline method we took ranking by plain cosine similarity  cos . the required singular vectors and values were computed from a standard tf.idf matrix for cos  lsi  irr  and lsi-rn  and from a row-normalized matrix for corr and our tn and ts  because that is what the theory of the latter methods asks for  note that row-normalization undoes tf.idf normalization . all computations were done in matlab.
1 results of the experiments
　on all three collections  our two dimensionless algorithms consistently outperformed their four competitors. moreover  the performance of those four varied significantly between different collections and different choices of dimension  while ts and tn consistently gave very similar results  although the numbers they are based on are mathematically quite different  for each term pair  the number of times its curve becomes negative versus the curve's smoothness . this further adds to the evidence built up in the previous sections that spectral retrieval works by identifying pairs of related terms and that the overall shape of the curve of relatedness scores is a more reliable indicator for term-pair relatedness than the score at any fixed dimension can be.

time
cos1%1%lsi1%1%lsi-rn1%1%corr1%1%irr1%1%
tn1%ts1%figure 1: average precision figures for the time collection. for the four variants of lsi  figures are given for dimension 1 and 1. the right figure shows the averaged precision-recall graphs of tn versus irr at its best dimension.
reutersohsumedcos1%1%1%1%lsi1%1%1%1%lsi-rn1%1%1%1%corr1%1%1%1%tn1%1%ts1%1%figure 1: average precision figures for the reuters and ohsumed collections. for the three variants of lsi  figures are for dimensions 1 and 1 for reuters and dimensions 1 and 1 for ohsumed. for tn and ts  the relatedness scores were computed up to dimension 1 for reuters and up to dimension 1 for ohsumed.
　we remark that dimension 1 is close to the optimal dimension for each of the four lsi variants in figure 1. in figure 1  irr does not appear because it is computationally too expensive for collections of this size; indeed  all experiments from  were done on less than a thousand documents. note that the low average precisions of around 1% for ohsumed are actually quite substantial given that on average 1 documents from over 1 were relevant for each query. to make the svd computation for the ohsumed collection feasible  we kept only the 1 most significant terms. for the lsi-style methods  all other terms were simply discarded. for tn and ts  we computed the expansion matrices for the restricted number of terms  but then used it to actually expand the originally matrix  which is possible only by the sparseness of our expansion matrices  another advantage of our approach .
1 binary versus fractional relatedness
　one surprising aspect of our experimental findings is that algorithms which do a simple binary classification into related and unrelated term pairs outperform schemes which seem to have additional power by giving a fractional assessment for each term pair.
　an intuitive explanation is that while in principle it is of course reasonable to deem some term pairs more related than others  it is plausible that deducing such fine distinctions from mere co-occurrence data can add more noise than precision.
　a more formal explanation comes from the histogram in figure 1  which illustrates how many curves have scores at or below zero at how many dimensions  before the earliest possible dimension of fall-off according to our theory . the main observation here is that most curves are at or below zero at either very few dimensions  and the vast majority of these never touches zero  or at quite a lot of dimensions. in fact  this conforms well with our theoretical findings: if all term pairs were either perfectly related or completely unrelated  we would have non-zero counts only at both endpoints of the histogram  and an infinitesimal perturbation would move the count for the unrelated terms to the middle of the histogram .

figure 1: histogram of the number of dimensions where the curves of relatedness scores are at or below zero  for reuters .
1. the many variations of lsi
　our experimental results confirmed the observation of several previous authors that the improvements entailed by the various variations of lsi are not consistent over collections of different sizes and types. as the authors of a recent sigir publication remark   to the best of our knowledge   so far  no study systematically evaluated these fundamental choices 
.
　in this section we will characterize how our curves of relatedness scores are affected by three fundamental types of variations found in the literature: rescaling of the terms or documents  scaling by the singular values  and iterative residual rescaling. this covers  in particular  the selection listed in .
　the bottom line of this section will be that on the one hand  relatedness scores for individual dimensions can indeed change a lot  which reflects the ability of each of these variations to boost retrieval quality. on the other hand  we will see that the characteristic of the shape of the curves which we found indicative of term relatedness remains basically unaffected by any of these normalizations. this is further evidence that the shapes we identified lie at the bottom of what makes spectral retrieval work in practice.
1 rescaling of the terms or documents
　many authors have normalized the rows or columns of the term-document matrix prior to computing the singular vectors. for the various methods listed in   for example  the rows of the term-document matrix are first centered and then normalized to length 1. the following lemma says that our definition of perfectly related terms is invariant under any combination of such normalizations.
　lemma 1. let a be a matrix of the form as in definition 1. then a remains a matrix of the same form after normalizing the rows or columns with respect to an arbitrary norm  and after centering of the rows or columns  or any combination of these.
　proof. it suffices to observe that the entries in the two rows of the perfectly related terms synonyms are the same up to permutation  hence their norms and means are equal   and that the same applies to columns j and n1 +j of a  for j = 1 ... n1  where n1 is the number of documents where the  say  first term occurs without the second. 
1 rescaling of the singular vectors
　after the matrix uk of the top k singular vectors has been computed  there is an option of also rescaling the rows or columns of that uk  or both. a widely used type of rescaling here is to multiply the columns of uk by the corresponding singular values raised to some power κ. for example  the original work on lsi  sets κ =  1  while  and  advocate κ = 1. the most frequently used setting is κ = 1.
　the effect of κ on our curves can be characterized as follows. since the singular values are sorted in descending order  positive values of κ will stretch the curves towards the beginning but shrink it towards the end. for negative values of κ  the opposite happens  which  given the shape of the curve for related terms  seems less favorable for lsi and its variants. indeed  the inventors of lsi used κ =  1 only in their very first work   while they used κ = 1 in all of their many follow-up works.
　the overall shape of the curves remains basically unchanged by a change of κ  however  and no particular choice of κ can therefore overcome the inherent problems illustrated by figures 1 and 1. this shows in the inconsistent results reported in   as well as in our experimental results from section 1. husbands et al.  observed that basic lsi gives undue preference to frequent terms and suggested to normalize the rows of uk  after scaling its columns by the singular values . the results were again inconclusive  however. in   the influence of the size of the collection on this phenomenon was discussed  however without a theoretical underpinning and also without a conclusive answer. from the point of view of our work  row-normalization of u simply brings all curves to the same order of magnitude  which partially overcomes the problem addressed in figure 1 but does not address the problem addressed in figure 1.
1 iterative residual rescaling
　ando and lee   proposed iterative residual rescaling  irr  to obtain  from an m〜n term-document matrix a  a sequence of pairwise orthogonal m-vectors u1 u1 ...  where u1 is just the top left singular vector of a  u1 is the top left singular vector of a column-normalized version of a u1ut1 a  and so on. the rationale of this method is to balance the bad effects when the assumed underlying concepts are present in the collection in widely different proportions.
　due to the pairwise orthogonality of the u1 u1 ...  irr can be viewed as document expansion just like any of the other spectral retrieval methods. it is also not hard to see that two perfectly related terms  according to definition 1  lead to the special singular vector from lemma 1  and hence to the characteristic shape of the curve from figure 1. the main observation needed is that if u is any vector with identical entries at the indices corresponding to the two perfectly related terms  then for any matrix a  of the form of definition 1  a    uuta  will again be of the same form. the problems illustrated in figures 1 and 1 therefore affect irr as much as they do affect any of the other methods.
1. conclusions and outlook
　we have introduced the curves of relatedness scores as a new angle of looking at retrieval schemes based on spectral analysis. we have given strong evidence  both theoretical and experimental  that identifying related terms on the basis of the shape of these curves is at the heart of what makes spectral retrieval work in practice.
　our new dimensionless algorithms do not only outperform previous schemes that work with relatedness scores computed from individual dimensions  but they are also more intuitive: they do nothing but identify pairs of related terms  in a 1 fashion   and expand documents via these relations. note that this immediately gives a thesaurus-like functionality  which could be used  for example  in an interactive scenario to prompt a user with a list of possible synonyms for each word of her query  possibly ranked by the smoothness scores computed by our algorithm ts .
　our view of spectral retrieval as a document expansion also makes it straightforward to incorporate external knowledge into the retrieval process. in   first ad-hoc steps have been taken in that direction. our findings appear to be a good foundation for a more principled approach.
　all spectral retrieval schemes so far  including our new algorithms  give rise to a symmetric document expansion matrix t  but word pairs like  nucleic  and  acid  actually call for an asymmetric such matrix: whenever there is  nucleic  there is  acid   but not vice versa. this is actually a frequent phenomenon and we would expect algorithms which are able to consider this asymmetry to give a further boost to the effectiveness of spectral retrieval.
