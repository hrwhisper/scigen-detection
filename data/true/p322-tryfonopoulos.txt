we study the problem of offering publish/subscribe functionality on top of structured overlay networks using data models and languages from ir. we show how to achieve this by extending the distributed hash table chord and present a detailed experimental evaluation of our proposals.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval-information filtering; c.1  computercommunication networks : distributed systems-distributed applications
general terms
algorithms  performance
keywords
publish/subscribe protocols distributed hash tables p1p overlay networks
1. introduction
　we are interested in the problem of distributed resource sharing in wide-area networks such as the internet and the web. in the architecture that we envision resources are annotated using attribute-value pairs  where value is of type text  and queried using constructs from information retrieval models. there are two kinds of basic functionality that we expect this architecture to offer: information retrieval  ir  and publish/subscribe  pub/sub . in an ir scenario a user poses a query  e.g.   i am interested in papers on bio-informatics   and the system returns a list of pointers
 this work was supported in part by project evergrow. christos tryfonopoulos is partially supported by a ph.d. fellowship from the program heraclitus of the greek ministry of education.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
to matching resources. in a pub/sub scenario  also known as information filtering  if   a user posts a subscription  or profile or continuous query  to the system to receive notifications whenever certain events of interest take place  e.g.  when a paper on bio-informatics becomes available .
　in this paper we concentrate on the latter kind of functionality  pub/sub  and show how to provide it by extending the distributed hash table chord . distributed hash tables  dhts  are the second generation structured p1p overlay networks devised as a remedy for the known limitations of earlier p1p networks such as napster and gnutella.
　we assume that publications and subscriptions will be expressed using a well-understood attribute-value model called awps in . awps is based on named attributes with value free text interpreted under the boolean and vector space models  vsm . the query language of awps allows boolean combinations of comparisons a op v  where a is an attribute  v is a text value and op is one of the operators  equals    contains  or  similar    equals  and  contains  are boolean operators and  similar  is interpreted using the vsm or lsi model .
　the contributions of this paper are the following. we present a set of protocols  collectively called dhtrie  that extend the chord protocols with pub/sub functionality assuming that publications and subscriptions are expressed in the model awps.
　we evaluate dhtrie experimentally in a distributed digital library scenario with hundreds of thousands of nodes and millions of user profiles. our experiments show that the dhtrie protocols are scalable: the number of messages it takes to publish a document and notify interested subscribers remains almost constant as the network grows. moreover  the increase in message traffic shows little sensitivity to increase in document size. we demonstrate that simple data structures with only local information can make a big difference in a dht environment: the routing table fcache manages to reduce network traffic by a factor of 1 in all the alternative methods we have studied.
　since probability distributions associated with publication and query elements are expected to be skewed in typical pub/sub scenarios  achieving a balanced load is an important problem. we briefly study an important case of load balancing for dhtrie and present a new algorithm which is also applicable to the standard dht look-up problem.
　the organization of the paper is as follows. section 1 positions our paper with respect to related work. section 1 introduces the model awps while section 1 presents the dhtrie protocols. section 1 presents the experimental evaluation of dhtrie. section 1 studies the problem of load balancing. finally  section 1 concludes the paper.
1. related work
　the problems of ir and if in p1p networks have recently received considerable attention. here we only discuss the papers that are more relevant to our work. in  the authors study the problem of content-based retrieval in distributed digital libraries focusing on resource selection and document retrieval. they propose to use a 1-level hierarchical p1p network where digital libraries  called leaf nodes  cluster around directory nodes that form an unstructured p1p network in the second level of the hierarchy. in a more recent paper  the authors define the concept of neighborhood in hierarchical p1p networks and use this concept to devise a method for hub selection and ranking. the planetp  system uses an unstructured p1p network where nodes propagate bloom filter summaries of their indices to the network using a gossiping algorithm. each peer uses a variation of tf/idf to decide what nodes to contact to answer a query. in psearch  the authors propose to use the can dht and document semantic vectors  computed using lsi  to efficiently distribute document indices in a p1p network. in pirs  the authors use an unstructured p1p network and careful propagation of metadata information to be able to answer queries in highly dynamic environments.
　early work on if includes sift  1  1  which uses the boolean and vector space models  and inroute  which is based on inference networks. both of these systems are centralised although some issues related to distribution have been studied in sift . recently  a new generation of if systems has tried to address the limitations imposed by centralized approaches by relying on ideas from p1p networks. the system p1p-diet   that builds on the earlier proposal dias   is a retrieval and filtering system that uses the model awps and is implemented as an unstructured p1p network with routing techniques based on shortest paths and minimum-weight spanning trees. pfilter  is the closest system to the ideas presented in this paper. it uses a hierarchical extension of can  to filter unstructured documents and relies on multi-cast trees to notify subscribers. vsm and lsi can be used to match documents to user queries. by comparing pfilter with the proposals of this paper  we can see that we have a more expressive data model and query language  and do not need to maintain multi-cast trees to notify subscribers. however  the multi-cast trees of pfilter take into account network distance something that we do not consider at all in this paper. we also consider load balancing issues that are not studied in pfilter. finally  regarding routing  it would be interesting to compare experimentally a hierarchical extension of our work with the hierarchical routing protocols of pfilter.
1. the data model awps
　we will use a well-understood attribute-value model  called awps in . a  resource  publication is a set of attributevalue pairs  a s   where a is a named attribute  s is a text value and all attributes are distinct. the following is an example of a publication:
{  author  john smith   
 title  information dissemination in p1p ...     abstract  in this paper we show that ...   }
　the query language of awps offers equality  containment and similarity operators on attribute values. the containment operator is interpreted under the boolean model and enables the expression of boolean and word-proximity queries. the similarity operator is defined as the cosine of the angle of two vectors corresponding to text values from a publication and a query. vector representations of text values can be computed as usual using the vsm or lsi models  but only the vsm model has been used in our implementation and experiments .
　formally  a query is a conjunction of atomic queries of the form a = s  a w wp or a ゛k s  where a is an attribute  s is a text value  wp is a conjunction of words and proximity formulas with only words as subformulas  and k is a similarity threshold i.e.  a real number in the interval  1 . thus  queries can have two parts: a part interpreted under the boolean model and a part interpreted under the vsm or lsi model. the following is an example of a query:
 author =  john smith   …
 title w p1p …  information   1  alert   …
 abstract ゛1  p1p architectures have been...  
　this query requests resources that have john smith as their author  and their title contains the word p1p and a word pattern where the word information is immediately followed by the word alert. additionally  the resources should have an abstract similar to the text value  p1p architectures have been ...  with similarity greater than 1.
1. the dhtrie protocols
　we implement pub/sub functionality by a set of protocols called the dhtrie protocols  from the words dht and trie . the dhtrie protocols use two levels of indexing to store queries submitted by clients. the first level corresponds to the partitioning of the global query index to different nodes using dhts as the underlying infrastructure. each node is responsible for a fraction of the submitted user queries through a mapping of attribute values to node identifiers. the dht infrastructure is used to define the mapping scheme and also manages the routing of messages between different nodes. the set of protocols that regulate node interactions are described in the next sections.
　the second level of our indexing mechanism is managed locally by each node and is used for indexing the user queries the node is responsible for. in this level  each node uses a hash table to index all the atomic queries contained in a complex query by using their attribute name as the key. for each atomic boolean query the hash table points to a trie-like structure that exploits common words and a hash table that indexes text values in equalities as in . additionally for atomic vsm queries an inverted index for the most  significant  query words is used as in .
　vsm relies on term frequencies  tf   and inverse document frequencies  idf   to compute the vector representation of a text value. the computation of idf in an ir or pub/sub scenario needs global statistical information.  has shown that in ir scenarios it is enough to have an approximation of the exact idf values. in  each peer uses a set of randomly chosen peers to collect such statistics and merge the results to create an approximation of the global idf values. these statistics are updated periodically using sampling. it is an open problem how to achieve this in a pub/sub scenario  although the ideas of  1  1  are relevant in this context as well. we are currently working on this problem and expect to report our results in a future paper.

figure 1: an example of a lookup operation over a chord ring with m=1
　in this paper we will focus on the first level of indexing and the protocols that regulate node interactions. the local indexing algorithms we use and their experimental evaluation are thoroughly discussed in  1  1 .
1 mapping keys to nodes
　we use an extension of the chord dht  to implement our network. chord uses a variation of consistent hashing to map keys to nodes. in the consistent hashing scheme each node and data item is assigned a m-bit identifier where m should be large enough to diminish the possibility of different items hashing to the same identifier  a cryptographic hash function such as sha-1 is used . the identifier of a node can be computed by hashing its ip address. for data items  we first decide a key and then hash it to obtain an identifier. for example  in a file-sharing application the name of the file can be the key  this is an application-specific decision . identifiers are ordered in an identifier circle  ring  modulo 1m i.e.  from 1 to 1m   1. figure 1 shows an example of an identifier circle with 1 identifiers  m = 1  and 1 nodes.
　keys are mapped to nodes in the identifier circle as follows. let h be the consistent hash function used. key k is assigned to the first node which is equal or follows h k  clockwise in the identifier space. this node is called the successor node of identifier h k  and is denoted by successor h k  . we will often say that this node is responsible for key k. for example in the network shown in figure 1  a key with identifier 1 would be stored at node n1. in fact node n1 would be responsible for all keys with identifiers in the interval  1 .
　if each node knows its successor  a query for locating the node responsible for a key k can always be answered in o n  steps where n is the number of nodes in the network. to improve this bound  chord maintains at each node a routing table  called the finger table  with at most m entries. each entry i in the finger table of node n  points to the first node s on the identifier circle that succeeds identifier h n + 1i 1. these nodes  i.e.  successor h n +1i 1  for 1 ＋ i ＋ m  are called the fingers of node n. since fingers point at repeatedly doubling distances away from n  they can speedup search for locating the node responsible for a key k. if the finger tables have size o logn   then finding a successor of a node n can be done in o logn  steps with high probability . in  the details of the chord protocols for node joins and leaves  stabilisation and fault-tolerance are provided. in the rest of this section we show how to extend chord to implement our pub/sub functionality.
1 the subscription protocol
　let us assume that a node p wants to submit a query q containing both boolean and vsm parts of the form:
　　a1 = s1 … ... … am = sm … am+1 w wpm+1 … ... … an w wpn … an+1 ゛an+1 sn+1 … ... … ak ゛ak sk
to do so  p randomly selects a single word w contained in any of the text values s1 ... sm or word patterns wpm+1 ... wpn and computes h w  to obtain the identifier of the node that will be responsible for query q. then p creates message fwdquery id p  ip p  qid q  q   where qid q  is a unique query identifier assigned to q by p and ip p  is the ip address of p. this message is then forwarded in o logn  steps to the node with identifier h w  using the routing infrastructure of the dht. this forwarding is done using the dht lookup function to locate successor h w    which is then directly contacted by p. notice also that both id p  and ip p  need to be sent to the node that will store the query to facilitate notification delivery.
　when p wants to submit a query q  i.e.  with a vsm part only  of the form an+1 ゛a1 s1 … ... … an ゛an sn  it sends q to all nodes in the list l = {h wj  : wj （ d1 “ ，，，“dn}  where d1 ... dn are the sets of distinct words in text values s1 ... sn. in contrast to queries with a boolean part  queries with a vsm part only need to be stored in all the nodes computed as above in order to ensure correctness in the filtering process. sending the same message to more than one recipients is discussed in detail in the next section  where publication forwarding poses the same problem.
　when a node p1 receives a message fwdquery containing q  it stores q using the second level of our indexing mechanism. p1 uses a hash table to index all the atomic queries of q  using as key the attributes a1 ... ak. to index each atomic query  three different data structures are also used:  i  a hash table for text values s1 ... sm   ii  a trielike structure that exploits common words in word patterns wpm+1 ... wpn  and  iii  an inverted index for the most  significant  words in text values sn+1 ... sk. p1 utilises these data structures at filtering time to find quickly all queries q that match an incoming publication p. this is done using an algorithm that combines algorithms bestfittrie  and sqi .
1 the publication protocol
　when a node p wants to publish a resource  it first constructs a publication p = { a1 s1   a1 s1  ...  an sn }  the resource description . let d1 ... dn be the sets of distinct words in s1 ... sn. then publication p is propagated to all nodes with identifiers in the list
l = {h wj  : wj （ d1 “ ，，， “ dn}.
the subscription protocol guarantees that l is a superset of the set of identifiers responsible for queries that match p.
　the propagation of publication p in the dht proceeds as follows. p removes duplicates from l and sorts it in ascending order clockwise starting from id p . this way we obtain at most as many identifiers as the distinct words in d1“，，，“ dn  since a node may be responsible for more than one of the words contained in the document. having obtained l  p creates a message fwdresource id p  pid p  p l   where pid p  is a unique metadata identifier assigned to p by p  and sends it to node with identifier equal to head l   the first element of l . this forwarding is done by the following recursive method: message fwdresource is sent to a node p1  where id p1  is the greatest identifier contained in the finger table of p  for which id p1  ＋ head l  holds.
　upon reception of a message fwdresource by a node p  head l  is checked. if id p    head l  then p just forwards the message as described in the previous paragraph. if id p  − head l  then p makes a copy of the message  since this means that p is one of the intended recipients contained in list l  in other words p is responsible for key head l  . subsequently the publication part of this message is matched with the node's local query database using the algorithm mentioned in section 1 and the appropriate subscribers are notified. additionally list l is modified to l1 in the following way. p deletes all elements of l that are smaller than id p  starting from head l   since all these elements have p as their intended recipient. in the new list l1 that results from these deletions we have that id p    head l1 . this happens because in the general case l may contain more than one node identifiers that are managed by p  these identifiers are all located in ascending order at the beginning of l . finally  p forwards the message to node with identifier head l1 .
　the publication protocol essentially involves sending the same message to a group of other nodes  namely those that are responsible for the distinct words contained in the text values of the different attributes of p. the obvious way to handle this over chord is to create h different lookup messages  where h is the number of different nodes to be contacted  and then locate the recipients of the message in an iterative fashion using o hlogn  messages. we have also implemented this algorithm for comparison purposes.
　once all matching queries have been retrieved from the database of a node p  notifications are sent to the appropriate nodes using their ip address associated with the query they submitted. if a node is not online at that time the notification message is sent to its successor. to utilise the network in a more efficient way  notifications can also be batched and sent to the subscribers when traffic is expected to be low.
1 frequency cache
　in this section we introduce an additional routing table that is maintained in each node. this table  called frequency cache  fcache   is used to reduce the cost of publishing a resource. using the protocols described earlier  each node is responsible for handling queries that contain a specific word. when a resource r with h distinct words is published at node p  p needs to contact at most h other nodes to match the incoming resource against their local query databases. this procedure costs o hlogn  messages for each resource published at p. since some of the words will be used more often at published resources  it is useful to store the ip addresses of the nodes that are responsible for queries containing these words. this allows p to reach in a single hop the nodes that are contacted more often.
　fcache is a hash table used to associate each word that appears in a published document with a node ip address. fcache uses a word w as a key  and each fcache entry is a data structure that holds an ip address. thus  whenever p needs to contact another node p1 that is responsible for queries containing w  it searches its fcache. if fcache contains an entry for w  p can directly contact p1 using the ip stored in its fcache. if w is not contained in fcache  p uses the standard dht lookup protocol to locate p1 and stores contact information in fcache for further reference. using fcache the cost of processing a published resource p is reduced to o v + h v logn   where v is the number of words of p contained in fcache.
　fcache entries are populated as follows. each time a resource p is published at a node p  p contacts the nodes responsible for storing queries with words contained in p  as we described in section 1. after this process is over  p knows the contact information  namely the ip address  of those nodes  and stores it to fcache along with the word each node is responsible for. after that  for each publication taking place at p  p maintains this routing information for the most frequent words contained in resources published to it. notice that the construction and maintenance of fcache comes at no extra message cost and node routing information is discovered only when needed. in the experiments presented in the next section we discuss good choices for fcache size  section 1 .
　the extra cost involved with fcache is possible cache misses because of network dynamicity. in an fcache miss the node needs to utilise the routing infrastructure at the cost of o logn  messages to locate a node. however the new contact information is used to update the fcache entry for future reference. misses are most likely to occur for infrequent words  since nodes responsible for storing queries with frequent words will be contacted repeatedly.
1. experimental evaluation
　for our experiments we use 1 documents downloaded from citeseer and used in . the documents are research papers in the area of neural networks and we will refer to them as the nn corpus. because no database of queries was available to us  our queries are synthetically generated by exploiting 1 documents of the corpus. the remaining 1 documents are used to generate publications.
　each query q has two parts:  i  a boolean part which consists of at most 1 conjuncts that are atomic boolean queries of the form a w wp  where wp is a conjunction of at most 1 words or proximity formulas  and  ii  a vsm part which consists of at most 1 conjuncts of the form a ゛k s  where s is a text value. each atomic boolean query of the form a w wp is generated using the methodology of . we set a to be title  authors  abstract or body with some probability. then we set wp to a conjunction of words or proximity formulas obtained from technical terms mined from the document corpus. each atomic vsm query of the form a ゛k s is generated as follows. we set a to be title  abstract of body with some probability. then we choose randomly a corpus document and set s equal to the title  abstract or some part of the body field depending of our earlier choice of a. finally  we set k to a value between  1 1  using the uniform distribution.
　we have implemented and experimented with four variations of the dhtrie protocols. the first one  named it  utilises the iterative method in the publication protocol and does not use fcache. this algorithm was implemented mainly for comparison reasons. the second algorithm  named itc  utilises again the iterative method and also an fcache  and is intended to show the effect of fcache when using the iterative method in the publication protocol. the third algorithm  named re  utilises the recursive method in the publication protocol but does not use the fcache. finally 

figure 1: performance for various network sizes

figure 1: total document processing cost
rec uses the recursive method and fcache and shows a significant improvement in performance compared to the rest of the algorithms.
　to carry out each experiment described in this section  we execute the following steps. initially the network is set up by assigning keys to nodes. these keys are calculated using the sha-1 cryptographic hash function and randomly created ip addresses and ports. after the network is set up  we create 1m user queries and distribute them among the nodes using the protocol described in section 1. once the queries are stored  we publish the corpus documents at different nodes and record the network activity.
1 varying the size of the network
　the first experiment that we conducted to evaluate our protocols targeted the performance of the algorithms under different network sizes. in this experiment we randomly selected 1 documents  with 1 words average size  from the nn corpus and used them as incoming publications by randomly assigning each one to a publisher node. in each one of the 1 different runs  each document was assigned to a different node. having published the documents  we recorded the total number of dhtrie messages generated by the network in order to match these documents against the posted user queries.
　in figure 1 the performance of the different algorithms in terms of dhtrie messages per document is shown. the main observation is that the number of messages generated by all the different algorithms to match the incoming documents against the user queries  remains relatively unaffected by the network size  mainly due to the routing infrastructure used. thus  a document needs in the worst case 1% more messages for a 1 times larger network  see for example it for 1k and 1k nodes .
　a second observation emerging from the graphs in figure 1 is the effectiveness of the fcache independently of the message routing algorithm used. the use of fcache  with 1k entries  results in the reduction of messages sent using the routing infrastructure by more than 1 times in all cases  by using either the iterative or the recursive method . notice also that using algorithm rec reduces the dhtrie message cost of publishing a document of about 1 words to only 1 messages for a network consisting of 1k nodes managing to process both boolean and vsm queries.
　finally in figure 1 we present the total cost for processing a single document in terms of message traffic for networks of 1k and 1k nodes. by total cost we mean the messages sent using information from fcache plus the messages sent using the dhtrie infrastructure. for algorithm itc and a network of 1k nodes the dhtrie messages were about 1% of the total messages sent  whereas for a network of 1k nodes they were about 1%. on the other hand  for algorithm rec the dhtrie messages were around 1% for both network sizes. the above observations show the importance of the recursive method and fcache in the reduction of the total document matching cost and in the relief in terms of messages of the dhtrie routing infrastructure.
1 varying the fcache size
　the second experiment targeted the performance of the algorithms under different fcache sizes and studied the effect of fcache in the dhtrie utilisation. we used the document corpus as the training set for populating the fcache of the different nodes. we randomly selected a node p and published 1k documents to it. these publications resulted in the population of its fcache with the ip addresses of the nodes that are responsible for the most frequent words contained in the published documents  and served as a training set for the fcache. we then published 1 documents to p and limited the size of fcache to different values. subsequently  we recorded the total number of messages generated by the network in order to match these documents against the stored user queries. figure 1 shows the utilisation of the overlay network in messages per document as the size of fcache grows. the values shown are averaged over 1 runs with different nodes.
　as it is shown in figure 1  left y-axis   the number of messages sent using the dhtrie routing infrastructure reduces quickly as the size of fcache increases to reach a state where the effect of an fcache increase causes no significant change in the number of messages  aroung 1k entries  the rightmost point in x-axis . notice that the cost for each node to maintain an fcache consists only in storing this information in its local data store  namely about 1 bytes per entry  for storing the hash value of the word and the ip address of the node responsible for this word . additionally the routing information of the fcache of node p depends only on the documents that get published to p  causing no additional

figure 1: performance for different fcache size
maintenance messages. the only extra cost involved with fcache is its update cost as nodes come and go from the network. this causes fcache entries to be outdated  costing more extra messages through the routing infrastructure to publish a resource. these extra messages though are sent only once  since the fcache field is updated when the new node responsible for the word with an outdated entry is located. from some initial measurements we found out that when 1% of the fcache entries are outdated  the message cost increase was no more than 1% showing that fcache is able to cope up with misses. notice also that in the recursive method  algorithm rec  the performance of fcache in different network sizes remains constant  whereas in the case of the iterative method  algorithm itc  the performance deteriorates  1% more dhtrie messages per document for an 1% increase .
　the right y-axis of figure 1 shows the utilisation of fcache per document  showing again that after a threshold value  in our example around 1k entries  the rightmost point in xaxis  its effect is significantly reduced. this is also the reason that we chose 1k fcache entries as a baseline value for the rest of our experiments. we also observe that the number of messages sent using the fcache is about the same for both itc and rec  showing that fcache is equally utilised despite the algorithms used. for readability reasons we did not put the results for the 1k nodes network in the second y-axis  but they are similar to those presented.
1 effect of fcache training
　in this experiment we measure the effect of fcache training to the message cost imposed to the network by the publication of a single document. we randomly selected a node p and trained p's fcache with a varying number of documents. through this process the node was able to collect statistics about the most frequent words used in documents  published to it   and as a result it was able to populate its fcache with the appropriate pointers to frequently contacted nodes. thus  for an fcache with 1k entries  the baseline value used in the experiments   the node would know the ip addresses for the nodes responsible for the 1k top most frequent words. we then published 1 documents  with 1 words average document size  at p and recorded the message cost to match these documents against the stored user queries. the results shown in figure 1 are

# of documents  x1  published per node 
figure 1: different levels of fcache training
averaged over 1 runs for different nodes to eliminate network topology effects.
　figure 1 shows that the performance of the different algorithms improves as more documents get published. algorithm rec seems less sensitive in this parameter  as the difference in the number of messages observed is about 1 messages for 1 times more documents  the leftmost and rightmost point in the x-axis   whereas itc presents a difference of more than 1 messages. additionally  rec shows less sensitivity with respect to the network size  contrary to itc that needs about 1% more messages. finally  both itc and rec show a similar behaviour for the two network sizes we tested.
　the right y-axis of figure 1 shows the number of hits of fcache for different levels of fcache training. notice that both algorithms have roughly the same number of hits for a network of 1k nodes  showing that fcache hits are not affected by the algorithm used. for readability reasons we did not put the results for the 1k nodes network in the second y-axis  but they are similar to those presented. looking at the scale in the right y-axis  we can also see that the number of fcache hits shows only a slight improvement of around 1% for a 1% increase in the number of documents used for training. this is attributed to the skewed nature of the data  documents  used to train the fcache. it is however important to note that even a small increase in fcache hits can significantly reduce message load  as it is already shown in the graphs of figure 1   since every fcache hit  saves us from o logn  dhtrie messages.
1 varying the document size
　document  i.e.  publication  size is an important parameter in the performance of our algorithms. this experiment targeted the performance of the different algorithms for varying document sizes. each one of the bars in figure 1 is an averaging of 1 documents  published at 1 different nodes  in a network of 1k nodes in total  to normalise network topology effects. figure 1 shows the message cost for publishing documents of varying size by using each one of the four different algorithms. notice that the graph is truncated to a maximum of 1 messages to show clearly the best performing algorithms.
　figure 1 shows that for small documents the use of the recursive method  contrary to fcache  does not improve

average document size  words  
figure 1: performance for different document size
performance significantly  since algorithms itc and rec perform similarly. this is because for a large proportion of the words contained in small documents an fcache entry exists  thus needing a single message to reach the node responsible for queries that contain these words. the remaining words that are not listed in node's fcache use the dhtrie infrastructure  but their number is so small that we cannot observe significant differences in message cost. for large documents though  the use of the recursive method and fcache are shown to be significantly better than their counterparts  managing to process documents of average size of around 1k words by using around 1 messages.
　finally another interesting observation emerging from figure 1 is the increase rate for the different algorithms. the increase in message cost is linear to the document size with algorithm rec presenting the smaller increase rate  thus showing a smaller sensitivity to document size.
1. load balancing
　in typical ir scenarios the probability distributions associated with documents and queries can be arbitrary and are typically skewed. for example  the frequency of occurrence of words in a document collection follows the zipf distribution  subscriptions to an electronic journal might refer mostly to current hot topics while publications appearing in the same journal might reflect its established tradition etc. thus  a key issue that arises when trying to partition the query space among the different nodes of a dht in a pub/sub scenario is to achieve load balancing. in any pub/sub setting we can distinguish three types of node load: query  routing and filtering.
　the query load of a node p is the number of queries stored at node p. the routing load of a node p is the number of messages that p has to forward due to the dhtrie protocols. finally  the filtering load of a node p is the number of filtering requests  i.e.  publications  that need to be processed at node p. filtering is arguably the heaviest of the above tasks  since for each filtering request  a node has to search its local data store  retrieve the matching queries and notify interested subscribers. for this reason we choose to concentrate on balancing filtering load in this section. the filtering load of a node p depends on the number of words that hash to the interval of the identifier space owned by p and the frequency distribution of these words in published documents.
　in the dht literature  work on load balancing has recently concentrated on two particular problems: addressspace load balancing and item load balancing. the former problem is how to partition the address-space of a dht  evenly  among keys; it is typically solved by relying on consistent hashing and constructions such as virtual servers  or potential nodes . in the latter problem  we have to balance load in the presence of data items with arbitrary load distributions  1  1  as in our case.
　we have implemented and evaluated a simple algorithm for distributing the filtering load evenly throughout the different nodes of the network. the algorithm is based on the well-known concept of load-shedding  where an overloaded node attempts to off-load work  i.e.  filtering requests  to less loaded nodes. the algorithm is in fact applicable to the standard dht look-up problem but here we utilize it in a pub/sub scenario. we will present a detailed analysis of the more general algorithm in a forthcoming paper.
　the load balancing algorithm is as follows. once a node p understands that it has become overloaded  it chooses the most frequent word w it is responsible for and a small integer k1. then p contacts the nodes responsible for words wj for all j  1 ＋ j ＋ k  wj is the concatenation of strings w and j  and asks them to be its replicas. then p notifies the rest of the network about this change in responsibilities1. we each node m that receives this message notes down the word w. later on  if m has a new publication containing w  it breaks the filtering responsibility for w among p and k other nodes by concatenating a random number from 1 to k to the end of the w and using dhtrie to find the node responsible for this word. in this way the filtering responsibility of w for p is reduced by k + 1 times  k new nodes plus p . we call k + 1 the split factor  sf  in subsequent experiments.
　in the experiments carried out in this section  a node p considers itself overloaded if it exceeds the threshold  t  of 1 filtering requests for the same word w in a time window of 1 document publications  in other words  if at least 1% of the published documents contain w . in a real network a node would not know how to define such a time window. in this case it could use sampling to estimate the average document publication rate  and thus be able to discover if it is doing more filtering work than other nodes.
　the results of our experiments for the load balancing algorithm are shown in figures 1 and 1. figure 1 shows the average number of filtering requests received by each node in a time window for a period of 1 time windows. sf was set to 1 nodes and t was set to 1 requests/time window respectively. for readability purposes only the first 1k nodes  out of a total of 1k  are shown and the y-axis is truncated to 1 filtering requests  the highest point in the unbalanced case is 1 filtering requests . notice that prior to the load balancing algorithm the first 1k nodes get a very large proportion of the filtering requests  whereas the rest of the network receives very few or no requests at all. on the contrary  after the load balancing algorithm is run  only a small amount of nodes receive more that 1 filtering requests  with the rest of the filtering load being distributed

1 
	1 	1 1 1 1 1 
ranked peers 
figure 1: average number for filtering requests

1 
	1 	1 1 1 1 1 
ranked peers 
figure 1: routing load for the first 1k nodes
in a more uniform way among the nodes. we also experimented with different values for sf  1 and 1 nodes  and the t  1 and 1 requests/time window  but we did not observe significant differences in the load distribution.
　figure 1 shows the price we pay to achieve filtering load balancing in terms of message routing. in this graph we show the number of routing requests received by the first 1k nodes of our network. notice that the number of messages needed per document increases significantly after the load balancing algorithm is run  we observed increases of as much as 1% . this increase is due to fcache misses occurring from the splitting of queries and filtering responsibilities. the increase in fcache misses causes a significant increase in dhtrie messages as it was expected  see section 1   which is reduced when the fcache entries are updated. the important point however in figure 1 is that the new load imposed on the network is uniformly distributed among the nodes and does not cause overloading in any group of nodes. the new load distribution follows closely the old one. this observation leads us to the conclusion that the load balancing algorithm shown here manages to efficiently distribute the filtering load among the nodes  while imposing a relatively small additional cost for routing purposes which in our scenario is considered an easier task to perform.
1. summary and outlook
　the evaluation of the dhtrie protocols revealed strengths and weaknesses of the different algorithms developed. in our experiments we showed that the dhtrie protocols are scalable: the number of messages it takes to publish a document remains almost constant as the network grows. additionally we showed that the use of data structures that exploit local knowledge can significantly reduce network traffic  up to a factor of 1   with little overhead in training. network traffic also presents little sensitivity to document size when fcache is utilised. finally section 1 presents a load balancing algorithm that trades message traffic for balance in the peer load. distributed ir as studied in this paper can benefit from techniques of traditional ir  distributed systems and networking  especially p1p networks   databases and distributed ai. with this perspective in mind our current work concentrates on two open problems: algorithms for computing idf in distributed ir and pub/sub environments  section 1   and implementing the load balancing algorithm of section 1 for the standard dht look-up problem and comparing it with related work.
