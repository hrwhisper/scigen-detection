advances in hardware  storage  devices  connectivity  and web technology are changing the way applications are designed  deployed  and managed. applications are increasingly becoming data-centric and data is everywhere  and in all tiers  from the client to the cloud . data across multiple tiers requires data access and management capabilities across these tiers. 
the microsoft data platform presents a vision for an end-to-end data platform that offers data services across all tiers. 
 
categories and subject descriptors:  
h.1  database management   h.1  information storage and retrieval   h.1  information systems applications  
general terms: design  management 
keywords: client  cloud  data platform  device  model-
centric data and services  synchronization  tiers 
 
1. introduction 
most business  consumer  and entertainment media is now born and delivered in digital form. dramatic advances in storage technology allow us to capture and store this data relatively inexpensively. managing this data - organizing and accessing it however is becoming more of a challenge. the advances in processors  memory  storage  and connectivity have paved the way for next-generation applications that are data-driven  whose data can reside anywhere  i.e. on the server  desktop  devices  embedded in applications  and that support access from anywhere  i.e. local  remote  over the network  from devices  in connected and disconnected mode . this naturally leads to distributed  multitiered  and composite application architectures. application components can run on different tiers  in different service boundaries  and on different platforms  e.g. server  desktop  devices . for example  in a typical web application  most of the application resides on a server supporting a large number of users; however  some client components of the application may run on desktops  mobile devices  and web browsers. the advances in connectivity and cheap storage combined with the complexity of software management are making way for on-line services and software-as-a-service. in such services models  applications  and their data  are hosted in central data centers  also sometimes referred to as the 'cloud'  and are accessible and shared over the web.  
as mentioned above  data and applications can reside in different tiers with different semantics and access patterns. for example  data in back-end servers/clusters or in the cloud tends to be authoritative; data on the wire is message-oriented; data in the mid-tier is either cached data for performance or application session data; data on the devices could be local data or data offlined from back-end sources. the data access and management needs in these tiers are different from what commercial dbmss support. to allow cheap fault-tolerant and self-managing clusters  databases need to be built on very inexpensive  commodity hardware. in addition  in a world where hundreds of gigabytes of storage is the norm  the ability to work with most data in memory and easily shift from tables to trees to graphs of objects is the key to programmer productivity for next generation applications.  
in this paper  we describe the microsoft data platform vision to address data and application needs across all tiers - devices  desktops  servers/clusters  and the cloud. we describe the trends and the challenges that drive the data platform vision followed by the microsoft data platform overview. 
the rest of this paper is organized as follows. section 1 provides a description of the trends that define the new environment for datacentric applications. section 1 describes the microsoft data platform. section 1 provides a summary of this paper.  
1. the new environment  
the vision for the microsoft data platform is driven by mega trends in hardware  storage  devices  and in web applications. in this section we highlight some of the compelling trends that have significant influence on the thinking for the microsoft data platform. 
1 hardware trends 
1-bit cpus and 1-bit oss 
with the maturity of 1-bit hardware  1-bit cpus are becoming the norm for client and server machines. true 1-bit architectures support 1-bit cpus  1-bit data or address buses  virtual addressability etc  and dramatically increase memory limits  to  bytes . operating systems  e.g. windows  linux  have also 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod'1  june 1  1  beijing  china. 
copyright 1 acm 1-1-1/1...$1. 
been upgraded to support and take advantage of 1 bit addressspace and large memories.  
multi-core cpus 
next generation cpus are going to be multi-core processors. dual-cores are already very common; quad-cores are expected to be common by 1; and by 1  cpus with tens of cores  if not hundreds  are likely to be the norm. 
main memory sizes are growing... 
with the costs of memory going down  configurable memory size on desktop and server machines is increasing. for example  desktops can be configured with 1gb ram  and servers can be configured with up to 1 tb of ram. large ram does provide significant performance benefits to applications. while very large memory configurations  1s gbs  are possible  they are still prohibitively expensive compared to equivalent disk capacities. however   
nv rams  flash etc. 
non-volatile memories are becoming more common - the canonical example being flash. flash memory is already being used instead of disks in many scenarios  cell phones and other mobile devices . given flash memory's attractive read-write characteristics  power consumption  and their decreasing cost  we believe it will become a key storage media.  
1 storage trends 
low cost commodity storage 
the storage technology advances have caused significant increase in densities and decrease in cost. large capacity disks have become a commodity. terabyte  1 tb  disk units are available for  $1; 1 tb disk units are expected in 1 years and the unit costs are expected to go below $1. however  these commodity disks are considered less reliable.  
commercial databases have traditionally depended on high cost scsi disks for high performance and reliability. as commodity disks gain more widespread usage  database software must evolve to produce low cost  reliable and scalable solutions from off-theshelf components  
cost differential between raw and delivered storage 
the operational costs required to manage data  whether in a database or a file system  vastly exceeds the cost of the hardware required to store it. furthermore  there is a significant difference in hardware costs between  large scale enterprise class  hardware and  off the shelf  high volume  industry standard components.  
blurring structured and unstructured worlds 
new technologies like xml  especially in the document management world  are blurring the line between the worlds of structured and unstructured data. these allow for modeling and management of complex items such as an insurance claim that contains structured data  date  location  policy  etc.  and unstructured data  a sketch of the incident and photographs . 
 
1 device and client trends 
more capable devices 
devices are improving dramatically. tomorrow's devices are likely to be as powerful and as capable as desktops of today. devices  with all the different form factors  will become the environment of choice for most applications. 
mobility 
as more users adopt wi-fi enabled laptops  increasingly capable pdas  and smartphones  the need for mobile applications is increasing. applications like email  calendaring  crm  customer relationship management  already target mobile devices. middleware infrastructures like application servers and workflow services are becoming mobile-aware. some reasons for such mobility trends are: 
  more employees are becoming mobile 
  mobile usage is broadening and becoming mainstream. it is already prevalent in certain vertical domains like healthcare  insurance  and field services.  
  email and offline access is becoming pervasive 
  mobile applications are more than just browser windows - more and more applications now run natively on mobile devices. 
data access and management on mobile devices is central to mobile applications. as these applications grow more disconnected and sophisticated  data sizes increase and the need for rich data processing capabilities increases. mobile and embedded dbmss - which are needed to support such 
applications - become an important part of the it infrastructure 
streaming 
in certain scenarios  data is simply streamed intelligently through application logic. there is no additional need to store the data - except transiently  for the duration of the operation. conventional dbmss  however  require data to be first loaded into the database; the operation is then performed  and the data may be later removed from the database. all of this adds significant complexity to the application  and dramatically reduces its performance and throughput. 
consider  for example  rfid data in applications. rfid tags are portable sensors that communicate over specialized protocols with rfid reader devices. an increasing number of applications have begun to utilize rfid technology. in these applications  rfid readers  devices  generate events when they identify rfid tags. the rfid event streams are filtered  aggregated  transformed  and correlated so that the events can be monitored in real-time. the event processing is data-centric and typically requires an inmemory rule engine and a query processor for real-time processing. 
occasionally connected  
distributed and occasionally connected  or disconnected  application architectures fundamentally change the way applications access and manage data. instead of locking data in a central location  data is moved closer to the applications  and this enables data processing to be performed in an autonomous and efficient fashion. these applications may run in the mid-tier  on desktops  or on mobile devices. such an environment is inherently disconnected - there is no need for continuous connectivity between the data sources. data may be fetched from its original sources  using data synchronization mechanisms  transformed and cached  or stored  close to the applications. offline email is most pervasive applications that most of us are familiar with; applications like aggregated catalog is another example where product information across multiple backend applications and data sources is aggregated and cached decoupled  actually occasionally connected to get changes  from its original data sources. operations on these offline caches are data-centric and often require rich search  query  and other database capabilities. 
1 data and applications trends 
data everywhere 
as described earlier  data can reside in different tiers  in different service boundaries  with different semantics. for example  data in back-end data sources tends to be authoritative; data on the wire is message-oriented; data in mid-tier is cached or application session data.  refers to such data as resource-oriented request response data and activity-oriented reference data. resource data is authoritative data - it is transactionally consistent and up to date; reference data is typically a cached copy  in some other service boundary  of resource data. reference data may be stale; however  it is consistent enough for many applications  e.g. product catalog data cached as reference data . 
data proximity 
different forms of data have different processing requirements. for example  activity data  e.g. shopping cart  and reference data  e.g. catalog data  can be cached close to the application and have minimal update requirements. in fact  in most applications  such data is either created  session data  or cached in memory by the application for performance and throughput reasons  within the application process. in-memory dbms capabilities like query and transactions are required on the cached data. also  on devices  typically the applications are simple and single user apps; they require data to be embedded in the application.  
desktop applications like access require query and expression evaluation capabilities.  
embedded 
many applications perform simple to moderate manipulation of data. they need a way of storing  retrieving and manipulating the data within the application. these applications themselves are not complex in nature  and are designed to meet a specific user need. typically  these applications are developed by application vendors specializing in industry verticals  domains  - e.g. healthcare  finance etc. these vendors are domain experts but not database specialists. they would rather not spend their time in database installation  deployment  and management. their focus is the application and everything is embedded within the application. the databases are typically single application databases; the applications do not share  or coordinate  their database with others. these applications could run on devices  tablets  desktops  or servers. the ideal way of deployment is deploying the database components along with the application as a single install.  
enterprise application and data services have moved beyond the traditional relational database model 
ten years ago  the services provided by relational database systems such as: insert  delete  query and load were defined in terms of the logical relational model itself - namely relations  tables  and tuples  rows . client/server applications developed a decade ago were also constructed by operating directly over the relational model. the code behind the  new order  form knew to start a new transaction and perform a query to get the customer id; insert a new row in the  orderheaders  table  and then insert a series of rows in the  orderdetails  table. committing the transaction brought the new order to life.  a case of function literally following form...  see figure 1 for an illustration. 

 
figure 1: traditional client/server application pattern 
today's database services as well as current and emerging application patterns are no longer directly built upon the relational model. we'll include several examples to briefly illustrate the shift.  
sql server 1 introduced a feature known as  merge 
replication  that allowed an application to update rows in a sql table configured in a replication relationship with another server and then merge the changes back into the publishing table. once shipped  customers ran into logical consistency errors. using the same  new order  example from above  imagine a salesman used an application that created a new order with 1 line items  and thus 1 new rows in the orderdetails table . now assume the salesman is to synchronize the local changes with the publishing orderheaders and orderdetails table and during the operation  replication is interupted after the orderheader row is inserted and 1 of the 1 orderdetails rows have been transferred. the orderprocessing logic  when invoked  will process an incomplete order. this problem was fixed in sql server 1 with the addition of a new replication feature known as  logical records  in which metadata describing the logical transfer units is described in the sql catalog and the replication subsystem has been enhanced to only process complete logical records. merge replication now represents a sql server data service that is defined over a logical  entity  rather than the underlying storage representation in a relational schema.  
sql server 1 adds several other new data services over higher level entities such as the end user reporting tool:  report builder . analysts want to author reports directly over customers and orders rather than expressing joins over the 1 tables that may represent customers and orders. analysts would also want to build information in terms of customers and orders instead of having to express the underlying joins to create the cube. see figure 1. 

 
figure 1: entity based database services 
the movement towards assembling the data associated with a logical business  entity  has been underway for some time. application designers have been creating  business objects  for some time by constructing these entities with database queries and then adding other services  such as business behavior on top. emerging information products  such as enterprise information integration  eii  systems  enterprise search and business data catalogs are services based upon higher level logical entities such as customers  salespeople  and fixedassets as well1. 
1 services trends 
data in the cloud 
with the increased connectivity and low cost storage  more data is being stored in web  especially in the consumer markets. the explosion of on-line search  e.g. google  msn   collaboration  e.g. yahoo   and shopping  e.g. amazon  ebay   since the early 1s  is driving a shift in how users want to store and share their information. the convenience of browser-based access to data from anywhere and at any time is causing more data that was traditionally stored on desktops  to migrate to the web  cloud . to facilitate this trend  the cloud tier has to provide reliable storage at very low cost to the users; the data service providers have to build the cloud infrastructure with low capital and operational expenditure. also  as users trust more data storage to the cloud  the cloud tier must provide trustworthy storage with security and privacy.  
in the cloud-centric applications  the client also plays an important role. while the thin browser based client can provide access from anywhere  the access is dependent on continuous connectivity and bandwidth. also the user experience is limited by the browser experience. the desktops have traditionally provided rich client experience but they are not suitable for the mobile  anywhere access. mobile devices with local storage  caching  can provide such anywhere access to the cloud  in a connected and disconnected fashion.  
1. the microsoft data platform 
in this section we describe the microsoft data platform. the microsoft data platform is not monolithic piece of software; instead it is collection of product components and provides mechanisms to address the changing environment  as described in the previous section.  
this section is organized as two sub sections - the key tenets that drive the microsoft data platform vision  and an architecture overview that describes the different tiers in the data platform and a product map that support these tiers.  
1 key tenets 
platform for all data 
technology trends have pushed us to a point where the data organizational constructs of traditional file systems are no longer sufficient. many customers are collecting and storing petabytes of event data such as web click streams or access control audit logs. personal digital photograph collections of tens of thousands of photographs are not uncommon. and customers want to search  query  and rate their photo collections. windows media player uses a small relational database engine  at its core to manage music.  
by all data we mean: 
  traditional structured data such as that stored in relational databases today 
  byte streams of data  audio  video  images etc.  traditionally stored in a file system today 
  semi-structured streams such as xml documents 
  massive streams of data such as web click stream  feeds from sensor networks  audit logs  etc. 
in the microsoft data platform we envision the database system and the file system to be better integrated to exploit each others' strengths. for example  the file system can store its properties in the database for queryability; and the database system can store unstructured data  blobs  in the file system for performance and functionality. currently the transactional capabilities in the windows file system  the filestream based blobs in sql server  and full-text search are on the path to enabling the platform for all data. we also envision a common architecture that provides uniform data access  e.g. search  query  and management across all formats of data. 
low cost  scalable  reliable storage 
there is a significant opportunity to use high volume off the shelf hardware and smart software to build infinitely scalable and adaptive clusters for both structured and unstructured storage. our aim is to reduce the total cost  both capital and operational expenditures  of delivered storage significantly in both enterprise and mega-scale scenarios. 
several msn  properties or affiliates  like hotmail  messenger  use combination of sql server and file system products for supporting mega-scale services at low cost. however  there are significant opportunities for the database and file systems to leverage commodity storage to deliver reliable storage at significantly lower cost.  
key goals are: 
  deliver significant improvements in acquisition and operations  maintenance and management  costs for structured and unstructured storage. 
  enable friction free addition of necessary resources such as processing and disk with zero-touch provisioning and resource balancing   including dynamic data repartitioning . 
enable reach and rich 
in 1 the volume of storage delivered in high volume off-theshelf configurations  e.g. sata and ata drives  exceeded the volume of storage delivered via enterprise interconnects  scsi and fiber channel  by a 1 margin. this provides a measure for the amount of data capacity being placed  at the edge  in consumer pcs  phones  cameras  and personal entertainment devices. 
the sql server compact edition   as part of the microsoft data platform  is targeted to provide compelling experience by allowing users to store data locally and to synchronize and access their data across all of their personal devices whether they are phones  pcs  usb flash drives  pdas  etc.  
another compelling capability of the data platform is the ability to provide a much better offline experience with zero-friction deployment web applications. simple web applications such as time and attendance  expense report generation and approval could all benefit from a rich offline data experience. it should be possible for a developer to develop a rich offline capable web application as easily as they can create an online version today. 
model-centric data and services 
data centric applications have evolved significantly over the last decade. a decade ago the most common enterprise application model was a 1 or 1 tier client/server topology where all durable state was typically stored in a single relational database and most application logic interacted with the data in terms of the relational logical schema. today's applications models and patterns are much richer involving composition and integration of applications with vastly different data models and the data is no longer typically stored in a single database. this shift presents a number of challenges and opportunities.  
one implication of the change is that platforms and applications must deal with a  dual schema  tension. part of the application must work with the relational schema and other parts of the application and platform services must deal with alternative representations such as business objects or entities. by creating a concrete model to represent more complex entities such as  customer    order   or  asset  and then re-casting many of our services such as workflow processing  web services  synchronization in terms of these entities  will allow us to cleanly move many services which must be built in the application tier today to the platform level. ultimately  we will evolve the sql server data engine towards natively understanding and processing  entities  and  entity sets  rather than today's rows and tables. 
the entity data model 
in this section we describe a conceptual model  called the entity data model  edm  that provides a higher level of abstraction closer to applications. the edm is intended for developing rich data-centric applications. it extends the classic relational model with concepts from the e-r domain. the central concepts in the edm are entities and associations. entities represent top-level items with identity  while associations are used to relate  or  describe relationships between  two or more entities. 
the edm and the associated query language  entity sql  are implemented by the ado.net entity framework . several object programming models can be easily layered on top of the edm. similarly  the edm can map to one or more dbms implementations for persistence. 
the edm and entity sql represent a richer data model and data manipulation language for a data platform and are intended to enable applications such as crm and erp  data-intensive services such as reporting  business intelligence  replication and synchronization  and data-intensive applications to model and manipulate data at a level of structure and semantics that is closer to their needs. edm details are available in . 
future versions of the edm will provide support for xml  support for open properties  uris  nested collections and other capabilities. these capabilities are becoming more and more important  especially in web applications   
it should be mentioned that many existing eii and mdm solutions create and present logical entities as described here by composing and coordinating data from multiple sources. once formed  these entities are often retained in an in-memory cache and  in some cases; updates may be made to the cache and propagated back to the appropriate system of record. the key point is the  top end  of the entity vision which includes model development and mapping between various transformations is equally viable for these eii and mdm like scenarios. 
data integration and rich bi 
we are rapidly moving from an era where most of our focus was on data management such as loading  querying  backing up  data to an era where much of the focus is on deriving value from data. as the cost of data acquisition and storage has dropped  we have moved from designing information systems to answer specific questions to a design where integration and mining of disparate data sources reveals questions and answers we never anticipated. allowing users and analysts to access  enrich  and report on the data they need to do their job  while allowing it to control the source data quality  will significantly improve productivity. today  complex bi solutions require too much up front planning and suffer from poor time-to-solution and poor return of business value. microsoft has been a leader in providing  bi to the masses   and we envision bi solutions across all tiers of the data platform in the future. 
cloud scaled data services 
today's web 1 mashups must often  scrape  results to create data for the mashup. part of the problem has been that there has been no good way to present data models and query in a consistent way for data services .  
 
a rough requirements framing for cloud data services could be: 
1. provide a reference data service with checkpoint restart and incremental update capabilities with no server side state. rss feeds are getting closer to meeting this requirement. here is an example service: a zip code to city and state reference service. a subscriber could download the entire service database to start with and then request incremental updates as appropriate. 
1. provide a means to update shared data. a typical pattern is based upon optimistic concurrency control with a variety of conflict resolution schemes. an example data service would be a shared list service  with offline synch and edit capabilities. 
as part of the microsoft data platform  we envision a data service based on the edm that allows web clients to navigate entities via http  using rest based apis  and receive results in an encoded xml or json . 
powerful management 
we must move to a model where it manages information and applications rather than servers and disks. this is a global imperative that involves many teams but it has unique requirements for data management and leads us towards a world of  intelligent data management  where policies placed on content control its management rather than the container or store in which it's placed. given the amount of intelligence being placed on inexpensive storage devices  we can imagine incredible policy driven data protection schemes. for example  removable usb media devices such as flash drives or hard disks that implement a trusted platform module could allow us to implement a policy whereby certain files stored on the device could only be read in specific authorization contexts. that is  i could copy files from my windows domain onto a flash drive but only access those files while logged in with my domain credentials or smart card.  
hosted saas  software as a service  platform 
there is a blizzard of activity around loose data integration and providing data services on the web. there are many places in the value chain where we could offer storage services from low level blob storage much like amazon's s1 to allowing third parties to host application logic which integrates with existing storage and services extending them in interesting ways. fully realizing such hosted platform requires innovations in the areas described above.  
1 architecture 
the microsoft data platform vision is to provide services for data across all tiers. we sometimes refer to such a platform as end-toend data platform. the microsoft data platform provides data management and data access capabilities everywhere. in the client-server world   everywhere  includes data services on the client and the  data  server; in the enterprise world   everywhere  includes the data  server  tier  the app server  mid  tier  and the client tier; the mobile world includes the mobile device tier also; and the next generation web  cloud  world includes data in the shared web space. figure 1 illustrates the end-to-end data platform stack.    

 
figure 1: the microsoft data platform 
the data management requirements at each of the data tiers are different: 
  cloud tier: data sizes are very large  typically at web scale. data tends to be highly partitioned; there is lots of unstructured and semi-structured data; and these require lesser degrees of transactional integrity. hands-off administration is typically a must  yet explicit control on provisioning  monitoring  and maintenance is essential; and simplicity and low cost  cheap  of application development  deployment  and management is critical.  
  data server tier: typically deals with enterprise data and enterprise scale. data tends to be mostly structured  and highly transactional; data integrity is absolutely critical. customization and manageability is important as well. while tco is an important factor  these systems tend to be somewhat expensive.  
  application server tier: typically deals with reference  or cached  data  and tends to be very application-centric - data is usually part of the application. hosting machines tend to be cheap  off-the-shelf boxes; cost and resource consumption is therefore an important factor. databases at this tier must be self-managed 
  client tier: on devices  small footprint and embedded-ness are critical. while the core relational functionality must be available  advanced functionality is not required. data sizes are not very large. zero admin  or self management  is critical for such tiers.  
on desktops  applications deal with all types of data - structured and unstructured; pim applications deal with file and structured data  and easy to manage dbmss  sql express or sql server compact edition  are more suitable; lob applications on the desktops are client-side enterprise apps and use stand-alone dbmss. however  in the low-end developer environments - tablets  devices etc. - most applications  including pim and lob applications  are more like the embedded applications described previously and require simple  embedded dbmss.  
as described above and in previous sections  data in applications and in different tiers requires different styles of dbmss  e.g. 
server dbms  mobile dbms  and embedded dbms . these different dbmss have different functional and physical characteristics  e.g. footprint  process boundaries . it is not practical to have a single dbms that satisfies all the requirements of all the applications in all the tiers. we believe it is more pragmatic to envision a small number  two or three  of dbmss  with some commonality  to address application needs across all tiers. figure 1 illustrates the different tiers and the sql products that support the data processing needs in these tiers.  
1.1 uniform application development 
while applications in different tiers require different styles of dbmss with varying levels of data management  they all prefer  require  significant uniformity in application development environment  programming and tools .often  the same application may be deployed across multiple tiers  e.g. on the devices and on the desktops  and it is highly desirable to develop once and deploy on different tiers. in addition  as application scale increases and the application moves up tiers  it must be possible to upsize the database  e.g. from sql server ce to sql server   without requiring  significant  application changes.  
support for uniform application development requires the following: 
  entity model across tiers 
* the entity model  which supports row  xml  and blob data  is desired across all the tiers 
  consistent programming model across tiers 
* same  or subset  apis; for example  ado.net  rest based apis  etc. 
* same  or subset  query and data definition language; for example  tsql  entity sql  xquery 
  same tool environment 
* it must be possible to use the same visual studio tools to develop applications against dbmss in different tiers 
* it must be possible to develop once and deploy applications in different dbmss in different tiers 
  consistent management across tiers 
* while it seems like a desirable goal  the difference in data scale across tiers makes the management experience different on different tiers. for example  on the client tier  it is highly desirable to have zero  or hands free  administration; in the cloud tier  it is necessary to have control on provisioning  monitoring etc. we believe declarative or policy-based management technology is critical for achieving consistent and yet customizable management across tiers 
 
 
1.1 synchronization across tiers 
in the distributed environment  where data is everywhere across the tiers  synchronization is a critical mechanism to move the data and keep it consistent across the tiers. synchronization manifests in the microsoft data platform in different ways. in the data server tier  the sql server transaction and merge replications are examples of synchronization mechanisms that replicate enterprise data for scalability and availability. transaction replication can be used between servers for availability purposes. merge replication is typically used in client-server scenarios where data from the server is replicated across large numbers of clients.  
synchronization mechanisms are also used to replicate data from the data server tier to the mid-tier and application server caches. data access to data server is offloaded to multiple middle-tier caches  thereby increasing the overall throughput  and scalability  of the application. in cases of mostly read data  these mid-tier caches also provide significant performance improvements for applications. it is critical that the synchronization mechanisms provide support for keeping the caches and the original data sources mutually consistent.  
as described in section 1  synchronization is critical for supporting occasionally connected and offline applications. in these scenarios  synchronization works across tiers between clients and servers  mid-tier  data server tier  cloud tier . microsoft outlook's cached mode operation is an example of such offline application. the mail user always interacts with the local outlook application; however  outlook is synchronizing its mail items with the backend exchange server in the background. whether outlook is connected to the backend exchange server or disconnected from the server  the mail user experience remains the same.  
the microsoft data platform provides comprehensive synchronization services: 
  transactional and net-change based synchronization 
  master-slave and peer-to-peer configurations 
  synchronization over different communication protocols and media - e.g. over connected networks  wireless networks  using http  etc. 
  synchronization of data at different abstractions - e.g. synchronization of files  rows  logical collection of rows  entities  etc. 
  synchronization from and to all tiers 
1.1 other services 
other services in the microsoft data platform include search  caching  notifications and subscriptions  security at different levels of abstraction  etc. 
1. summary 
this paper presented an overview of the microsoft data platform. the microsoft data platform is driven by the changing trends in hardware  software  storage  and in applications. these trends are having a huge impact on the way data-centric applications are being developed and deployed across the various data tiers. this paper presented key tenets and a high level product map to address the data processing requirements across tiers. 
