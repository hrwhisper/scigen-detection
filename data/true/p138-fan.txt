both ranking functions and user queries are very important factors affecting a search engine's performance.  prior research has looked at how to improve ad-hoc retrieval performance for existing queries while tuning the ranking function  or modify and expand user queries using a fixed ranking scheme using blind feedback. however  almost no research has looked at how to combine ranking function tuning and blind feedback together to improve ad-hoc retrieval performance.  in this paper  we look at the performance improvement for ad-hoc retrieval from a more integrated point of view by combining the merits of both techniques. in particular  we argue that the ranking function should be tuned first  using user-provided queries  before applying the blind feedback technique. the intuition is that highly-tuned ranking offers more high quality documents at the top of the hit list  thus offers a stronger baseline for blind feedback. we verify this integrated model in a large scale heterogeneous collection and the experimental results show that combining ranking function tuning and blind feedback can improve search performance by almost 1% over the baseline okapi system.   
categories and subject descriptors 
h.1  information storage and retrieval : 
information search and retrieval - query formulation  relevance feedback  retrieval models  search process  selection process 
general terms 
algorithms  measurement  performance  experimentation 
keywords 
intelligent information retrieval  blind feedback  query expansion  search engine  ranking function  genetic programming  information retrieval 
1. introduction 
text resources in digital format are quickly increasing with the rapid development of the it industry  leading to ever growing repositories of information to support not only corporate interests but also the public at large. however  many challenges face those who aim to make full use of that information. information retrieval  ir  systems or search engines are designed to address such needs.  given a user query representing a user's information need  an ir system will search through its index of documents and return an ordered list of documents according to their match with the user query.  
for years  ir researchers have conducted extensive experiments to improve the search performance of an ir system or a search engine. it is known in the ir literature that there are many factors that could affect an ir system's performance: user query specificity and accuracy  document indexing  ranking function  user's familiarity with an ir system  etc. most of the recent work on performance tuning center on query improvement through blind feedback   and ranking function tuning using machine / statistical learning techniques        .  
blind feedback is a widely used technique to improve retrieval effectiveness. in blind feedback  a small number of documents are first retrieved according to the user's query and these documents are assumed to be relevant.  words in those documents as well as words in the original query are used for relevance feedback  more details later . the expanded query will replace the original short user query and retrieve a new set of documents. generally  blind feedback can improve search performance by 1% to 1% 
.   
another line of performance improvement is to tune or optimize the ranking functions used in the document query matching process. various machine learning or statistical learning techniques  ranking fusion techniques have been used in the past for this purpose .  
since both blind feedback and ranking function tuning are effective in performance improvement  it is natural to consider the effects if we combine them together. we hypothesize that the ranking function tuning stage should be followed by the blind feedback stage. the intuition is that a well-tuned high-performing ranking function should provide more relevant documents at the 

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  
top of the search hit list  and thus enable more effective blind feedback.  
requires prior specific permission and/or a fee. 
sigir'1  july 1  1  shefield  south yorkshire   uk. 
copyright 1 acm 1-1/1...$1. 
 thus our research goal in this paper is to perform extensive large scale experiments to test our hypothesis  along with its implications  as stated in the following research questions: 
1. does blind feedback work well on fine-tuned ranking functions as compared to traditional ranking functions such as okapi bm1  
1. does the type of queries  very short vs. very long queries  have any impact on the combination approach  
1. can the ranking function discovered in combination with blind feedback extrapolate well for new unseen queries  
in order to answer these questions  we choose arranger - a genetic programming-based discovery engine  to perform the ranking function tuning. this engine has been shown to be very effective in discovering new ranking functions that perform better than many well-known ranking schemes such as okapi bm 1  pivoted tfidf  etc.  we then apply various blind feedback techniques to the newly discovered ranking functions and test the effectiveness of our combination approach. 
our paper is organized as follows. in section 1  we review related work on ranking function optimization  especially the arranger ranking discovery engine used in our experiments. we also review the related work on blind feedback techniques in this section. in section 1  we outline our two-stage combination model and give the intuition behind it. in section 1  we describe the settings of our experiments  including data collection characteristics. the experimental results on both old queries  the regression experiment  and new queries  the extrapolation experiment  are presented and discussed in section 1. section 1 concludes this paper and points out future work. 
1. background 
1 ranking function optimization 
there have been several efforts on ranking function optimization in ir literature. some of the earliest work could be traced to n. fuhr et al.    using probabilistic models as machine learning approaches. the concept of relevance description used in    is very similar to the common weighting evidence such as tf  idf  etc. in  and   the ranking function  called retrieval function in     is either a polynomial regression function   or logistic regression/loglinear function . similar ideas using logistic regression for ranking function design and optimization have also been explored in .  
mixture of experts approach  or ranking fusion  forms another front of research on ranking function optimization  in which a set of ranking functions are combined either numerically through linear combination   or simple majority vote .  
1 ranking function discovery using 
arranger 
arranger  automatic rendering of ranking functions by genetic programming  is a discovery engine developed by fan et al. .  the core of this engine is a machine learning technique called genetic programming  gp . genetic programming  an extension of genetic algorithms  ga   is an artificial intelligence technique inspired by darwin's theory of evolution.   computer programs that evolve in ways that resemble natural selection can solve complex problems even their creators do not fully understand  . genetic programming has been widely used and proved to be effective in solving optimization problems  such as financial forecasting  engineering design  data mining  and operations management . gp makes it possible to solve complex problems for which conventional methods can not find an answer easily. we next review basic knowledge about gp in order to explain how arranger works.        
in gp  a large number of individuals  called a population  are maintained at each generation.  an individual represents a tentative solution for the target problem.  all these solutions form a space  say  ¦².  individuals can be stored using complex data structures  such as a tree  a linked list  or a stack.  a tree is the most popular form to store and represent individuals.   figure 1 shows an individual representing a ranking function.  a fitness function  f  ¡¤ : ¦² ¡ú r  is also needed in genetic programming.  a fitness function takes the solution space  ¦²  as its domain and returns a real number.  hence tentative solutions  represented by individuals  can be evaluated and ordered according to their return values.  the return value of a fitness function must appropriately measure how well an individual  which represents a solution  can solve the target problem.                  

 
figure 1. a simple ranking function 
genetic programming searches for an  optimal  solution by evolving the population generation after generation.  individuals in a new generation are produced based on those in the previous one.  three genetic operators are usually used to produce the new generation.  they are reproduction  crossover  and mutation.  the reproduction operator directly copies or  in a more appropriate term  clones some individuals into the next generation.  the probability for an individual to be selected for reproduction should be proportional to its fitness.  therefore the better a solution solves the problem  the higher probability it has to enter the next generation.  while reproduction keeps the best individuals in the population  crossover and mutation introduce transformation and so provide variations to enter into the new generation.  the crossover operator randomly picks two groups of individuals  selects the best  according to the fitness  individual in each of the two groups as parent  exchanges a randomly selected gene fragment of each parent and produces two  children .  thus  a  child  may obtain the best fragments of its excellent parents and so may surpass them  providing a better solution to the problem.  since parents are selected from a  competition   good individuals are more likely to be used to generate offspring.  the mutation operator randomly changes a gene code  which could be a function or a parameter in our ranking function discovery task  of an individual.  using these genetic operators  a new generation is produced.  the new generation keeps individuals with the best fitness in the last generation and takes in  fresher air   providing creative solutions to the target problem.  better solutions are obtained either by inheriting and reorganizing old ones or by lucky mutation  simulating darwinian evolution.  as we can see  genetic programming takes a so-called stochastic search approach  intelligently  extensively  and  randomly  searching for the optimal point in the entire solution space.  it is less likely to be trapped in local optima  which is the major problem of many other search algorithms.  it provides sound solutions to many difficult problems  for which people have not found a theoretical or practical breakthrough.     
1.1 motivation for using genetic programming in ranking function discovery 
the choice of using gp for ranking function discovery was motivated by three factors: a  the large size of the search space; b  the characteristics of the objective function; and c  modeling and representation advantage.  
  first  a ranking function can essentially be represented in a tree structure  figure 1 . for our trees we have used ten terminals  table 1   four functions  trees of depth no more than ten  and real constants which can vary between 1 and 1. langdon et al.  shows that for these parameters the search space for a tree is very large and the problem is essentially a needle-in-a-haystack problem. hence other search mechanisms like random search and exhaustive search would take inordinate time . gp has been shown to perform well under such conditions.  
  second  most performance measures in ir are discrete in nature  for example  relevance . gp does not require that objective functions be continuous in nature as long as it can distinguish good solutions from bad ones .    third  a tree data structure is used in our gp implementation. we found that gp is extremely suitable for the ranking function discovery task as it can automatically combine the various weighting features to approximate a user's ranking preference. an example of representing one ranking function using a tree structure is shown in figure 1. the tree-based representation allows for ease of parsing and implementation.   
1.1 outline of our gp-based ranking function discovery system - arranger 
in this section  we give a brief introduction to the arranger engine. please refer to  for details and validation. 
a ranking function consists of three parts: variables  constants  and operations  which connect the first two parts   as shown in figure 1.  hence we need to identify all the potential variables that are used in the ranking function by arranger.  some examples for these variables are the term weighting factors shown in the first column of table 1.  the second column of table 1 gives the meaning of these variables. 
there are two different types of variables  scalar and vector.  some of these predefined variables are summaries calculated for the whole collection or a specific document  such as tf max  n  tf avg col  etc.  these variables belong to the category of scalar variable.  constants are defined to be scalar only.  the remaining variables have a vector nature  such as tf doc and tf query.  we defined that when such variables appear in a ranking function  they represent vectors  instead of single numbers.  for example  if a query has n words in it  tf doc could be represented by  x1  x 1  ...  x n    where x i   i = 1 ... n   is the term frequency  tf  of the query's ith word in the document. 
table 1.   definitions for variables used for ranking discovery by arranger 
 tf query term frequency in the document 
 vector    tf query query term frequency in the query  vector   tf max the maximum term frequency in a 
document  scalar   length document length in the number of words 
 scalar   length avg average document length in the number of words  scalar   n number of documents in the collection 
 scalar   tf avg average term frequency in the current document  scalar   tf avg col average term frequency for all the 
documents in the collection  scalar   df max col maximum document frequency for a word 
in the collection  scalar   df document frequency for the query words  vector   
based on pre-selected variables and constants  we define two types of functions  operations   single-parameter functions  denoted by ¦Ò ¡¤   and two-parameter functions  denoted by ¡ð .  single-parameter functions include log    and sqrt   .  twoparameter functions include +  -  *  /.  some functions  such as log     sqrt    and /  need to be protected  since the domain of these functions is not the whole real number space.  as a variable could be a scalar or a vector  the functions must take that into consideration.  for one-parameter functions  we define ¦Ò x  = y and ¦Ò   x1  x 1  ...  x n     =  ¦Ò  x1   ¦Ò  x 1   ...  ¦Ò  x n     where x  y and x i represent scalar variables and  x1  x 1  ...  x n   is used to represent vectors.  for two-parameter functions  we define      x ¡ð y = z   x ¡ð  x1  x 1  ...  x n   =  x ¡ðx1  x ¡ðx 1  ...  x ¡ðx n     and  x1  x 1  ...  x n    ¡ð  y1  y 1  ...  y n   =   x1¡ð y1  x 1 ¡ð y 1  ...  x n ¡ð y n    where x  y  z  x i and y i represent scalars and  x1  x 1  ...  x n    represents a vector.  following our definitions for variables and functions  when a vector variable appears in the ranking function  the final result also is a vector  but a scalar usually is needed to measure the similarity between a document and a query.  hence  we further define that the return value of a ranking function is the summation of all the elements when a vector is finally returned by that function.  by following these rules  the arranger can work on discovering ranking functions.  note that when we plug in the newly-discovered functions into our search engine  the same rules must be followed.            
 
arranger works as follows: first  the best ranking functions learned from the training set are stored and the rest are discarded.  then those functions are tested on the validation set.  according to their performance  the functions which do not have consistent performance on both data sets are screened out.  only the most robust and consistent functions are selected. they form the ranking function candidate pool.  since an appropriate stopping rule is hard to find for the genetic programming approach  overtraining is inevitable unless protecting rules are set.  by running the ranking functions on an independent holdout data set  the socalled residual collection method   however  over-trained functions are filtered out. 
1 blind feedback 
blind feedback  also called pseudo-relevance feedback or automatic query expansion  is a technique that automatically adds more terms to a user's query to enhance the performance of search engines.  it is a widely-used and effective technique  especially for very short queries.  in blind feedback  a small number of documents are first retrieved according to a user's query and these documents are assumed to be relevant.  words in those documents as well as words in the original query are sorted according to a weighting function.  an expanded query is generated by selecting some words from this list. 
two popular forms of blind feedback techniques are shown in equations  1  and  1 . as we can see from these formulas  they are essentially modified rocchio relevance feedback formula  and ide dec-hi formula   respectively. 
	qi+1 =¦Áqi +¦Â	    	 	 1  
r
	qi+1 =qi +¡Ærj 	 	 	 1  
j=1
rj is the set of retrieved documents. qi is the old query. qi+1 is the new query. 
besides the above-mentioned two most popular approaches based on the vector space model  there are also other blind feedback techniques designed based on probability or information theory. for example  a information theory based measure called relative entropy  or kullback-leibler divergence  kld   has been used to select terms for blind feedback in . other traditional probabilistic query expansion formulas such as rsv   chi  and drc  also can be modified  replacing the non-relevant documents statistics with the collection level statistics since we do not know which documents are non-relevant  for blind feedback.  uses noun phrases instead of simple single terms for automatic blind feedback in web document retrieval. 
as shown later in the experiments section  we also experiment with various blind feedback techniques and find rocchio or ide dec-hi to be the most effective ones in our settings. 
1. combining ranking tuning and feedback - an integrated model 
in this paper  we propose a two-stage integrated model for ad hoc retrieval performance improvement. as shown in figure 1  this model involves the ranking function tuning stage followed by the blind feedback stage. the ranking tuning stage uses historical user queries with relevance information to tune the ranking function. the end product of this process is a highly optimized ranking function  designed to work well for these training queries  that also may work well for new queries  our third research question .  
the intuition behind this model is that tuning the ranking function provides a stronger baseline for blind feedback. in other words  a well-tuned high-performing ranking function should provide more 

figure 1: the integrated model for performance improvement 
relevant documents at the top of the search hit list  thus a stronger baseline than a low-performing ranking function. when combined with the blind feedback  the overall performance gains by the new ranking function should be higher than the low-performing ranking function. 
the model  shown in figure 1  combines the merits of both blind feedback and ranking function tuning. recent empirical research on ranking function tuning shows that it can generally improve the ad hoc retrieval performance by 1% to 1% . similarly  blind feedback can generally improve performance by 1% to 1% . since blind feedback is supposed to work for all kinds of ranking functions  based on the feedback framework as shown in equations  1  and  1   it is safe to infer that these two techniques are essentially independent of each other. this means that the performance gain of applying both techniques is the sum of applying them individually. we test this conjecture in the experiment. 
1. experiment setting 
1 collections 
we use the document collections from the ad hoc track of trec 1  1  and 1. there are four collections in total. some statistics about these collections are shown in table 1. 
 
 
table 1. collection statistics 
 size number 	of documents average doc 
length ft '1 1 mb 1 1 words fr '1 1 mb    1 1 words fbis 1 mb  1 1 words la '1 1 mb 1 1 words  
this set of collections is quite heterogeneous in nature. while financial times  ft  and los angeles  la  times are better tagged with basically one newspaper article per document  many documents in fbis are very long  with several pieces of segments that are not related to each other. for example  fbis1 has 1 words  after parsing  and it is actually a book chapter about  tactics: company  battalion  translated from russian. similarly  the length of federal register  fr  documents is unevenly distributed. some documents just have a patent number and some have an extremely long statement of federal regulation.  
the wide variance in the set of collections affords an ideal setting for our experiments since we want to test the robustness of our model in a very heterogeneous environment. 
1 data processing 
all of our experiments were run on a two-1ghz processor dell server running the linux operating system.  we did not make use of the document structure. in the parsing process  we simply removed the non-informative content in the collection and kept only the texts in the text field. these texts were indexed into both forward index and inverted index formats for our experimental purposes after removing stop words and stemming. no phrases were used in our experiments. 
for query processing  we indexed three different versions of the topic descriptions. the first version is description queries  which are generated based on the description field only. the second  short queries  are based on the title and description fields. the third  long queries  are extracted based on all three fields  i.e.  the topic title  description  and narrative. 
1. experiments 
we conducted two separate experiments to validate our proposed integrated model. the first experiment is a regression experiment in which we tune the search engine's ranking for 1 old queries. after the tuning is done  we apply the blind feedback on the new ranking scheme on a separate residual collection  test collection  and compare performance results. this experiment is to test whether the proposed two-stage model works for the same queries in different collections.  the second experiment is an extrapolation experiment. we need to find out whether the newly discovered ranking function combined with blind feedback can work well for other unseen new queries - testing the robustness of our integrated model. in this experiment  we use the 1 queries from the latest robust track of trec 1. 
1 experiment 1: performance tuning on old queries 
for the first experiment  we give details of tuning the search engine's performance using our proposed two-stage model. this tuning is based on 1 old queries available from the ad-hoc retrieval track of trec 1  1  and 1. 
1.1 ranking tuning using arranger 
we used arranger to discover  optimal  functions on the robust track collection.  these ranking functions are trained on various versions of the user queries. we tested the automatically learned functions on three types of queries: description query  short query  and long query as described in section 1  using a residual collection method. table 1 shows the results on the residual collection.  from this table  we can see that significant improvement is achieved by replacing the okapi bm1 function with our newly-discovered functions. this proves the effectiveness of arranger ranking function tuning.     
table 1. performance comparison between okapi bm1 and ranking functions discovered by arranger on 1 queries  
 desc short long okapi bm1 
 baseline  1 1 1 rf 1 1 
 +1%  1 
 +1%  1 
 +1%  rf 1 1 
 +1%  1 
 +1%  1 
 +1%  rf 1 1 
 + 1%  1 
 +1%  1 
 +1%  rf 1 1 
 +1%  1 
 +1%  1 
 +1%   
among the four new ranking functions discovered  rf 1 performs the best. it improves upon okapi bm 1  with description queries  by more than 1%.  
1.1 blind feedback 
we apply various blind feedback techniques  based on new functions discovered by our arranger.  they are rocchio   ide dec-hi   chi   kld   rsv   and a variation of kld   which we derived based on probability theory.  those techniques are applied on both description queries and long queries.  they yield performance improvement on both types of queries.  as we expected  they improve more on description queries than long queries.  for each approach  there are several parameters to be adjusted  for example  the number of documents assumed relevant  the number of terms for the expanded query  and parameters in the weighting function.  since it would take an enormous amount of time to try out all combinations  a factorial design was used to look for the  best  parameter settings. we found that using the top 1 documents  with 1 feedback terms  gives the best overall performance. 
 
we then applied sensitivity analysis around these optimal numbers.  
figure 1 shows the number of documents effect on map  mean average precision . we used the rocchio feedback method with 1 feedback terms. as can be seen from figure 1  using the top 1 documents gives the best performance for gp function 1. however  using the top 1 documents is not ideal for the other functions. in general  using the top 1 documents gives the best performance for all gp ranking functions. therefore we chose to use the top 1 documents for feedback. 

figure 1. number of  documents effect on gp ranking function feedback. query terms are fixed at 1. 
 
in terms of the sensitivity analysis on the effect of the number of feedback terms using different weighting schemes  namely rocchio  kld and chi  figure 1 shows map at various levels of terms. number of documents is fixed at 1 and queries are description queries only. we found that rocchio feedback performed the best for our discovered ranking functions. the number of terms has very little impact on map.  the ranking function used in figure 1 is rf 1. overall  using 1 terms for query expansion seems optimal. 

figure 1. feedback methods on various feedback terms 
 
table 1 gives the performance comparisons between rf 1 and okapi using rocchio blind feedback method as shown in equation  1  on the 1 old queries. all results are reported in terms of map. only description queries and short queries results are reported here in table 1. the performance improvement results on long queries using blind feedback are not reported because of lack of statistical significance. 
as can be seen  blind feedback technique works well for both description queries and short queries  with description queries gaining the most.  
compared with the okapi baseline without blind feedback  blind feedback on okapi ranking improves by 1% for description queries  and 1% for short queries.  for rf 1 discovered by the arranger engine  blind feedback further improves the performance by 1% for description queries and 1% for short queries. blind feedback gains more in rf 1 than okapi. this clearly answers our first research question that highly-tuned ranking function helps blind feedback since they can provide more relevant documents at the top of the rank list. in other words  ranking function tuning does provide a stronger baseline and helps for blind feedback. 
overall  when combining the two stages of ranking function tuning and blind feedback  we see that we get more than 1% improvement over the okapi baseline system with no feedback for description queries. for short queries  the improvement is more than 1%. these improvements clearly prove that the combination of ranking function tuning and blind feedback is very beneficial. it also verifies our hypothesis that the effect of ranking function performance tuning is independent of the effect of blind feedback. in other words  these performance gains are additive.  the higher performance gain in description queries than short queries does indicate the query effect raised in our second research question.  
table 1. the effect of blind feedback  bf  
run no. desc short okapi  without bf 
 baseline  1 1 okapi with bf 1 
 +1%  1 
 +1%  rf 1 without bf 1 
 +1%  1 
 +1%  rf 1with bf 1 
 +1%  1 
 +1%  1 experiment 1: extrapolation test 
the purpose of this experiment is to see whether the two-stage model using the newly discovered ranking function from the previous experiment can extrapolate well for other user queries. to test this hypothesis  we test the new rf 1 again combined with blind feedback on the 1 testing queries from trec 1 robust track. the 1 queries contain 1 old very hard queries from the 1 queries from trec 1  1  1  and 1 new queries. the results for the 1 queries are shown in table 1. table 1 lists the results for the 1 new queries only. 
if we compare the results in tables 1 and 1 with those in experiment 1  we can see that the improvement of rf 1 over okapi on description queries drops from more than 1% in experiment 1 to more than 1% in experiment 1 on the 1 new queries. this is largely due to the variance of user queries. in fact  the 1 new users queries are relatively easier queries compared to previous ones. this makes the ranking function we tuned based on old 1 queries not extrapolate very well on the new 1 queries. nevertheless  the new ranking function rf 1 still has performance edge over the baseline okapi bm 1 by more than 1%.  
table 1. performance results on 1 test queries   1 old hard queries + 1 new test queries  
run no. desc long okapi  without bf 
 baseline  1 1 okapi with bf 1 
 +1%  1 
 +1%  rf 1 without bf 1 
 +1%  1 
 +1%  rf 1with bf 1 
 +1%  1 
 +1%   
table 1. performance results on 1 new test queries 
run no. desc long okapi  without bf 
 baseline  1 1 okapi with bf 1 
 +1%  1 
 +1%  rf 1 without bf 1 
 +1%  1 
 +1%  rf 1with bf 1 
 +1%  1 
 +1%   
the results in table 1 look better than those in table 1. this is not a surprise as the 1 queries contain 1 old queries that are part of the training queries in our first experiment.  the differences imply that the ranking function discovered in experiment 1 works better on the old 1 hard queries than the 1 new queries. the differences also indicate that tuning a ranking function that can work well for all queries is not a trivial task. the ranking function discovered is typically very context dependent  which is in consistence with previous ranking function optimization findings . 
another observation that can be made from the results in tables 1 and 1 is that blind feedback works consistently well. it improves okapi baseline by more than 1% for description queries and more than 1% for long queries. on the other hand  blind feedback improves on rf 1 by almost 1% and 1% for description queries and long queries  respectively. the higher performance gain of blind feedback in rf1 over okapi is in consistent with the results in experiment 1 and also proves our initial hypothesis that ranking function tuning helps blind feedback. 
the overall performance gain by doing the two-stage model is clearly beneficial. the performance gain of the two stages is almost equivalent to the sum of performance gain of doing each of the two stages individually. this point is consistent with our hypothesis raised earlier and the results in experiment 1. this answers our third research question that our proposed model still can extrapolate well for new unseen user queries. 
1.  conclusion 
in this paper  we look at the performance improvement for ad-hoc retrieval from a more integrated point of view by combining the merits of ranking function tuning and blind feedback. in particular  we argue that a ranking function should be tuned first using user provided queries before applying the blind feedback technique.  our experimental results confirm our initial hypothesis that highly-tuned ranking offers more high quality documents at the top of the hit list  and thus offers a stronger baseline for blind feedback. we also find that this integrated model works well for very short queries. the performance gain using both techniques is almost the sum of the gains by applying the individual techniques alone. our results on the 1 old queries in table 1 show that combining ranking function tuning and blind feedback can improve the search performance by almost 1% over the baseline okapi system.  this is very encouraging considering the fact that most ad hoc search queries are very short and many of these queries are repetitive. our model also extrapolates well for new users queries. 
note that in the current implementation  we use arranger  a gp-based discovery engine  to tune the ranking function. it is possible that this stage could be replaced by other ranking function tuning techniques  such as ranking fusion or logistic regression.  the effect of such implementations will be left for future research. 
we also plan to test this integrated model on more document collections such as web collections and news collections  to see whether this integrated model will work well for those contexts.   
1. acknowledgment 
 
this material is based on work supported by the national science foundation under grant number iis-1  due-1 and due-1.  
any opinions  findings and conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect the views of the national science foundation. 
we are also grateful to ye zhou  yui yang for their programming support.   
