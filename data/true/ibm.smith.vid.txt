in this paper  we describe the ibm research system for analysis  indexing  and retrieval of video  which was applied to the trec1 video retrieval benchmark. the system explores novel methods for fully-automatic content analysis  shot boundary detection  multi-modal feature extraction  statistical modeling for semantic concept detection  and speech recognition and indexing. the system supports querying based on automatically extracted features  models  and speech information. additional interactive methods for querying include multiple-example and relevance feedback searching  cluster  concept  and storyboard browsing  and iterative fusion based on user-selected aggregation and combination functions. the system was applied to all four of the tasks of the video retrieval benchmark including shot boundary detection  concept detection  concept exchange  and search. we describe the approaches for each of the tasks and discuss some of the results.
1 introduction
the growing amount of digital video is driving the need for more effective methods for indexing  searching  and retrieving video based on its content. recent advances in content analysis  feature extraction  and classification are improving capabilities for effectively searching and filtering digital video content. furthermore  the recent mpeg-1 standard promises to enable interoperable content-based retrieval by providing a rich set of standardized tools for describing features of multimedia content . however  the extraction and use of mpeg-1 descriptions and the creation of usable fully-automatic video indexing and retrieval systems remains a significant technical challenge.
모the trec video retrieval benchmark is facilitating the technical advancement of content-based retrieval of video by standardizing a benchmark video corpus along with different video retrieval and detection tasks. the benchmark provides a consistent evaluation framework for assessing progress as researchers experiment with novel video indexing techniques. this year  we participated in the trec video retrieval benchmark and submitted results for four tasks:  1  shot boundary detection   1  concept detection   1  concept exchange   1  search. we explored several diverse methods for video analysis  indexing  and retrieval  which included automatic descriptor extraction  statistical modeling  and multi-modal fusion. we conducted experiments that individually explored audio-visual and speech modalities as well as their combination in manual and interactive querying. in the paper  we describe the video indexing and retrieval system and discuss the results on the video retrieval benchmark.
1 outline
the outline is as follows: in section 1  we describe our process for video and speech indexing. in section 1  we describe the video retrieval system including methods for content-based search  model-based search  speech-based search  and other methods for interactive searching and browsing. in section 1  we discuss the approaches for each of the benchmark tasks and examine some of the results.
1 video indexing system
the video indexing system analyzes the video in an off-line process that involves video content indexing and speech indexing. the video content indexing process consists of shot boundary detection  key-frame extraction  feature extraction  region extraction  concept detection  and clustering  as shown in figure 1. the basic unit of indexing and retrieval is a video shot.

figure 1: summary of video content indexing process.
1 shot boundary detection  sbd 
shot boundary detection  sbd  is performed using the real-time ibm cuevideo system  which automatically detects shots and extracts key-frames. this year  we explored several methods for making sbd more robust to poor video quality. some of the methods include using localized edge gradient histograms and comparing pairs of frames at greater temporal distances. overall  our 1 sbd system showed reduction in errors by more than 1% compared to our 1 sbd system .
모the baseline cuevideo sbd system uses sampled  threedimensional color histograms in rgb color space to compare pairs of frames. histograms of recent frames are stored in a buffer to allow a comparison between multiple image pairs up to seven frames apart. statistics of frame differences are computed in a moving window around the processed frame and are used to compute the adaptive thresholds  shown in figure 1 as a line above the difference measures  diff1  diff1 and edge1 . a state machine is used to detect the different events  states . the sbd system does not require any sensitivity-tuning parameters. more details about the baseline system can be found in  1  1 .
 

figure 1: plot of frame-to-frame processing of the sbd algorithm. notice the ground truth  gt  and system output  sys  plots for this segment of video which has six dissolves  one missed  and twelve cuts.
모several changes were incorporated to the baseline sbd algorithm to accommodate lower video quality  as was the case for the videos in the trec-1 data set. localized edge-gradient histograms were added to overcome color errors. the 1-bin edgegradient histogram counts the number of pixels in each of eight image regions  having similar ix iy derivatives  each derivative is quantized into three bits . thus it is less sensitive to lighting and color changes. rank filtering was added in time/space/histogram at various different points along the processing to handle the new types and higher levels of noise. the comparison of pairs of frames at wider distances up to thirteen frames apart was added to overcome the high mpeg-1 compression noise. several new states were added to the state machine to detect certain types of video errors and to detect very short dissolves that were 1 frames long. these changes were tuned based on precision-recall measurements using data subsets from trec1 test set and trec1 training set.
1 feature extraction
the system extracts a number of descriptors for each video shot. some of the descriptors  as indicated below  are extracted in multiple ways from each key-frame image using different normalization strategies  see   as follows:  1  global   1  1 grid   1  1-region layout  and  1  automatically extracted regions. the following descriptors were extracted:
  color histogram  global per key-frame  1 grid  1-region layout  segmentation regions : one based on a 1-bin hsv color space  and another based on 1-bin rgb color space 
  color correlogram  global per key-frame  1 grid  1-region layout : based on a single-banded auto-correlogram coefficients extracted for 1 radii depths in 1-color hsv color space  
  edge orientation histogram  global per key-frame  1 grid  1-region layout : based on sobel filtered image and quantization to 1 angles and 1 magnitudes  
  wavelet texture  global per key-frame  1 grid  1-region layout : based on wavelet spatial-frequency energy of 1 bands using quadrature mirror filters  
  tamura texture  global per key-frame  segmentation regions : three values representing the coarseness  contrast  and directionality  respectively  
  co-occurrence texture  global per key-frame  1 grid  1region layout : based on entropy  energy  contrast  and homogeneity features extracted from gray-level co-occurrence matrices at 1 orientations  
  motion vector histogram  global per shot  segmentation regions : based on 1 뫄 1 motion estimation blocks in the mpeg-1 decoded i and p frames. a six-bin histogram is generated based on the motion vector magnitudes 
  mel-frequency cepstral coefficients  mfcc : transformation of uncompressed pcm signal to 1 mfcc features including the energy coefficient.
1 region extraction
in order to better extract local features and detect concepts  we developed a video region segmentation system that automatically extracts foreground and background regions from video. the system runs in real-time with extraction of regions from i-frames and p-frames in mpeg-1 video. the segmentation of the background scene regions uses a block-based region growing method based on color histograms  edge histograms  and directionality. the segmentation of the foreground regions uses a spiral searching technique to calculate the motion vectors of i- and p- frames. the motion features are used in region growing in the spatial domain with additional tracking constraints in the time domain. although we tested mpeg-1 compressed-domain motion vectors  we found them to be too noisy. we also found that combining motion vectors  color  edge  and texture information for extraction of foreground objects did not give significantly better results than using only motion.
1 clustering
we used the extracted visual descriptors  see section 1  to cluster the video shots into perceptually similar groups. we used a k-means clustering algorithm to generate 1 clusters. we found color correlograms to achieve an excellent balance between color and texture features. the clusters were later used to facilitate browsing and navigation for interactive retrieval  as described in section 1 .
1 concept detection
the concept detection system learns from labeled training video content to classify unknown video content  in our case  the feature test and search test data . we have investigated several different types of statistical models including support vector machines  svm   gaussian mixture models  gmm  and hidden markov models  hmm .
1.1 lexicon design
the first step in designing a semantic concept detection system is the construction of a concept lexicon . we viewed the training set video and identified the most salient frequently occurring concepts and fixed a lexicon of 1 concepts  which included the 1 concepts belonging to the trec concept detection task  denoted as primary concepts . overall  we generated training and validation data and modeled the following 1 primary concepts: outdoors  indoors  cityscape  landscape  face  people  text overlay  music  speech and monologue. we also modeled the following 1 secondary generic concepts:
  objects: person  road  building  bridge  car  train  transportation  cow  pig  dog  penguin  fish  horse  animal  tree  flower  flag  cloud 
  scenes: man made scenes  beach  mountain  greenery  sky  water  household setting  factory setting  office setting  land  farm  farm house  farm field  snow  desert  forest  canyon 
  events: parade  explosion  picnic  wedding.
1.1 annotation
in order to generate training and validation data  we manually annotated the video content using two annotation tools1 - one produced the visual annotations and the other produced audio annotations. the ibm mpeg-1 video annotation tool  a.k.a. videoannex   shown in figure 1  allows the shots in the video to be annotated using terms from an imported lexicon. the tool is compatible with mpeg-1 in that the lexicons can be imported as mpeg-1 classification schemes and generates mpeg-1 descriptions of the video based on the detected shots and annotations. the tool also allows the users to directly create and edit lexicons.

figure 1: videoannex mpeg-1 video annotation tool. the system enables semi-automatic annotation of video shots and editing of the lexicon.
모the second tool  the ibm multimodal annotation tool  provides three modes of annotation: video  audio with video  or audio without video. the audio annotation is based upon audio segments in which the user manually delimits each segment within the audio upon listening and selects from the lexicon those terms that describe the audio content. multimodal concepts  e.g. monologues  are annotated using audio with video mode of annotation.
1.1 concept modeling
semantic concept detection was investigated using a statistical classification methodology  as described in  1  1  1  . the system learns the parameters of the classifiers using training data for each concept using statistical methods. we considered two approaches: one based on a decision theoretic approach and the other based on a risk minimization approach.
decision theoretic approach in this approach  the descriptors are assumed to be independent identically distributed random variables drawn from known probability distributions with unknown deterministic parameters. for the purpose of classification  we assume that the unknown parameters are distinct under different hypotheses and can be estimated.
structural risk minimization unlike the decision theoretic approach  the discriminant approach focuses only on those characteristics of the feature set that discriminate between the two hypotheses of interest. the idea of constructing learning algorithms based on the structural risk minimization inductive principle was proposed in . in particular  we used support vector machines  svm 1  which map the feature vectors into a higher dimensional space through nonlinear function and constructing the optimal separating hyper-plane.
training and validation training and validation of models was done using the nist feature training data set. we randomly partitioned the nist feature training data set into a 1 hour feature training  ftr  collection and a 1 hour feature validation  fv  collection. we used the ftr collection to construct the models and the fv collection to select parameters and evaluate the concept detection performance. the validation process was beneficial in helping to avoid over-fitting to the ftr collection.
1.1 fusion
since no single descriptor is powerful enough to encompass all aspects of video content and separate the concept hypotheses  combining information is needed at several levels in the concept modeling and detection processes. we experimented with two distinct approaches involving early fusion and late fusion. for early fusion we experimented with fusing descriptors prior to classification. for late fusion we experimented with retaining soft decisions and fusing classifiers. in addition  we explored various combining methods and aggregation functions for late fusion of search results as described in section 1. two modeling procedures are used. they use different subsets of visual features. the first procedure utilizes both early and late fusions  while the second procedure uses only late fusion.
feature fusion the objective of feature fusion is to combine multiple features at an early stage to construct a single model. however  since this increases the dimensionality of the feature space-which makes it sparser-it also makes the classification problem harder and increases the risk of over-fitting the data. this approach is therefore most suitable for concepts that have sufficiently large number of training set examples that would allow the classifier to exploit correlations between the features. we experimented with feature fusion by simply normalizing and concatenating descriptors. different combinations of descriptors were used to construct models. we used the validation set to choose the best combination.
classifier fusion in an ideal situation  early fusion should work for all concepts  since all of the information is available to the classifier. however  practical considerations  such as limited number of training examples and the increased risk of over-fitting necessitate an alternate strategy. if the features are fairly de-correlated  then treating them independently is less of a concern. in such situations  we model concepts in each modality or feature space independently  and fuse individual classifier decisions later. we used a separate model  svm or gmm  for each descriptor  which results in multiple classifications and associated confidences for each shot depending on the descriptor. while the classifiers can be combined in many ways  we explored normalized ensemble fusion to improve overall classification performance.
1.1 specialized detectors
although we used the above generic approaches for detection of most concepts  for two concepts  monologues and text overlay  we explored specialized approaches as follows:
monologue detection for monologue detection  we first performed speech and face detection on each shot. then  for shots containing speech and face  we further evaluated the synchrony between the face and speech using mutual information and used
the combined score thus generated to rank all shots in the corpus. based on experimental results of a variety of synchrony detection techniques  we used a scheme that models audio and video features as locally gaussian distributions  see  for more details .
text overlay detection we explored two algorithms for extracted overlay text in video and fused the results of the classifiers to produce the final concept labeling. the first method  see   works by extracting and analyzing regions in a video frame. the processing stages in this system are:  1  isolating regions that may contain text characters   1  separating each character region from its surroundings and  1  verifying the presence of text by consistency analysis across multiple text blocks. a confidence measure is computed as a function of the number of characters in text objects in the frame. the second method uses macro-block-based texture and motion energy. layout analysis is used to verify the layout of these character blocks. a text region is identified if the character blocks can be aligned to form sentences or words.
1 speech recognition and indexing
as in trec-1  we constructed a speech-based system for video retrieval. significant improvements were made to both the automatic speech recognition  asr  performance and the speech search engine performance relative to our trec-1 submission.
1.1 automatic speech recognition  asr 
a series of increasingly accurate speech transcriptions for the entire corpus were produced in the period leading up to the evaluation. the first set of transcriptions were produced using an ibm real-time transcription system tuned for broadcast news; this is the same transcription system as was used in trec-1 . later transcriptions were produced using an off-line  multiple pass transcription system comprising the following stages  see  for more details and citations :
  remove silent videos
  divide each video into segments using bayesian information criteria  bic 
  detect  music  and  silence  and transcribe using an ibm 1뫄real-time broadcast news transcription system
  apply supervised maximum likelihood linear regression  mllr  adaptation of speaker-independent hub1 models using a set of eight  word-level transcribed  videos
  decode  speech-only  segments using interpolated trigram language model  lm 
  cluster  speech-only  segments into  speaker- and environment- similar  clusters
  apply unsupervised mllr adaptation of trec-1adapted hub1 models to each cluster using single global mllr mean and precision transforms
모the word error rate  wer  of the final transcripts is estimated at 1% on a held out set of six videos from search test and feature test which were manually transcribed1. this compares favorably to 1% for the best of the publicly-released transcriptions on the same set and represents a 1% improvement over the transcriptions used as the basis for ibm's trec-1 sdr system.
1.1 speech indexing
indexes were constructed for sdr from the final most accurate speech transcriptions. three types of indexes were generated: document-level indexes  an inverse word index  and a phonetic index. no attempt was made to index the set of silent videos.
document-level indexes: the document-level indexes support retrieval at the document level  where a document is defined to span a temporal segment containing at most 1 words1. consecutive documents overlap by 1 words in order to address boundary truncation effects. once documents are defined and their associated time boundaries are recorded  the documents are preprocessed using  1  tokenization to detect sentence/phrase boundaries;  1   noisy  part-of-speech tagging such as noun phrase  plural noun etc;  1  morphological analysis  which uses the partof-speech tag and a morph dictionary to reduce each word to its morph eg. verbs  lands    landing  and  land  reduce to /land/;  1   stop  words are removed using standard stop-word lists. after pre-processing  indexes are constructed and statistics  such as word and word pair term- and inverse-document frequencies  are recorded for use during retrieval.
inverse word index: the inverse word index supports boolean search by providing the  videoi timei  of all the occurrences of a query term in the videos. preprocessing of transcripts is similar to that above.
phonetic index: the phonetic index supports search of out-ofvocabulary words. the  imperfect  speech transcript is converted to a string of phones . the phonetic index can be searched for sound-like phone sequences  corresponding to out-of-vocabulary query terms such as some acronyms  names of people  places  and so forth1
1 video retrieval system
the video retrieval system provides a number of facilities for searching  which include content-based retrieval  cbr   modelbased retrieval  mbr   speech-based search or spoken document retrieval  sdr  and other interactive methods.
1 content-based retrieval  cbr 
the objective of cbr is to match example query content to target video content using the extracted descriptors  see section 1 . the degree of match is determined on basis of feature similarity  which we have measured using minkowski-form metrics considering values of r = 1  manhattan distance  and r = 1  euclidean distance  as follows: given descriptors represented as multi-dimensional feature vectors  vq and vt be the query and target vectors  respectively  then
mx 1
	drq t =  	|vq m    vt m |r .	 1 
m=1
1 model-based retrieval  mbr 
model-based search allows the user to retrieve video shots based on the concept labels produced by the models  see section 1 . in mbr  the user enters the query by typing label text  or the user selects from the label lexicon. since a confidence score is associated with each automatically assigned label  mbr ranks the shots using a distance d derived from confidence c using d = 1   c.
1 speech-based search  sdr 
speech-based search allows the user to retrieve video shots based on the speech transcript associated with the shots. we used multiple sdr systems independently and combined the results to produce the final sdr results for trec-1; we refer to the three systems as okapi-system-1  okapi-system-1  boolean-system-1. to evaluate different design decisions  a limited ground truth was created for the combined ftr and fv collections by pooling the results and performing relevance assessment.
query development and preprocessing: all sdr systems operate using a textual statement of information need. query strings are pre-processed in a similar manner to the documents: tokenization  tagging and morphing gives the final query term sequence for use in retrieval.
video segment retrieval: given a query  the three sdr systems rank documents or video segments as follows:
  okapi-system-1  okapi-system-1: a single pass approach is used to compute a relevancy score for each document. each document is ranked against a query  where the relevancy score is given by the okapi formula . the total relevancy score for the query string is the combined score of each of the query terms. the scoring function takes into account the number of times each query term occurs in the document and how rare that query term is across the entire corpus  with normalization based upon the length of the document to remove the bias towards longer documents since longer documents are more likely to have more instances of any given word.
  boolean-system-1: a boolean search was applied to boolean queries. this search also supported phonetic search of out-of-vocabulary words using the phonetic index  in conjunction with in-vocabulary words which can be located in the inverse word index.
모many sdr systems use the results of first pass retrieval as the basis for automatic query expansion scheme prior to running a second pass of retrieval. experiments showed little gain from using an lca-based scheme  on ftr+fv  since the number of relevant documents retrieved per query in the first pass is quite low  so the approach was not investigated further.
video segment-to-shot mapping: nist evaluates video retrieval performance at the level of shots  rather than at the level of documents or video segments which span one or more shots. thus we must somehow use the scores assigned to documents or video segments by sdr to assign scores at the level of shots1. the mappings used in the three component systems are:
  okapi-system-1: the score assigned to a document is assigned to the longest shot overlapping that document;
  okapi-system-1: the score assigned to a document is assigned to all the overlapping shots. a slightly higher score given to the later shots than to the first ones;
  boolean-system-1: first  the boundaries of the video segment are determined by the coverage of the relevant words. then the overlapping shots are scored the same way as with okapi-system-1.
모the video segment-to-shot mapping is critical to overall sdr performance. post-evaluation experiments show the schemes above were not optimal choices; for example  since multiple relevant shots often overlap a single document  okapi-system1 performance can be improved simply by assigning a document score to all overlapping shots. our current research is investigating more sophisticated schemes.
fusion of multiple sdr systems: analysis of the results from the different systems shows that they are often complementary on ftr+fv: no system consistently outperforms the others. thus we hypothesized fusion of scores might lead to improved overall performance. whilst various fusion schemes are possible  for trec-1 we use a simple additive weighted scheme to combine shot-level  zero-to-one range normalized scores from each of our basic sdr systems. weights can be optimized on ftr+fv prior to the final run on  held-out  search test data. this combined system is termed  sdr-fusion-system .
1 term vector search
we used term vectors constructed from the asr text for allowing similarity search based on textual content. given the entire collection of shots  we obtained a list of all of the distinct terms that appear in the asr for the collection. the order of this list was fixed to give a one-to-one mapping of distinct terms and dimensions of the vector space. each shot was then represented by an n-dimensional vector  where the value at each dimension represented the frequency of the corresponding term in each shot. this allows the comparison of two shots based on frequency of terms. we constructed several term vector representations based on asrtext.
1 browsing and navigation
the system provides several methods for browsing and navigation. for each video a story-board overview image was generated that allowed its content to be viewed at a glance. the system also generated these overview images for each cluster  see section 1  and each model  see section 1 .
1 iterative fusion
the interactive fusion methods provide a way for combining and rescoring results lists through successive search operations using different combination methods and aggregation functions defined as follows:
combination methods consider results list rk for query k and results list qr for current user-issued search  then the combination function ri+1 = fc ri qr  combines the results lists by performing set operations on list membership. we explored the following combination methods:
  intersection: retains only those items present in both results lists.
	ri+1 = ri 뫌 qr	 1 
  union: retains items present in either results list.
	ri+1 = ri 뫋 qr	 1 
aggregation functions consider scored results list rk for query k  where dk n  gives the score of item with id = n and qd n  the scored result for each item n in the current user-issued search  then the aggregation function re-scores the items using the function di+1 n  = fa di n  qd n  . we explored the following aggregation functions:
  average: takes the average of scores of prior results list and current user-search. provides  and  semantics. this can be useful for searches such as  retrieve items that are indoors and contain faces. 
		 1 
  minimum: retains lowest score from prior results list and current user-issued search. provides  or  semantics. this can be useful in searches such as  retrieve items that are outdoors or have music. 
	di+1 n  = min di n  qd n  	 1 
  maximum: retains highest score from prior results list and current user-issued search.
	di+1 n  = max di n  qd n  	 1 
  sum: takes the sum of scores of prior results list and current user-search. provides  and  semantics.
	di+1 n  = di n  + qd n 	 1 
  product: takes the product of scores of prior results list and current user-search. provides  and  semantics and better favors those matches that have low scores compared to  average .
	di+1 n  = di n  뫄 qd n 	 1 
  a: retains scores from prior results list. this can be useful in conjunction with  intersection  to prune a results list  as in searches such as  retrieve matches of beach scenes but retain only those showing faces. 
	di+1 n  = di n 	 1 
  b: retains scores from current user-issued search. this can be useful in searches similar to those above but exchanges the arguments.
	di+1 n  = qd n 	 1 
1 normalization
the normalization methods provide a user with controls to manipulate the scores of a results list. given a score dk n  for each item with id = n in results set k  the normalization methods produce the score di+1 n  = fz di n   for each item n as follows:
  invert: re-ranks the results list from bottom to top. provides  not  semantics. this can be useful for searches such as  retrieve matches that are not cityscapes. 
	di+1 n  = 1   di n 	 1 
  studentize: normalizes the scores around the mean and standard deviation. this can be useful before combining results lists.
	 	 1 
where 뷃i gives the mean and 횰 the standard deviation  respectively  over the scores di n  for results list i.
  range normalize: normalizes the scores within the range
1...1.
		 1 
1 shot expansion
the shot expansion methods allow the user to expand a results list to include for each shot its temporally adjacent neighbors. this can be useful in growing the matched shots to include a larger context surrounding the shots  as in searches such as  retrieve shots that surround those specific shots that depict beach scenes. 
1 multi-example search
multi-example search allows the user to provide or select multiple examples from a results list and issue a query that is executed as a sequence of independent searches using each of the selected items. the user can also select a descriptor for matching and an aggregation function for combining and re-scoring the results from the multiple searches. consider for each search k of k independent searches the scored result sk n  for each item n  then the final scored result qd n  for each item with id = n is obtained using a choice of the following fusion functions:
  average: provides  and  semantics. this can be useful in searches such as  retrieve matches similar to item  a  and item  b .
		 1 
  minimum: provides  or  semantics. this can be useful in searches such as  retrieve items that are similar to item  a  or item  b .
qd n  = min sk n  
k
  maximum: 1 qd n  = max sk n  
k
  sum: provides  and  semantics. 1 		 1 
  product: provides  and  semantics and better favors those items that have low scoring matches compared to  average .
y
	qd n  =	 sk n  	 1 
k
1 relevance feedback search
relevance feedback based search techniques enhance interactive search and browsing. the user's feedback on a set of shots is used to refine the search and retrieve in minimum number of iterations the desired matches. the user implicitly provides information about the matches being sought or query concept by marking whether shots are relevant or non-relevant in relation to his/her desired search output. the system utilizes this feedback to learn and refine an approximation to the user's query concept and retrieve more relevant video-clips in the next iteration.
모we use a robust relevance feedback algorithm  that utilizes non-relevant video-clips to optimally delineate the relevant region from the non-relevant one  thereby ensuring that the relevant region does not contain any non-relevant video-clips. a similarity metric estimated using the relevant video-clips is then used to rank and retrieve database video-clips in the relevant region. the partitioning of the feature space is achieved by using a piecewise linear decision surface that separates the relevant and non-relevant videoclips. each of the hyper-planes constituting the decision surface is normal to the minimum distance vector from a non-relevant point to the convex hull of the relevant points. with query concepts that can reasonably be captured using an ellipsoid in the feature space. the proposed algorithm gives a significant improvement in precision as compared to simple re-weighting and svm-based relevance feedback algorithms.
1 tasks and results
we participated four tasks: shot boundary detection  sbd   concept detection  concept exchange  and search.
1 shot boundary detection  sbd  results
for the shot boundary detection task  the results of five systems were submitted  one of which was last year's sbd system as a baseline. a large difference in performance relative to last year was anticipated due to the degraded video quality of the trec '1 data. the other four were different versions of the improved system  mainly applying different logic to the fusion of color histogram and the localized edges histogram information. three of them performed well and yielded very similar results  while the forth one did not perform as well. table 1 summarizes the evaluation of the baseline system  alm1  and the best new system  sys1  on last year's and this year's trec video data test sets. the results on trec-1 data set were computed by us  while the results for the trec-1 data set are taken from the official nist trec 1 evaluation of those systems. two additional rows are provided on trec-1 benchmark that compare our results to the best and average systems  respectively  among the 1 sbd runs submitted by trec participants.
모as anticipated  the sbd performance on trec-1 data was lower than on trec-1 data set. this was very noticeable in other participating systems as well. never-the-less  the error rates of the
videoallcutsgradualframesys.datarc	prrc	prrc	prrc	pralm1tr-1.1.1.1.1.1.1.1.1sys1tr-1.1.1.1.1.1.1.1.1alm1tr-1.1.1.1.1.1.1.1.1sys1tr-1.1.1.1.1.1.1.1.1s-1tr-1.1.1.1.1.1.1.1.1meantr-1.1.1.1.1.1.1.1.1table 1: shot boundary detection results  comparing the new system with last year system on both trec-1 and trec-1 video data test sets. if all participating systems are to be ranked by prall + rcall then system s-1 would be found the best one  provided here for comparison. system mean reflects the average of all 1 submitted systems.
new system sys1 were 1% lower than of the baseline system alm1 in almost all measures on both data sets.
1 concept detection results
overall  concept detection results were submitted for ten concept classes. the evaluation results are plotted in figure 1  which shows shows average precision measured at a fixed number of documents  1 for the feature test set . the  average  bars correspond to the performance averaged across all participants. the  best  bars correspond to the system returning the highest average precision. the  ibm  bars correspond to ibm's submitted concept detection run  priority=1 . the ibm system performed relatively well on the concept detection task giving highest average precision on 1 of the 1 concepts1.

figure 1: comparison of concept detection performance using average precision.
1 concept exchange results
apart from running the primary and secondary detectors on the search test set to assist the search task  we participated in the concept exchange task by submitting results of eight primary detectors on the search test set. we generated shot based mpeg-1 descriptions for this exercise thus permitting easy exchange of the detection results between participants.
1 search results
the search task required retrieving video shots from the search test collection for a given set of query topics. we investigated both manual and interactive methods of searching. we submitted four runs of all 1 query topics using the content-based  model-based  speech-based  and interactive search methods described above. table 1 summarizes the results for the four search runs.
systemtypecodemapcbrmanualm b m 1.1sdrmanualm b m-11cbr+sdrmanualm b m-11cbr+sdrinteractivei b m-11table 1: summary of search results for four submitted runs.
1.1 manual cbr
the manual cbr run consisted of mapping the query topics into one or more content-based or model-based queries and fusing the results in a predetermined fashion. as described in section 1  cbr was based on a variety of descriptors. the manual cbr run was generated by allowing the following operations to answer each query topic:
1. issue a content-based search by selecting one or more queryexamples  a feature type  and a fusion method  as necessary;
1. issue a model-based search by selecting one or more conceptmodels  a fusion method  and model weights  as necessary;
1. fuse results lists from one or more content-based or modelbased search by selecting a fusion method.
모for example  the following sequence of operations was executed for query 1: people spending leisurely time at the beach:
1. pick examples 1  1  1  1  1  1  1 from query content set
1. perform cbr search with edge histogram layout using  minimum  fusion  eq 1 
1. combine with  landscape  model using  intersection  combining method  eq 1  and  product  aggregation function  eq 1 .
모the exact mapping of query topics into a fixed sequence of the above operations was performed manually by visually optimizing performance over the ftr and/or fv collections without knowledge of the search test collection. once a query topic was mapped to system operations  the operations were applied to the search collection by a designated person who did not participate in the mapping process or have prior knowledge of the search test collection. figure 1 shows the results for topic 1  which is looking for shots depicting  james chandler.  as shown  some matches

figure 1: results for topic 1: james chandler.
are found in the results list  however  many shots of  james chandler  are not retrieved using cbr.
모with respect to performance  it was our experience that the trec 1 query topics were at a higher semantic level than what cbr can handle. while cbr and semantic modeling are generally able to capture low- to mid-level semantics  they are fairly limited in the case of only a few query examples or mid- to high-level semantics. we found that purely cbr worked best for refining candidate lists generated from semantically rich sources  such as speech  or explicit semantic models that closely match the query need. for example  refining the face model by cross-comparison with examples images of  james chandler  did produce a few relevant hits near the top  see figure 1 . model-based retrieval on the other hand worked well when the query topic was a close match to an existing model and was built with sufficient training data  such as the  musician  topic. however  in the case of limited example content  such as of query topic looking for  butterflies   or given a lack of closely related explicit semantic models  cbr and mbr techniques alone are not sufficient. in addition  some of the query topics were so general  e.g.  beach query  or specific  e.g.  price tower query  that it is doubtful whether any reasonable discrimination can be done using low-level features alone.
1.1 manual sdr
manual searching using spoken document retrieval  sdr  was based on the indexed speech information. we explored multiple methods of sdr and their fusion  where the sdr queries were developed through interaction with the feature training collection.
모query strings were created manually for each query. queries derived from the audio and textual statement of information need supplied by nist were expanded by hand in ad-hoc fashion based on retrieval on the ftr+fv sets1. more complicated query strings are used in the boolean system  since it was hypothesized that the boolean retrieval would be less susceptible to the effects of query over-tuning on ftr+fv.
모the query terms used in the submitted multiple-sdr fusion system for topic 1   find shots with one or more snow-covered mountain peaks or ridges. some sky must be visible behind them   were  ice snow covered mountain peaks valley vista . twenty relevant items were retrieved in the top 1  with average precision 1. for topic 1   find shots of price tower  designed by frank lloyd wright and built in bartlesville  oklahoma   the query terms are  price tower frank lloyd wright bartlesville oklahoma   the top three items recalled are relevant and average precision is 1.
모weights for the sdr-fusion-system were optimized using the limited ground truth that was compiled for ftr+fv. as expected  this scheme led to mean average precision  map  improvements ftr+fv; more importantly  fusion gave performance improvements  1%  over our best single sdr system on the unseen search test data  as shown in table 1 . note that simple postevaluation changes in the video segment-to-shot mapping scheme improved the performance of the individual okapi systems  eg. okapi-system-1 increased to map 1  and the fusion system performance might be expected to improve further as the component systems improve. the results overall are a significant improvement over those for ibm's speech-only retrieval submission to trec-1. the system was ranked second among 1 evaluated manual search results.
systemmapokapi-system-1.1okapi-system-1.1boolean-system-1.1sdr-fusion-system1table 1: search test performance of the fusion system and its three components.
1.1 manual cbr and sdr
the combination of cbr and sdr was explored for manual searching  where queries were developed through interaction with the feature training collection. an example of  successful  sdr and cbr integration is query topic 1   find overhead views of cities - downtown and suburbs; the viewpoint should be higher than the highest building visible  . in the following  we assume that the sdr results and cbr results have been found independently prior to the integrated query:
1. retrieve results for sdr query of  view panorama overheaddowntown suburbs city town urban 
1. expand results list to include adjacent shots  repeat twotimes  using expand operation  see section 1 
1. combine with cbr results using  union  combinationmethod  eq 1  and  product  aggregation function  eq 1 .
the final average precision improved from cbr 1 and sdr 1 to cbr+sdr 1. a similar approach was used for the other queries with minor differences such as the number of shot expansions and the choice of the combination method and ag-
gregation function  for example  using  intersection  rather than  union  and  sum  rather than  product . however  this approach was not always successful; for example  the same scheme was used for topic 1   price tower   sdr+cbr but performance was degraded below that obtained using sdr alone. this approach to sdr and cbr integration improved 1 of the 1 queries beyond the performance attained with sdr alone.
1.1 interactive search
we explored interactive search using cbr and sdr in which the user interacted with the search test collection at query-time  we chose various combinations of these methods and selected among different methods for fusion  multiple examples search  relevance feedback  and browsing. the wall-clock time was measured to gauge the user effort for each interactive query. the following describes the interactive search operations for query topic 1 for  butterflies   which took just over seven minutes of user time:
1. search for shots of butterflies using sdr with terms such as monarch    butterfly    wings    flower .
1. view grouping of results by video  clusters shots accordingto source video  to get idea of which videos contribute which shots
1. remove two irrelevant shots at top of results list
1. expand all shots t adjacent shots
1. results show 1 hits at the top  stop.
1 summary
we presented the ibm research video indexing system. the system explores fully-automatic content analysis methods for shot detection  multi-modal feature extraction  statistical modeling for semantic concept detection  and speech recognition and indexing. the system supports manual methods of querying based on automatically extracted features  models  and speech information. in this paper we described the system and the experiments runs that are part of the trec-1 video retrieval benchmarking effort. the results show good performance on tasks such as shot boundary detection  concept detection  and search.
모acknowledgments: we thank prof. chiou-ting hsu  national tsing-hua university  hsinchu  taiwan and her students for their assistance in annotating the feature training data sets.
