n-gram and repeating pattern based prediction rules have been used for next-web request prediction.  however  there is no rigorous study of how to select the best rule for a given observation. the longer patter may not be the best pattern  because such patters are also more rare.  in this paper  we propose several rule-pruning methods that enable us to build efficient  compact and high-quality classifiers for webrequest prediction.
keywords
web-log mining.  web request prediction
1. introduction
 in this work  we focus on web-log based prediction of future http requests. predictions on near-future user behavior can be very useful for a number of purposes. much work has been done on recommendation systems that rely on prediction models to make inferences on users' interests. many researchers have studied how to use of web-log based prediction for pre-sending or pre-fetching of web objects in anticipation of users potential requests.    an important issue is to use n-grams or frequent sequential patterns for future web request prediction.  the work in this area includes  1  1  1  1  1 .   once a prediction system is built  one can pre-send documents for clients.  albrecht et al.  used a markov model to make predictions in order to send documents ahead of time.  these predictions are shown to reduce the access latency for web users.
in this work  we view n-gram or frequent subsequence based algorithms for web-request prediction as the task of classification.   in this view  the existing work in web-request prediction suffers from the fact that there is no clear quality metric on what constitutes a good prediction rule.  previous work have used the minimum support notion for this purpose  where the minimum support of an n-gram prediction rule is a lower bound threshold on how many cases the rule has to cover in the training data set.  for example  in  the minimum support  where support of rule lhs  rhs is the probability that the pattern {lhs  rhs} holds in the training data   is set to be five occurrences of the pattern.   however  this usage is ad-hoc.  for example  in the event that two different n-grams for different n apply to a same testing case  which n-gram rule should we trust more   traditional wisdom is to pick the longer rule.  however  longer rules are supported by fewer cases in training data  and are thus less confident in the statistical sense.  as we will see  they do not always give the best result.
we propose to use a rigorous statistical metric to measure the quality of the rules  using the potential error rate derived from both the support and confidence  percent of correct predictions  factors.  the result is a  pruning method  for classifiers  which gives significant improvements over previous methods.
in this paper  in order to unify the notion of n-gram and the notion of longest repeating patterns  we use a single term  prediction rule  to denote the rules under consideration for both the n-gram rules and the longest repeating subseuences. further  we study all methods by always using a default rule  a rule which has empty lhs.  this default rule is the most popular object in the training log.  when no matching is possible  we can always use it for prediction.
1. different rule pruning strategies 1 longest match
the longest-match method chooses the rule with the longest left-hand-side  lhs  that matches a case. the rationale of longest match method is based on the conclusion that longer surfing path will contain more accurate and richer information about the user access pattern than the shorter ones .  a problem with this method is that it always prefers to rules with a longer lhs  regardless of how many cases the rule covers.  we know that although higher-order ngrams will be more specific and accurate  their support decreases exponentially with n  essentially making them less confident.
1 most confident selection
with the most confident selection  we always choose a rule with the highest confidence among all the applicable association rules. if we have a tie  we choose the longer rule. the confidence of a rule lhs rhs is the conditional probability that rhs holds given lhs.  the rationale of the most confidence selection is based on the assumption that the testing data will share the same characteristics as the training data  which we built our classifier on. so if a rule has a higher confidence in the training data  then this rule will also show a higher confidence in the testing data  which means the class predicted by that rule will be most likely to occur next.
1 pessimistic selection
a problem with the previous methods is that in most real cases  training data will not reflect exactly the same aspects of the testing data. therefore  they are prone to generating overfitting rules.  to deal with over-specific rules  which are the longest-n-grams  we choose to use statistics pessimistic error estimates -- a powerful tool in statistics.
given a training log file  we denote the number of correctly classified cases as c  the number of incorrectly classified cases as e  and the total number of cases classified by the rule as n.  then the confidence of the rule is c / n = c /  c + e  = 1 - e / n.  the pessimistic confidence of the rule is:
pessimistic confidence =  1 - ep / n = 1 - ucf   e  n   / n
where ep  is the pessimistic estimated error rate using the formula ep = ucf   e  n   / n  and ucf   e  n  is the pessimistic estimated error.
for a given observation  the pessimistic selection method picks up the rule with the highest pessimistic confidence in all the applicable rules  regardless of the length of the lhs of a rule.   we will see later that this method gives better result than the longest match method.
1 last-substring index tree  lsit 
finally  we propose a method to compile the pessimistic selection pruning method into a tree highly compact structure of the rules  enabling efficient use of cpu time and memory during run time
we introduce a tree-like structure  called the 'last-substring index tree'  lsit   to store all the rules in the prediction model as nodes in the tree  and store the relative pessimistic confidence in the relative positions of the nodes.
we say that a rule lhs1	rhs1 is a parent of
lhs1 rhs1  if lhs1 is a trailing substring of lhs1.  we build the lsit tree according to this parent child relation. furthermore  we require that the children of all rules in this tree to strictly more confident than their parents.  this allows for dramatic pruning of the trees  resulting a smaller but more efficient tree.  when applying the lsit tree to a given observed case  a sequence of objects   we trace the tree topdown  reaching a deepest node where the rules apply.  the deepest rule will be selected.
experiments have been done on a nasa data set to compare all the pruning methods as well as the individual n-gram methods with or without minimum support and minimum confidence.  . the nasa data set contains one month worth of all http requests to the nasa kennedy space center www server in florida.  the performance is measured against the precision of the classifiers obtained.   in the nasa data  we used the first 1 requests as training data set and the next 1 requests as the testing data set. in the figure  the most-confidence selection and the pessimistic selection methods under each value of n are the results of rule set with lhs equal or smaller than n.  as can be seen  the pessimistic selection pruning method  represented by the top-curve in the figure  together with the lsit classification tree  gives the best overall result.
we have also performed experiments using a number of other web logs  including one from epa.  our results from these other logs also confirm the superiority of the pessimistic pruning method.
1. conclusions
in this paper  we have presented an effective method for pruning n-gram rules and build a compact tree structure for web request prediction.  we have shown that using the longest repeating subsequences algorithm and the n-gram based algorithms may not always give the best result.  our pessimistic rule pruning methods and the associated lsit compression method predict with the highest accuracy.
1. acknowledgement
we would like to thank canadian natural science and
engineering council  nserc  and the iris for their support.
