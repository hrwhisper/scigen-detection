we present a novel sequential clustering algorithm which is motivated by the information bottleneck  ib  method. in contrast to the agglomerative ib algorithm  the new sequential  sib  approach is guaranteed to converge to a local maximum of the information  as required by the original ib principle. moreover  the time and space complexity are significantly improved. we apply this algorithm to unsupervised document classification. in our evaluation  on small and medium size corpora  the sib is found to be consistently superior to all the other clustering methods we examine  typically by a significant margin. moreover  the sib results are comparable to those obtained by a supervised naive bayes classifier. finally  we propose a simple procedure for trading cluster's recall to gain higher precision  and show how this approach can extract clusters which match the existing topics of the corpus almost perfectly.
categories and subject descriptors
i.1  pattern recognition : clustering-algorithms; i.1
 pattern recognition : applications-text processing; e.1  data : coding and information theory-data compaction and compression
general terms
algorithms  theory  performance  experimentation
1. motivation
　unsupervised document clustering is a central problem in information retrieval. possible applications includes use of clustering for improving retrieval   and for navigating and browsing large document collections  1  1  1 . several recent works suggest using clustering techniques for unsupervised document classification  1  1  1 . in this task  we
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  tampere  finland.
copyright 1 acm 1-1/1 ...$1.
are given a collection of unlabeled documents and attempt to find clusters that are highly correlated with the true topics of the documents. this practical situation is especially difficult since no labeled examples are provided for the topics  hence unsupervised methods must be employed.
　the results of  1  1  show that agglomerative clustering methods that are motivated by the information bottleneck  ib  method  perform well in this task. agglomerative procedures  however  suffer from two main obstacles. first  in general there is no guarantee to find a solution which is a local maximum of the target function. second  an agglomerative procedure is typically computationally expensive  and in fact infeasible for relatively large data sets.
　in this paper  we suggest a simple framework for casting any given agglomerative procedure into a sequential clustering algorithm. the resulting sequential algorithm is guaranteed to find a local maximum of the target function  under very mild conditions . moreover  it has time and space complexity which are significantly better than those of the agglomerative procedure. in particular we use this framework to suggest a new algorithm-the sequential information bottleneck  sib  algorithm.
　we provide theoretical justification as to why this sequential algorithm might find clusters that have high accuracy  and evaluate the algorithm on real life corpora. our results demonstrate the superiority of sib over a range of clustering methods  agglomerative  k-means  and other sequential procedures  typically by a significant margin. moreover  sib performance was comparable to that of a standard supervised naive bayes classifier trained over a significant number of labeled documents.
1. the information bottleneck method
　most clustering algorithms start either from pairwise 'distances' between points  pairwise clustering  or with a distortion measure between a data point and a class centroid  vector quantization . too often the choice of the distance or distortion function is arbitrary  sensitive to the specific representation  which may not accurately reflect the relevant structure of the high dimensional data.
　in the context of document clustering  a natural measure of similarity of two documents is the similarity between their word conditional distributions. specifically  for every document we can define
	  	 1 
where n y|x  is the number of occurrences of the word y in the document x. to avoid an undesirable bias due to different document lengths we also require a uniform prior distribution   where |x| is the number of documents in the corpus. roughly speaking  one would like documents with similar conditional word distributions to belong to the same cluster. this formulation of finding a cluster hierarchy of the members of one set  e.g.  documents   based on the similarity of their conditional distributions with respect to the members of another set  e.g.  words   was first introduced in  and was termed  distributional clustering .
　the issue of selecting and justifying the 'right' distance measure between distributions remains  however  unresolved in that earlier work. recently  tishby  pereira  and bialek  proposed a principled approach to this problem  which avoids the arbitrary choice of a distortion measure. in this approach  given the joint distribution p x y    one looks for a compact representation of x  which preserves as much information as possible about the relevant variable y . the mutual information  i x;y    between the random variables x and y is given by  e.g.   
		 1 
and is the natural statistical measure of the information that variable x contains about variable y . in  it is argued that both the compactness of the representation and the preserved relevant information are naturally measured by mutual information  hence the above principle can be formulated as a trade-off between these quantities. more formally stated  we introduce a compressed representation t of x  by defining p t | x . the compactness of the representation is now determined by i t;x   while the quality of the clusters  t  is measured by the fraction of the information they capture about y   namely  i t;y  /i x;y  . this general problem has an exact optimal formal solution without any assumption about the origin of the joint distribution p x y   . this solution is given in terms of the three distributions that characterize every cluster t （ t: the prior probability for this cluster  p t   its membership probabilities p t|x   and its distribution over the relevance variable  p y|t . in general  the membership probabilities  p t|x   are 'soft'  i.e.  every x （ x can be assigned to every t （ t with some
 normalized  probability. the information bottleneck principle determines the distortion measure between the points x and t to be the  the kullback-leibler divergence  between the conditional distributions p y|x  and p y|t . specifically  the formal solution is given by the following equations which must be solved
self-consistently  p t|x  = zp β x t    exp  βdkl p y|x |p y|t   
	p y|t  = p 1t 	x p t|x p x p y|x 	 1 
	p t  =	x p t|x p x   
where z β x  is a normalization factor  and the single positive  lagrange multiplier  parameter β determines the tradeoff between compression and precision and the  softness  of the classification. intuitively  in this procedure the information contained in x about y is 'squeezed' through a compact 'bottleneck' of clusters t  that is forced to represent the 'relevant' part in x with respect to y .
1. sequential clustering
　consider the following general scenario. we are given a set of objects x and we would like to find a partition t x  which maximize some score function f t . there is a variety of score functions that we might consider  for each we may derive specialized algorithms. one algorithm that can be applied to almost any score function is agglomerative clustering. in this approach we start with a partition of x into singletons  and at each step we greedily choose the merger of two clusters that maximizes the score. we repeat such greedy agglomeration steps until we get the desired number of clusters  which we will denote by k. agglomerative clustering is particularly attractive when the score function f is decomposable  i.e.  if t = {t1 ... tk}  then f t  = i f {ti} . in this case  the change in the total score by merging two clusters is simply df ti tj  = f {ti} “ {tj}    f {ti}    f {tj} .
　there are two main obstacles to agglomerative clustering. first  this greedy approach is not guaranteed to find the optimal partition of x into k clusters. in fact  it is not even guaranteed to find a stable solution  in the sense that each object belongs to the cluster it is most similar to. second  an agglomeration procedure have the time complexity of o |x|1|y |   where |y | is the dimension of the representation of every x  and a memory consumption of o |x|1  which makes it infeasible for large data sets.
　we describe a simple idea for solving these two problems by casting an agglomerative  known  algorithm into a new sequential clustering procedure. unlike agglomerative clustering  this procedure maintains a partition with exactly k clusters. we start from an initial random partition t = {t1 t1 ... tk} of x. at each step  we  draw  some x （ x out of its current cluster t x  and represent it as a new singleton cluster. using a greedy agglomeration step we can now merge x into tnew such that tnew = argmint（t df {x} t  
                                                   new to obtain a new partition t  with the appropriate cardinality . assuming that it is easy to verify that f tnew    f t . therefore  each such step either improves the score  or leaves the current partition unchanged. if f t  is known to be upper bounded we are guaranteed to converge to a local maximum in the sense that no more assignment changes can be performed.
　since this algorithm can get trapped in a local optima  we repeat the above procedure for random initializations of t to obtain n different solutions  from which we choose the one which maximize f t . finally  to avoid too slow convergence we define two  convergence  parameters denoted by ε and maxl. specifically we declare that the algorithm converged if we already performed maxl loops over x or if in the last loop we got less than ε ， |x| assignment changes. a pseudo-code for the algorithm is given in figure 1.
　what is the complexity of this sequential approach  in each  drawing  step we should calculate df {x} t  for every t （ t which is an order of o k|y | . our time complexity is thus bounded by o nlk|x||y |  where l is the number of loops we should perform  over x  until convergence is attained. since typically we get a significant
input:

|x| objects to be clustered
   parameters: k  n  maxl  ε output:

a partition t of x into k clusters
main loop:

for i = 1 ... n
ti ○ random partition of x.
c ○ 1  c ○ 1  done = false
while not done
for j = 1 ... |x| draw xj out of t xj 
tnew xj  = argmin
	if	  then c ○ c + 1
merge xj into tnew xj 
c ○ c + 1
ifthen
t ○ argmaxti f ti figure 1: pseudo-code for the sequential clustering algorithm.
run time improvement. additionally  we dramatically improve our memory consumption toward an order of o k1 . one clear disadvantage of this approach is in loosing the tree structure output of the agglomeration procedure.
　our sequential clustering algorithm is reminiscent of the standard k-means algorithm. the main difference  is that k-means perform parallel updates  in which first we choose for each x its new cluster  and then we move all the elements to their new clusters in one step. as a consequence  the definition of the clusters  i.e.  their centroids in k-means  changes only after all the elements move to their preferred clusters. to show that such a step is justified  we have to require more structure of the target function f. we also note here that our sequential framework has some relations to the incremental variant of the em algorithm for maximum likelihood   which still needs to be explored.
1. sequential ib clustering
　the application of the above discussion in the context of the information bottleneck method is straightforward. we define f t  = i t;y   and represent each x by p x y . the greedy merging criterion is known from the agglomerative information bottleneck  aib  algorithm  1  1 . specifically  in this context we get
	d x t  =  p x  + p t   ， js p y|x  p y|t    	 1 
where js p q  is the jensen-shannon divergence  1  1  defined as
js p q  = π1dkl p|p．  + π1dkl q|p．   
where in our context
{p q} 《 {p y|x  p y|t }
{π1 π1} 《 {p xp + xp  t   p xp + t p t }  1  p． = π1p y|x  + π1p y|t  .
notice that any given partition t defines some membership   hard   probability p t|x   which in turn defines p y|t  and p t  for every t （ t through eqs. 1 . additionally since i t;y   is indeed upper bounded we are guaranteed to converge to a local maximum of the information.
　the js divergence is non-negative and is equal to zero if and only if both its arguments are identical. it is upper bounded and symmetric  though it is not a metric. one interpretation of the js-divergence relates it to the  logarithmic  measure of the likelihood that the two sample distributions originate by the most likely common source  denoted here by ．p . using this interpretation we can interpret the new algorithm as follows. at each step we draw some x and merge it back into its most probable source. we refer to this algorithm as the 'sib' algorithm.
1. other clustering methods
　we can use the same sequential framework with other similarity criteria to construct other algorithms for purposes of comparison. in each of these algorithms we used exactly the same procedure described in figure 1. the only difference was in the choice of df x t .
　a common divergence measure among probability distributions is the kl-divergence. an interesting question is how well the sequential clustering algorithm will perform while using this measure instead of the js-divergence. more specifically  we define d x t  =  p x +p t  ，dkl p y|x |p y|t  .
we refer to this algorithm as the 'skl' algorithm.
　another common divergence measure among probability distributions is the l1 norm defined as y |   |
     p y  q y  . unlike the js  and the kl  divergence the l1 norm satisfies all the metric properties. therefore we defined the 'sl1' algorithm by setting d x t  =  p x +p t  ，
.
　in the third comparison algorithm we use the standard cosine measure under the vector space model. specifically we define where ．x is the counts vector of x normalized such that = 1. the centroid t．is defined as the average of all the  normalized  count vectors representing the documents assigned into t  again  normalized to 1 under the l1 norm . due to this normalization is simply the cosine of the angle between these two vectors  and is proportional to . notice that in this case we update assignments by merging x into tnew x  = argmax we will term this algorithm 'sk-means'. we also implemented a standard parallel version of this algorithm which we will term 'k-means'.
　lastly  we also compare our results to the original aib algorithm  and to the recent iterative double clustering  idc  procedure suggested by el-yaniv and souroujon . this method  which is a natural extension of the previous work in   uses an iterative double-clustering procedure over documents and words. it was shown in  to work surprisingly well on relatively small data sets  and even to be competitive with a supervised svm classifier trained with a small training set.
1. the experimental design
1 the datasets
　following  1  1  we used several standard labeled data sets to evaluate the different clustering methods described above. as our first data set we used the 1newsgroup corpus collected by lang . this corpus contains about 1 articles evenly distributed among 1 usenet discussion groups  some of which are of very similar topics. after removing all file headers1 our pre-processing included lowering upper case characters  uniting all digits into one symbol and ignoring non alpha-numeric characters. we also removed stop words and words that occurred only once  ending up with a vocabulary of 1 unique words. we further included a standard feature selection procedure  e.g.     where we selected the 1 words with the highest contribution to the mutual information about the documents. specifically  we sorted all words by and selected the top 1.
　for a medium-scale experiment we used the whole corpus except for documents with less than 1 words occurrences  ending up with a counts matrix of 1 documents versus 1 words. we constructed two different tests over this data. first we measured our performance with respect to all the 1 different classes. additionally we applied an easier test where we measured our performance with respect to 1 meta-categories in this corpus.1 we will term these two tests ng1 and ng1 respectively. for small-scale experiments we used the 1 subsets of this corpus already used in  1  1 . each of these subsets consist of 1 documents randomly chosen from several discussion groups.
　as a third medium scale test we used the 1 documents of the 1 most frequent categories in the reuters1 corpus  http://www.daviddlewis.com/resources/ testcollections/reuters1/  under the modapte split. after the same pre-processing we got a counts matrix of 1 documents versus 1 words.
　as the last medium scale test we used a subset of the new release of reuters-1 corpus. specifically we used the 1 documents of the 1 most frequent categories in the 1 first days of this corpus  last 1 days in august 1 . after the same pre-processing  except for not uniting digits due to a technical reason   we ended up with a counts matrix of 1 documents versus 1 words. notice that these two last corpora are multi labeled.
　one issue is how to evaluate different restarts of the algorithms. for the sib we naturally choose the run that found the most informative clusters and report results for it. for other algorithms  we can use their respective scoring function. however  to ensure that this does not lead to poor performance  we choose to present for each of these algo-

1
 unfortunately there is no clear standard about what should be referred as a file header in this corpus. in particular  the results reported in  1  1  stripped of the header including the subject line  as instructed in http://www.cs.cmu.edu/゛mccallum/bow . on the other hand  the results reported in  1  1  does make use of the subject line which in many cases contain useful information. to make our results comparable with  we decided to use the subject line in this paper.
1
 specifically we united the 1  comp  categories  the 1  religion  categories  the 1  politics  categories  the two  sport  categories and the two  transportation  categories into 1 big meta-categories.
rithms the best result  in terms of the correlation to the true classification  among all n iterations. this choice provides an overestimate of the performance of these algorithms  and thus penalizes the sequential ib algorithm in the comparisons below.
1 the evaluation method
　as our evaluation measures we used micro-averaged precision and recall. to estimate these measures we first assign all the documents in some cluster t （ t with the most dominant label in that cluster.1 given these uni-labeled assignments we can estimate for each category c （ c the following quantities: α c t  defines the number of documents correctly assigned to c  i.e.  their true label sets include c   β c t  defines the number of documents incorrectly assigned to c and γ c t  defines the number of documents incorrectly not assigned to c. the micro-averaged precision is now defined by
		 1 
and the micro-averaged recall is defined by
	 .	 1 
it is easy to verify that if the corpus and the algorithm are both uni-labeled then p t  = r t   thus for our uni-labeled data sets we will report only p t .
　as a simplifying assumption we assume that the user is  approximately  aware of the correct number of categories in the corpus. therefore  for all the unsupervised techniques we measure p t  and r t  for |t| = |c|. choosing the appropriate number of clusters is in general a question of model selection which is beyond the scope of this work.
1. experimental results
1 maximizing information and clusters precision
　a first natural question to ask is what are the performance of the new sib algorithm versus the aib algorithm in terms of maximizing i t;y  . comparing the results over the 1 small data sets  for which running aib is feasible  we found that sib  with n=1  always extract solutions that preserve significantly more information than aib  the improvement is of 1% on the average . moreover  even if we do not choose for sib the iteration which maximized i t;y   but compare all the 1 random restarts  for every data set  with the aib results  we found that more than 1% of these runs preserve more information than aib.
　the next question we address is whether clustering solutions that preserve more information are better correlated with the real statistical sources  i.e.  the categories . in figure 1 a  we present the progress of the information and precision for a specific restart of sib over the ng1 data set. we clearly see that while the information is increasing for every assignment update  as guaranteed by the algorithm   p t  is increasing in parallel. in fact  less than 1% of the updates reduced p t . similar results obtained for all the other data sets.

1
 the underlying assumption here is that if the cluster is relatively homogeneous the user will be able to correctly identify its most dominant topic.

	 a 	 b 	 c 
figure 1:  a  progress of i t;y   and p t  during the assignment updates of sib over the ng1 data set. correlation of final values of i t;y   and p t  for all random restarts of sib over  b  the three multi1 testsand  c  the ng1 test.
　lastly  we would like to check whether choosing the iteration which maximized i t;y   is a reasonable unsupervised criterion for identifying solutions with high precision. in figure 1 b c  we see the final values of i t;y   versus p t  for all the n random restarts of sib over the three multi1 tests and the ng1 test. clearly these final values are correlated. in fact  in 1 out of our 1 tests the iteration which maximized i t;y   also maximized p t   and when it did not the gap was relatively small.
1 results for small-scale experiments
　in table 1 we present the results for the small-scale 1 subsets of the 1newsgroups corpus. the results for the idc algorithm are taken from . for all the unsupervised algorithms we set n = 1  ε = 1 and maxl = 1. however  all algorithms except for sl1 attained full convergence in all 1 restarts and over all datasets after less than 1 loops.
　to gain some perspective about how hard is the classification task we also present results of a supervised naive bayes  nb  classifier  see  for the details of the implementation . the test set for this classifier consisted of the same 1 documents in each data set while the training set consisted of different 1 documents randomly chosen from the appropriate categories. we repeated this process 1 times and averaged the results.
several results should be noted specifically.
  sib outperformed all the other unsupervised techniques in all data sets  typically with an impressive gap. taking into account that for the other techniques we present an  unfair  choice of the best result  among all 1 restarts  we see these results as especially encouraging.
  in particular sib was clearly superior to idc and aib which are both also motivated by the information bottleneck method. nonetheless  in contrast to sib  the specific implementation of idc in  is not guaranteed to maximize i t;y   which might explain its inferior performance. we believe that the same explanation holds for the inferiority of aib.
  sib was also competitive with the supervised nb classifier. a significant difference between these two was evident only for the three multi1 subsets  i.e.  only when the number of real categories was relatively high.
  the poor performance of the skl algorithm was due to a typical fast convergence into one huge cluster which consisted of almost all documents. this tendency is due to the over sensitivity of this algorithms to  zero  probabilities in the centroid representations and it was clearly less dominant in the medium scale experiments.
  the aib results are significantly better than those reported in   p t =1  although the algorithm is the same. the difference is probably due to the inclusion of the subject lines of the messages  in contrast to these previous results.
1 results for medium-scale experiments
　in table 1 we present the results for the medium-scale data sets. to the best of our knowledge our results are the first reported results  using direct evaluation measures as precision and recall  for unsupervised methods over corpora in that scale  order of 1 documents .
　for all the unsupervised algorithms we set n = 1  ε = 1|x|  where |x| is the number of documents in the corpus  and maxl = 1. we note here that this last choice of maxl = 1 was in fact probably too low for some of the algorithms  see below . for these tests as well we applied the supervised nb classifier. for each test  the training set consisted of 1 documents  randomly chosen out of the dataset  while the test set consisted of the remaining documents. again  we repeated this process 1 times and averaged the results.
　notice that the two reuters tests are multi-labeled while all our classification schemes are uni-labeled. therefore the recall of these schemes is inherently limited. this is especially evident for the new-reuters test in which the average number of labels per document was 1 and hence the maximum attained  micro-averaged  recall was limited to 1%. our main findings are listed in the following.
table 1: micro-averaged precision results over the small data sets. in all unsupervised algorithms the number of clusters was taken to be identical with the number of real categories  indicated in parenthesis . for kmeans sk-means sl1 and skl the results are the best results among all 1 restarts. for sib the results are for the restart which maximized i t;y  ;. the test set for the nb classifier consisted of the same 1 documents in each data set while the training set consisted of different 1 documents randomly chosen from the appropriate categories. we repeated this process 1 times and averaged the results.

p t sibidcsk-meansk-meansaibsl1sklnbbinary1 1 11.1.1.1.1.1.1binary1 1 11.1.1.1.1.1.1binary1 1 11.1.1.1.1.1.1multi1 1 11.1.1.1.1.1.1multi1 1 11.1.1.1.1.1.1multi1 1 11.1.1.1.1.1.1multi1 1 11.1.1.1.1.1.1multi1 1 11.1.1.1.1.1.1multi1 1 11.1.1.1.1.1.1average11111111
table 1: micro-averaged precision results over the medium scale data sets. in all the unsupervised algorithms the number of clusters was taken to be identical with the number of real categories  indicated in parenthesis . the nb classifier was trained over 1 randomly chosen documents and tested over the remaining. we repeated this process 1 times and averaged the results.

p t sibsk-meansk-meanssl1sklnbng1  1 111111ng1  1 111111reuters  1 111111new-reuters  1 111111average111111  similar to the small-scale experiments  sib outperforms all the other unsupervised techniques  typically with a significant margin and in spite of the  unfair  comparison.
  interestingly  sib was almost competitive with the supervised nb classifier which was trained over 1 labeled documents.
  both our sequential and parallel k-means implementations performed surprisingly well  especially over the uni-labeled ng1 and ng1 tests. as in the small data sets  the differences between the parallel and the sequential implementation were minor.
  the convergence rate of the sib and the sk-means algorithms were typically better than those of the other algorithms. in particular  sib and sk-means converged in most of their iterations while  for example  sl1 did not converged in any iteration.
1 improving clusters precision
　in supervised text classification one is able to trade precision versus recall by defining some thresholding strategy. in the following we suggest a similar idea for the unsupervised classification scenario. notice that once a partition t is obtained we are able to estimate d x t x    x （ x. clearly this provide us with an estimate of how  typical  is x in t x . specifically in the context of sib  d x t x   is related to the loss of information by not holding x as a singleton cluster.
　by sorting the documents in each cluster t （ t with respect to d x t  and  labeling  only the top r% of the documents in that cluster we can now reduce the recall while  hopefully  improving the precision. more specifically while defining the  label  for every cluster we use only documents that were sorted among the top r% for that cluster  and refer to the remaining as  unlabeled  . notice that this procedure is independent of the specific definition of d x t  and thus could be applied for all the sequential algorithms we tested.
　in figure 1 we present the precision-recall curves for some of our medium scale tests. again  we find sib to be clearly superior to all the other unsupervised methods examined. in particular for r = 1% sib attains very high precision in a totally unsupervised manner for our real world corpora. these results raise the possibility of a new approach for combining unsupervised and supervised classification methods. specifically we could use these high precision  labeled  clusters as a training set for a supervised classifier. assuming that the precision of our  labels  is indeed relatively high and that the supervised classifier is not too sensitive for a small amount of  noise  in its training set  we predict that we can now label the remaining  unlabeled  documents  and new ones  with a reasonable performance. this issue  however  is left for future research.

figure 1: precision-recall curves for some of our medium scale tests. for the other tests the results were similar. notice that the results for sib are for the specific restart which maximized i t;y   while for the other methods we present the best result over all 1 restarts.1 using other representations
　it is well known that a word-counts representation of documents is typically noisy and different techniques can provide more sophisticated representations. in this work we concentrated on comparing different algorithms for a fixed  counts  representation. nonetheless  it is interesting to ask what are the performance using different representations. although this is not the focus of this work we performed additional experiments regarding this issue. a well known approach in the context of text classification is the tf-idf representation . specifically  each word count is multiplied by the inverse-document-frequency of the word  defined by
  where |x y | is the number of documents in which y occurred. rich empirical evidence support the benefits of using this representation for text processing applications.
　therefore we applied the sk-means and the parallel kmeans for all our tests  using the tf-idf representation. the results presented in table 1 indeed verify that this representation could significantly improve k-means performance. the improvement is especially impressive for the small  and more noisy  data sets and for the multi-labeled reuters data sets. interestingly  under this representation we see that the sequential implementation yield superior performance to the parallel one  which implies better robustness to local optima using a sequential approach.
　comparing sib with these results we still see that sib attained superior performance in almost all our tests  although the gap in some cases was minor . recall that this comparison is with the best result obtained using the tf-idf representation over all n restarts. comparing sib with the averaged tf-idf performance we obviously see a more significant margin in favor of sib.
　these results are of special interest taking into account that the tf-idf representation is specifically  tailored  for the context of text classification while sib is a general approach using the naive counts representation. moreover  there is strong empirical evidence suggesting that sib might improve its performance using more robust representations such as word-clusters  1  1  1 .
1. concluding remarks
　in this paper we introduced the sib algorithm. we showed that it has several important advantages over the agglomerative ib algorithm  both in terms of complexity and quality of clusters it finds. the reduced complexity allows us to use this algorithm on larger datasets. moreover  the performance of sib are superior to all the other unsupervised methods we examined  including methods which are especially designed for text classification. additionally  our unsupervised results are even competitive with a standard supervised naive bayes classifier. in the appendix below  we provide preliminary theoretical analysis to motivate these empirical observations.
　an interesting question is comparing sib with the parallel versions of ib algorithms . our preliminary results showed  rather surprisingly  that using sib we seem to obtain solutions that preserve more information than by solving the self-consistent equations directly. however  this issue calls for a more thorough investigation and theoretical understanding.
　lastly  we note here that extending the sib algorithm to solve multi-variate variations of the information bottleneck principle  is straightforward  using the recent extension of the aib algorithm into the multi-variate case .
acknowledgments
we thank yoram singer  leo kontorovich  benjy weinberger  and in particular koby crammer for useful discussions and comments on previous drafts of this paper. this work was supported in part by the israel science foundation  isf  and by the us-israel bi-national science foundation  bsf . n. friedman was also supported by an alon fellowship and the harry & abe sherman senior lectureship in computer science.
