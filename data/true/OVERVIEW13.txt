1.1 documents
the document set of a test collection should be a sample of the kinds of texts that will be encounteredin the operational setting of interest. it is important that the document set reflect the diversity of subject matter  word choice  literary styles  document formats  etc. of the operational setting for the retrieval results to be representative of the performance in the real task. frequently  this means the document set must be large. the primary trec test collections contain about 1 gigabytes of text  between 1 and 1 1 documents . the document sets used in various tracks have been smaller and larger depending on the needs of the track and the availability of data. the terabyte track was introduced this year to investigate both retrieval and evaluation issues associated with collections significantly larger than 1 gigabytes of text.
   the primary trec document sets consist mostly of newspaper or newswire articles  though there are also some government documents  the federal register  patent applications  and computer science abstracts  computer selects by ziff-davis publishing  included. high-level structures within each document are tagged using sgml  and each document is assigned an unique identifier called the docno. in keeping of the spirit of realism  the text was kept as close to the original as possible. no attempt was made to correct spelling errors  sentence fragments  strange formatting around tables  or similar faults.
1.1 topics
trec distinguishes between a statement of information need  the topic  and the data structure that is actually given to a retrieval system  the query . the trec test collections provide topics to allow a wide range of query construction methods to be tested and also to include a clear statement of what criteria make a document relevant. the format of a topic statement has evolved since the earliest trecs  but it has been stable since trec-1  1 . a topic statement generally consists of four sections: an identifier  a title  a description  and a narrative. an example topic taken from this year's robust track is shown in figure 1.
   the different parts of the trec topics allow researchers to investigate the effect of different query lengths on retrieval performance. for topics 1 and later  the  title  field was specially designed to allow experiments with very
 num  number: 1
 title  lead poisoning children
 desc 
how are young children being protected against lead poisoning from paint and water pipes 
 narr 
documents describing the extent of the problem  including suits against manufacturers and product recalls  are relevant. descriptions of future plans for lead poisoning abatement projects are also relevant. worker problems with lead are not relevant. other poison hazards for children are not relevant.
figure 1: a sample trec 1 topic from the robust track test set.
short queries; these title fields consist of up to three words that best describe the topic. the description   desc   field is a one sentence description of the topic area. the narrative   narr   gives a concise description of what makes a document relevant.
   participants are free to use any method they wish to create queries from the topic statements. trec distinguishes among two major categories of query construction techniques  automatic methods and manual methods. an automatic method is a means of deriving a query from the topic statement with no manual intervention whatsoever; a manual method is anything else. the definition of manual query construction methods is very broad  ranging from simple tweaks to an automatically derived query  through manual construction of an initial query  to multiple query reformulations based on the document sets retrieved. since these methods require radically different amounts of  human  effort  care must be taken when comparing manual results to ensure that the runs are truly comparable.
   trec topic statements are created by the same person who performs the relevance assessments for that topic  the assessor . usually  each assessor comes to nist with ideas for topics based on his or her own interests  and searches the document collection using nist's prise system to estimate the likely number of relevant documents per candidate topic. the nist trec team selects the final set of topics from among these candidate topics based on the estimated number of relevant documents and balancing the load across assessors.
1.1 relevance judgments
the relevance judgments are what turns a set of documents and topics into a test collection. given a set of relevance judgments  the retrieval task is then to retrieve all of the relevant documents and none of the irrelevant documents. trec usually uses binary relevance judgments-either a document is relevant to the topic or it is not. to define relevance for the assessors  the assessors are told to assume that they are writing a report on the subject of the topic statement. if they would use any information contained in the document in the report  then the  entire  document should be marked relevant  otherwise it should be marked irrelevant. the assessors are instructed to judge a document as relevant regardless of the number of other documents that contain the same information.
   relevance is inherently subjective. relevance judgments are known to differ across judges and for the same judge at differenttimes . furthermore  a set of static  binary relevance judgmentsmakes no provisionfor the fact that a real user's perception of relevance changes as he or she interacts with the retrieved documents. despite the idiosyncratic nature of relevance  test collections are useful abstractions because the comparative effectiveness of different retrieval methods is stable in the face of changes to the relevance judgments .
   the relevance judgments in early retrieval test collections were complete. that is  a relevance decision was made for every document in the collection for every topic. the size of the trec document sets makes complete judgments utterly infeasible-with 1 documents  it would take over 1 hours to judge the entire document set for one topic  assuming each document could be judged in just 1 seconds. instead  trec uses a technique called pooling  to create a subset of the documents  the  pool   to judge for a topic. each document in the pool for a topic is judged for relevance by the topic author. documents that are not in the pool are assumed to be irrelevant to that topic.
   the judgment pools are created as follows. when participants submit their retrieval runs to nist  they rank their runs in the order they prefer them to be judged. nist chooses a number of runs to be mergedinto the pools  and selects that many runs from each participant respecting the preferred ordering. for each selected run  the top x documents  usually  x = 1  per topic are added to the topics' pools. since the retrieval results are ranked by decreasing similarity to the query  the top documents are the documents most likely to be relevant to the topic. many documents are retrieved in the top x for more than one run  so the pools are generally much smaller than the theoretical maximum of x ¡Á the-number-of-selected-runs documents  usually about 1 the maximum size .
   the use of pooling to produce a test collection has been questioned because unjudged documents are assumed to be not relevant. critics argue that evaluation scores for methods that did not contribute to the pools will be deflated relative to methods that did contribute because the non-contributors will have highly ranked unjudged documents.
   zobel demonstrated that the quality of the pools  the number and diversity of runs contributing to the pools and the depth to which those runs are judged  does affect the quality of the final collection . he also found that the trec collections were not biased against unjudged runs. in this test  he evaluated each run that contributed to the pools using both the official set of relevant documents published for that collection and the set of relevant documents produced by removing the relevant documents uniquely retrieved by the run being evaluated. for the trec-1 ad hoc collection  he found that using the unique relevant documents increased a run's 1 point average precision score by an average of 1 %. the maximum increase for any run was 1 %. the average increase for the trec-1 ad hoc collection was somewhat higher at 1 %.
   a similar investigation of the trec-1 ad hoc collection showed that every automatic run that had a mean average precision score of at least 1 had a percentage difference of less than 1 % between the scores with and without that group's uniquely retrieved relevant documents . that investigation also showed that the quality of the pools is significantly enhanced by the presence of recall-oriented manual runs  an effect noted by the organizers of the ntcir  nacsis test collection for evaluation of informationretrieval systems  workshop who performedtheir own manual runs to supplement their pools .
   while the lack of any appreciable difference in the scores of submitted runs is not a guarantee that all relevant documents have been found  it is very strong evidence that the test collection is reliable for comparative evaluations of retrieval runs. the differences in scores resulting from incomplete pools observed here are smaller than the differences that result from using different relevance assessors .
1 evaluation
retrieval runs on a test collection can be evaluated in a number of ways. in trec  ad hoc tasks are evaluated using the treceval package written by chris buckley of sabir research . this package reports about 1 different numbers for a run  including recall and precision at various cut-off levels plus single-valued summary measures that are derived from recall and precision. precision is the proportion of retrieved documents that are relevant  while recall is the proportion of relevant documents that are retrieved. a cut-off level is a rank that defines the retrieved set; for example  a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list. the treceval programreports the scores as averages over the set of topics where each topic is equally weighted.  the alternative is to weight each relevant document equally and thus give more weight to topics with more relevant documents. evaluation of retrieval effectiveness historically weights topics equally since all users are assumed to be equally important. 
   precision reaches its maximal value of 1 when only relevant documents are retrieved  and recall reaches its maximal value  also 1  when all the relevant documentsare retrieved. note  however  that these theoretical maximum values are not obtainableas an averageovera set of topics at a single cut-offlevel because differenttopics havedifferent numbers of relevant documents. for example  a topic that has fewer than ten relevant documents will have a precision score less than one at ten documents retrieved regardless of how the documents are ranked. similarly  a topic with more than ten relevant documents must have a recall score less than one at ten documents retrieved. at a single cut-off level  recall and precision reflect the same information  namely the number of relevant documentsretrieved. at varying cut-off levels  recall and precision tend to be inversely related since retrieving more documents will usually increase recall while degrading precision and vice versa.
   of all the numbers reported by treceval  the recall-precision curve and mean  non-interpolated average precision are the most commonly used measures to describe trec retrieval results. a recall-precision curve plots precision as a function of recall. since the actual recall values obtained for a topic depend on the number of relevant documents  the average recall-precision curve for a set of topics must be interpolated to a set of standard recall values. the particular interpolation method used is given in appendix a  which also defines many of the other evaluation measures reported by treceval. recall-precision graphs show the behavior of a retrieval run over the entire recall spectrum.
   mean average precision is the single-valued summary measure used when an entire graph is too cumbersome. the average precision for a single topic is the mean of the precision obtained after each relevant document is retrieved  using zero as the precision for relevant documents that are not retrieved . the mean average precision for a run consisting of multiple topics is the mean of the average precision scores of each of the individual topics in the run. the average precision measure has a recall component in that it reflects the performance of a retrieval run across all relevant documents  and a precision component in that it weights documents retrieved earlier more heavily than documents retrieved later. geometrically  average precision is the area underneath a non-interpolated recall-precision curve.
   as trec has expandedinto tasks other than the traditional ad hoc retrieval task  new evaluationmeasures have had to be devised. indeed  developing an appropriate evaluation methodology for a new task is one of the primary goals of the trec tracks. the details of the evaluation methodology used in a track are described in the track's overview paper.
1 trec 1 tracks
trec's track structure was begun in trec-1  1 . the tracks serve several purposes. first  tracks act as incubators for new research areas: the first running of a track often defines what the problem really is  and a track creates the necessary infrastructure  test collections  evaluation methodology  etc.  to support research on its task. the tracks also demonstrate the robustness of core retrieval technology in that the same techniques are frequently appropriate for a variety of tasks. finally  the tracks make trec attractive to a broader community by providing tasks that match the research interests of more groups.
   table 1 lists the different tracks that were in each trec  the number of groups that submitted runs to that track  and the total number of groups that participated in each trec. the tasks within the tracks offered for a given trec have diverged as trec has progressed. this has helped fuel the growth in the number of participants  but has also created a smaller common base of experience among participants since each participant tends to submit runs to fewer tracks.
   this section describes the tasks performedin the trec 1tracks. see the track reports later in these proceedings for a more complete description of each track.
1 the genomics track
the genomics track was introduced as a  pre-track  in 1. it is the first trec track devoted to retrieval within a
specific domain; one of the goals of the track is to see how exploiting domain-specific information improves retrieval effectiveness.
   the 1 genomics track contained an ad hoc retrieval task and three variants of a categorization task. the ad hoc task used a 1-yearsubset  1 of medline  a bibliographicdatabase of the biomedical literature maintained by the us national library of medicine who donated the subset to the track. the subset used in the track contains about 1 million medline records  which include title and abstract as well as other bibliographic information  and is about 1gb of data. the 1 topics for the ad hoc task were derived from information needs obtained through interviews of biomedical researchers. pools were created using one run from each of the 1 participating groups using a depth of 1. relevance judgments were made by assessors with backgrounds in biology using a three-point scale of definitely relevant  probably relevant  and not relevant. both definitely relevant and probably relevant were considered relevant when computing evaluation scores.
   domain knowledge was most frequently exploited by using resources such as the mesh hierarchy  a controlled vocabulary used to index medical literature  to expand queries. careful use of such resources appears to increase retrieval effectiveness  though some attempts to exploit such information decreased effectiveness relative to a generic baseline.
   the genomicsdomainhas a numberof model organismdatabaseprojects in whichthe literatureregardinga specific organism  such as a mouse  is tracked and annotated with the function of genes and proteins. the classification tasks table 1: number of participants per track and total number of distinct participants in each trec
tracktrec1111111ad hoc1111-----routing111-------interactive--111111--spanish--11--------confusion---1--------db merging---1--------filtering---11111--chinese----1-------nlp----1-------speech-----11----cross-language-----1111--high precision-----1------vlc------1-----query------11----qa-------11111web-------11111video--------	-11--novelty--------	--111genomics--------	---11hard--------	---11robust--------	---11terabyte--------	----1total participants1111111used in the 1 track mimic some aspects of this curation process with the goal of eventually automating this now largely manual task. for the classification tasks  the track used the full text articles from a two-year span of three journals. this text was made available to the track through highwire press. the truth data for the tasks came from the actual annotation process carried out by the human annotators in the mouse genome informatics  mgi  system. evaluation scores were computed using normalized utility measures.
   as in the ad hoc task  manygroups used mesh terms as features to classify the documents. while these approaches were relatively effective  a subsequent analysis demonstrated the benefit was largely attributable to a single mesh term: a baseline run that classified documents solely by the presence of the mesh term mice in the medline record of the document would have been the second best run submitted to the track for the triage classification task.
1 the hard track
hard stands for  high accuracy retrieval from documents . the hard track was started in trec 1 with the goal of improving retrieval performance  especially at the top of the ranked list  by targeting retrieval results to the specific searcher. to facilitate such targeting  the hard track provides metadata in the topic statement. in addition   clarification forms  provide a limited means of interaction between the system and the searcher.
   the underlying task in the hard track was an ad hoc retrieval task. the document set was a set of newswire/newspaper articles from 1  including  english portions  of non-us papers. the collection is approximately 1mb of text and contains approximately 1 articles. topics were created at the linguistic data consortium  ldc   and were originally released in standard trec format  i.e.  just title  description  and narrative fields . once participants submitted baseline runs using the standard topics  they received the expanded version of the topics. there were 1 topics in the test set  though only 1 topics were used in the evaluation since five topics had no relevant documents.
the expanded version of the topics contained both a statement of the retrieval unit and the metadata. the retrieval unit was always specified  and was either  passage  or  document . the  passage  specification meant retrieval systems should return pieces of documents  rather than full documents  as a response. the types of metadata in the trec 1 topics included familiarity  genre  geography  subject  and related text. the first three types affected the relevance of a text: a text that was on-topic but did not satisfy one of these metadata constraints was considered not relevant when using stringent relevance criteria. the subject metadata item contained the subject domain of the topic  for example   sports   or  politics  ; a document that did not meet this criterion was off-topic. the related text metadata provided some examples of relevant or on-topic text drawn from outside the test corpus. different topics contained different kinds and amounts of metadata.
   in addition to the information included in the expanded version of the topics  participants could collect information from the searcher  the assessor who created and judged the topic  using clarification forms. a clarification form was a single  self-contained html form created by the participating group and specific to a single topic. there were no restrictions on what type of data could be collected using a clarification form  but the searcher spent no more than three minutes filling out any one form.
   participants then made new runs using any combination of information from the expanded topics and clarification forms. the goal was to see if the additional information helped systems to create a more effective retrieved set than the initial baseline result. retrieval results were evaluated both at the document level  for all 1 topics including those with retrieval unit  passage   using treceval and using passage level evaluation measures over just the 1 topics with retrieval unit  passage .
   sixteen groups submitted 1 runs to the hard track. most groups were able to exploit the additional information to improve effectiveness as compared to their baseline run  generally by performing some type of relevance feedback.
1 the novelty track
the goal of the novelty track is to investigate systems' abilities to locate relevant and new  nonredundant information within an ordered set of documents. this task models an application where the user is skimming a set of documents and the system highlights the new  on-topic information. the track was first introduced in trec 1  though the tasks changed significantly between 1 and 1. this year's track used the same tasks as the 1 track.
   the basic task in the novelty track is as follows: given a topic and an ordered set of documents segmented into sentences  return sentences that are both relevant to the topic and novel given what has already been seen. to accomplish this task  participants must first identify relevant sentences and then identify which sentences contain new information.
   fifty new topics were created for the 1 track. as in trec 1  half of the topics focused on events and the other half focused on opinions about controversial subjects. for each topic  the assessor created a statement of information need and queried the document collection using the nist prise search engine. the assessor selected 1 relevant documents and labeled the relevant and new sentences in each. the document collection used was the aquaint corpus of english news text which contains approximately 1 1 documents and 1 gigabytes of text. the document set for a topic in the test set contained the 1 relevant documents selected by the assessor as well as 1 or more irrelevant documents. the documents in a set were ordered chronologically.
   there were four tasks in the track  which allowed participants to test their approaches to novelty detection using no  partial  or complete relevance information.
task 1. given the complete document set for a topic  identify all relevant and novel sentences.
task 1. given the relevant sentences in the complete document set  identify all novel sentences.
task 1. given the relevant and novel sentences in the first 1 documents for the topic  find the relevant and novel sentences in the remaining documents.
task 1. given the relevant sentences in the complete document set  and the novel sentences in the first 1 documents  find the novel sentences in the remaining documents.
   given the set of relevant and new sentences selected by the assessor who created the topic  the score for a novelty topic was computed as the f measure where sentence set recall and sentence set precision are equally weighted.
   fourteen groups submitted 1 runs to the novelty track  with tasks 1 and 1 having the greater participation. the inclusion of nonrelevantdocuments in the retrieved set appears to make task 1 much more challenging. in trec 1 
1hale bopp comet
1	factoidwhen was the comet discovered 1	factoidhow often does it approach the earth 1	list
1	otherin what countries was the comet visible on its last return figure 1: a sample qa track question series.
the best-performing systems for task 1 were roughly comparable to human performance as measured by scoring a second assessor's sentence selection against the primary assessor's choices. this year  the best systems' effectiveness was well below human performance. the particular topics used this year may also have been more difficult given that the absolute scores of trec 1 systems were lower than trec 1 scores for task 1 and task 1 is unaffected by nonrelevant documents.
1 the question answering  qa  track
the question answering track addresses the problem of information overload by encouraging research into systems that return actual answers  as opposed to ranked lists of documents  in response to a question. the trec 1 version of the track used a combined task where the test set of questions consisted of factoid  list  and definition questions. each type of question was judged and scored separately  but the final score for a run was a weighted average of the component scores. the task in the 1 track was similar in that the test set consisted of a mix of question types  and the final score was a weighted average of the components. the task was reorganized  however  such that the systems were to answer a series of factoid and list questions that each related to a common target  and then to respond with a list of  other  information about the target that was not covered by the previous questions in the series. this last question in the series is a more difficult variant of the definition questions in trec 1. this reorientation of the task requires systems to track context when answering questions  an important element of question answering that the track has not yet successfully incorporated .
   the document set used in the track was the aquaint corpus of english news text. the test set consisted of 1 series of questions that together included 1 factoid questions  1 list questions  one had to be removed from the evaluation due to no correct answers in the collection   and 1 other questions  one had to be removed from the evaluation since it mistakenly went unjudged . each of the questions was explicitly tagged as to what type of question it was and what series it belonged to. the target of the series was given as metadata for the whole series. an example series is given in figure 1.
   the score for the factoid question component was accuracy  the percentage of factoid questions whose response was judged correct. the list and other question components were each scored using average f  though the computation of the f score differedbetween the two components. the final score for a run was computed as a weighted average of the three component scores: finalscore = .1accuracy + .1avelistf + .1aveotherf.
   sixty-three runs from 1 different groups were submitted to the track. in general  the use of pronouns and anaphora in questions later in a series did not seem to pose a very serious challenge for the systems  in part because the target was the correct referent a large majority of the time. for most systems  the average score for the first question in a series was somewhat greater than the average score for a question that was not the first question in a series  but the difference was not great and is confounded by other effects  there are many fewer first questions to compute the average over  first questions in a series might be intrinsically easier questions  etc. .
   the reorganization of the task into a set of question series had an unexpected benefit. the series proved to be an appropriate level of granularity for aggregating scores for an effective evaluation. the series is small enough to be meaningful at the task level since it represents a single user interaction  yet it is large enough to avoid the highly skewed score distributions exhibited by single questions. computing a combined score for each series  and averaging the series scores  produces a qa task evaluation that more closely mimics classic document retrieval evaluation.
1 the robust track
the robust track looks to improve the consistency of retrieval technology by focusing on poorly performing topics. trec 1 was the second time the track was run. the initial track provided strong evidence that optimizing average effectiveness using the standard methodology and current evaluation measures further improves the effectiveness of the already-effective topics  sometimes at the expense of the poor performers. that track also showed that measuring poor performance is intrinsically difficult because there is so little signal in the sea of noise for a poorly performing topic. new measures devised for the trec 1 robust track do emphasize poorly performing topics  but because there is so little information  the measures are unstable.
   the task in both years of the robust track was a classic ad hoc retrieval task. the trec 1 edition of the track used more topics than the 1 edition in hopes of getting a more stable evaluation. in particular  the test set for 1 consisted of 1 topics  one topic was dropped from the evaluation since it was judged to have no relevant documents . two hundred of the topics were used in previous trec tasks and 1 new topics were created for the track. to avoid needing new relevance judgments for the 1 old topics  an old document set was used: the set of documents on trec disks 1 and 1 minus the congressional record documents.
   the use of old topics had an additional motivation other than not needing new relevance judgments for those topics. since the retrieval results from the previous trec in which the topics were used are available  it is possible to select topics that are known to be challenging to a majority of retrieval systems. fifty topics from among the 1 old topics were designated as being difficult. these topics were selected for the trec 1 track by choosing topics that had a low median average precision score and at least one high outlying score.
   the retrieval results were evaluated using treceval  two measures introduced in the trec 1 track that emphasize poorly performing topics  and a new measure  geometric map  introduced in this year's track. the geometric map is a variant of the traditional map measure that uses a geometric mean rather than an arithmetic mean to average individual topic results. an analysis of the behavior of the geometric map measure suggests it gives appropriate emphasis to poorly performing topics while being more stable at equal topic set sizes.
   the robust track received a total of 1 runs from 1 participants. all of the runs submitted to the track were automatic runs. the results indicate that the most promising approach to improving poorly performing topics is exploiting text collections other than the target collection  though the process must be carefully controlled to avoid making the results worse. the web was the collection most frequently used as an auxiliary collection.
   an additional requirement in this year's track was for systems to submit a ranked list of the topics ordered by perceived difficulty. that is  the system assigned each topic a number from 1 to 1 where the topic assigned 1 was the topic the system believed it did best on  the topic assigned 1 was the topic the system believed it did next best on  etc. the purpose of the requirement was to see if systems can recognize whether a topic is difficult at run time  a first step toward doing special processing for difficult topics. while some systems were clearly better than others at predicting when a topic is difficult for that system  none of the systems were particularly good at the task. how much accuracy is required to make effective use of the predictions is still unknown.
1 the terabyte track
the terabyte track is a new track in 1. the goal of the track is is to develop an evaluation methodology for terabyte-scale document collections. the track also provides an opportunity for participants to see how well their retrieval algorithms scale to much larger test sets than other trec collections.
   the document collection used in the track is the gov1 collection  a collection of web data crawled from web sites in the .gov domain during early 1. this collection contains a large proportion of the crawlable pages in .gov  including html and text  plus extracted text of pdf  word and postscript files. the collection is 1gb in size and contains approximately 1 million documents. the collection is smaller than a full terabyte due to the difficulty of obtaining and processing enough documents while allowing sufficient time for distributing the collection to participants. the collection will be expanded using data from other sources in future years. the current collection is at least an order of magnitude greater than the next-largest trec collection.
   the task in the track was a classic ad hoc retrieval task. the test set consisted of 1 topics created specifically for the track. while the document set consists of web pages  the topics were standard information-seeking requests  and not navigational requests or topic distillation requests  for example. systems returned the top 1 documents per topic so various evaluation strategies can be investigated. participants also answered a series of questions about timing and resources required to produce the retrieval results.
   seventy runs from 1 different groups were submitted to the track. the top 1 documents per topic for two runs per group were added to the judgment pools. initial analysis of the track results has revealed little difference in the relative effectiveness of different approaches when evaluated by map or by bpref  a measure created for evaluation environments where pools are known to be very incomplete . there are a variety of reasons why this might be so: it may mean that current pooling practices are adequate for collections of this size  or that the runs submitted to the terabyte track happened to retrieve a sufficient set of relevant documents  or that the terabyte topics happened to be particularly narrow  and so forth. the terabyte track will continue in trec 1 to examine these questions.
1 the web track
the goal in the web track is to investigate retrieval behavior when the collection to be searched is a large hyperlinked structure such as the world wide web. previous trec web tracks had separately investigatedtopic distillation  named page finding  and home page finding tasks . since web search engines must process these types of searches  among others  without explicit knowledge of which type of search is wanted  this year's web task combined them into a single task.
   for a topic distillation search a system is to return a list of entry points for good websites principally devoted to the topic. since there are only a few good websites for any particular topic  there are only a few key   relevant   pages for a topic distillation search. the emphasis is on returning entry pages rather than pages containing relevant information themselves since a result list of homepages provides a better overview of the coverage of a topic in the collection.
   named page and home page finding searches are similar to each other in that both are known-item tasks where the system is to return a particular page. for home page finding  the target page is the home page of the entity in the topic. for named page finding  a particular page is sought  but that page is not an entry point to a site  e.g.   1 tax form  .
   for the trec 1 task  participants received a set of 1 title-only topics such as  west indian manatee information  and  york county . the assessor specified which type of search was intended when the topic was created  but the test set did not include this information. systems returned a ranked list of up to 1 pages per topic. during judging  the assessors made binary judgments as to whether a page was appropriate with respect to the intended task. that is  the pages returned for topics whose search type was topic distillation were judged relevant if the page was a key entry page and not relevant otherwise. for the named page finding and home page finding topics  a page was judged relevant if and only if the page was the target page  or a mirror/alias of the target page . the runs were evaluated using map  which is equivalent to the mean reciprocal rank  mrr  measure for known-item searches.
   the track used the .gov collectioncreated for the trec 1web track and distributed by csiro. this collection is based on a january  1 crawl of .gov web sites. the documents in the collection contain both page content and the information returned by the http daemon; text extracted from the non-html pages is also included in the collection.
   in addition to the search task  the track also contained a classification task in which the goal was simply to label each of the 1 test topics as to what type of search was intended.
   eighteen groups submitted a total of 1 runs to the track. nine of the runs were classification task runs. the retrieval results showed that systems are able to obtain effective overall retrieval without having to classify the queries by type. that is  groups were able to devise a single technique that performed well for home page  named page  and distillation topics. these techniques were not based solely on the text of a page  but also needed to exploit some sort of web information such as link structure or anchor text. systems that did attempt to classify topics were generally able to do so  with most classification errors confusing named page and home page topics.
1 the future
a significant fraction of the time of one trec workshop is spent in planning the next trec. a majority of the trec 1 tracks will continue in trec 1  including the genomics  hard  qa  robust  and terabyte tracks. as described in the web track overview paper  the web track as such will end  with a new enterprise track taking its place. the goal of the enterprise track is to study enterprise search-satisfying a user who is searching the data of an organization to accomplish some task. the novelty track will also end. finally  a new track  the spam track  will be introduced in trec 1. the goal of the spam track is to provide a standard evaluation of current and proposed spam filtering approaches  thereby laying the foundation for the evaluation of more general email filtering and retrieval tasks.
acknowledgements
special thanks to the track coordinators who make the variety of different tasks addressed in trec possible.
