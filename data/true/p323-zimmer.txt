information filtering  also referred to as publish/subscribe  complements one-time searching since users are able to subscribe to information sources and be notified whenever new documents of interest are published. in approximate information filtering only selected information sources  that are likely to publish documents relevant to the user interests in the future  are monitored. to achieve this functionality  a subscriber exploits statistical metadata to identify promising publishers and index its continuous query only in those publishers. the statistics are maintained in a directory  usually on a per-keyword basis  thus disregarding possible correlations among keywords. using this coarse information  poor publisher selection may lead to poor filtering performance and thus loss of interesting documents.1
¡¡based on the above observation  this work extends query routing techniques from the domain of distributed information retrieval in peer-to-peer  p1p  networks  and provides new algorithms for exploiting the correlation among keywords in a filtering setting. we develop and evaluate two algorithms based on single-key and multi-key statistics and utilize two different synopses  hash sketches and kmv synopses  to compactly represent publishers. our experimental evaluation using two real-life corpora with web and blog data demonstrates the filtering effectiveness of both approaches and highlights the different tradeoffs.
categories and subject descriptors
c.1  computer-communication networks : distributed systems-distributed applications; h.1  information storage and retrieval : information search and retrieval- selection process  information filtering
general terms
algorithms  design  performance
keywords
peer-to-peer  p1p   information systems  approximate publish/subscribe  distributed information filtering  if   distinctvalue  dv  estimation

1this work has been partly supported by the eu project aeolus.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  singapore.
copyright 1 acm 1-1-1/1 ...$1.
1. introduction
¡¡in an information filtering  if  scenario a subscriber submits a continuous query  or subscription  and waits to be notified from the system about certain events of interest that take place  i.e.  about newly published documents relevant to the continuous query . most approaches to if taken so far have the underlying hypothesis of potentially delivering notifications from every information producer to subscribers  1  1  1 . this exact publish/subscribe model imposes an information overload burden on the user  and also creates an efficiency and scalability bottleneck that is probably not desirable in applications like news or blog filtering.
¡¡in approximate information filtering only a few carefully selected  specialized  and promising publishers store the user query and are monitored for new publications. thus  the user query is replicated to these sources and only published documents from these sources are forwarded to the subscriber. the system is responsible for managing the user query  discovering new potential sources and moving queries to better or more promising sources. since in an if scenario the data is originally highly distributed residing on millions of sites  e.g.  with people contributing to blogs   approximate if seems an ideal candidate for such a setting. this is also supported by the fact that exact if functionality has proven expensive for such distributed environments  1  1  1 . thus  approximate if achieves much better scalability of such systems by trading faster response times and lower message traffic for a moderate loss in recall.
¡¡in approximate if the publisher selection for a given continuous query with multiple keywords is driven by statistical summaries that are stored by the system. these summaries are provided to the directory by the publishers and can be managed in different ways ranging from centralized solutions like servers or server farms  to super-peer or pure peer-to-peer  p1p  solutions in the form of a distributed p1p directory built on top of a dht  1  1  or other kinds of overlay networks. for scalability  the summaries have publisher granularity  not document granularity  thus capturing the best publisher for certain keywords  also referred to as keys  but not for specific documents. this  together with per-key organization of the directory that disregards keyword correlations  also referred to as key sets  are two of the basic reasons that may possibly lead to insufficient recall. on the other hand  considering statistics for all possible key sets is clearly not possible due to the explosion in the feature space.
¡¡as an example scenario  consider a user bob who wants to follow the discussion about the presidential elections in the us  and wants to receive notifications from a number of different sites like news agencies  portals  and user blogs. clearly  bob would be interested in monitoring a variety of publishers but is not interested in receiving all the articles published by all sources  as it would be the case for exact if. thus  in an approximate if scenario  bob would submit the continuous query us presidential elections to the filtering system. the basic approach would decompose the continuous query into the three individual keys and use the statistics from the directory to compute a combined score  e.g.  intersection or some other kind of aggregation of individual key scores  for each key and publisher. this score would represent the probability of each source to publish documents about us presidential elections in the near future. this approach may lead to poor filtering quality as the top-ranked publishers for the complete query may not be among the top selected publishers. in the worst case  a selected publisher may deliver many documents for each single keyword  but no single document matching all keywords  since this information is not present in the directory.
¡¡in this paper  we develop and evaluate two approaches that use correlations among keys to improve filtering quality in the scenario described above. the first algorithm  coined uss  uses existing single-key synopses stored in the directory to estimate the publishing behavior of information sources for key sets  while the second  coined css  enhances the directory to explicitly maintain statistical metadata about selectively chosen key sets.
¡¡our work builds upon previous work in the area of information retrieval  ir  over p1p networks  and extends it to the approximate if setting in various novel ways. our contribution can be summarized as follows:
  in contrast to distributed ir settings for one-time searching where sources are ranked according to their document collections  in approximate if the publishers are ranked according to their probability to publish relevant documents in the near future  which poses different requirements for the maintenance of statistics. this is the first paper to develop algorithms for exploiting keyword correlations in such an if setting.
  we extend the approach of  for two-key queries  to the case of multi-key continuous queries for an arbitrary number of keys. we also provide new algorithms to approximate multi-key statistics by combining the statistics of arbitrary subsets.
  we show that hash sketches  used in  for compactly representing the documents  yield inaccurate results when considering continuous queries with more than two keys. we  thus  propose the usage of very recent state-of-the-art techniques for compact representation of multisets . these new techniques allow us to compute accurate synopses for multi-key queries  and improve the filtering effectiveness.
¡¡the remainder of this paper is organized as follows. section 1 introduces background information on hash sketches  and kmv synopses. section 1 discusses the architecture of our approximate if system. in section 1 we outline measures of correlation for keywords and present the two proposed algorithms. section 1 shows our experimental results. section 1 discusses related research. finally  section 1 concludes the paper.
1. background
¡¡synopses for compact approximation of sets  multisets  and their statistical properties have recently received great attention in the context of sensor networks  data streams  content delivery  and estimation issues in structured databases . the methods under consideration include bloom filters
  hash sketches   min-wise independent permutations   and kmv synopses   all of which are hash-based but differ in their strengths and limitations of representing various kinds of properties. the latter include set membership testing  set cardinality estimation  estimating the number of distinct elements in a multiset  forming intersections  unions  differences  etc. as shown in   these compact representations are a necessary component also for distributed ir approaches  since they assist in lowering the volume of data transferred over the network and thus increase system scalability. following this line  we also utilize these techniques for compactly disseminating statistics about the publishers. in this section we give brief background information on hash sketches and kmv synopses that are utilized by our algorithms.
1 hash sketches
¡¡hash sketches denote a well-known statistical tool for probabilistically estimating the cardinality of a multiset s. this distinct-value estimation technique was proposed in  and further improved in . hash sketches rely on the existence of a pseudo-uniform hash function which spreads input values pseudo-uniformly over its output values.
¡¡a key property of hash sketches lies in the ability to combine them. we can derive the hash sketch of the union of an arbitrary number of multisets from the hash sketches of each multiset by taking their bit-wise or. thus  given the compact synopses of a set of multisets  one can instantly estimate the number of distinct items in the union of these multisets.
¡¡furthermore  hash sketches can be used to estimate the cardinality of the intersection  overlap  of two sets. first  recall that |a¡Éb| = |a|+|b| |a¡Èb|. second  by utilizing the union method outlined above  one can derive the hash sketch for a ¡È b  and thus compute the cardinality of |a ¡É b|. however  it is not possible to create the hash sketch synopsis of the intersection for future use. the above can be generalized to more than two sets  using the inclusionexclusion principle and the sieve formula by poincar¡äe and sylvester. obviously  to compute the intersection of a huge number of hash sketches  the relative error is propagated and the distinct-value estimation is getting inaccurate. even regarding the overlap of three multisets  the sieve formula needs a high computation complexity.
1 kmv synopses
¡¡in   the kmv synopses and appropriate dv estimators are introduced. the main difference to hash sketches are constituted in the lower computational costs and the more accurate dv estimation. especially  the main focus of kmvs is on arbitrary multiset operations including union  intersection  and differences. first  we explain the main motivation behind the dv estimators. then  we introduce the kmv data structure and the basic dv estimator. using the kmv synopses and the basic estimator  the multiset operations union and intersection can be applied.
¡¡assume that d points are placed randomly and uniformly on the unit interval. the expected distance between two neighboring points is 1/ d + 1  ¡Ö 1/d  such that the expected value of uk  the k-th smallest point  is e uk  ¡Ö k/d. thus d ¡Ö k/e uk . if we know uk itself  a basic estimator for the number of points as proposed in  is
	d kb = k/uk	 1 
¡¡in the dv estimation problem  we have an enumeration of distinct values v1 v1 ... vd in dataset a with domain ¦È a . using a hash function h : ¦È a  1¡ú {1 ... m} such that the sequence h v1  h vn  ... h vd  looks like a sequence of independent and identically distributed samples from the discrete uniform distribution on {1 ... m}. assuming that m sufficiently greater than d  the sequence u1 = h v1 /m u1 = h v1 /m ... ud = h vd /m will approximate the realization of a sequence of samples from the continuous uniform distribution on  1 . the requirement that m is much larger than d avoids collisions and ensures with high probability  h vi  =1 h vj  for all i =1 j.
1.1 creating kmv synopsis and dv estimator
¡¡using the idea of the basic estimator introduced in   a kmv synopsis for a multiset s is created as described in : by applying the hash function h   to each value of ¦È s   the k smallest of the hashed values are recorded. this simple synopsis  e.g.  set ls of hashed values  is called kmv synopsis  for k minimum values .
¡¡in   the dv estimator for kmv synopses extends the basic estimator 1 and uses the following computation:
	d k =  k   1 /uk	 1 
¡¡it is shown that  under the assumption that d   k  this estimator is unbiased  in contrast to equation 1  and d k is used for the multiset operations described below. but  if d ¡Ü k  then it is easily possible to detect this situation and return the exact value of d from the synopsis.
1.1 multiset operations on kmv synopsis
¡¡so far  we have an estimator for kmv synopses. now  we focus on multiset operations using two or more kmv synopses in combination with estimating the compound set and present the union and intersection operation.
¡¡throughout  all synopses are created using the same hash function h : ¦È ¡ú1 {1 ... m} where ¦È denotes the data value domain appearing in the synopses and m = o |¦È|1 . ordinary set operations are denoted by {¡È ¡É} and multiset operations by {¡Èm ¡Ém}.
¡¡union operation: we assume two multisets a and b with their kmv synopses la and lb of size ka and kb  respectively. the goal is to estimate the number of distinct values in the union of a and b as d¡È = |¦È a¡Èm b| . here  ¦È s  denotes the set of dvs in multiset s. thus  d¡È can also be interpreted as d¡È = |¦È a  ¡È ¦È b |.
¡¡let l = la ¨’ lb be defined as the set including the k smallest values in la ¡È lb  where k = min ka kb  and l is the kmv synopsis of size k describing la ¡Èm lb. thus  by applying the dv estimator for kmv synopses  d¡È is estimated by following equation:
	d ¡È =  k   1 /uk	 1 
¡¡using the symmetric and associative operator ¨’  this result can be extended to multiple sets: l = la1 ¨’la1 ¨’¡¤¡¤¡¤¨’
lan estimates the number of dvs in a1¡Èma1¡Èm¡¤¡¤¡¤¡Èman.
¡¡intersection operation: as before  consider two multisets a and b with corresponding kmv synopses la and lb of sizes ka and kb  respectively. the goal is to estimate
d¡É = |¦È a ¡Ém b | = |¦È a  ¡É ¦È b |. set l = la ¨’ lb with l = {h v1  h v1  ... h vk }  where k = min ka kb . each value vi is an element of ¦È a  ¡È ¦È b . also set vl = {v1 v1 ... vk} and k¡É as follows:
	k¡É = |{v ¡Ê vl : v ¡Ê ¦È a  ¡É ¦È b }|	 1 
¡¡obviously  v ¡Ê ¦È a  ¡É ¦È b  if and only if h v  ¡Ê la ¡É lb such that k¡É can be computed from la and lb alone. k¡É is utilized to estimate d¡É using the jaccard distance ¦Ñ = d¡É/d¡È estimated by ¦Ñ  = k¡É/k  the fraction of sampling elements in vl   ¦È a ¡È b  that belong to ¦È a ¡É b . this leads to the proposed estimator:
	d ¡É =  k¡É/k  ¡¤  k   1 /uk	 1 
1. system architecture
¡¡our approximate if architecture consists of three components: the directory  the publishers and the subscribers. the notation used in the following sections is summarized in table 1.

|x|number of distinct documents in a multiset xdset of documents in the systemdiset of documents on publisher pia bindividual keysabkey set  both of a and b d a set of documents in d containing key adi a set of documents in di containing key adf a frequency of key a in d  = |d a | dfi a frequency of key a in di  = |di a | sy n a synopsis representing documents in d a sy ni a synopsis representing documents in di a d a directory node responsible for key atable 1: summary of notation
¡¡publishers. publishers are information sources that want to expose their content to the if system  and can be user blogs  digital libraries  or peers with local crawlers that perform focused crawling at portals of their interest. each publisher exposes its content to the system in the form of per-key statistics  called posts  about its local index. these statistics consist of inverted lists of documents relevant to the corresponding key  and are made available to the directory. the posts contain contact information about publishers  together with statistics to calculate quality measures for a given key  e.g.  frequencies . typically  such statistics include also the length of the inverted list and other quality measures to support the publisher ranking procedure carried out by subscribers. specifically  the size of the inverted list for a key - that is  the frequency for key a  or dfi a  - can be maintained in the form of a hash sketch or kmv synopsis; a publisher inserts an identifer for each document contained in its collection into a local hash sketch or kmv synopsis for this key to obtain sy ni a . the publishers have to update the directory statistics after a certain number of publications by sending the new summaries to the directory. finally  publishers are responsible for locally storing continuous queries submitted by subscribers and matching them against new documents they publish. more than one publisher may be used to expose the contents of large digital libraries  portals or blog hosting sites.
¡¡directory. the main functionality of the directory is to store compact  aggregated metadata about the publishers' local indexes  and make these statistics available when requested by the subscribers. this information will be used by the subscribers to determine which publishers are promising candidates to satisfy a given continuous query in the future. there are different ways to implement this type of directory  ranging from centralized solutions that emphasize accuracy in statistics and rely on server farms  to two-tier architectures  as in super-peer systems  that emphasize scalability and fault-tolerance. in our approach  we utilize a distributed directory maintained by super-peers that are organized using a chord dht  forming a conceptually global  but physically distributed directory that manages the statistics provided by the publishers in a scalable manner with good properties regarding system dynamics  e.g.  churn . the dht is used to partition the key space  such that every super-peer is responsible for the statistics of a randomized subset of keys within the directory. since there is a welldefined directory node responsible for each key  through the dht hash function   the synopses representing the index lists of all publishers for a particular key a are all sent to the same directory node d a . thus  d a  can compute a moving-window estimate for the global df value of a - df a  - by performing an union operation for all synopses sy ni a  sent by every publisher pi for key a.
¡¡subscribers. subscribers are information consumers that seek to satisfy their long-term information needs by subscribing to publishers that will publish interesting documents in the future. to subscribe to a potentially promising publisher p  a subscriber forwards to p the related continuous query  which is stored at the publisher side  and is matched with every new publication that takes place. subscribers utilize directory statistics to score and rank publishers  and this scoring is based on appropriate publisher selection and behavior prediction strategies  that utilize time-series analysis of statistics. to follow the changes in the publishing behavior of information producers  subscribers periodically re-rank publishers by obtaining fresh statistics from the directory  and use the new publisher ranking to reposition their continuous queries.
1 subscription protocol
¡¡let us assume that a subscriber s wants to subscribe with a continuous query containing multiple keywords. to do so  s needs to determine which publishers in the network are promising candidates to satisfy the continuous query q = {k1 k1 ... kn} with appropriate documents published in the future.
¡¡to collect statistics about the publishers  s needs to contact the directory to retrieve statistics for all keys ki 1 ¡Ü i ¡Ü n. based on the collected statistics  a ranking of publishers is determined and the highest ranked publishers are candidates for storing q. thus  only publications occurring in those publishers will be matched against q and create appropriate notifications. publishers that make available documents relevant to q  but not indexing q  will not produce any notification  since they are not aware of q. when the publishers that will store q have been determined  s sends the continuous query to them. a publisher p receiving q  will store q in its local database using appropriate query indexing mechanisms such as .
¡¡this baseline approach intersects the statistics for the single keys  i.e.  sends the continuous query only to  a subset of  the publishers that published  or will publish  statistics for all queried keys. however  this approach may lead to reduced recall  since a publisher appearing in publisher lists for both keys ki and kj will not necessarily publish documents containing both ki and kj. thus  to select an appropriate publisher for q  we have to consider the statistics of the key set to determine more accurately its future publishing behavior. obviously  the larger the subset of q we maintain statistics for  the more accurate our prediction about the behavior of the publisher will be.
