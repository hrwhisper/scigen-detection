in this paper we address the problem of evaluating xpath queries over streaming xml data. we consider a practical xpath fragment called univariate xpath  which includes the commonly used '/' and '//' axes and allows  -node tests and arbitrarily nested predicates. it is well known that this xpath fragment can be efficiently evaluated in o |d||q|  time in the non-streaming environment   where |d| is the document size and |q| is the query size. however  this is not necessarily true in the streaming environment  since streaming algorithms have to satisfy stricter requirement than non-streaming algorithms  in that all data must be read sequentially in one pass. therefore  it is not surprising that state-of-the-art stream-querying algorithms have higher time complexity than o |d||q| .
　in this paper we revisit the xpath stream-querying problem  and show that univariate xpath can be efficiently evaluated in o |d||q|  time in the streaming environment. specifically  we propose two o |d||q| -time stream-querying algorithms  lq and eq  which are based on the lazy strategy and on the eager strategy  respectively. to the best of our knowledge  lq and eq are the first xpath stream-querying algorithms that achieve o |d||oq || dtime performance. fur||q|  time performance-
ther  our algorithms achieve without trading off space performance. instead  they have better buffering-space performance than state-of-the-art stream-querying algorithms. in particular  eq achieves optimal buffering-space performance. our experimental results show that our algorithms have not only good theoretical complexity but also considerable practical performance advantages over existing algorithms.
categories and subject descriptors: h.1  database management : systems - query processing general terms: algorithms  theory.
keywords: xml  xpath  streams  query processing.

 this work was supported in part by nsf career award grant 1 and nsf iis grant 1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod'1  june 1  1  beijing  china.
copyright 1 acm 1-1-1/1 ...$1.
1.	introduction
　there has been a growing practical need for querying xml data efficiently. in many emerging applications  such as monitoring stock market data  subscribing to real-time news  and managing network traffic information  xml data are available in streaming form only. an essential difference between querying streaming xml data and querying nonstreaming xml data is that the former requires one-pass algorithms over unindexed xml data  in which all data have to be read sequentially and only once into memory.
1	preliminaries
  data model for xml streams
an xml document can be modeled as a rooted  labeled  and ordered tree  which we call xml data tree. each node in the data tree corresponds to an element  attribute  or text value in the xml document. an xml streaming algorithm accepts input xml documents as a stream of sax  events. two core sax events are startelement n  and endelement n   which are activated  respectively  when the opening or closing tag of a streaming element arrives  and accept the name of that element  n  as input parameter.
  xpath
xpath  is a popular language for querying xml data. it has been used in many xml applications and in some other languages for querying and transforming xml data  such as xquery  and xslt . in this paper we address a practical fragment of xpath called univariate xpath   see figure 1 . a key characteristic of univariate xpath is that in each predicate  a path can be compared with a constant  but not with another path. this paper does not treat explicitly the attribute axis    since it can be handled in a way similar to the child axis /.
　an xpath query specifies a twig pattern q to navigate an xml data tree d. we denote the document size  i.e.  the number of elements in d  by |d|  and the query size  i.e.  the number of nodes in q  by |q|. there are three types of nodes in q. there is exactly one result node  which specifies the output of xpath. all non-result nodes on the main path of q  i.e.  on the path from the root to the result node  are axis nodes. all other nodes are predicate nodes. figure 1 shows the twig pattern of an xpath query q: '//a /e /b /f //c /g/i and //h /d /j '1  where the single-line edges represent /-axes  the double-line edges repfigure 1: grammar of univariate xpath.
resent //-axes  and the result node 'd' is shaded. further  we define four sub-patterns of q w.r.t a given query node x in q  as illustrated in figure 1  where for qroot x  and qdown x   x can be any type of query node  and for qup x  and qself x   x is an axis node or result node  section 1 . we use these sub-patterns to explain our algorithms later.
　in this paper we focus on addressing the xpath querying problem  which requires outputting all nodes in d  answer nodes  that satisfy a specified xpath twig pattern q at its result node. we also address the xpath  ltering problem  which requires determining whether there exists at least one match of q in d. as we shall see in section 1  stream-filtering algorithms could be used as a basis for stream-querying algorithms.
　we define two usage-based memory-space classes for streaming algorithms:  1  caching space  which is the memory space dynamically allocated for the run-time stack s   section 1 .  1  bu ering space  as required by streamquerying algorithms  which is the memory space dynamically allocated for temporarily storing potential answer nodes. we measure the size of buffering space as the maximal number of potential answer nodes buffered at a time during the running time  and denote it by b. b might reach |d| in the worst case  which cannot be avoided by any streamquerying algorithms. a simple worst-case example is querying '//a /b /c' in an xml fragment of the form 'hai hc1i 1 h/c1i ...hcni n h/cni hbi 1 h/bi h/ai'  where c1 through cn have to be buffered until hbi arrives.
　we categorize streaming algorithms into two classes based on when they evaluate the predicates in queries. we say that a streaming algorithm is lazy if it evaluates the predicates only when the closing tags of streaming elements are encountered  and is eager if it does that as soon as an atom in a predicate is evaluated to true. the eager strategy usually helps save buffering space significantly. a simple best-case example is querying '//a /b /c' in an xml fragment of the form 'hai hbi 1 h/bi hc1i 1 h/c1i ...hcni n h/cni h/ai'. in the eager strategy  b = 1  since the predicate of a is evaluated  to true  as soon as hbi arrives. thus  each ci can be flushed as query result as soon as it arrives and does not need to be buffered at all. in the lazy strategy  b = n  since the predicate of a is not evaluated until h/ai arrives. thus  all c1 through cn have to be buffered. we will compare these two strategies in more detail in section 1.
  recursion in xml data
recursion  where some elements with the same name are nested on the same path in the data tree  occurs frequently in xml data in practice. for instance  among 1 dtds surveyed in   1 are recursive.  formally defines the recursion depth of an xml data tree d w.r.t query node q in q  denoted by rq  as the length of the longest sequence of nodes e1  ...  erq in d  such that  1  all the nodes lie on the same path  and  1  all the nodes match the sub-pattern qroot q . it is easy to see that rq ＋ dd  where dd is the maximum depth of d. we define the recursion depth r of

figure 1: sub-patterns of an xpath query q.
d w.r.t q as the maximum among all rq's  r ＋ dd . for example  for the data tree and query in figure 1  ra = rb = 1 and rc = rd = re = 1  and thus r = 1.
1 related work   non-streaming algorithms
a large amount of work has been done on efficiently querying indexed xml data  such as stacktree  and twigstack   which improve query i/o performance by avoiding access to query-irrelevant data. these algorithms first pre-partition xml documents into multiple inverted lists and encode the position of each element  and then perform structural joins among the query-relevant inverted lists. these algorithms are not suitable for querying streaming data  since they require preprocessing or indexing xml data.
　another body of work addresses evaluating xpath over unindexed xml data  where the focus is on cpu performance. gottlob et al.  identified an xpath fragment called core xpath  which can be evaluated in o |d||q|  time. core xpath is slightly more expressive than univariate xpath  in that it includes axes other than / and //. however  algorithms in  are not suitable for querying streaming data  since they require scanning xml documents in multiple passes. papers by gottlob et al.  and segoufin et al.  study the theoretical complexity of evaluating xpath.
  streaming algorithms
stream-filtering algorithms. considerable work has been done in the context of filtering xml streams with a collection of xpath expressions. the key idea for improving the filtering performance is to share common sub-expressions among multiple xpath expressions  e.g.  common path prefixes  yfilter    common predicates  xpush    or common substrings  xtrie    as much as possible. most of these algorithms are automata-based. yfilter  and the algorithm proposed in  mainly address linear path queries. for twig queries with predicates  yfilter uses an expensive post-processing step to join derived path tuples. xpush  supports twig queries with nested predicates  but the number of states in its automaton grows exponentially in |q| in the worst case  and thus it might incur exponential memory space costs. these automata-based algorithms are proposed in the context of stream filtering  and it is not clear how to extend them to efficient stream querying. stream-querying algorithms. peng et al.  developed an eager automaton-based xpath stream-querying system xsq. similarly to xpush   the number of states in its automaton is exponential in |q| in the worst case. xsq works in o |d| ， 1|q| ， k  time  where k is o r|q|  in the worst case  1  1 . xsq might have to buffer multiple physical copies of a potential answer node at a time  since  due to the recursion in xml data streams  an answer node might have multiple matchings with the query. bar-yossef et al.  studied the theoretical caching-space lower bound for evaluating xpath over streams  and also proposed a lazy stream-filtering algorithm s-stack  singlestack . s-stack uses a single run-time stack for caching  works in o |d| ， |q| ， r  time  and uses o |q| ， r  caching space for univariate xpath. it lays a foundation for turboxpath   a lazy stream-querying system for evaluating xquery-like queries. more recently  chen et al.  proposed a lazy stream-querying algorithm  twigm  to avoid the exponential time and space complexity incurred by xsq. twigm extends the multi-stack framework of the twigstack algorithm . in   it is shown that twigm can evaluate univariate xpath in polynomial time and space in the streaming environment. specifically  twigm works in o |d||q| |q| + dd ， b   time and uses o |q| ， r  caching space. however  like xsq  twigm might have to buffer multiple physical copies of a potential answer node at a time. xaos   based on a single-stack framework similar to the framework of s-stack  addresses evaluating xpath queries with both forward and backward axes. bar-yossef et al.  studied the theoretical buffering-space lower bound for evaluating multi-variate xpath over streams  and also proposed a stream-querying algorithm based on the eager strategy  which works for querying non-recursive xml streams only.
　some stream-querying systems for evaluating xquery queries have recently been developed  such as bea/xqrl   flux   and xsm .
1	motivations and contributions
　as we saw in section 1  univariate xpath can be evaluated in o |d||q|  time in the non-streaming environment . however  this excellent time performance has never been achieved by existing stream-querying algorithms. although the time performance of evaluating univariate xpath in the streaming environment has been improved from the
oexponential o |dd| ，b1  |qof twigm  it has not yet reached the| ， r|q|  of xsq to the polynomial
 |d||q| |q|+d ，
see in section 1  thefactors  such as 1o |d||q|  of the non-streaming environment. as we shallq	|dq|||qin xsq and|-independent time-complexitydd ， b in twigm 
　　　　　　　　　　　　| | ， r| could have a significant negative impact on the time performance of querying streaming xml data.
　therefore  a key question we ask here is: could univariate xpath be evaluated in o |d||q|  time in the streamd||q|-independenting environment  we observe that the | time-complexity factors of the existing stream-querying algorithms  such as r|q| in xsq and dd ， b in twigm  result from the fact that these algorithms have to exhaustively visit a large number of  and sometimes almost all  elements cached in run-time stack s  on the arrival of a streaming element in d. we believe that such nearly exhaustive exploration can be avoided. by carefully organizing and processing the elements cached in stacks and by carefully managing the buffered potential answer nodes in a multi-stack framework  we can obtain more efficient stream-querying algorithms that only need to visit a small portion of the relevant elements cached in stacks on the arrival of a streaming
we now summarize our main contributions.element in d  and thus achieve o |d||q|  time performance.
　 1  we show that univariate xpath can be efficiently evaluated in o |d||q|  time in the streaming environment  using either the lazy strategy or the eager strategy. specifically  we propose twolq and eq  which are based on the lazy strategy and on theo |d||q| -time stream-querying algorithms 

figure 1: query table for the query of figure 1.
eager strategy  respectively. to the best of our knowledge  lq and eq are the first xpath stream-querying algorithms that achieve o |d||q|  time performance.
　 1  we show that our algorithms are not only time-efficient but also space-efficient. both lq and eq have the same  |d|-independent  caching-space complexity o |q|，r  as the existing state-of-the-art stream-querying algorithms. in particular  eq tries to flush buffered query results out of memory as soon as possible  and achieves optimal buffering-space performance.
　 1  our extensive performance evaluation shows that our algorithms have not only good theoretical complexity but also considerable practical performance advantages over state-of-the-art stream-querying algorithms.
　the rest of this paper is organized as follows. in section 1 we propose a lazy stream-filtering algorithm lf  which lays a foundation for our lazy stream-querying algorithm lq described in section 1. in section 1 we propose an eager stream-querying algorithm eq. we report the results of an experimental performance evaluation in section 1. we conclude in section 1.
1.	lazy filtering algorithm  lf 
　we begin this section by introducing the query-preprocessing step  section 1 . in section 1 we propose a basic lf algorithm  which is extended in section 1 to a full lf algorithm that addresses queries with  -nodes  wildcards  or same-name nodes.
1	query preprocessing
　a query table tq  illustrated in figure 1  with size linear in |q|  is statically stored in memory throughout stream processing. each column in tq corresponds to a query node in q. column 1 represents a virtual node that is the parent of the root query node. a mapping table  implemented in the form of a hash table  is built over all  name  column number  pairs for retrieving the column number of any query node given the name of that node. we assume for now that q has no  -nodes or same-name nodes.
　tq includes the following fields for each query node cn.  1  axis: / or //.  1  type: result node  'r'   axis node  'a'   or predicate node  'p' . this field can be ignored for the filtering problem.  1  parent  the column number of the parent node of cn.  1  pos  which denotes the position of cn among its sibling nodes. in univariate xpath the order of sibling nodes is not significant  and we can select an arbitrary fixed order.  1  host  for the querying problem only  see section 1 .  1  p children  the set of all predicate-node children of cn.  1  f  the boolean formula defined w.r.t the predicatenode children of cn. we do not explicitly record f cn  in tq if and is the only operator in f cn   e.g.  for our example
algorithm 1: b-lf::startelement n 1 depth=depth+1;
1 cn = mapping n ; if cn =1	fail then lf::startblock cn ;
function mapping n 
1 hash id = hashing n ;
1 if mappingtable hash id .name = n then
1  return mappingtable hash id .column number;
1 else return fail;
procedure lf::startblock cn 
1 cp = parent cn ;	if stack cp  is empty then return;
1 p = top stack cp  ;
1 if p.flags pos cn   = 1 then
1 if axis cn  = '//' or p.depth+1 = depth then
1 if leaf cn  = true then
1 p.flags pos cn   = 1;
1
1 s = newelement stack cn   depth ;
1 push stack cn   s ;
figure 1: b-lf  query: '//a //d //b /e /c' .
queries with and predicates only .  1  leaf  which indicates whether cn is a leaf query node.  1  stack  pointer to a run-time stack. in our algorithms  one run-time stack is created for each non-leaf query node.
　note that the full version of tq shown in figure 1 is designed for the querying problem  section 1 . for the filtering problem we consider here  the p children field of each axis node should also include its axis-node/result-node child  since all query nodes are viewed as predicate nodes in the filtering problem.
　each element in stack cn  corresponds to an xml element e  data node  named n  and has two fields:  1  depth: the  integer  depth of e in the data tree.  1  flags: a bit array of size |p children cn |. given cm （ p children cn   e.flags pos cm   indicates whether a match with qdown cm  has been found under e. further   ags are partitioned into two groups  cflags and dflags  which correspond to / and // axis  predicate-node  children of cn  respectively. we use the following stack functions:  1  top stack  returns the top element of stack.  1  push stack element  pushes a new element into stack.  1  pop stack  pops out the top element of stack.  1  destroy element  recycles the memory space of a stack element.  1  evaluate element.flags f cn   computes f cn  based on the bits in flags of a stack cn  element.
1	the basic lf algorithm  b-lf 
　our basic lf algorithm  algorithms 1 and 1   b-lf  addresses queries without  -nodes or same-name nodes. for brevity  we assume that no value comparisons are involved in predicates. it is straightforward to extend our algorithms to handle value comparisons.
　initially  an element v is pushed into stack  the stack of the virtual query node at column 1  with v.depth = 1 and v.flags = 1. the global variable depth  which denotes the depth of the streaming element being processed in the data tree  is initially set to 1  and is incremented/decremented by 1 in each startelement n  / endelement n  event.
algorithm 1: b-lf::endelement n 1 cn = mapping n ; if cn =1	fail then lf::endblock cn ;
1 depth = depth-1;
procedure lf::endblock cn 
1 if leaf cn  = true or stack cn  is empty then return;
1 s = top stack cn  ;
1 if s.depth = depth then
1 pop stack cn  ;
1 if stack cn  is not empty then
1 q = top stack cn  ; q.dflags = q.dflags | s.dflags;
1 if evaluate s.flags  f cn   = true then
1 cp = parent cn ;
1 if cp = 1 then confirm that a match has been found;
1 else
1p = top stack cp  ; p.flags pos cn   = 1;
1if axis cn  = '//' then clearpredstacks cn ;
1	destroy s ;
procedure clearpredstacks cn 
1 if stack cn  is not empty then
1 destroy all elements in stack cn ;
1 for each ci in p children cn  do
1 if leaf ci  = false then clearpredstacks ci ;　the basic idea of startblock cn  is that an element qualifies for being pushed into stack cn   line 1  only if it has a match with qroot cn   lines 1 and 1 . newelement stack cn   depth   line 1  creates a new stack cn  element s with s.depth = depth  and initializes all bits in s.flags to 1. the basic idea of endblock cn  is evaluating s.flags  line 1  to determine whether a match with qdown cn  has been found under p  line 1   as well as passing all those bits in s.dflags that have been set to 1 down to the new top element of stack cn   line 1 . note that line 1  which calls clearpredstacks cn  to clear all elements cached in stack cn  and all descendant stacks of cn  can be removed without impacting the correctness of b-lf. the cleared stack elements will not need to be evaluated when their closing tags arrive in future. the correctness of the b-lf algorithm is intuitive1. we now illustrate it using a running example.
　example 1. figure 1 shows a running example with several snapshots of stacks  where the depth numbers of the stack elements are not shown for simplicity. when hg1i is read  g1 is simply discarded  since its tag name g is queryirrelevant  line 1 in mapping n  . when h/b1i is read  b1 is popped out of stack b   line 1 in endblock cn    and then a1.flags b  is set to 1 and clearpredstacks b  is called to destroy b1 in stack b   lines 1 in endblock cn  . when h/a1i is read  a1 is popped out of stack a   and a1.flags b  = 1 is passed down to a1.flags b   lines 1 in endblock cn  . note that b1 does not need to be pushed into stack b   since a1.flags b  = 1  line 1 in startblock cn  . finally  when h/a1i is read  a match at a1 is con rmed  since a1.flags d  = 1 and a1.flags b  = 1  lines 1 in endblock cn  .
recalling the definition of r in section 1  we can see that r represents the maximal length of all run-time stacks during stream processing. thus  the length of each stack is bounded from above by r. the caching-space complexity1of b-lf depends mainly on the number of flags bits in
algorithm 1: lf::startelement n 1 depth=depth+1;
1 c-sequence = mappingnametocolumns n ;
1 for each ci in c-sequence in the c-sequence order do
1lf::startblock ci ;
function mappingnametocolumns n 
1 hash id = hashing n ;
1 if mappingtable hash id .name = n then
1return mappingtable hash id .column sequence;
1 else return  -sequence;
figure 1:	mapping table  lf : name ★ column sequence.
all stack elements  which is， | | o pci（i q  r ， fanout ci   = o r q    where i q  is the set of all non-leaf query nodes in q. for the time complexity  startblock cn  works in o  max{fanout cn  1}   due to line 1 that initializes all bits in
s.flags to 1. endblock cn  also works in o max{fanout cn   1}   due to line 1 that passes down s.dflags  and to line 1 that evaluates s.flags.  note that line 1  which can be removed without impacting the correctness of b-lf  has o 1  amortized time cost for each cleared stack element  which will not need to be evaluated any more.  the bottleneck is the function mapping n   which takes only o 1  time if the mapping table is a well-implemented hash table  or o |q|  time if the mapping table is implemented as a naive sequential table. thus  b-lf has time complexity o |d||q|   or o |d|，fq  when using a well-hashed mapping table  where fq is the maximal fanout of q  fq ＋ |q|  .
1	the full lf algorithm  lf 
　it is easy to extend b-lf to address queries with  -nodes or same-name nodes. first  query preprocessing still creates one column for each query node. but each name in the mapping table corresponds to a sequence of column numbers whose corresponding query nodes either have that name or are  -nodes  see figure 1 . a special sequence for the column numbers of all  -nodes  called  -sequence  is also created. all nodes in column sequences follow a special order  such that each node must not have any of its ancestor nodes in front of itself. here we implement this order using the post-order of nodes in the query twig. our extended algorithm  algorithms 1 and 1   called lf  iteratively calls lf::startblock and lf::endblock described in algorithms 1 and 1.
　the intuition behind lf is that when a streaming element named n arrives  lf visits all query nodes named n and all  -nodes. the order of visiting those nodes is crucial. as described in algorithms 1 and 1  startblock ci  should be called in the c-sequence order  and endblock ci  should be called in the reverse c-sequence order  in order to prevent an element relevant to ci from seeing its own copy in stack parent ci  . for example  in case 1 in figure 1  when ha1i is read  a1.flags a  will be set to 1 if startblock a  is called after startblock     which will cause lf to eventually confirm an incorrect match at a1. in case 1 in figure 1  when h/b1i is read  d1.flags b  will miss being set to 1 if endblock b  is called before endblock     which will cause lf to eventually miss confirming a correct match at d1.
algorithm 1: lf::endelement n 1 c-sequence = mappingnametocolumns n ;
1 for each ci in c-sequence in the reverse c-sequence order do 1	 lf::endblock ci ;
1 depth=depth-1;
figure 1: lf: incorrect order of calling blocks  query: '//* //a /b/c' .
　as discussed in section 1  both lf::startblock ci  and lf::endblock ci  work in o fanout ci   time if ci is a nonleaf query node  and in o 1  time otherwise. since the in algorithms 1 and 1 work in timei | | o pci|（i| q  fanout ci + length of each c-sequence is bounded by q   both for-loops 1  = o  q    where l q /i q  is the set of all
c （l q 
leaf/non-leaf nodes in q. thus  algorithms 1 and 1 work in o |q|  time. further  as discussed in section 1  the length of each stack is bounded by r. therefore  lf has the same caching-space complexity o |q|，r  as b-lf  although some elements might have multiple copies in different stacks. note that in b-lf the sum of the lengths of all stacks is bounded by dd  while in lf the lengths of all stacks may reach dd at a time.
　theorem 1. for all queries in univariate xpath  the lf algorithm correctly determines whether there exists a match of q with d. it has time complexity o |d||q|  and cachingspace complexity o |q| ， r .
1.	lazy querying algorithm  lq 
　in this section  we extend our lazy stream-filtering algorithm lf to a lazy stream-querying algorithm lq. lq works in o |d||q|  time and uses o |q|，r  caching space. that is  the time and caching-space complexity of lq are not higher than those of lf  although the querying problem seems to be more complex than the filtering problem. we begin by introducing in section 1 a query table  which is a slight extension of the table of section 1. next  in section 1 we introduce algorithm u-lq in which the answer nodes might be output not in the document order. in section 1 we extend u-lq to the full lq algorithm that outputs answer nodes in the document order.
1	query preprocessing
　in addition to the fields used in lf  the query table in lq includes two more fields.  1  type  as described in section 1.  1  host  for axis nodes only  which records the column number of the segment host of an axis node. specifically  we partition the main path of q into multiple segments by removing all // edges on it. the host of a segment is just the axis node at the tail of that segment. meanwhile  we restrict the segment including the result node to not have a host. for example  for the query in figure 1  its main path '//a/b//c/d' is partitioned into two segments: 'a/b' and 'c/d'  where host a  = host b  = b  and c and d have no host.
　unlike lf  lq requires buffering space for storing potential answer nodes  since lq serves the querying purpose.
algorithm 1: procedure u-lq::startblock cn  id 1 cp = parent cn ; if stack cp  is empty then return; 1 p = top stack cp  ;
1 if type cn  =1	'p' or p.flags pos cn   = 1 then
1 if axis c   = '//' or p.depth+1 = depth then
1 if leaf cn  = true then
1 if type cn  = 'p' then
1p.flags pos cn   = 1;
1	else if type cn  = 'r' then
1b = newbuffernode id ;
1if cp = 1 then flushnode b ;
1else appendnode p.list  b ;
1
1	if type cn  = 'p' or type cn  = 'a' then 1s = newelement stack cn   depth ;
1 else s = newelement stack cn   depth  id ;
1 push stack cn   s ;
figure 1: u-lq  query: '//a //d /b /e //c' .
specifically  lq creates two more fields for some stacks.  1  list  for the stacks of axis nodes only  which is a pointer to the head of a list that is used to buffer potential answer nodes.  1  id  for the stack of the result node only  if the result node is a non-leaf node   which records the id of a possible answer node.
1	the unordered lq algorithm  u-lq 
　similarly to algorithms 1 and 1  u-lq iteratively calls procedures startblock and endblock  algorithms 1 and 1  in the c-sequence order and in the reverse c-sequence order  respectively. for xpath queries  an answer node might be output in the form of its text value  its unique node id  if available   or the xml fragment rooted at it. here for brevity we assume that the ids of all answer nodes are available  and all answer nodes are output in the form of their ids. it is easy to extend our algorithms to produce outputs in either of the other two forms.
　it is easy to see that u-lq shares substantial portions of code with lf  since u-lq processes predicate nodes in the same way as lf does.  recall that in lf all query nodes are considered predicate nodes.  however  u-lq has extra code for processing axis nodes and result node. we define a potential answer node e as a data node that corresponds to the result node of q and has been found to have a match with qdown ck   where ck is some axis node or result node of q. if ck is the root node  then e is a real answer node. u-lq creates exactly one physical copy for each potential answer node  and buffers it in the list of some stack element. u-lq works in such a way that all nodes buffered in the list of a stack ci  element  where ci is an axis node  have been found to have a match with qdown ci+1   where ci+1 is an axis node or result node with parent ci.
　u-lq includes several additional functions not used in lf.  1  newbuffernode id  buffers a potential answer node in the form of its id.  1  appendnode list node  appends a potential answer node to the tail of list.  1  appendlist list1 
algorithm 1: procedure u-lq::endblock cn 1 if leaf cn  = true or stack cn  is empty then return; 1 s = top stack cn  ;
1 if s.depth = depth then
1 pop stack cn  ;
1 if stack c   is not empty then
1 q = top stack cn  ; q.dflags = q.dflags | s.dflags;
1 if evaluate s.flags  f cn   = true then
1 = parent cn ;	p = top stack cp  ;
1 if type cn  = 'p' then
1 p.flags pos cn   = 1;
1 if axis cn  = '//' then clearpredstacks cn ;
1 else if type cn  = 'a' then
1 p = 1 then flushlist s.list ; 1 else appendlist p.list  s.list ;
1 else if type cn  = 'r' then
1 b = newbuffernode s.id ;
1p = 1 then flushnode b ;
1	else appendnode p.list  b ;
1
1 if type cn  = 'a' then
1 if stack ch = host cn   is not empty then
1h = top stack ch  ;
1appendlist h.list  s.list ;
1	else destroylist s.list ;
1list1  appends list1 to the tail of list1.  1  destroylist list  recycles the memory space of all nodes in list.  1  flushlist  list  flushes all answer nodes in list from memory to the user.  1  flushnode node  flushes an answer node from memory to the user.
example 1. figure 1 shows a running example for u-
lq  which returns sequence c1c1c1 as answer nodes. when hc1i is read  c1 is appended to b1.list  line 1 in algorithm 1 . when h/b1i is read  b1.list = c1 is appended to a1.list  line 1 in algorithm 1   since b1's predicate is a successful match. when h/a1i is read  the failure to match a1's predicate causes a1.list = c1 to be appended to b1.list  since stack b = host a   is not empty  lines 1 in algorithm 1 . when h/a1i is read  the successful matching of a1's predicate causes c1 in a1.list to be  ushed to the user  line 1 in algorithm 1 . finally  when h/a1i is read  c1c1 in a1.list are  ushed to the user since a1's predicate is a successful match.
　now  suppose a1's predicate fails to match when h/a1i is read. then  c1c1 in a1.list will be destroyed  since stack b = host a   is now empty  line 1 in algorithm 1 .
note that unlike the existing stream-querying algorithms  u-lq does not need to exhaustively visit a large number of stack elements when a streaming element in d arrives. specifically  startblock cn  id  only needs to visit at most one stack element  p  line 1   and endblock cn  only needs to visit at most three stack elements: s  line 1   q  line 1   and p  line 1  or h  line 1 . that is  the time performance of u-lq never depends on the stack length  i.e.  on the recursion depth r. therefore  compared to the existing stream-querying algorithms  a very nice property of u-lq is that recursion in xml data streams has no impact on its time performance. thus  u-lq does not have the r-relevant time complexity factors of the existing stream-querying algorithms  such as r|q| of xsq and dd ，b of twigm  section
1 .

figure 1: lq  same query/data as in figure 1 .
　specifically  u-lq has the same time complexity o |d||q|  as lf. the flushlist list  and destroylist list  functions have o 1  amortized time cost for each node in list. in fact  after a potential answer node is buffered  it is visited only exactly once - when it is finally either flushed to the user by flushlist list  or destroyed by destroylist list . the other additional functions  such as appendlist list1  list1   take exactly o 1  time. also  it is easy to see that u-lq has the same caching-space complexity o |q| ， r  as lf. however  unlike lf  u-lq requires buffering space for storing potential answer nodes in the lists of stack elements. it is easy to see that u-lq uses no more buffering space than any lazy stream-querying algorithm. in particular  unlike twigm and xsq  section 1   which might have to buffer multiple physical copies of a potential answer node at a time  u-lq needs to buffer only exactly one physical copy for each potential answer node. therefore  the bufferingspace complexity of u-lq is o |d|  in the worst case.  see the worst-case example in section 1. 
1	the full lq algorithm  lq 
　the lq algorithm  which outputs answer nodes in the document order  is basically the same as u-lq. the main difference is that lq creates a global queue to collect all possible answer nodes in the document order. specifically  in lq each buffered node still has only one physical copy  but it might be linked into both the global queue and the list of some stack element  based on the double-link strategy illustrated in figure 1. also  each buffered node has an extra con rmed bit  which indicates whether this node has been confirmed to be an answer node or not. flushlist list   as well as flushnode node   in lq does not flush answer nodes in list to the user immediately as u-lq does  since at this time some other nodes in the queue that are in front of the nodes in list might have not been confirmed whether to be answer nodes or not. thus  flushlist list  in lq just flips the confirmed bits of the nodes in list from 1 to 1  and then calls function tryflushingqueue    which tries to sequentially flush the nodes in the queue  beginning from the head of the queue  until a node with confirmed = 1 is encountered. destroylist list  in lq unlinks those nodes in list from the queue  recycles their memory space  and then calls tryflushingqueue  . the reason is  after the removal of those nodes in list from the queue  some answer nodes in the queue that have been confirmed earlier might be able to reach the head of the queue  and thus can be flushed to the user immediately.
　example 1. figure 1 shows a running example for lq. when h/a1i is read  c1 in a1.list are con rmed as answer nodes  and have their confirmed bits  ipped to 1 by flushlist a1.list . but the head of the queue  c1  still has confirmed = 1. thus  no answer nodes can be  ushed to the user at this time. all answer nodes will be  ushed only when c1c1 in a1.list have their con rmed bits  ipped to 1 by flushlist a1.list  when h/a1i arrives.
　now  suppose b1's predicate fails to match  then c1c1 in b1.list would be unlinked from the queue by destroylist  b1.list  when h/b1i is read. as a result  c1 and c1 can be  ushed to the user immediately  since c1 has reached the head of the queue.
since the global queue collects nodes in the document order  all answer nodes output by lq are in the document order. it is easy to see that while lq does not have extra time or space complexity compared to that of u-lq  in practice lq might use more buffering space than u-lq  since lq might have to delay flushing some confirmed answer nodes in order to output the answer nodes in the document order.
　theorem 1. for all queries in univariate xpath  the lq algorithm correctly outputs all answer nodes in the document order. it has time complexity o |d||q|   caching-space complexity o |q|，r   and bu ering-space complexity o |d| .
theorem 1 shows that univariate xpath can be efficiently evaluated in the same o |d||q|  time in the streaming environment as in the non-streaming environment .
1.	eager querying algorithm  eq 
　although lq achieves o |d||q|  time performance  it does not reach optimal buffering-space performance. it can be seen from figure 1 that  once d1 arrives  there is enough information to confirm buffered nodes  c1 and c1  as answer nodes. thus  they can be flushed to the user immediately. also  c1  c1  and c1 can be confirmed as answer nodes and be flushed as soon as they arrive. that is  the size of buffering space can be reduced from 1 nodes in u-lq  or from 1 nodes in lq  to 1 nodes. the reason for the lower buffering-space performance of lq is that it lazily evaluates the predicates only when the closing tags of the streaming elements are encountered  line 1 in algorithm 1 . motivated by this  in this section we propose an eager stream-querying algorithm eq  which improves buffering-space performance  while not trading off time performance.
1	query preprocessing
　the query table for eq includes two extra fields:  1  axis child cn   for axis nodes only  which records the column number of the axis-node/result-node child of cn.  1  pf cn   predicate fanout   which is the size of p children cn .
　eq uses four more fields for some stacks.  1  the parent pointer. given a stack cn  element e  e.parent is a pointer to the closest ancestor of e among all stack parent cn   elements. the left arrows between stacks in figure 1 illustrate parent pointers.  1  the child pointer  for the stacks of axis nodes only. given a stack cn  element e1  e1.child points to the closest descendant  say e  of e1 among all stack axis child cn   elements such that e.parent = e1  and is null if such e does not exist. the right arrows between stacks in figure 1 illustrate child pointers. for example  b1.child = c1 while b1.child = null and b1.child = null  since c1.parent = b1. note that in figures 1 and 1  all stack elements come from the same path in a data tree  and the subscript number of each stack element indicates  but is not exactly1  the depth of that element in the data tree. for simplicity  we do not show parent pointers in figure 1: u-eq: bottom up evaluate u .
figure 1; they can be inferred from the subscript numbers of stack elements.  1 - 1  the self and up bits  for the stacks of axis nodes and result node only. given a stack cn  element e  e.self indicates whether e has a match with qself cn   i.e.  whether e.flags has been evaluated to true  and e.up indicates whether e has a match with qup cn   where qself cn  and qup cn  are as illustrated in figure 1.
1	algorithm
we first describe our u-eq algorithm  in which answer nodes might be output not in the document order. similarly to algorithms 1 and 1  u-eq iteratively calls procedures startblock and endblock  algorithms 1 and 1  in the c-sequence order and in the reverse c-sequence order  respectively. here we address queries with and predicates only. it is straightforward to extend u-eq to address queries with the and  or and not operators.
　the main work of u-eq is lines 1 in algorithm 1. in line 1  bottom up evaluate cn   bue  eagerly evaluates a predicate as soon as an atom in that predicate becomes true. bue works in a bottom-up way. figure 1 illustrates this process  in which the ● symbol in a white square indicates that that bit is being flipped from 1 to 1 by this bue call  and the left arrows with numbers indicate the go-forward path of this bue call. also  the path from node b to node s in the query is logically partitioned into three segments by removing all // edges on that path. the main idea of bue is that given a stack cn  element e  bue evaluates e.flags as soon as a corresponding bit in e.flags is flipped to 1. if e.flags is evaluated to true  in this case we say that e becomes activated   then bue goes forward to e.parent and flips e.parent.flags pos cn   to 1. otherwise  bue has to go down to the lower segment-tail element. in figure 1  u1 flips s1.flags u  to 1  which activates s1. thus  bue goes forward to p1. but p1 cannot be activated  and thus bue has to go down to s1. the remaining process follows similar rules. when the axis-node elements b1 and b1 become activated  their self bits are flipped to 1  and then they are pushed into astack that is eventually returned by this bue call. finally  clearpredstacks● k i1  is called  which destroys all those stack k  elements whose depth is smaller than i1.depth  and then recursively calls clearpredstacks● m k1  and clearpredstacks● l k1 . bue terminates as soon as it finds that i1.flags k  has been set to 1 earlier  which implies that no more elements will become activated even if bue continues .
　in line 1  top down propagate astack   tdp  processes the axis-node elements returned by bue  whose self bits were just flipped to 1 in bue   as illustrated in figure 1. if the up bits of those elements have been set to 1  then tdp will try flipping the up bits of their corresponding descenfigure 1: u-eq: top down propagate b1 .

figure 1: u-eq  same query/data as in figure 1 .
dant elements to 1 in a top-down way. specifically  given a stack cn  element e  tdp checks the value of e.self as soon as e.up is flipped to 1. if e.self has been set to 1  in this case we say that e becomes activated   then tdp goes forward to e1.child in stack axis child cn   and flips e1.child.up to 1  where e1 is the lowest element in stack cn  that is e itself or e's descendant with e1.child =1 null. otherwise  bue has to go up to the upper segment-head element. in figure 1  initially  b1 is activated. then tdp goes forward to b1.child  c1  since b1.child and b1.child are null  before clearaxisstacks b b1  is called to destroy those stack a  and stack b  elements whose depth is greater than b1.depth. meanwhile  all nodes buffered in the lists of the cleared elements and of b1 can be flushed immediately as answer nodes  since  1  b1 has matches with qup b  and qself b   and  1  these buffered nodes have matches with qdown c . also  clearpredstacks◎ a a1  and clearpredstacks◎ b b1  are called to recursively destroy the corresponding upper elements in the stacks of the descendant predicate nodes of a and b  based on a1.depth and b1.depth  respectively. next  c1.up is flipped to 1. but c1 cannot be activated  and thus tdp goes up to c1. the remaining process follows similar rules. note that tdp will terminate as soon as it reaches an element whose up bit has been set to 1 earlier  which implies that no more elements will become activated even if tdp continues .
　example 1. figure 1 shows a running example for ueq. when hd1i is read  bue d   line 1 in algorithm 1  is called and returns a1. then  tdp a1  is called  line 1 in algorithm 1 : a1 becomes activated  rst  and then b1 becomes activated. finally  a1 and b1 are cleared  and c1 in b1.list and c1 in b1.list are  ushed as answer nodes. later  when c1  c1  and c1 arrive  they are  ushed immediately  since b1.up = 1 and b1.self = 1  line 1 in algorithm 1 . as a result  throughout the processing  u-eq only needs to bu er at most 1 nodes  c1 and c1  rather than 1 nodes in u-lq and 1 nodes in lq  see examples 1 and 1 .
　now  suppose that edge  b1  c1  in the data tree in figure 1 is extended to a path b1-ax-by-c1; ax and by never need to be pushed into stacks  because when ax arrives  b1.up = 1 and b1.self = 1  lines 1 in algorithm 1 . thus  c1 will still be  ushed as soon as it arrives.
　note that u-eq::startblock cn  id  seems to have particularly high time costs  since both bue and tdp might visit a lot of elements in stacks. however  such costs are amor-
algorithm 1: procedure u-eq::startblock cn  id 1 cp = parent cn ; if stack cp  is empty then return; 1 p = top stack cp  ;
1 if type cn  =1	'p' or p.flags pos cn   = 1 then
1 if axis c   = '//' or p.depth+1 = depth then
1 if leaf c   = true then
1 if type cn  = 'p' then
1 astack = bottom up evaluate cn ; 1	top down propagate astack ;
1 else if type cn  = 'r' then
1 b = newbuffernode id ;
1 if p.up & p.self = 1 then flushnode b ;
1 else appendnode p.list  b ;
1
1 if type cn  = 'p' then
1 s = newelement stack cn   depth ;
1 else if type cn  = 'a' then
1 if stack ch = host cn   is not empty then
1 h = top stack ch  ;
1 if h.up & h.self = 1 then return;
1 s = newelement stack cn   depth ;
1 else s = newelement stack cn   depth  id ;
1
1 if type cn  = 'a' or type cn  = 'r' then
1 s.up = p.up & p.self;
1 if pf cn  = 1 then s.self = 1;
1 else s.self = 1;
1 s.child = null;
1 if p.child = null then p.child = s;
1 push stack cn   s ;tized when the closing tags of these stack elements are processed in future: u-eq::endblock cn  works in o 1  time  since it does not need to do the work of passing / evaluating s.flags as lines 1 in algorithm 1 do1: all such work has been transferred into bue in u-eq::startblock cn  id .
　further  the key point for bue is that during the lifetime of each stack cn  element e  each bit in e.flags is flipped from 1 to 1 at most once  by some bue call   since bue terminates as soon as it finds that the corresponding bit in e.flags that it is trying to flip to 1 has been set to 1 earlier. each such flip action causes e.flags to be evaluated once  to test whether e will be activated. that is  e.flags is evaluated at most pf cn  times. we can create a counter field for each stack element e  to record the number of bits in e.flags that have been set to 1. the value of e.counter is initialized to 1 and is incremented by 1 every time a bit in e.flags is flipped to 1. then  e.flags can be evaluated in o 1  time by simply checking whether e.counter = pf cn . therefore  on the whole  each stack cn  element takes only o max{pf cn  1}  time for the bue computation during its lifetime  although a bue call may visit many stack elements. similarly  each axis-node/result-node stack element e takes o 1  time for the tdp computation during its lifetime  since e.up is flipped from 1 to 1 at most once.  recall that tdp terminates as soon as it finds that an up bit that it is trying to flip to 1 has been set to 1 earlier.  otherwise  as in lf and lq  all clearstacks functions in bue and tdp have o 1  amortized time cost for each cleared stack element  which does not need to be processed further in future. therefore  on the whole  each stack element has o max{pf cn  1}  time cost in u-eq  as in lq.
algorithm 1: procedure u-eq::endblock cn 1 if leaf cn  = true or stack cn  is empty then return; 1 s = top stack cn  ;
1 if type c   = 'a' or type cn  = 'r' then
1 n	p = top stack cp  ;
1 if p.child = s then p.child = null;
1 if type c   = 'a' then
1 if s.self = 1 then appendlist p.list  s.list ;
1
1 if stack ch = host cn   is not empty then
1 h = top stack ch  ;
1 appendlist h.list  s.list ;
1 else destroylist s.list ;
1　it is easy to see that u-eq has the same caching-space complexity o |q| ， r  as lq. also  similarly to lq  u-eq has o |d|  buffering-space complexity in the worst case. such complexity is unavoidable for any stream-querying algorithm.  see the worst-case example in section 1.  however  unlike lq  u-eq is able to save buffering space significantly  as illustrated by the simple best-case example in section 1 and by example 1. in fact  u-eq achieves optimal buffering-space performance  since it always flushes confirmed answer nodes out of memory as soon as possible  and never buffers any answer nodes unnecessarily. further  similarly to the procedure of section 1  we can extend ueq to a full eq algorithm  called eq  which outputs answer nodes in the document order by using a global queue. this does not incur extra complexity over that of u-eq.
　theorem 1. for all queries in univariate xpath  the eq algorithm correctly outputs all answer nodes in the document order. it has time complexity o |d||q|   has cachingspace complexity o |q| ， r   and achieves optimal bu eringspace performance.
　theorem 1 shows that for the problem of evaluating univariate xpath over streams  optimal time performance and optimal space performance can be achieved at the same time.
1.	experimental results
1	experimental setup
　in this section we compare the performance of our algorithms with that of two state-of-the-art xpath streamquerying systems  twigm  and xsq . these two systems  in particular twigm  have shown comprehensive performance advantages over many stream-querying systems . other xpath or xquery stream-querying systems  such as bea/xqrl   turboxpath   xaos  and xsm   are not publicly available at this time  while some publicly available xpath or xquery querying systems  such as galax   xmltaskforce  and saxon   use nonstreaming algorithms. xsq is an open-source system   while twigm is not publicly available at this time; we implemented it based on the algorithm described in . all the algorithms were implemented using java 1.1  and called the same sax xml parser  xerces1-java xml parser 1.1  . we ran all the experiments on a 1ghz pentium1 machine with 1gb memory running windows server 1.
figure 1: datasets for the experimental evaluation.
figure 1: queries for the experimental evaluation. we tested the performance of all algorithms on three datasets  figure 1 :  1  the book dataset  which is a synthetic dataset generated using ibm's xml generator   with numberlevels = 1 and maxrepeats = 1   based on a real book dtd from w1c xquery use cases . the book
dtd includes only one recursive element  section. that is  different section nodes can be nested on the same path in the data tree.  1  the treebank dataset   which is a real dataset with a narrow and deeply recursive structure that includes multiple recursive elements.  1  the xmark dataset  with factor = 1    which is a well-known benchmark dataset. this dataset does not include recursive elements.
　on each of the three datasets  we tested 1 queries  as shown in figure 1. these queries include / and // axes   -node tests  and predicates. we used queries with and operators only  since neither twigm nor xsq support other operators  and xsq even does not support and operators. further  xsq does not support same-name nodes or  -node tests  and requires that each axis node have at most one  /-axis  predicate node child. thus  some queries listed in figure 1 are not supported by xsq  and we use n/a to indicate such cases in figures 1 and 1.
1	time performance
　we tested the cpu time performance of the stream-querying algorithms  measuring it as t = ttotal  tin  tout  where ttotal is the total running time  tin is the time taken by reading  from disk into memory  and parsing xml documents  and tout is the time taken by outputting the query results from memory to disk. figure 1 shows the cpu time performance of our algorithms  of twigm  and of xsq on the three datasets. from this figure we can see that eq and lq have the best time performance among all the algorithms. this performance advantage is rather intuitive: as shown in theorems 1 and 1  both lq and eq have o |d||q|  time complexity  and thus other factors  such as recursion in xml data or the size of buffering space  have no impact on their time performance. we now summarize several main observations based on figure 1.
　 1  xsq has much higher time costs than the other algorithms in all applicable test cases. this is intuitive  since xsq has exponential time complexity o |d|，1|q|，r|q|   section 1   while eq  lq  and twigm each have polynomial time complexity. in particular  in presence of deep recursion in xml data  e.g.  when evaluating q1 or q1 on the treebank dataset  figure 1  b    xsq reports  there are too many path combinations for one element   and terminates running. the reason is  xsq exhaustively enumerates all potential main-path pattern matches for each potential answer node  and the number of such matches might reach r|q| in the worst case .
　 1  eq and lq show different degrees of time-performance advantages over twigm on different datasets.
　on the book dataset  eq and lq have a marked performance advantage over twigm for q1 through q1. as we can observe from the time complexity o |d||q| |q|+dd ，b   of twigm  section 1   the size of the run-time buffering space has a significant impact on the time performance of twigm. figure 1  a  shows the buffering-space size of twigm for q1 through q1 on the book dataset. from this figure we can see that twigm has high buffering-space costs for q1 through q1  which results in its high time costs for these queries. note that o |d||q| |q|+dd，b   serves only as the theoretical upper bound of the time complexity of twigm. it does not imply that each streaming element in d requires o b  processing time. in fact  there might be only a few query-relevant streaming elements that require o b  processing time  for the expensive set-union operation  that is used to eliminate duplicate copies of buffered nodes . in particular  most query-irrelevant streaming elements  such as g1 in figure 1  can be simply discarded in o 1  time by hashing. on the other hand  twigm's time performance is similar to that of eq and lq on q1  since the book dataset is not recursive w.r.t q1  the axis of section in q1 is / . when there is no recursion  twigm does not need to buffer duplicate copies of potential answer nodes  and thus does not incur the o b  set-union costs anymore. that is  in non-recursive cases  the size of the run-time buffering space has no impact on the time performance of twigm. thus  twigm has the same o |d||q|  time complexity as eq and lq in non-recursive cases.
　on the treebank dataset  the performance advantage of eq and lq over twigm is not as pronounced as on the book dataset. the reason is  while treebank is deeply recursive  it is a very narrow dataset  on which twigm has very low buffering space costs  as indicated by figure 1  b . despite this  eq and lq still have a performance advantage over twigm on this dataset  mainly due to the impact of the deeply recursive structure of this dataset on the time performance of twigm.
　on the xmark dataset  twigm's time performance is very similar to that of eq and lq  since xmark is a non-recursive complexity in non-recursive cases.dataset. as discussed above  twigm has o |d||q|  time
　 1  eq and lq show stable time performance for different queries in each fixed dataset  except for some queries involvingand lq have markedly higher costs for -node tests. for example  in figure 1  b   eqq1 than for other queries. the reason is  for such queries all streaming eleements can be simply discarded during the stream-queryingments become query-relevant  w.r.t.  -nodes . thus  no elprocess.

figure 1: query time1.

figure 1: size of buffering space  number of nodes 1.　 1  our two o |d||q| -time stream-querying algorithms  eq and lq  show very similar time performance in practice  as can be seen from figure 1. the maximum and average difference between their time performance is 1% and 1%  respectively.
1	memory space performance
　the caching-space costs of stream-querying algorithms depend on the number of elements cached in the run-time stack s   which is bounded by the maximum document depth dd when queries do not involve  -nodes or same-name nodes  and does not exceed |q| ， dd in the worst case. many practical xml documents are not very deep. thus  the cachingspace costs of stream-querying algorithms are almost always negligible in practice. as we observed in our test cases  the number of stack elements cached by each algorithm does not exceed 1 on the book dataset and 1 on the treebank dataset at any running time  and in particular  is at most 1 on the xmark dataset  since each stack only needs to cache at most one corresponding element in non-recursive cases. on the other hand  stream-querying algorithms could typically have very high buffering-space costs in practice  since hundreds of thousands of potential answer nodes might have to be buffered  see figures 1  a  and  c . therefore  the run-time memory usage of stream querying is typically dominated by buffering space. in figure 1 we compare the buffering-space size of eq  lq  twigm  and xsq1. note that twigm and xsq might buffer multiple physical copies of a potential answer node at the same time. thus  their buffering-space size is measured using the maximal number of physical copies of potential answer nodes buffered at the same time during the running time. we now summarize several main observations based on figure 1.
　 1  buffering-space costs could be very large in practice  depending on the structure of xml data  on the specific queries  and on the query-evaluation strategy. for example  for q1 in figure 1  the lazy algorithms lq and twigm have to buffer almost 1k nodes. the reason is  the open auctions element in the xmark dataset has a very large number of open auction child elements  most of which have a privacy child element. therefore  q1 has a very large number of answer nodes in this dataset. the lazy strategy has to buffer all these answer nodes until h/open auctionsi arrives. it is easy to see that there will be a lot more  than 1k  nodes that need to be buffered when q1 is lazily evaluated on other larger xmark datasets  with higher value of the factor parameter . on the other hand  queries on the treebank dataset need very little buffering space  since treebank is a narrow dataset.
　 1  eq always has the lowest buffering-space costs among all the algorithms. this is intuitive; as we discussed in section 1  eq always flushes buffered answer nodes as soon as possible  and thus achieves optimal buffering-space performance. although xsq also eagerly evaluates queries  it sometimes has to buffer multiple physical copies of some potential answer nodes. thus  xsq could have higher bufferingspace costs than eq in some cases  e.g. for q1 in figure 1  a . both lq and twigm use the lazy strategy to evaluate queries  and therefore have higher buffering-space costs than eq in most test cases. however  compared to lq  twigm might have to sometimes buffer multiple physical copies of some potential answer nodes. thus  twigm typically has higher buffering-space costs than lq  see figures 1  a  and  b . note that compared to other algorithms  the amount of buffering space eq saves could be very large. for example  for q1 in figure 1  c   eq only needs to buffer at most 1 nodes  while lq and twigm have to buffer almost 1k nodes.
　 1  in non-recursive cases  eq and xsq have the same buffering-space costs  while lq and twigm have the same buffering-space costs  as shown in figure 1  c . the reason is  both eq and xsq use the eager strategy to evaluate queries  while both lq and twigm use the lazy strategy to evaluate queries. in non-recursive cases  xsq and twigm do not have to buffer multiple physical copies of potential answer nodes.
1	summary
our experiments illustrate two points.
　 1  our algorithms eq and lq show the best time performance among all the tested algorithms in practice  which is consistent with their good o |d||q|  worst-case time complexity. in presence of recursion  they have a marked timeperformance advantage over the state-of-the-art stream-querying algorithm twigm. at the same time  they guarantee time performance that is similar to that of twigm in the absence of recursion.
　 1  eq shows the best buffering-space performance among all the tested algorithms in practice  which is due to the fact that eq eagerly evaluates queries and buffers only one physical copy of each potential answer node. in particular  compared to the other algorithms  the amount of buffering space eq saves could be very large.
1.	conclusion
　in this paper we revisited the xpath stream-querying problem and showed that a practical xpath fragment  univariate xpath   can be efficiently evaluated in o |d||q|  time in the streaming environment. specifically  we proposed two stream-querying algorithms  lq and eq  which are based on the lazy strategy and on the eager strategy  respectively. to the best of our knowledge  our algorithms are the first xpath stream-querying algorithms that guarantee
eq achieveo |d||q|  worst-case time performance. moreover  lq ando |d| |q|  time performance without trading off space performance. instead  they have better bufferingspace performance than the state-of-the-art algorithms that use the lazy or eager strategy  respectively. in particular  eq achieves the optimal buffering-space performance.
　we showed that our algorithms are not only of theoretical value. the results of our extensive performance evaluation show that our algorithms have considerable practical time and space performance advantages over the state-ofthe-art algorithms in presence of recursion. when there is no recursion  our algorithms' performance is guaranteed to be similar to that of the state-of-the-art algorithms. we have observed that our algorithms lq and eq show similar time performance in practice  but eq has better bufferingspace performance than lq. therefore  we select eq as the best-performance representative of our algorithms. we are currently extending eq to evaluate more expressive classes of xml queries over streams  such as multi-variate xpath   xpath with backward axes   and xquery-like queries
.
