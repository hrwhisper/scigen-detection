we propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy  while dealing in realtime with the query volume of a commercial web search engine. we use a blind feedback technique: given a query  we determine its topic by classifying the web search results retrieved by the query. motivated by the needs of search advertising  we primarily focus on rare queries  which are the hardest from the point of view of machine learning  yet in aggregation account for a considerable fraction of search engine traffic. empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. we believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval- relevance feedback  search process
general terms
algorithms  measurement  performance  experimentation
keywords
query classification  web search  blind relevance feedback
1. introduction
　in its 1 year lifetime  web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. one thing  however  has remained constant: people use very short queries. various studies estimate the average length of a search query between 1 and 1 words  which by all accounts can carry only a small amount of information. commercial search engines do a remarkably good job in interpreting these short strings  but they are not  yet!  omniscient. therefore  using additional external knowledge to augment
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
the queries can go a long way in improving the search results and the user experience.
　at the same time  better understanding of query meaning has the potential of boosting the economic underpinning of web search  namely  online advertising  via the sponsored search mechanism that places relevant advertisements alongside search results. for instance  knowing that the query  sd1  is about cameras while  nc1  is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries. in this study we present a methodology for query classification  where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 1 nodes. given such classifications  one can directly use them to provide better search results as well as more focused ads. the problem of query classification is extremely difficult owing to the brevity of queries. observe  however  that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. of course  the sheer volume of search queries does not lend itself to human supervision  and therefore we need alternate sources of knowledge about the world. for instance  in the example above   sd1  brings pages about canon cameras  while  nc1  brings pages about compaq laptops  hence to a human the intent is quite clear.
　search engines index colossal amounts of information  and as such can be viewed as very comprehensive repositories of knowledge. following the heuristic described above  we propose to use the search results themselves to gain additional insights for query interpretation. to this end  we employ the pseudo relevance feedback paradigm  and assume the top search results to be relevant to the query. certainly  not all results are equally relevant  and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. for the purpose of this study we first dispatch the given query to a general web search engine  and collect a number of the highest-scoring urls. we crawl the web pages pointed by these urls  and classify these pages. finally  we use these result-page classifications to classify the original query. our empirical evaluation confirms that using web search results in this manner yields substantial improvements in the accuracy of query classification.
　note that in a practical implementation of our methodology within a commercial search engine  all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. thus  at run-time we only need to run the voting procedure  without doing any crawling or classification. this additional overhead is minimal  and therefore the use of search results to improve query classification is entirely feasible in run-time.
　another important aspect of our work lies in the choice of queries. the volume of queries in today's search engines follows the familiar power law  where a few queries appear very often while most queries appear only a few times. while individual queries in this long tail are rare  together they account for a considerable mass of all searches. furthermore  the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1
　searching and advertising platforms can be trained to yield good results for frequent queries  including auxiliary data such as maps  shortcuts to related structured information  successful ads  and so on. however  the  tail  queries simply do not have enough occurrences to allow statistical learning on a per-query basis. therefore  we need to aggregate such queries in some way  and to reason at the level of aggregated query clusters. a natural choice for such aggregation is to classify the queries into a topical taxonomy. knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. consequently  in this work we focus on the classification of rare queries  whose correct classification is likely to be particularly beneficial.
　early studies in query interpretation focused on query augmentation through external dictionaries . more recent studies  1  1  also attempted to gather some additional knowledge from the web. however  these studies had a number of shortcomings  which we overcome in this paper. specifically  earlier works in the field used very small query classification taxonomies of only a few dozens of nodes  which do not allow ample specificity for online advertising . they also used a separate ancillary taxonomy for web documents  so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies .
　the main contributions of this paper are as follows. first  we build the query classifier directly for the target taxonomy  instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. the taxonomy used in this work is two orders of magnitude larger than that used in prior studies. the empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. since our taxonomy is considerably larger  the classification problem we face is much more difficult  making the improvements we achieve particularly notable. we also report the results of a thorough empirical study of different voting schemes and different depths of knowledge  e.g.  using search summaries vs. entire crawled pages . we found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. this result is in contrast with prior findings in query classification   but is supported by research in mainstream text classification .
1. methodology
our methodology has two main phases. in the first phase  we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified. in the second phase  we develop a query classifier that invokes the document classifier on search results  and uses the latter to perform query classification.
1 building the document classifier
　in this work we used a commercial classification taxonomy of approximately 1 nodes used in a major us search engine  see section 1 . human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.
　given a taxonomy of this size  the computational efficiency of classification is a major issue. few machine learning algorithms can efficiently handle so many different classes  each having hundreds of training examples. suitable candidates include the nearest neighbor and the naive bayes classifier   as well as prototype formation methods such as rocchio  or centroid-based  classifiers. a recent study  showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently  we used a centroid classifier in this work.
1 query classification by search
　having developed a document classifier for the query taxonomy  we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields. let's assume that there is a set of documents d = d1 ...dm indexed by a search engine. the search engine can then be represented by a function f~ = similarity q d  that quantifies the affinity between a query q and a document d. examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query  e.g.  pagerank ; and dynamic score-the closeness of the query and the document.
　query classification is determined by first evaluating conditional probabilities of all possible classes p cj|q   and then selecting the alternative with the highest probability cmax = arg maxcj（c p cj|q . our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query. we use the following formula that incorporates classifications of individual search results: p cj|q  =
.
 we assume that p q|cj d  「 p q|d   that is  a probability of a query given a document can be determined without knowing the class of the query. this is the case for the majority of queries that are unambiguous. counter examples are queries like 'jaguar'  animal and car brand  or 'apple'  fruit and computer manufacturer   but such ambiguous queries can not be classified by definition  and usually consists of common words. in this work we concentrate on rare queries  that tend to contain rare words  be longer  and match fewer documents; consequently in our setting this assumption mostly holds. using this assumption  we can writep p cj|q  = d（d p cj|d ，p d|q . the conditional probability of a classification for a given document p cj|d  is estimated using the output of the document classifier  section 1 . while p d|q  is harder to compute  we consider the underlying relevance model for ranking documents given a query. this issue is further explored in the next section.
1 classification-based relevance model
　in order to describe a formal relationship of classification and ad placement  or search   we consider a model for using classification to determine ads  or search  relevance. let a be an ad and q be a query  we denote by r a q  the relevance of a to q. this number indicates how relevant the ad a is to query q  and can be used to rank ads a for a given query q. in this paper  we consider the following approximation of relevance function:

　the right hand-side expresses how we use the classification scheme c to rank ads  where s c a  is a scoring function that specifies how likely a is in class c  and s c q  is a scoring function that specifies how likely q is in class c. the value w c  is a weighting term for category c  indicating the importance of category c in the relevance formula.
　this relevance function is an adaptation of the traditional word-based retrieval rules. for example  we may let categories be the words in the vocabulary. we take s cj a  as the word counts of cj in a  s cj q  as the word counts of cj in q  and w cj  as the idf term weighting for word cj. with such choices  the method given by  1  becomes the standard tfidf retrieval rule.
　if we take s cj a  = p cj|a   s cj q  = p cj|q   and w cj  = 1/p cj   and assume that q and a are independently generated given a hidden concept c  then we have
x
	rc a q 	=	p cj|a p cj|q /p cj 
cj（c
x
	=	p cj|a p q|cj /p q  = p q|a /p q .
cj（c
 that is  the ads are ranked according to p q|a . this relevance model has been employed in various statistical language modeling techniques for information retrieval. the intuition can be described as follows. we assume that a person searches an ad a by constructing a query q: the person first picks a concept cj according to the weights p cj|a   and then constructs a query q with probability p q|cj  based on the concept cj. for this query generation process  the ads can be ranked based on how likely the observed query is generated from each ad.
　it should be mentioned that in our case  each query and ad can have multiple categories. for simplicity  we denote by cj a random variable indicating whether q belongs to category cj. we use p cj|q  to denote the probability ofp q belonging to category cj. here the sum cj（c p cj|q  may not equal
to one. we then consider the following ranking formula:
x
	rc a q  =	p cj|a p cj|q .	 1 
cj（c
we assume the estimation of p cj|a  is based on an existing text-categorization system  which is known . thus  we only need to obtain estimates of p cj|q  for each query q.
　equation  1  is the ad relevance model that we consider in this paper  with unknown parameters p cj|q  for each query q. in order to obtain their estimates  we use search results from major us search engines  where we assume that the ranking formula in  1  gives good ranking for search. that is  top results ranked by search engines should also be ranked high by this formula. therefore given a query q  and top k result pages d1 q  ... dk q  from a major search engine  we fit parameters p cj|q  so that rc di q  q  have high scores for i = 1 ... k. it is worth mentioning that using this method we can only compute relative strength of p cj|q   but not the scale  because scale does not affect ranking. moreover  it is possible that the parameters estimated may be of the form g p cj|q   for some monotone function g ，  of the actually conditional probability g p cj|q  . although this may change the meaning of the unknown parameters that we estimate  it does not affect the quality of using the formula to rank ads. nor does it affect query classification with appropriately chosen thresholds. in what follows  we consider two methods to compute the classification information p cj|q .
1 the voting method
　we would like to compute p cj|q  so that rc di q  q  are high for i = 1 ... k and rc d q  are low for a random document d. assume that the vector  p cj|d  cj（c is random for an average document  then the condition thatpcj（c p cj|q 1 is small implies that rc d q  is also small
averaged over d. thus  a natural method is to maximize 	subject	to pcj（c p cj|q 1	being
small  where wi are weights associated with each rank i:
  
where we assume	= 1  and λ   1 is a tuning
regularization parameter. the optimal solution is
.
since both p cj|di q   and p cj|q  belong to  1   we may just take λ = 1 to align the scale. in the experiment  we will simply take uniform weights wi. a more complex strategy is to let w depend on d as well:
 
where g x  is a certain transformation of x.
　in this general formulation  w d q  may depend on factors other than the rank of d in the search engine results for q. for example  it may be a function of r d q  where r d q  is the relevance score returned by the underlying search engine. moreover  if we are given a set of hand-labeled training category/query pairs  c q   then both the weights w d q  and the transformation g ，  can be learned using standard classification techniques.
1 discriminative classification
　we can treat the problem of estimating p cj|q  as a classification problem  where for each q  we label di q  for i = 1 ... k as positive data  and the remaining documents as negative data. that is  we assign label yi q  = 1 for di q  when i ＋ k  and label yi q  =  1 for di q  when i   k.
　in this setting  the classification scoring rule for a document di q  is linear. letp xi q  =  p cj|di q     and w =  p cj|q    then cj（c p cj|q p cj|di q   = w，xi q . the values p cj|d  are the features for the linear classifier  and  p cj|d   is the weight vector  which can be computed using any linear classification method. in this paper  we consider estimating w using logistic regression  as follows:
p ，|q  = argminw pi ln 1 + e w，xi q yi q  .
　

figure 1: number of categories by level
1. evaluation
　in this section  we evaluate our methodology that uses web search results for improving query classification.
1 taxonomy
　our choice of taxonomy was guided by a web advertising application. since we want the classes to be useful for matching ads to queries  the taxonomy needs to be elaborate enough to facilitate ample classification specificity. for example  classifying all medical queries into one node will likely result in poor ad matching  as both  sore foot  and  flu  queries will end up in the same node. the ads appropriate for these two queries are  however  very different. to avoid such situations  the taxonomy needs to provide sufficient discrimination between common commercial topics. therefore  in this paper we employ an elaborate taxonomy of approximately 1 nodes  arranged in a hierarchy with median depth 1 and maximum depth 1. figure 1 shows the distribution of categories by taxonomy levels. human editors populated the taxonomy with labeled queries  approx. 1 queries per node   which were used as a training set; a small fraction of queries have been assigned to more than one category.
1 digression: the basics of sponsored search
　to discuss our set of evaluation queries  we need a brief introduction to some basic concepts of web advertising. sponsored search  or paid search  advertising is placing textual ads on the result pages of web search engines  with ads being driven by the originating query. all major search engines  google  yahoo!  and msn  support such ads and act simultaneously as a search engine and an ad agency. these textual ads are characterized by one or more  bid phrases  representing those queries where the advertisers would like to have their ad displayed.  the name  bid phrase  comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query. a discussion of bidding and placement mechanisms is beyond the scope of this paper .
　however  many searches do not explicitly use phrases that someone bids on. consequently  advertisers also buy  broad  matches  that is  they pay to place their advertisements on queries that constitute some modification of the desired bid phrase. in broad match  several syntactic modifications can be applied to the query to match it to the bid phrase  e.g.  dropping or adding words  synonym substitution  etc. these transformations are based on rules and dictionaries. as advertisers tend to cover high-volume and high-revenue queries  broad-match queries fall into the tail of the distribution with respect to both volume and revenue.
1 data sets
　we used two representative sets of 1 queries. both sets contain queries that cannot be directly matched to advertisements  that is  none of the queries contains a bid phrase  this means we eliminated practically all popular queries .
　the first set of queries can be matched to at least one ad using broad match as described above. queries in the second set cannot be matched even by broad match  and therefore the search engine used in our study does not currently display any advertising for them. in a sense  these are even more rare queries and further away from common queries. as a measure of query rarity  we estimated their frequency in a month worth of query logs for a major us search engine; the median frequency was 1 for queries in set 1 and 1 for queries in set 1.
　the queries in the two sets differ in their classification difficulty. in fact  queries in set 1 are difficult to interpret even for human evaluators. queries in set 1 have on average 1 words  with the longest one having 1 words; queries in set 1 have on average 1 words  with the longest query of 1 words. recent studies estimate the average length of web queries to be just under 1 words1  which is lower than in our test sets. as another measure of query difficulty  we measured the fraction of queries that contain quotation marks  as the latter assist query interpretation by meaningfully grouping the words. only 1% queries in set 1 and 1% in set 1 contained quotation marks.
1 methodology and evaluation metrics
　the two sets of queries were classified into the target taxonomy using the techniques presented in section 1. based on the confidence values assigned  the top 1 classes for each query were presented to human evaluators. these evaluators were trained editorial staff who possessed knowledge about the taxonomy. the editors considered every queryclass pair  and rated them on the scale 1 to 1  with 1 meaning the classification is highly relevant and 1 meaning it is irrelevant for the query. about 1% queries in set 1 and 1% queries in set 1 were judged to be unclassifiable  e.g.  random strings of characters   and were consequently excluded from evaluation. to compute evaluation metrics  we treated classifications with ratings 1 and 1 to be correct  and those with ratings 1 and 1 to be incorrect.
　we used standard evaluation metrics: precision  recall and f1. in what follows  we plot precision-recall graphs for all the experiments. for comparison with other published studies  we also report precision and f1 values corresponding to complete recall  r = 1 . owing to the lack of space  we only show graphs for query set 1; however  we show the numerical results for both sets in the tables.
1 results
　we compared our method to a baseline query classifier that does not use any external knowledge. our baseline classifier expanded queries using standard query expansion techniques  grouped their terms using a phrase recognizer  boosted certain phrases in the query based on their statistical properties  and performed classification using the

figure 1: the effect of external knowledge
nearest-neighbor approach. this baseline classifier is actually a production version of the query classifier running in a major us search engine.
　in our experiments  we varied values of pertinent parameters that characterize the exact way of using search results. in what follows  we start with the general assessment of the effect of using web search results. we then proceed to exploring more refined techniques  such as using only search summaries versus actually crawling the returned urls. we also experimented with using different numbers of search results per query  as well as with varying the number of classifications considered for each search result. for lack of space  we only show graphs for set 1 queries and omit the graphs for set 1 queries  which exhibit similar phenomena.
1.1 the effect of external knowledge
　queries by themselves are very short and difficult to classify. we use top search engine results for collecting background knowledge for queries. we employed two major us search engines  and used their results in two ways  either only summaries or the full text of crawled result pages. figure 1 and table 1 show that such extra knowledge considerably improves classification accuracy. interestingly  we found that search engine a performs consistently better with full-page text  while search engine b performs better when summaries are used.
enginecontextprec.f1prec.f1set 1set 1set 1set 1afull-page1111bfull-page1111asummary1111bsummary1111baseline1111table 1: the effect of using external knowledge
1.1 aggregation techniques
　there are two major ways to use search results as additional knowledge. first  individual results can be classified separately  with subsequent voting among individual classifications. alternatively  individual search results can be bundled together as one meta-document and classified as such using the document classifier. figure 1 presents the results of these two approaches when full-text pages are used  the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin. however  in the case of summaries  bundling together is found to be consistently better than individual classification. this is because summaries by themselves are too short to be classified correctly individually  but when bundled together they are much more stable.

figure 1: voting vs. bundling
1.1 full page text vs. summary
　to summarize the two preceding sections  background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results. full page text was found to be more in conjunction with voted classification  while summaries were found to be useful when bundled together. the best results overall were obtained with full-page results classified individually  with subsequent voting used to determine the final query classification. this observation differs from findings by shen et al.   who found summaries to be more useful. we attribute this distinction to the fact that the queries we used in this study are tail ones  which are rare and difficult to classify.
1.1 varying the number of classes per search result
　we also varied the number of classifications per search result  i.e.  each result was permitted to have either 1  1  or 1 classes. figure 1 shows the corresponding precision-recall graphs for both full-page and summary-only settings. as can be readily seen  all three variants produce very similar results. however  the precision-recall curve for the 1-class experiment has higher fluctuations. using 1 classes per search result yields a more stable curve  while with 1 classes per result the precision-recall curve is very smooth. thus  as we increase the number of classes per result  we observe higher stability in query classification.
1.1 varying the number of search results obtained
　we also experimented with different numbers of search results per query. figure 1 and table 1 present the results of this experiment. in line with our intuition  we observed that classification accuracy steadily rises as we increase the number of search results used from 1 to 1  with a slight drop as we continue to use even more results  1 . this is because using too few search results provides too little external knowledge  while using too many results introduces extra noise.
using paired t-test  we assessed the statistical significance

figure 1: varying the number of classes per page

figure 1: varying the number of results per query
of the improvements due to our methodology versus the baseline. we found the results to be highly significant  p   1   thus confirming the value of external knowledge for query classification.
1 voting versus alternative methods
　as explained in section 1  one may use several methods to classify queries from search engine results based on our relevance model. as we have seen  the voting method works quite well. in this section  we compare the performance of voting top-ten search results to the following two methods:
  a: discriminative learning of query-classification based on logistic regression  described in section 1.
  b: learning weights based on quality score returned by a search engine. we discretize the quality score s d q  of a query/document pair into {high  medium  low}  and learn the three weights w on a set of training queries  and test the performance on holdout queries. the classification formula  as explained at the end ofp section 1  is p cj|q  = d w s d q  p cj|d .
　method b requires a training/testing split. neither voting nor method a requires such a split; however  for consistency  we randomly draw 1 training/testing splits for ten times  and report the mean performance ＼ standard deviation on the test-split for all three methods. for this experiment  instead of precision and recall  we use dcg-k  k = 1   popular in search engine evaluation. the dcg  discounted cumulated gain  metric  described in   is a ranking measure where the system is asked to rank a set of candidates  in
number of resultsprecisionf1baseline111.1.1111.1.1111.1.1table 1: varying the number of search results
our case  judged categories for each query   and computes for each query	+ 1   where ci q  is the i-th category for query q ranked by the system  and g ci  is the grade of ci: we assign grade of 1 1 to the 1-point judgment scale described earlier to compute dcg. the decaying choice of log1 i + 1  is conventional  which does not have particular importance. the overall dcg of a system is the averaged dcg over queries. we use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output. therefore as a single metric  it is convenient for comparing the methods. note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the dcg numbers.
set 1methoddcg-1dcg-1oracle1 ＼ 11 ＼ 1voting1 ＼ 11 ＼ 1method a1 ＼ 11 ＼ 1method b1 ＼ 11 ＼ 1set 1methoddcg-1dcg-1oracle1 ＼ 11 ＼ 1voting1 ＼ 11 ＼ 1method a1 ＼ 11 ＼ 1method b1 ＼ 11 ＼ 1table 1: voting and alternative methods
　results from our experiments are given in table 1. the oracle method is the best ranking of categories for each query after seeing human judgments. it cannot be achieved by any realistic algorithm  but is included here as an absolute upper bound on dcg performance. the simple voting method performs very well in our experiments. the more complicated methods may lead to moderate performance gain  especially method a  which uses discriminative training in section 1 . however  both methods are computationally more costly  and the potential gain is minor enough to be neglected. this means that as a simple method  voting is quite effective.
　we can observe that method b  which uses quality score returned by a search engine to adjust importance weights of returned pages for a query  does not yield appreciable improvement. this implies that putting equal weights  voting  performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents  method b   at least for the top search results. it may be possible to improve this method by including other page-features that can differentiate top-ranked search results. however  the effectiveness will require further inves-
　
tigation which we did not test. we may also observe that the performance on set 1 is lower than that on set 1  which means queries in set 1 are harder than those in set 1.
1 failure analysis
　we scrutinized the cases when external knowledge did not improve query classification  and identified three main causes for such lack of improvement.  1 queries containing random strings  such as telephone numbers - these queries do not yield coherent search results  and so the latter cannot help classification  around 1% of queries were of this kind .  1  queries that yield no search results at all; there were 1% such queries in set 1 and 1% in set 1.  1  queries corresponding to recent events  for which the search engine did not yet have ample coverage  around 1% of queries . one notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine  search results are likely to be of little use.
1. related work
　even though the average length of search queries is steadily increasing over time  a typical query is still shorter than 1 words. consequently  many researchers studied possible ways to enhance queries with additional information.
　one important direction in enhancing queries is through query expansion. this can be done either using electronic dictionaries and thesauri   or via relevance feedback techniques that make use of a few top-scoring search results. early work in information retrieval concentrated on manually reviewing the returned results  1  1 . however  the sheer volume of queries nowadays does not lend itself to manual supervision  and hence subsequent works focused on blind relevance feedback  which basically assumes top returned results to be relevant  1  1  1  1 .
　more recently  studies in query augmentation focused on classification of queries  assuming such classifications to be beneficial for more focused query interpretation. indeed  kowalczyk et al.  found that using query classes improved the performance of document retrieval.
　studies in the field pursue different approaches for obtaining additional information about the queries. beitzel et al.  used semi-supervised learning as well as unlabeled data . gravano et al.  classified queries with respect to geographic locality in order to determine whether their intent is local or global.
　the 1 kdd cup on web query classification inspired yet another line of research  which focused on enriching queries using web search engines and directories  1  1  1  1  1 . the kdd task specification provided a small taxonomy  1 nodes  along with a set of labeled queries  and posed a challenge to use this training data to build a query classifier. several teams used the web to enrich the queries and provide more context for classification. the main research questions of this approach the are  1  how to build a document classifier   1  how to translate its classifications into the target taxonomy  and  1  how to determine the query class based on document classifications.
　the winning solution of the kdd cup  proposed using an ensemble of classifiers in conjunction with searching multiple search engines. to address issue  1  above  their solution used the open directory project  odp  to produce an odp-based document classifier. the odp hierarchy was then mapped into the target taxonomy using word matches at individual nodes. a document classifier was built for the target taxonomy by using the pages in the odp taxonomy that appear in the nodes mapped to the particular target node. thus  web documents were first classified with respect to the odp hierarchy  and their classifications were subsequently mapped to the target taxonomy for query classification.
　compared to this approach  we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in section 1. this simplifies the process and removes the need for mapping between taxonomies. this also streamlines taxonomy maintenance and development. using this approach  we were able to achieve good performance in a very large scale taxonomy. we also evaluated a few alternatives how to combine individual document classifications when actually classifying the query.
　in a follow-up paper   shen et al. proposed a framework for query classification based on bridging between two taxonomies. in this approach  the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. for this  an intermediate taxonomy with a training set  odp  is used. then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy. as opposed to this  we built a document classifier for the target taxonomy directly  without using documents from an intermediate taxonomy. while we were not able to directly compare the results due to the use of different taxonomies  we used a much larger taxonomy   our precision and recall results are consistently higher even over the hardest query set.
1. conclusions
　query classification is an important information retrieval task. accurate classification of search queries is likely to benefit a number of higher-level tasks such as web search and ad matching. since search queries are usually short  by themselves they usually carry insufficient information for adequate classification accuracy. to address this problem  we proposed a methodology for using search results as a source of external knowledge. to this end  we send the query to a search engine  and assume that a plurality of the highestranking search results are relevant to the query. classifying these results then allows us to classify the original query with substantially higher accuracy.
　the results of our empirical evaluation definitively confirmed that using the web as a repository of world knowledge contributes valuable information about the query  and aids in its correct classification. notably  our method exhibits significantly higher accuracy than methods described in prior studies1 compared to prior studies  our approach does not require any auxiliary taxonomy  and we produce a query classifier directly for the target taxonomy. furthermore  the taxonomy used in this study is approximately 1 orders of magnitude larger than that used in prior works. we also experimented with different values of parameters that characterize our method. when using search results  one can either use only summaries of the results provided by the search engine  or actually crawl the results pages for even deeper knowledge. overall  query classification performance was the best when using the full crawled pages  table 1 . these results are consistent with prior studies   which found that using full crawled pages is superior for document classification than using only brief summaries. our findings  however  are different from those reported by shen et al.   who found summaries to yield better results. we attribute our observations to using a more elaborate voting scheme among the classifications of individual search results  as well as to using a more difficult set of rare queries.
　in this study we used two major search engines  a and b. interestingly  we found notable distinctions in the quality of their output. notably  for engine a the overall results were better when using the full crawled pages of the search results  while for engine b it seems to be more beneficial to use the summaries of results. this implies that while the quality of search results returned by engine a is apparently better  engine b does a better work in summarizing the pages.
　we also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. for a classifier that is external to the search engine  retrieving full pages may be prohibitively costly  in which case one might prefer to use summaries to gain computational efficiency. on the other hand  for the owners of a search engine  full page classification is much more efficient  since it is easy to preprocess all indexed pages by classifying them once onto the  fixed  taxonomy. then  page classifications are obtained as part of the meta-data associated with each search result  and query classification can be nearly instantaneous.
　when using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document  and then using its classification as a whole. we believe the reason for this observation is that summaries are short and inherently noisier  and hence their aggregation helps to correctly identify the main theme. consistent with our intuition  using too few search results yields useful but insufficient knowledge  and using too many search results leads to inclusion of marginally relevant web pages. the best results were obtained when using 1 top search hits.
　in this work  we first classify search results  and then use their classifications directly to classify the original query. alternatively  one can use the classifications of search results as features in order to learn a second-level classifier. in section 1  we did some preliminary experiments in this direction  and found that learning such a secondary classifier did not yield considerably advantages. we plan to further investigate this direction in our future work.
　it is also essential to note that implementing our methodology incurs little overhead. if the search engine classifies crawled pages during indexing  then at query time we only need to fetch these classifications and do the voting.
　to conclude  we believe our methodology for using web search results holds considerable promise for substantially improving the accuracy of web search queries. this is particularly important for rare queries  for which little perquery learning can be done  and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the web. we believe our findings will have immediate applications to improving the handling of rare queries  both for improving the search results as well as yielding better matched advertisements.
in our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.
