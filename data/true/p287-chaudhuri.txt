block-level sampling is far more efficient than true uniform-random sampling over a large database  but prone to significant errors if used to create database statistics. in this paper  we develop principled approaches to overcome this limitation of block-level sampling for histograms as well as distinct-value estimations. for histogram construction  we give a novel two-phase adaptive method in which the sample size required to reach a desired accuracy is decided based on a first phase sample. this method is significantly faster than previous iterative methods proposed for the same problem. for distinct-value estimation  we show that existing estimators designed for uniform-random samples may perform very poorly if used directly on block-level samples. we present a key technique that computes an appropriate subset of a block-level sample that is suitable for use with most existing estimators. this  to the best of our knowledge  is the first principled method for distinct-value estimation with block-level samples. we provide extensive experimental results validating our methods.
1. introduction
　building database statistics by a full scan of large tables can be expensive. to address this problem  building approximate statistics using a random sample of the data is a natural alternative. there has been a lot of work on constructing statistics such as histograms and distinct values through sampling  1  1  1 . most of this work deals with uniform-random sampling. however  true uniform-random sampling can be quite expensive . for example  suppose that there are 1 tuples per disk block and we are retrieving a 1% uniformrandom sample. then the expected number of tuples that will be chosen from each block is 1. this means that our uniform-random sample will touch almost every block of the table. thus  in this case  taking a 1% uniform-random sample will be no faster than doing a full scan of the table.
　clearly  uniform-random sampling is impractical except for very small sample sizes. therefore  most commercial relational database systems provide the ability to do block-level sampling  in which to

 this work was done while the author was visiting microsoft research
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  paris  france.
copyright 1 acm 1-1/1 . . . $1.
sample a fraction of q tuples of a table  a fraction q of the diskblocks of the table are chosen uniformly at random  and all the tuples in these blocks are returned in the sample. thus  in contrast to uniform-random sampling  block-level sampling requires significantly fewer block accesses for the same sample size  blocks are typically quite large  e.g.  1k bytes .
　the caveat is that a block-level sample is no longer a uniform sample of the table. the accuracy of statistics built over a blocklevel sample depends on the layout of the data on disk  i.e.  the way tuples are grouped into blocks. in one extreme  a block-level sample may be just as good as a uniform-random sample  for example  when the layout is random  i.e.  if there is no statistical dependence between the value of a tuple and the block in which it resides. however  in other cases  the values in a block may be fully correlated  e.g.  if the table is clustered on the column on which the histogram is being built . in such cases  the statistics constructed from a block-level sample may be quite inaccurate as compared to those constructed from a uniform-random sample of the same size. given the high cost of uniform-random sampling  we contend that most previous work on statistics estimation through uniformrandom sampling is of theoretical significance  unless robust and efficient extensions of those techniques can be devised to work with block-level samples. surprisingly  despite the widespread use of block-level sampling in relational products  there has been limited progress in database research in analyzing impact of block-level sampling on statistics estimation. an example of past work is ; however it only addresses the problem of histogram construction from block-level samples  and the suggested scheme carries a significant performance penalty.
　in this paper  we take a comprehensive look at the significant impact of block-level sampling on statistics estimation. to effectively build statistical estimators with block-level sampling  the challenge is to leverage the sample as efficiently as possible  and still be robust in the presence of any type of correlations that may be present in the sample. specifically  we provide a foundation for developing principled approaches that leverage block-level samples for histogram construction as well as distinct-value estimation.
for histogram construction  the main challenge is in determin-
ing the required sample size to construct a histogram with a desired accuracy: if the layout is fairly random then a small sample will suffice  whereas if the layout is highly correlated  a much larger sample is needed. we propose a 1-phase sampling algorithm that is significantly more efficient  1% or more  than what was proposed in . in the first phase  our algorithm uses an initial blocklevel sample to determine  how much more to sample  by using cross-validation techniques. this phase is optimized so that the cross-validation step can piggyback on a standard sort-based algorithm for building histograms. in the second and final phase  the algorithm uses block-level sampling to gather the remaining sample  and build the final histogram. this is in sharp contrast to the algorithm in  that blindly doubles the sample size iteratively until the desired accuracy is reached- thus increasing sample size significantly  and paying significant overheads at each iteration. we back up our rationale for 1-phase histogram construction with a formal analytical model  and demonstrate its overwhelming superiority experimentally.
　distinct-value estimation is fundamentally different from histogram construction. to the best of our knowledge  despite a very large body of work on many distinct-value estimators for uniformrandom sampling  1  1  1  1   no past work has analyzed the impact of block-level sampling on such estimators. we formally show that using such estimators directly on the entire block-level sample may yield significantly worse estimates compared to those obtained by using them on an  appropriate subset  of the blocklevel sample. our experiments confirm that our procedure for selecting such an appropriate subset does indeed result in distinctvalue estimates that are almost as accurate as estimates obtained from uniform-random samples of similar size  and often vastly better than the estimates obtained by the na：ive approach of applying the estimator on the entire block-level sample.
　finally  our study led to the identification of novel measures that quantify the  degree of badness  of the layout for block-level sampling for statistics estimation. interestingly  these measures are found to be different for histograms and distinct-values  thus emphasizing the fundamental differences between the two problems.
　the rest of the paper is organized as follows. in section 1  we survey related work. in section 1  we investigate the problem of histogram construction  and in section 1  the problem of distinctvalue estimation  with block-level samples. we have prototyped our algorithms using microsoft sql server. we present the experimental results in section 1  and conclude in section 1.
1. related work
　random sampling has been used for solving many database problems. in statistics literature  the concept of cluster sampling is similar to block-level sampling being considered here . a large body of work addresses the problem of estimating query-result sizes by sampling  1  1  1  1 . the idea of using cluster sampling to improve the utilization of the sampled data  was first proposed for this problem by hou et. al. . however  they focus on developing consistent and unbiased estimators for count queries  and the approach is not error driven. for distinct-value estimation with block-level samples  they simply use the goodman's estimator  directly on the block-level sample  recognizing that such an approach can lead to a significant bias. the use of two-phase  or double sampling was first proposed by hou et. al.   also in the context of count query evaluation. however  their work considers uniform-random samples instead of block-level samples  and does not directly apply to histogram construction.
　the use of random sampling for histogram construction was first proposed by piatetsky-shapiro et. al. . in this context  the problem of deciding how much to sample for a desired error  has been addressed in  1  1 . however  these derivations assume uniform random sampling. only chaudhuri et. al.  consider the problem of histogram construction through block-level sampling. they propose an iterative cross-validation based approach to arrive at the correct sample size for the desired error. however  in contrast to our two-phase approach  their approach often goes through a large number of iterations to arrive at the correct sample size  consequently incurring much higher overhead. also  their approach frequently samples more than required for the desired error.
　the problem of distinct value-estimation through uniform random sampling has received considerable attention  1  1  1  1  1 . the goodman's estimator  is the unique unbiased distinct-value estimator for uniform random samples. however  it is unusable in practice  due to its extremely high variance. the hardness of distinct-value estimation has been established by charikar et. al. in . most estimators that work well in practice do not give any analytical error guarantees. for the large sampling fractions that distinct-value estimators typically require  uniform-random sampling is impractical. haas et. al. note in  that their estimators are useful only when the relation is laid out randomly on disk  so that a block-level random sample is as good as a uniform-random sample . however  distinct value estimation through block-level sampling has remained unaddressed. to the best of our knowledge  our work is the first to address this problem in a principled manner.
1. histogram construction
1 preliminaries
　almost all query-optimization methods rely on the availability of statistics on database columns to choose efficient query plans. histograms have traditionally been the most popular means of storing these statistics compactly  and yet with reasonable accuracy. any type of histogram can essentially be viewed as approximation of the underlying data distribution  and is a partitioning of the domain into disjoint buckets and storing the counts of the number of tuples belonging to each bucket. these counts are often augmented with density information  i.e.  the average number of duplicates for each distinct value. to estimate density  a knowledge of the number of distinct values in the relevant column is required. bucket counts help in cardinality estimation of range queries while density information helps for equality queries.
　histogram algorithms differ primarily in how the bucket separators are selected to reduce the error in approximating the underlying data distribution. for example  an equi-width bucketing algorithm forms buckets with equal ranges  an equi-depth bucketing algorithm forms buckets with equal number of tuples  a maxdiff bucketing algorithm places separators where tuple frequencies on either side differ the most  while the optimal v-opt algorithm places separators such that this error is minimized .
1.1 error-metrics
　we distinguish between two types of errors of histograms. the first type of error measures how accurately a histogram captures the underlying data distribution. the second type of error arises when the histogram is constructed through sampling. this error measures to what degree a histogram constructed over a sample  approximates a histogram constructed by a full scan of the data  i.e.  a perfect histogram . in this paper  we are concerned with the second type of error.
　various metrics have been proposed for the second type of error. we first develop some notation. consider a table with n tuples  containing an attribute x over a totally ordered domain d. an approximate k-bucket histogram over the table is constructed through sampling as follows. suppose a sample of r tuples is drawn. a bucketing algorithm uses the sample to decide a sequence of separators s1 s1 ... sk 1 （ d. these separators partition d into k buckets b1 b1 ... bk where bi = {v （ d|si 1   v ＋ si}  we take s1 =  ± and sk = ± . let n i be the size of  i.e.  number of tuples contained in  bi in the sample  and ni be the size of bi in the table. the histogram estimates ni as n i = nr ， n i. the histogram is perfect if ni = n i for i = 1 ... k.
the variance-error metric  measures the mean squared error across all buckets  normalized with respect to the mean bucket size:
		 1 
for the special case of equi-depth histograms  the problem of deriving the uniform-random sample size required to reach a given variance-error with high probability has been considered in .
　the max-error metric  measures the maximum error across all buckets:
		 1 
for equi-depth histograms  the uniform-random sample size needed to reach a desired max-error with high probability is derived in . although the methods developed in our paper can work for both kinds of metrics  in practice we observed that the max-error metric was overly conservative: a single bad bucket unduly penalizes a histogram whose accuracy is otherwise tolerable in practice. conversely  an unreasonably large sample size is often required to achieve a desired error bound. this was especially true when the layout was  bad  for block-level sampling  see section 1.1 . due to these difficulties with the max-error metric  in the rest of this paper we chose to describe our results only for the variance-error metric.
1.1 problem formulation
　the layout of a database table  i.e.  the way the tuples are grouped into blocks  can significantly affect the error in a histogram constructed over a block-level sample. this point was recognized in   and is illustrated by the following two extreme cases:
  random layout: for a table in which the tuples are grouped randomly into blocks  a block-level sample is equivalent to a uniform-random sample. in this case  a histogram built over a block-level sample will have the same error as a histogram built over a uniform-random sample of the same size.
  clustered layout: for a table in which all tuples in a block have the same value in the relevant attribute  sampling a full block is equivalent to sampling a single tuple from the table  since the contents of the full block can be determined given one tuple of the block . in this case  a histogram built over a block-level sample will have a higher error as compared to one built over a uniform-random sample of the same size.
　in practice  most real layouts fall somewhere in between. for example  suppose a relation was clustered on the relevant attribute  but at some point in time the clustered index was dropped. now suppose inserts to the relation continue to happen. this results in the table becoming  partially clustered  on this attribute. as another example  consider a table which has columns age and salary  and is clustered on the age attribute. since an older age usually  but not always  implies a higher salary  the table shall be  almost clustered  on salary too.
　suppose  we have to construct a histogram with a desired error bound. the above arguments show that the block-level sample size required to reach the desired error depends significantly on the layout of the table. in this section  we consider the problem of constructing a histogram with the desired error bound through blocklevel sampling  by adaptively determining the required block-level sample size according to the layout.
　the rest of this section is organized as follows. in the next subsection we briefly describe an iterative cross-validation based approach  previously developed in   for this problem  and discuss its shortcomings. in section 1  we provide the formal analysis which motivates our solution to the problem. our proposed algorithm 1phase is given in section 1.
1 cross-validationbasediterativeapproach
　the idea behind cross-validation is the following. first  a blocklevel sample s1 of size r is obtained  and a histogram h is constructed on it. then another block-level sample s1 of the same size is drawn. let n i  resp. m  i  be the size of the ith bucket of h in s1  resp. s1 . then  the cross-validation error according to the variance error-metric is given by:
		 1 
　intuitively  the cross-validation error measures the similarity of the two samples in terms of the value distribution. cross-validation error is typically higher than the actual variance error : it is unlikely for two independent samples to resemble each other in distribution  but not to resemble the original table. based on this fact  a straightforward algorithm has been proposed in  to arrive at the required block-level sample size for a desired error. let runf  resp. rblk  be the uniform-random sample size  resp. block-level sample size  required to reach the desired error. the algorithm starts with an initial block-level sample of size runf. the sample size is repeatedly doubled and cross-validation performed  until the crossvalidation error reaches the desired error target. henceforth  we shall refer to this algorithm as double. the major limitation of double is that it always increases the sample size by a factor of two. this blind step factor hurts in both the following cases:
  each iteration of the algorithm incurs considerable fixed overheads of drawing a random block-level sample  sorting the incremental sample  constructing a histogram  and performing the cross-validation test. for significantly clustered data where rblk is much larger than runf  the number of iterations becomes a critical factor in the performance.
  if at some stage in the iterative process  the sample size is close to rblk  the algorithm is oblivious of this  and samples more than required. in fact in the worst case  the total sample drawn maybe four times rblk  because an additional sample of the same size is required for the final cross-validation.
　to remedy these limitations  the challenge is to develop an approach which  a  goes through a much smaller number of iterations  ideally one or two  so that the effect of the overhead per iteration is minimized  and  b  does not overshoot the required sample size by much. clearly  these requirements can be met only if our algorithm has a knowledge of how the cross-validation error decreases with sample size. we formally develop such a relationship in the following subsection  which provides the motivation for our eventual algorithm  1phase.
1 motivating formal analysis
　in this subsection we formally study the relationship between cross-validation error and sample size. to keep the problem analyzable  we adopt the following simplified model: we assume that the histogram construction algorithm is such that the histograms produced over any two different samples have the same bucket separators  and differ only in the estimated counts of the corresponding buckets. for example  an equi-width histogram satisfies this assumption. this assumption is merely to motivate our analysis of the proposed algorithm. however  our algorithm itself can can work with any histogram construction algorithm  and does not actually fix bucket boundaries. indeed  our experimental results  section 1  demonstrate the effectiveness of our approach for both equi-depth
histograms and maxdiff histograms  neither of which satisfies the above assumption of same bucket separators. of course  it is an interesting open problem whether these histograms can be formally analyzed for sampling errors without making the above simplifying assumption.
　recall the notation introduced in section 1.1. let there be n tuples and n blocks in the table with b tuples per block  n = n/b . given a histogram h with k buckets  consider the distribution of the tuples of bucket bi among the blocks. let a fraction aij of the tuples in the jth block belong to bucket
let σi1 denote the variance of the numbers {aij|j = 1 ... n}. intuitively  σi1 measures how evenly the tuples of bucket bi are distributed among the blocks. if they are fairly evenly distributed  σi1 will be small. on the other hand  if they are concentrated in relatively few blocks  σi1 will be large.
　let s1 and s1 be two independent block-level samples of r tuples each. we assume blocks are sampled with replacement. for large tables  this closely approximates the case of sampling without replacement. suppose we construct a histogram h over s1  and cross-validate it against s1. let  cvvar be the cross-validation error obtained.
theorem 1. 
　proof. let n i  resp. m  i  be the size of bi in s1  resp. s1 . for fixed bucket separators  both n i and m  i have the same distribution. we first find the mean and variance of these variables. the mean is independent of the layout  and is given by

　the expression for the variance is more involved and depends on the layout. a block-level sample of r tuples consists of r/b blocks chosen uniformly at random. if block j is included in the blocklevel sample  it contributes baij tuples to the size of bi. thus  n i  or m  i  is equal to b times the sum of r/b independent draws with replacement from the aij's. hence  by the standard sampling theorem  

by equation 1 for the cross-validation error:
 1 
there are three key conclusions from this analysis:
1. the expected squared cross-validation error is inversely proportional to the sample size. this forms the basis of a more intelligent step factor than the blind factor of two in the iterative approach of .
1. in equation 1  the first term inside the summation represents the actual variance-error. since both terms are equal in expectation  the cross-validation error can be expected to be

about 〔1 times the actual variance-error. thus it is sufficient to stop sampling when the cross-validation error has reached the desired error target.
1. the quantity represents a quantitative measure of the  badness  of a layout for constructing the histogram h. if this quantity is large  the cross-validation error  and also the actual variance-error  is large  and we need a bigger blocklevel sample for the same accuracy. besides the layout  this measure also naturally depends on the bucket separators of h. henceforth we refer to this quantity as hist badness.
　we next describe our 1phase algorithm for histogram construction  which is motivated by the above theoretical analysis.
1 the 1phase algorithm
　suppose we wish to construct a histogram with a desired error threshold. for simplicity  we assume that the threshold is specified in terms of the desired cross-validation error  req  since the actual error is typically less . theorem 1 gives an expression for the expected squared cross-validation error  i.e.  it is proportional to hist badness and inversely proportional to the block-level sample size. since in general we do not know hist badness  such information about the layout is almost never directly available   we propose a 1-phase approach: draw an initial block-level sample in the first phase and use it to try and estimate hist badness  and consequently the required block-level sample size   then draw the remaining block-level sample and construct the final histogram in the second phase. the performance of this overall approach critically depends on how accurate the first phase is in determining the required sample size. an accurate first phase would ensure that this approach is much superior to the cross-validation approach of  because  a  there are far fewer iterations and therefore significantly fewer overheads   b  the chance of overshooting the required sample size is reduced  and  c  there is no final cross-validation step to check whether the desired accuracy has been reached.
　a straightforward implementation of the first phase might be as follows. we pick an initial block-level sample of size 1runf  where runf is the theoretical sample size that achieves an error of  req assuming uniform-random sampling . we divide this initial sample into two halves  build a histogram on one half and cross-validate this histogram using the other half. suppose the observed crossvalidation error is  obs. if  obs ＋  req we are done  otherwise the required block-level sample size rblk can be derived from theorem 1 to be. however  this approach is not very robust. since theorem 1 holds only for expected squared crossvalidation error  using a single estimate of the cross-validation error to predict rblk may be very unreliable. our prediction of rblk should ideally be based on the mean of a number of trials.
　to overcome this shortcoming  we propose our 1phase algorithm  in which the first phase performs many cross-validation trials for estimating rblk accurately. however  the interesting aspect of our proposal is that this robustness comes with almost no performance penalty. a novel scheme is employed in which multiple cross-validations are piggybacked on sorting  so that the resulting time complexity is comparable to that of a single sorting step. since most histogram construction algorithms require sorting anyway1  algorithm 1phase
input:
 req : desired maximum cross-validation error in histogram r1	: input parameter for setting initial sample size lmax : number of points needed to do curve-fitting
phase i:
1. a 1...1  = block-level sample of 1 tuples
1. sortandvalidate a 1...1  1 
1. rblk = getrequiredsamplesize   phase ii:
1. a 1 + 1...rblk  = block-level sample of rblk   1 tuples
1. sort a 1 + 1...rblk  
1. merge a 1...1  a 1 + 1...rblk   1.	createhistogram a 1...rblk  
sortandvalidate a 1...r  l 
1. if  l = lmax 
1. sort a 1...r  
1. else
1. m = br/1c
1. sortandvalidate a 1...m  l + 1 
1. sortandvalidate a m + 1...r  l + 1 
1. lh = createhistogram a 1...m  
1. rh = createhistogram a m + 1...r  
1. sqerr l  += getsquarederror lh a m + 1...r  
1. sqerr l  += getsquarederror rh a 1...m  
1. merge a 1...m  a m + 1...r  
getrequiredsamplesize  
1. if  sqerr/1 ＋   req 1 
1. return 1
1. else
1. fit a curve of the form y = c/x through the points  r1i sqerr i /1i+1  for i = 1 ... lmax   1
1. return   reqc  1
figure 1: 1-phase approach to sampling for histogram construction

this sharing of cross-validation and sorting leads to a very robust yet efficient approach.
　the pseudo-code for 1phase is shown in figure 1. we assume merge  sort  createhistogram and getsquarederror are externally supplied methods. the first two have their standard functionality. the function createhistogram can be any histogram construction algorithm such as the equi-depth algorithm  or the maxdiff algorithm . the function getsquarederror cross-validates the given histogram against the given sample  and returns the squared crossvalidation error   cvvar 1  according to equation 1.
　in phase i  the algorithm picks an initial block-level sample of size 1 where r1 is an input parameter. this parameter can be set as runf  however in practice we found that a setting that is 1 to 1 times larger yields much more robust results. then  crossvalidation is performed on different size subparts of the initial sample  where the task of cross-validation is combined with that of sorting. this piggybacking idea is illustrated in figure 1  and is implemented by the sortandvalidate procedure in figure 1. we use an in-memory merge-sort for sorting the sample  the sample sizes used in the first phase easily fit in memory . to sort and cross-validate a sample of size r  it is divided into two halves. each of these are recursively sorted and cross-validated. then  histograms are built

figure 1: combining cross-validation with sorting for lmax = 1
on the left and right halves. each histogram is tested against the other half  and two estimates of   cvvar 1 for a sample of size r/1 are obtained. note that the recursive cross-validation of the two halves will give several   cvvar 1 estimates for each sample size r/1 r/1... etc. effectively  we are reusing subparts of the sample to get several different cross-validation error estimates. we note that the standard statistical technique of bootstrap is also based upon reusing different subparts of a sample   and it would be interesting to explore its connections with our technique. however  the approach of piggybacking on merge-sort is very specific to our technique  and is motivated by efficiency considerations.
　although quick-sort is typically the fastest in-memory sort  and is the method of choice in traditional in-memory histogram construction algorithms   merge-sort is not much slower. moreover  it allows us to combine cross-validation with sorting. the merge-sort is parameterized to not form its entire recursion tree  but to truncate after the number of levels has increased to a threshold  lmax . this reduces the overall overhead of cross-validation. also  at lower sample sizes  error estimates lose statistical significance. usually a small number such as lmax = 1 suffices for our purposes. at the leaves of the recursion tree  we perform quick-sort rather than continuing with merge-sort.
　once this sorting phase is over  we have several   cvvar 1 estimates corresponding to each sample size r1 r1 ... r1lmax 1. we compute the mean of these estimates for each of these sample sizes. we then find the best fitting curve of the form  1 = c/r
 justified by theorem 1  to fit our observed points  where c is a constant  and  1 is the average squared cross-validation error observed for a sample of size r. this curve fitting is done using the standard method of least-squares. the best-fit curve yields a value of c which is used to predict rblk by putting   =  req. this is done in the procedure getrequiredsamplesize.
　finally  once we have an estimate for rblk  we enter phase ii. the additional sample required  of size rblk  1  is obtained and sorted. it is merged with the  already sorted  first-stage sample  a histogram is built on the total sample  and returned.
　in summary  the 1phase algorithm is significantly more efficient than double  mainly because it uses a more intelligent step factor that enables termination after only two phases. note that 1phase seeks to reach the cross-validation error target in the expected sense  thus there is a theoretical possibility that the error target may not be reached after the second phase. one way to avoid this problem would be to develop a high probability bound on the cross-validation error  rather than just an expected error bound as in theorem 1   and modify the algorithm accordingly so that it reaches the error target with high probability. another alternative would be to extend 1phase to a potentially multi-phase approach  where the step size is decided as in 1phase  but the termination criterion is based on a final cross-validation step as in double. although this will reduce the number of iterations as compared to double  it will still not solve the problem of oversampling due to the final cross-validation step. however  neither of these extensions seem to be necessary since 1phase in its present form almost always reaches the cross-validation error target in practice. even in the few cases in which it fails  the actual variance-error  which is typically substantially smaller than the cross-validation error  is always well below the error target.
1. distinct value estimation
1 problem formulation
　the number of distinct-values is a popular statistic commonly maintained by database systems. distinct-value estimates often appear as part of histograms  because in addition to tuple counts in buckets  histograms also maintain a count of the number of distinct values in each bucket. this gives a density measure for each bucket  which is defined as the average number of duplicates per distinct value. the bucket density is returned as the estimated cardinality of any query with a selection predicate of the form x = a  where a is any value in the range of the bucket  and x is the attribute over which the histogram has been built. thus  any implementation of histogram construction through sampling must also solve the problem of estimating the number of distinct values in each bucket.
　there has been a large body of work on distinct-value estimation using uniform-random sampling  1  1  1  1  1 . here we address the different problem of distinct-value estimation through blocklevel sampling. to the best of our knowledge  this problem has not been addressed in a principled manner in previous work. we shall only consider the problem of estimating the number of distinct values on the entire column x through block-level sampling. the most straightforward way to extend it to histogram buckets is to use the distinct value estimators on subparts of the sample corresponding to each bucket.
　we clarify that this problem is different in flavor compared to the one we addressed for histogram construction. here we focus on developing the best distinct-value estimator to use with blocklevel samples. the problem of deciding how much to sample to reach a desired accuracy  which we had addressed for histograms   remains open for future work. this seems to crucially depend on analytical error guarantees  which are unavailable for most distinctvalue estimators even with uniform-random sampling  1  1 .
　let d be the number of distinct values in the column  and let d  be the estimate returned by an estimator. we distinguish between the bias and error of the estimator:
	bias	=	|e d     d|
	error	=	max{d/d d/ 	d }
our definition of error is according to the ratio-error metric defined in . a perfect estimator shall have error = 1. notice that it is possible for an estimator to be unbiased  i.e. e d   = d   but still have high expected error.
　most prior work has been to develop estimators with small bias for uniform-random sampling. getting a bound on the error is considerably harder  1  1 . in fact  there are no known estimators that guarantee error bounds even for uniform-random sampling 1. ideally  we would like to leverage existing estimators which have been designed for uniform-random samples and make them work for block-level samples. moreover  we seek to use these estimators with block-level samples in such a way  that the bias and error are not much larger than when these estimators are used with uniformrandom samples of the same size.
　the rest of this section is organized as follows. in the next subsection  we show that if existing distinct-value estimators are used na：ively with block-level samples  highly inaccurate estimates may be produced. then  in section 1  we develop an exceedingly simple yet novel technique called collapse. using formal arguments  we show that collapse allows us to use a large class of existing estimators on block-level samples instead of uniformrandom samples such that the bias remains small. finally  in section 1  we study the performance of collapse in terms of the ratio-error metric. as with histograms  we identify a novel measure that quantifies the  degree of badness  of the layout for block-level sampling for distinct-value estimation. interestingly  this measure is found to be different from the corresponding measure for histograms  thus emphasizing the fundamental differences between the two problems.
1 failure of naive approach
　consider the following naive approach  called takeall  for distinct-value estimation with block-level sampling:
takeall: take a block-level sample sblk with sampling fraction q. use sblk with an existing estimator as if it were a uniformrandom sample with sampling fraction q.
　we show that many existing estimators may return very poor estimates if used with takeall. our arguments apply to most estimators which have been experimentally evaluated  and found to perform well on uniform-random samples  e.g.  the hybskew estimator   the smoothed jackknife estimator  1  1   the shlosser estimator   the gee estimator   and the ae estimator .
　let d be the number of distinct values in the sample. let there be fi distinct values which occur exactly i times in the sample. all the estimators mentioned above have the common form d  = d+k，f1  where k is a constant chosen adaptively according to the sample  or fixed according to the sampling fraction as in gee . the rationale behind this form of the estimators is as follows. intuitively  f1 represents the values which are  rare  in the entire table  have low multiplicity   while the higher frequency elements in the sample represent the values which are  abundant  in the table  have high multiplicity . a uniform-random sample is expected to have missed only the rare values  and none of the abundant values. hence we need to scale-up only the rare values to get an estimate of the total number of distinct values.
　however  this reasoning does not apply when these estimators are used with takeall. specifically  consider a table in which the multiplicity of every distinct value is at least 1. further  consider a layout of this table such that for each distinct value  its multiplicity in any block is either 1 or at least 1. for this layout  in any blocklevel sample  of any size   f1 = 1. thus  in this case  all the above estimators will return d  = d. effectively  no scaling is applied  and hence the resulting estimate may be highly inaccurate.
　more generally  the reason why these estimators fail when used with takeall  is as follows. when a particular occurrence of a value is included in a block-level sample  any more occurrences of the value in that block are also picked up- but by virtue of being present in that block  and not because that value is frequent. thus  multiplicity across blocks is a good indicator of abundance  but multiplicity within a block is a misleading indicator of abundance.
1 proposed solution: collapse
　in this section  we develop a very simple yet novel approach called collapse which enables us to use existing estimators on block-level samples instead of uniform-random samples.
　the reasons for the failure of takeall given in the previous subsection  suggest that to make the existing estimators work  a value should be considered abundant only if it occurs in multiple blocks in the sample  while multiple occurrences within a block algorithm collapse
input:	q:block-level sampling fraction1. sampling step:take a block-level sample sblk with sampling fraction q.1. collapse step:in sblk  collapse all multiple occurrences of a value within a block into one occurrence. call the resulting sample scoll.1. estimation step:use scoll with an existing estimator as if it were a uniform-random sample with sampling fraction q.figure 1: distinct-value estimation with block-level samples

should be considered as only a single occurrence. we refer to this as the collapsing of multiplicities within a block.
　in fact  we can show that such a collapsing step is necessary  by the following adversarial model: if our estimator depends on the multiplicities of values within blocks  an adversary might adjust the multiplicities within the sampled block so as to hurt our estimate the most  while still not changing the number of distinct values in the table. for example  if our estimate scales only f1  as most existing estimators   the adversary can give a multiplicity of at least 1 to as many of the values in the block as possible. thus  our estimator should be independent of the multiplicities of the values within blocks.
　this leads us to develop a very simple approach called collapse shown in figure 1. essentially  multiplicities within blocks of a block-level sample are first collapsed  and then existing estimators are directly run on the collapsed sample  i.e.  the collapsed sample is simply treated as if it were a uniform-random sample with the same sampling fraction as the block-level sample.
　we now provide a formal justification of collapse. let t be the table on which we are estimating the number of distinct values. let vj denote the jth distinct value. let nj be the tuple-level multiplicity of vj  i.e.  the number of times it occurs in t  and nj be the block-level multiplicity of vj  i.e.  the number of blocks of t in which it occurs. let sblk be a block-level sample from t with sampling fraction q  and scoll be the sample obtained after applying the collapse step to sblk. let tcoll be an imaginary table obtained from t by collapsing multiple occurrences of values within every block into a single occurrence. let sunf be a uniform-random sample from tcoll with the same sampling fraction q. notice that tcoll may have variable-sized blocks  but this does not affect our analysis. as before  let fi denote the number of distinct values which occur exactly i times in a sample.
　lemma 1. for the bernoulli sampling model  e fi in scoll  = e fi in sunf  for all i.
　proof. in the bernoulli sampling model  for picking a sample with sampling fraction q  each item is included with probability q independent of other items. this closely approximates uniformrandom sampling for large table sizes.
　a particular distinct value vj contributes to fi in scoll iff exactly i blocks in which it occurs are chosen in sblk. since vj occurs in
nj blocks  it contributes to fi i ＋ nj  in scoll with probability . thus 

now  in tcoll  the tuple-level multiplicity of vj is nj. thus  vj contributes to fi in sunf iff exactly i occurrences out of its nj occurrences are chosen in sunf. since the sampling fraction is q  the probability that vj contributes to fi in sunf is the same as in the above. hence the expected value of fi in sunf is the same as in
scoll.	
now consider any distinct-value estimator e of the form d  =
 where ai's are constants depending on the sampling fraction . we can show that for use with estimator e  scoll is as good as sunf  in terms of bias . let b tcoll q  be the bias of e when applied to uniform-random samples from tcoll with sampling fraction q. let bcoll t q  be the bias of e when applied to blocklevel samples from t with sampling fraction q  and which have been processed according to the collapse step.
theorem 1. b tcoll q  = bcoll t q .
　proof. first note that tcoll and t have the same number of distinct values. further  by lemma 1  e fi in scoll  = e fi in sunf . e is just a linear combination of fi's  and the coefficients depend only on the sampling fraction which is the same for scoll and sunf. thus  by linearity of expectations  the result follows. 
　the above theorem enables us to leverage much of previous work on distinct-value estimation with uniform-random samples. most of this work  1  1  tries to develop estimators with small bias on uniform-random samples. by theorem 1  we reduce the problem of distinct-value estimation using block-level samples to that of distinct-value estimation using uniform-random samples of a modified  i.e.  collapsed  table. for example  gee  is an estimator which has been shown to have a bias of at most on uniform-random samples with sampling fraction q. moreover  gee
is of the form as required by theorem 
 thus  if we use gee with collapse  our estimate also will be biased by at most for a block-level sampling fraction of q. other estimators like hybskew and ae do not exactly satisfy the conditions of theorem 1 since the ai's themselves depend on the fi's. however  these estimators are heuristic anyway. hence we experimentally compare the performance of collapse with these estimators  against using these estimators on uniform-random samples. the experimental results given in section 1 demonstrate the superiority of collapse against takeall with these estimators.
1 studying error for collapse
　in this subsection we discuss the impact of collapse on the ratio-error of estimators. however  unlike bias  formal analysis of the ratio-error is extremely difficult even for uniform-random sampling  1  1 . consequently  much of the discussion in this subsection is limited to qualitative arguments. the only quantitative result we give is a lower bound on the error of any estimator with blocklevel sampling  thus illustrating the difficulty of getting estimators with good error bounds.
　charikar et. al. give a negative result in   where they show that for a uniform-random sample of r tuples from a table with high probability on all inputs. we show that with block-levelp no distinct-value estimator can guarantee a ratio error   o  n/r 
sampling  the guarantees that can be given are even weaker. for a block-level sample of r tuples  this lower bound can be strength-

ened to o pnb/r  where b is the number of tuples per block.
　theorem 1. any distinct-value estimator that examines at most r blocks from a table of n blocks  cannot guarantee a ratio error

numberpof tuples per block.   o  nb/r  with high probability on all inputs  where b is the

	n 1's	n type i blocks
figure 1: negative result for distinct-value estimation
　proof. we first review the proof of the negative result in . consider two different attribute-value distributions a and b as shown in figure 1. consider any distinct-value estimator that examines at most r out of the n tuples. for distribution b  the estimator shall always obtain r copies of value 1. it is shown in  that for distribution a  with probability at least γ  the estimator shall obtain r copies of value 1 provided:
		 1 
in this case  the estimator cannot distinguish between distributions a and b. let α be the value returned by the estimator in this case. this gives an error of  k + 1 /α for distribution a  and α for distribution b. irrespective of α  the error is at least for one of the distributions. choose k according to equation 1. then  with

probability at least γ  the error is at least o 	n/r .
	to extend this argument to block-level sampling p	consider dis-
tributions c and d  and their layouts as shown in figure 1. type i blocks contain b duplicates of the value 1  while type ii blocks contain b new distinct values. consider a distinct-value estimator that examines at most r out of n blocks. for distribution d  it always obtains r type i blocks. for distribution c  using the same argument as above  if  then with probability at least γ  the estimator shall obtain r type i blocks. thus  the estimator cannot distinguish between distributions c and d in this case  and

must have an error of at least 〔kb + 1 for one of the distributions.
hence  it is not possible to guarantee an error   o pnb/rp   with thus  with probability at least γ  the error is at least o  nb/r .
high probability on all inputs.
　the above lower-bound notwithstanding  it is still instructive to evaluate the performance of estimators for more general layouts in terms of the ratio-error metric. we give a qualitative evaluation of the performance of collapse by comparing it with the approach of estimating distinct values using sunf  i.e.  a uniformrandom sample of the collapsed table tcoll. we assume that the same distinct-value estimator is used in each case  and is of the form d  = d + k ， f1 as in section 1. theorem 1 says that both approaches will have the same bias. however  the error of collapse may be higher. this is because although the expected value of f1 is the same in both scoll and sunf  recall lemma 1   the variance of f1 in scoll may be higher than in sunf. for example  for the layout c shown in figure 1  f1 in scoll can only take on values which are multiples of b  assuming   1 type i blocks are picked up in the sample . on the other hand  f1 in sunf can take on any value from 1 to kb. this larger variance leads to a higher average error for collapse.
　the layouts in which the variance of f1 in scoll  and hence the average error of collapse  is higher  are those in which the number of distinct values in blocks varies widely across blocks. based on this intuition  we introduce a quantitative measure for the  badness  of a layout for distinct-value estimation with block-level samples. we denote this measure as dv badness. let dj be the number of distinct values in the jth block. let μ be the mean  and σ be the standard deviation of dj's  j = 1 ... n . we define dv badness as the coefficient of variation of the dj's  i.e.  σ/μ. the higher the value of dv badness  the higher the error of collapse.
　notice that hist badness and dv badness are different measures. hence the layouts which are bad for histogram construction are not necessarily bad for distinct-value estimation  and viceversa. for example  while hist badness is maximized when the table is fully clustered  it is not so with dv badness. in fact  even when the table is fully clustered  collapse may perform very well  as long as the number of distinct values across blocks does not vary a lot  so that dv badness is still small .
1. experiments
　in this section  we provide experimental validation of our proposed approaches. we have prototyped and experimented with our algorithms on microsoft sql server running on an intel 1 ghz processor with 1gb ram.
　for histogram construction  we compare our adaptive two-phase approach 1phase  against the iterative approach double. we experimented with both the maxdiff bucketing algorithm  as implemented in sql server  as well as the equi-depth bucketing algorithm. the version of double which we use for comparison is not exactly the same as described in   but an adaption of the basic idea therein to work with maxdiff as well as equi-depth histograms 
and uses the variance-error metric instead of the max-error metric.
for distinct-value estimation  we compare our proposed approach
collapse  with the na：ive approach takeall  and the ideal  but impractical  approach uniform. for uniform  we used a uniform-random sample of the same size as the block-level sample used by collapse. we experimented using both the hybskew   and the ae  estimators.
　our results demonstrate for varying data distributions and layouts:
  for both maxdiff and equi-depth histograms  1phase accurately predicts the sample size required  and is considerably faster than double.
  for distinct value estimation  collapse produces much more accurate estimates than those given by takeall  and almost as good as those given by uniform.
  our quantitative measures hist badness and dv badness  accurately reflect the performance of block-level sampling as compared to uniform-random sampling for histogram construction and distinct-value estimation respectively.
we have experimented with both synthetic and real databases.
synthetic databases: to generate synthetic databases with a wide variety of layouts  we adopt the following generative model: a fraction c between 1 and 1 is chosen. then  for each distinct value in the column of interest  a fraction c of its occurrences are given consecutive tuple-ids  and the remaining  1 c  fraction are given random tuple-ids. the resulting relation is then clustered on tupleid. we refer to c as the  degree of clustering . different values of c give us a continuum of layouts  ranging from a random layout for c = 1  to a fully clustered layout for c = 1. this is the model which was experimented with in . besides  this model captures many real-life situations in which correlations can be expected to exist in blocks  such as those described in section 1.1. our experimental results demonstrate the relationship of the degree of clustering according to our generative model  c   with the measures of badness hist badness and dv badness.
　we generated tables with different characteristics along the following dimensions:  1  degree of clustering c varied from 1 to 1

figure 1: effect of table size on sample size for maxdiff histograms
according to our generative model   1  number of tuples n varied from 1 to 1   1  number of tuples per block b varied from 1 to 1  and  1  skewness parameter z varied from 1 to 1  according to the zipfian distribution .
real databases: we also experimented with a portion of a home database obtained from msn  http://houseandhome.msn.com/ . the table we obtained contained 1 tuples  each tuple representing a home for sale in the us. the table was clustered on the neighborhood column. while the table had numerous other columns  we experimented with the zipcode column  which is expected to be strongly correlated with the neighborhood column. the number of tuples per block was 1.
1 results on synthetic data
1.1 histogram construction
　we compared 1phase and double. in both approaches  we used a client-side implementation of maxdiff and equi-depth histograms . we used block-level samples obtained through the sampling feature of the dbms. both double and 1phase were started with the same initial sample size.
　in our results  all quantities reported are those obtained by averaging five independent runs of the relevant algorithm. for each parameter setting  we report a comparison of the total amount sampled by 1phase  against that sampled by double. we also report  the actual amount  denoted by actual  to be sampled to reach the desired error. this was obtained by a very careful iterative approach  in which the sample size was increased iteratively by a small amount until the error target was met. this actual size does not include the amount sampled for cross-validation. this approach is impractical due to the huge number of iterations  but reported here only for comparison purposes. we also report a comparison of the time taken by 1phase  against that taken by double. the reported time1 is a sum of the server-time spent in executing the sampling queries  and the client time spent in sorting  merging  cross-validation  and histogram construction.
　we experimented with various settings of all parameters. however  due to lack of space we only report a subset of the results. we report the cases where we set the cross-validation error target at  req = 1  the number of buckets in the histogram at k = 1  and the number of tuples per block at b = 1. for each experiment  we provide results for only one of either maxdiff or equi-depth histograms  since the results were similar in both cases.

figure 1: effect of table size on total time for maxdiff histograms

figure 1: effect of clustering on sample size for equi-depth histograms
effect of n: figure 1 shows a comparison of the amount sampled  and figure 1 shows a time comparison for varying n  for the case of maxdiff histograms. it can be seen that the amount sampled by each approach is roughly independent of n. also  double substantially overshoots the required sample size  due to the last cross-validation step   whereas 1phase does not overshoot by as much. for n=1  the total amount sampled by double exceeds the table size  but this is possible since the sampling is done in steps until the error target is met.
　in terms of time  1phase is found to be considerably faster than double. interestingly  the total time for both 1phase and double increases with n even though the amount sampled is roughly independent of n. this shows that there is a substantial  fixed overhead associated with each sampling step which increases with n. this also explains why the absolute time gain of 1phase over double increases with n. double incurs the above overhead in each iteration  whereas 1phase incurs it only twice. consequently  1phase is much more scalable than double.
effect of degree of clustering: figure 1 shows the amount sampled  and figure 1 gives a time comparison for varying degree of clustering  c  for equi-depth histograms. figure 1 also shows  by the dotted line  the badness measure hist badness on a secondary axis. hist badness was measured according to the bucket separators of the perfect histogram. since hist badness is maximized when the table is fully clustered  we have normalized the measure with respect to hist badness for c = 1. as c increases  both hist badness  and the required sample size increase. thus  hist badness is a good measure of the badness of the layout.

figure 1: effect of clustering on time for equi-depth histograms

figure 1: effect of skew on sample size for maxdiff histograms
　for c = 1  double overshoots the required sample size almost by a factor of 1  which is the worst case for double . hence  the total amount sampled becomes almost the same as that for c = 1. this is a consequence of the fact that double picks up samples in increasingly large chunks. the time gain of 1phase over double increases with c  since the latter has to go through a larger number of iterations when the required sample size is larger. the results for maxdiff histograms were similar.
effect of z: figure 1 compares the amount sampled for varying skew  z  of the distribution  for maxdiff histograms. as the skew increases  some buckets in the maxdiff histogram become very large. consequently  a smaller sample size is required to estimate these bucket counts accurately. thus  the required sample size decreases with skew. however  1phase continues to predict the required sample size more accurately than double. a time comparison for this experiment is omitted  as the gains of 1phase over double were similar to that observed in previous experiments. also  we omit results for equi-depth histograms  which showed very little dependence on skew.
　due to space constraints  we omit results of experimenting with varying b  k and  req. the results in these experiments were as expected. as b increases  or k increases  or  req decreases  the required sample size goes up  by theorem 1 . the amount by which double overshoots 1phase increases. so does the time gain of 1phase over double.
1.1 distinct-value estimation
　for distinct value estimation  we use the two contending estimators ae  and hybskew   which have been shown to work best in practice with uniform-random samples. we consider each of

figure 1: variation of error with the sampling fraction for hybskew

figure 1: variation of error with the sampling fraction for ae these estimators with each of the three approaches- collapse  takeall  and uniform. we use ae collapse to denote the collapse approach being used with the ae estimator. other estimates are named similarly. the usability of a distinct-value estimator depends on its average ratio-error rather than on its bias  since it possible to have an unbiased estimator with arbitrarily high ratio-error . thus  we only report the average ratio-error for each of the approaches. the average was taken over ten independent runs. in most cases  we report results only with the ae estimator  and omit those with hybskew  as the trends were similar.
　for the following experiments  we added another dimension to our data generation process- the duplication factor  dup . this is the multiplicity assigned to the rarest value in the zipfian distribution. thus  increasing dup increases the multiplicity of each distinct value  keeping the number of distinct values constant. the number of tuples per block was again fixed at b = 1.
effect of sampling fraction: figures 1 and 1 show the error of the hybskew and ae estimators respectively with the three approaches  for varying sampling fractions. with both estimators  takeall leads to very high errors  as high as 1 for low sampling fractions   while collapse performs almost as well as uniform for all sampling fractions.
effect of degree of clustering: figure 1 shows the average ratioerror of the ae estimator with the three approaches  for a fixed sampling fraction  and for varying degrees of clustering  c . as expected  the performance of uniform is independent of the degree of clustering. the performance of takeall degrades with increasing clustering. however  collapse continues to perform almost as well as uniform even in the presence of clustering. in

figure 1: effect of clustering on error for the ae estimator

figure 1: effect of skew on the error for the ae estimator
figure 1  we also show  by the dotted line  the measure of badness of the layout  dv badness   on the secondary y-axis. it can be seen that the trend in dv badness accurately reflects the performance of collapse against uniform. thus  dv badness is a good measure of the badness of the layout.
　note that unlike hist badness  figure 1   dv badness is not maximized when the table is fully clustered. in fact  for c = 1  collapse outperforms uniform. this is because when the table is fully clustered  ignoring the values which occur in multiple blocks  since there are few of them   the problem of distinct-value estimation through block-level sampling can be viewed as an aggregation problem - each block has a certain number of distinct values  and we want to find the sum of these numbers by sampling a subset. moreover  the variance of these numbers is small  as indicated by a small hist badness. this leads to a very accurate estimate being returned by collapse. we omit the results with the hybskew estimator  which were similar.
effect of skew: figure 1 shows the average error of ae with the three approaches  for a fixed sampling fraction  and for varying skew  z . here again  collapse performs consistently better than takeall. we again show dv badness by the dotted line on the secondary axis. the trend in dv badness accurately reflects the error of collapse against that of uniform.
　although we do not report results with hybskew here  it was found that for high skew  hyb takeall actually performed consistently better than hyb collapse or hyb uniform. this seems to violate our claim of collapse being a good strategy. however  at high skew  the hybskew estimator itself is not very accurate  and overestimates the number of distinct values. this  combined with the tendency of takeall to underestimate  due

figure 1: effect of duplication factor on the error for the ae estimator to its failure to recognize rare values   produced a more accurate final estimate than either collapse or uniform. thus  the good performance of hyb takeall for this case was coincidental  resulting from the inaccuracy of the hybskew estimator.
effect of bounded domain scaleup: for this experiment  the table size was increased while keeping the number of distinct values constant  by increasing dup . figure 1 shows the average error of ae with the three approaches for a fixed sampling fraction  and for various values of dup. it can be seen that at low values of dup  takeall performs very badly. however  as dup increases  almost all distinct values are picked up in the sample  and the estimation problem becomes much easier. thus  at high values of dup  takeall begins to perform well. collapse performs consistently almost as well  or better than uniform.
　for low values of dup  the superior performance of collapse against uniform is only because the underlying ae estimator is not perfect. for low values of dup  there are a large number of distinct values in the table  and ae uniform tends to underestimate the number of distinct values. however  for ae collapse  the value of f1 is higher due to the collapse step. hence the overall estimate returned is higher.
effect of unbounded domain scaleup: for this experiment  the table size was increased while proportionately increasing the number of distinct values  keeping dup constant . similar to the observation in   for a fixed sampling fraction  the average error remained almost constant for all the approaches  charts omitted due to lack of space . this is because when dup is constant  the estimation problem remains equally difficult with increasing table size.
1 results on real data
histogram construction: we experimented with 1phase and double on the home database. the number of buckets in the histogram was fixed at k = 1. figure 1 shows the amount sampled by both approaches for various error targets   req  for maxdiff histograms. again  1phase predicts the required size accurately while double significantly oversamples. note that for this database the number of tuples per block is only 1  for higher b we expect 1phase to perform even better than double. also  in this case  the sampled amounts are large fractions of the original table size  e.g.  1% sampled by 1phase for  req = 1 . however  our original table is relatively small  about 1 million rows . since required sample sizes are generally independent of original table sizes  e.g.  see figure 1   for larger tables the sampled fractions will appear much more reasonable. we omit the time comparisons as the differences were not substantial for this small table. also  the results for equi-depth histograms were similar.

figure 1: variation of sample size with error target for maxdiff histograms on the home database

figure 1: distinct values estimations on the home database
distinct value estimation: the results of our distinct-value estimation experiments on the home database are summarized in figure 1. as expected  ae collapse performs almost as well as ae uniform  while ae takeall performs very poorly.
1. conclusions
　in this paper  we have developed effective techniques to use blocklevel sampling instead of uniform-random sampling for building statistics. for histogram construction  our approach is significantly more efficient and scalable than previously proposed approaches. to the best of our knowledge  our work also marks the first principled study of the effect of block-level sampling on distinct-value estimation. we have demonstrated that in practice  it is possible to get almost the same accuracy for distinct-value estimation with block-level sampling  as with uniform-random sampling. our results here may be of independent interest to the statistics community for the problem of estimating the number of classes in a population through cluster sampling.
acknowledgements
we thank peter zabback and other members of the microsoft sql server team for several useful discussions.
