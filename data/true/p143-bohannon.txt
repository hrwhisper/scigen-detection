data integrated from multiple sources may contain inconsistencies that violate integrity constraints. the constraint repair problem attempts to find  low cost  changes that  when applied  will cause the constraints to be satisfied. while in most previous work repair cost is stated in terms of tuple insertions and deletions  we follow recent work to define a database repair as a set of value modifications. in this context  we introduce a novel cost framework that allows for the application of techniques from record-linkage to the search for good repairs. we prove that finding minimal-cost repairs in this model is np-complete in the size of the database  and introduce an approach to heuristic repair-construction based on equivalence classes of attribute values. following this approach  we define two greedy algorithms. while these simple algorithms take time cubic in the size of the database  we develop optimizations inspired by algorithms for duplicate-record detection that greatly improve scalability. we evaluate our framework and algorithms on synthetic and real data  and show that our proposed optimizations greatly improve performance at little or no cost in repair quality.
1.	introduction
　when overlapping or redundant information from multiple sources is integrated  inconsistencies or conflicts in the data may emerge as violations of integrity constraints on the integrated data  see  e.g.   1  1  1  1  1  1  1  1  . one important example of this situation is in the enterprise  where different departments such as sales  billing  and order- or service-fulfillment often have separate applications storing overlapping data. conflicts in this data may be introduced for many reasons   including misspellings or differing conventions used during data entry  e.g.  a person's name may appear as  john smith  and  j. smith    different processes and time-scales for performing updates  e.g.  address changes may take a few days to a few months to propagate   and so on. this problem becomes particularly evident with data warehousing or other integration scenarios because combining data makes conflicts visible: errors in a single database can seldom be detected without inspection of the real world or other manual effort. yet the consequences of poor enterprise data can be severe-for telecommunication ser-

supported in part by epsrc gr/s1  gr/t1  and nsfc 1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  baltimore  maryland  usa. copyright 1 acm 1-1/1 $1.
vice providers  for instance  errors routinely lead to problems like failure to bill for provisioned services  delay in repairing network problems  unnecessary leasing of equipment  and so on . as a result  data sources may be integrated in order to reconcile and correct the source data. for example  revenue recovery applications  1  1  compare billing and service databases to ensure that all services are billed  and presumably vice-versa . we now introduce our running example to illustrate these issues. example 1: consider a hypothetical provider of network services to residential users  e.g.  a phone or cable company . customer and equipment information is maintained by separate databases in the billing and maintenance departments. data from these two databases is merged according to the following target schema with two tables  and :


the table contains address information on customers with phone number as a key  while the table catalogs equipment installed at the customer's location and includes manufacturer  model number  install date and the serial number which serves as the key. figure 1 depicts an example instance    of the and tables. tuples are labeled as for ease of reference 
and tuples	are from the billing database.
　figure 1 also shows the set of inclusion dependencies  referred to as ind      and functional dependencies  referred to as fd      on . for example  ind  1  ensures that every piece of equipment is associated with a valid customer in   while fds  1  and  1  are key dependencies specifying that phone number and serial number are keys for the customer and equipment tables  respectively. fd  1  is not a traditional key dependency  but asserts  perhaps somewhat arbitrarily  that a given customer will have only one instance of a given piece of equipment.
　the wt column in the figure does not appear in the original data  and instead reflects the confidence placed by the user in the accuracy of the data. in this example  a greater confidence is placed in records from billing.
　an example of a source database inconsistency is differing spellings for  alice smith  in tuples  from billing  and  from maintenance   which violates fd  1 . other constraint-violating discrepancies include a  the tuples and for bob jones with different phone numbers and states which violate fds  1  and  1   and b  the tuple which violates the inclusion dependency ind  1  between the and tables.
cust
wt1alice smith1 bridgemidvilleaz11bob jones1 valley rdcentreny11bob jones1 valley rdcentrenj11carol blake1 mountaindavisca11ali stith1 bridgemidvilleaz1inclusion dependencies:
	equip	 1 
wt1ac1acxe1mar-11l1luze1jan-11l1luze1mar-11ac1acxe1feb-11l1luze1oct-1functional dependencies:
 1 
 1 
 1 
 1 
 1 
	figure 1: customer equipment example data	  and dependencies	.　while substantial previous work has explored query answering and constraint repair in inconsistent databases  the bulk of that work  1  1  1  1  1  1  1  restricts repair actions to inserting and deleting tuples. however  in these models  repairs of inclusion dependencies may lose important information. for example  deletion of tuple to repair the violation of inclusion dependency id  1  in figure 1 would lose information about a piece of equipment  and inserting a new tuple in the table would not help the user resolve the location of that equipment accurately. recent work  1  1  has introduced repairs in which attribute values are modified to restore the database to a consistent state  allowing more satisfying resolution of common constraint violations.
　record linkage is a broad field  see  e.g.   1  1  1    also known as  duplicate removal  or  merge-purge   and refers to the task of linking pairs of records that refer to the same entity in different data sets. this is commonly applied to household information in census data  mailing lists or medical records as well as many other uses. while to our knowledge not observed by prior work  there are in fact strong connections between record linkage and constraint repair. first  individual repairs of inclusion and functional dependencies involve entity matching. consider ind  1  in the above example: repairing this constraint requires matching customer entities  in this case represented by their phone numbers. if this constraint were specified on several attributes including name and address information  the task of finding the appropriate repair for an unmatched tuple in the equipment table would exactly correspond to a record linkage task. second  specific record linkage tasks for a set of tables can be accomplished by specifying inclusion and functional dependencies and then invoking constraint repair. consider the task of removing approximate duplicates between tables and . this may be accomplished by specifying a pair of inclusion constraints from to and back on the set of attributes which should match  perhaps all the attributes .
　while there is a compelling need to help users correct conflicting data  it may be difficult to see what sort of automatic support will be helpful. clearly  it would be helpful to a user to enumerate constraint violations  but it may still be exceedingly onerous for the user to manually correct all the problems. in this situation  it may be more helpful to automatically propose a repair  1  1  1   which can be informally thought of as a database that is  close  to the original but which satisfies integrity constraints.
our contributions. the first contribution of this paper is a repair framework that focuses  like  1  1   on value modification  but which improves on previous work in several ways. first  we offer a general framework that deals with both fds and inds  unlike   which focuses on the specific domain of census data  and   in which cleaning is restricted to relatively simple constraints that cannot express inds  for example. furthermore  we propose a novel cost model for repairs based on two factors  accuracy and similarity  which we now define. the accuracy of data is reflected in a weight for each tuple and represents the confidence placed by the user in the values therein. for example  the tuples in fig. 1 from the billing department are given weight   reflecting greater confidence in their accuracy than records from the maintenance department  which are given a weight . in this work  we assume that the weights default to 1 but may be given by the user  and leave to future work the development of techniques for automatically setting them. a variety of similarity of data is available at the attribute or tuple level  for example  string-edit distance. while we use a relatively simple similarity function in our experiments  our goal is to allow a variety of attribute-similarity metrics  see  1  1   from the data linkage community to be applied.
　our second contribution consists of complexity results for minimum-cost constraint repair based on value-modification. we show that value-modification complicates the analysis of the problem: it becomes np-complete in the size of the data  even with a small  constant number of either fds or inds. in contrast  the corresponding problems in which a database is repaired by deleting a minimal set of tuples is in ptime; the problem only becomes intractable if arbitrary fds and inds are both present  1  1 . in this context  we find that simple heuristic repair approaches based on repairing one constraint at a time suffer from a number of problems including a failure to terminate.
　in light of the intractability results and problems with simple heuristics  our third contribution is an approach to repair construction based on equivalence classes of  tuple  attribute  pairs that are assigned identical values in the repair. the introduction of equivalence classes has three benefits. first  it is fundamental to ensuring termination of the algorithm  even when tuple inserts are allowed. second  it separates the relationship among attribute values  represented by the equivalence classes  from the choice of value assigned to each attribute in the suggested repair  dictated by a cost metric . this separation has the potential to improve value selection by delaying it until a larger group of related values is known- for example  the phone number  1  is the highest weighted value of all the numbers for bob jones  while locally in the table  1  may look like a better choice. third  equivalence classes potentially ease user interaction by illustrating the relationships between parts of a proposed repair and allowing the user to validate updates one equivalence-class at a time.
　in this framework  we first consider straightforward use of the greedy method. in the example of figure 1  our repair procedure will group the phone-number attributes of and in a single equivalence class  and pick one of the values as the value proposed in the repair. we consider different cost models for the greedy step  as well as a variant of the greedy method which resolves fds aggressively. while naive implementations of our greedy methods require time cubic in the size of the data in practical cases  we introduce two optimizations  one relaxing the greedy method by sometimes taking a step which is not locally optimal  and another inspired by optimizations for duplicate elimination . together and under reasonable assumptions  these optimizations improve our algorithm's running time to   where is the size of the database including inserted tuples  and is the set of constraints to be enforced.
　our fourth and final contribution is an experimental study of our heuristic constraint repair methods. we evaluate the quality and scalability of our methods with both synthetic data from the tpc-h benchmark and real-world data scraped from web pages. we further evaluate the effectiveness of our two optimizations on running time and result quality. we find that the optimizations seldom degrade quality  but improve the performance significantly and hence allow the approach to be applied to real-world size problems. based on these results  we contend that heuristic constraint repair based on tuple-attribute equivalence classes is a promising tool for constraint repair in data integration and reconciliation scenarios.
organization. in the next section  we introduce our repair model. we prove intractability results in section 1 and introduce our heuristic approach in section 1  giving detailed algorithms in section 1. we present our experimental results in section 1  discuss related work in section 1 and conclude in section 1.
1.	system model and problem formulation
　in this section  we present our cost-based constraint repair problem formulation. in our model  each database instance  equivalently  database   contains a fixed set of tables where table is defined over a set of attributes . each tuple is associated with a table and a weight . note that weights are assigned at the tuple level to simplify the presentation  but in practice attribute-level weights may be preferable. to simplify the discussion we assume that one can keep track of a given tuple in during the constraint repair process despite arbitrary attribute value changes  using a temporary unique id  for example . we use to denote the value of a given attribute of in some database . this value is drawn from
         the domain of   plus the special value . further  for a subset of attributes from   we use to represent the projection of on attributes in .
constraints.	we consider the following two types of constraints:
1. functional dependencies  fds . each functional dependency has the form	  where	and	are subsets of attributes from	.	a database	is said to satisfy the fd	if for every pair of tuples such that	  it is the case that	.
1. inclusion dependencies  inds . inclusion dependencies have the form   where and are lists of attributes  with the same cardinality  from and
         respectively. a database	is said to satisfy the ind	if for every tuple	there exists a tuple	such that	.
a database satisfies a constraint set of fds and inds if it satisfies every constraint in .
database repairs. we now formally define the notion of a database repair introduced above. a repair of a database is a database such that 1  tuples appearing in are carried over to
 identified by  e.g.  id   possibly with modified attribute values 
1  zero or more inserted tuples appear in	but not in	  and 1  satisfies the constraint set . for convenience  we refer to the inserted tuples appearing in table in as .
　intuitively  an inconsistent database may be neither sound nor complete   and thus our model supports both value modifications and tuple insertions. we modify the values of tuples in rather than simply deleting them as in other models  e.g.    in order to minimize loss of information.
repair cost. the cost of an attribute-level modification in a repair is essentially the weight of the changed tuple times the distance according to a similarity metric between the original value of the attribute and its value in the repaired database. similarity measurement for strings and other structured values is itself a broad field  see  e.g.     and our setting does not depend on a particular approach. rather  we assume that for two values from the same domain  a distance function is available  with lower values indicating greater similarity. a common distance function for strings  the damerau-levenshtein or d-l metric   is defined as the minimum number of single-character insertions  deletions and substitutions required to transform to . we use this metric in the examples below and a similar metric in the experiments.
	finally  we assume that a cost	is associated
with each table   which is the cost of inserting tuples into in . this cost is a user-defined parameter closely related to the threshold set for a good match by the similarity metric. we find that an effective setting for is a value slightly higher than the distance between two  similar  strings  see section 1 for details .
　in our examples  the cost of a repair is the sum of the cost of the tuples in the repair. to summarize
if
otherwise
for instance  consider in the repair described at the end of example 1. given and string edit distances of
both from 1  to  1  and from  nj  to  ny  we get
                         while if 's phone number and state had been modified instead  we would have had since
	. the cost of the repair	of database	is defined as
.
　we are now ready to formally state our database cleaning problem in terms of computing a minimum-cost repair.
problem statement.	given a database	comprising tables and a set of constraints	defined on them  find the repair	of	for which	is minimum. 1.	constraint repair approach overview
　in this section  we investigate solutions to the constraint repair problem defined in the last section. we begin by outlining our approach to repairing individual constraint violations. we then consider the problem of finding minimum-cost repairs  but show that optimal solutions are generally intractable to find.
1	constraint repairs
　in general  it is useful to think of a database repair as the result of some modifications to database as illustrated by the following example.
example 1: we present a set of modifications that together constitute a possible repair of the constraint violations discussed in example 1.
1. tuple	: modify phone number to  1  repairing fd  1  and state to  ny   repairing fd  1    in both cases by matching .
1. tuple : modify name to  alice smith  and street to  1 bridge   repairs fd  1  by matching  .
1. tuple : modify phone number to  1   repairs ind  1    serial number to  l1   repairs fd  1   and installation date to  jan-1   repairs fd  1  .
1. tuple : modify phone number to  1   repairs ind  1  .
1. tuple : modify phone number to  1   repairs ind  1  by matching  .
　for a functional dependency over attributes and of table   consider a pair of tuples in that violate   that is    but . in this case  we can resolve this constraint violation by setting the attribute value of to be equal to  or vice versa  in the repair . this is illustrated in step 1 of example 1  where is modified to match . note that it is also possible to fix the fd by setting the value of attribute in tuple to be different from the -attribute value in tuple . we do not consider this option for fd repair because it is unclear as to what  different  value should be assigned to tuple 's attribute  and moreover  when the fds are keys  it may lead to insertions of entities that are not meaningful. for example  to repair the violation of fd  1  in the first step  we could have made up a new zip code for either or   but there does not exist a small reasonable set of candidates from which to choose.
　similarly  inds can be repaired by modifying attribute values. for example  if a tuple does not satisfy ind
       then we can modify 's -attribute value so that it is equal to the -attribute value for some tuple in table . alternately  we can consider modifying the -attribute value for some tuple in so that it is equal to 's -attribute value. step 1 above illustrates such a correction. here  when the phone numbers in and are similar  it is likely that one or the other is correct. it also seems clear from this example that an attribute-modification cost model is preferable to one based on tuple insertion and deletion: the violation of ind  1  can be repaired with the deletion of   presumably with minimal cost in a tuple-cost model. however  this seems to lose important information in this situation.
　finally  note that if no similar value exists for some unmatched tuple from   inserting a tuple in  in   may be preferable to modifying . in this case  the attribute s  of are set to match 's attribute s   in    and all other attribute values are set to the special value
　a subtle issue arises from the value. the sql standard  supports three different semantics for comparing the values of and which may involve   where
are sequences of attributes.	 1  the simple semantics defines
	to be	if either one of them contains
 1  the partial semantics evaluates
to if each non- value in equals its corresponding value in and vice versa.  1  the full semantics evaluates to if either one of them contains
　　while the sql standard does not explicitly support arbitrary fds and inds  it allows one to use any of the three semantics when dealing with unique and referential constraints  which are  special cases of  fds and inds  respectively. in the sequel we assume the partial semantics when is involved. note that this semantics allows to participate in comparisons of attribute values.
1	minimum-cost repair  intractability results 
　we present two intractability results for min-cost database repair  showing that the problem is np-complete even for a small  fixed number of only fd or only ind constraints.
theorem 1: let be a set of only fd or only ind constraints defined on database . then  for a constant   the problem of determining if there exists a repair of whose cost is at most is np-complete.
proof sketch: the proofs are by reductions from the vertex cover problem  even when contains only fds   and from the 1dimensional matching problem  even when contains only inds . each reduction uses only a constant number of constraints  either fds or inds . proofs are omitted due to space constraints  and can be found in the full version of the paper. 
　interestingly  the corresponding repair problems  when contains only fds or only inds  are shown to be tractable for a deleteonly repair model by . this demonstrates that the repair problem becomes much more difficult when we consider value modifications.
1.	using equivalence classes for constraint repair
　in light of theorem 1  we necessarily consider heuristic approaches to constraint repair. a particular heuristic algorithm will take as input a database and a set of constraints defined on   and find a repair of . it should be able to find efficiently  with the tradeoff that is not necessarily minimum. in fact  we found it non-trivial to develop such a heuristic. the key difficulty  of course  is that repairing one constraint can break another  and most simple heuristics we considered could fail to terminate in the presence of complex  inter-related dependencies.
1	equivalence classes
　to overcome these problems  our approach to constraint repair for fds and inds is built around the notion of equivalence classes of attribute value coordinates   where identifies a tuple in a table in which is an attribute. the semantics of an equivalence class of pairs is that the tuple attributes contained in the class are assigned the same value in .  we assume that all such attributes in an equivalence class have the same domain . our motivation for considering equivalence classes is that both fd and ind constraints can be seen as specifying equivalence between certain sets of attribute coordinates. for example  an fd essentially specifies that if a pair of tuples
in matches on the attribute set   then and must be in the same equivalence class for all . similarly  for an
ind   we require that each tuple is covered by some tuple   or alternately  and
are in the same equivalence class for each attribute in and the corresponding attribute in .
　a key observation here is that it is useful to separate the decision of which attribute values need to be equivalent from the decision of exactly what value should be assigned to the eventually-produced equivalent set. delaying value assignment allows poor local decisions to be improved-for example  consider a name that is sometimes spelled correctly and sometimes incorrectly. if the correct spelling is more frequent and/or has higher weight  then the accumulation of versions of the name in an equivalence class over time will allow the correct spelling to be chosen in many cases. further  we believe that the equivalence class abstraction will be valuable
procedure fd-resolve-tup
input: set of tuples	that match on attribute set	 
       fd	. begin
1. for each attribute	in	do
1. ;
1. ;
1.
end
	figure 1: resolving set of tuples	for fd	.

to a user who needs to check or modify a repair. the classes help expose the structure of data relationships  and if the user wants to override a value chosen by the repair algorithm  it can be accomplished on the whole equivalence class in one step.
	an equivalence class	is a set of tuple  attribute pairs
our repair algorithm maintains a global set of equivalence classes that covers  that is  the tuples in the original database plus insertions . for a given pair   returns the current equivalence class containing in . associated with each class is a  target value  . the target value is funda-
mental to the construction of the database repair	  since
is defined as . thus  all attributes in a class are assigned the value of in the repair.
equivalence class cost. the cost of the equivalence class for a particular target value is defined as the contribution of elements in the equivalence class to the cost of  ignoring the cost of inserts ; that is 	.
consistent with the goal of finding a low-cost repair  is chosen to minimize the cost of   and unless specified otherwise  is simply the minimum over some universe of potential values  such as the values taken by elements of in . as an example  in the database shown in fig. 1  for
 
while	. thus  the value	is
 1   and	.
merging equivalence classes. whenever two equivalence classes are merged  this may result in additional attribute modifications in   increasing its cost. for a subset of equivalence classes from   we formalize this increase in cost as
; that is  the difference between
the cost of the merged class and the sum of the costs of the individual classes. for instance  referring back to fig. 1  the cost of merging classes	and
to form
is given by
.
1	repairing violations
　we next discuss how individual constraint violations are repaired by resolving tuples.
repairing an fd violation. we say that a tuple is resolved w.r.t. an fd if  for all other tuples   either for some   or for every  
　　　　　　　　　. note that if is resolved  it is not part of a violation in   but the converse need not hold since for some   might have the same target value as without and being in the same equivalence class. clearly  a tuple can become unresolved w.r.t. due to a change in the target value of an attribute in for some other tuple in   a fact which we refer to as the collision property of fd resolution. this might happen  for example  due to changes in target values when
procedure ind-resolve-tup
input: tuple	to resolve  a target	which is either a tuple	or	  ind	. begin
1. if  	  then
1. tuple in	with 1 weight; 1.	;
1.
1. for each attribute	in	and corresponding	in	do
1. ; end
	figure 1: resolving tuple	for ind	.

equivalence classes merge.
　while a violation can be explained in terms of pairs of tuples  we define the act of resolving a tuple w.r.t. in terms of a set of tuples from . here includes and all other tuples that agree with on  target values of  attributes in . the procedure fd-resolve-tup shown in fig. 1 shows how to resolve such a set by merging  for each attribute in   the equivalence classes for . accordingly    the merge cost of resolving w.r.t.   is the sum  for each attribute in   of . for example  in
fig. 1  the tuple set is resolved w.r.t. fd  1  by merging the classes and . thus 
.
repairing an ind violation. for an ind   a tuple is said to be resolved with respect to if there is some tuple such that and are in the same equivalence class for every pair of corresponding attributes and
　　　. it is easy to see that  in contrast to fds  a tuple resolved w.r.t. an ind will not become unresolved  a fact we refer to as the permanency property of ind resolution. thus  by resolving all tuples w.r.t. inds  we can ensure that no inds are violated.
　tuple is resolved by  covering  it with either a new or existing tuple . here a new tuple consists of   i.e.  for each attribute of . this is accomplished by procedure ind-resolve-tup shown in fig. 1.
this procedure creates	if required  and merges	with for corresponding attributes	and	from	and
respectively. accordingly  the cost of resolving w.r.t. using     is the sum of the attribute-wise costs 
	for corresponding attributes	and
  plus the insert cost of	if it is new.	 note that if
is new  then	is simply	since	is assigned 1 weight. 	for example  in fig. 1  tuple	can be resolved w.r.t. ind  1  by merging the classes and	. thus 
.
1.	repair algorithms
　in this section  provide detailed descriptions of equivalenceclass-based constraint repair. we present a general heuristic framework that guarantees termination  and develop two specific heuristic methods greedy-repair and greedy-repair-fdfirst.
finally  we discuss optimizations and extensions.
　at a high level  our repair algorithm begins by putting each tuple  attribute pair in its own equivalence class. it then greedily merges the equivalence classes of pairs until all constraints in are satisfied. to illustrate  consider tuples and for bob jones in fig. 1. in order to satisfy fd  1   we group tuples and on phone number to form the equivalence class
procedure gen-repair  	 
input: database	  constraint set	.
output: database repair	.
begin
1. ;
1. initialize	sets for fds and inds;
1. while  	is not empty 
1. picknext   ;
1. if  	is an fd  then
1. fd-resolve-tup	;
1. else
1. ind-resolve-tup	;
1. process	sets affected by resolution step for	;
1.
1.return	; /* obtained by inserting new tuples into	and assigning
	each	the value for	  i.e. 	*/
end
figure 1: generic equivalence-class based repair procedure.

. next  to ensure that ind  1  holds  we
form two equivalence classes:
. this ensures that
	tuple	in	is covered by	and	in	.
         . this ensures that tuples and are also covered by and .
thus  in the final repaired database  all tuples for bob jones: will have identical phone number values; as a result  these tuples will satisfy constraints fd  1  and ind  1 .  additional equivalence classes involving the other attributes will be needed to satisfy the remaining constraints - we list these in example 1  . we now present our heuristic algorithms in detail.
tracking unresolved tuples.	our overall approach is to resolve  unresolved  tuples one at a time  until no unresolved tuples remain. while not strictly required for correctness  an important efficiency optimization is to keep track of potentially unresolved tuples for each dependency in	. to accomplish this  we maintain a data structure	which maps each constraint	to a set of tuples. our repair algorithms ensure that the maintained sets satisfy the following two invariants:  1  if	is unresolved w.r.t.	 	  and
 1  if is unresolved w.r.t.   then some tuple which matches on attributes in is guaranteed to be in
             ; here serves as a proxy for   and when it is resolved  will also be resolved.
　our maintenance algorithms perform the following actions on the sets that can be shown to preserve the abovementioned invariants:
initialization:	for each ind	  is initially set to	. for each fd	 	is initialized to contain all the tuples in	.
after each resolution step: when a tuple	is resolved w.r.t. a constraint	  the following actions are taken: 1 	is removed from	  1  a newly inserted tuple into table is added to	if	is an fd on table	or an ind of the form	   and 1  if resolution causes equivalence class merging  such that	changes due to the merge  then we add	to	for any where	and	.
procedure pickgreedy   
output: the constraint to repair next  and the tuples to resolve for the constraint.
begin
1. ;
1. foreach fd	 	do
1. ;
1. if	then
1. :=  	;	;
1.
1. /* if  	and	  then return	; */
1. for each ind	do
1. let	be	with minimum	;
1. if  	  then
1.
1. if  	  then
1. :=	;	;
1.
1.return	;
end
figure 1: greedy selection of the lowest-cost resolution.

it is easy to see that  1  above preserve the two invariants on sets  since this follows directly from the permanency
and collision properties of inds and fds  respectively.
1	repair with equivalence classes
　in fig. 1  we present gen-repair  the overall driver for all of our constraint repair procedures. it is abstracted in terms of a function picknext which selects the next tuple to be resolved w.r.t.a constraint . if is an fd   then picknext also returns the target set of tuples to resolve-this set essentially consists of tuples in that agree with on attributes in . on the other hand  if is an ind  then the target returned by picknext is either another tuple or to indicate that should be covered by a newly-created tuple. note that at line 1 of gen-repair  is maintained as described earlier. the proposed repair is produced by inserting new tuples and replacing values in with . the arbitrary selection of what tuple and constraint to address represents the degree of freedom for designing an equivalence-class-based technique  and we present two intuitive greedy approaches in the next subsection.
correctness.	clearly  the same tuple may enter and leave for an fd	many times. nevertheless  we now argue that picknextselects a tuple from	to resolve next and returns only a bounded number of	tuples to fix ind constraints  and that consequently gen-repair terminates and produces a repair	of	.
theorem 1: the number of tuple inserts is bounded for picknext  and gen-repair terminates and produces a repair of that satisfies the constraints in .
proof sketch: the number of inserts by picknext is bounded because  1  the number of equivalence classes determined by the original database is bounded by   where is the maximum number of attributes in any table   1  for each unique pattern of such equivalence classes at most one new tuple is inserted   1  for each attribute in a table no new atomic value  resp. new equivalence class  is added  at any time  and  1  an inserted tuple consists of only data from and . from these it follows that the number of equivalent classes containing data from and newly inserted tuples is bounded by at any time  and thus the number of newly inserted tuples is bounded.
　termination of gen-repair follows from the following points.  1  every iteration removes at least one tuple from .  1  tuples are only added to when tuples are inserted  whose number is bounded  or equivalence classes are merged.  1  the number of merge events is bounded by the number of equivalence classes  which is at most . thus gen-repair invokes picknext at most times  where is the number of tuples in the database  including new tuples . the correctness follows from the fact that is empty when gen-repair terminates  and thus  due to the invariants maintained on   all tuples are resolved w.r.t. constraints in at termination. 
1	two flavors of greedy repair
　in this subsection  we build our two proposed algorithms for constraint repair in this paper  greedy-repair and greedyrepair-fdfirst by making simple changes to picknext. our first algorithm  greedy-repair  is built from gen-repair by replacing picknext with pickgreedy shown in fig. 1. this routine picks and returns an unresolved tuple to repair with the minimum . in the case of an ind constraint  it also returns the lowest cost target of the resolution  which may be a tuple or if an insert in the target relation is the lowest cost step.
　to motivate our next algorithm  greedy-repair-fdfirst  we observe that there is a fundamental difference in the manner in which fds and inds are repaired in our framework. fd repair  in many respects  is more rigid than ind repair. for an fd
	  and a pair of tuples	that violate the
fd  repair involves modifying tuple attribute values so that and match on . while we have some flexibility in the tuples we choose to modify   or   for each non-matching attribute in   the only choice we have is between the values appearing in and   which may or may not be similar. in contrast  ind repair is much more flexible. for an ind   and a tuple that violates the ind  repair can be achieved by considering
any tuple in   and modifying attribute values so that and match on the corresponding attributes in and . essentially  any tuple in can be considered as the covering tuple for in order to repair the ind.
　due to the rigidity of fd repair  we consider the variant which gives precedence to fixing fds. this is accomplished by uncommenting line 1 of pickgreedy so that an unresolved tuple for an fd will be returned if available. if not  the lowest cost tuple for an ind repair is returned.
example 1: consider the	and	tables depicted in fig. 1.
we trace the sequence of resolution steps performed by the variant of our greedy heuristic when it is run on tuples for bob jones. in the following  the  target value  of an attribute
of tuple refers to . we only list below the steps that result in new classes due to merges.
　1. resolve tuples w.r.t. fd  1   since these two tuples match on name  street  and zip    . this results in the equivalence class with target value  1 . also  since the phone number in changes  it is added to	.
　1. resolve tuples	w.r.t. fd  1  since	and	now match
on	  . this causes classes forandto be merged for every
to be updated to  ny .attribute  and	's targetvalue	1.	resolve tuples	andw.r.t. ind  1     . this yields the class value  1 .with	1.	resolve tuples	andw.r.t. ind  1     . this causes the phone numbers of tuples and to be merged into the same equivalence class with value  1 . since the target value of the phone number attribute for changes to  1   it is added to .
　1. resolve tuples and w.r.t. fd  1  since their target values now match on      . this results in the equivalence class with value  l1 . since the target value of the serial number attribute for changes to  l1   it is added to .
　1. resolve tuples and w.r.t. fd  1  since they now match on    . this causes classes for and to be merged for every attribute  and 's install date value
to be updated to  jan-1 .
　1. resolve tuples and w.r.t. ind  1 . the phone numbers for tuples are merged into the same class with value  1 . the value of the phone number attribute for is changed to  1 .
　thus  in the final repair  all of bob jones' tuples have identical phone number values of  1 . further  the state in tuple is modified to  ny   and the serial number and installation date in are modified to  l1  and  jan-1   respectively. thus  the tuples satisfy all the fd and ind constraints.
similarly  for alice smith's tuples  our heuristic will correct the name and street in tuple   and the phone number in tuple .
looking ahead for fd costs. in the algorithm greedyrepair-fdfirst  we perform fd repair first to limit the effect of cross-tuple merging on the larger equivalence classes created by ind repair. another approach to minimizing undue effects from ind repairs is to attempt to avoid bad repairs  that cause many fd violations  by adding some degree of lookahead to the cost model. in order to do this  we modify the of tuples to include an approximation of the cost of resolving tuples added to the set of functional dependencies. note that this is similar in spirit to the idea of preferring 1 entity matching .
1	improving performance
we now analyze the running time of greedy-repair and
greedy-repair-fdfirst in an attempt to predict their practical behavior  and introduce three important heuristic optimizations.
running time. with very complicated overlapping constraints  a finite but exponential number of new tuples could be inserted in the course of a constraint repair. however  our experiments  which will be presented in the next section  demonstrate that when is correctly set  the number of tuples inserted is far fewer than the number of tuples in the original database. accordingly  we assume below that the number of inserts is linear in the original database size  and for simplicity we use the parameter as the number of original tuples in the database plus all the inserts.
　recall that is the maximum number of attributes of any table in   and that is the number of constraints. another important parameter is   the largest size of any equivalence class encountered during a run. of course  this is in the worst case  but is practically much smaller. given these parameters  the while loop of gen-repair  figure 1  repeats at most times  since one equivalence class is removed by each pass. in the worst case  in pickgreedy  finding the lowest cost target for each tuple in for an ind can take steps  where each step takes time to compute the cost of the new equivalence classes. there are potentially unresolved tuples. the cost of resolving the tuple is not trivial since it includes the cost of adding any new tuples to   but is not excessive since an index can be maintained on the set of attributes involved in the lhs of each fd. thus  the time complexity of pickgreedy is
                 and the overall running time of gen-repair  in the worst case  is   is omitted since it is a constant when the schema is fixed .
this clearly indicates that  while technically tractable  neither
greedy-repair nor greedy-repair-fdfirst will scale well to large data sets. we now introduce three optimizations which  as will be shown in the next section  make a substantial improvement in scalability without greatly affecting quality.
redundant computation. as mentioned above  the most expensive part of is the search for covering tuples for inds in lines 1 of fig. 1  since the computation of at line 1 can be assisted by a hash table. we observe that most cost evaluations between one execution of pickgreedy and the next are redundant; this optimization seeks to avoid this redundant computation. we now define some notation: let be a global timestamp which is incremented before any equivalence class change. let be the value computed for a tuple with respect to ind in pickgreedy. assume that represents the timestamp at which the last evaluation of pickgreedy for this tuple took place. let be the set of tuples for which the equivalence class of some attribute has changed at a timestamp greater than .
　now consider how to find the new for with respect to constraint on the next call to pickgreedy. when neither or has changed since   we argue that the new for with respect to is either   or it inolves a tuple from . this follows directly from the definitions  since was optimal at time and no other costs have changed. if an equivalence class of or has changed  however  then we must scan the entire target table. this optimization does not affect quality  and is always used.
　for a given tuple   the equivalence class of can only change times  potentially reducing the term to . however  extreme cases can be constructed in which for every step  1  some tuple appears as for every unresolved tuple and ind   and 1  is modified at that step entering
. thus  though the optimization is prac-
tically important  the worst-case running time is unchanged.
nearby tuples. to further improve running time  when trying to satisfy inclusion dependencies  we limit the number of target tuples considered for each source tuple  based on techniques from duplicate elimination  1  1  1 . for each attribute in relation appearing on the right-hand side of some ind  we produce a set of indexes of based on different features abstracted from the attribute . in particular  we keep one list sorted by attribute value; and we create another list by first sorting the characters in the attribute and then using this attribute to sort the list. when looking for target tuples with which to resolve w.r.t. ind
       we probe values from 's attributes to access each sorted list for each attribute in . we then examine tuples  starting with the best-matching attribute according to the attribute-level similarity metric employed. this produces a candidate set of tuples  which we then order on our tuple-cost metric  and return the first . we refer to the resulting optimization as nearby   . when combined with the last optimization  we intersect the nearby list with the recently changed list. this optimization improves the worst-case running time to	.
relaxing greedy. our third optimization is to relax the pickgreedy routine so that it is much more efficient  but does not always choose the lowest cost merge to do next. to this end  we initially create a queue of triples    where for some ind   and is the best fix found by pickgreedy  i.e.
　is either or an existing potentially covering tuple . the tuples in the queue are sorted by the cost of resolving with . whenever the gen-repair calls   the lowest-cost unresolved tuple in the queue is considered  and the following steps are taken: 1  s best repair and repair cost with respect to is recomputed  using nearby if this option is in force. 1  if 's cost is unchanged or reduced  it is chosen without examining other tuples. 1  if 's cost has increased  it is resorted into the queue  and the process continues with the next tuple. this technique is referred to as queue. since at step 1 there may be some entry later in the queue with a better cost  the lowest cost greedy step is not always found. however  queue does ensure that when resolving a tuple
w.r.t. an ind   the currently-lowest cost tuple from the target table with which to cover is used.
　construction of the queue takes at most time. as with the first optimization  if every tuple's best match is the same tuple  and that tuple changes on each step  this optimization does not improve running time. however  if we limit each step to resorting a constant number of tuples  then in conjunction with nearby the worst case running time is in . as we will see in the next section  this optimization is also practically very important.
1.	experimental evaluation
　in this section  we present an experimental study of our constraint repair techniques. we investigate the utility  scalability and sensitivity to noise of our low-cost constraint repair heuristics on synthetic tpc-h data and a collection of real datasets.
1	experimental setting
　we perform our experiments with 1  artificial datasets generated by the tpc-h  benchmark and 1  real-life datasets containing dvd information. all the experiments are run on similar machines  powered by either 1 mhz or 1 ghz pentium 1 processors.
tpc data. for tpc data  we create a clean tpc-h instance by using the tpc program with different scaling factors. we refer to this dataset by its size. for example  using a scaling factor of .1  yielding approximately 1 tuples  is referred to as  tpc.1k . we then introduce noise to each attribute involved in an
ind or appearing on the left-hand side of an fd with probability to produce . when noise is introduced  with probability
    the confusion metric  the value of the attribute is replaced with another value found in the same column. otherwise  the noise introduced is a textual error guaranteed not to cause a collision  insertion of a  !  at a random location in the string . note that noise may only be applied to those attributes that may cause constraint violations in . finally  one of our algorithms is used to repair constraint violations in   producing a proposed repair .
　the tpc model includes 1 fds and 1 inds generalizing the keys and foreign keys in the dataset. an fd on the supplier table is  e.g.  thesupplierkey name address phonenumber region.
dvd data. for dvd datasets  we scrape information from three e-commerce web sites  amazon  dvd empire and digital eyes. each dataset is obtained by searching each site for a single keyword  and filling in a table of information about the dvds found. we consider 1 arbitrary search words to generate 1 different datasets. each generated table has the following columns: title  dvd release year  theatrical release year  aspect ratio  length  rating  anamorphic  'y' or 'n'   and a unique id where possible  amazon and dvd empire . the title and dvd year columns
 1	 1	 1	 1	 1	 1	 1
noise
	figure 1: tpc quality 	.
 1	 1	 1	 1	 1	 1	 1
noise
	figure 1: tpc performance 	.

form the key. because there were many natural discrepancies between the three sites  no artificial errors were introduced.
　for each table we define an fd from its key to all of its other fields. furthermore  we define an ind from the two key fields in each table to another table  such that the only way to satisfy all of the dependencies is for each table to have an identical set of movies. algorithms. we have implemented prototypes in java of the greedy-repair  gr  and greedy-repair-fdfirst  gf  algorithms  with the redundant computation optimizations mentioned in section 1. we also implemented the nearby and queue optimizations  which can be used independently or together  as well as lookahead for fd costs  fdl .
cost models. we now show the cost function used in these experiments. we start with a string edit distance function from damerau-levenshtein. for tpc data  we use a modified that excludes character replacement operations  allowing only inserts and deletes. the distance between '!1' and '1' therefore increases from 1 to 1. we make this change because the key data in tpc consists of densely populated integers. without the change  an -digit string with a '!' noise character inserted  e.g. '!1'  would have the same edit distance of 1 to many    -digit numbers  e.g. '1'  '1'  '1'  etc.  as it would to the original string. even with this change  however  '1' is just as close to '1'  '1'  etc.  as it is to '!1'  so errors are still often hard to correct.
	we then define the attribute-level distance function	as

where when   otherwise  to ensure that different strings can never have a vanishingly small cost. longer strings with a 1-character difference are closer than shorter strings with a 1-character difference  but a 1-character difference in short strings does not cause a disproportionately high cost. the used in pickgreedy  shown in fig. 1  is then defined to be the sum of across all attributes being merged  multiplied by the percentage of attributes being merged that are not exact matches.
　another parameter that must be set is the insert cost . for tpc  no entities are missing  so for all tables is set to infinity. for the movie data  however  missing data is common. we find that a generally good setting for is slightly more than
  where	and	are short strings that differ by 1 character. we therefore use	for the dvd experiments.
measuring repair quality. to measure repair quality  one approach is to consider the cost of the repair found by the algorithm  and compare this cost to some reference  such as the optimal  minimum cost  repair. unfortunately  we know of no effective algorithm to find the optimal repair for non-trivial data sizes. furthermore  since our cost metric is new  showing that a low cost solution is found does not prove that a good solution is found.
　since the tpc-h data is synthetically generated  we have an original correct instance available  though it is not seen by the algorithm  of course . thus we can measure the number of errors corrected as the errors  the number of attribute-level differences between and   minus the errors remaining  the difference between and  . the quality of a tpc solution refers to the ratio of errors corrected to errors.
　for movie data  measuring quality is more problematic. one approach would be to hand-merge each data set to determine an ideal merge. however  this is impractical for the larger data sets. we chose to approximate the hand-merge by building a perl script to evaluate the merges done to reach . this script has a set of  expert  rules  created by studying the evaluated movie data  which determines if two movies are the same. given a repair  this script counts the number of bad merges between movies that are not the same. it then calculates an  adjusted size   adjsz   which is
                   where is the number of movies  and is the number of bad merges. lower values of adjsz mean that the repair created a more compact  accurate database. so  for example  if a database should have 1 tuples  the correct result would have an adjsz of 1  with 1 bad merges. but if a repair merged those two tuples into one incorrectly  the adjsz would be .
in order for the bad repair to have a worse adjsz than the correct repair  we have set to be in our experiments. we compare our algorithms with a repair  which simply merges any two dvds when the title and year are exact matches  and report the quality as the improvement in adjsz of the given algorithm over the naive repair.
1	experimental results
tpc result quality. these experiments quantify how much our algorithms improve the quality of the data. for the tpc data  each experiment was repeated 1 times with different random seeds for error introduction  except for the scaling experiments  which were done 1 times each . we test with values ranging between 1% and 1%. the results with on tpc.1k are shown in
fig. 1.	we see that greedy-repair performs worse than the
greedy-repair-fdfirst-based algorithms. we also find that larger increases the difference between greedy-repair and
greedy-repair-fdfirst  not shown .
　the large error bars show that greedy-repair occasionally makes very bad repairs. this happens when an expensive fd is deferred while many mainly similar equivalence classes are merged due to inds  only for them all to ultimately be merged together.  greedy-repair-fdfirst avoids this problem by never deferring fds.  interestingly  as noise increases  this problem happens less frequently  as large merges of mainly similar equivalence classes become less common.
figure 1 also shows that the optimization techniques do not de-

ball blue city girl heart house kill king figure 1: repair quality on movie dataset

figure 1: performance on dvd data

grade the quality of the results for this dataset and dependency set. meanwhile  figure 1 shows that the optimization techniques do have a significant effect on runtime  user-time .
dvd result quality. figure 1 shows the percentage decrease in adjsz over naive for each algorithm for a subset of the databases we test. whereas greedy-repair was inferior to greedyrepair-fdfirst on tpc data with non-zero   it is the same on the dvd dataset  and is not shown.
　in these results  once again the queue and nearby optimizations do not degrade the results  compared to greedy-repairfdfirst  hollow bar . in fact  the nearby optimization improves the results  because for matches at equal edit distances  it prefers the match which is closer alphabetically.  this is consistent with research  that it is beneficial to include the length of the common prefix of two strings in a string matching score. 
　for some movie datasets  our string matching function does poorly matching titles in the generated database. for example  the query  ball  generates many long similar titles that however refer to different movies  such as  dragon ball z: garlic jr. - vanquished  and  dragon ball z: garlic jr. saga . on such a data set  the results are in general poor  as is shown in the figure. adding the fd lookahead heuristic  however  enables the algorithm to avoid many of the bad merges  greatly improving the quality of the result.
　figure 1 shows the performance of the different algorithms. nearby and queue are needed to scale  and adding fd lookahead degrades performance  but not dramatically.
　furthermore  gf + nearby    is actually slower than gf by itself. the reason for this is that the redundant computation optimization is very effective when the set of changed tuples that needs reevaluation is small. when only one or two tuples have changed  the time needed to use nearby to determine if those tuples should be considered is greater than the time needed to simply recompute for those tuples. so  for small change sets  nearby actually degrades performance. for greedy-repair-fdfirst when all tuples are considered after each change  only one new equivalence class can be created in a column between computations  and thus the change set is limited by the size of an equivalence class.  with the queue optimization  multiple equivalence classes are created between computations on a given tuple  and thus change sets are bigger.  for dvd data  the equivalence classes are always small and so gf+nearby    performs poorly.
scalability.	in fig. 1 we vary the size of the tpc instance from
1 to 1 tuples  and compare the running time of greedy-
repair-fdfirst alone with nearby  queue  and with both
nearby and queue. because the equivalence classes are larger than those in the dvd data  the nearby optimization is very effective  and is critical for the algorithm to scale with this dataset

figure 1: scalability of greedy-repair-fdfirst
size.
impact of constraint sets. an advantage of the algorithms discussed in this paper is that they give the user flexibility in specifying the integrity constraints that need to be met. there are often many constraint sets with equivalent semantics. in figure 1  using greedy-repair-fdfirstwith queue and nearby     we compare two runs on the dvd data. in one run  we use a dependency set that consists of inds across all of the attributes of all of the tables  instead of just the key fields . in the other run  for each table we add fds from the key fields to all the other fields. either dependency set could be reasonably used for matching up dvds between the different databases. as the figure shows  much more consistent and better results are obtained with the fd presence.
varying nearby.	we also experiment with greedy-repair-
fdfirst and queue with 1% noise  while varying nearby. the results are shown in fig. 1.  the right-hand y-axis  used for the two movie datasets  is inverted so that better  lower values of adjsz are higher.  it shows that for the dvd data  with real strings  even a nearby of 1 is sufficient for good results. for the tpc data  where the text data is actually integers  it shows that higher values of nearby are needed. in fact  as is shown for in
tpc.1k  values of nearby that are too low may cause the algorithm to fail dramatically  finding a repair with a very high cost. if the nearby string set frequently fails to include the correct choice  a good repair cannot be found. moreover  because our nearby sort orders do not perform well on the integer keys seen in tpc  larger values of nearby are needed. for tpc.1k  nearby of approximately 1 is sufficient  whereas for tpc.1k  nearby of 1 is required to get optimal results.
　this graph also shows that for the same amount of noise in tpc data  a higher percentage of errors can be corrected in a larger database. this is because a larger tpc database has more redundant values  and hence more errors are correctable.
　figure 1 shows that excluding the failed repair  performance scales linearly with nearby  as expected.
summary. we have presented several results from our experimental study of cost-based constraint repair. first  we find that our equivalence-class-based constraint repair heuristics  in almost all cases  substantially improve the quality of an inconsistent tpc or dvd data set. second  we compare several heuristics and find that greedy-repair-fdfirstwith nearby and queue scales much better than other combinations yet produces repairs of similar quality. finally  we note that the inclusion of fds and fd lookahead can significantly improve repair quality. while our experiments on the dvd dataset perform essentially a data-linkage task  we do not compare ourselves with other data linkage systems. these systems have rule-bases for matching and carefully tuned comparison metrics  and will presumably achieve better results if applied to matching movie titles. instead  our goal is to show that a general constraint-repair facility can reasonably perform this wellknown task  and that our cost-based model affords the opportunity to integrate techniques from record linkage  like nearby  into a practical constraint repair system. finally  it is easy to see that constraint repair generalizes data linkage  since constraint repair 1  has the added burden of suggesting a repair once linkage is accomplished  and 1  handles complex data models and can generalize to additional kinds of constraints.
1.	related work
　data cleaning systems described in the research literature include the ajax system  which provides users with a declarative language for specifying data cleaning programs  and the potter's wheel system  that extracts structure for attribute values and uses these to flag discrepancies in the data. most commercial etl tools for data warehouses have little built-in data cleaning capabilities covering mainly data transformation needs such as data type conversions  string functions  etc.  presents a comprehensive survey of commercial data cleaning tools  as well as a taxonomy of current approaches to data cleaning. while a constraint repair facility will logically become part of the cleaning process supported by these systems  we are not aware of analogous functionality currently in any of the systems mentioned.
　most closely related to our work is the line of research on inconsistent databases  e.g.   1  1  1  1  1  1  1  1  1    i.e.  databases that violate given integrity constraints. a semantic notion of minimal repair was first introduced in  in terms of symmetric difference of the original database and its repair under set containment. consistent information is obtained from an inconsistent database following two approaches: repair is to find another database that is consistent and minimally differs from the original database  1  1  1  1 ; and consistent query answer  a notion also introduced by   is to find an answer to a given query in every repair of the original database  1  1  1  1  1  1 . most earlier work  except  1  1   either adopts tuple deletions as repair primitive and requires that a repair is a subset of the original database   assuming that the database is complete yet not necessarily correct  or allows both tuple insertions and deletions  1  1  1  1  1  1   assuming that the database is neither sound nor complete. recently tuple modifications were studied as repair actions for consistent conjunctive query answer  and census data repair . in these settings  complexity results  1  1  1  1  1   algorithms  1  1  1  1  1   constraint rewriting techniques   representations of all repairs with logic programming  1  1  or tableau   and constraint repair based on techniques from model-based diagnosis  were developed  for single database  1  1  1  1  1  1  1  and integration systems  1  1  1   see recent surveys on consistent query answer  and on constraint repair  .
　while our work was inspired by prior work on constraint repair  it differs from earlier approaches in the following. first  our repair model allows attribute values to be modified for restoring constraints and introduces a cost-based notion of minimal repairs. while  1  1  also allow value modifications  the applicability of their techniques is restricted to specific databases or certain constraints. indeed  in contrast to the generic setting of data cleaning in the presence of both fds and inds studied in this paper   considers detecting and solving conflicts for specific census databases of a fixed schema  and  studies consistent answer of  conjunctive  queries in the presence of universal  full  constraints  which cannot express inds. furthermore  our model introduces the novel and practical notion of minimality for repairs based on costs measured by tuple weights and value similarity for each modification  which was not considered by any previous work. while most of prior models focus on tuple deletions/insertions  1  1  1  1  1  1  as repair actions  we would like to emphasize that an attribute-modification cost model seems preferable as discussed in section 1. however  computing repair cost in terms of value modification rather than tuple insertion and deletion significantly complicates the repair problemin the presence of fds and inds. for instance  with modifications  an uncovered tuple that violates an ind can be covered by modifying any of the tuples in the covering table  in example 1  tuple that violates ind  1  can be covered by updating the phone number field in any of the tuples in the customer table to  1 .  thus  with modifications  the search space for repairing constraint violations explodes  making the search techniques of the earlier work impractical. second  our np-completeness results  section 1  extend the complexity results developed for constraint repair  and consistent query answer  due to the connection between the complexity of these two issues established in : in the presence of foreign keys  the problem of constraint repair is logspace-reducible to the complement of the problem for consistent query answer . indeed  our results show that in the presence of attribute value modifications as legitimate repair actions  the repair problem is intractable even when either inds alone or only fds are allowed  while in contrast   shows that in a delete-only repair model  the corresponding problem is tractable  and it becomes co-np hard if arbitrary fds and inds are put together. finally  to the best of our knowledge  our equivalenceclass-based approach yields the first effective heuristic algorithm for restoring the database to a consistent state  and leads to a practical tool for data reconciliation and data cleaning. in contrast  earlier approaches in the presence of value modifications require exponential time  combined complexity  for corrections   or expensive tableau construction   the termination problem for its chasebased process is undecidable when it is generalized to deal with both inds and fds .
　a related topic of interest in data cleaning is the elimination of approximate duplicates  also referred to as the object identity or merge/purge problem; see  for example   1  1  1  1  1  1 . this problem frequently occurs during the integration of disparate data sources  such as medical records  address lists or census records. previous work has focused on the statistical foundation of feature matching   issues associated with string matching  1  1  and the performance issues associated with avoiding pair-wise comparisons of every tuple in a large table. in   the authors present the ajax system for cleaning which includes approximate matching with an sql-like syntax. for performance   proposes a method which considers multiple sorts of the database using different combinations of attributes  which inspires our nearby optimization. it should be mentioned that the algorithms developed for detecting approximate duplicates  1  1  1  compute clusters  transitive closures  of records  which can also be understood as computing  equivalence classes  of  tuples . our algorithms are more involved than theirs as we need to compute equivalence classes of attribute and tuple pairs in order to repair fds and inds across multiple tables of related entities  rather than to find approximately duplicate records in one or two tables.
　finally  there has been a considerable amount of work in recent years on schema matching  see  e.g.   for a survey  in the context of schema integration. this  along with value reformatting  is a necessary pre-step to database reconciliation by constraint repair.
1.	concluding remarks
　in this paper  we observe that an important use of data integration is database reconciliation  that is  to correct errors introduced by source databases. we model this problem as one of finding lowcost repairs of constraint violations in an integrated database. having shown the intractability of the problem  we introduce a heuristic approach based on equivalence classes of  tuple  attribute  pairs. in this context we consider specific algorithms and a number of performance optimizations. we demonstrate the utility of the approach and the scalability of our algorithms with experimental evaluation on synthetic and real data. not only does this approach help to suggest reasonable repairs  but it allows the reuse of match metrics developed for record linkage in the context of constraint repair.
　for future work  we intend to investigate extensions within the relational model  since sql allows more general  not exists  style constraints. second  we are looking to extend our results to cover semi-structured data as well; specifically  we intend to develop schemes for repairing constraint violations in the context of xml data integration. finally  we intend to more explicitly address the interactive formulation of the problem since we believe user involvement will be important in constraint repair  as in any other area of data cleaning. this will involve giving the user appropriate control over the repair process as well as the development of appropriate visualization tools for the proposed repairs.
acknowledgements. the authors wish to thank jan chomiki for helpful discussions and comments on a previous draft of this paper.
