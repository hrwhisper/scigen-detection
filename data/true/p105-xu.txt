this work proposes and evaluates a probabilistic cross-lingual retrieval system.  the system uses a generative model to estimate the probability that a document in one language is relevant  given a query in another language.  an important component of the model is translation probabilities from terms in documents to terms in a query.  our approach is evaluated when 1  the only resource is a manually generated bilingual word list  1  the only resource is a parallel corpus  and 1  both resources are combined in a mixture model.  the combined resources produce about 1% of monolingual performance in retrieving chinese documents.  for spanish the system achieves 1% of monolingual performance using only a  pseudo-parallel  spanish-english corpus. retrieval results are comparable with those of the structural query translation technique  pirkola  1  when bilingual lexicons are used for query translation.  when parallel texts in addition to conventional lexicons are used  it achieves better retrieval results but requires more computation than the structural query translation technique.  it also produces slightly better results than using a machine translation system for clir  but the improvement over the mt system is not significant. 
1. introduction 
the goal of cross-lingual information retrieval  clir  is to find documents in one language for queries in another language.  we use a probabilistic cross-lingual retrieval system  whose theoretical basis is probabilistic generation of a query in one language from a document in another.  hidden markov models  hmms   rabiner  1  were used to approximate the query generation process.  a key component of the retrieval model is probabilistic translation from terms in a document to terms in a query.  the retrieval model integrates term translation probabilities with corpus statistics of query terms and statistics of term occurrences in a document to produce a probability of relevance for the document to the query.  similar approaches have been proposed for both monolingual ir  ponte and croft 1;  berger and lafferty 1  and for clir  hiemstra and de jong  1 ; the differences are discussed later in the paper. 
the focus of this study is on empirical evaluation of the proposed system.  the probabilistic approach will be compared empirically with two popular clir techniques  structural query translation and machine translation  mt .  the major difference between our approach and structural query translation is that ours uses translation probabilities while the other treats all translations as equals.  a comparison between the two approaches will show the advantages and disadvantages of using probabilistic term translation for clir.  the major difference between the mtbased technique and our approach is that the former does not use multiple translations for a term while the latter does.  a comparison between them will show the advantages and disadvantages of using multiple translations in clir.  the basic idea of structural query translation was used by a number of studies  including  pirkola  1; ballesteros and croft  1; sperer and oard 1; hull 1 .  past studies that used mt systems for clir include  oard  1; ballesteros and croft  1 .   
a common problem with past research on mt-based clir is that a direct comparison of retrieval results with other approaches is difficult because the lexical resources inside most commercial mt systems cannot be directly accessed.  to overcome the problem we will use a technique to hypothesize the term translations inside a mt system based on the text it translated.  by treating the translated text as a pseudo-parallel corpus  we can automatically induce a bilingual lexicon and use it with our system for crosslingual retrieval.  that will establish a lower bound on the performance of our system if it had direct access to the linguistic knowledge in the mt system.  
in the next section we describe our retrieval model  including its limitations and potential extensions.  section 1 discusses related work.  section 1 describes the lexical resources used in this work. section 1 describes the test collections used in our experiments and how they were processed.  the test collections are the trec1 chinese track  the trec1 cross-lingual track and the trec1 

permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. sigir'1  september 1  1  new orleans  louisiana  usa copyright 1 acm 1-1/1...$1. 
 spanish track  voorhees and harman  1; voorhees and harman  1 . section 1 compares clir performance of our system with monolingual ir performance.  section 1 and 1 compare our system with structural query translation and mtbased clir. the last section summarizes this work and outlines directions for future work.  
1. retriveal model 
the basic function of an ir system is to rank documents against a query according to relevance. by bayes'  rule  
p doc is rel |q  = p doc is rel p q | doc is rel  
p q 
here doc is a document and q is a query. p doc is rel  is the prior probability of relevance for doc  which we assume to be a constant.1 p q  is the prior probability that q is generated; since q is a constant  p q  has no effect on document ranking. we can therefore rank documents by p q | doc is rel   the probability that query q is generated given document doc. 
we use hidden markov models to simulate the process of query generation.   rabiner  1  contains an excellent introduction to hmm theory.  for convenience  we will assume that queries are in english and documents are in chinese.  we assume two states  the general english state and the document state.  in the general english state  an english word for the query is generated; it may or may not describe the content of the document.  in the document state  a word from the chinese document is chosen and translated to an english word for the query.  the following pseudo-code describes the query generation process.  
until all query words are generated  
{ 
toss a biased coin with probabilities ¦Á for heads and 1-¦Á for tails. enter the general english state if it is heads and the document state otherwise.  
general english state: pick an english word from the english vocabulary according to a probability distribution. 
 
document state: pick a chinese word from the document according to a probability distribution and translate it to an english word according to another probability }distribution.  
to minimize the need for training data  we estimate the parameters as follows: 
1. the parameter ¦Á is a constant. we fix it at 1 in this study  based on prior experience.  
1. in the general english  ge  state  we estimate the probability distribution as follows: 
p e | ge  = freq e ge / | ge | 
where  freq e  ge  is the frequency of english word e in an english corpus and |ge| is the size of the english corpus.  any large english corpus can be used for this purpose.  in this study  we used trec volumes 1 of english data.   
1. in the document state  doc   we estimate the probability distribution as follows: 
p c | doc  = freq c doc / | doc | 
where  freq c  doc  is the frequency of chinese word c in doc and  |doc| is the length of the document. 
1. the probability of translation to an english word e given a 
chinese word c  p e|c   depends on c and e only. in section 1  we will discuss how to estimate the translation probabilities from parallel texts and from bilingual lexicons.  
 
with these assumptions  it is easy to verify that: 
 
p q|doc  = ¡Ç  ap e|ge  +  1 a  ¡Æ p c|doc  p e| c   e in q chinese words c
 
this cross-lingual retrieval model is an extension of the monolingual retrieval model proposed by  miller et al  1 .  in our discussion  we assume that the translation of a term is independent of the document and independent of the query in order to deal with data sparseness. the assumption dramatically reduces the number of parameters we need to estimate.  if more data  such as a very large parallel corpus  becomes available in the future for parameter estimation  the independence assumption can be weakened to make the model more powerful.  one possible technique is to employ bigram and trigram information to improve term translation. 
1. related work 
our retrieval model is similar to a number of existing ones. one such model was proposed in  hiemstra and de jong  1 .  a significant difference is that our model makes use of corpus statistics of the query language  english  while hiemstra's does not.  roughly speaking  corpus statistics of a term can indicate the importance of a term in a query.  in general  frequent terms are less useful than rare terms. this fact has been exploited by the traditional tf.idf model as inverse document frequency  idf .  instead of using the corpus statistics of the english terms  query terms   hiemstra's model uses the corpus statistics of the chinese terms  terms in documents .  this is an attempt to model the importance of an english term based on the corpus statistics of its chinese translations. this is a reasonable approximation if we do not have sufficient english text at our disposal. but given the vast amount of available textual data nowadays  we think a direct estimation procedure is more reliable because it avoids the noise introduced by translation.  
our model is an alternative to the structural query translation technique proposed in  pirkola  1   whose basic idea can be traced to an earlier study in  hull  1 .  it has been used in a number of studies  including  sperer and oard  1; ballesteros and croft  1; kwok  1 .  this technique treats translations of a query term as synonyms of the term: occurrences of the chinese translations of an english term in the chinese documents are treated as instances of the english term.  the technique is typically applied with a tf.idf retrieval model.  this technique treats all translations as equals while our model does not. 
  berger and lafferty  1  views query generation as a translation process. so far  the model has only been used for monolingual retrieval  but potentially it can be applied to clir as well.  
studies that used mt systems for clir include  ballesteros and croft 1; oard 1 .  as discussed earlier  direct comparisons with other techniques have been a problem because lexicons in most mt systems are inaccessible.   mccarley  1  studied both query and document translations and concluded the combination of the two translations can improve retrieval performance.   levow and oard  1  studied the impact of lexicon coverage on clir performance.     
1. lexical sources 
two manual lexicons and one parallel corpus were used for english and chinese clir experiments:  
1. the ldc lexicon. it contains 1 english entries  1 chinese entries and 1 translation pairs. it is available from the linguistic data consortium  ldc . 
1. the ceta lexicon. it contains 1 english entries  1 chinese entries and 1 translation pairs. it can be obtained through the mrm corporation  kensingston  md. 
1. hknews  hong kong sar news  corpus. this parallel corpus consists of 1 pairs of documents in english and chinese  with about 1 million english words. an algorithm developed in-house was used to align the corpus  resulting in 1 pairs of sentences.  the corpus is available from ldc.  
we use two techniques to estimate translation probabilities.  for the manual bilingual lexicons  we assume uniform translation probabilities. that is  if a chinese word c has n translations e1 to en  we assume p ei|c  =1/n.  
for a parallel corpus  we use brown et al's statistical machine translation models  brown et al  1  to automatically induce a probabilistic bilingual lexicon.  we used the weaver system developed by john lafferty for this purpose  lafferty  1 .  the weaver system implemented three of the five models proposed by brown et al. model 1 was used in this work for its efficiency.  in order to keep the size of the induced lexicon manageable  a threshold  1  was used to discard low probability translations.  
in order to increase lexicon coverage and to produce more robust probability estimates  different lexicons  including manual and induced  were combined to produce a single lexicon.  translation probabilities from different sources were linearly combined with equal weights: 
       p e | c  =  pldc e | c  + pceta e | c  + phknews e | c  /1 an exception is that if c does not occur in a source  the weight for that source will be equally distributed to the remaining sources. this ensures that the sum of the translation probabilities given a chinese term is equal to 1. we should note that the weights given to the lexical sources could be adjusted to optimize retrieval performance. we will not explore this issue because it is not the focus of this work. 
for english and spanish clir  we used a lexicon induced from a translated corpus by a mt system  systran .  we will discuss that in detail in section 1. table 1 summarizes the statistics about the lexical sources. 
table 1:  statistics about lexical sources. hknews is a statistically derived lexicon. the combined lexicon is a 
combination of  ldc  ceta and hknews. english words are stemmed. 
lexical source english terms chinese terms translation pairs ldc 1 1 1 ceta 1 1 1 hknews 1 1 1 combined 1 1 1 1 1. test collections 
three test corpora were used in our experiments:  trec1 chinese track  trec1c   trec1 cross-lingual track  trec1x  and trec1 spanish track  trec1s . trec1c and trec1x consist of chinese documents with queries in english and chinese.  having two versions of the same queries allows both monolingual and cross-lingual experiments.  trec1s consists of spanish documents with queries in english and spanish.  english stemming used the porter stemmer  porter  1  and spanish stemming used the stemmer by  xu and croft  1 .  all three fields  title  description and narrative  of the trec topics were used in query formulation.  table 1 shows statistics about the test corpora. 
for chinese text segmentation  we used a simple dictionary-based algorithm.  a list of valid chinese words was obtained by combining the chinese entries in the ldc and ceta lexicons.  to segment chinese text  the algorithm examines every substring of 1 or more characters and treats it as a word if it appears in the chinese word list.  in addition  a single chinese character is also treated as a word if it is not part of any of the words recognized in the first step.  the goal of the algorithm is to optimize crosslingual performance  since it allows as many matches between english terms and chinese terms as possible.  for monolingual retrieval in chinese  however  it has been shown that the best search strategy is to use a combination of bigrams and unigrams of chinese characters  kwok  1 .  that strategy was used in our monolingual experiments in order to produce the strongest monolingual baseline.  
table 1: statistics about test collections. trec1c=trec1 chinese track. trec1s=trec1 spanish track. 
trec1x=trec1 cross-lingual track 
corpus trec1c trec1s trec1x query  language english english english document language chinese spanish chinese query count 1 1 1 document count 1 1 1 query length 1 1 1  
throughout this paper  we will use the trec average noninterpolated precision to measure retrieval performance  voorhees  1 .  
1. chinese retrieval results 
table 1 shows the retrieval results of our clir system on trec1c and trec1x.  our monolingual results were obtained using miller et al's hmm monolingual retrieval system  miller et al  1 .  the monolingual results form a strong baseline; they are better than the best official monolingual results in the trec1 and trec1 proceedings  voorhees and harman  1  1 . given the strong baseline  the cross-lingual results using the combined lexicon are very impressive because they are around 1% of monolingual results  1% on trec1c and 1% on trec1x .  
table 1: retrieval results on trec1c and trec1x. 
corpora trec1c trec1x monolingual 1 1 ldc 1 1 ceta 1 1 hknews 1 1 combined 1 1  
retrieval results using individual lexicons are significantly worse than those using the combination of the three lexical resources  confirming findings by other researchers that lexicon coverage is critical for clir performance  levow and oard  1 .  the results show that dialect similarity can also affect retrieval performance.  both the trec1x corpus and the hknews parallel corpus are in cantonese  a chinese dialect .  therefore  hknews is more effective on trec1x than ldc and ceta  which have a strong bias toward mandarin  standard chinese .  on the other hand  since trec1c is a mandarin corpus  ldc and ceta are better than hknews on trec1c. 
1. comparison with structural query translation for chinese 
in this section we compare the retrieval results of our system with those of the structural query translation technique.  our experiments followed the query translation procedure described in  pirkola  1 .  a term in a chinese document is treated as an instance of an english term if it is a translation of the english term according to a bilingual lexicon.  given a chinese corpus  the term frequency and the document frequency of an english term are computed as: 
tf  e  doc  = ¡Æ tf  ci   doc 
 
df  e  = |   doc   set ci   |
where ci's are chinese translations of e and doc set ci  is the set of chinese documents containing ci.  the tf and df values of english terms were used with the inquery tf.idf  function  allan et al  1  to compute the retrieval score of a chinese document for an english query.  
table 1 shows that our system and structural query translation achieved similar retrieval results when ldc and ceta were used.  the exception is that on trec1x using ceta our system is significantly better  1 vs. 1 .  when hknews and the combined lexicon were used  our system is significantly better.  
table 1: retrieval results of structural query translation. 
corpora structural model on trec1c hmm on trec1c structural model on trec1x hmm on trec1x ldc 1 1 1 1 ceta 1 1 1 1 hknews 1 1 1 1 combined 1 1 1 1  
since the procedure we used to obtain translation pairs from parallel texts is statistically based  it is error prone for infrequent terms.  most of the incorrect translations have a small probability estimate.  these bad translations are automatically discounted by our system because they have small probabilities.  however  since the structural query translation technique treats all translations equally  the bad translations become a serious problem.  experiments show that removing the low probability translations significantly improves the performance of structural query translation. figure 1 shows the performance curves when we vary the probability cut off values on trec1. the results confirm that noisy translations from the parallel corpus are a serious problem for structural query translation. however  these noisy translations are useful information to our system; removing them hurts retrieval performance of our system. the advantage of our system seems to be its capability of utilizing noisy translations to improve retrieval performance.  
the disadvantage of our system is that it is less efficient than structural query translation due to the extra computation incurred by the using of translation probabilities in our model. the efficiency issue can be addressed by pre-computing p e|doc  of the retrieval function. such optimization techniques have been used in previous work  hiemstra and de jong  1 . they were not used in this work because they would prevent us from experimenting with different bilingual lexicons without reindexing.  

figure 1:  trec1x  performance of the probabilistic term translation model and structural translation approach with 
varying thresholds on including low probability translations. 
1. comparison with mt-based 
approaches for spanish 
the major difference between mt-based clir and our approach is that the former uses one translation per term and the latter uses multiple translations.  it has been suggested that clir can potentially utilize the multiple useful translations in a bilingual lexicon to improve retrieval performance  klavans and hovy  1 .  in our experiments  we used systran version 1  http://www.systransoft.com  for query and document translation. systran is generally accepted as one of the best commercial mt systems for english-spanish translation. 
we performed four retrieval runs on the trec1s corpus: 
1. query translation.  english queries are translated to spanish via systran.  retrieval was performed using the translated queries on the spanish corpus. 
1. document translation.  the spanish corpus is translated to english via systran.  retrieval was performed using english queries on the translated corpus. 
1. combined run.  the two retrieval scores for each document obtained in 1 and 1 were multiplied to produce a combined score for that document.  documents were then ranked based on the combined scores. previous studies  mccarley  1  suggested that such a combination can improve clir performance. 
1. probabilistic clir.  we induced a bilingual lexicon from the translated corpus by treating the translated corpus as a pseudo-parallel corpus.  weaver was used to induce a bilingual lexicon for our approach to clir. table 1 shows that probabilistic clir using our system outperforms the three runs using systran  but the improvement over the combined mt run is very small. its performance is around 1% of monolingual retrieval. please note that the induced lexicon is probably a trimmed version of the true lexicon in systran. had we had direct access to the relevant linguistic knowledge  including lexicon and disambiguation knowledge  in the mt system  we could probably make a better probabilistic bilingual lexicon than the one induced from a pseudo-parallel corpus. as a result  we could produce better retrieval performance.  on the other hand  the test set has only 1 queries and the difference between our system and the combined mt run is very small.  therefore  we cannot draw a firm conclusion about the retrieval advantage of probabilistic clir without further study.   
nonetheless  the results suggest that a simple dictionary-based approach can be as effective as a sophisticated mt system for clir.  this is particularly important for languages where mt may not be available  but where bilingual word lists may have been compiled. 
table 1: comparing our clir system and mt-based clir. 
monolingual 1 query translation 1 doc translation  1 doc and query translation 1 probabilistic clir 1  
the goal of our experiments is not to dismiss the mt-based approach; it is viable for at least two reasons. first  it is much faster than our clir system. it is about 1 times as fast as our clir system in the above experiments. even though precomputation can improve the efficiency of our system  as we discussed earlier   we expect mt-based clir would still be faster due to a sparser term-document matrix. second  the retrieved documents are readable by end users. these properties make it the ideal search strategy in an interactive clir environment.  the advantage of the dictionary-based approach is also twofold.  it is relatively inexpensive to build and it can potentially produce better retrieval results by using more than one translation per term.  
1. conclusions 
we proposed and evaluated a probabilistic clir retrieval system.  the system achieved roughly 1% of monolingual performance in retrieving chinese documents and 1% in retrieving spanish documents.  we have shown how a simple mixture model combining bilingual word lists and parallel corpora can outperform either alone.  it also appears that  with this approach  additional bilingual lexicons and parallel text improve performance substantially in spite of the increased ambiguity. 
experiments show that while our system is more effective than the structural query translation technique when parallel texts are available for term translation  the latter is more efficient.  our system is also slightly more effective than the combined technique of query and document translation using a commercial mt system  but the difference in retrieval performance is small.  
one area for future work is to improve our retrieval model by incorporating contextual information for better term translation.  term disambiguation has been a subject of intensive study in clir  ballesteros  1 .  applying the research results in that area will be helpful. a second area is to make better use of the translation models in weaver. some of the translation models allow a word to be translated to several words  e.g. a phrase  in the other language. we believe if properly used  this feature can improve retrieval performance because it more accurately accounts for the query generation process than our current retrieval model. 
