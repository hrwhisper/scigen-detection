one of the most well-studied problems in data mining is computing the collection of frequent item sets in large transactional databases. one obstacle for the applicability of frequent-set mining is that the size of the output collection can be far too large to be carefully examined and understood by the users. even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem.
¡¡in this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem: what are the k sets that best approximate a collection of frequent item sets  our measure of approximating a collection of sets by k sets is defined to be the size of the collection covered by the the k sets  i.e.  the part of the collection that is included in one of the k sets. we also specify a bound on the number of extra sets that are allowed to be covered. we examine different problem variants for which we demonstrate the hardness of the corresponding problems and we provide simple polynomial-time approximation algorithms. we give empirical evidence showing that the approximation methods work well in practice.
categories and subject descriptors
h.1  database management : database applications- data mining; f.1  analysis of algorithms and problem complexity : nonnumerical algorithms and problems
general terms
algorithms

 part of this work was done while the author was visiting hiit basic research unit  department of computer science  university of helsinki  finland.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
kdd'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ... 1.
keywords
foundations of data mining  mining frequent itemsets
1. introduction
¡¡the notion of discovery of frequent patterns started from the work of agrawal  imielinski  and swami  on finding association rules and frequent item sets. the same basic idea of searching for patterns which occur frequently enough in the data carries over to several pattern domains  see e.g.   1  1  1  1  . the collection of frequent patterns can be used in at least two different ways: first  one can be interested in the individual patterns and their occurrence frequencies; second  one can be interested in the whole collection  trying to obtain a global view of which patterns are frequent and which are not. the algorithms for finding frequent patterns are complete: they find all patterns that occur sufficiently often. completeness is a desirable property  of course. however  in many cases the collection of frequent patterns is large  and obtaining a global understanding of which patterns are frequent and which are not is not easy. even restricting the output to the border of the frequent item-set collection does not help much in alleviating the problem.
¡¡in this paper we consider the problem of finding a succinct representation of a collection of frequent sets. we aim at finding small and easy-to-understand approximations of the collection. the premise of our work is that such small approximations give a better understanding of the global structure of the data set without a significant sacrifice of information. indeed  the collection of frequent patterns is always computed with respect to a frequency threshold  i.e.  a lower limit on the occurrence probability of the pattern. this threshold is almost always somewhat arbitrary  and thus  there is no single  correct  collection of frequent patterns. hence  one can argue that there is no reason to in-
sist on computing the exact collection with respect to that threshold.
¡¡our measure of approximating a set collection by k sets is defined to be the number of sets in the collection that are included in at one of the k sets. to avoid overgeneralization  we restrict the number of false positives allowed. as a simple example  consider the collection of frequent sets containing the sets abc  abd  acd  ae  be  and all their subsets  i.e.  these five sets are the border of the collection . we can represent this collection approximately as the set of all subsets of abcd and abe; this covers all the original sets  and there are only two false positives.
we show that while the problem of finding the best kset approximation for a given collection is np-hard  simple algorithms can be used to obtain very good approximation quality  1   1/e . on real data  our empirical results show that using only k = 1 sets  corresponding to 1% of the size of the border of the collection   and allowing a false-positive ratio of 1%  one can cover the 1% of the original frequent set collection. relatively simple visualization techniques can be used to give a good intuitive feel for collections of 1 sets  and hence it seems that our approach yields a good summary of the frequent set structure of large 1 data sets. our algorithm is based on the greedy approximation strategy  importance sampling  and a combinatorial lemma on the structure of collections of frequent sets. the method is simple but its analysis is somewhat intricate.
¡¡next  we describe the problem in more detail. we are given a set u of n attributes a1 ... an and a database consisting of transactions  which are subsets of u. the collection d of frequent sets consists of all attribute sets x such that at least a fraction of ¦Ò of the transactions in the database contain x as a subset. then d is downwards closed  i.e.  if x ¡Ê d and y   x  then y ¡Ê d. given the collection d  we define the border b+ d  of d as the collection of maximal sets in d  i.e.  b+ d  = {x ¡Ê d|d¡Éi x  = {x}}  where i x  denotes the collection of supersets of x. finally  given a set x  we denote by p x  the powerset of x. we refer to the lattice u as the partial order naturally defined on the powerset p u  using the subset relation    . we are interested in describing succinctly the downwards closed collection d  and in order to do so successfully we are willing to tolerate some error. a natural way of representing d is to write it as the union of all subsets of k sets z1 ... zk. that is  denoting

we look for sets z1 ... zk such that d ¡Ö s z1 ... zk . we say that s z1 ... zk  is spanned by the k sets z1 ... zk  and we call s z1 ... zk  a k-spanned collection. the problem of succinct representation of d can now be formulated as follows:
¡¡problem 1. given a downwards closed collection of sets d  find a collection of sets a such that a is spanned by at most k sets and a approximates d as well as possible.
¡¡to make the statement of problem 1 concrete we need to define the notion of distance between the input set collection d and a solution set collection a. we measure the quality of approximation between two set collections a and d using the coverage measure c a d   defined as the size of the intersection between a and d. naturally the goal is to maximize coverage.
¡¡next  one has to define which sets are allowed as spanners. without the restriction that the spanners of a should belong to d  one can very easily maximize coverage by setting a = u  which is a solution that covers the whole d and it is spanned by just one set. however  a = u is not an intuitive solution  since it introduces the maximum possible number of false positives  the sets in a   d . the first choice to avoid this kind of unintuitive solutions is to restrict the spanners of a to be sets from d and therefore we have a   d. under the restriction a   d the goal of maximizing the coverage is equivalent with maximizing |a|.
obviously the spanners of a   the k-spanned collection that best approximates d  reside at the border of d  and thus it is sufficient to restrict our search in the border of d. to
be ina middle road between restricting the spanners ofd and having to choosein some other collectiona =	u is to restrict thed athat
selection of spanners of a.	the choice of d can be natis a supercollection of d
ural in some applications.tion of frequent item sets for a support thresholdfor example  if d is a collec- can
be the collection of frequent item sets for a smaller support threshold ¦Ò. in other cases d can be defined implic-
itly in terms of  and d dhaving at mostthere exists set  for example  one could use all super-. we will mainly consider a third alter-tzadditional elements  i.e. for which z   x z ¡Ê sets of sets of
d
native  where consists of those sets which have a sufficiently small fraction of false positives. we write c to make explicit that the spanners of a are chosen from the collection d. as before  it is sufficient to search for the.
solution in the border ofwe now briefly describe the results of the paper: we dis-d tinguish between the case that the input to our problem is
specified by the whole collectioninput is specified only by the border ofd and the case that the. as the size of the collection can be exponential on the size of its border  thed second case is more challenging.
  the best approximationfor the first case and when the spanning sets are se-lected from d  we show that the problem of findinga spanned by k sets is np-
hard  but it can be approximated to within a factor of 1   1e .
  when the spanning sets are selected from those subsetsfor which the false-positive ratio is smaller than ¦Á  we
show a compact lemma stating that the number of such lemma yields asets is bounded by the square of the size of1   1e approximation result.d. the
  for the case that the input is specified by the borderof d  we are able to obtain an almost identical result-by giving only the price of specifying the collection d  loss in its border  i.e.  with no redundancy  is only an the approximation factor  for any    1. for showing this we use techniques from importance sampling in combination with the results from the previous case.
  we give empirical results demonstrating that a frequent-set collection can be approximated well by a small
number of maximal sets. we performed experiments on three real data sets. as mentioned above  a typical result shows that using only k = 1 sets  corresponding to 1% of the size of the border of the collection   and allowing a false-positive ratio of 1%  we could cover the 1% of the total frequent set collection. although the exact numbers depend on the input data set  similar trends showed in all cases.
¡¡the rest of this paper is organized as follows. in section 1 we define the problem variants in more detail. section 1 considers the case when d is given as input  and section 1 the case where the input is the border of the collection. in section 1 we describe our experiments  and in section 1 we discuss related word. finally  in section 1 we offer our concluding remarks.
1. problem variants
¡¡we distinguish two variants of the problem depending on how the collection d is specified. in section 1 we show that the task of approximating d is np-hard  so polynomial time approximation algorithms need to be designed. however  the different ways of specifying d might change the size of the input at an exponential rate  so different techniques are required for each problem variant. below we describe the two variants in order of increasing difficulty   or equivalently  in order of decreasing input size .
¡¡collection. the complete collection d is given as input. considering as input the complete d creates a lot of redundancy since d can be precisely specified by its border b+ d . however  the exact requirement in this variant is that our algorithm should be polynomial in |d|.
¡¡border. the border of d is given as input. in this case we allow the running time of our approximation algorithm to be o poly |b+ d |  . the main problem here is that the size of d might be exponential in the size of b+ d   therefore different techniques are required in order to stay within polynomial running time.
¡¡to unify our notation and distinguish more easily among the two cases  we restate problem 1 as follows: problem 1. given a downwards closed collection of sets d  find a collection of sets a  such that a is spanned by at most k sets and aaprxapproximatescollectiond as well as possible. we call the problem - when the whole collection is specified as input  and aprx-border when only the border of the collection is given as input. the quality of approximation is measured according to the coverage measure c. the optimal solution to the problem for all cases is denoted by a .
1. whole collection as input
1 selecting spanners from the collection
¡¡we first consider the case that the spanner sets in the solution a are restricted to be inside d. the problem to solve is that of selecting the k sets in d that maximize the intersection with d. we can notice immediately that thismax -cover problem is a special case of k . an instance of
max k-cover is specified by a collection of sets  and the goal is to select the k sets in the collection that maximize the number of covered elements. a well-known algorithm for the max k-cover problem is the greedy algorithm  which can be described as follows: initially  the algorithm puts all the elements in a list of uncovered elements. then it proceeds in performing k iterations  where in each iteration one new set is added to the solution. during the j-th iteration  for 1 ¡Ü j ¡Ü k  the algorithm i  finds the set aj that covers the most uncovered elements  ii  adds aj to the solution  and iii  removes the elements covered by aj from its list of uncovered elements. the greedy algorithm is known to provide a 1   1e approximation ratio to max k-cover
 e.g.  see  1  pg. 1    so the following is immediate.
¡¡theorem 1. for aprx-collection as defined in problem 1  we can find a collection a spanned by k sets such that
c .
¡¡next we show that aprx-collection is indeed an nphard problem. notice that the connection with max kcover described above does not imply immediately the nphardness result  since it is not clear how to transform an arbitrary collection to a downwards closed collection.
theorem 1. aprx-collection is np-hard.
¡¡proof. we show a transformation from the 1d matching problem . an instance of 1d matching is specified by a set of  edges  e   x ¡Á y ¡Á z  where x  y   and z are disjoint sets having the same number q of elements. the goal is to determine if there is a complete matching for e  i.e.  a subset m   e of cardinality |m| = q such that no elements in m agree in any coordinate. the transformation to aprx-collection is defined as follows:
¡¡consider an instance i =  x y z e  of 1d matching. an instance of aprx-collection can then be defined by considering a collection of sets d i  over the universe of elements u = x ¡È y ¡È z. for each edge e =  xi yj zk  ¡Ê e we add in the collection d i  the subcollection d e  = p {xi yj zk}  = {{xi yj zk}  {xi yj}  {xi zk}  {yj zk} 
struction  the collection{xi}  {yj}  {zk}   }  that isd idcollection i  =	e¡Êe d e . by conthe-
                             is downwards closed. value of k required for aprx- is set to q  and the target value of c a d i   for a solution a is set to 1q + 1.
¡¡the transformation can be clearly computed in polynomial time. furthermore  we can show that in the instance i there exists a complete matching if and only if in the collection d i  there exists a collection a that is spanned by q sets and it has coverage at least 1q+1. to prove the equivalence  disregarding the   set that is covered by selecting any other set  any set in the collection d i  covers at most 1 sets. therefore  the only way to obtain a solution a that is spanned by q sets and it has coverage value 1q+1 is that all of the q spanners of a are 1-element sets and their powersets are pairwise disjoint. however  such a solution in d i  corresponds to a complete matching in i. the proof of the converse is based on the same idea  i.e.  a complete matching in i corresponds to disjoint  except the   set  1-element spanners in d i . 
1 selecting spanners outside the collection
¡¡in this section we consider the case that the spanner sets for a solution a are allowed to be outside the collection d. a motivating example for investigating this case is the following.
¡¡example 1. imagine the contrived but illustrative situation that all sets of size t of the lattice are frequent  and no sets of size are frequent. intuitively  we say that the collection of frequent item sets is  flat . in this situation  it is reasonable to consider reporting a set of size t + 1  even though such a set is not frequent itself. quantitatively  by reporting one infrequent set  one false positive   we capture t + 1 frequent sets of the border. if t is large enough  one could also consider reporting a set of size: in that way by having t+1 false positives  we capture 1 frequent sets of the border.
¡¡assuming that the collection of candidate spanners d is somehow specified to the problem instance  one can use the greedy algorithm described in the previous section and obtain a similar kind of approximation guarantee for the measure c . two methods of specifying the collection

figure 1: a schematic view of the concepts used in the proof of lemma 1.
d were mentioned earlier. the first method is to reduce the support threshold from and consider as d the  border of the  ¦Ò-frequent item sets. the second method is to extend all sets in d by considering some of their supersets. however  one can verify that in both of these methods the size of the collection d can potentially be superpolynomial in the size of d. on the other hand  the running time of the greedy algorithm is polynomial in both   therefore one cannot guarantee that the running time remains polynomial in the size of d. in the rest of this section  we will describe an intuitive definition for the candidate collecthat guarantees that its size is polynomial in the size of d  and therefore the running time of the greedy algorithm is polynomial.
¡¡after looking again at example 1  it seems intuitive to consider adding in d a set s that is not in d  only if s covers a large part of d while it does not introduce many additional sets. to formalize this notion  for each set s we define the false-positive ratio function f+ ¡¤  to be the ratio of the number of sets not in d over the number of sets in d covered by s. in other words
.
notice that the false-positive ratio of a set s can always be defined since at least the empty set belongs simultaneously in d and in p s . for a set s  f+ s  = 1 is equivalent to s ¡Ê d.
¡¡a collection of candidate spanners can now be defined using the notion of false-positive ratio. given a false-positivethreshold value ¦Á  we define the collection of candidates d¦Á to be
d¦Á ¡Ô {s | f+ s    ¦Á}.
that is  sets in d¦Á introduce at most a fraction of ¦Á false positives over the sets in d that they cover. the threshold ¦Á can take any positive value  however  as we will see shortly  we are particularly interested in d1  i.e.  candidates whose false-positive ratio is smaller than 1.
¡¡we will first show that the collection d¦Á is downwards closed  and thus it can be computed using a simple aprioritype algorithm.
¡¡lemma 1. for any threshold value ¦Á   1  the collection d¦Á is downwards closed.
¡¡proof. assume that there exists a collection d and a value ¦Á for which the hypothesis of the lemma is not true. then there should exist two sets x and y such that f+ x  ¡Ý ¦Á  f+ y     ¦Á  and y = x ¡È {e}  that is  y extends x by just one element.
we partition the powerset p y   into four disjoint collec-
   
d}  and. define si = |ci|  for i = 1 1  and observe that
	 	and	.
finally  define ce¡¥ = c1 ¡È c1  which  in fact  is p x   and ce = c1 ¡Èc1  that is  p y   p x  . for a visualization aid of the above concepts and definitions see figure 1.
¡¡notice that given our definitions  each set a in the collection ce¡¥ has a  copy  set a ¡È {e} in the collection ce  and vice versa. this one-to-one mapping of ce¡¥ to ce implies that s1 + s1 = s1 + s1. the crucial observation for the proof of the lemma is that since d is downwards closed  for each set in ce that also belongs to c1  its  copy  in ce¡¥ should belong to c1. in other words  the  copies  of the sets in c1 is a subset of c1  which implies that s1 ¡Ý s1. combining the facts s1 + s1 = s1 + s1 and s1 ¡Ý s1 we obtain s1 ¡Ü s1  and
therefore

however  the above conclusion contradicts with our initial assumption that. 
¡¡one potential obstacle in using the definition of d¦Á  is that  although it is intuitive  it does not provide us with an obvious upper bound on the number of candidates to be used. however  we next show how to overcome this problem and obtain such a bound for the special case of false-positive threshold value ¦Á = 1. our bound is based on the rather interesting containment property of d1.
¡¡lemma 1. any set in d1 can be expressed as the union of two sets in d  that is 
d1   {x ¡È y | x y ¡Ê d}.
¡¡proof. consider a set z for which there are not exist two sets x y ¡Ê d such that z = x ¡Èy . we will show that f+ z  ¡Ý 1  and so . define the partition of p z  into the disjoint collections d+ z  = p z ¡Éd and d  z  = p z    d. notice that f+ z  = |d  z |/|d+ z |. let x be any set in d+ z . the complement of x with respect to z  i.e.  the set z   x  should belong to d  z   otherwise the assumption for z would be violated. therefore  i.e.  by complementation with respect to z  we correspond each set from d+ z  to a set in d  z   and no two sets from d+ z  correspond to the same set in d  z . thus |d  z | ¡Ý |d+ z | or f+ z  ¡Ý 1. 
corollary 1. we have |d1| = o |d|1 .
¡¡using the fact that the collections d¦Á are downwards closed  it is clear to see that d¦Á   d¦Â for ¦Á ¡Ü ¦Â. therefore  the same upper bound of corollary 1 can be used for all values ¦Á   1  that is |d¦Á| = o |d|1 . for small values of ¦Á the bound might be crude  but nevertheless polynomial. furthermore  the algorithm will perform much better in practice than the bound suggests  the running time depends on the actual size of d¦Á . an empirical estimation of the real bound for ¦Á   1 is discussed in section 1. also notice that lemma 1 sheds some light in understanding the structure of d¦Á. for example  if d is spanned by only one set  i.e.  d = p x   then we get d1 = d  which can also be verified by the definition of false-positive ratio. we now combine all of the above steps and obtain the main result of this section.
¡¡theorem 1. consider aprx-collection  as defined in problem 1. for a given false-positive threshold value ¦Á  we write c¦Á to denote the coverage measure of approximating d when the collection of candidate spanners allowed to be used is the collection d¦Á. then  for any ¦Á ¡Ü 1  we can find in polynomial time a collection a spanned by k sets such that c¦Á a d  ¡Ý 1   1e c¦Á a  d .
¡¡proof. from corollary 1  we know that the size of the candidate collection d¦Á is quadratic in |d|. using lemma 1  we can compute d¦Á in an apriori fashion  and the running time is polynomial in |d|. now  we use the greedy algorithm with candidate sets restricted in b+ d¦Á . the overall running time is clearly polynomial. finally  the analysis of the greedy guarantees that the approximation ratio is at least 1   1e . 
1. the border as input
¡¡in this section we explain how one can use the greedy algorithm in order to deal with the case that only the border is specified as input to the problem. our main contribution is to show that we are able to obtain a result almost identical to the one presented in section 1-the price of specifying d with no redundancy is only an  loss in the approximation factor  for any    1. we start by explaining where the difficulty lies in using the greedy algorithm of section 1  and then we describe the necessary remedy for the algorithm to run in polynomial time.
¡¡as we already mentioned in section 1  the size of d can be exponentially large in the size of b+ d . the greedy algorithm actually utilizes resources polynomial in |d|  since at each step it evaluates how many new elements of d are covered by a potential candidate set to be added in the solution a. assume now that we apply the greedy algorithm in the case that only the border is specified as input. the first set s1 to be added in the solution is the set in b+ d  that covers the most sets in d. a base set on t items covers exactly 1t itemsets  therefore the first set s1 will be the set with maximum cardinality in b+ d   breaking ties arbitrarily . the second set s1 will be the one that maximizes |s1¡Ès1| given s1  which can be computed using the formula |s1 ¡È s1| = |s1| + |s1|   |s1 ¡É s1|.
¡¡in general  in order to find at each step of the greedy algorithm the set in the border that covers the most uncovered sets in d  we need to compute the size of the the union s1 ¡È ... ¡È sm. resorting to the inclusion-exclusion formula   as we did for s1  is a possibility but not an efficient method  since the number of terms in the formula is exponential in m.
¡¡the first idea for computing the size of the union s1 ¡È ... ¡È sm is to use a monte carlo sampling method: denote by s m  the union s1 ¡È...¡Èsm. to estimate |s m | sample n sets uniformly at random from u and count how many of them belong in s m . let this count be x. then the ratio x is a good estimator for the ratio s m  /
n	| | | u|  and since we know that | u| = 1n we can estimate s m  as.
in particular  using the chernoff bounds we can show the following.
¡¡fact 1. the sampling method described above provides and -approximation to |s m | with probability at least 1 ¦Ä  provided that
.
unfortunately  the idea of uniform sampling is not good enough. the reason is that if |s m | is small compared to 1n  then we need to sample many sets from u-not to mention that to obtain the required number of samples requires knowledge of |s m |  which is precisely what we are trying to compute.
¡¡fortunately  the problem posed by uniform sampling can be overcome by resorting to the technique of importance sampling. here we give a short overview of the method  more details can be found in  1  section 1.1 . recall that our goal is to compute to compute |s m | = |s1 ¡È ... ¡È sm|  where each si is a subset of u. for simplifying the notation  denote v = u  so each si contains elements from the universe v . also assume that we are given small positive numbers  and ¦Ä. given the above setting  the method of importance sampling provides an -accurate estimate for |s m | with probability at least 1  ¦Ä  provided that the following three conditions are satisfied.
 i  for all i  we can compute |si|.
 ii  we can sample uniformly at random from each si.
 iii  for all v ¡Ê v   we can verify efficiently if v ¡Ê si.
in our case  all of the above conditions are satisfied: for  i  and  ii  we used the fact that si are downwards closed sets. in particular  for  i   notice that if the base set of si contains t elements then |si| = 1t. for  ii   let r be a set that contains each element of the base items of si with probability 1. then  it is easy to see that r is a uniform sample from the itemsets in si. finally  for  iii   given an element v ¡Ê v we can trivially verify if it is also an element of si: just check if the base set of v is a subset of the base set of si.
the importance sampling method considers the multiset
  where the elements of m m  are
ordered pairs of the form  v i  corresponding to v ¡Ê si. in other words  the elements of m m  are the elements of s m  appended with an index that indicates due to which si they appear in s m . notice that the size of m m  can be trivially computed as |m m | = mi |si|.
¡¡the multiset m m  is then divided into equivalent classes  where each class contains all pairs  v i  that correspond to the same element v ¡Ê s m . that is  each equivalent class corresponds to an element of v ¡Ê s m  and contains all indices i for which v ¡Ê si. for each equivalent class one pair  v i  is defined to be the canonical representation for the class. now |s m | can be approximated by generating random elements in m m  and estimating the fraction of those that correspond to a canonical representation of an equivalent class. the intuition is that instead of sampling from the whole space v = u  we sample only from the set m m . the problem that appeared before with the uniform sampling disappears now because each element in s m  can contribute at most m elements in m m   and therefore the ratio  is bounded from below by 1/m. now by applying a sampling result similar to the one given in fact 1  we can estimate the ratio  using just  samples  i.e.  the number of samples required is polynomial in m. after estimating  and since the value of |m m | is known we can also estimate |s m |.
¡¡now we can use the above approximation scheme as part of the greedy algorithm. the idea is to approximate the value of the coverage measure for each candidate solution a by the method of importance sampling  and then select the set that maximizes the estimated coverage. by the approximation result  the coverage c of each set is estimated within the range   1   with probability at least 1   ¦Ä. thus  in each iteration of the greedy algorithm  we can find a set whose coverage is at least a  1     fraction of the largest coverage  and therefore the quality of approximation of the greedy is multiplied by a factor of  1
notice that the greedy calls the importance-sampling approximation scheme a polynomial number of times  therefore  in order to obtain a high probability result we need to set 1¦Ä =   poly b+ d   . however  this setting for ¦Ä does not create a serious overhead in the algorithm  since the complexity of the importance-sampling approximation scheme is only logarithmic in 1¦Ä. summarizing the above discussion  we have shown the following.
¡¡theorem 1. for aprx-border as defined in problem 1 and for any  we can find with high probability a kspanned collection a such that
c .
notice that the np-hardness of aprx-border can also be established; the same reduction as in theorem 1 can be used.
¡¡in fact  the technique described in this section can be used as a fully polynomial randomized approximation scheme  fpras  for the problem of estimating the size of a frequent itemset collection given the border of the collection.
1. experimental evaluation
¡¡to verify the applicability of the problem studied in this paper  we implemented the proposed algorithms and we tested their behavior on three different real sets of data.
¡¡the first data set  mushroom  was obtained from the machine learning repository of uc irvine. a support threshold of 1% was used to obtain a collection of 1 item sets. the number of sets in the border was 1 and the average number of items for the border sets was 1. the second set  course  is from anonymized student/course registration data in the department of computer science at the university of helsinki. frequent course sets were obtained using a support threshold of 1%  yielding a collection of size 1. the size of the border for the second data set was 1 and the average number of items per border set was 1. finally  our third data set  bms  is owned by blue martini and it has been made available by ronny kohavi . the data set contains click-stream data of a small company  and it was used at the kdd cup 1. applying a support threshold of 1% we obtained a collection of 1 item sets with border size 1 and average item size for the border sets equal to 1. the three data sets  in the order described above  can be characterized from  narrow   small border figure 1: coverage of frequent set with using up to k = 1 sets for mushroom dataset.
consisting of large item sets  to  wide   large border consisting of small sets . thus  we believe that our experiments capture a fairly large range of typical cases.
¡¡for extracting frequent sets from the above data sets we used implementation of frequent itemset mining algorithms available by bart goethals .
¡¡we run the greedy algorithm with value of k up to 1  and values of the false-positive threshold parameter ¦Á in the range  1 . notice that the value k = 1 corresponds to about 1%  1% and 1% of the border size  for the data sets mushroom  course  and bms  respectively. the results for the data sets mushroom  course  and bms are shown in figures 1  1  and 1  respectively. in all cases we see that with  few  sets we can cover a  large  part of the collection. for instance  for the mushroom data set  only 1 out of 1 item-sets in the border can cover about 1% of the collection without introducing any false positives  whereas if we allow a percentage of at most .1 false positives  then 1 sets cover 1% and 1 sets cover 1% of the collection. obviously  increasing ¦Á corresponds to better coverage  with a single exception !  in the course data set for values ¦Á = 1 and ¦Á = 1; this is probably due to fact that the greedy is an approximate algorithm . also  as one can expect  the more  narrow  the data set  the better the coverage achieved for the same  absolute  value of k.
¡¡next we measure the number of candidates introduced and the size of the border of the candidates  as a function of ¦Á. this is shown in figures 1 and 1. as we mentioned in section 1  the quadratic upper bound used from the case ¦Á = 1 is expected to be rather crude for smaller values of ¦Á. in practice  the number of the candidates can be much smaller than quadratic  e.g.  for ¦Á = .1 our experiments show that the number of candidates is at most |d|1. assuming that the candidate size |d¦Á| has polynomial dependency on |d|  the exponent of the polynomial can be estimated from the ratio log|d¦Á|/log|d|  which is computed using our data sets and plotted in figure 1 for various values of ¦Á. from figure 1  a reasonable conjecture is that the true exponent figure 1: coverage of frequent set with using up to k = 1 sets for course dataset.
is 1 + ¦Á. also one can see that less candidates are generated for the more  narrow  data sets. finally  in figure 1  we show a similar kind of dependency between the border size of the candidate collection and the border size of the input collection  that is  the ratio log|b+ d¦Á |/log|b+ d | is plotted as a function of ¦Á. in this case  we see that the  narrowness  of the data set does not influence the exponent of growth as much.
¡¡the results indicate that using as few as 1 sets in the approximation gives often quite a good approximation of the original collection. this implies that there is hope to obtain good succinct representations of large 1 datasets.
1. related work
¡¡related to our paper in the respect of attempting to reduce the size of the output of frequent item-set patterns is the work of han et al.  on mining top-k frequent closed patterns  as well as work on closed frequent item-sets and condensed frequent item sets  see for example  pei et al.   pasquier et al.   and calders and goethals . in  the goal is to find the k most frequent sets containing at least minl items. this goal  however  is different from our setting where we ask for the k sets that best approximate the frequent item-set collection in the sense of set coverage. the work on frequent closed item sets attempts to compress the collection of frequent sets in a lossless manner  while for the condensed frequent item sets the idea is to be able to reduce the output size by allowing a small error on the support of the frequent item sets.
1. concluding remarks
¡¡we have considered the problem of approximating a collection of frequent sets. we showed that if the whole collection of frequent sets or its border are given as input  the best collection of sets spanned by k sets can be approximated to within 1   1e . we also showed that the same results hold when the sets used to span the collection are figure 1: coverage of frequent set with using up to k = 1 sets for bms dataset.
from the collections d¦Á. the results used the greedy approximation algorithm  importance sampling  and a lemma bounding the size of d¦Á. the results can also be generalized to any setting of frequent pattern discovery  provided some mild computability conditions hold. the empirical results show that the methods work well in practice.
¡¡several open problems remain. first of all  the properties of different measures of approximation merit further study. the measure we use counts the number of positive examples covered  and the negative examples are bounded by the choice of the collection d. another measure of approximation quality would simply be the number of positive examples covered minus the number of negative examples covered. it turns out  however  that in this case the greedy algorithm performs arbitrarily bad.
¡¡along the lines of using more elaborate measures is the idea of taking into account the support of the itemsets in the covered area  as well as the support of the false positives. one conceptual difficulty is how exactly to integrate the support information. in our current formulation we count the number of itemsets covered by a collection of spanners  however  extending this formulation to simply adding the supports of the itemsets is perhaps less intuitive.
¡¡the case when the input is the original database is perhaps the most interesting open algorithmic question. this case presents significant difficulties. first  computing the border in time polynomial to its size is a main open problem. furthermore  the size of the border can be exponential in the size of the database  and therefore one cannot afford looking at the whole search space-some kind of sampling method needs to be applied. one can try to form an approximation of the border of the collection of frequent sets by random walks on the subset lattice. however  for such a  random walk  sampling method we were able to construct  bad  cases  i.e.  cases in which the probability of finding any set that covers a not so small fraction of what the best set covers is not sufficiently large. notice  however  that given the transactional database one can always compute
false positive threshold  a 
figure 1: size of the candidate collection.
the border of d using one of the many algorithms in the data-mining literature and then apply our technique for the border case. we expect this method to work quite well in practice.
¡¡finally  from the practical point of view  user interface techniques that use approximation of frequent set collections might be interesting.
1. acknowledgments
¡¡we would like to thank floris geerts  bart goethals  taneli mielik¡§ainen  and jouni sepp¡§anen for many useful comments.
