the exponential growth of data demands scalable infrastructures capable of indexing and searching rich content such as text  music  and images. a promising direction is to combine information retrieval with peer-to-peer technology for scalability  fault-tolerance  and low administration cost. one pioneering work along this direction is psearch  1  1 . psearch places documents onto a peerto-peer overlay network according to semantic vectors produced using latent semantic indexing  lsi . the search cost for a query is reduced since documents related to the query are likely to be co-located on a small number of nodes. unfortunately  because of its reliance on lsi  psearch also inherits the limitations of lsi.  1  when the corpus is large and heterogeneous  lsi's retrieval quality is inferior to methods such as okapi.  1  the singular value decomposition  svd  used in lsi is unscalable in terms of both memory consumption and computation time.
　this paper addresses the above limitations of lsi and makes the following contributions.  1  to reduce the cost of svd  we reduce the size of its input matrix through document clustering and term selection. our method retains the retrieval quality of lsi but is several orders of magnitude more efficient.  1  through extensive experimentation  we found that proper normalization of semantic vectors for terms and documents improves recall by 1%.  1  to further improve retrieval quality  we use low-dimensional subvectors of semantic vectors to cluster documents in the overlay and then use okapi to guide the search and document selection. categories and subject descriptors
h.1  information search and retrieval : clustering  search process general terms
algorithms  performance  experimentation
keywords
latent semantic indexing  dimensionality reduction  peer-to-peer ir

 work in this paper was done at the university of rochester. chunqiang and sandhya were supported in part by nsf grants ccr-1  ccr-1  ecs-1  and eia-1; by darpa/ito under afrl contract f1-k-1; by the u.s. dept. of energy office of inertial confinement fusion under cooperative agreement no. de-fc1sf1; and by equipment or financial grants from compaq  ibm  intel  and sun.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  sheffield  south yorkshire  uk.
copyright 1 acm 1-1/1 ...$1.
1. introduction
　according to a recent report   the unique information added each year exceeds 1 bytes and is estimated to grow exponentially. this trend calls for equally scalable infrastructures capable of indexing and searching rich content such as text  music  and images. meanwhile  peer-to-peer  p1p  systems  are gaining popularity quickly due to their scalability  fault-tolerance  and selforganizing nature  raising hope for building large-scale information retrieval  ir  systems at a low cost.
　building a p1p ir system  however  still remains particularly challenging. the fundamental problem that makes search in existing p1p systems  e.g.  gnutella  difficult is that  with respect to semantics  documents are randomly populated. given a query  the system has to search a large number of nodes to find some relevant documents  rendering the system unscalable. to address this problem  we proposed psearch  1  1 . psearch organizes nodes into an application-level overlay network and populates documents in the network according to document semantics derived from latent semantic indexing  lsi   1  1 . the distance  e.g.  routing hops  between two documents in the network is proportional to their dissimilarity in semantics. the search cost for a query is therefore reduced since documents related to the query are likely to be concentrated on a small number of nodes. our initial results have shown the great promise of this approach . psearch can efficiently approximate a centralized implementation of lsi.
　unfortunately  our recent extensive experimentation with lsi reveals some limitations of lsi itself  which may also cripple psearch's efficiency and efficacy because of its reliance on lsi.  1  when the corpus is large and heterogeneous  lsi's retrieval quality is inferior to methods such as okapi .  1  the singular value decomposition  svd  that lsi uses to derive low-dimensional representations  i.e.  semantic vectors  of documents is not scalable in terms of both memory consumption and computation time.
　in this paper  we propose techniques to address these limitations of lsi and show their use in the psearch framework.   to improve the efficiency of lsi  we propose an algorithm we call elsi  efficient lsi  to reduce the size of the input matrix for svd while retaining the matrix's important content. we partition documents into clusters and use the centroids of the clusters as  representative  documents. we further reduce the dimensionality of the centroid vectors by filtering out elements corresponding to low-weight terms. the resulting matrix  which has short centroid vectors as columns  is several orders of magnitude smaller than the original matrix. finally we apply svd to this matrix to derive the basis of the semantic space. experiments show that elsi retains the retrieval quality of lsi but is several orders of magnitude
　more efficient. it outperforms four major fast dimensionality reduction methods  1  1  1  1  in retrieval quality.   we conducted extensive experiments with lsi using a large corpus and found that proper normalization of semantic vectors for terms and documents improve recall by 1% compared with the standard lsi that strictly follows svd.   without sufficient dimensions  lsi cannot accurately rank documents for large corpora and is therefore noticeably inferior to okapi. unlike works that use lsi to improve retrieval quality  we use lsi as an implicit document clustering method that can work with low-dimensional data. 1 we use low-dimensional subvectors of semantic vectors to implicitly cluster documents in an overlay  which helps reduce the search space. we then use okapi to guide the search process and document selection.
　it should be emphasized that our contributions are beyond their use in psearch  since the problems we address are common to many other systems.  1  deriving low-dimensional representation for high-dimensional data is a common theme for many fields. existing methods such as principal component analysis  pca  and lsi are not scalable. elsi is efficient and produces high-quality low-dimensional data. therefore it can be used in many systems to replace pca or lsi.  1  the proper configuration we found for lsi should be of general interest to the lsi community.  1  existing lsi implementations compare the semantic vector of a query with that of every document. dumais noticed the inefficiency of this method and commented that no known technique can effectively reduce the search space for high-dimensional data . the fundamentals of our techniques  despite the fact that they were originally developed for p1p systems  can also be applied to centralized systems to reduce the search space.
　this paper addresses the challenge of using lsi in psearch. one can also build a p1p ir system around our document clustering idea without using lsi  by partitioning documents into clusters and assigning them to different nodes. given a query  it searches only nodes whose centroids are the closest to the query. since nearest neighbor search in a high-dimensional space is prohibitive  this approach cannot employ a distributed search strategy as psearch does. in a naive implementation  each node would need to know the ip and centroid of all other nodes. however  it is important for nodes in a dynamic p1p system to maintain only a small amount of global information to be scalable. it is a subject of future work to pursue this approach and address this challenges.
　the remainder of the paper is organized as follows. section 1 gives an overview of the psearch system. sections 1 and 1 describe and evaluate techniques to improve lsi's retrieval quality and efficiency  respectively. section 1 puts all these techniques together and evaluates the complete psearch system. related work is discussed in section 1. section 1 concludes the paper.
1. system overview
　to set the stage for our discussion  we first present an overview of the psearch system  see  for details . in psearch  a large number of nodes are organized into an application-level overlay network to offer ir service. nodes in the overlay collectively form a psearch engine. inside the engine  nodes have completely homogeneous functions. a client intending to use psearch connects to any engine node to publish document indices or submit queries.
　figure 1 shows an example of how the system works. node a publishes a document to node b inside the engine. b builds the index for the document and routes the index in the overlay. the index is finally stored on node f based on its semantics. when a query is submitted to node e  the query is routed to node c based on the semantics of the query. c then takes the responsibility for finding relevant documents and returning them to e. in this example  c may return the index published by a and stored on f.
　psearch uses a can  to organize engine nodes into an overlay and uses an extension of lsi to answer queries. we call this algorithm plsi. in the following  we first give some background and then present the plsi algorithm.
1 vector space model  vsm 
　in vsm   a term-document matrix a =  aij  （ rt〜d is formed to represent a collection of d documents containing words from a vocabulary of t terms. each column vector aj  1 ＋ j ＋ d  corresponds to a document j. weight aij represents the importance of term i in document j. the weights are usually computed from variants of tfidf . for instance  the ltc  term weighting scheme computes aij as follows 
 1 
 1 
 where fij is the frequency of term i in document j and di is the number of documents that contain term i. normalization in equation 1 ensures that document vector aj is of unit length. queries are represented as vectors in a similar fashion. the similarity between two vectors is measured as their inner product. when vectors are normalized  as they are in ltc   the inner product is the same as the cosine of the angle between the vectors.
1 latent semantic indexing  lsi 
　literal matching schemes suffer from synonyms and noise in documents. lsi overcomes these problems by using statistically derived concepts instead of terms for retrieval. it uses truncated singular value decomposition  svd   to transform a highdimensional document vector into a lower-dimensional semantic vector  by projecting the former into a semantic subspace.
　suppose the rank of the term-document matrix a is r. svd decomposes a into the product of three matrices 
	a = uΣv t	 1 
wherediag σ1 ... σr  （
rr〜r  and v =  v1 ... vr  （ r 〜 . v is the transpose of v . σi's are a's singular values  σ1 − σ1 − ... − σr. u and v are column-orthonormal. lsi approximates a with a rank-k matrix
	ak = ukΣkvkt	 1 
by omitting all but the k largest singular values  where uk =  u1 ... uk   Σk = diag σ1 ... σk   vk =  v1 ... vk .
　row i of uk （ rt〜k is the representation of term i in the kdimensional semantic space. a document  or query  vector q （ rt〜1 can be folded into the k-dimensional semantic space using equation 1 or 1 . the difference is whether to scale the vector by the inverse of the singular values. similar to vsm  the similarity between semantic vectors is measured as their inner product.
q =uktq 1 q =Σ k 1uktq 1 	e	1	object key	1	search region for the query
	 a 	 b 	 c 
figure 1:  a  overview of the psearch system.	 b  a 1-dimensional can.	 c  plsi in a 1-dimensional can.1 content-addressable network  can 
　recent p1p overlay networks such as can  offer an administration-free and fault-tolerant storage space. the basic functionality these systems provide is a distributed hash table  dht  that maps  keys  to  objects . can partitions a d-dimensional cartesian space into zones and assigns each zone to a node. two nodes are routing neighbors in the overlay if their zones overlap in all but one dimension along which they abut each other. an object key is a point in the cartesian space and the object is stored at the node whose zone contains the point. locating an object is reduced to routing to the node that hosts the object.
　an example can is shown in figure 1 b . there are five nodes a-e in the overlay. each node owns a zone in the cartesian space. initially c owns the entire zone at the upper-right corner. when d joins  the zone owned by c splits and part of the zone is given to d. when d wishes to retrieve the object with key  1 1   it sends the request to e and e forwards the request to a.
1 the plsi algorithm
　both lsi and can employ a cartesian space. the plsi algorithm splices them together to build psearch. it sets the dimensionality of a can to be equal to that of lsi's semantic space  k . the index for a document is stored in the can using its semantic vector as the key. the effect is that indices stored close in the overlay are also close in semantics.
figure 1 c  illustrates the basic steps of plsi.
1. when receiving a new document  the engine node derives itssemantic vector using lsi and uses the semantic vector as the key to store its index in the can.
1. when receiving a query  the engine node derives its semanticvector and routes the query in the can using the semantic vector as the key.
1. upon reaching the destination  the query is flooded to nodeswithin a small radius r.
