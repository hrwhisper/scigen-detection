aslam  pavlu  and savell  introduced the hedge algorithm for metasearch which effectively combines the ranked lists of documents returned by multiple retrieval systems in response to a given query. it has been demonstrated that the hedge algorithm is an effective technique for metasearch  often significantly exceeding the performance of standard metasearch and ir techniques over small trec collections. in this work  we explore the effectiveness of hedge as an automatic metasearch engine over the much larger gov1 collection on about 1 topics as part of the million query track of trec 1.
1 introduction
aslam  pavlu  and savell introduced a unified framework for simultaneously solving the problems of metasearch  pooling  and system evaluation based on the hedge algorithm for on-line learning . given the ranked lists of documents returned by a collection of ir systems in response to a given query  hedge is capable of matching and often exceeding the performance of the best underlying retrieval system; given relevance feedback  hedge is capable of  learning  how to optimally combine the input systems  yielding a level of performance which often significantly exceeds that of the best underlying system.
¡¡in previous experiments with smaller trec collections   it has been shown that after only a handful of judged feedback documents  hedge is able to significantly outperform the combmnz and condorcet metasearch techniques. it has also been shown that hedge is able to efficiently construct pools containing significant numbers of relevant documents and that these pools are highly effective at evaluating the underlying systems . although the hedge algorithm has been shown to be a strong technique for metasearch  pooling  and system evaluation using the relatively small or moderate trec collections  trecs 1  1  1  1  1   it has yet to be demonstrated that the technique is scalable to corpora whose data size is at the terabyte level. in this work  we assess the performance of hedge on a terabyte scale  presenting testing results using the million query track topics and data.
¡¡finally  we note that in the context of this trec  in the absence of feedback  hedge is a fully automatic metasearch algorithm.
1 results
in the million query track  hedge was run with no feedback  ergo the name of the submission  hedge1   as an automatic metasearch engine. we indexed the collection using the lemur toolkit; that process took about 1 days using a 1-processor dual-core opteron machine  1 ghz/core .
1 underlying ir systems
the underlying systems include:  1  two tf-idf retrieval systems;  1  three kl-divergence retrieval models  one with dirichlet prior smoothing  one with jelinek-mercer smoothing  and the last with absolute discounting;  1  a cosine similarity model;  1  the okapi retrieval model;  1  and the inquery retrieval method. all of the above retrieval models are provided as standard ir systems by the lemur toolkit . for each query and retrieval system  we considered the top 1 scored documents for that retrieval system. once all retrieval systems were run against all queries  we ran the hedge algorithm 
to perform metasearch on the ranked lists obtained. these models were run on 1 topics using the

table 1: performance over the 1 terabyte1 topics  where the evaluation was performed using traditional
table 1: results for input and metasearch systems methods and metrics.
on the terabyte1 collection and topics. combmnz  cordorcet  and hedge1 were run over all input systems.
systemmapp 1jelinek-mercer11dirichlet11tfidf11okapi11log-tfidf11absolute discounting11cosine similarity11combmnz11condorcet11hedge1.1.1map1r-prec1bpref1recip-rank1retrieved1relevant1relevant retrieved1statap  1 topics 1statr-prec  1 topics 1statprec 1  1 topics 1emap  1 topics 1precision at recall  1 terabyte1 topics recallprecision.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1precision at rank  1 terabyte1 topics rankprecisionat 1 docs1at 1 docs1at 1 docs1at 1 docs1at 1 docs1at 1 docs1at 1 docs1at 1 docs1at 1 docs1r-precision1table 1: performance over mq topics  where the evaluation is performed using the mq evaluation methodology. statap  statr-prec  and statprec 1 refer to estimates of average precision  r-precision  and precision 1  averaged over 1 topics. emap refers to the mtc evaluation over 1 topics; note that the emap value is not on the same scale as traditional map values .
gov1 collection.
¡¡for reference  table 1 illustrates that both hedge1 and combmnz are able to exceed the performance of the best underlying system  terabyte1 data . this demonstrates that hedge alone  even without any relevance feedback  is a successful metasearch technique  exceeding the metasearch performance of condorcet and rivaling the performance of combmnz.
1 results for million query 1
tables 1 & 1 and figure 1 present the performance
table 1: performance over the 1 terabyte1 topics 
of hedge1 on the 1 million query track collection where the evaluation was performed using traditional
and topics  separately for the mq evaluation methmethods and metrics.
ods and for the 1 terabyte topics using traditional evaluation methods and metrics. this performance was in line with expectations and previous results.

figure 1: hedge1: precision-recall curve averaged over the 1 terabyte1 topics.
