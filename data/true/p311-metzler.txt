query expansion  in the form of pseudo-relevance feedback or relevance feedback  is a common technique used to improve retrieval effectiveness. most previous approaches have ignored important issues  such as the role of features and the importance of modeling term dependencies. in this paper  we propose a robust query expansion technique based on the markov random field model for information retrieval. the technique  called latent concept expansion  provides a mechanism for modeling term dependencies during expansion. furthermore  the use of arbitrary features within the model provides a powerful framework for going beyond simple term occurrence features that are implicitly used by most other expansion techniques. we evaluate our technique against relevance models  a state-of-the-art language modeling query expansion technique. our model demonstrates consistent and significant improvements in retrieval effectiveness across several trec data sets. we also describe how our technique can be used to generate meaningful multi-term concepts for tasks such as query suggestion/reformulation.
categories and subject descriptors
h.1  information storage and retrieval : information search and retrieval
general terms
algorithms  experimentation  theory
keywords
information retrieval  query expansion  markov random fields
1. introduction
¡¡users of information retrieval systems are required to express complex information needs in terms of boolean expressions  a short list of keywords  a sentence  a question  or
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  july 1  1  amsterdam  the netherlands.
copyright 1 acm 1-1-1/1 ...$1.
possibly a longer narrative. a great deal of information is lost during the process of translating from the information need to the actual query. for this reason  there has been a strong interest in query expansion techniques. such techniques are used to augment the original query to produce a representation that better reflects the underlying information need.
¡¡query expansion techniques have been well studied for various models in the past and have shown to significantly improve effectiveness in both the relevance feedback and pseudo-relevance feedback setting  1  1  1  1 .
¡¡recently  a markov random field  mrf  model for information retrieval was proposed that goes beyond the simplistic bag of words assumption that underlies bm1 and the  unigram  language modeling approach to information retrieval  1  1 . the mrf model generalizes the unigram  bigram  and other various dependence models . most past term dependence models have failed to show consistent  significant improvements over unigram baselines  with few exceptions . the mrf model  however  has been shown to be highly effective across a number of tasks  including ad hoc retrieval  1  1   named-page finding   and japanese language web search .
¡¡until now  the model has been solely used for ranking documents in response to a given query. in this work  we show how the model can be extended and used for query expansion using a technique that we call latent concept expansion  lce . there are three primary contributions of our work.
¡¡first  lce provides a mechanism for combining term dependence with query expansion. previous query expansion techniques are based on bag of words models. therefore  by performing query expansion using the mrf model  we are able to study the dynamics between term dependence and query expansion.
¡¡next  as we will show  the mrf model allows arbitrary features to be used within the model. query expansion techniques in the past have implicitly only made use of term occurrence features. by using more robust feature sets  it is possible to produce better expansion terms that discriminate between relevant and non-relevant documents better.
¡¡finally  our proposed approach seamlessly provides a mechanism for generating both single and multi-term concepts. most previous techniques  by default  generate terms independently. there have been several approaches that make use of generalized concepts  however such approaches were somewhat heuristic and done outside of the model  1  1 . our approach is both formally motivated and a natural extension of the underlying model.
¡¡the remainder of this paper is laid out as follows. in section 1 we describe related query expansion approaches. section 1 provides an overview of the mrf model and details our proposed latent concept expansion technique. in section 1 we evaluate our proposed model and analyze the results. finally  section 1 concludes the paper and summarizes the major results.
1. related work
¡¡one of the classic and most widely used approaches to query expansion is the rocchio algorithm . rocchio's approach  which was developed within the vector space model  reweights the original query vector by moving the weights towards the set of relevant or pseudo-relevant documents and away from the non-relevant documents. unfortunately  it is not possible to formally apply rocchio's approach to a statistical retrieval model  such as language modeling for information retrieval.
¡¡a number of formalized query expansion techniques have been developed for the language modeling framework  including zhai and lafferty's model-based feedback and lavrenko and croft's relevance models  1  1 . both approaches attempt to use pseudo-relevant or relevant documents to estimate a better query model.
¡¡model-based feedback finds the model that best describes the relevant documents while taking a background  noise  model into consideration. this separates the content model from the background model. the content model is then interpolated with the original query model to form the expanded query.
¡¡the other technique  relevance models  is more closely related to our work. therefore  we go into the details of the model. much like model-based feedback  relevance models estimate an improved query model. the only difference between the two approaches is that relevance models do not explicitly model the relevant or pseudo-relevant documents. instead  they model a more generalized notion of relevance  as we now show.
¡¡given a query q  a relevance model is a multinomial distribution  p ¡¤|q   that encodes the likelihood of each term given the query as evidence. it is computed as:
		 1 
where rq is the set of documents that are relevant or pseudorelevant to query q. in the pseudo-relevant case  these are the top ranked documents for query q. furthermore  it is assumed that p d  is uniform over this set. these mild assumptions make computing the bayesian posterior more practical.
¡¡after the model is estimated  documents are ranked by clipping the relevance model by choosing the k most likely terms from p ¡¤|q . this clipped distribution is then interpolated with with the original  maximum likelihood query model . this can be thought of as expanding the original query by k weighted terms. throughout the remainder of this work  we refer to this instantiation of relevance models as rm1.
¡¡there has been relatively little work done in the area of query expansion in the context of dependence models . however  there have been several attempts to expand using multi-term concepts. xu and croft's local context analysis  lca  method combined passage-level retrieval with concept expansion  where concepts were single terms and phrases . expansion concepts were chosen and weighted using a metric based on co-occurrence statistics. however  it is not clear based on the analysis done how much the phrases helped over the single terms alone.
¡¡papka and allan investigate using relevance feedback to perform multi-term concept expansion for document routing . the concepts used in their work are more general than those used in lca  and include inquery query language structures  such as #uw1 white house   which corresponds to the concept  the terms white and house occur  in any order  within 1 terms of each other . results showed that combining single term and large window multi-term concepts significantly improved effectiveness. however  it is unclear whether the same approach is also effective for ad hoc retrieval  due to the differences in the tasks.
1. model
¡¡this section details our proposed latent concept expansion technique. as mentioned previously  the technique is an extension of the mrf model for information retrieval . therefore  we begin by providing an overview of the mrf model and our proposed extensions.
1 mrfs for ir
1.1 basics
¡¡markov random fields  which are undirected graphical models  provide a compact  robust way of modeling a joint distribution. here  we are interested in modeling the joint distribution over a query q = q1 ... qn and a document d. it is assumed the underlying distribution over pairs of documents and queries is a relevance distribution. that is  sampling from the distribution gives pairs of documents and queries  such that the document is relevant to the query.
¡¡a mrf is defined by a graph g and a set of non-negative potential functions over the cliques in g. the nodes in the graph represent the random variables and the edges define the independence semantics of the distribution. a mrf satisfies the markov property  which states that a node is independent of all of its non-neighboring nodes given observed values for its neighbors.
¡¡given a graph g  a set of potentials ¦×i  and a parameter vector ¦«  the joint distribution over q and d is given by:
;¦« 
c¡Êc g 
where z is a normalizing constant. we follow common convention and parameterize the potentials as ¦×i c;¦«  = exp ¦Ëifi c    where fi c  is a real-valued feature function.
1.1 constructing g
¡¡given a query q  the graph g can be constructed in a number of ways. however  following previous work  we consider three simple variants . these variants are full independence  where each query term is independent of each other given a document  sequential dependence  which assumes a dependence exists between adjacent query terms  and full dependence  which makes no independence assumptions.
1.1 parameterization
¡¡mrfs are commonly parameterized based on the maximal cliques of g. however  such a parameterization is too coarse for our needs. we need a parameterization that allows us to associate feature functions with cliques on a more fine grained level  while keeping the number of features  and thus the number of parameters  reasonable. therefore  we allow cliques to share feature functions and parameters based on clique sets. that is  all of the cliques within a clique set are associated with the same feature function and share a single parameter. this effectively ties together the parameters of the features associated with each set  which significantly reduces the number of parameters while still providing a mechanism for fine-tuning on the level of clique sets.
¡¡we propose seven clique sets for use with information retrieval. the first three clique sets consist of cliques that contain one or more query terms and the document node. features over these cliques should encode how well the terms in the clique configuration describe the document. these sets are:
  td - set of cliques containing the document node and exactly one query term.
  od - set of cliques containing the document node and two or more query terms that appear in sequential order within the query.
  ud - set of cliques containing the document node and two or more query terms that appear in any order within the query.
note that ud is a superset of od. by tying the parameters among the cliques within each set we can control how much influence each type gets. this also avoids the problem of trying to determine how to estimate weights for each clique within the sets. instead  we now must only estimate a single parameter per set.
¡¡next  we consider cliques that only contain query term nodes. these cliques  which were not considered in   are defined in an analogous way to those just defined  except the the cliques are only made up of query term nodes and do not contain the document node. feature functions over these cliques should capture how compatible query terms are to one another. these clique features may take on the form of language models that impose well-formedness of the terms. therefore  we define following query-dependent clique sets:
  tq - set of cliques containing exactly one query term.
  oq - set of cliques containing two or more query terms that appear in sequential order within the query.
  uq - set of cliques containing two or more query terms that appear in any order within the query.
¡¡finally  there is the clique that only contains the document node. features over this node can be used as a type of document prior  encoding document-centric properties. this trivial clique set is then:
  d - clique set containing only the singleton node d
¡¡we note that our clique sets form a set cover over the cliques of g  but are not a partition  since some cliques appear in multiple clique sets.
¡¡after tying the parameters in our clique sets together and using the exponential potential function form  we end up with the following simplified form of the joint distribution:

fdq d q  - document and query dependent
  - query dependent  	
fd d  - document dependent	document + query independent
where fdq  fq  and fd are convenience functions defined by the document and query dependent  query dependent  and document dependent components of the joint distribution  respectively. these will be used to simplify and clarify expressions derived throughout the remainder of the paper.
1.1 features
¡¡any arbitrary feature function over clique configurations can be used in the model. the correct choice of features depends largely on the retrieval task and the evaluation metric. therefore  there is likely not to be a single  universally applicable set of features.
¡¡to provide an idea of the range of features that can be used  we now briefly describe possible types of features that could be used. possible query term dependent features include tf  idf  named entities  term proximity  and text style to name a few. many types of document dependent features can be used  as well  including document length  pagerank  readability  and genre  among others.
¡¡since it is not our goal here to find optimal features  we use a simple  fixed set of features that have been shown to be effective in previous work . see table 1 for a list of features used. these features attempt to capture term occurrence and term proximity. better feature selection in the future will likely lead to improved effectiveness.
1.1 ranking
¡¡given a query q  we wish to rank documents in descending order according to pg ¦« d|q . after dropping document independent expressions from logpg ¦« q d   we derive the following ranking function:
rank
	pg ¦« d|q 	=	fdq d q  + fd d 	 1 
which is a simple weighted linear combination of feature functions that can be computed efficiently for reasonable graphs.
1.1 parameter estimation
featurevalueftd qi d 	log  1   ¦Á 	d|	+ ¦Á |c|
|fod qi qi+1 ... qi+k d fud qi ... qj d ftq qi foq qi qi+1 ... qi+k fuq qi ... qj fd1table 1: feature functions used in markov random field model. here  tfw d is the number of times term w occurs in document d  tf#1 qi...qi+k  d denotes the number of times the exact phrase qi ...qi+k occurs in document d  tf#uw qi...qj  d is the number of times the terms qi ...qj appear ordered or unordered within a window of n terms  and |d| is the length of document d. the cf and |c| values are analogously defined on the collection level. finally  ¦Á and ¦Â are model hyperparameters that control smoothing for single term and¡¡now that the model has been fully specified  the final step is to estimate the model parameters. although mrfs are generative models  it is inappropriate to train them using phrase features  respectively.
conventional likelihood-based approaches because of metric divergence . that is  the maximum likelihood estimate is unlikely to be the estimate that maximizes our evaluation metric. for this reason  we discriminatively train our model to directly maximize the evaluation metric under consideration  1  1  1 . since our parameter space is small  we make use of a simple hill climbing strategy  although other more sophisticated approaches are possible .
1 latent concept expansion
¡¡in this section we describe how this extended mrf model can be used in a novel way to generate single and multiterm concepts that are topically related to some original query. as we will show  the concepts generated using our technique can be used for query expansion or other tasks  such as suggesting alternative query formulations.
¡¡we assume that when a user formulates their original query  they have some set of concepts in mind  but are only able to express a small number of them in the form of a query. we treat the concepts that the user has in mind  but did not explicitly express in the query  as latent concepts. these latent concepts can consist of a single term  multiple terms  or some combination of the two. it is  therefore  our goal to recover these latent concepts given some original query.
¡¡this can be accomplished within our framework by first expanding the original graph g to include the type of concept we are interested in generating. we call this expanded graph h. in figure 1  the middle graph provides an example of how to construct an expanded graph that can generate single term concepts. similarly  the graph on the right illustrates an expanded graph that generates two term concepts. although these two examples make use of the sequential dependence assumption  i.e. dependencies between adjacent query terms   it is important to note that both the original query and the expansion concepts can use any independence structure.
¡¡after h is constructed  we compute ph ¦« e|q   a probability distribution over latent concepts  according to:

where r is the universe of all possible documents and e is some latent concept that may consist of one or more terms. since it is not practical to compute this summation  we must approximate it. we notice that ph ¦« q e d  is likely to be peaked around those documents d that are highly ranked according to query q. therefore  we approximate ph ¦« e|q  by only summing over a small subset of relevant or pseudo-relevant documents for query q. this is computed as follows:

where rq is a set of relevant or pseudo-relevant documents for query q and all clique sets are constructed using h. as we see  the likelihood contribution for each document in rq is a combination of the original query's score for the document  see equation 1   concept e's score for the document  and e's document-independent score. therefore  this equation can be interpreted as measuring how well q and e account for the top ranked documents and the  goodness  of e  independent of the documents. for maximum robustness  we use a different set of parameters for fqd q d  and fqd e d   which allows us to weight the term  ordered  and unordered window features differently for the original query and the candidate expansion concept.
1.1 query expansion
¡¡to use this framework for query expansion  we first choose an expansion graph h that encodes the latent concept structure we are interested in expanding the query using. we then select the k latent concepts with the highest likelihood given by equation 1. a new graph g is constructed by augmenting the original graph g with the k expansion concepts e1 ... ek. finally  documents are ranked according to  using equation 1.
1.1 comparison to relevance models

figure 1: graphical model representations of relevance modeling  left   latent concept expansion using single term concepts  middle   and latent concept expansion using two term concepts  right  for a three term query.¡¡inspecting equations 1 and 1 reveals the close connection that exists between lce and relevance models. both models essentially compute the likelihood of a term  or concept  in the same manner. it is easy to see that just as the mrf model can be viewed as a generalization of language modeling  so too can lce be viewed as a generalization of relevance models.
¡¡there are important differences between mrfs/lce and unigram language models/relevance models. see figure 1 for graphical model representations of both models. unigram language models and relevance models are based on the multinomial distribution. this distributional assumption locks the model into the bag of words representation and the implicit use of term occurrence features. however  the distribution underlying the mrf model allows us to move beyond both of these assumptions  by modeling both dependencies between query terms and allowing arbitrary features to be explicitly used.
¡¡moving beyond the simplistic bag of words assumption in this way results in a general  robust model and  as we show in the next section  translates into significant improvements in retrieval effectiveness.
1. experimental results
¡¡in order to better understand the strengths and weaknesses of our technique  we evaluate it on a wide range of data sets. table 1 provides a summary of the trec data sets considered. the wsj  ap  and robust collections are smaller and consist entirely of newswire articles  whereas wt1g and gov1 are large web collections. for each data set  we split the available topics into a training and test set  where the training set is used solely for parameter estimation and the test set is used for evaluation purposes.
¡¡all experiments were carried out using a modified version of indri  which is part of the lemur toolkit  1  1 . all collections were stopped using a standard list of 1 common terms and stemmed using a porter stemmer. in all cases  only the title portion of the trec topics are used to construct queries. we construct g using the sequential dependence assumption for all data sets .
1 ad hoc retrieval results
¡¡we now investigate how well our model performs in practice in a pseudo-relevance feedback setting. we compare unigram language modeling  with dirichlet smoothing   the mrf model  without expansion   relevance models  and lce to better understand how each model performs across the various data sets.
¡¡for the unigram language model  the smoothing parameter was trained. for the mrf model  we train the model parameters  i.e. ¦«  and model hyperparameters  i.e. ¦Á  ¦Â . for rm1 and lce  we also train the number of pseudo-
namedescription# docstrain topicstest topicswsjwall	st.
journal 1111apassoc.	press 1111robustrobust 1 data111wt1gtrec	web
collection1 1-1-1gov1 crawl of
.gov domain1 1-1-1table 1: overview of trec collections and topics.
relevant feedback documents used and the number of expansion terms.
1.1 expansion with single term concepts
¡¡we begin by evaluating how well our model performs when expanding using only single terms. before we describe and analyze the results  we explicitly state how expansion term likelihoods are computed under this setup  i.e. using the sequential dependence assumption  expanding with single term concepts  and using our feature set . the expansion term likelihoods are computed as follows:

where b ¡Ê q denotes the set of bigrams in q. this equation clearly shows how lce differs from relevance models. when we set = 1 and all other parameters to 1  we obtain the exact formula that is used to compute term likelihoods in the relevance modeling framework. therefore  lce adds two very important factors to the equation. first  it adds the ordered and unordered window features that are applied to the original query. second  it applies an intuitive tf.idf-like form to the candidate expansion term w. the idf factor  which is not present in relevance models  plays an important role in expansion term selection.

figure 1: histograms that demonstrate and compare the robustness of relevance models  rm1  and latent concept expansion  lce  with respect to the query likelihood model  ql  for the ap  robust  and wt1gdata sets.
¡¡the results  evaluated using mean average precision  are given in table 1. as we see  the mrf model  relevance models  and lce always significantly outperform the unigram language model. in addition  lce shows significant improvements over relevance models across all data sets. the relative improvements over relevance models is 1% for ap  1% for wsj  1% for robust  1% for wt1g  and
1% for gov1.
¡¡furthermore  lce shows small  but not significant  improvements over relevance modeling for metrics such as precision at 1  1  and 1. however  both relevance modeling and lce show statistically significant improvements in such metrics over the unigram language model.
¡¡another interesting result is that the mrf model is statistically equivalent to relevance models on the two web data sets. in fact  the mrf model outperforms relevance models on the wt1g data set. this reiterates the importance of non-unigram  proximity-based features for content-based web search observed previously  1  1 .
¡¡although our model has more free parameters than relevance models  there is surprisingly little overfitting. instead  the model exhibits good generalization properties.
1.1 expansion with multi-term concepts
¡¡we also investigated expanding using both single and two word concepts. for each query  we expanded using a set of single term concepts and a set of two term concepts. the sets were chosen independently. unfortunately  only negligible increases in mean average precision were observed.
¡¡this result may be due to the fact that strong correlations exist between the single term expansion concepts. we found that the two word concepts chosen often consisted of two highly correlated terms that are also chosen as single term concepts. for example  the two term concept  stock market  was chosen while the single term concepts  stock  and  market  were also chosen. therefore  many two word concepts are unlikely to increase the discriminative power of the expanded query. this result suggests that concepts should be chosen according to some criteria that also takes novelty  diversity  or term correlations into account.
¡¡another potential issue is the feature set used. other feature sets may ultimately yield different results  especially if they reduce the correlation among the expansion concepts.
¡¡therefore  our experiments yield no conclusive results with regard to expansion using multi-term concepts. instead  the results introduce interesting open questions and directions for future exploration.
lmmrfrm1lcewsj.1.1¦Á.1¦Á.1¦Á¦Â¦Ãap.1.1¦Á.1¦Á¦Â.1¦Á¦Â¦Ãrobust.1.1¦Á.1¦Á¦Â.1¦Á¦Â¦Ãwt1g.1.1¦Á.1¦Á.1¦Á¦Â¦Ãgov1.1.1¦Á.1¦Á.1¦Á¦Â¦Ãtable 1: test set mean average precision for language modeling  lm   markov random field  mrf   relevance models  rm1   and latent concept expansion  lce . the superscripts ¦Á  ¦Â  and ¦Ã indicate statistically significant improvements  p   1  over lm  mrf  and rm1  respectively.
1 robustness
¡¡as we have shown  relevance models and latent concept expansion can significantly improve retrieval effectiveness over the baseline query likelihood model. in this section we analyze the robustness of these two methods. here  we define robustness as the number queries whose effectiveness are improved/hurt  and by how much  as the result of applying these methods. a highly robust expansion technique will significantly improve many queries and only minimally hurt a few.
¡¡figure 1 provides an analysis of the robustness of relevance modeling and latent concept expansion for the ap  robust  and wt1g data sets. the analysis for the two data sets not shown is similar. the histograms provide  for various ranges of relative decreases/increases in mean average precision  the number of queries that were hurt/improved with respect to the query likelihood baseline. as the results show  lce exhibits strong robustness for each data set. for ap  relevance models improve 1 queries and hurt 1  whereas lce improves 1 and hurts 1. although relevance models improve the effectiveness of 1 more queries than lce  the relative improvement exhibited by lce is significantly larger. for the robust data set  relevance models improve 1 queries and hurt 1  and lce improves 1 and hurts 1. finally  for the wt1g collection  relevance models improve 1 queries and hurt 1  and lce improves 1 and hurts 1. as with ap  the amount of improvement exhibited by the lce versus relevance models is significantly larger for both the robust and wt1g data sets. in addition  when lce does hurt performance  it is less likely to hurt as much as relevance modeling  which is a desirable property.
1 word concepts1 word concepts1 word conceptstelescopehubble telescopehubble space telescopehubblespace telescopehubble telescope spacespacehubble spacespace telescope hubblemirrortelescope mirrorspace telescope nasanasatelescope hubblehubble telescope astronomylaunchmirror telescopenasa hubble spaceastronomytelescope nasaspace telescope mirrorshuttletelescope spacetelescope space nasatesthubble mirrorhubble telescope missionnewnasa hubblemirror mirror mirrordiscoverytelescope astronomyspace telescope launchtimetelescope opticalspace telescope discoveryuniversehubble opticalshuttle space telescopeopticaltelescope discoveryhubble telescope flawlighttelescope shuttletwo hubble spacetable 1: fifteen most likely one  two  and three word concepts constructed using the top 1 documents retrieved for the query hubble telescope achievements on the robust collection.¡¡overall  lce improves effectiveness for 1%-1% of queries  depending on the data set. when used in combination with a highly accurate query performance prediction system  it may be possible to selectively expand queries and minimize the loss associated with sub-baseline performance.
1 multi-term concept generation
¡¡although we found that expansion using multi-term concepts failed to produce conclusive improvements in effectiveness  there are other potential tasks that these concepts may be useful for  such as query suggestion/reformulation  summarization  and concept mining. for example  for a query suggestion task  the original query could be used to generate a set of latent concepts which correspond to alternative query formulations.
¡¡although evaluating our model on these tasks is beyond the scope of this work  we wish to show an illustrative example of the types of concepts generated using our model. in table 1  we present the most likely one  two  and three term concepts generated using lce for the query hubble telescope achievements using the top 1 ranked documents from the robust collection.
¡¡it is well known that generating multi-term concepts using a unigram-based model produces unsatisfactory results  since it fails to consider term dependencies. this is not the case when generating multi-term concepts using our model. instead  a majority of the concepts generated are well-formed and meaningful. there are several cases where the concepts are less coherent  such as mirror mirror mirror. in this case  the likelihood of the term mirror appearing in a pseudo-relevant document outweighs the  language modeling  features  e.g. foq   which causes this non-coherent concept to have a high likelihood. such examples are in the minority  however.
¡¡not only are the concepts generated well-formed and meaningful  but they are also topically relevant to the original query. as we see  all of the concepts generated are on topic and in some way related to the hubble telescope. it is interesting to see that the concept hubble telescope flaw is one of the most likely three term concepts  given that it is somewhat contradictory to the original query. despite this contradiction  documents that discuss the telescope flaws are also likely to describe the successes  as well  and therefore this is likely to be a meaningful concept.
¡¡one important thing to note is that the concepts lce generates are of a different nature than those that would be generated using a bigram relevance model. for example  a bigram model would be unlikely to generate the concept telescope space nasa  since none of the bigrams that make up the concept have high likelihood. however  since our model is based on a number of different features over various types of cliques  it is more general and robust than a bigram model.
¡¡although we only provided the concepts generated for a single query  we note that the same analysis and conclusions generalize across other data sets  with coherent  topically related concepts being consistently generated using lce.
1 discussion
¡¡our latent concept expansion technique captures two semiorthogonal types of dependence. in information retrieval  there has been a long-term interest in understanding the role of term dependence. out of this research  two broad types of dependencies have been identified.
¡¡the first type of dependence is syntactic dependence. this type of dependence covers phrases  term proximity  and term co-occurrence  1  1  1  1  1 . these methods capture the fact that queries implicitly or explicitly impose a certain set of positional dependencies.
¡¡the second type is semantic dependence. examples of semantic dependence are relevance feedback  pseudo-relevance feedback  synonyms  and to some extent stemming . these techniques have been explored on both the query and document side. on the query side  this is typically done using some form of query expansion  such as relevance models or lce. on the document side  this is done as document expansion or document smoothing  1  1  1 .
¡¡although there may be some overlap between syntactic and semantic dependencies  they are mostly orthogonal. our model uses both types of dependencies. the use of phrase and proximity features within the model captures syntactic dependencies  whereas lce captures query-side semantic dependence. this explains why the initial improvement in effectiveness achieved by using the mrf model is not lost after query expansion. if the same types of dependencies were capture by both syntactic and semantic dependencies  lce would be expected to perform about equally as well as relevance models. therefore  by modeling both types of dependencies we see an additive effect  rather than an absorbing effect.
¡¡an interesting area of future work is to determine whether or not modeling document-side semantic dependencies can add anything to the model. previous results that have combined query- and document-side semantic dependencies have shown mixed results  1  1 .
1. conclusions
¡¡in this paper we proposed a robust query expansion technique called latent concept expansion. the technique was shown to be a natural extension of the markov random field model for information retrieval and a generalization of relevance models. lce is novel in that it performs single or multi-term expansion within a framework that allows the modeling of term dependencies and the use of arbitrary features  whereas previous work has been based on the bag of words assumption and term occurrence features.
¡¡we showed that the technique can be used to produce high quality  well formed  topically relevant multi-term expansion concepts. the concepts generated can be used in an alternative query suggestion module. we also showed that the model is highly effective. in fact  it achieves significant improvements in mean average precision over relevance models across a selection of trec data sets. it was also shown the mrf model itself  without any query expansion  outperforms relevance models on large web data sets. this reconfirms previous observations that modeling dependencies via the use of proximity features within the mrf has more of an impact on larger  noisier collections than smaller  well-behaved ones.
¡¡finally  we reiterated the importance of choosing expansion terms that model relevance  rather than the relevant documents and showed how lce captures both syntactic and query-side semantic dependencies. future work will look at incorporating document-side dependencies  as well.
acknowledgments
this work was supported in part by the center for intelligent information retrieval  in part by nsf grant #cns-1  in part by arda and nsf grant #ccf-1  and in part by microsoft live labs. any opinions  findings and conclusions or recommendations expressed in this material are those of the author s  and do not necessarily reflect those of the sponsor.
