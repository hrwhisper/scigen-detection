this paper describes the retrieval experiments for the main task and list task of the trec-1 questionanswering track. the question answering system described automatically finds answers to questions in a large document collection. the system uses a two-stage retrieval approach to answer finding based on matching of named entities  linguistic patterns  and keywords. in answering a question  the system carries out a detailed query analysis that produces a logical query representation  an indication of the question focus  and answer clue words.
1. introduction
question-answering systems retrieve answers rather than documents in response to a user's question. in the trec question-answering track a number of question-answering systems attempt to answer a predefined list of questions by using a previously determined set of documents. the research is carried out in an unrestricted domain.
the cnlp question answering system uses a two-stage retrieval approach to answer-finding based on matching of named entities  linguistic patterns  and keywords. in answering a question  the system carries out a detailed query analysis that produces a logical query representation  an indication of the question focus  and answer clue words. then the information is passed on to answer finding modules  which take the documents  retrieved in the first stage  for further processing and answer finding. the answer finding module uses three separate strategies to determine the correct answer. two strategies are based on question focus  and the third strategy  based on keywords  is used when the question focus is not found or when the first two strategies fail to identify potential answers. a detailed system overview can be found in section 1.
1. problem description
cnlp participated in two of the three qa track tasks: the main task and the list task. the main task is a continuation of last year's qa track in that systems are required to answer 1 short  fact-based questions. two new aspects were introduced this year: unanswerable questions  with no answer present in the collection   and the notion of an answer confidence level. systems needed to identify unanswerable questions as such  in order for them to be counted as correct. for the confidence level systems needed to state the rank of their final answer or state that they were unsure about their answer. for each question  up to five ranked answer responses were permitted  with the most likely answer ranked first. the maximum length of the answer string for a submitted run was 1 bytes. a response to a question consisted of the question number  the document id of the document containing the answer  rank  run name  and the answer string itself.
the list task questions are similar to those of the main task but include an indication as to how many answer instances needed to be provided for an answer to be considered complete. a response to a list task question consisted of an unordered list with each line containing the question number  the document id of the document containing an answer instance  and the answer string itself. the length of the list or the number of answer instances for each question is specified in the question. as in the main task  the maximum length of each answer string is 1 bytes. the different answer instances could be found within single documents or across multiple documents or a combination of both. there was no guarantee that all requested answer instances could indeed be found in the collection.
answers to both main task and list task questions had to be retrieved automatically from approximately 1 gigabytes of data. sources of the data were: ap newswire 1  1 mb   wall street journal 1  1 mb   san jose mercury news 1  1 mb   financial times 1  1 mb   los angeles times 1  1  1 mb   and foreign broadcast information service 1  1 mb . the submitted answer strings for all tasks were evaluated by nist's human assessors for correctness.  examples of questions for both the main task and list task can be found in table 1.
trec-1 qa questionsmain task questions:
how much does the human adult female brain weigh    who was the first governor of alaska    when was rosa parks born    where is the mason/dixon line    why is a ladybug helpful     where is milan     in which state would you find the catskill mountains    what are invertebrates list task questions:
name 1 u.s. dams that have fish ladders.   what are 1 names of navigational satellites    who are 1 actors who have played tevye in  fiddler on the roof     name 1 countries that produce coffee.table 1. examples of trec-1 questions.
1. system overview
the cnlp question-answering system consists of four different processes: question processing  document processing  paragraph finding  and answer finding. each of the processes is described below.
1 question processing
question processing has two major parts - conversion of questions into a logical query representation and question focus recognition. our l1l  language-to-logic  module was used this year to convert the query into a logical representation suitable for keyword matching and weighting in our answer finder module  see section 1. and section 1 . last year we used our l1l module for first-stage retrieval but this year we relied solely on the ranked list of documents retrieved and provided by nist. l1l was modified this year to also include query expansion for nouns and verbs found in wordnet 1 . based on the parts-ofspeech of the question words  the system added all related synonyms of the first  most frequently used sense  see example at the end of this section   to the l1l representation.
question focus recognition is performed in order to identify the type of answer expected.  expected answers fall into two broad groups - those based on lexical categories  and those based on answer patterns. expected answers based on lexical categories can be identified from the terms used in the question. for example  in the question  what river flows between fargo  north dakota and moorhead  minnesota    we identify that the questioner is looking for the name of a river. the expected answer type  and therefore the question focus  is river. expected answers based on answer patterns are predicted by the recognition of certain question types. if the question is recognized as a definition question  then the answer sentence is likely to include one of several patterns  such as apposition  presence of a form of the verb be  etc.
the question focus recognition routine extracted four elements - the question focus  the lexical answer clue  the number of answers required  used for the list task only   and the confidence level  not fully implemented . in an effort to improve question focus recognition this year  we trained the brill part-ofspeech tagger  on questions from trec 1  trec 1 and howstuffworks.  the resulting rules were used to tag the trec 1 questions. the tagged questions were then run through the collins parser   for a full parse.
there are three steps to question focus assignment. in the first step  the question type is determined using predefined search patterns based on regular expressions. there are 1 special question types  acronym  counterpart  definition  famous  standfor  synonym  why  and 1 standard question types  name-a  name-of  where  when  what/which  how . if a special question type is recognized  then the question type becomes the question focus. second  the parsed question is examined to extract the lexical answer clue  word or phrase  using the predefined search patterns. in the third step  which applies only to standard question types  the lexical answer clue is used to assign the question focus based on lexical categories where possible. table 1 is a review of the questions types. predefined search patterns were developed for these question types:
question type# of search patternsexample questionstandard question typesname-a1name a food high in zinc.  trec 1  question 1 name-of1what is the name of neil armstrong's wife   trec 1  question 1 where1where is john wayne airport   trec 1  question 1 when1when is the official first day of summer   trec 1  question 1 what/which1what is the capital of mongolia   trec 1  question 1 who1who lived in the neuschwanstein castle   trec 1  question 1 how1how tall is the gateway arch in st. louis  mo   trec 1  question 1 special question typesacronym1what is the abbreviation for texas   trec 1  question 1 counterpart1what is the islamic counterpart to the red cross   trec 1  question 1 definition1what is autism   trec 1  question 1 famous1why is jane goodall famous   trec 1  question 1 standfor1what does the technical term isdn mean   trec 1  question 1 synonym1what is the colorful korean traditional dress called   trec 1  question 1 why1why does the moon turn orange   trec 1  question 1 table 1. question types
additional processing performed by the question focus assignment routine includes the extraction of the number of answers required  used for the list task only   and assignment of a confidence level. the number of answers required was extracted based on the predefined search patterns for each question type. the confidence level assigned ranged from 1 to 1  with 1 being the highest level of confidence in the question focus. if the question focus could not be determined  the confidence level was 1. otherwise  the confidence level was set at a value ranging up to 1 depending on the certainty of the question focus. due to the short time available for development  confidence level assignment was only partially implemented for trec 1 and therefore not used in the experiments.
the output resulting from the l1l module and the question focus recognition module is passed on to the paragraph finding module  the answer candidate recognition module  and the answer formatting module. a standard question type  in this case what/which   will produce the following output for the question  what is the deepest lake in the us  :
logical representation:deep* lake* +us    united states   united states of america  america u.s. usa u.s.a. query focus:lake#deepest lake#1tagged: sentence sid= s1   what|wp be|vbz the|dt  cn  deep|jjs lake|nn  /cn  in|in the|dt  np cat= cntry  id= 1   us|np  /np   |.
 /sentence a special question type  in this case definition   will produce the following output for the question  who is duke ellington  :
	logical representation:	+duke* +ellington*
	query focus:	def#duke ellington#1
	tagged:	 sentence sid= s1   who|wp be|vbz  np cat= per  id= 1  
duke|np ellington|np  /np   |.  /sentence 
as can be seen in the examples above  expansions from wordnet are enclosed in parentheses  and the four elements in the question focus are separated by '#'.
1 document processing
for document retrieval  we used the ranked document list as provided by nist. the top 1 documents from the list for each question were extracted from the trec collection as the source documents for paragraph finding.
1 paragraph finding
in the paragraph finding stage  we aim to select the most relevant paragraphs from the top 1 retrieved documents from the first stage retrieval step. paragraph selection was based on keyword occurrences in the paragraphs. although we used the same strategy as last year to identify the paragraphs  we decided to experiment with the selection process itself. for one set of runs we took the original document and divided it up into paragraphs  based on textual clues. after selecting the top 1 most relevant paragraphs we tag only those paragraphs. this approach is identical to our trec1 approach and these runs are labeled  par   paragraph tagging . for the other set of runs we tagged the original document first  then divided it up into paragraphs from which the top 1 paragraphs were selected. these runs are labeled  doc   document tagging . paragraph detection is no longer based on orthographic clues  i.e. indentations  for the  doc  runs because this information is removed during the tagging process. the tagged document is divided into several sentence groups based on a pre assigned value that specifies the approximate number of words in each sentence group.
we hypothesized that tagging the whole document versus isolated paragraphs should provide better named entity identification. named entities are often referred to in their full form early in a document  only to be reduced to a shorter form later on. when an isolated paragraph is presented to our system for tagging  the context information of the preceding paragraphs is not available for entity categorization  thus hindering tagging performance. the complete documents as well as the individual paragraphs were part-of-speech tagged and categorized by  !metamarker tm using cnlp's categorization rules. the quality of selected paragraphs and the system's categorization capabilities directly impact later processing such as answer finding.
1 answer finding
the answer finding process  see sections below  takes the tagged paragraphs from the paragraph finding stage  for  doc  as well as  par  runs  and identifies different paragraph windows within each paragraph. a weighting scheme was used to identify the most promising paragraph window for each paragraph. these paragraph windows were then used to find answer candidates based on the question focus or additional clue words. all answer candidates were weighted and the top 1  main task  or top n  list task  were selected. the answer finding process expanded answer finding strategies without making major changes to the weighting strategy.
1.1 paragraph-window identification and selection
paragraph windows were selected by examining each occurrence of a question keyword in a paragraph. each occurrence of a keyword in relation to the other question keywords was considered to be a paragraph window. a keyword that occurred multiple times thus resulted in multiple paragraph windows  one for each occurrence. a weight for each window was determined by the position of the keywords in the window and the distance between them. an alternative weighting formula was used for single-word questions. the window with the highest score was selected to represent that paragraph. the process was repeated for all 1 paragraphs resulting in an ordered list of paragraph windows - all potentially containing the answer to the question.
1.1 answer candidate identification
this year we focused on expanding the answer candidate identification ability of the system by changing the answer finding strategies and adjusting our weighting schemes based on the trec1 question set.
answer candidate identification involves three separate strategies. two strategies are based on question focus  and the third strategy  based on keywords  is used when the question focus is not found or when the first two strategies fail to identify potential answers. the two question focus strategies include search for a specific lexical category in the case of standard question types and search for a specific answer pattern in the case of special question types  see section 1 . which strategy is initially employed for a particular question is based on the value found in the question focus element in the question focus line. if the question focus value matches one of the special question types  then the specific answer pattern strategy is used. if the question focus has a value of  unknown   the third strategy involving keywords is invoked as a
fallback. for all other values of the question focus element  the specific lexical category strategy is employed. for a discussion of the specific lexical category strategy and the keyword strategy see our trec 1 paper.  for each special question type  acronym  counterpart  definition  famous  standfor  synonym  why   one or more answer patterns have been identified and defined in the answer candidate identification routine.
1.1 answer-candidate scoring and answer selection
the system used a weighting scheme to assign a weight to each answer candidate. although we intended to change the weighting scheme to accommodate the new answer finding strategies we ran out of time. the weight was based on the keywords  presence  order  and distance   whether the answer candidate matched the question focus  and punctuation near the answer candidate.
this resulted in a pool of at least 1 candidates for each question. a new unique-answer-identifier module removed duplicate answers from the answer-candidate list. the top 1 highest scoring answer candidates were selected as the final answers for each question for the main task. the required number of answers  identified during question processing  determined the number of answers for the list task questions. the answer strings were formatted according to nist specifications.
1. results
we submitted four runs for the trec1 qa track: two runs for the main task and two runs for the list task. each run name can be parsed into four components: 1  organization name  1  trec  1  tagging approach  see section 1   and 1  task.1
1 main task results
averages over 1 questions1  strict evaluation :sut1docmtsut1parmtmean reciprocal rank11questions with no correct answer found1  1 % 1  1 % questions with rank above the median1  1 % 1  1 % questions with rank on the median1  1 % 1  1 % questions with rank below the median.1  1 % 1  1 % correctly answered nil questions1  out of 1 1  out of 1 table 1. question answering results for the main task.
the evaluation measure for the main task  see table 1  is the mean reciprocal answer rank. for each question  a reciprocal answer rank is determined by evaluating the top five ranked answers starting with one. the reciprocal answer rank is the reciprocal of the rank of the first correct answer. if there is no
                                                          
1
  su = syracuse university  t1 = trec1  doc = tag entire document  /  par = tag individual paragraphs  mt = main task / lt = list task
1
  the initial question set of 1 questions was reduced to 1 questions after 1 questions were discarded by the national institute for standards and technology.
correct answer among the top five  the reciprocal rank is zero. since there are only five possible ranks  the mean reciprocal answer ranks can be 1  1  1  1  1  or 1. the mean reciprocal answer ranks for all the questions are summed together and divided by the total number of questions to get the mean reciprocal rank for each system run.
1 list task results
averages over 1 questionssut1docltsut1parltaverage accuracy11questions with no correct answer found          1  1 % 1  1 % questions above the median1  1 % 1  1 % questions on the median1  1 %1  1 % questions below the median1  1 % 1  1 % table 1. question answering results for the list task.
the evaluation measure for the list task  see table 1  is average accuracy. for each question accuracy is determined by the number of distinct correct answers over the target number of instances to retrieve. accuracy for all the questions is summed together and divided by the total number of questions to get the average accuracy.
1. analysis
the main task analysis examines:  1  retrieval performance of first stage retrieval based on the ranked list provided by nist   1  the language-to-logic module   1  question focus assignment   1  query expansion  and  1  the difference between the tagged document and tagged paragraph run performance. the list task analysis  1  examines list task performance  instance assignment  and the difference between the tagged document and tagged paragraph run performance.
1 first stage retrieval
as mentioned previously  we used the ranked document list as provided by nist for first stage retrieval. the retrieved lists were created using the prise system . for trec1 nist used the smart  information retrieval system  see table 1 .
top 1 resultstrec1trec1questions without any retrieved documents1questions without any relevant retrieved documents1questions for which there are no relevance judgments1questions with relevant retrieved documents1total number of questions1total number of documents retrieved11number of known relevant documents11total number of relevant documents retrieved11average precision11table 1. first stage retrieval performance.
compared to last year's retrieval results  both the number of known relevant documents as well as the average number of retrieved relevant documents for each question decreased. the trec1 retrieval results might have increased the difficulty of finding correct answers.
1 question representation
a logical representation of the question is created in the question processing stage  see section 1 . the question representation analysis of this year is based on the main task tagged  par  run  sut1parmt . we noticed that there were much more short questions this year than the previous two years. even after query expansion  our system still produced 1  1 %  single word queries and 1  1 %  two-word queries. many of these questions are  what/who is/are/was/were  questions which asked for a definition of a person or a thing. short queries  although represented correctly  may lead to failure in answer finding because the current weighting strategy has not been adapted to them. after excluding short queries  1  1 %  questions had various representation problems. the major query representation problems include keyword selection problems; part-of-speech errors; and misplaced wildcards  see table 1 .
problem countproblems with description1
1
1keyword selection problems
content words such as numbers were erroneously filtered out or truncated  or inappropriate words were selected
part-of-speech tagging errors
wrong tags led to incorrect morphological processing and query expansion error misplaced wildcards
wildcards placed in the wrong place of single words created bad stemstable 1. question representation problems.
compared with the query representation of last year  the system has improved the part-of-speech tagging  but did worse on keyword selection. some important numbers  such as the number in question  what city has the zip code of 1    were filtered out by the system  which had a negative impact on answer finding.
our conclusion of last year held true - query representation problems only accounted for part of the failure of answer finding. the  par  run contained 1 questions that did find the correct answer: 1  1 %  were short queries  and 1  1 %  had various query representation problems. the procedure we developed for answer candidate identification helped finding answers for short queries. however  the system did not find the correct answers for most of the questions even when the query representations were correct. further analysis is needed to identify why this is the case.
1 question focus
as described in section 1  we determined the question focus based on special question patterns and lexical answer clues. the question focus analysis is based on the main task  par  run  sut1parmt . out of 1 answerable questions  our system determined a question focus for 1  1%  of the questions  more than 1 percent better than trec-1  see table 1 .   our efforts to improve focus recognition aided in this increase.  out of these 1 questions  1 questions  1 %  had a correct focus  and 1 questions  1 %  had an incorrect focus. not only did we find a question focus for a greater percentage of questions this year  we also found the correct focus for a greater percentage of questions as well.  for 1  1 %  questions  our system could not determine a focus.
correct question focusincorrect question focusno determinable question focusrank 1     1 % 1     1 % 1       1 % rank 1     1 % 1     1 % 1       1 % rank 1     1 % 1     1 % 1       1 % rank 1     1 % 1     1 % 1       1 % rank 1     1 % 1     1 % 1       1 % rank 1   1 % 1   1 % 1   1 % total11table 1. answer rank distribution of question focus status.
an analysis of the special question types  see table 1  shows that some of the special question routines  definition  standfor  aided in finding the answer.  our ability to find the answers for definition type questions in particular is improved over last year.  but since the majority of special question types still failed to find a correct answer  more work is needed.
acronymdefinitionstandforsynonymwhyrank 11  1 % 1  1 % rank 11    1 % rank 11    1 % rank 11    1 % rank 11    1 % rank 1  1 % 1   1 % 1  1 % 1  1 % 1  1 % total111table 1. analysis of special question types1
an analysis of lexical answer clues  see table 1  shows that having the correct lexical answer clue aids in finding the correct question focus.
correct question focusincorrect question focusno determinable question focuscorrect lexical answer
clue1    1 % 1     1 % 1    1 % incorrect lexical
answer clue1      1 % 1     1 % 1      1 % total = 11table 1. lexical answer clue vs. question focus
in summary  our efforts to improve focus recognition led to a greater percentage of both identified question focus and correctly identified question focus.  having a question focus is clearly important for finding the answer  as 1 %  1  of the questions with no determinable focus failed to find an answer.  finding the correct lexical answer clues aids in finding the correct question focus.  special question processing helps  but needs improvement.
since the majority of the questions with a correct focus  1 = 1 %  did not retrieve an answer  we need to examine this finding in more detail.
1 effects of query expansion
as discussed in section 1  we used wordnet 1 to expand nouns and verbs in the questions this year. experiments using the trec1 questions showed that the expansion helped find more relevant paragraphs  but whether it helped in locating the final answer within those paragraphs was not investigated. query terms added from wordnet were found in 1 out of 1  1 %  questions with correct answers in our paragraph run sut1parmt.
query expansion had an additional  positive  impact. it actually provided correct answers for some short queries. for the question  what does the acronym nasa stand for     the phrase  national aeronautics and space administration   was added to the l1l representation. this feature has been used in our procedure for identifying answer candidates for some question types.
1 document tagging versus paragraph tagging
contrary to our expectation  the  doc  run  see section 1  did not achieve better performance  but did worse than the  par  run  see table 1 . this held true for both the main task and the list task. following is the comparison of the two runs for main task  see table 1 .
runid# of correct answerrank 1rank 1rank 1rank 1rank 1document tagging
 sut1docmt 111paragraph tagging
 sut1parmt 111table 1. comparison of correct answers found by document run and paragraph run.
we hypothesized that the low performance of the document run might be caused by a lack of system testing due to time constraints. the analysis provided a good opportunity to find system bugs as well as evaluate our approach. we noticed that in most cases where the two runs got the right answers at the same rank  they found the answers in different documents or different paragraphs. a careful examination of the results for the first 1 questions  questions 1 to 1  demonstrated that the following sources contributed to the poor performance of the  doc  run and the difference between the two runs:
1. a bug in the paragraph finding program truncated some documents when they were split into paragraphs  see section 1 . the impact of this bug was minor.
1. the keyword weighting strategy used for paragraph finding inadvertently differed slightly between runs  which led to different scores even when the same answer strings were found. the influence of this difference was minor because it did not cause a change in rank.
1. the sentence alignment procedure truncated part of the texts for some documents. the word alignment procedure occasionally failed to record some of the keywords in the paragraph window  which threw out some paragraphs with the correct answers and dramatically changed the weighting score for some answer candidates. the alignment problems were the major cause of the low performance of the document run and the difference between the two runs.
1. the size of the paragraphs also played a role in making the two runs different. in the  par  run  we identified paragraphs according to text indentation while the  doc  run uses a predefined value   1 bytes  to group sentences into paragraphs. normally the sentence groups are longer than the natural paragraphs. the difference in length changed the position of paragraph windows and led to different scores for the same candidates.
after fixing the bugs and adjusting the alignment procedures  sources 1 and 1   we ran the  doc  run again and achieved comparable results between the two runs. for the first 1 questions  both runs found correct answers for 1 questions out of which 1 were identical.
we also compared the tagging and categorization between complete documents and individual paragraphs. no difference between the two was found in this analysis. it might be that the trec1 questions did not bring out the need for context information in tagging. this issue will need further investigation. ultimately we need to decide between these two approaches.
1 list task evaluation
both list task runs  sut1parlt  and sut1doclt  are based on the same question processing output. the list task analysis examines the performance of the answer instance identification as well as the reasons for the large performance difference between main and list tasks.
a special feature was added to our question processing module this year to handle the extraction of the number of desired instances from the list questions. analysis revealed that for 1  1%  of the questions  the number of instances was determined correctly. for three questions the program could not determine the correct number of instances so it defaulted to 1 instances  enough instances to make up a list . out of the 1 questions that provided the right number of instances none of the questions managed to get all of the desired answers correct.
the system seemed to perform better on the list task than on the main task  see tables 1 and 1 . for the sut1parlt run only 1% of the questions could not be answered versus 1% in the main task counterpart run  sut1parmt . in observing the questions themselves it appears that the list task questions are more straightforward compared to the more complicated main task questions where 1 questions required more advanced linguistic pattern searches. the fact that the questions seem to be easier is reflected in the performance of the focus assignment module for the list task. out of 1 list questions  1 questions had a correct focus assignment  1 questions had a wrong focus assignment  and for 1 questions the system correctly indicated that the focus was unknown. for the list task 1 % of the questions had a correct focus assignment versus 1% questions in the main task. two out of the three questions with the wrong focus assignment were of identical form  name n people who/from ...  and both indicated the answer should be a number instead of a person. the error is due to a clue in the focus program dealing with how many people questions.
1. conclusions and future research
the expansion of our question processing module clearly improved the accuracy of our focus assignment although there are still a large number of questions for which the system did not provide the correct answer. it appears that tagging the entire document before splitting it into paragraphs versus splitting it into paragraphs before tagging does not make a lot of difference. the decision on what tagging approach to take will depend on processing speed.
after the trec1 experiments it is clear that a lot of work remains to be done. our analysis shows that a one-size-fits-all approach to answer-finding does not work well. the system needs alternative answerfinding strategies for different question types and the means to differentiate between these question types. these different strategies also imply more advanced weighting schemes than are currently implemented. our work on answer confidence level assignment needs to be completed and refined. the confidence level work will also include the ability to decide whether an answer can indeed by provided. in addition the system also needs to be adapted to deal with the context specific task  the third trec q&a track task  where each answer provides contextual information to help answering the next  related  question.
