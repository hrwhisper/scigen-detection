to improve performance in text categorization  it is important to extract distinctive features for each class. this paper proposes topic difference factor analysis  tdfa  as a method to extract projection axes that reflect topic differences between two document sets. suppose all sentence vectors that compose each document are projected onto projection axes. tdfa obtains the axes that maximize the ratio between the document sets as to the sum of squared projections by solving a generalized eigenvalue problem. the axes are called topic difference factors  tdf's . by applying tdfa to the document set that belongs to a given class and a set of documents that is misclassified as belonging to that class by an existent classifier  we can obtain features that take large values in the given class but small ones in other classes  as well as features that take large values in other classes but small ones in the given class. a classifier was constructed applying the above features to complement the knn classifier. as the results  the micro-averaged f1 measure for reuters-1 improved from 1 to 1%. 
categories & subject descriptors: h.1.m  miscellaneous . general terms: algorithms  experimentation  performance. 
1. introduction 
recently text categorization research has become more and more popular. according to yang's comparative study of classifiers   k nearest neighbors  knn  1 1   support vector machines  svm  1   and linear least squares fit  llsf  outperform other methods proposed so far. adaboost also achieves high performance . however  because these methods have been studied in depth  increasing performance by improving individual methods seems difficult. a new approach is necessary. 
every classifier has information about document classes  and compares it with an input document. we call this information the class model. the class model is the average vector of documents belonging to the same class in rocchio's model   the set of documents belonging to the same class in knn  and a set of simple hypotheses in adaboost. the class model needs to be precise to enable high performance. however  many classifiers  even precise class models  do not consider class-model overlapping. in most classifiers the class model of a certain class shares information with other classes. if an overlap exits in class models  unnecessary likelihood can be generated for classes an input document does not belong to. therefore  classmodel overlapping may cause misclassification. to prevent misclassification  a class model should be described using each class's distinctive information so that class model overlapping is reduced. 
this paper focuses on this issue. to clarify each class's distinctive information  it is essential to extract features that reflect differences between a document set of a given class and a document set that belongs to other classes. 
first  this paper proposes a method to extract projection axes that reflect topic differences between two document sets. suppose that each document is represented as a set of sentence vectors  whose components represent frequency related values of corresponding terms  and that all sentence vectors are projected onto projection axes. the projection axes are obtained so that the ratio between the document sets as to the sum of the squared projections is maximized. by projecting the sentence vectors onto the projection axes  we can obtain the features that take large values in one document set but small ones in the other. here  a feature denotes a linear combination of terms. since the projection axis reflects the topic difference between the document sets  

 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  august 1  1  tampere  finland. 
copyright 1 acm 1-1/1...$1. 
we call it a topic difference factor  tdf   and the method topic-difference-factor analysis  tdfa . by applying tdfa to text categorization  we can obtain features that take large values in a given class but small ones in other classes  and features take small values in the given class but large ones in other classes. this paper proposes a classification scheme that applies the features in a complementary classifier to an existent one. the proposed complementary classifier corrects the likelihood scores of an input document that are assigned to each class by an existent classifier. 
the rest of this paper is organized as follows. section 1 describes how tdf's are obtained and how tdfa is interpreted. what tdf's are obtained is also illustrated for simple examples of sentence vectors. section 1 shows how tdf's are obtained for the complementary classifier and how likelihood scores are corrected. section 1 presents experimental results using reuters-1. they show that the micro averaged f1 measure was significantly improved. section 1 summarizes the paper. 
1. topic difference factor  tdf   1 approach 
we consider two document sets  d={d1 .. dm} and t={t1 ..  tn }. let the k-th sentence vector in document dm and tn be dmk  k=1 .. kd m   and tnk k=1 .. kt n    respectively. here  let ¦Á be the projection axis to be obtained. we assume ||¦Á||=1. let pd and pt be the sums of squared projections of all sentence vectors in document sets d and t onto ¦Á  respectively. they are obtained as follows. that is  
	pd = ¡Æmm=1¡Ækk=d1 m  dmkt¦Á 1 =¦Át sd¦Á  	 1  
	pt = ¡Ænn=1¡Ækk=t1 n  tnkt¦Á 1 =¦Át st¦Á   	 1  
where t denotes transpose  and sd and st are the matrices defined by 
	sd = ¡Æmm=1¡Ækk=d1 m dmkdmkt    	 1  
	st = ¡Ænn=1¡Ækk=t1 n  tnktnkt . 	 	 1  
we call these matrices square sum matrices. the matrix divided by the number of sentences is the so-called autocorrelation matrix. let j ¦Á  be a criterion function which represents how strongly the differences between the document sets d and t are reflected on ¦Á. we define j ¦Á  
as  
	j ¦Á  = pptd = ¦Á¦Áttsstd¦Á¦Á . 	 	 1  
since the sum of squared projections of all sentence vectors onto the ¦Á that maximizes j ¦Á  should be large for document set d and small for t  the ¦Á reflects information that appears frequently in document set d but rarely in t. in other words  it reflects distinctive information that should exist in document set d. therefore  we call such an ¦Á the positive topic difference factor  p-tdf  of document set d. criterion function j ¦Á  has the same form as that in linear discriminant analysis 1 . as in linear discriminant analysis  plural ¦Á 's are obtained as the eigenvectors of the following generalized eigenvalue problem. 
	sd¦Á= ¦Ëst¦Á. 	 	 	 1  
 alternatively  ¦Á 's can be expressed as the eigenvectors of st-1 sd. 
let ¦Â be another projection axis to be obtained. when j ¦Â  is defined as  
	j ¦Â  = pptd = ¦Â¦Âtt sstd¦Â¦Â   	 	 1  
information that appears frequently in document set t but rarely in d is reflected on the ¦Â that maximizes j ¦Â . the ¦Â  becomes the p-tdf of document set t. we also call the ¦Â the negative topic difference factor  n-tdf  of document set d. in this case  plural ¦Â 's are obtained as the eigenvectors of the following generalized eigenvalue problem. 
	st ¦Â =¦Ësd ¦Â . 	 	 	 1  
in eqs. 1  and  1   dmk and tnk might be replaced by their normalized forms  d mk = dmk / dmk and t nk = tnk / tnk   respectively  to prevent influence from sentence-length variance. in this case  criterion j ¦Á  or j ¦Â  represents the ratio between the document sets as to the sum of the squared cosine similarities of ¦Á or ¦Â with all sentence vectors. 
1 interpretation 
we consider the case of eq. 1 . let ¦Ëi and ¦Ái be the i-th eigenvalue and eigenvector  respectively. since ¦Ëi=¦Áitsd¦Ái/¦Áitst¦Ái  ¦Ëi represents the value of j ¦Ái . it is known that the eigenvectors of eq. 1  can be acquired in a two-stage procedure  and that the matrices sd and st are diagonalized simultaneously.  
let ¦Ñj and ¦Õj be the j-th eigenvalue and eigenvector of st  respectively. the sum of the squared projections of all sentence vectors in document set t onto ¦Õ1 is greater than that onto any other vector  and ¦Ë1 represents the sum of the squared projections onto ¦Õ1. the ¦Õ1 gives the largest sum of the squared projections of all sentence vectors under the constraint that ¦Õ1 is orthogonal to ¦Õ1  and ¦Ë1 represents the sum of the squared projections onto ¦Õ1. the same relationship holds for the higher order eigenvectors. suppose a sentence vector dmk in document set d be mapped into vector ymk in space y. the j-th component of ymk is given by 
	ymkj =¦Õjtdmk / ¦Ñj .  	 	 1  
the eigenvectors of the squared sum matrix of document set d in space y correspond to ¦Á 's in the original space. that is  the principal components of document set d in space y correspond to ¦Á 's. 
on mapping to space y  the inner product between ¦Õj and dmk is divided by the square root of ¦Ñj as shown in eq. 1 . this means that document set d in space y is compressed in the directions of the ¦Õj's with large eigenvalues  and expanded in those with small eigenvalues. for the principal components of the scaled document set d in space y  consequently  the directions of the principal components of document set t become less dominant  and the sum of squared projections of all sentence vectors on ¦Á becomes large for document set d and small for t. if dmk describes the content that document set t does not include but d does  
¡Æil=1 dmkt¦Ái  1 takes a large value. the value of l should be determined experimentally.  
1 regularization 
we consider the case of eq. 1  again. for eigenvectors to be obtained in eq. 1   matrix st must be regular. when the number of sample sentence vectors is less than the dimension of the vectors  or when a certain pair of terms always co-occurs  matrix st is actually not regular. in these cases  matrix st must be regularized. regularization can be achieved by biasing diagonal components as follows 
 1 . 
	s t = st +¦Ò1i   	 	 	 1  
where ¦Ò1 and i are a bias parameter and the identity matrix  respectively. 
the meaning of eq.  1  is as follows. suppose that every term has a vector with the same dimension as the sentence vector and that value ¦Ò is given only to the component that corresponds to each term. let ul be the l-th term vector. since ¡Æl ulul t =¦Ò1i  adding ¦Ò 1 to all diagonal components is equivalent to adding all term vectors to document set t. as mentioned above  the eigenvectors of st are the projection axes that maximize the sum of the squared projections of all sentence vectors in document set t  and the eigenvalues are the sums. since the sum of the squared projections of all term vectors onto any normalized vector is always ¦Ò 1  in this case  the eigenvectors of st do not change by adding all term vectors to document set t. since the sum of squared projections of all term vectors is added to that of all sentence vectors  however  the eigenvalues of st get biased by ¦Ò 1 and the criterion function also changes to 
	j ¦Á  = pd / pt +¦Ò1  . 	 	 1  
higher order eigenvalues of st usually take very small values  nearly 1. very small eigenvalues are easily affected by noises in document set t. since the scaling factors for document set d in space y become very large along the directions of higher order eigenvectors  the principal component of document set d easily suffer from noises in t. thanks to eigenvalue biasing  those scaling factors become smaller and noise influence can be reduced. 
1 example 
this section illustrates what tdf's are obtained for simple examples. suppose that four 1-dimensional sentence vectors are given in document set d and t as shown in table 1. the n-th component in the vectors is supposed to represent the existence of term n. let d-m or t-m represent the m-th sentence vector in document set d or t. the differences and similarities between the document sets d and t are as follows. 
  term 1 does not occur in document set t  but it occurs in d-1. 
  in document set d  term 1 co-occurs with term 1 in d-1  and term 1 with term 1 in d-1. 
  in document set t  term 1 co-occurs with term 1 in t-1 and term 1 with term 1 in t-1. 
  in both document sets  term 1 co-occurs with term 1 in d-1 and t-1  and term 1 with term 1 in d-1 and t-1. 
the tdf's were obtained by setting ¦Ò 1 at 1. table 1 shows eigenvalue ¦Ën  n=1 ..1  and eigenvector ¦Án= ¦Án1 ..  ¦Án1   n=1 ..1  of eq. 1  as p-tdf's of document set d. table 1 shows eigenvalues ¦Ìn  n=1 ..1  and eigenvector ¦Ân= ¦Ân1 ..  ¦Ân1   n=1 ..1  of eq. 1  as n-tdf's of document set d. in table 1 and 1  ¦Án or ¦Ân is normalized so that ¦Ántsd¦Án=¦Ën or ¦Ântst¦Ân=¦Ìn. consequently  the dynamic range of the squared inner products between sentence vectors and tdf get larger for lower order pdf. this normalization means weighting according to the degree of topic differences reflected on each eigenvector. this normalization was applied in the experiments. table 1 shows the projections of each sentence vector onto both the first and the second eigenvector.  
from these results the following can be noted. 
 1  for the first eigenvector  ¦Á1  shown in table 1  both ¦Á1 and ¦Á1 take negative values  and both ¦Á1 and ¦Á1 positive values. this shows that ¦Á1 reflects the term cooccurrence between 1 and 1 and the one between 1 and 1 in document set d   so the projections of d-1 and d-1 onto ¦Á1 have large absolute values with different signs  and the projections of any other sentence vectors take zero  as shown in table 1 . 
¡¡¡¡table 1. sentence vectors comprising document set d and t .
#dt111111table 1. p-tdf's of document set d.
n¦Ën¦Án1¦Án1¦Án1¦Án1¦Án11-111-111.1.1.1-1-111.1-1-1-1-111.1-1-11111.1.1-11-11table 1. n-tdf's of document set d.
n¦Ìn¦Ân1¦Ân1¦Ân1¦Ân1¦Ân111-11-111.1-1-11111.1.1.1-1-1-11.1-111-111.1.1-1-111table 1. projections of each document.
text¦Á1¦Á1¦Â1¦Â1d-1.1.1.1-1d-1.1-111d-1.1.1.1-1d-1-111-1t-1.1.1.1-1t-1.1-111t-1.1.1.1-1t-1.1.1-1-1 1  for the second eigenvector ¦Á1 shown in table 1  only ¦Á1 takes a large value. this shows that ¦Á1 reflects the occurrence of term 1 in document set d   so only d-1 takes a large projection value as shown in table 1. 
 1  similarly  the term co-occurrence between 1 and 1 and that between 1 and 1 in document set t are reflected on ¦Â1 shown in table 1. only projections of t-1 and t-1 take non-zero values. 
 1  the difference between d-1 and t-1 is reflected on ¦Â1  as shown in table 1. the projection of t-1 takes a large absolute value. 
 1  in table 1 and 1  the eigenvectors of the 1rd order or higher are not effective as tdf's because the eigenvalues of those orders are small. these observations confirm that not only the term occurrence difference between the document sets but also the term co-occurrence differences are reflected on tdf's. in this paper  a document is represented as a set of sentence vectors so that the term co-occurrence difference is reflected precisely on the tdf's.  
1. application to text 
categorization 
1 approach 
as described in section 1  this paper does not aim at developing a classifier that only uses tdf's  but at a classification scheme that combines an existent classifier with its complementary one which uses tdf's. the reasons are as follows. 
 1  we define a set of documents belonging to class l as document set d and a set of documents belonging to classes other than l as document set t. if we construct a classifier that only uses tdf's  we have to apply tdfa to the document sets. in this case  document set t must include all documents that are not included in d. as a result the number of documents in document set t would probably be much larger than that in d  and document set t would probably include many documents irrelevant to class l. in such a case  it is doubtful whether subtle differences between class l and documents easily confused with those in class l can be reflected exactly on the tdf's. to reflect such subtleties   only the documents that are easily confused should be included in document set t. those documents can be obtained by using the results of an existent classifier. 
 1  it is considered easier to achieve high performance by combining tdf-based classifier with an existent high performance classifier than by constructing a classifier using tdf's only. 
based on these considerations  a classification scheme was constructed such that the complementary classifier corrects likelihood scores obtained by an existent classifier. the existent classifier acts as the main classifier. if an input document has features that should appear in a given class  a gain is added to the likelihood score of the class in the complementary classifier  and if an input document has features that should not appear in a given class  a penalty is imposed. 
1 complementary classifier 
the tdf's of class l is obtained as follows. first  all training documents are classified by the main classifier and the likelihood scores of each class are obtained. let ¦Ã be a threshold given for each class. the training documents that have a likelihood score of class l larger than ¦Ã are selected. among the selected documents  the documents belonging to class l are added to document set d and the documents belonging to other classes to t. every document in document set t is the one that has been misclassified or nearly misclassified as belonging to class l. we call such a document a competing document of class l. document sets d and t are thus defined and used for solving eq. 1  and  1 . the p-tdf's of class l are given by eigenvector {¦Án} n=1 .. lg  of eq. 1  and n-tdf's by eigenvector {¦Ân} n=1 .. lp  of eq. 1 .  
let g x  and p x  be a gain and a penalty of input document x for class l  respectively. suppose document x is composed of a sentence vector set {x1 ..  xk}. g x  and p x  can be obtained as follows. 
g x  = ¡Æil=g1¡Ækk=1 xkt¦Ái 1     1  p x   = ¡Æil=p1¡Ækk=1 xkt¦Âi 1     1  
where lg and lp are parameters whose values should optimally be determined by experiments. let lik x  be the likelihood score of document x for class l in the main classifier and likc x  the corrected likelihood score. likc x  can be given as follows. 
¡¡¡¡likc x = lik x  + a g x  - b p x     1  where a and b are positive parameters  which should be determined by experiments. likc x  is computed for documents that have lik x  larger than ¦Ã. a document with lik x  smaller than or equal to ¦Ã is not judged as belonging to class l under any conditions. if likc x  exceeds another threshold ¦Ä  the input document x is judged as belonging to class l. 
when eqs. 1  or  1  are used as a gain or a penalty  g x  or p x  tend to take a large value for long documents. to reduce the influence of document-length variance  normalizing g x  or p x  by the number of sentences in document x might be effective. when sentence vectors are normalized to reduce influence from sentence-length variance in the tdf calculation  the normalized sentence vectors in document x should be used in eqs. 1  and  1 .  
1. experiments 
1 experimental conditions 
the experimental conditions in this paper follow yang's experiments  and reuters-1 was used. according to modapte split  the documents were selected which belong to classes that have at least one document in the training set and the test set. this resulted in 1 classes  a training set of 1 documents  and a test set of 1 documents.  
for the training and the test data  sentence segmentation  lemmatization  replacement of uppercases by lowercases  replacement of all digits by  1   removal of  -    /  and all punctuation  stop-word removal  and term selection were conducted as a preprocess. sentence segmentation was needed since a document is represented as a set of sentence vectors in this paper. ordinary documents were segmented by finding periods. in the corpus  however  there are many table-like documents where terms are spaced by the same interval without any punctuation. for such documents one line was regarded as one sentence. for the term selection  1 terms were selected based on ¦Ö1 statistics.  
 to represent sentences as vectors  each term was weighted based on tf-idf. the weight of the i-th term  wi  was determined as follows.  
	wi =  1+log fi log nd / ni     	 1  
where fi  ni  and nd represent the frequency of an i-th term in a given sentence  the number of documents including an i-th term and the total number of documents  respectively.  
1 experimental methods 
as the main classifier  knn classifier  was used  which enables high performance by using a simple algorithm. for a given input document  the classifier calculates similarity scores with each training document and then  finds k nearest neighbors out of the training documents. as the similarity measure  the cosine similarity was adopted  as it is commonly done. the value of k was set at 1 in accordance with yang's experiment . the similarity scores with the documents belonging to the same class out of all k documents are summed up as the likelihood score of that class. thus  the likelihood scores are obtained for the classes that the documents of k nearest neighbors belong to. the input document is judged as belonging to a class if the likelihood score of that class is larger than a threshold. the threshold is determined in advance for each class so that it maximizes the evaluation score. 
to evaluate the proposed method  the standard recall  precision  and f1 measure were used. recall  r  is defined as the ratio of correct assignments by the classifier divided   table 1. performance comparison for reuters-1. 
classifierrecallprecisionf1svm111knn111lsf111nnet111nb111knn this paper 111by the total number of correct assignments. precision  p  is defined as the ratio of correct assignments by the classifier divided by the total number of classifier's assignments. the f1 measure is defined as f1rp/ r+p . table 1 shows the micro-averaged evaluation scores reported by yang  for several well-known methods including the knn classifier and those of the knn classifier in this paper. in table 1  nnet and nb stand for neural network and naive bayes  respectively. the f1 measure of the knn result in this paper is 1%  which is inferior to that in yang's report by 1%. 
when lg and lp in eqs. 1  and  1   and a and b in eq. 1  were determined  the following problem occurred. although these parameters should be determined using training data  tdf's obtained by using training data were tuned to the training data. if the parameters are determined by using such tdf's  the parameters would be doubly tuned to the training data. evaluating test data using the parameters obtained in such a way is obviously inappropriate. to remedy this problem  cross validation was also conducted. the test data were divided into n blocks  and the parameters were determined by using n-1 blocks as the secondary training data and the remaining block was used as true test data. after running experiments n times by rotating the blocks  the results for each true test data were summed up as the results of total test data. since the summed up results were not tuned to the test data  the results can be compared with other method's results. 
the parameters were obtained as follows. the a and b in eq. 1  were obtained by applying linear discriminant analysis. threshold ¦Ä was obtained for each class to maximize the f1 measure. the linear discriminant analysis and threshold determination were conducted for every combination of lg with lp  each of which was restricted to be lower than 1. the lg and lp were determined by selecting the combination which gave the best results. the linear discriminant analysis was conducted between a document set and a competing document set of each class. the competing documents were selected by using threshold ¦Ã that was used in the tdf-calculation stage. for the analysis  a 1-dimensional vector  whose components were given by lik x   g x  and p x   was generated for each document. as a result of the linear discriminant analysis  the weights of lik x   g x   and p x   which optimally separate a document set from a competing document set of each class  were obtained. by dividing the weights of g x  and p x  by that of lik x   a and b in eq. 1  were determined.  
furthermore  the experiments confirmed that combining sentence vector normalization with normalizing eqs. 1  and  1  by the number of sentences in an input document was effective. the experimental results shown in the next paragraph are the best ones obtained by varying the parameters including ¦Ã and ¦Ò1.  

figure 1. probability density distribution of g x  for correctly classified and misclassified documents by knn as belonging to class  earn .

figure 1. probability density distribution of p x  for correctly classified and misclassified documents by knn as belonging to class  earn .
 
1 experimental results 
figure 1 shows the probability density distributions of gain g x  for test documents correctly classified and misclassified by knn as belonging to class  earn . in fig.1  the horizontal axis shows z=g x  and the vertical axis the probability density prob zk   which is derived as follows. 
	prob zk   =n zk  /¡Æk n zk    	 	 1  
where n zk  represents the number of documents whose g x  take value zk. similarly  fig. 1 shows the probability density distributions of penalty p x . the number of documents correctly classified as belonging to class  earn  is 1 and the number of the misclassified documents 1. both lg and lp in eqs. 1  and  1  were set at 1. out of all documents  the misclassified documents are the most difficult ones to separate from the documents belonging to class  earn . as shown in figs.1 and 1  however  the misclassified documents are well separated though slight overlaps occurred.  
the effectiveness of introducing ¦Ò1 in eq. 1  was investigated when cross validation was conducted. the ¦Ò1 was set at the average of the diagonal components of st multiplied by ¦Ó. figure 1 shows the relationship between ¦Ó and the f1 measure when a likelihood score was corrected only by g x . the n was set at 1. as the figure shows  the f1 measure peaks at ¦Ó=1. as mentioned in 1  ¦Á tends to be easily affected by noises in document set t if ¦Ò 1 is small. this is the reason why the f1 measure is small for ¦Ó 1. if ¦Ò 1 is too large  on the other hand  pt in eq. 1  is not necessarily small because ¦Ò 1 becomes dominant in the denominator. therefore  pd /pt may be small. this is the reason why the f1 measure is small for ¦Ó 1. table 1 shows the classification results for training data and test data before the likelihood correction. table 1 illustrates the classification results after the likelihood correction without cross validation. it shows the following three cases:  a  evaluate trainig data using parameters trained by training data   b  evaluate all test data using parameters trained by all test data   c  evaluate all test data using parameters trained by training data. when parameters were trained using training data  the f1 measure of training data significantly increased. the f1 measure  nevertheless  remains less than 1%. this implies that class boundaries are not clear between many classes.  the f1 measures of all test data using parameters trained by all test data  1%  is quite different from that by training data  1%. the former seems to be due to overtuning to test data  and the latter due to overtuning to training data. these f1 measures do not represent true performance for test data. 
table 1 shows the classification results  based on cross validation for n=1  1  1  and 1. table 1 shows the f1 measures when the likelihood was corrected by either only g x  or p x   and the f1 measures  precision  and recall when the likelihood was corrected by both g x  and p x . these measures are micro-averaged ones. from this table  the following can be noted.  
  likelihood correction by g x  or p x  effectively improves performance. 
  likelihood correction by g x  is more effective than that by p x . this fact shows that  in each class  it is easier to extract features that should occur than features that should not occur. this is because a competing document set of each class is composed of documents belonging to various classes and because ptdf's of each competing document set n-tdf's of each class  are not clear in comparison with p-tdf's of each class. 

¦Ó
figure 1. relationship between f1 measure and regularization parameter.
table 1. performance before liklihood correction.
evaluation datarecallprecisionf 1training data111test data111table 1. performance after liklihood correction without cross validation.
evaluation dataparameter
training dstarecallprecisionf 1atraining datatraining data111btest datatest data111ctest datatraining data111table1. performance after liklihood correction with cross validation .
likc x measuren11lik x +ag x f1.1.1.1.1lik x -bp x f1.1.1.1.1lik x +ag x   -bp x f1.1.1.1.1precision1111recall1111  the f1 measure improved from 1% obtained by knn classifier alone to 1% when the likelihood was corrected by both g x  and p x . the score is significantly better than the scores of the other classifiers shown in table 1. 
the averages of lg and lp were 1 and 1  respectively  when n was set at 1. when the dimension of sentence vectors was individually determined for each class by assigning only terms appearing in each class and its competing document set to the vector components  the processing time to obtain 1 eigenvectors of eqs. 1  or  1  for all classes was about 1 minutes using a workstation with a 1-mhz clock. thus  the computational cost of the proposed method is not high. 
1. summary 
the objective of this paper is to extract distinctive features for each document class and to apply them to text categorization. this paper proposes topic difference factor analysis as a method to extract difference factors between two document sets by obtaining projection axes which maximize the ratio between the document sets as to the sum of squared projections of all sentence vectors. tests confirmed that not only term occurrence difference between document sets but also term co-occurrence difference are reflected on the topic difference factors. by applying this method to document classification  we can obtain the features that should occur in a given class and the features that should not occur in the class. this paper proposes a classification scheme that uses the features in a complementary classifier to correct the likelihood of an input to each class of an existent classifier. by applying the knn classifier as the existent one  improved the micro- averaged f1 measure for reuters-1 from 1 to 1%.  
these experimental results prove that topic difference factors have been successfully extracted. this paper  however  has not shown what the topic difference factors are for each class. interpreting topic difference factors is an important remaining problem. tdfa has many possible applications  and the interpretation will be important for new applications. 
