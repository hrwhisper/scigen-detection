with the fast growth of semantic web  more and more rdf data and ontologies are created and widely used in web applications and enterprise information systems. it is reported that the w1c linking open data community project consists of over two billion rdf triples  which are interlinked by about three million rdf links. recently  efficient rdf data management on top of relational databases gains particular attentions from both semantic web community and database community. in this paper  we present effective and efficient semantic web data management over db1  including efficient schema and indexes design for storage  practical ontology reasoning support  and an effective sparql-to-sql translation method for rdf query. moreover  we show the performance and scalability of our system by an evaluation among well-known rdf stores and discuss future work.  
categories and subject descriptors 
e.1  data storage representations  
h.1  information storage and retrieval  
general terms 
management  performance  design  experimentation  languages 
keywords 
rdf  ontology  semantic web  sparql  triple store. 
1. introduction 
the semantic web provides a common framework that allows data to be shared and reused across applications  enterprises  and community boundaries . for this purpose  the world wide web consortium developed several recommendations: the resource description framework  rdf    the rdf schema  rdfs    the web ontology language  owl    and the sparql query language . rdf is a data model for information representation and exchange on semantic web. rdfs and owl are used to publish and share ontologies which are explicit and common descriptions of domain knowledge and provide support for efficient knowledge management. owl has three increasingly-expressive sub-languages: owl-lite  owl-dl  and owl-full. universal resources identifiers  uris  and ontologies are central to semantic web. the former is to uniquely identify resources and the latter is to make the meanings of the data explicit by linking them to sets of domain concepts and properties and making inference over them. description logics  dl  is the logic foundation of the owl and a dl knowledgebase includes a terminology box  tbox  and an assertion box  abox . the tbox introduces the terminology  i.e.  the vocabulary of an application domain  while the abox contains assertions about named individuals  i.e.  instances  in terms of this vocabulary. correspondingly  dl reasoning includes tbox reasoning  i.e.  reasoning with concepts  and abox reasoning  i.e.  reasoning with individuals . ontologies define concepts  classes  and relations  properties  in a domain and sometimes are called the tbox in knowledge representation. from the perspective of data management  ontologies could be considered as a data model  similar to relational schema and xml schema  and a large amount of rdf data can be regarded as the instances of ontologies  corresponding to abox of a dl knowledgebase . compared with relational and xml data management  semantic web data management has following unique characteristics in terms of data model and query processing.  
* rdf is the data model of semantic web and in essence a directed labeled graph model. an rdf graph can be represented by a set of triples  each describing a statement of the form  subject  property  object . so  rdf repositories are also often called rdf triple stores. syntactically  rdf model is more general and flexible than the tabular representation of relational data and the tree representation of xml data.  
* rdfs and owl are used to encode the formal semantics of rdf data  which is much easier to use in practice compared with xml schema and er model. an xml schema defines a syntax-valid xml document and has no formal semantics. an er model can capture data semantics well but it is hard for end-users to use when it is transformed into a physical database model on which user queries are evaluated. in particular  rdfs and owl ontologies enable knowledge representation and expressive reasoning over rdf data.  
* sparql provides a graph pattern matching based paradigm for flexible rdf data graphs. theoretically  graph pattern matching is more expensive than sql and xquery/xpath evaluation. subgraph isomorphism checking has been proven to a npcomplete problem in graph theory. 

permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigmod'1  june 1  1  vancouver  bc  canada. 
copyright 1 acm  1-1-1/1...$1. 
 
the above unique features of semantic web data make its storage and retrieval highly challenging. currently  more and more rdf data and ontologies are created and widely used in web applications and enterprise information systems  with the fast growth of semantic web. it is reported that the w1c linking open data community project consists of over two billion rdf triples  which are interlinked by about three million rdf links. swoogle  a wellknown semantic web search engine  currently has collected more than 1 1 rdf/owl documents in internet. therefore  efficient rdf data management over relational databases gains particular attentions from database community in recent years. many papers on semantic web data management appeared in major database conferences and oracle released the first commercial rdf store in oracle 1gr1 spatial database in 1. in this paper  we introduce high performance semantic web data management over db1  including efficient schema and indexes design for storage  practical ontology reasoning  and an effective sparql-to-sql translation method for rdf query.  
the rest of this paper is organized as follows. section 1 introduces an overview of rdf triple stores. the proposed rdf data management over db1 is presented in section 1  including schema and indexes design  as well as detailed sparql query evaluation and reasoning. section 1 reports a performance and scalability evaluation among well-known rdf stores  and discusses future work. section 1 concludes this paper. 
1. an overview of rdf triple stores 
in this section  we briefly introduce some widely-used or representative rdf repositories  summarize methods to store rdf data in relational databases  and introduce reasoning methods used by these repositories briefly. 
 
figure 1. a taxonomy to classify rdf triple stores 
figure 1 shows a classification scheme for rdf tripe stores based on their storage models. in general  rdf stores can be divided into two major categories  i.e.  native stores and database based stores. native stores are directly built on the file system  whereas database based repositories use relational or object relational databases as the backend store. representative native stores include owlim   hstar   allegrograph  and yars . owlim and 
allegrograph adopt simple triple  n-triple  files to store rdf data  which results in the extremely fast speed for load and update. it is reported that allegrograph can load rdf data at the speed of more than 1 triples per second . owlim uses b+ trees to index triples and allegrograph just sorts triples in the order of  s  p  o    p  o  s   and  o  s  p  for indexing purpose  where s  p and o indicate subject  property  and object of a triple respectively through this paper. in yars  rdf triples are represented as quads  s  p  o  and c  with an additional flag c for context. it builds 1 indices on  s  p  o  c    p  o  c    o  c  s    c  s  p    c  p  and  o  s  over the quads to cover all possible access patterns. the triple reasoning and rule entailment engine  trree  is utilized by owlim  which performs forward chaining reasoning in main memory  and inferred results are materialized for query answering. allegrograph can expose rdf data to racer  a highly optimized dl reasoner   for inference. hstar is a hierarchy store and organizes typeof triples  asserting class information of an instance  using a class hierarchy and other non-typeof triples  asserting attributes of instances or relationships among instances.  using a property hierarchy. range labeling  which assigns labels to all nodes of an xml tree such that the labels encode all ancestor-descendant relationships between the nodes  is leveraged to improve query performance. a set of rules derived from owl-lite is categorized into two groups  which are executed at load time using forward chaining and are evaluated at query time using backward chaining  respectively.  
compared with database based stores  native stores greatly reduce the load and update time. however  databases provide many query optimization features  thereby contributing positively to query response time. our evaluation  found that a simple exchange of the order of triples in a query may make the query time of native stores 1 times  or even more  slower. furthermore  native stores need to re-implement the functionality of a relational database such as transaction processing  query optimization  access control  logging and recovery. one potential advantage of database based stores is that they allow users and applications to access both ontologies and other relational data in a more seamless way at a lower level  namely database level. for instance  the oracle rdf store  translates an rdf query into a sql query which can be embedded into another sql query retrieving non-rdf data. in this way  query performance can be improved by efficiently joining rdf data with other application data using well-optimized database query engines. currently  lots of research and development efforts are made on database based stores.  
a generic rdf store mainly uses a relational triple table of three columns  s  p  o  to store all rdf statements  in addition to symbol tables for encoding uris and literals with internal unique ids. both jena and the oracle rdf store are generic rdf stores. in jena1   most of uris and literal values are stored as strings directly in the triple table. only the uris and literals longer than a configurable threshold are stored in separated tables and referenced by ids in the triple table. such a design trades storage space for time. the property table is also proposed to store property patterns separately. an n-column property table stores n-1 single-valued property statements  one column per property . this is efficient in terms of storage and access  but less flexible for ontology changes. jena1 provides by default several rule sets with different inference capabilities. these rule sets could be implemented in memory by forward chaining  backward chaining or a hybrid of forward and backward chaining. the oracle rdf store is the first commercial system for rdf data management on top of rdbms. particularly  it supports so-called rulebases and rule indexes. a rulebase is an object that contains rules which can be applied to draw inferences from rdf data. two built-in rulebases are provided  namely rdfs and rdf  a subset of rdfs . a rule index is an object containing precalculated triples that can be inferred from applying a specified set of rulebases to rdf data. materializing inferred results would definitely speed up retrieval. different from the generic rdf store  improved triple stores  such as sesame on mysql   manage different types of triples using different tables. for instance  class and property information is separated from instances  and typeof triples are isolated from other triples. the improved triple store is efficient since some self-joins on a big triple table are reduced to some joins among smaller tables. both the generic rdf store and the improved triple store make use of a fixed database schema. that is  the schema is independent of ontologies. the schemas of ontology-dependent stores  however  change with ontologies. these kinds of stores  such as dldb-owl  and sesame on 
postgresql   create a table for each class  resp. each property  in an ontology. a class table stores all instances belonging to the same class and a property table stores all triples which have the same property. for the subsumption of classes and properties  dldbowl exploits database views to capture them  whereas sesame leverages the sub-tables from object relational databases to handle them naturally. one of advantages of the ontology-dependent store is to decrease the traversal space and improve data access for queries. that is  instances of unrelated classes or properties to a query will not be accessed. an obvious drawback is the alteration of the schema  e.g.  deleting or creating tables  when ontologies change. also  this approach is not suitable for very huge ontologies having tens of thousands of classes  such as snomed ontology. too many tables may increase serious overhead to databases. 
more recently  abadi et al.  made a good analysis and evaluation on the storage model of rdf triple stores  without discussing reasoning and sparql query processing. it is reported that the vertical partitioning approach used by ontology-dependent stores mentioned above achieves the best performance  followed by the jena's property table approach. both of them are much better than generic rdf stores using a triple table of three columns. but  as introduced above  they trade performance for flexibility. more precisely  once ontologies and rdf data seriously change  the database schema may need to rebuild accordingly. furthermore  an extension of the c-store   an open source column-oriented database system  is developed to validate the vertical partitioning approach  which shows much better performance than the ontologydependent approach. the extended c-store implementation in  is promising for rdf data management. but  ontology reasoning and the complete sparql query processing need to be investigated in such an implementation. 
besides rdf triple stores classified by figure 1  some memory based repositories exist  such as sesame's memory store . this kind of lightweight rdf stores can support applications where the size of rdf data is small. in this paper  we focus on the large scale rdf data management. 
1. rdf data management over db1 
in this section  we present efficient and effective rdf data management over db1. as introduced in the first section  besides the flexibility of the rdf model  ontology reasoning support over rdf data is also a valuable feature and needs to work in a scalable way. so  we firstly present our schema design which supports efficient reasoning  and then discuss sparql query processing. 
1 reasoning and storage design 
the state-of-the-art in-memory dl reasoners  such as racer   offer good performance on expressive but small ontologies. it is proved that the data complexity1 of expressive dl reasoning is intractable  and even exptime-hard in worse case. so  current dl reasoners are not scalable for reasoning over millions of rdf triples. a recent survey  reported that around 1% of existing ontologies in the web are less expressive than owl-dl and 1% of them are simple rdfs ontologies. given that real ontologies used to organize a large amount of rdf data are not very complex and expressive  we support description logic programming  dlp   reasoning in our rdf data management system over db1. dlp is an intersection of description logic and logic programs and a practical subset of owl-dl. in order to support both expressive tbox reasoning  reasoning with concepts in ontologies  and scalable abox reasoning  reasoning with instances   we propose to combine a dl reasoner with a rule engine for rdf data reasoning. the rule engine implements dlp rules for abox reasoning  and the data complexity is o |a|1   where |a| is the number of rdf assertions in abox. the proposed reasoning scheme works in a forward-chaining manner and materializes all inferred results into databases for query answering. that is  we trade storage space for query response time. the following diagram shows our reasoning steps and dlp rules. firstly  the dl reasoner computes all subsumption relationships among classes in ontologies and two rules are used to infer all subproperties and inverse properties of each property. these inferred tbox results are materialized in the backend database  together with explicitly asserted tbox assertions  and used by the subsequent abox reasoning. then  the rule engine iteratively runs all dlp rules until no new triples can be inferred. here  we analyze the dependency among rules and categorize them into three groups  which can reduce the number of iterations for rule reasoning and thus improve efficiency. for instance  rules in group  triple-totriple  which infers new triples based on existing triples  and tbox axioms  can fire rules in group  triple-to-typeof  which infers new typeof triples based on existing triples  but converse does not hold. similarly  rules in group  triple-to-typeof  can fire rules in group  typeof-to-typeof  which infers new typeof triples based on existing typeof triples.  
reasoning steps: 
do tbox reasoning 
call a dl reasoner for tbox class subsumption inference; 
  call following two rules for reasoning on property hierarchy 
subpropertyof p q  :- subpropertyof p r   subpropertyof r q  
　　inversepropertyof q p :- inversepropertyof p q  do abox reasoning 
  do triple-to-triple reasoning until no triples can be inferred  triple x  q  y  :- triple x  p  y    subpropertyof p q  
triple x  q  y  :- triple x  p  y    inversepropertyof p q  
triple x  p  z  :- triple x  p  y    triple y  p  z   transitive p  
triple y  p  x  :- triple x  p  y    symmetric p  
  do triple-to-typeof reasoning until no triples can be inferred 
typeof x  c  :- triple x  p  y    domain p  c  
typeof y  c  :- triple x  p  y    range p  c  
typeof  x c  :- triple x  p  y   mincardinality c  p  1  
typeof x  c  :- triple x  p  y   hasvalue c  p  y  
  do typeof-to-typeof reasoning until no triples can be inferred 
typeof x  d  :- typeof x  c    subclassof c  d  
typeof x  c  :- triple x  p  y    typeof x  d   somevaluefrom c  p  d 
typeof x  d  :- triple x  p  y    typeof x  c   allvaluefrom c  p  d  
typeof x  d  :- typeof x  c    intersectionmemberof d  c  
typeof x  c  :- typeof x  d    unionmemberof d  c  
typeof x  c  :- intersectionclass c  d1  ...dn   typeof x  d1   ...   typeof x  dn  
 
the tbox precomputation by a dl reasoner ensures complete and sound owl-dl inference over classes  which can not be covered by dlp rules. for example  if we have axioms {mother 《 woman 
  haschild.person  parent 《 person     haschild.person  
woman   person}  the implicit relationship that mother is a subclass of parent cannot be derived by the dlp rules but can be found by a dl reasoner. after the complete tbox reasoning  our rule engine provides complete and sound abox inference with respect to the semantics of dlp  which covers most practical owl semantics . therefore  our method provides practical inference capability for real applications. 
the database schema is well designed to effectively support the proposed inference algorithm. generic rdf stores  such as the oracle rdf store  persist owl ontologies as a set of rdf triples and do not consider specific processing for complex class descriptions generated by class constructors  boolean combinators  various kinds of restrictions  etc. . the highlight of our database schema is that all predicates in the dlp rules have corresponding tables in the database. therefore  the rules can be easily translated into sequences of relational algebra operations. for example  a rule typeof x c :-triple x r y .typeof y d .somevaluesfrom c r d  has four terms in the rule head and body  resulting in three tables: relationshipind  typeof and somevaluesfrom. it is straightforward to execute this rule by simple sql select and join operations among these three tables. leveraging well-optimized database engines for rule inference is expected to significantly improve the efficiency. figure 1 discloses the relational storage model. 
 
figure 1. the database schema 
we categorize tables of our database schema into 1 types: atomic tables  tbox axiom tables  abox fact tables and class constructor tables. the atomic tables include: ontology  primitiveclass  property  datatype  individual  literal and resource. these tables encode each uri with an integer  the id column   which reduces the overhead caused by the long uri to a minimum. the hashcode column is used to speed up search on uris and the ontologyid column denotes which ontology an rdf resource comes from. the property table stores characteristics  symmetric  transitive  etc.  of properties as well. there are three important kinds of abox assertions involved in reasoning: typeof triples  object property triples and datatype property triples. they are stored in three different tables  namely tables typeof  relationshipind and relationshiplit. a view named relationship is constructed as an entry point to object property triples and datatype property triples. triples irrelevant to reasoning  such as those with rdfs:comment as the property  are stored in the utility table. tables subclassof  subpropertyof  domain  range  inversepropertyof  disjointclas are used to keep tbox axioms. the class constructor tables are used to store class expressions. we decompose the complex class descriptions into instantiations of owl class constructors  assign a new id to each instantiation and store it in the corresponding class constructor table. taking the axiom mother 《 woman    haschild.person as an example  we first define s1 for   haschild.person in table somevaluesfrom. then i1  standing for the intersection of woman and s1  will be defined in table intersectionclass. finally  {mother   i1  i1   mother} will be added to the subclassof table. such a design is motivated by making the semantics of complex class description explicit. in this way  all class nodes in the owl subsumption tree are materialized in database tables  and rule inference can thus be easier to implement and faster to execute via sql statements. also  a view named classes is defined to provide an overall view of both named and anonymous classes in owl ontologies. 
based on the proposed reasoning algorithm and storage design  we define the following architecture for the support of rdf data management over db1  which is extended from our previous work on owl ontology repository . the import module consists of an owl parser and two translators. the parser parses rdf/owl documents into an in-memory eodm model  emf ontology definition metamodel    and then the db translator populates all abox triples into db1. the function of the tbox translator is twofold  one is to populate all asserted tbox axioms into a dl reasoner and the other is to obtain inferred results from the dl reasoner and insert them into db1. the storage module is to store both original triples  as well as inferred assertions by the dl reasoner and the rule inference engine. but  there is a way to distinguish original assertions from inferred assertions via a specific flag. a sparql query is translated into a single sql statement to retrieve both inferred and asserted triples. 
 
figure 1. the architecture  
1 performance improvement 
we can see from our schema that the triple tables  relationshipind and relationshiplit  are used to store rdf triples. these tables are efficient in space  but its retrieval often requires some self-joins. when the triple table has a very large size  such as including hundreds of millions of records  its physical layout can have a critical effect on the performance due to expensive io costs. this becomes a bottleneck in the case of complex queries or a large number of records involved  although using some indexes. moreover  analyzing the query processing of databases may give an insight into why the triple table sometimes results in long query response time. most relational databases transform a user query into a physical query plan which represents the operations  the method of performing the operations  and the order of processing the different operations. a query optimizer of the database considers multiple physical plans and estimates their costs  and then selects a plan with the least estimated cost and passes it to the execution engine. so  the accuracy of the cost estimation seriously affects the efficiency of a query execution. usually  statistics collected from the data are used to estimate the cost of a query plan. the query optimizer may build a histogram for each column. the histogram contains information about the distribution of the corresponding column and is stored in a database catalog. apparently  if the statistical information represented by the histogram is inaccurate  the query optimizer may make a wrong selection among different physical query plans. since values of different properties are stored in the object column of the triple table  the corresponding histogram can not accurately reflect the value distribution of each property. this may affect the query plan selection and execution of a query which needs to access information in the triple table. in summary  two major factors could seriously affect the query performance of the triple table: 1  the effectiveness of its physical layout and 1  the accuracy of statistics about the value distribution. here  we propose two effective schemes to solve these problems. 
currently  most commercial databases support organization of a table using a primary clustering index. in this design  an index could be identified as the basis for data clustering. all records are arranged on the basis of their attribute values for these index key columns by which the data is ordered on the disk. that is  two records are placed physically close to each other if the attributes defined as the clustering index key columns have similar values or are in the same range. clustering indexes could be faster than normal indexes since they usually store actual records within the index structure and the access on the ordered data needs less io costs. but similar to regular unclustered indexes  the clustering index contains one entry for each record and can be very large in size. in addition  frequent update operations may incur periodic table reorganizations. more recently  db1 introduces a new physical layout which is more efficient than regular clustering indexes  named multi-dimensional clustering  mdc  . the mdc table supports block indexes which point to groups of records  instead of to individual records. unlike the primary clustering index  the block index can include multiple clustering dimensions. in fact  the mdc table mimics a multidimensional cube by using a physical region for each unique combination of dimension attribute values. a physical block contains only records which have the same unique values for dimension attributes and could be addressed by block indexes  a higher granularity indexing scheme. block indexes identify multiple records using one entry and are thus quite compact and efficient. queries using block indexes could benefit from faster block index scan  optimized prefetching of blocks  as well as lower path length overheads while processing the records. it is reported that the mdc table could dramatically improve query performance by 1 orders of magnitude. more details of the mdc table can be found in  and db1 online help center. 
the following figure schematically shows a triple table in the mdc form and a key of the block index. this example shows students and their education information. the table is clustered at the dimensions  property  and  subject . triples in the table are stored in blocks  which contain a fixed number of consecutive pages on disk. in figure 1 a   a numbered block is represented by a gray rectangle. the grid in the figure represents the logical partitioning of these blocks  and each square represents a logical cell. a column or row in the grid represents a slice for a specific dimension. for example  all triples with  #takecourse  as the property are found in the blocks contained in the slice defined by the  #takecourse  column in the grid. similarly  all triples of student  p1  are found in the blocks contained by the  p1  row in the grid. when the mdc table is created  a block index is automatically created for each dimension to determine which blocks contain all records having a particular key value. the block index's structure is similar to that of the traditional index  except that keys at the leaf level point to a block identifier instead of a record identifier. figure 1 b  shows a key from the block index on  property . the key includes a key value  namely  #takescourse   and a list of block ids. each block id contains a block location. 
 
 a  a mdc table with dimensions of  subject  and  property  
 
 b  a key from the block index on  property  
figure 1. examples of the mdc table and block index 
in practice  it is not useful to create an index on a column with few distinct values because the index does not narrow the search too much. but  it is valuable to build a mdc table using such a column as the clustering dimension because records are clustered by blocks which are physically stored in consecutive pages on disk and can localize the result set to a selected fraction of the table. considering that real ontologies have a limited number of properties  the  property  column of the triple table  could be a good candidate for clustering. columns  subject  and  object  of the triple table can have millions of distinct values  and clustering on these two columns may generate millions of blocks  each block includes just several triples  and is thus expensive in space consumption. therefore  we propose to build triple tables in our schema as the mdc table clustered along the  property  column. also  the typeof table is built as a mdc table clustered on the  class  column.  
for the issue of the accuracy of statistics on the value distribution  we propose to categorize rdf literals into four major types aligned with the datatypes supported by popular databases and separately store them in four tables to make effective use of built-in value comparison operations in databases. for instance  all numeric literals  e.g.  literals of xsd:integer or xsd:float  are stored in a table and their values are represented by a column whose datatype is decimal in db1. similarly  one table for boolean literals  one for datetime literals and one for string literals. in fact  this method does not solve the issue of inaccurate statistics completely  although it is effective in practice. wang et al.  encountered similar problems for ecommerce data management and proposed to build external histograms for values of different attributes and rewrite the physical query plan based on these external histograms. that is  with the external histograms  the query engine could generate an optimal query plan. therefore  we are investigating a similar method for the performance of triple stores  which needs to access the core engine of db1.  
the proposed method  the mdc table for storage and separating typed literals for better statistics  and the vertical partitioning approach defined in  adopt similar ideas from technical perspectives. but our storage model is obviously more flexible  allowing frequent ontology changes without altering the database schema and leveraging industrial strength block indexes techniques and optimizations. the evaluation results in section 1 show that the proposed schemes for performance improvement are very effective. 
1 sparql query processing 
essentially  sparql is a graph-matching query language over rdf triples. an example is shown in figure 1  which is to find uris and optional email addresses of all undergraduates who are less than 1 years old and have an interest of music.  presents the semantics and complexity bounds of sparql and shows that the evaluation of sparql patterns is pspace-complete without filter expressions which restrict the value of literals and arithmetic expressions. existing methods  e.g.   translate a sparql query into sql statements which are evaluated on the triple store in relational databases. they handle sparql queries with basic graph patterns well. but  they either ignore filter expressions due to complexity or adopt memory-based methods for filter expressions evaluation. figure 1 illustrates the challenge for the processing of filter expressions. the sparql query in the right side is to get computers whose monitor and keyboard have the same color. however  the color values in rdf triples might be of a string in some cases or of an encoded integer in other cases. sparql specification specifies how to compare resources of different types. for sql generation  therefore  we have to use different comparison methods for different types of operands. this is the reason that existing methods evaluate filter expressions using procedural code as a post-processing. in some cases  such as the combination of optional patterns and filter expressions  lots of sqls are issued for evaluation   which leads to expensive i/o and network costs. here  we propose an effective method to translate a sparql query with complex filter expressions into a single sql statement to make full use of well-developed sql query engines for evaluation against our designed schema. also  the generated sql can be easily embedded into other sqls retrieving non-rdf data  which provides a way to effectively integrate rdf data with relational data.  
select  person  email where { 
   person rdf:type bm:undergraduatestudent 
  { 
    {  person bm:like  interest } union     {  person bm:love  interest }    } . 
   optional {  person bm:emailaddress  add }.     person bm:age  age . 
   filter    age   1 && 
      regex str  interest    music$     } 
 
figure 1. an example of sparql query 
 
 computer1  color of monitor  1 	 y select  x  computer1  color of keyboard  1 	where {
 computer1  color of monitor   black  	 x color of monitor  y .
 z
 computer1  color of keyboard   black  	 x color of keyboard  z .
 computer1  color of monitor  1 	filter   y =  z 
 computer1  color of keyboard  1 	}
figure 1. dynamic matching in sparql evaluation 
1.1 sparql pattern tree 
generally  basic graph patterns in a sparql query can be expressed as a sparql pattern tree. the tree shows the backbone of the sparql query and is used in the translation. similar pattern trees are used in . figure 1 illustrates the proposed sparql pattern tree of the query in figure 1. there are four types of nodes in the pattern tree: 
* and node. it corresponds to the conjunction of graph patterns in a sparql query. in our approach  consecutive nested and patterns  i.e.  two and patterns have direct parent-child relationships  are flatten into one and node. so  an and node can have multiple child nodes. each child node has a flag indicating whether the node is optional or not. in this way  the optional patterns in sparql are also covered by and nodes. 
* or node. it corresponds to a union pattern of sparql. similar to the and pattern  nested union patterns are flatten and represented by one or node. 
* triple node. it represents a single rdf triple. the subject  property and object of the triple could be constants or variables. 
* filter node. it represents a filter expression in a sparql query. a filter node is always associated with an and node as a special child node. in the bottom-up semantics of sparql  only variables appearing in the corresponding and pattern can be used in filter expressions. 
and node  or node and triple node represent graph patterns in the rdf graph. we call them pattern nodes. a sparql filter expression can be parsed into a filter expression tree. non-leaf nodes are operators or functions  while leaf nodes are constants or variables. figure 1 shows the defined filter expression tree of the filter clause of the query in figure 1. 
 

 figure 1. an example of sparql pattern tree 
1.1 facets of an rdf object 
the result of a filter expression or a sub-expression is an rdf object  which could be a literal  an iri reference or an error  e.g. applying functions on unbound values. . note that uri is also called iri in sparql. however  the result of a sql expression is always a primitive value  such as a string  a boolean or an integer. taking literals as an example  we know that a literal could have a lexical form  an optional language tag and an optional datatype. it is difficult to express a literal object by a primitive value. fortunately  we found that usually only a primitive part of an rdf object is used in a function or operator. thus  we define  facets  for rdf objects to facilitate the translation from a sparql filter to a sql expression. a facet of an rdf object is somewhat like a view  or a data field of an object. the value of each facet is always a primitive value of a specific type. facets used in our method are defined in table 1. 
table 1. the facets of an rdf object 
