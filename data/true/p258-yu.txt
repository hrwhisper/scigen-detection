latent semantic indexing  lsi  is a well-known unsupervised approach for dimensionality reduction in information retrieval. however if the output information  i.e. category labels  is available  it is often beneficial to derive the indexing not only based on the inputs but also on the target values in the training data set. this is of particular importance in applications with multiple labels  in which each document can belong to several categories simultaneously. in this paper we introduce the multi-label informed latent semantic indexing  mlsi  algorithm which preserves the information of inputs and meanwhile captures the correlations between the multiple outputs. the recovered  latent semantics  thus incorporate the human-annotated category information and can be used to greatly improve the prediction accuracy. empirical study based on two data sets  reuters1 and rcv1  demonstrates very encouraging results.
categories and subject descriptors
h.1  information storage and retrieval : content analysis and indexing-indexing methods
general terms
algorithms  theory
keywords
latent semantic indexing  dimensionality reduction  supervised projection  multi-label classification
1. introduction
¡¡information retrieval and pattern recognition often suffer from the problem of high dimensionality of the data  for the reason of learnability or computational efficiency. therefore dimensionality reduction in terms of semantic indexing or feature projection is of great importance and is commonly applied to solve real world problems  1  1  1 .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  salvador  brazil.
copyright 1 acm 1-1/1 ...$1.
¡¡among various methods  latent semantic indexing  lsi  turns out to be a successful approach and is widely applied to document analysis and information retrieval . to apply lsi  documents are represented in a vector space model  and singular value decomposition  svd  is performed to find the sub-eigenspace with large eigenvalues. it is shown that lsi can find the best subspace in terms of frobenius norm of matrix. thus the technology behind lsi is also called principal component analysis  pca  in the sense that each  latent semantic  can be viewed as a  component  to represent the data  see  e.g.  .
¡¡lsi is purely unsupervised and is not capable to incorporate some additional knowledge. there are at least two reasons for further improvements on this issue. first  considerable information about the content of documents is reflected by document's labels  which is often annotated by human experts. this is particularly the case in the multi-label setting where each document is assigned to multiple categories. the semantic correlations of assignments for variant categories and the hierarchical structure of categories expresses the semantic relationships between documents. therefore  it is desired to have a lsi technique that can be informed by this additional knowledge and produce semantically more meaningful latent factors.
¡¡second  the unsupervision of lsi leads to results that may be or may not be useful in discriminative analysis like automatic text categorization. however in one specific classification or regression problem  output information is in general very important and should be incorporated into the feature mapping or selection process. in particular we consider problems with multiple labels: for an input x the corresponding output is no longer a scalar but a vector y =  y1 ... yl t. thus the text categorization system solves many related tasks at the same time. in this setting the dependencies between multiple labels are worth considering for multivariate data analysis  and can be used to improve the indexing for these specific tasks. furthermore  training a system with multiple labels might lead to smaller parameter variance and the prediction for a particular label is improved if the labels are correlated.
¡¡this setting is very common in real-world applications. one example is the problem of multi-label document categorization  where each document is allowed to be associated with more than one category and where categories often have semantic correlations . the well-known text data set reuters-1 contains such documents  and the new text data corpus rcv1 has additionally a topic hierarchy . these two data sets will be used in the experiments.
¡¡in this paper we introduce a supervised lsi called multilabel informed latent semantic indexing  mlsi . mlsi maps the input features into a new feature space that retains the information of original inputs and meanwhile captures the dependency of output dimensions. the mapping is derived by solving an optimization problem for linear projections  and can be easily extended for nonlinear mappings with kernels. we use this method as a preprocessing step and achieve encouraging results on the multi-label text classification problems.
1 notations
¡¡we consider a set of n documents. for i = 1 ... n  each document i is described by an m-dimensional feature vector xi ¡Ê x  and is associated with an l-dimensional output vector yi ¡Ê y. we denote the input data as a matrix x =  x1 ... xn t ¡Ê rn¡Ám  and the output data as y =
 y1 ... yn t ¡Ê rn¡Ál  where  ¡¤ t denotes matrix transpose. we aim to derive a mapping ¦· : x 1¡ú v that projects the input features into a k-dimensional latent space.
¡¡in the following  lower-case bold roman letters denote column vectors  and upper-case ones denote matrices. in particular  i is reserved for identity matrix. eigenvalues are usually denoted as ¦Ë and it should be clear from the context which matrix they are corresponding to. k¡¤k denotes frobenius norm for matrices and 1-norm for vectors  and tr ¡¤  denotes trace for square matrices.
1 paper organization
¡¡the paper is organized as follows. in section 1 we formulate the data projection as an optimization problem in the linear case and then propose a regularized version to prevent overfitting  which is generalized to nonlinear mapping by using kernels. then we point out its connections to related work in section 1 and report the experimental results in section 1. in section 1 we conclude the paper.
1. the mlsi algorithm
¡¡we begin by introducing an optimization explanation for lsi  and then take into account the output information.
1 optimization problem for lsi
¡¡in lsi  we aim at finding a linear mapping from the input space x to some low-dimensional latent space v  while most of the structure in the data can be explained and recovered. we can achieve this by taking a latent variable model and solving the following optimization problem which minimizes the reconstruction error  see  e.g.   :
		kx   vak1	 1 
	subject to:	vtv = i 
where v ¡Ê rn¡Ák and a ¡Ê rk¡Ám  given k ¡Ü m. each column of v corresponds to one latent variable or latent semantic  and by vtv = i we constrain that they are uncorrelated and each has unit variance1. for each document in x  represented as one row in x   the corresponding row in v explicitly gives its projection in v. a is sometimes called factor loadings and gives the mapping from latent space v to input space x. at the optimum  va leads to the best k-rank approximation of the observations x.
¡¡the derived indexing explains the covariance of input data  which is however not necessarily relevant to the output quantities. thus lsi may or may not be beneficial to supervised learning problems. generally speaking  it is more desirable to consider the correlation between input x and output y  and the intra-correlation within y  if multiple labels . therefore  we turn to supervised indexing in the next subsection  incorporating both input x and output y.
1 a supervised lsi
¡¡the unsupervised indexing problem  1  explicitly represents the projections of input data x in matrix v. to consider the label information  we can enforce the projections v in problem  1  sensitive to y as well. thus in supervised lsi we solve the following optimization problem:
	min	 1   ¦Â kx   vak1 + ¦Âky   vbk1	 1 
a b v
	subject to:	vtv = i 
where v ¡Ê rn¡Ák gives the k-dimensional projections of documents  for features of both x and y; a ¡Ê rk¡Ám  b ¡Ê rk¡Ál are the factor loadings for x and y  respectively. 1 ¡Ü ¦Â ¡Ü 1 is a tuning parameter determining how much the indexing should be biased by the outputs. as before  vtv = i restricts the k latent variables to be uncorrelated and have unit variance. clearly  the cost function is a trade-off between the reconstruction error of both x and y. we wish to find the optimal indexing that gives the minimum reconstruction error. the second part in the objective function of problem  1  enforces the latent semantics to explain the dependency structure of multiple labels. the following theorem states the interdependency between a  b and v at the optimum.
¡¡theorem 1. denote c =  1 ¦Â xxt +¦Âyyt  and let ¦Ë1 ¡Ý ... ¡Ý ¦Ën be eigenvalues of c with corresponding eigenvectors v1 ... vn. if v a and b are the optimal solutions to problem  1   then:
 a  a = vtx  b = vty;
 b  v =  v1 ... vk r  where r is an arbitrary k ¡Á k orthogonal rotation matrix;
 c  at the optimum  the objective function intr c    tr¡êvtcv¡è  or equivalently 	 1  equals to.
¡¡to improve readability  we put all proofs into appendix. theorem 1 states that the leading eigenvectors of c form a solution for matrix v  and any arbitrary rotation for v does not change the optimum. therefore to remove the ambiguity  we focus on the solution given by the leading eigenvectors of c  i.e.  v =  v1 ... vk . problem  1  can thus be achieved by solving the eigenvalue problem cv = ¦Ëv for the first k leading eigenvectors  which is equivalent to solving1:
	max	vtcv	 1 
v¡Êrn
	subject to :	vtv = 1.
then v =  v1 ... vk   a = vtx  and b = vty gives the optimal solution for problem  1 .
1 mlsi - primal form
¡¡to complete the mlsi algorithm  we still need to consider two things. firstly  the indexing should not rely on the labels  since for new documents we have no target information yet. secondly  the stability of indexing should be taken into account  because otherwise overfitting is likely to occur.
1.1 linear constraint
¡¡it is not hard to see that solving problem  1  only gives the projections for training data with both features in x and y. we wish to construct a mapping ¦· : x 1¡ú v that is able to handle the input features of any new documents  thus we add a linear constraint to problem  1  and restrict the latent variables as linear mappings of x  i.e. 
v = xw.
therefore we have vi = xwi  for i = 1 ... k  if we denote w =  w1 ... wk  ¡Ê rm¡Ák. plugging v = xw into  1   we have an optimization problem with respect to w:
	max	wtxtcxw	 1 
w¡Êrm
	subject to :	wtxtxw = 1.
1.1 overfitting and regularization
¡¡similar to other linear systems  the learned mappings can be unstable when the span{x1 ... xn} has a lower rank than m  due to the small size of training set or dependence between input features1. as a result  a disturbance of w with an arbitrary w  ¡Í span{x1 ... xn} does not change the objective function of optimization since  w +w  txi = wtxi  but may dramatically change the projections of unseen test documents which are not in the spanned space. to improve the stability  we have to constrain w in some way.
¡¡suppose rank c  = n  then maximizing  1  is equivalent to minimizing vtc 1v.1 we introduce the tikhonov regularization  into problem  1  as the following
	min	wtxtc 1xw + ¦Ãkwk1	 1 
w¡Êrm
	subject to :	wtxtxw = 1 
where kwk1 = wtw is a penalty term and ¦Ã is a tuning parameter. the following theorem shows that the regularization term kwk1 removes the ambiguity of mapping functions by restricting w in the span of xi  i = 1 ... n  and thus improves the stability of mapping functions.
¡¡theorem 1. if w is an eigenvector of the generalized eigenvalue problem  1   then w must be a linear combination of xi i = 1 ... n  namely
xn
t
	w = x ¦Á =	 ¦Á ixi
i=1
where ¦Á ¡Ê rn.
¡¡problem  1  is easily solvable by setting the derivative of its lagrange formulism with respect to w to be zero. then
we obtain a generalized eigenvalue problem h	i
	xtc 1x + ¦Ãi w = ¦Ë xtxw 	 1 
which gives generalized eigenvectors w1 ... wm with eigenvalues ¦Ë 1 ¡Ü ... ¡Ü ¦Ë m. note we sort eigenvalues in a nondecreasing order  since we take the k eigenvectors with the smallest eigenvalues to form the mapping. the first k eigenvectors are used to form the mapping functions as the following
		 1 
where the scaling caused by eigenvalues ensures that more important dimensions get larger weights. as the main results we obtain ¦· x  =  ¦×1 x  ... ¦×k x  t which maps x into a k-dimensional space.
¡¡in problem  1  we are interested in the eigenvectors with the smallest eigenvalues  whose computation is however the most unstable part in solving an eigenvalue problem. thus we let ¦Ë = 1/¦Ë  and turn the problem into an equivalent one: h	i
	xtxw = ¦Ë xtc 1x + ¦Ãi w 	 1 
where we are seeking the k eigenvectors with the largest eigenvalues. this gives the mlsi algorithm in primal form  as summarized in table 1.
inputx	1 
¦Ã ¡Ý 1 k   1steps i  calculate c =  1   ¦Â xxt + ¦Âyyt;
 ii  solve the generalized eigenvalue problemtxw = ¦Ë¡êxtc 1x + ¦Ãi¡èw 	:
x
obtain eigenvectors w1 ... wk with largest k eigenvalues ¦Ë1 ¡Ý ...p ¡Ý ¦Ëk.outputmapping functions ¦×j x  =	¦Ëjwtj x  j = 1 ... ktable 1: mlsi in primal form
1 mlsi - dual form
¡¡so far we have considered linear mappings that project inputs x into a meaningful space v. however  theorem 1 implies that we can also derive a nonlinear mapping ¦·.
let a kernel function kx ¡¤ ¡¤  be the inner product in x 
i.e.  kx xi xj  = hxi xji = xti xj  then from theorem 1  v = xw = xxt¦Á = kx¦Á 
where kx is the n ¡Á n kernel matrix satisfying  kx i j = kx xi xj . kwk1 can also be calculated with kernel kx: kwk1 = wtw = ¦Átxxt¦Á = ¦Átkx¦Á.
similarly  we can define a kernel function ky ¡¤ ¡¤  for inner product in y and obtain a kernel matrix ky = yyt. then we can calculate the matrix c using kernels:
¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡c =  1   ¦Â kx + ¦Âky   1  and express the dualformalism of problem  1  with respect to coefficients ¦Á as
		 1 
	subject to :	 
which gives rise to again a generalized eigenvalue problem
	.	 1 
we obtain the generalized eigenvectors ¦Á1 ... ¦Án  with ¦Ë 1 ¡Ü ... ¡Ü ¦Ë n. the first k eigenvectors are applied to form the mappings. scaled by the eigenvalues  the j-th mapping function  j = 1 ... k  is given by
.
as before we define ¦Ë = 1/¦Ë  and change  1  to the following equivalent form:
¡¡¡¡¡¡¡¡¡¡¡¡k 	 1  and hence we can choose the k eigenvectors with the largest eigenvalues. the mlsi algorithm in dual form is summarized in table 1.
table 1: mlsi in dual form
inputx ¡Ê rn¡Ám y ¡Ê rn¡Ál 1 ¡Ü ¦Â ¡Ü 1 ¦Ã ¡Ý 1 k   1steps i   kx i j = kx xi xj    ky i j = ky yi yj  
c =  1   ¦Â kx + ¦Âky;
 ii  solve the generalized eigenvalue problem:
	k	 
obtain eigenvectors ¦Á1 ... ¦Ák with largest eigenvalues ¦Ë1 ¡Ý ... ¡Ý ¦Ëk.outputmapping functions¡¡several advantages of dual mlsi can be seen from table 1. first of all  in contrast of solving a generalized eigenvalue problem for m ¡Ám matrices in primal mlsi  in dual mlsi we only need to solve a similar problem for n ¡Á n matrices. in a general indexing problem  the input dimension m  i.e.  number of words  is much larger than the number of documents n  and therefore working in dual form is more efficient. in the experiments we will use the dual form for indexing. second  mlsi in dual form is ready to deal with nonlinear mappings. for this we consider a nonlinear mapping ¦Õ : x ¡Ê x 1¡ú ¦Õ x  ¡Ê f  which maps x into a high-dimensional or even infinite-dimensional feature space f  and change x to be  ¦Õ x1  ... ¦Õ xn  t. then the kernel function is accordingly defined as kx xi xj  = h¦Õ xi  ¦Õ xj if 
where we still have kx = xxt. therefore  we can directly work with kernels  e.g.  rbf kernel kx xi xj  = exp  kxi  xjk1¦Ò1    without knowing ¦Õ ¡¤  explicitly. similarly  we can define a nonlinear mapping for y and directly work on the corresponding kernel matrix ky. although this paper mainly considers the linear kernel to explore the linear correlation of inputs and multivariate labels  the formulism implies that the method can generally handle more complex inputs and outputs  e.g.  images  by using some other suitable kernels.
1. connections to related work
¡¡the proposed algorithm mlsi is seen to solve the same optimization problem as lsi when ¦Â = 1  as seen in  1  and  1 . therefore mlsi takes as special case the unsupervised lsi  or more specifically  kernel pca  1  1 . kernel pca is the dual form of pca and turns out to solve the eigenvalue problem kx¦Á = ¦Ë¦Á with kernel matrix  kx i j = kx xi xj . to build this connection  we see from  1  that c = kx holds when ¦Â = 1 in mlsi. therefore from table 1 it is easy to check that mlsi solves the generalized eigenvalue problem
k1x¦Á = ¦Ë 1 + ¦Ã kx¦Á 
which is identical to kernel pca since kx is invertible. under this situation  the regularization term controlled by ¦Ã is just a rescaling of the cost function  as can be seen in  1 . hence ¦Ã is just a nuisance parameter and we obtain rescaled eigenvalues compared to kernel pca. from this perspective  mlsi in general performs label informed kernel pca or supervised kernel pca  since it can be viewed as directly modifying the kernel matrix c with label information.
¡¡in the literature there are some other well-known supervised projection methods  like linear discriminant analysis  lda   e.g.     canonical correlation analysis  cca   e.g.   1  1   and partial least squares  pls   1  1 . mlsi substantially differs from them. lda is focusing on single classification problem where the output is one-dimensional  while in contrast mlsi considers predictions with multivariate labels and is thus more general. cca finds the correlations between two representatives of the same documents  e.g.  inputs x and outputs y in our setting  by minimizing kvx   vyk1 subject to both vx and vy being unitary and linear mappings of xi and yi  see a recent discussion in  . however  it does not require the projections vx and vy to promise low-reconstruction error of x and y and thus ignores the intra correlation of either  especially y . instead  mlsi takes into account all the inter and intra dependencies  since the projections minimize the reconstruction error of inputs and outputs simultaneously. pls can be seen as a penalized cca  but it cannot find a space of larger dimensionality than that of y  thus its generalization performance on new dimensions of outputs is restricted  see discussions in  . instead  mlsi can find in principle n orthogonal dimensions  if kx is positive definite .

	 d 	 e 	 f  
figure 1: classification performance on reuters data set. upper rows   a   b   c   show results with setting  i   and lower rows   d   e   f   show results with setting  ii .1. empirical study
¡¡in this section we evaluate the proposed mlsi algorithm based on the task of multi-label text classification  in which we allow one document to be assigned to multiple labels. one can treat each classification problem separately  but these problems could have correlations between each other and could be solved simultaneously. we solve this problem by applying mlsi and encoding the labelling information into the mapping  and then each classification problem is solved independently using the projected features. by incorporating the output information that may be difficult to reveal from inputs  the indexing is biased by the specific classification tasks and is thus more suitable for discriminate analysis.
¡¡we compare the classification performance using features learned by mlsi and normal lsi  where in the latter case no labelling information is used in indexing. experiments are performed on two text data sets taken from reuters-1 and rcv1  respectively  followed by detailed discussions.
1 data sets and preparation
¡¡our first data set is a text corpus which contains all the documents in reuters-1 that are associated with multiple categories. eliminating those minor categories that contain less then 1 documents  we have 1 categories to work with. picking up all the words that occur at least in 1 documents  we finally obtain 1 documents with 1 words that are used in computing tfidf feature vectors. in average  each document is assigned to 1 categories  and each category has 1 positive documents.
¡¡the other data set is a subset of the rcv1-v1 text data set  provided by reuters and corrected by lewis et al. . the data set contains the information of topics  regions and industries for each document and a hierarchical structure for topics and industries. since it is common that one document is assigned to multiple topics  this is an ideal data set for multi-label text classification. we use topics as the classification tasks and simply ignore the topic hierarchy. a small part of the data set is chosen  and similar preprocessing as for reuters-1 is done by picking up words with more than 1 occurrences and topics with more than 1 positive assignments. we end up with 1 documents with 1 words  and have 1 topics left. in average  each topic contains 1 positive documents  and each document belongs to 1 topics. in the following we denote  reuters  and  rcv1  for these two data sets respectively.
1 experimental design
¡¡we have two settings in this experiment. in the first setting  i   we randomly pick up 1% categories for classification and employ 1-fold cross-validation with one fold training and 1 folds testing. this is a standard classification setting  and our goal is to evaluate whether mlsi trained on the training data are able to derive high-quality features for new test points and obtain good classification results. we will compare the following three cases in our experiment:
1. original features: a linear svm with all the text features is trained for each category  and this serves as the baseline for comparison.
1. lsi: standard unsupervised projection is performed which maps the input data into a low-dimensional space. then a linear svm is trained on this projected space.
1. mlsi: additional label information for training data is used for making a supervised mapping. then the same linear svm is trained on this projected space.

	 d 	 e 	 f  
figure 1: classification performance on rcv1 data set. upper rows   a   b   c   show results with setting  i   and lower rows   d   e   f   show results with setting  ii .in both of the projection methods lsi and mlsi  we use the dual form in this experiment simply because this gives much improved efficiency. in case of linear kernels  this will give the same results as that in primal form. the second setting  ii  aims to test the generalization performance of the projection methods on new categorization tasks and compare it with using original features. for this we consider the classification problems for the rest 1% categories. to make a fair comparison  we perform 1-fold cross-validation on previous unseen data  with the same size as training data   using the feature mappings derived from setting  i .
¡¡the classification performance is compared using f1 macro  micro and auc  area under curve  score. f1-measure defines a trade-off between precision and recall  and is known to be a good metric for classification evaluation. in case of multiple outputs  f1 macro is just the arithmetic average of f1 measures of all output dimensions  and f1 micro can be seen as a weighted average that emphasize more on the accuracy of categories with more positive examples. alternatively  auc score is the area under the roc  receiver operating characteristics  curve  which plots sensitivity versus 1-specificity. it is known to measure the objective quality of ranking for specific classification problems. a higher auc indicates a better ranking. it is also averaged over all the output dimensions. we also tried classification accuracy  but didn't get informative comparison because most of the classification problems are very unbalanced  more than 1% of data are negative examples .
¡¡for these algorithms we choose all the parameters as follows. we use libsvm with linear kernel and fix c = 1  which gives original features the best performance and is then fixed for the other two methods. for mlsi we set the parameter ¦Â to 1 after we scale kx and ky to ensure they have equal traces for balance. ¦Ã is simply fixed as 1 to give the best performance. for both settings we repeat the experiments 1 times with randomization  and the performance versus dimensionality of projection is shown with means and standard deviations in figure 1 and figure 1 for reuters and rcv1  respectively.
¡¡the first observation from these figures is that mlsi outperforms lsi in all the cases in setting  i . this indicates that the mapping functions in mlsi are generalizable to new test data  by incorporating the output information for the training data.
¡¡another encouraging observation is that mlsi in most cases can even lead to better classification performance than original features  which uses almost 1 times more features. mlsi in this case can not only greatly accelerate the classification tasks  but also improve the performance. this is especially true for f1 macro and auc score  where a large gap can be observed for all the figures. for f1 micro the effect of mlsi is mixed  and an interesting decrease can be observed in figure 1 e  and figure 1 e . consider the difference between f1 macro and f1 micro measures  we conclude that that mlsi mainly improves the classification accuracy for categories with fewer positive training examples. for those categories with many positive examples  learning methods are easily to achieve good results and thus the space for further improvement is not big.

	¦Â	¦Â
	 a 	 b 

	 c 	 d 
figure 1: performance of mlsi with respect to ¦Â   a   b   and ¦Ã   c   d   for reuters data set.  a   c  show results with setting  i   and  b   d  show results with setting  ii .¡¡mlsi has two tunable parameters ¦Â and ¦Ã that controls the kernel combination weights and the strength of regularization  respectively. for previous figures it is assumed fixed  and in this last experiments we study the classification performance when they varied. since we can see similar results for both data sets on all the evaluation measures  we only show in figure 1 the illustrations for reuters with auc score. figures for ¦Â are shown with dimensionality k fixed as 1 since it is insensitive to the results.
¡¡a first impression from figure 1  a   b  is that the curves are rather smooth  except when ¦Â approaching 1 in setting  ii  . this indicates that the performance is not very sensitive to small changes of ¦Â value. when ¦Â increases from 1 to 1  it is seen that all the curves first increase and then decrease  indicating that a good trade-off should be identified for best performance. when ¦Â approaches 1  mlsi tends to be lsi and thus unsupervised. outputs are ignored in this case  and poor performance is observed for both settings. on the other hand when ¦Â approaches 1  the mappings tend to solely explain outputs y  ignoring the intrinsic structure of inputs x. this also leads to poor performance  especially for setting  ii  because the mappings are not good to generalize to new outputs. overfitting occurs in this case  where a sharp decrease can be observed with even a much worse performance than lsi  ¦Â = 1 . finally  ¦Â = 1 is seen to be a good trade-off for both settings. from our experiences  a slightly larger ¦Â  e.g.  1  is better for setting  i   and a slightly smaller ¦Â  e.g.  1  is more stable for setting  ii . for ¦Ã we have the observation that small ¦Ã leads to better performance for setting  i   while an appropriately chosen ¦Ã is necessary for setting  ii . this reflects its regularization effect  since for setting  ii  new categories are considered and setting ¦Ã = 1 will lead to overfitting.
1. conclusions
¡¡in this paper we propose a novel indexing algorithm mlsi for multi-label informed latent semantic indexing. the mappings are supervised and retain the statistical information of not only input features but also the multivariate outputs. we present both the primal and the dual formalisms for the linear mappings  and nonlinear mappings can also be derived by using reproducing kernels. the final solution ends up as a simple generalized eigenvalue problem that can be easily solved. the algorithm is applied for multi-label text classification with very encouraging results. currently we are mainly exploiting linear dependency of inputs as well as outputs. in the near future we plan to apply the algorithm to other types of objects like images with suitable kernels  e.g.  rbf kernels   and define kernels to explore richer structured outputs.
