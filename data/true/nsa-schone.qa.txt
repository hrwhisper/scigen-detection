we provide a description of the qactis question-answering system and its application to and performance in the 1 trec question-answering evaluation. since this was also qactis's first year competing at trec  we provide a complete overview of its purpose  development  structure  and its future directions.
1. introduction
automatic question answering  qa  has been an area of technical interest at trec for more than a half decade and it has been the subject of major investment from the government-driven aquaint program for almost three years. qactis  pronounced like  cactus    which stands for  question-answering for cross-lingual text  image  and speech   is a still-in-formulation protoype system that is being developed by the u.s. department of defense as a
means of gaining greater understanding of qa as a whole while focusing on cross-lingual and cross-media qa--areas which have received less attention from aquaint. the final goal for this effort is to develop a prototype which can allow users to ask questions in more than one language  e.g.  english and spanish   interpret those questions regardless of the language  and return answers which have been derived from multilingual and/or multimedia sources. particular interest for this project is the capability of eventually answering questions from speech which is a virtually unresearched area of study. for the purposes of this trec competition  however  we  like others  concentrate on english newswire text and the hope we have is that the knowledge gained by this experience can be useful in other languages and other media types.
　　in terms of its question-answering ability  qactis consists of three major components. two of these components are competing mechanisms for interpreting questions and postulating possible answers. the third component is a
* dragon development corporation  columbia  md
 johns hopkins applies physics laboratory  laurel md
 method of validating proposed answers. the first mechanism for answer generation is designed to be general and handle any kind of factoid-style question that might be posed. this technique  which we will here refer to as a
  knowledge-graph induction  strategy makes use of sophisticated natural language processing  nlp  techniques to automatically build attributed object-relationship graphs for documents which likely contain answers . this technique then performs graphical searches to find relevant components to answers and to ensure such components have the interrelationships required by the question. this approach is fairly costly and there are many times when cheaper and more convenient solutions are available. for this reason  qactis has a second search mechanism which we call a  filter cascade  strategy. this strategy consists of finding potential answers by initially hypothesizing many possible answers and then successively applying increasingly restrictive filters to narrow down potential answers to the one or few most appropriate selections. these filters consist of template-matching  shallow grammatical rule applications  and others. the fusion of results from these two systems represents the prototype's first kind of answers. this fusion can stand on its own as a questionanswerer. however  depending on a user's needs and computing environment  it might be feasible and even desirable to take advantage of the largely unstructured knowledge which exists on the world wide web. to satisfy this possibility  the third major component of the prototype is one which will receive answers from either of the individual systems or the fusion thereof and will use the web to eliminate undesirable potential answers.
　　in the sections that follow  we provide a description of these components as well as a description of the foundations which are required in order for these components to function. additionally  we indicate the level of performance we observed on the trec 1 evaluation as well as the successes and the difficulties we experienced in the evaluation. lastly  we outline the future directions that we plan to undertake between this and the next trec.
1. system description
figure 1: system overview

1.  the required foundations figure 1 provides a depiction of qactis as a whole.
since this trec evaluation focuses on the processing of english newswire text  those components that are related to multilingual or multilmedia issues have not been depicted in the diagram and we will not go into a discussion of those. several of the components which are emphasized have been mentioned before and will be described in greater detail later: namely  the induced knowledge-graph search  the filter cascade strategy  and the web validation. before we describe those pieces  we will first provide a description of the few remaining groundwork components  namely question interpretation  knowledge incorporation  and document search which provide the foundations to the remaining components.
1.1. question interpretation
obviously  it is essential that before a qa system can answer a question  it must be able to receive that question from a user and provide an interpretation of that question. qactis can interpret questions either via batch mode or graphical user interface. figure 1 provides a depiction of qactis's gui and an example question-answer pair  where the answer also includes the corresponding document support .
   prior to the 1 trec qa evaluation  qactis was designed to handle questions in a stateless fashion as if each question were independent of the others. moreover  the system was primarily designed to handle exclusively factoid-style questions. since the evaluation for 1 was to involve question sessions consisting of factoids  lists  and definitions  these pieces has to be incorporated into the system.
   the first issue needing to be addressed was expanding from only factoids to also allow lists and 'other.' since factoid-style answers are lists with only a single element  and since qactis actually produces a list and selects the best answers from that list as factoid answers  the process of deriving lists means that the system only had to be told how many answers to respond with. based on some empirical calculations  we determined that a fixed number of list answers  between 1 and 1  seemed to provide optimal list results. these are the parameters that we then used in the evaluation.
   regarding 'other' questions  we recognized from the trec 1 evaluation proceedings that the best-valued definition responses were wordy and tended to report most sentences that contained words of interest . we decided that our primary system would therefore select out the best sentences and return those as answers to 'other' questions. however  since older trec evaluations had incorporated definition-like questions of the form  who is x  or  what is x  where a factoid answer might satisfy  we thought to also try answering 'other' questions as if they were list questions of  what is x  questions. we submitted results using both methods.
   the next issue that had to be tackled was to determine how to handle a series of questions within a given 'user session'  that is  under a given topic . we resolved that the easiest strategy for handling such problems would be to cast them as a list of independent questions with resolved anaphora. a source of difficulty here was to determine how complicated the trec questions might be in terms of resolving anaphora. nist only provided a few simple examples and it was unclear if this level of simplicity would be comparable to what would be seen at evaluation. we assumed we would need the ability to handle more complex questions. two members of our team wrote and evaluated a large number of questions in the 'user session' format. there were six main styles of questions that we observed in this analysis. these were:  1  non-anaphoric questions;  1  questions where a simple anaphor related directly to the topic;  1  questions requiring more complicated anaphor resolution;  1  questions where neither the topic nor anaphora were indicated;  1  questions that referred to the answers to previous questions; and  1  questions that referred to the verbage of previous questions. though the first kind of such questions is trivial to solve  the rest are more interesting. we here provide examples of each kind:
type 1: simple anaphor
 kosovo : where is it located 
 roses : where were the first ones grown... 
 george w bush : when did bush take office 
type 1: complex anaphor
 synchronized swimming : what country gave origin to this sport 
 abraham & mary lincoln : when did she marry him 
type 1: no anaphor nor topic in question
 philippines :when was the leyte gulf invasion 
 climate : when was the last decadal oscillation 
type 1: reference to previous answer
 spain : who did spain want extradited  where was he prior to extradition 
 guerrilla : who are the founders of ...  what is their nickname 
type 1: reference to previous verbage
 venus : when venus crosses the sun is known as what type of eclipse  prior to 1  when did the last one occur 
   we developed software that we believed could handle the first five kinds of these questions. we anticipated that the sixth kind would be out of scope of this trec evaluation. at evaluation time  our code was able to handle most of the questions reasonably well  but as will be made more clear in section 1  there were some catastrophic failures which damaged our performance.
1.1. knowledge incorporation
in addition to determining the kind of question and resolving session issues  another component of question interpretation is determining the intent behind the wording of the question. taxonomic  ontologic  and dictionary-based information can be beneficial for judging the meanings of the question words. to satisfy this need  we made appeal to versions of the english wordnet . we also made use of an electronic dictionary from one of our earlier efforts . in addition to accessing meaning  we also needed to contend with issues regarding syntactic variation. we made some use of wordnet's word stemming capability and we also created a stemmer of our own which would provide multiple possible stems or conflations of the words in question.
1.1. document search
a common qa practice is to initially interpret the incoming question as if it were a request for relevant documents rather than a request for an answer . using this strategy allows the system to use typical information retrieval  ir  practices to identify documents that are likely to contain answers to the user's questions. although there is no guarantee that this assumption will be valid  this strategy has served useful in most experiements we have tried thus far. in the event that it is unsuccessful  there is not reason to believe that our qa system would have found the answer if the ir could not return appropriate documents.
   we experimented with several ir systems  including one of our own making. we determined that the lemur system  provided the best out-of-the-box results to our searches. we therefore incorporated lemur into qactis. although it is possible to massage the question so as to optimize ir results  we have not done this yet in qactis. our results therefore reflect the use of lemur as a black box where a query is supplied as input and a list of documents is generated as output. we use no pseudo-relevance feedback  prf  in searching since preliminary experiments suggested that although prf improves mean average precision  there seems to be a diluting amongst the top answer-bearing documents.
　　one last comment is in order regarding the kind of ir search qactis performs. in the qa community  some believe that the optimal form of ir for improving the ability to answer is to do optimal passage retrieval. this might be a good strategy for qa on factoid-style questions from newswire data where answers are often isolated in a limited number of sentences. however  it is not clear that restrictive passage retrieval is as beneficial when questions are more general  when answers occur across documents  and when the data consists of non-news or non-textual material. we therefore apply ir to full document retrieval which documents then get mined by the specific answer-searcher which later gets applied.
1. knowledge-graph induction strategy after the introductory steps described above  the next process is to perform a search for the answer. as was mentioned before  qactis has a two-phase approach to question answering  namely a knowledge-graph induction strategy and a separate filter cascade strategy. the knowledge graph induction and search is to be general and to potentially handle any kind of questions whereas the filter cascade was developed to handle specific kinds of questions with greater accuracy. we first describe the general knowledge-graph strategy and later discuss the filter cascade mechanism.
   figure 1 provides a detailed graphical view of the methodology employed by the knowledge-graph induction strategy. the basic objective of this strategy is to convert the top n potentially relevant documents  as returned by lemur  into a single  indexed  directed  attributed entity-relationship graph which can be mined to find connected subgraphs containing the desired components of the question. there is insufficient space to describe all of this process in exhaustive detail  so we provide a general overview of the major system components which allow us to induce and mine such a graph.
figure 1.  knowledge-graph induction/search

1.1. graph building
   to begin the graph induction process  we first perform a deep syntactic parse of the lemur documents using the charniak parser . although we would like parsing to happen at question time  it is currently an extremely slow process  taking about 1 second per sentence on a 1 ghz pentium iv . therefore  we parse all documents earlier at indexing time and then need only to pull back the correct parses during the query phase. in addition to parsing the documents  we run bbn's identifindertm system  to provide entity information which can support answering questions regarding people  places  and times. identifinder runs sufficiently fast that we apply it at query time. upon completion of both of these efforts  the system is then ready to induce a graph from this information.
we will consider an example of how this is done.
suppose there were a document in our collection:  johan vaaler invented the paper clip 1 years ago.  charniak's parser would convert this into something of the form
 s1  s  np  nnp johan   nnp vaaler  
 vp  vbd invented 
 np  dt the   nn paper   nn clip  
 advp  np  cd 1   nns years    rb ago     . .   
identifinder will also indicate that  johan vaaler  is a person. the graph builder then reprocesses the string to convert relative times like  1 years ago  into absolute times by using the document's metadata that indicates it was
written in 1  subtract 1 years  and reporting
 s1  s  np  nnp johan   nnp vaaler  
 vp  vbd invented 
 np  dt the   nn paper   nn clip  
 advp  pp  in in   nn 1       . .   
the graphbuilder next converts nouns and noun phrases into entities  verb phrases into relationships  and quantifiers  prepositional phrases  and adjectives into attributes. as it does this  it tries to some degree to resolve anaphora and the meanings of definite articles. when complete  it produces a graph akin to the  that in figure 1:
figure 1: indexed  attributed entity-rel graph

1.1. creating a bag of searches
in parallel with the graph-building effort  there is a separate process within the graph induction strategy which tries to interpret the user's needs based on the question. if the question were  who invented the paper clip    the system attempts first to convert the question into a definitive statement like  person.q invented the paper clip.  it then parses the statement and applies the graph building process thereto. the major objects  entities  relationships  quantifiers  and attributes  and some relations between them  like quantifier and quantified  are mined from the graph as entities to search for.
   next  the system builds a set of searches to process each kind of major object. in the sentence above  the major objects would be the  person.q  and  paper clip  entities and the  invented  relationship. the system determines the kinds of objects that these are and induces a collection of subroutines  one per object  that will be used to test each word/phrase of the top n documents to see if each provides evidence of the object. for example  the subroutine that is induced for the word  paper clip  would test each word in the top n documents to see if they are wordnet synonyms of the noun  paper clip  or if they are some sort of stem or conflation of it. the same is true for the word  invented  except that it knows to be looking for synonyms of a verb. for the word  person.q   the system knows that words or phrases that satisfy this must at least be entities  but entities that have been marked by identifinder as people or possibly organizations provide better solutions  as might entities that have been referred to as  he    she   and so forth. each different kind of question word  namely  who    when    where    what    why   and so forth generate a separate type of subroutine.
1.1.  growth using reachability and distance
the collection of subroutines is applied to all of the words/phrases within the top n documents and any word that is found to match a subroutine's needs is saved off as well as the location where the it appears. a language model-based score  see     is then used to weight each candidate answer.
   the main task for this system is to determine which objects  from among those that satisfy question words  can best address the needs of the user. this means one needs to identify the objects that are candidate answers and see if  through graph connectivity  it is possible to grow a subgraph which contains all or most of the desired elements from the incoming question. suppose figure 1 represents one of the top n documents. clearly  entity #1   paper clip   and relationship #1   invented   satisfy the question's requirement for  paper clip  and  invented.  entity #1   johan vaaler    according to the graph  was identified as a person ... so this also satisfies the need of
 person.q. 
   however  these three objects by themselves do not solve the user's question. they need to be woven together  if possible  so as to guarantee that the answer is correctly found. since figure 1 represents a directed graph  objects can be 'woven together' if an arrow exists between them and if the arrows points in the appropriate direction. from figure 1  relationship #1 is reachable from entity #1 and entity #1 is reachable from relationship #1. hence  we can link together each of the desired terms using this strategy and report  as a final answer  the product of the scores of each object.
   for many graphs  however  it is impossible to obtain reachability between all of the needed components. as a final supplement to the answer search process  we make use of the distance that a missing component is away from a reachable subgraph. the score for incorporating such a word is related to the reciprocal of the square of its distance from the subgraph.
   the scores of the distance-augmented reachable subgraphs are sorted  and the subgraphs with largest scores are reported as answers. for factoid answers  only the best such answer is kept  and for lists  the top m answers are reports.
1. cascade of filters strategy
a separate module was developed to provide a more indepth analysis and corresponding answers to certain kinds of questions such as the  how many  and  other  type questions. this was the initial starting point for qactis development and the feeling was that certain types of questions are likely to be amenable to straightforward solutions. the cascade of filters approach  cfa  has significantly better mrr scores for questions like the  how many  over those of the knowledge-graph induction algorithm  so qactis typically fuses both methods with the idea that specifically-tailored question answering through cfa should be used when available. the cfa uses different filters to identify potential answers and also eliminate others. any values left at the end of all the filters was considered to be an appropriate answer.
1.1. trigrams n tags -  tnt 
the cfa relies on information retrieval using lemur  wordnet as a lexical reference system  and tnt for partof-speech tagging. tnt  short for  trigrams 'n tags   is an efficient statistical part-of-speech tagger that is trainable on different languages and virtually any tagset .
1.1. filters
the cfa evaluates the top n  n usually set to 1  documents returned by lemur.  the original question is tokenized using tnt  and the main noun/noun phrase of
the question is extracted.   wordnet is also used to generate synonyms for the noun/noun phrase.  the retrieved

documents are then successively filtered to identify candidate answers. after all candidate answers are scored  the top score is considered the correct answer and ties go to the answer from the highest scoring lemur document. the cfa filters for  how many  questions are described below.
1.1 sentence extractor filter  sef : the sef identifies sentences of each top ir document that contain a match to the question noun phrase or synset synonyms  and also contain a numeric value. the distance in words between the noun and value are then calculated; the shortest distance is the best. a count of the important question words and synset words is also recorded and added to the minimum distance score. sentences with the highest scores for each of the top n documents are saved in a
hashtable with the candidate  how many  numerical value as the key. each of these sentences are then pos-tagged using tnt and saved in another hashtable ... again using the candidate value as the key. both hashtables are evaluated by the template matcher filter.
1.1 template matcher filter  tmf : shallow parsing was performed on the question and template match filters were formed. for example   how many hexagons are on
a soccer ball  could generate templates
a    #  hexagons are on a soccer ball 
b   soccer ball has  #  hexagons 
c   soccer ball contains  #  hexagons 
exact matches indicated a high degree of certainty. all potential answer sentences were tested against the template filters. if any matches were found from the templates  the system progressed to the sentence score recalculation module using these template matched answers  otherwise the semantic rules filter is applied to the hashtable sentences.
1.1 semantic rules filter  srf : these filters attempt to use semantic rules for validation. this set of filters eliminates candidates from the hashtable of possibilities. one such filter deals with the verb and its synonyms. for example  given the following two sentences  the first would be eliminated for the question  how many atheletes participated in the 1 summer olympics games  
potential filtered sentence 1: 1 atheletes of the 1 summer olympics won medals for the united states.
potential filtered sentence 1: 1 atheletes competed for medals in the summer olympics.
both sentences were in the candidate answer hash because they contained the main question word  athelete   had a numeric value with a distance of 1 from that question word  and contained the same number of important question words  summer olympics . in this case the verb  won  did not match  participated  or a synonym of participated  competed   so that sentence was eliminated from the candidate hashtable. the system also accounted for a conjugated form of the verb to appear in the answer as well as a verb to be presented in a different tense than the question verb. if the candidate hashtable still had entries  the processing continued to the next set of filters.
1.1 trigram  shallow parsing filter  tspf : all word-trigrams of the question and candidate answer sentences are computed. if any candidate sentences has trigrams in common with the question  then all sentences that do not have commonality are purged. next  the pos tags of the question are used to help form a direct object/ verb/value triple from the question. such triples are also formed for each of the remaining candidate sentences. again  if there are any candidate answers whose triple matches that of the question  then any candidates that do not have matches are discarded.
1.1 answer reporting: if there are still more than one candidate answer  the system rescores those that remain. the highest-scoring sentence is declared to be the answer. ties go to the highest lemur-scoring document.
1. providing web validation
several researchers have proposed using large external corpora for the purpose of validating candidate answers to factoid questions . in particular  the size of the web and the availability of online search tools make it convenient to use search engines as a data source to confirm answers using the web as a source for validation.
   magnini et al. compared two different approaches for web-based answer validation: a statistical approach that examined question and answer word co-occurences and a content based method that measures the proximity of question words to an answer in short text extracts. the two methods performed similarly and conferred roughly a 1% increase in performance over a baseline that did not apply answer validation. for our participation in the trec 1 qa task we adopted the content-based method of magnini et al. for both the factoid and list subtasks.
   the algorithm we used is based on the notion that the candidate answer pattern will appear in close proximity to the question terms. a query consisting of the exact answer pattern conjoined with content words from the question was submitted to the altavista search engine. thus one web request per candidate answer is required. like many search engines  the altavista engine returns a ranked lists of documents along with short textual extracts which contain query terms. it is these textual portions that are examined - not full documents. we want to reward answers that are very close to many question words and which are found with question words in multiple documents. we only considered the top ten responses from our baseline system which found a correct answer in 1%  trec-1  and 1%  trec 1  of cases.
　　an answer's score is produced by aggregating scores obtained from individual textual snippets returned by the search engine. the candidate answer string can contain multiple words and may occur more than once in the extract. we search for the presence of each non-stopword question word in the snippet. if found  we determine the distance  in 'non-question' words  between question word and answer.  a non-question word is any word not appearing in the question . for each word  the closest location is used. a product is computed over all query terms using 1/ dist+1  as factors. this results in a factor of 1 when a term is absent and a factor of 1 when a question word is adjacent to the answer  or separated only by other question words . values between 1 and 1 are obtained when question words are more distant from the answer location. per-snippet scores are summed for each candidate answer and the highest score is deemed the most likely response.
   we experimented with this approach using data from the trec-1 and trec 1 qa tracks and found that substantial gains could be obtained. answer validation enhances overall performance on all factoid questions  but the technique appears more effective with certain question types. we left our baseline system's answers unmodified for amount-type questions  i.e.   how many ...  or  how long is   which make up about 1% of the factoid questions. in table 1 we show the improvement observed for factoid questions from previous trec qa tracks. we produced ten candidate answers and computed both mean reciprocal rank  mrr  and the number correct  #corr  at rank 1. given the improvements that this technique provided at development time  we expected comparable gains to occur at evaluation time as well.
table 1: effectiveness of web validation on previous qa data
trec 1  1 trec1 #corrmrr#corrmrrbaseline1.1.1web-validated1.1.1 improvement  +1%  +1%  +1%  +1% 1. system evaluations
1. description of results
for trec-1  we opted to submit three separate systems. the first system   nsaqactis1   was our baseline system which represented the output fusion of the knowledge-graph and cfa strategies. cfa provided the answers to definition and  how many  style questions  and the knowledge graph produced the other types of results. the second system   nsaqactis1   was the same as the first except web validation was applied afterwards. the third system   nsaqactis1  used only the knowledge graph and web validation to answer the questions. the performance of the trec 1 evaluation is provided in table 1. to our surprise  our baseline  #1  system performed approximately as well as our trec 1 baseline had performed. we had hoped that a comparable web-validation improvement would have been seen as well  but to our dismay  web validation actually resulted in a loss of performance on non-list questions  but there was a reasonable gain using web validation on lists. we were also delighted to have  other  style questions perform very well given that this was a last-minute and untested feature of our system. in the sections that following  we provide a brief analysis of these results to include an indication of where things went as well or better than expected and where the system failed to respond as desired.
table 1: trec 1 performance
strategyfactoidlistotherallknowledge graph+
filter cascade  #1 1111system #1+
web validation  #1 1111knowledge graph+
web validation  #1 11111. times when things go well perhaps the component of our system whose performance we are most pleased with is that of  other  answering. early in the paper  we mentioned that just prior to the trec evaluation  qactis had very little capacity to handle  other  style questions. through the knowledgeinduction search we did have limited capacity of declaring hypernyms of the entities needing definition to be defining statements about those entities  but we did not have the ability to handle definitions in the encyclopedic style that trec evaluates. for interest sake  we did submit our hypernym output as one version  #1  of definitions  but we had interest in creating definitions that would be more aligned with the evaluation. in last year's trec evaluations  definitions that consisted of well-chosen sentences seemed to outperform most other types of definitions  so our throught was to build some comparable mechanism. the cfa strategy works primarily from well-chosen sentences  so it seems well-suited to building up our definer. definitional questions were therefore handled by modifying the sentence extractor filter  1.1  used in cfa. in particular  sentences from the top n documents were scanned for matches to the target topic and those sentences were saved off. no attempt was made to avoid redundancy with prior questions from the same session. a simple filter was then applied that accepted only those sentences as answer components that had lengths greater than 1 and less than 1 words. the number of sentences per topic was capped at 1.
   table 1 suggests that this strategy seemed to work reasonably well. in our best-scoring run  the baseline  #1   the answers were on average about 1 characters long and had an average precision of 1 and an average recall of 1. since recall played a significant role in performance  and since long answers are well tolerated  this led to the reasonable score of 1 which was better than we had hoped to achieve.
    as a matter of comment  the third system  which scored poorly was based purely on lists of potential hypernyms. in this case  the average answer length was reduced to 1 characters and the precision dropped somewhat to 1. yet the average recall -- the most important component of the evaluation -- was very
low...only 1. 
1. difficulties: the unexpected
1.1. series-style question problems
we had expected that our module to convert series-style questions into independent questions would work satisfactorily. there were a number of questions where the system had significant difficulties  however.
   in the 1th squestion-session  the target was  the band nirvana  and the second and third questions of that session asked about the band members and the band's formation. the resolver recognized that  band  and  band nirvana  were the same but did not make the corresponding substitutions into the independent questions. thus  the question did not indicate what kind of band was being looked for and all corresponding answers were missed.
   in this same series  the next questions that were posed used the terms  their  and  they.  the resolver assumed that since the former question mentioned band members whereas the topical 'band' is a singular entity   their  and  they  must refer to the answer of the second question in the series. thus  instead of using  nirvana  as a substitution for the anaphora  it used what it thought mught be an answer to the group members question  and it thus substituted  desmond chase  for  they  and  their. 
   these two phenomena  namely  failing to make the substitution when part of the topic was found in the question and using answers to previous series questions to resolve the anaphors  was a condition which occurred a total of 1 times throughout the question collection.
1.1. inexact answers
another difficulty for our system was that by and large  until the evaluation  we paid little heed to the actual documents where answers appeared and we did not concern ourselves heavily on whether the cleanest answer  i.e.  an exact answer  was presented -- but only if the right kind of answer was identified. the evaluation  however  was concerned with providing document support and answer exactness.
   the first of these issues is rather complicated to resolve in a system which tries to identify answers across documents -- which document did the answer actually appear in  to prepare for the evaluation  we had to equip the system to know  to the best of its ability  where the original answer came from. these modifications were reasonably successful in that only one of the otherwise correct answers we returned was declared to be unsupported.
   on the other hand  the requirement for answer exactness resulted in a high penalty for us. our best system only produced 1 correct answers. yet the system produced 1 answers which were deemed to be inexact  thus resulting in significant damage to our overall performance.
1.1. experiments and test in web validation
as mentioned  we applied answer validation on two of three trec 1 submissions. although the method was untested  we also attempted to validate responses to list questions as with factoids.
table 1: comparing performance by question type
totalw/o validationw/ validationabbrev11amount11location11miscellany11process11time11what11who1111location questions fared worse with web-validation and 'what' and 'who' questions were less popular in this year's set of questions. these differences in the distribution of question types might account for the observed differences.
   for list questions we returned as many as 1 responses after considering up to 1 possibilities from the baseline system  more or less possibilities may have been provided . here web-validation had a significant positive effect  improving the f-score from 1  without validation  to 1.
1  future directions
the results we have presented represent our first attempt to participate in the trec qa evaluation. across the course of the next year  we expect that our system will improve as we add functionality for better resolving anaphora  so this will be one of early expansions. moreover  as we have analyzed documents  we have noted that a significant number of potential answers are not observed since inference is required in order to find them. this  then  will be our next area of concern. lastly  since multimedia and multilingual qa is of interest to us  we expect to enable the capabilities we have shown here in spanish and began to try them on non-text media.
1 acknowledgments
the authors would like to thank carol vaness-dykema for her efforts to help establish and oversee the foundations of the qactis effort. additional appreciation goes to john prange and the aquaint program for providing funding for portions of the work involved in this effort.
