microarray datasets typically contain large number of columns but small number of rows. association rules have been proved to be useful in analyzing such datasets. however  most existing association rule mining algorithms are unable to efficiently handle datasets with large number of columns. moreover  the number of association rules generated from such datasets is enormous due to the large number of possible column combinations.
모in this paper  we describe a new algorithm called farmer that is specially designed to discover association rules from microarray datasets. instead of finding individual association rules  farmer finds interesting rule groups which are essentially a set of rules that are generated from the same set of rows. unlike conventional rule mining algorithms  farmer searches for interesting rules in the row enumeration space and exploits all user-specified constraints including minimum support  confidence and chi-square to support efficient pruning. several experiments on real bioinformatics datasets show that farmer is orders of magnitude faster than previous association rule mining algorithms.
1.	introduction
모with recent advances in dna chip-based technologies  we can now measure the expression levels of thousands of genes in cell simultaneously resulting in a large amount of highdimension data. these microarray datasets typically have a large number of columns but a small number of rows. for example  many gene expression datasets may contain up to 1-1 columns but only 1 rows.
모recent studies have shown that association rules are very useful in the analysis of microarray data. due to their relative simplicity  they are more easily interpreted by biologists. association rules can be applied in the following two

 contact author
 this work was supported in part by nus arf grant r1-1 and r1-1.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigmod 1 june 1  1  paris  france.
copyright 1 acm 1-1/1 . . . $1.
scenarios:  1  it is shown in  1  1  that classifiers built from association rules are rather accurate in identifying cancerous cell;  1  it is suggested in  that association rules can be used to build gene networks since they can capture the associations among genes.
모in this paper  we focus on a special type of association rule which takes the form of lhs 뫸 c  where lhs is a set of items and c is a class label. we use the term  support of a  to refer to the number of rows containing a in the database and denote this number as sup a . the probability of the rule being true is referred to as  the confidence of the rule  and is computed as sup lhs뫋c /sup lhs . the number of rows in the database that match the rule is defined as  the support of the rule . user-specified constraints such as minimum support  a statement of generality  and minimum confidence  a statement of predictive ability  are often imposed on mining such association rules.
모microarray datasets pose a great challenge for existing rule mining algorithms in both runtime and the number of discovered rules. while there are a large number of algorithms that have been developed for association rule mining  1  1  1  1   their basic approaches are all column enumeration in which combinations of columns are tested systematically to search for association rules. such an approach is unsuitable for microarray datasets. this is because if i is the maximum length of a row in a dataset  the search space based on column enumeration could be as large as 1i. previous column enumeration methods work well for datasets with small average row length  usually i   1 . however  for micorarray datasets  i can be in the range of tens of thousands. these high-dimension bioinformatics datasets with thousands of columns render most of the existing algorithms impractical.
모on the other hand  the number of rows in such datasets is typically in the order of hundreds to a thousand. if m is the number of rows  the size of the row enumeration space will be 1m. in our application domain  e.g.  microarray datasets   the size of the row enumeration space is much less than the size of the column enumeration space. therefore  it seems reasonable to devise the algorithm that does not perform column enumeration but row enumeration. to the best of our knowledge  none of existing studies investigate the possibilities of discovering rules by row enumeration.
모a large number of long frequent itemsets may be discovered from microarray datasets due to their large number of columns. as a result  large number of association rules may be generated due to the combinatorial explosion of frequent itemsets . for example  given a dataset with one row  five columns and one class label: {a b c d e cancer}  we could have 1 rules of the form lhs 뫸 cancer since any combination of a  b  c  d and e could be the lhs for the rule. these 1 rules all cover the same row and have the same confidence  1% . such a large set of rules contains a lot of redundancy and is difficult to interpret. instead of generating all 1 rules  we propose to discover these rules as a rule group whose consequent is cancer  and which can be identified by a unique upper bound plus a set of lower bounds. the upper bound of a rule group is the rule with the most specific lhs among the rules. in our example  the upper bound rule is abcde 뫸 cancer. the lower bounds of the rule group are the rules with the most general lhs in the rule group. for our example  the rule group has 1 lower bounds  a 뫸 cancer  b 뫸 cancer  c 뫸 cancer  d 뫸 cancer  and e 뫸 cancer . given the upper bound and the lower bounds of the rule group  other rules within the group can be easily derived.
모we further reduce the number of rules by finding interesting rule groups only. consider two rules abcd 뫸 cancer with confidence 1% and ab 뫸 cancer with confidence 1%  it is obvious that ab is a better indicator of cancer since ab 뫸 cancer has a higher confidence and all rows covered by abcd 뫸 cancer must be covered by ab 뫸 cancer. with ab 뫸 cancer  rule abcd 뫸 cancer is not interesting1.
모in this paper  we describe a novel algorithm farmer 1  that is specially designed to mine interesting rule groups from microarray datasets. farmer discovers upper bounds of interesting rule groups by performing depth-first rowwise enumeration instead of the usual column-wise approach taken by existing rule mining algorithms. this basic idea is combined with efficient search pruning strategies based on user-specified thresholds  minimum support  minimum confidence and minimum chi-square value   yielding a highly optimized algorithm. we also describe an efficient algorithm for computing the lower bounds. our experiments show that farmer substantially outperforms other rule mining algorithms described in    charm  and  closet+  to further illustrate the usefulness of the discovered interesting rule groups in biology  we build a simple classifier based on these interesting rule groups  which outperforms the wellknown cba  and svm  on 1 real-life datasets.
모the rest of this paper is organized as follows: in the next section  we will introduce some preliminaries and give our problem definitions. the farmer algorithm will be explained in section 1. experimental results will be given in section 1 on real-life microarray datasets. section 1 introduces some of the related work for this paper. we will conclude our discussion in section 1.
1.	preliminary
모in this section  we introduce some basic notations and concepts that are useful for further discussion.
1	the basics
dataset: the dataset  or table  d consists of a set of rows 
r={r1  ...  rn}. let i={i1 i1 ... im} be the complete set of items of d  and c = {c1 c1 ... ck} be the complete set of class labels of d  then each row ri 뫍 r consists of one or more items from i and a class label from c.
모as an example  figure 1 a  shows a dataset where items are represented with alphabets from 'a' to 't'. there are altogether 1 rows  r1 ... r1  in the dataset  the first three of which are labeled c while the other two are labeled  c. to simplify the notation  we use the row id set to represent a set of rows and the item id set to represent a set of items. for instance   1  denotes the row set {r1 r1 r1}  and  acf  denotes the itemset {a c f}.
모given a set of items i1   i  we define the row support set  denoted r i1    r  as the largest set of rows that contain i1. likewise  given a set of rows r1   r  we define item support set  denoted i r1    i  as the largest set of items that are common among the rows in r1.
example 1.	r i1  and i r1 
consider again the table in figure 1 a . let i1 be the itemset {a e h}  then r i1  = {r1 r1 r1}. let r1 be the row set {r1 r1}  then i r1 ={a e h} since this is the largest itemset that occurs in both r1 and r1. 1
association rule: an association rule 붺  or just rule for short  from dataset d takes the form of a 뫸 c  where a   i is the antecedent and c is the consequent  here  it is a class label . the support of 붺 is defined as the |r a뫋c |  and its confidence is |r a 뫋 c |/|r a |. we denote the antecedent of 붺 as 붺.a  the consequent as 붺.c  the support as 붺.sup  the confidence as 붺.conf and the chi-square value is 붺.chi.
모as discussed in the introduction  in real biological applications  people are often interested in rules with a specified consequent c that meet specified thresholds  like minimum support and minimum confidence.
1	interesting rule groups  irgs 
모the interesting rule group is a concept which helps to reduce the number of rules discovered by identifying rules that come from the same set of rows and clustering them conceptually into one entity.
definition 1.	rule group
let d be a dataset with itemset i and c be a specified class label. g = {ai 뫸 c|ai   i} is a rule group with antecedent support set r and consequent c  iff  1   ai 뫸 c 뫍 g  r ai  = r  and  1   r ai  = r  ai 뫸 c 뫍 g. rule 붺u 뫍 g  붺u: au 뫸 c  is an upper bound of g iff there exists no 붺1 뫍 g  붺1:a1 뫸 c  such that a1   au. rule 붺l 뫍 g  붺l: al 뫸 c  is a lower bound of g iff there exists no 붺1 뫍 g  붺1: a1 뫸 c  such that a1   al. 1
lemma 1. given a rule group g with the consequent c
and the antecedent support set r  it has a unique upper bound 붺  붺: a 뫸 c .
proof: assume there exists another upper bound 붺1 a1 뫸 c  뫍 g such that a1 =1 a and a1  a. let a1 = a 뫋 a1. because of r a1  = r a  = r  we get r a1  = r  and then a1 뫸 c 뫍 g and a1   a. therefore  붺 a 뫸 c  cannot be an upper bound of g. so the upper bound of a rule group must be unique.	1
모based on lemma 1  a rule group g can be represented with its unique upper bound 붺u.
ijr ij c ca1 11b11c1d11e11f1g1h11l11o1p11q11r11s11t11iriclass1a b c l o sc1a d e h p l rc1a c e h o q tc1a e f h p r c1b d f g l q s t c a  example table
 b  transposed table  tt
figure 1: running example
ijr ij c ca1 1e11h11figure 1: tt|{1}
example 1.	rule group
a running example is shown in figure 1 in which r {e}  = r {h}  = r {ae}  = r {ah}  = r {eh}  = r {aeh}  =
{r1 r1 r1}. they make up a rule group {e 뫸 c h 뫸 c ... aeh 뫸 c} of consequent c  with the upper bound aeh 뫸 c and the lower bounds e 뫸 c and h 뫸 c. 1
모it is obvious that all rules in the same rule group have the same support  confidence and chi-square value since they are essentially derived from the same subset of rows. based on the upper bound and all the lower bounds of a rule group  we can identify its remaining members according to the lemma below.
lemma 1. suppose rule group g with the consequent c and antecedent support set r has an upper bound au 뫸 c
and a lower bound al 뫸 c. rule 붺 a 뫸 c   where a   au and a   al  must be a member of g. proof: since a   au  r a    r au . likewise  r a   
r al . since r al  = r au  = r  r a  = r. so 붺 a 뫸
c  belongs to g.	1
definition 1. interesting rule group  irg  a rule group g with upper bound 붺u is an interesting rule group iff for any rule group with upper bound 붺u1   붺u  붺u1 .conf   붺u.conf. for brevity  we will use the abbreviation irg to refer to interesting rule group. 1
모our algorithm farmer is designed to find irgs that satisfy user-specified constraints including minimum support  minimum confidence and minimum chi-square value 1. farmer finds the upper bounds of all irgs first  and then gathers their lower bounds. this makes it possible for users to recognize all the rule group members as and when they want to.
1.	the farmer algorithm
모to illustrate our algorithm  we first give a running example  figure 1 . table tt  figure 1 b   is a transposed version of the example table  figure 1 a  . in tt  the items become the row ids while the row ids become the items. a row id rm in the original table will appear in tuple in of tt if and only if the item in occurs in the row rm of the original table. for instance  since item d occurs in row r1 and r1 of the original table  row ids  1  and  1  occur in tuple d of tt. to avoid confusion  we hereafter refer to the rows in the transposed table as tuples while referring to those in the original table as rows.
모we provide a conceptual explanation of farmer algorithm to discover upper bounds of interesting rule groups in section 1  the pruning strategies in section 1  and the implementation details in section 1. in section 1  we describe subroutine minelb of farmer to discover the lower bounds of interesting rule groups.
1	enumeration
모unlike existing column-wise rule mining algorithms which perform their search by enumeration of columns   farmer performs search by enumeration of row sets to find interesting rule groups with consequent c. figure 1 illustrates the enumeration tree which represents the search of farmer conceptually for the interesting rule groups in the absence of any pruning strategies. each node x of the enumeration tree corresponds to a combination of rows r1 and is labeled with i r1  that is the antecedent of the upper bound of a rule group identified at this node. for example  node  1  corresponds to the row combination {r1 r1} and  al  indicates that i {r1 r1}  = {a l}. an upper bound al 뫸 c can be discovered at node  1 . this is correct because of the following lemma.
lemma 1. let x be a subset of rows from the original table  then i x  뫸 c must be the upper bound of the rule group g whose antecedent support set is r i x   and consequent is c.
proof: first  according to definition 1  i x  뫸 c belongs to rule group g with antecedent support set r i x   and consequent c. second  assume that i x  뫸 c is not the upper bound of g  then there must exist an item i such that i /뫍 i x   and i x  뫋 {i} 뫸 c belongs to g. so we get r i x   = r i x  뫋 {i} . since rows in x contain all items of i x   we get x   r i x    and then x   r i x  뫋 {i} . this means that i is also found in every row of x  which contradicts the definition that i x  is the largest set of items that are found in every row of x. so i x  뫸 c is the upper bound of the rule group with antecedent support set r i x  . 1
모farmer performs a depth-first search on the enumeration tree by moving along the edges of the tree. by imposing an order ord  in which the rows with consequent c are ordered before the rows without consequent c this is done to support efficient pruning which will be explained later   we are able to perform a systematic search by enumerating the combinations of rows based on the order ord. for example  let  1   1   1   1   1  according to ord  the order of search in figure 1 will be { 1    1    1    1    1    1  ...  1    1 } in absence of any optimization and pruning strategies. note that the order also serves for confidence pruning purpose  explained in section 1.1 .

figure 1: the row enumeration tree.
모next  we prove that the complete rule groups can be discovered by a complete row enumeration.
lemma 1. by enumerating all possible row combinations on the row enumeration tree  we can obtain the complete set of upper bounds and the corresponding complete set of rule groups in the dataset.
proof: with lemma 1  we know that each rule group can be represented by a unique upper bound. based on the definition of rule group  definition 1   all possible antecedent support sets of rule groups can be obtained by enumerating all possible row combinations. each antecedent support set x corresponds to a rule group with upper bound  i x  뫸 c .
hence the proof.	1
모it is obvious that a complete traversal of the row enumeration tree is not efficient. various pruning techniques will be introduced to prune off unnecessary searches in the next section. we will next introduce the framework of our algorithm for discovering the upper bounds of rule groups. we first introduce two concepts.
definition 1. conditional transposed table  tt|x  given the transposed table tt  used at the root of the enumeration tree   a x-conditional transposed table  tt|x  at node x  x is the row combination at this node  is a subset of tuples from tt such that for each tuple t of tt that t   x  there exists a tuple t1 = t in tt|x. 1
example 1. let tt be the transposed table in figure 1 b  and let x = {1}. the x-conditional transposed table 
tt|x is shown in figure 1.	1
definition 1. enumeration candidate list  tt|x.e  let tt|x be the x-conditional transposed table and rmin 뫍 x be the row id with the lowest ord order in row combination x. let ep = {r|r  ord rmin 뫇 r 뫍 r c }  all rows of consequent c ordered after rmin   and en = {r|r  ord rmin 뫇 r 뫍 r  c }  all rows with class c ordered after rmin . the enumeration candidate list for tt|x  denoted as tt|x.e  is defined to be ep 뫋 en. 1
notationdescriptiontt|x.eenumeration candidates;tt|x.epenumeration candidates with label c;tt|x.enenumeration candidates without label c;tt|x.yenumeration candidates that occur in each tuple of tt|x.figure 1: notations for conditional transposed table
모in the rest of this paper  we will use the notations in figure 1 to describe various operations on the conditional transposed table  tt|x.
모our formal algorithm is shown in figure 1. farmer involves recursive computations of conditional transposed tables by performing a depth-first traversal of the row enumeration tree. each computed conditional table represents a node in the enumeration tree of figure 1. for example  the {1}-conditional table is computed at node  1 . after initialization  farmer calls the subroutine mineirgs to recursively generate x-conditional tables.
모the subroutine mineirgs takes in four parameters at node x: tt1|x  supp  supn and irg. tt1|x is the xconditional transposed table at node x with enumeration candidates tt1|x.ep and tt1|x.en. supp is the number of identified rows that contain i x  뫋 c while supn is the number of identified rows that contain i x  뫋  c before scanning tt1|x. irg stores the upper bounds of interesting rule groups discovered so far.
모steps 1  1  1 and 1 in the subroutine mineirgs perform the pruning. they are extremely important for the efficiency of farmer algorithm and will be explained in the next subsection. step 1 scans the table tt1|x. step 1 moves on into the next level enumerations in the search tree. step 1 checks whether i x  뫸 c is the upper bound of an irg that satisfies the user-specified constraints before inserting it into irg. note that step 1 must be performed after step 1  the reason will be clear later . we first prove the correctness of the two steps by two lemmas as follows:
lemma 1.	tt|x|ri = tt|x+ri  ri 뫍 tt|x.e.	1
모lemma 1 is useful for explaining step 1. it simply states that a x +ri conditional transposed table can be computed from a x conditional transposed table tt|x in the next level search after node x.
모lemma 1 ensures that at step 1 only upper bounds of rule groups are possibly inserted into irg. to determine whether an upper bound 붺 discovered at node x represents an interesting rule group satisfying user-specified constraints  we need to compare 붺.conf with all 붺1.conf  where 붺1.a   붺.a and 붺1 satisfies user specified constraints. farmer ensures that all such 붺1 have already been discovered and kept in irg at step 1 by lemma 1 below.
lemma 1. let 붺 : i x  뫸 c be the upper bound rule discovered at node x. the rule group with upper bound 붺1 : a1 뫸 c such that a1   i x  can always be discovered at the descendent nodes of node x or in an earlier enumeration. proof: since a1   i x   and 붺1 and 붺 are the upper bounds of two different rule groups  we see r a1    r i x     x. let rs = {r|r 뫍 r a1  뫇 r /뫍 x} and rmin 뫍 x be the row with the lowest ord rank in row set x. if  r1 뫍 rs such that r1   rmin  then node r a1  is traversed before node x; otherwise node r a1  is traversed at a descendent node of node x.	1
algorithm farmer
input: table d  specified consequent c  minsup  minconf  and minchi.
output: interesting rule groups with consequent c satisfying minimum measure thresholds. method:
1. initialization: let tt be the transposed table of ord ordered d; irg =  .
1. mine interesting rule groups:	mineirgs tt|   1  1  irg .
1. mine lower bounds of interesting rule groups: optional.

subroutine: mineirgs tt1|x  supp  supn  irg . parameters:
tt1|x: a x-conditional transposed table; supp and supn: support parameters;
모모모irg: the set of discovered interesting rule groups; method:
1. apply pruning 1: if i x  뫸 c is already identified  then return.
1. apply pruning 1: if prunable with the loose upper bounds of support or confidence  then return.
1. scan tt1|x and count the frequency of occurrences for each enumeration candidate  ri 뫍 tt1|x.e  let up   tt1|x.ep be the set of rows from tt1|x.ep which occur in at least one tuple of tt1|x; let un   tt1|x.en be the set of rows from tt1|x.en which occur in at least one tuple tt1|x; let yp   tt1|x.ep be the set of rows from tt1|x.ep found in every tuple of tt1|x;
let yn   tt1|x.en be the set of rows from tt1|x.en
found in every tuple of tt1|x; supp = supp + |yp|  |r i x  뫋 c | ; supn = supn + |yn|  |r i x  뫋  c | ;
1. apply pruning 1: if prunable with one of the three tight upper bounds  then return.
1. apply pruning 1: update enumeration candidate list  tt1|x.ep = up   yp  tt1|x.en = un   yn.
1. for each ri 뫍 tt1|x.e do if ri 뫍 r c  then
	tt1	|	.e	{	|	tt1	.e
;
a = supp + 1; b = supn;
else
tt1|x r .ep =  ; tt1 x r .en = r r 뫍
a = supp; b = supn + 1;
mineirgs tt1|x|ri a b irg ;
1. let conf =  supp / supp + supn ; if  supp 뫟 minsup  뫇  conf 뫟 minconf 뫇  chi supp  supp+ supn  뫟 minchi  then if  붺   붺 뫍 irg  뫇  붺.a   i x      conf   붺.conf  then add upper bound rule i x  뫸 c into irg.

figure 1: the farmer algorithm
모step 1 is done after step 1 to ensure that all descendant nodes of x are explored before determining whether the upper bound rule 붺 at x is an irg. together with lemma 1  we know that the complete and correct set of interesting rule groups will be in irg.
모note that step 1 implicitly does some pruning since it is possible that the enumeration candidate list is empty  i.e. tt1|x.e =  . it can be observed from the enumeration tree that there exist some combinations of rows  x  such that i x  =    an example is node  1  . this implies that there is no item existing in all the rows in x. when this happens  tt1|x.e is empty and no further enumeration will be performed.
1	pruning strategies
모we next look at the pruning techniques that are used in farmer  which are essential for the efficiency. our emphasis here is to show that our pruning steps do not prune off any interesting rule groups while preventing unnecessary traversals of the enumeration tree. combining this with our earlier explanations on how all interesting rule groups are enumerated in farmer without the pruning steps  the correctness of our algorithm will be obvious.
1.1	pruning strategy 1
모pruning strategy 1 is implemented at step 1 of mineirgs by pruning tt|x.y   the set of enumeration candidate rows that occur in all tuples of the tt|x. we partition tt|x.y to two subsets: yp with consequent c and yn without. the intuitive reason for the pruning is that we obtain the same set of upper bound rules along the branch x without such rows. the correctness of such a pruning strategy is due to the following lemma.
lemma 1. let tt1|x be a x-conditional transposed table. given any subset r1  r1   tt1|x.e  we have i x 뫋 r1  = i x 뫋 tt1|x.y 뫋 r1 .
proof: by definition  i x 뫋 r1  contains a set of items which occur in every row of  x 뫋 r1 . suppose candidate y 뫍 tt1|x.y  y occurs in every tuple of tt1|x   then either y 뫍 x 뫋 r1  if y 뫍 r1  or y occurs in every tuple of the tt1|x뫋r1  if y /뫍 r1 . in either case  i x 뫋 r1  = i x뫋r1뫋{y} . thus  i x뫋r1  = i x뫋tt1|x.y 뫋r1 . 1
모with lemma 1  we can safely delete the rows in tt1|x.y from the enumeration candidate list tt1|x.e.
example 1. consider tt|{1}  the conditional transposed table in figure 1. since enumeration candidate row 1 occurs in every tuples of tt|{1}  we can conclude that i {1}  = i {1 1}  = {a e h}. thus  we need not traverse node  1  and create tt|{1 1}. row 1 can be safely deleted from tt|{1}.e.	1
모since i {1 1}  = i {1}   the upper bound rule is identified at node  1  and node  1  is redundant. we say that node  1  is compressed to node  1 .
모we argue here that lemma 1 still holds after applying pruning strategy 1. without applying pruning strategy 1  for each node x  a1 뫸 c  where a1   i x   is identified at a node x1  which is traversed before node x or is a descendent node of node x. with pruning strategy 1  x1 might be compressed to a node x1  x1   x1 and i x1  = i x1  = a1   and we can see node x1 is either traversed before the subtree rooted at node x  or inside this subtree.
1.1	pruning strategy 1
모this pruning strategy is implemented at step 1 of mineirgs. it will stop searching the subtree rooted at node x if the upper bound rule i x  뫸 c was already discovered previously in the enumeration tree because this implies that any rules to be discovered at the descendants of node x would have been discovered too.
lemma 1. suppose pruning strategy 1 is utilized in the search. let tt1|x be the conditional transposed table of the current node x. all upper bounds to be discovered in the subtree rooted at node x must have already been discovered if there exists such a row r1 that satisfies the following conditions:  1 r1 뫍/ x;  1 r1 뫍/ tt1|x.e;  1 for any ancestor node xi of node x  r1 뫍/ tt|xi.y  pruned by strategy 1 ; and  1 r1 occurs in each tuple of tt1|x.
proof: let x = {r1 r1 ... rm}  where r1  ord r1  ord ...  ord rm. suppose that there is a node x1 x1 = x 뫋
{r1}   we can have the following properties:  1  i x  =
  since r1 뫍/ tt1|x.e and r1 뫍/ x;  1  tt |x.e = tt |x1.e.
모x1 is either enumerated or compressed to a node xc  where i xc  = i x1  and tt1|x1.e   tt1|xc.e. we can prove that either node x1 or node xc is traversed before node x by considering the following two cases:  1  if r1  ord r1  node x1 or node xc falls in the subtree rooted at node {r1}  which is traversed before node x.  1  if row ids in x1 follow the order r1  ord r1  ord ...  ord rt  ord r1  ord rt+1  ord ...  ord rm  node x1 or
node xc falls in the subtree rooted at node x1 = {r1 ... rt r1}  which is also traversed before node x. because tt1|x.e = tt1|x1.e and tt1|x1.e   tt1|xc.e  we can conclude that all upper bounds to be discovered in the subtree rooted at node x must have already been discovered earlier in the subtree rooted at node x1 or node xc. 1
모in the implementation of pruning strategy 1  the existence of such a r1 can be efficiently detected by a process called back counting without scanning the whole tt1|x. details are explained in section 1.
example 1. consider node  1  in figure 1 where the upper bound rule {a e h} 뫸 c is identified for the first time. when it comes to node  1   we notice that row  1  occurs in every tuple of tt|{1}   1  뫍/ tt|{1}.e  and  1  뫍/ tt|{1}.y . so we conclude that all upper bounds to be discovered down node  1  have already been discovered before  i {1}  = i {1}  = {a e h}. i {1 1}  =   . we can prune the search down node  1 . 1
1.1	pruning strategy 1
모pruning strategy 1 performs pruning by utilizing the userspecified thresholds  minsup  minconf and minchi. we estimate the upper bounds of the measures for the subtree rooted at the current node x. if the estimated upper bound at x is below the user-specified threshold  we stop searching down node x. a important thing to note here is that our pruning strategy is highly dependent on the order ord which rank all rows with consequent c before rows with consequent  c.
모pruning strategy 1 consists of 1 parts: pruning using confidence upper bound  pruning using support upper bound and pruning using chi-square upper bound. this strategy is executed separately at step 1 and step 1  figure 1 . at step 1  we will perform pruning using the two loose upper bounds of support and confidence that can be calculated before scanning tt1|x. at step 1 we calculate the three tight upper bounds of support  confidence and chi-square value after scanning tt1|x.
모for clarity  we will use the notations in figure 1.1 to explain our pruning strategy here.
notationdescriptionxthe current enumeration node;붺the upper bound rule i x  뫸 c at node x;x1the immediate parent node of x;붺1the upper bound rule i x1  뫸 c at node x1;rma row id such that tt|x = tt|x1|rm;figure 1: notations for search pruning
pruning using support upper bound
we have two support upper bounds for the rule groups identified at the subtree rooted at node x: the tight support upper bound us1  after scanning tt1|x  and the loose support upper bound us1  before scanning tt1|x . if the estimated upper bound is less than the minimum support minsup  the subtree can be pruned.
if rm has consequent c:
us1 = 붺1.sup + 1 + max |tt1|x.ep 뫌 t|   t 뫍 tt1|x; us1 = 붺1.sup + 1 + |tt1|x.ep|;
if rm has consequent  c then us1 = us1 = 붺1.sup;
lemma 1. us1 and us1 are the support upper bounds for the upper bound rules discovered in subtree rooted at node x.
proof: because of the ord order  definition 1   if the consequent of rm is  c  the enumeration candidates of nodes down node x will also have consequent  c. the support can not increase down node x  so the support of upper bounds discovered in the subtree rooted at node x is less than 붺1.sup. if rm has consequent c  for node x and its descendent nodes  the maximum increase of support from 붺1.sup must come from the number of enumeration candidates with consequent c  |tt1|x.ep|  at node x plus 1  1 for rm  us1   or more strictly  from the maximum number of enumeration candidates with consequent c within a tuple of tt1|x  max |tt1|x.ep 뫌 t|   t 뫍 tt1|x  plus 1  us1 . 1
모note that we need to scan tt1|x to get us1 while us1 can be obtained directly from the parameters supp and x passed by the parent node.
pruning using confidence upper bound
similarly  we estimate two confidence upper bounds for the subtree rooted at node x  the tight confidence upper bound uc1 and the loose confidence upper bound uc1. if the estimated upper bound is less than minimum confidence minconf  the subtree rooted at node x can be pruned.
모given us1 and us1  the two confidence upper bounds of subtree rooted at node x  uc1 tight  and uc1 loose   are:
uc1 = us1/ us1 + |r 붺.a 뫋  c | ;
uc1 = us1/ us1+|r 붺1.a뫋 c |   rm has consequent c ;
모uc1 = us1/ us1 +|r 붺1.a뫋 c |+1   rm has consequent  c .
	1	implementation
모in the implementation of farmer  we use memory pointers  to point at the relevant tuples in the in-memory transposed table to simulate the conditional transposed table. our implementation assumes that despite the high dimensionality  the microarray datasets that we are trying to handle are still sufficiently small to be loaded completely into the main memory. this is true for many gene expression datasets which have small number of rows.
모following is the running example. suppose the current node is node  1   figure 1 a    and minsup = 1. the inmemory transposed table is shown on the right hand side of the figure. memory pointers are organized into conditional pointer lists.
figure 1: the possible chi-square variables
 c |. thus the minimum value for y is |r 붺.a 뫋  c | or loosely at |r 붺1.a 뫋  c | + 1 if rm has no consequent c  and |r 붺1.a 뫋  c |  if rm has consequent c . 1
example 1. suppose minimum confidence minconf = 1%. at node  1   the discovered upper bound rule is  a 뫸 c  with confidence 1   1. since row 1 has no consequent c  any descendent enumeration will only reduce the confidence. thus we can stop next level searching.
pruning using chi-square upper bound
the chi-square value of an association rule is the normalized deviation of the observed values from the expected values.
모let 붺 be a rule in the form of a 뫸 c of dataset d  n be the number of rows in d  and m be the number of instances with consequent c in d. the four observed values for chi-square value computation are listed in the following table. for example  oa c represents the number of rows that contain a but do not contain c. let x = oa and y = oac. since m and n are constants  the chi-square value is determined by x and y only and we get chi-square function chi x y .모in figure 1 a   the  1 -conditional pointer list  at the top left corner of the figure  has 1 entries in the form of   fi pos   which indicates the tuple  fi  that contains r1 and the position of r1 within the tuple  pos . for example  the entry   a 1   indicates that row r1 is contained in the tuple 'a' at position 1. we can extend the  1 -conditional transposed table tt1|{1} by following the pos. during one full scan of the transposed table  farmer also generates the conditional pointer lists for other rows  i.e. r1  r1  r1 and r1 . however  the generated  1 -conditional pointer list is slightly different in that it contains an entry for each tuple that contains r1 but not r1. for example  although the tuple 'a' contains r1  it does not appear in the  1 conditional pointer list. it will be inserted subsequently as we will see later.
모a further scan through the  1 -conditional pointer list will allow us to generate the  1    1    1  and  1  conditional pointer lists. figure 1 b  shows the state of memory pointers when we are processing node {1}.
모finally  we show the state of conditional pointer lists after node {1} and all its descendants have been processed  figure 1 c  . since all enumerations involving row r1 have been either processed or pruned off  the entries in the  1 conditional pointer list are moved into the remaining conditional pointer lists. the entries in the  1 -conditionalc ctotalaoac = yoa coa = x ao aco a co a = n   xtotaloc = mo c= n   mnpointer list will be moved to the other conditional pointer lists after node {1} and its descendants are processed  and so on.
모the following lemma gives an estimation of upper bound of chi square value for rules down the node x.
lemma 1.	suppose rule 붺 is discovered at enumeration node x. the chi-square upper bound for the upper bound rules discovered at the subtree rooted as node x is: max{모throughout all the enumerations described above  we need to implement our three pruning strategies. the implementation of strategies 1 and 1 is straightforward. for pruning strategy 1  we do a back scan through the conditional list to see whether there exists some row that satisfies the condition of lemma 1. for example at node  1  in figurechi x 붺    붺 +m m   chi   붺 +n   붺    chi x 붺   붺   . 1 c   we scan from the position of each pointer to the head proof:   is identified in the sub- of each tuple  instead of scanning the transposed table fromlemma 1. uc1 and uc1 are the confidence upper bounds for the rules discovered in the subtree rooted at node x. proof: for a rule 붺1 discovered in subtree rooted at node x  its confidence is computed as |r 붺1.a 뫋 c |/ |r 붺1.a 뫋 c |+|r 붺1.a뫋 c | . this expression can be simplified as x/ x+y   where x = |r 붺1.a뫋c | and y = |r 붺1.a뫋 c |. this value is maximized by choosing the maximum value for x  us1 and us1  and minimum value for y. suppose rule 붺 is discovered at node x. for any rule 붺1 discovered under the enumeration tree under node x  붺1.a   붺.a because of pruning strategy 1  so we can see |r 붺1.a뫋 c | 뫟 |r 붺.a뫋 tree rooted at node x  x1 = oa1 and y1 = oa1c. since o a  = |r a | and a1   a. the followings are satisfied.
1  x 뫞 x1 뫞 n  |r a | 뫞 |r a1 | 
1  y 뫞 y1 뫞 m  |r a 뫋 c | 뫞 |r a1 뫋 c | 
1  y1 뫞 x1  |r a1 뫋 c | 뫞 |r a1 | 
1  n m 뫟 x1  y1 뫟 x y  |r a1 뫋 c | 뫟 |r a뫋 c | 
the value pair  x1 붺1  y1 붺1   falls in the gray parallelogram  x 붺  y 붺     x 붺  y 붺 +m m    n m    y 붺 +n m y 붺   figure 1 . since the chi-square function chi x y  is a convex function   which is maximized at one of its vertexes  and chi n m  = 1  please refer to    we only need to consider the remaining three vertexes. 1 the position of each pointer to the end of each tuple. in this example  there is no row that satisfies the pruning condition of lemma 1. such an implementation is proven to be efficient for our purpose as shown in our experiments.
1	finding lower bounds

figure 1: conditional pointer lists모in this section  we describe the algorithm  minelb  which is designed to find the lower bounds of a rule group. since a rule group has a unique upper bound and the consequent of a rule group is fixed  the problem can be regarded as generating the lower bounds for the antecedent of the upper bound rule. this antecedent could be regarded as a closed set  definition 1  and the problem can be solved as long as we can generate the lower bounds of a closed set.
definition 1.	closed set
let d be the dataset with itemset i and row set r. a  a   i  is a closed set of dataset d  iff there is no a1   a such that r a  = r a1 . al  al   a  is a lower bound of closed set a  iff and there is no a1   al such that
r a   = r a .	1
모minelb is an incremental algorithm that is initialized with one closed set a  the antecedent of an upper bound rule  a 뫸 c for a rule group. it then updates the lower bounds of a incrementally whenever a new closed set a1 is added  where a1   a and a1 is the antecedent of the newly added upper bound a1 뫸 c. in this way  minelb keeps track of the latest lower bounds of a. minelb is based on the following lemma.
lemma 1. let a be the closed set whose lower bounds will be updated recursively and z be the set of closed sets that are already added. let a.붞 be the current collection of lower bounds for a. when a new closed set a1   a is added  a.붞 is divided into two groups  a.붞1 and a.붞1  where a.붞1 = {li|li 뫍 a.붞 뫇 li   a1}  a.붞1 = a.붞   a.붞1. then the newly generated lower bounds of a must be in the form of l1 뫋 {i}  where l1 뫍 a.붞1  i 뫍 a   a1.
proof:
suppose l is a newly generated lower bound of a.
 1  we prove l   l1. since r l  = r a   definition 1  before a1 is added  there must exist a li 뫍 a.붞 such that li   l   a. if li 뫍 a.붞1  l can not be a new lower bound  since li 뫍 a.붞1 is still a lower bound of a after a1 is added. so l   l1  l1 뫍 a.붞1.
 1  obviously  the newly generated lower bound must contain at least one item from the set  a   a1 .
 1  l1 = l1 뫋 {i} is a bound for a after adding a1  where l1 뫍 a.붞1  i 뫍 a and i /뫍 a1. before a1 is added  l1 = l1뫋{i} is a bound  so for any x 뫍 z  l1 * x. after a1 is added  l1 * a1 because i /뫍 a1. so  l1 = l1 뫋 {i} is a new bound for a after adding a1.
based on  1    1  and  1   we come to the conclusion that the newly generated lower bound for a after inserting a1 takes the form of l1 뫋{i}  where l1 뫍 a.붞1 and i 뫍  a a1 .
1
모itemset l1 뫋 {i} described in lemma 1 is a candidate lower bound of a after a1 is added. if l1 뫋 {i} does not cover any l1 뫍 a.붞1 and other candidates  l1 뫋 {i} is a new lower bound of a after a1 is added. minelb adopts bit vector for the above computation. thus a.붞 can be updated efficiently. the detailed algorithm is illustrated in figure 1.
모we can ensure that the closed sets  those that cover all the longest closed set a1   a  obtained at step 1 are sufficient for the correctness of minelb because of lemma 1
lemma 1. if a closed set a1   a is already added and the collection of a's lower bounds  a.붞  is already updated  a.붞 will not change after adding closed set a1  a1   a1. proof:
after a1   a is added  a.붞 is updated so that no li 뫍 a.붞 can satisfy li   a1. so no li 뫍 a.붞 can satisfy li   a1  a1   a1. since a1 will not cover any li 뫍 a.붞  a.붞 will not change  according to lemma 1. 1
example 1.	finding lower bound
given an upper bound rule with antecedent a = abcde and two rows  r1 : abcf and r1 : cdeg  the lower bounds a.붞 of a can be determined as follows:
1 initialize the set of lower bounds a.붞 = {a b c d e};
모1 add  abc   = i r1  뫌 a : we get a.붞1 = {a b c} and a.붞1 = {d e}. since all the candidate lower bounds   ad    ae    bd    be    cd    ce  cover a lower bound from a.붞1  no new lower bounds are generated. so a.붞 = {d e};
모1 add  cde   = i r1  뫌 a : we get a.붞1 = {d e} and a.붞1 =  . the candidate lower bounds are  ad    bd    ae  and  be . because none of them is covered by another candidate and a.붞1 =    a.붞 = {ad bd ae be}. 1
1.	performance studies
모in this section  we will look at both the efficiency of farmer and the usefulness of the discovered irgs. all our experiments were performed on a pc with a pentium iv 1 ghz cpu  1gb ram and a 1gb hard disk. algorithms were coded in standard c.
datasets: the 1 datasets are the clinical data on lung cancer  lc 1  breast cancer  bc  1  prostate cancer  pc  1 
subroutine: minelb table:d  upper bound rule: 붺 .
1. a = 붺.a; a.붞 = {i|i 뫍 a};  =  ;
1. for each row rid of d that rid 뫍 r/	 a : if  i rid  뫌 a    a then add  i rid  뫌 a  to ;
1. for each closed set a1 뫍 : a.붞1 = a.붞1 =  ; for each lower bound li 뫍 a.붞: if li   a1 then add li to a.붞1; else add li to a.붞1; candiset =  ; for each li 뫍 a.붞1 and each i 뫍 a && i /뫍 a1:
   add candidate li 뫋 {i} to candiset; a.붞 = a.붞1; for each candidate ci 뫍 candiset if ci does not cover any li 뫍 a.붞1 and ci does not cover any other cj 뫍 candiset then add ci to a.붞
1. output a.붞. figure 1: minelb

all-aml leukemia  all  1  and colon tumor  ct  1. in such datasets  the rows represent clinical samples while the columns represent the activity levels of genes/proteins in the samples. there are two categories of samples in these datasets.
dataset# row# colclass 1class 1#row of class 1bc1relapsenonrelapse1lc1mpmadca1ct1negativepositive1pc1tumornormal1all1allaml1table 1: microarray datasets
모table 1 shows the characteristics of these 1 datasets: the number of rows  # row   the number of columns  # col   the two class labels  class 1 and class 1   and the number of rows for class 1  # class 1 . all experiments presented here use the class 1 as the consequent; we have found that using the other consequent consistently yields qualitatively similar results.
모to discretize the datasets  we use two methods. one is the entropy-minimized partition  for cba and irg classifier 1and the other is the equal-depth partition with 1 buckets. ideally  we would like to use only the entropy discretized datasets for all experiments since we want to look at the classification performance of irgs. unfortunately  the two rule mining algorithms that we want to compare against are unable to run to completion within reasonable time  we ran them for several days without results  on the entropy discretized datasets  although farmer is still efficient. as a result  we will report our efficiency results based on the equal-depth partitioned data while our classifier is built using the entropy-discretized datasets.
1	efficiency of farmer
모the efficiency of farmer will first be evaluated. we compare farmer with the interesting rule mining algorithm in . the algorithm in  is the one most related to farmer in terms of interesting rule definition  but not the same  see related work . to our best knowledge  it is also the most efficient algorithm that exists with the purpose of mining interesting rules of our kind. we denote this algorithm as columne since it also adopts column enumeration like most existing rule mining algorithms. we also compare farmer with the closed set discovery algorithms charm  and closet+   which are shown to be more efficient than other association rule mining algorithms in many cases. we found that charm is always orders of magnitude faster than closet+ on the microarray datasets and thus we do not report the closet+ results here. note that the runtime of farmer includes the time for computing both the upper bound and lower bounds of each interesting rule group. compared with charm  farmer does extra work in: 1 computing the lower bounds of irgs and 1  identifying the irgs from all rule groups. unlike farmer that discovers both upper bound and lower bounds for each irg  columne only gets one rule for each irg.
1.1	varying minimum support
모the first set of experiments  figure 1  shows the effect of varying minimum support threshold minsup. the graphs plot runtime for the three algorithms at various settings of minimum support. note that the y-axes in figure 1 are in logarithmic scale. we set both minconf and minchi as zero  which disables the pruning with confidence upper bound and the pruning with the chi-square upper bound of farmer.
모for charm  minsup represents the least number of rows that the closed sets must match. the runtime of charm is not shown in figures 1 a  and 1 b  because charm runs out of memory even at the highest support in figure 1 on datasets bc and lc.
모figure 1 shows that farmer is usually 1 to 1 orders of magnitude faster than columne and charm  if it can be run . especially at low minimum support  farmer outperforms columne and charm greatly. this is because the candidate search space for columne and charm  dependent on the possible number of column combinations after removing the infrequent items  is orders of magnitude greater than the search space of farmer  dependent on the possible number of row combinations  on microarray datasets.
모as shown in figure 1 f   the number of interesting rule groups discovered at a low minsup is much larger than that at a high minsup. besides the size of row enumeration space  the number of irgs also affects the efficiency of farmer due to two reasons. first  since farmer discovers irgs by comparison  see algorithm section  step 1   more time will be spend when the number of irgs to be compared against increase. second  the time complexity of computing lower bounds in farmer is o n   where n is the number of irgs. we observe that at high minsup  the time used to compute lower bounds takes 1% to 1% of the runtime of farmer while the time taken at low minsup can be up to 1%. columne also does the comparison to get interesting rules while all the runtime of charm is used to discover closed sets.

figure 1: varying minsup


	 d  all-aml leukemia	 e  colon tumor	 f  number	of	irgs	vs	minconf
 minchi=1 
figure 1: varying minconf모we choose our minimum support such that the runtime of farmer is around 1 seconds. although columne and charm could perform better that farmer for higher minsup  the absolute time difference however will be less than 1 seconds and thus is not interesting for comparison. this is negligible compared to the difference in runtime at low minsup.
1.1	varying minimum confidence
모the next set of experiments  figure 1  shows the effect of varying minconf when minsup is fixed. the minchi pruning is still disabled by setting it to zero. for all the parameter settings in figure 1  charm is unable to finish because of insufficient memory after several hours while columne always has a runtime of more than 1 day. this is because we adopt a relative low minsup to study the effectiveness of confidence pruning in the experiment. to show the effect of various minconf clearly  we do not give the runtime of columne. we will first ignore the lines marked with  minchi=1  here. we set minsup = 1  which means that minimum support pruning is almost disabled.
모figure 1 shows that the runtime of farmer decreases when increasing minconf on all the 1 datasets  figure 1 f  lists the number of irgs . this shows that it is effective to exploit the confidence constraint for pruning. there is only a slight decrease in runtime of farmer when the minconf increases from 1% to 1% since there are few upper bound rules with confidence between 1% and 1%. we observe that nearly all irgs discovered at confidence 1% on these 1 datasets have a 1% confidence. as a result  farmer does no additional pruning when minconf increases from 1% to 1%.
모the result that many discovered irgs have a 1% confidence is interesting and promising. it means that the irgs are decisive and have good predictability.
1.1	varying minimum chi-square value
모the last set of experiments was performed to study the effectiveness of the chi-square pruning. minimum chi-square constraint is usually treated as a supplementary constraint of minimum support and minimum confidence. we set minchi = 1 and plot the runtime vs various minconf in figure 1 due to the space limitation  where minconf is set the same as in section 1.1.
모the pruning exploited by constraint minchi = 1 is shown to be very effective on datasets bc  pc  ct and all. in some cases  the saving can be more than an order of magnitude. the pruning effect is not so obvious on dataset lc. by checking the identified irgs  we found that discovered irgs from lc usually have higher chi-square value. if we impose a tighter chi-square constraint by increasing minchi  the minchi pruning will be more obvious. due to space constraint  we do not discuss this further.
모as can be seen  in all the experiments we conducted  farmer outperforms columne and charm. moreover  the prunings based on minsup  minconf and minchi are effective. in general  the runtime of farmer correlates strongly with the number of interesting rule groups that satisfy all of the specified constraints. our experimental results demonstrate that farmer is extremely efficient in finding irgs on datasets with small number of rows and large number of columns.
모in additional to these experiments  we also look at how the performance of farmer varies as the number of rows increase. this is done by replicating each dataset a number of times to generate a new dataset. it is observed that the performance of farmer still outperform other algorithms even when the datasets are replicated for 1 times. due to lack of space  we refer readers to  for these additional experiments.
1	usefulness of irgs
모in order to show the usefulness of the discovered irgs  we build a simple classifier called irg classifier based on those irgs that we discovered. note that our emphasis here is not to build a new classifier but to provide some evidence that the discovery of irgs is at least useful for such purpose.
모we will compare our irg classifier with two well-known classifiers cba  and svm   both available through the internet. the open-source cba algorithm  and all competitors we look at in the earlier section  failed to finish running in one week. to go around this problem  we build the cba classifier by obtaining the frequent rules based on the upper bounds and lower bounds generated by farmer. our irg classifier is similar to cba but we use irgs to build classifiers instead of all rules. due to space limitation  we do not explain the details of the irg classifier.
모for cba  we set the minimum support threshold as 1  number of training data of class c for each class c and set the minimum confidence threshold as 1  according to our experiments  if we further lower the minimum confidence threshold  the final cba classifier is the same ; for irg classifier  we use the same parameters as cba; for svm  we always use the default setting of sv mlight.
dataset#training#testirg classifiercbasvmbc11%1%1%lc11%1%1%ct11%1%1%pc11%1%1%all11%1%1%average accuracy1%1%1%table 1: classification results
모table 1 illustrates the percentages of correctly predicted test data for the irg classifier  cba and svm on the 1 microarray datasets. we can see that the irg classifier has the highest average accuracy. although svm performs very well on lc and all  it fails on bc. no classifier outperforms the others on all datasets. our irg classifier is both efficient and easily understandable and thus could be a good reference tool for biological research.
1.	related work
모association rule mining has attracted considerable interest since a rule provides a concise and intuitive description of knowledge. it has already been applied on biological data  such as  1  1  1 .
모association rule can relate gene expressions to their cellular environments or categories  thus available for building accurate classifiers on microarray datasets as in  1  1 . moreover  it can discover the relationship between different genes  so that we can infer the function of an individual gene based on its relationship with others  and build the gene network. association rules might reveal more patterns than clustering   considering that a gene may belong to many rules while it is usually grouped to one cluster  or a hierarchy of clusters .
모there are many proposals about rule mining in the data mining literatures. they can be roughly divided into three classes. the first two classes are related to association rule mining. all existing association rule mining algorithms adopt the column enumeration in the mining process  therefore they are very time-consuming on microarray datasets. the first class of rule mining algorithms identifies the interesting  or optimal  rules with some interestingness measures . the interesting rule discussed in  is quite similar to our interesting rule group. however   randomly picks a rule for each rule group while farmer discovers the upper bound and lower bounds for each interesting rule group.
모the second class of rule mining algorithms aims to find all association rules satisfying user-specified constraints by identifying all frequent itemsets at the key step  such as  1  1 . recently the concept of closed itemset  is proposed to reduce redundant itemsets and rules . several efficient mining algorithms  1  1  1  are proposed to mine frequent closed itemsets. on the other hand  there is some work  1  1  that investigates incorporating item constraints to reduce the number of frequent itemsets. other work  1  1  leverages the item constraint as rule consequent and utilizes minimum thresholds of confidence  support and other statistic constraints. farmer differs from these approaches in term of its enumeration method and pruning strategies.
모the third class of algorithms aims at mining predictive rules. one example is the decision tree induction algorithm. alternatively  some work  1  1  has been done to build classifiers from association rules and has obtained better classification results than decision trees in many cases. it is obvious that these methods are also applicable based on the concept of interesting rule groups.
모in a short paper   the idea of using row enumeration for mining closed patterns in biological datasets is introduced. the idea is however restricted to finding frequent closed patterns that satisfy a certain support threshold. farmer on the contrary finds irgs that satisfy interestingness constraints like minconf and minchi. the effectiveness of pruning with such constraints is evident in our experiments.
1.	conclusions
모in this paper  we proposed an algorithm called farmer for finding the interesting rule groups in microarray datasets. farmer makes use of the special characteristic of microarray datasets to enhance its efficiency. it adopts the novel approach of performing row enumeration instead of the conventional column enumeration so as to overcome the extremely high dimensionality of microarray datasets. experiments show that farmer outperforms existing algorithms like charm and columne by a large order of magnitude on microarray datasets.
모our irg classifier built on interesting rule groups demonstrates the usefulness of discovered irgs. our experiments showed that it has the highest average accuracy compared with cba and svm.
acknowledgment: we will like to thank the anonymous reviewers from various conferences for their helpful suggestions which have led to great enhancements on the paper. anthony tung will like to thank his wife  monica  for the patience and support through the course of his work.
