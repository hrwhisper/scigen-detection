typical pseudo-relevance feedback methods assume the topretrieved documents are relevant and use these pseudo-relevant documents to expand terms. the initial retrieval set can  however  contain a great deal of noise. in this paper  we present a clusterbased resampling method to select better pseudo-relevant documents based on the relevance model. the main idea is to use document clusters to find dominant documents for the initial retrieval set  and to repeatedly feed the documents to emphasize the core topics of a query. experimental results on large-scale web trec collections show significant improvements over the relevance model. for justification of the resampling approach  we examine relevance density of feedback documents. a higher relevance density will result in greater retrieval accuracy  ultimately approaching true relevance feedback. the resampling approach shows higher relevance density than the baseline relevance model on all collections  resulting in better retrieval accuracy in pseudo-relevance feedback. this result indicates that the proposed method is effective for pseudo-relevance feedback. 
categories and subject descriptors 
h.1  information storage & retrieval : relevance feedback 
general terms 
algorithms  experimentation 
keywords 
information retrieval  pseudo-relevance feedback  a cluster-based resampling  dominant documents  query expansion   
1. introduction 
most pseudo-relevance feedback methods  e.g.   1  1  1   assume that a set of top-retrieved documents is relevant and then learn from the pseudo-relevant documents to expand terms or to assign better weights to the original query. this is similar to the process used in relevance feedback  when actual relevant documents are used . but in general  the top-retrieved documents contain noise: when the precision of the top 1 documents  p 1  is 1  1 of them are non-relevant. this is common and even expected in all retrieval models. this noise  however  can result in the query representation  drifting  away from the original query. 
 this paper describes a resampling method using clusters to select better documents for pseudo-relevance feedback. document clusters for the initial retrieval set can represent aspects of a query on especially large-scale web collections  since the initial retrieval results may involve diverse subtopics for such collections. since it is difficult to find one optimal cluster  we use several relevant groups for feedback. by permitting overlapped clusters for the top-retrieved documents and repeatedly feeding dominant documents that appear in multiple highly-ranked clusters  we expect that an expansion query can be represented to emphasize the core topics of a query.  
 this is not the first time that clustering has been suggested as an improvement for relevance feedback. in fact  clustering was mentioned in some of the first work related to pseudo-relevance feedback . previous attempts to use clusters have not improved effectiveness. the work presented here is based on a new approach to using the clusters that produces significantly better results.  
 our motivation for using clusters and resampling is as follows: the top-retrieved documents are a query-oriented ordering that does not consider the relationship between documents. we view the pseudo-relevance feedback problem of learning expansion terms closely related to a query to be similar to the classification problem of learning an accurate decision boundary  depending on training examples. we approach this problem by repeatedly selecting dominant documents to expand terms toward dominant documents of the initial retrieval set  as in the boosting method for a weak learner that repeatedly selects hard examples to change the decision boundary toward hard examples. the hypothesis behind using overlapped document clusters is that a good representative document for a query may have several nearest neighbors with high similarities  participating in several different clusters. since it plays a central role in forming clusters  this 
 
 
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  or republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee. 
sigir'1  july 1  1  singapore. 
copyright 1 acm  1-1-1/1...$1. 
  
document may be dominant for this topic. repeatedly sampling dominant documents can emphasize the topics of a query  rather than randomly resampling documents for feedback.  
 we show that resampling feedback documents based on clusters contributes to higher relevance density for feedback documents on a variety of trec collections. the results on large-scale web collections such as the trec wt1g and gov1 collections show significant improvements over the baseline relevance model.  
 the rest of the paper is organized as follows:  section 1 presents related work. section 1 describes a cluster-based resampling framework. section 1 shows experimental results on trec test collections  results analyses and justification of the results. we will conclude in section 1.  
1. related work 
our approach is related to previous work on pseudo-relevance feedback  resampling approaches  and the cluster hypothesis in information retrieval. 
 relevance feedback  rf  and pseudo-relevance feedback  prf  have been shown to be effective ways of improving retrieval accuracy by reformulating an original query using relevant or pseudo-relevance documents from the initial retrieval result. new interest in relevance feedback has resulted in the establishment of a relevance feedback track at trec 1 .  this track will provide a framework for exploring the effects of different factors on relevance feedback  such as initial retrieval  judgment procedure  core reformulation algorithm  and multiple iterations on large scale collection. the motivation of the track shows the current state of research: that relevance feedback is one of the successes of information retrieval over the past 1 years  in that it is applied in a wide variety of settings as both explicit and implicit feedback; however there is surprisingly little new basic research . at the ria workshop   there were comparative experiments on the effects of several factors for pseudo-relevance feedback. the report provides the effects of the number of documents  the number and source of terms used  the initial set of documents  and the effects of swapping documents or terms across systems. in some aspects it is not easy to see real effects  since some factors are mixed up with other effects.  
 traditional pseudo-relevance feedback algorithms such as okapi bm1  and lavrenko and croft's relevance model  are based on the assumption of relevancy for the top-retrieved documents.  research has been conducted to improve traditional prf by using passages  instead of documents  by using a local context analysis method   by using a query-regularized estimation method   and by using latent concepts . these methods follow the basic assumption that the top-retrieved documents are relevant to a query. 
 recently there has been some work on sampling and resampling techniques for the initial retrieval set. a selective sampling method by sakai et al  skips some top-retrieved documents based on a clustering criterion. the cluster is generated not by document similarity but by the same set of query terms. the sampling purpose is to select a more varied and novel set of documents for feedback. their assumption is that the top-ranked documents may be too similar or redundant. however  their results did not show significant improvements on ntcir collections. our approach of repeatedly using dominant documents is based on a different assumption.  
 a resampling method suggested by collins-tompson and callan  uses bootstrap sampling on the top-retrieved documents for the query and variants of the query obtained by leaving a single term out. the assumption behind query variants is that one of the query terms is a noise term. from their experimental analysis  the main gain is from the use of query variants  not document resampling. their results on robustness and precision at 1 documents  p 1  show improvements  but the performance in terms of mean average precision  map  is lower than the baseline relevance model on trec collections. our approach primarily focuses on the effects of resampling the top-retrieved documents.  
 on the other hand  many information retrieval techniques have adopted the cluster hypothesis to improve effectiveness. the cluster hypothesis states that closely related documents tend to be relevant to the same request . re-ranking using clusters  1  1  based on the vector space model has shown successful results. a cluster-based retrieval model  based on language modeling ranks clusters by the likelihood of generating the query. the results show improvements over the query-likelihood retrieval model on trec collections. a local score regularization method  uses a document affinity matrix to adjust initial retrieval scores so that topically related documents receive similar scores. the results on trec collections show that regularized scores are significantly better than the initial scores. our work is closely related to document re-ranking using cluster validation and label propagation   document-based language models by the incorporation of cluster information   re-ranking method using cluster-based language models within a graph-based framework   re-ranking using affinity graph   and iterative pseudoquery processing using cluster-based language models .   
 there has also been work on term expansion using clustering in the vector space model  1  1  1  1 . at trec 1  buckley et al  used document clustering on smart though the results of using clusters did not show improvements over the baseline feedback method. at the ria workshop to investigate the effects criteria for pseudo-relevance feedback   there are comparisons to investigate the effects of swapping documents and clusters by document clustering and passage-level clustering. the experimental setup is too complex to see the individual effects of clusters  since an outside source factor is mixed up with the clustering factor : using outside sources for feedback itself affects the performance. thus the analysis for the comparative experiments is inconclusive.   
 1. a cluster-based resampling framework for feedback 
this section describes the rationale for the method for selective resampling  our resampling procedure  and a justification based on relevance density. 
1 selective resampling approach 
the main issues in pseudo-relevance feedback are how to select relevant documents from the top-retrieved documents  and how to select expansion terms. here we deal with the problem of selecting better feedback documents.  
 the problem in traditional pseudo-relevance feedback is obtaining a set of expansion terms from the top-retrieved documents that may have low precision. if a method can select better documents from the given sample  it can almost certainly contribute better expansion terms. for pseudo-relevance feedback  the initial retrieval set can be seen as the sample space of query expansion terms from which we estimate the sampling distribution. 
 in statistics  resampling  bootstrapping   is a method for estimating the precision of sample statistics by sampling randomly with replacement from the original sample  leading to 
robust estimates. if a method is available for selecting better examples from the original sample space  selective sampling will perform better than random sampling. boosting  1  1  is a selective resampling method in machine learning. it is an iterative procedure used to adaptively change the distribution of training examples so that the weak learners focus on examples that previous weak learners misclassified. 
 to find some direction to change the distribution  we assume that a dominant document for a query is one with good representation of the topics of a query-i.e. one with several nearest neighbors with high similarity. in overlapped clusters  a dominant document will appear in multiple highly-ranked clusters. since a topic can contain several subtopics  the retrieved set can be divided into several subtopic groups. a document that deals with all subtopics will likely be in all subtopic clusters  so we call that document dominant. from such a dominant document  expansion terms that retrieve documents related to all subtopics can be selected.  
 based on the above assumption  we selectively resample documents for feedback using k-nearest neighbors  k-nn  clustering to generate overlapped clusters from the given topretrieved documents space.  
 1 resampling feedback documents using overlapping clusters 
a cluster-based resampling method to get better pseudo-relevant documents is based on the language model  and the relevance model  frameworks. relevance models have been shown to be a powerful way to construct a query model from the top-retrieved documents  1  1 . the essential point of our approach is that a document that appears in multiple highly-ranked clusters will contribute more to the query terms than other documents. the resampling process proceeds as follows:  
 first  documents are retrieved for a given query by the querylikelihood language model  with dirichlet smoothing .  
 a statistical language model is a probabilistic distribution over all the possible word sequences for generating a piece of text.  . in information retrieval  the language model treats documents themselves as models and a query as strings of text generated from these document models. the popular querylikelihood retrieval model estimates document language models using the maximum likelihood estimator. the documents can be ranked by their likelihood of generating or sampling the query from document language models: p q|d .  
m
p q| d  =¡Çp qi | d  	 1  
i=1
where qi is the ith query term  m is the number of words in a query q  and d is a document model. 
 dirichlet smoothing is used to estimate non-zero values for terms in the query which are not in a document.  it is applied to the query likelihood language model as follows. 
	| d |	¦Ì
p w | d  =	 pml  w | d  +	 pml  w | coll  	 1  
	| d | +¦Ì	| d | +¦Ì
¡¡¡¡¡¡¡¡¡¡¡¡freq w  d 	freq w coll  pml  w | d  =	   pml  w | coll  =
	| d |	| coll |	 1  
 
where pml w|d  is the maximum likelihood estimate of word w in the document d  coll is the entire collection  and ¦Ì is the smoothing parameter. |d| and |coll| are the lengths of a document d and collection c  respectively. freq w d  and freq w coll  denote the frequency of a word w in d and coll  respectively. the smoothing parameter is learned using training topics on each collection in experiments. 
 next  clusters are generated by k-nearest neighbors  k-nn  clustering method  for the top-retrieved n documents to find dominant documents.  in experiments  n is set to 1.  note that one document can belong to several clusters. 
 in k-nn clustering  each document plays a central role in making its own cluster with its k closest neighbors by similarity. we represent a document by tfidf weighing and cosine normalization. the cosine similarity is used to calculate similarities among the top-retrieved documents. 
 our hypothesis is that a dominant document may have several nearest neighbors with high similarities  participating in several clusters. on the other hand  a non-relevant document ideally makes a singleton cluster with no nearest neighbors with high similarity  though practically it will have neighbors due to noise such as polysemous or general terms. document clusters can also reflect the association of terms and documents from similarity calculation. in this work  if a document is a member of several clusters and the clusters are highly related to the query  we assume it to be a dominant document. a cluster-based resampling method is repeatedly feeding such dominant documents based on document clusters. 
 after forming the clusters  we rank them by a cluster-based query-likelihood language model described below . the documents in the top-ranked clusters are used for feedback. note that clusters are only used for selecting feedback documents. 
 a cluster can be treated as a large document so that we can use the successful query-likelihood retrieval model. intuitively  each cluster can be represented by just concatenating documents which belong to the cluster. if clu represents such a cluster  then: 
m
p q | clu  =¡Çp qi | clu  	 1 
i=1
	| clu |	¦Ë
p w | clu  =  pml w | clu  +  pml w | coll   1  | clu | +¦Ë | clu | +¦Ë
	freq w clu 	freq w coll 
pml  w | clu  =	   pml  w | coll  =	  1 
	| clu |	| coll |
where freq w clu  is sum of freq w  d  for the document d which belongs to the cluster clu.  
 finally  expansion terms are selected using the relevance model for each document in the top-ranked clusters. note that the set of feedback documents chosen from the top-ranked clusters are used to estimate the relevance model with their initial querylikelihoods. 
 a relevance model is a query expansion approach based on the language modeling framework. the relevance model  is a multinomial distribution which estimates the likelihood of words w given a query q. in the model  the query words q1 ... qm and the words w in relevant documents are sampled identically and independently from a distribution r. following that work  we estimate the probability of a word in the distribution r using   
	¡Æp d p w | d p q | d  	 1  
¡¡d¡Êr where r is the set of documents that are pseudo-relevant to the query q. we assume that p d  is uniform over the set.  
 after this estimation  the most likely e terms from p w|r  are chosen as an expansion query for an original query. the final expanded query is combined with the original query using linear interpolation  weighted by parameter ¦Ë. the combining parameter is learned using training topics on each collection in experiments. 
 the original relevance model and traditional pseudo-relevance feedback methods use the initial retrieval set to get expansion terms directly after the first step. the problem is that the topretrieved documents contain non-relevant documents  which add noise to expansion terms. our effort uses overlapping clusters to find dominant documents for the query. it may still find nonrelevant documents  but we will show it finds fewer. 
1 justification by relevance density   
the rationale for the proposed method is that resampling documents using clusters is an effective way to find dominant documents for a query from the initial retrieval set. we measure relevance density to justify our assumption that dominant documents are relevant to the query and redundantly appear over the top-ranked clusters. 
 the relevance density is defined to be the proportion of the feedback documents that contain relevant documents.  
the number of relevant feedback documents 
density =		 1  
the number of feedback documents
 a higher relevance density implies greater retrieval accuracy  ultimately approaching true relevance feedback.  
 if a resampling method is effective  it will produce higher relevance densities for pseudo-relevant documents than a set of top-retrieved documents. to justify the cluster-based resampling method  we will examine the relevance density of feedback documents through experimental analysis. 
1. experiments 
to validate the proposed method  we performed experiments on five trec collections and compared the results with a baseline retrieval model  a baseline feedback model  and an upper-bound model.  
1 experimental set-up 
1.1 test collections  
we tested the proposed method on homogeneous and heterogeneous test collections: the robust  ap and wsj collections are smaller and contain newswire articles  whereas gov1 and wt1g are larger web collections. for all collections  the topic title field is used as the query. a summary of the test collections is shown in table 1. 
table 1.  training and test collections 
collectiondescription # of docs topics train test gov1 1 crawl of .gov domain1 1 11 wt1g trec web collection 1 1 11 robustrobust 1 collection 1 11 ap association 
press 1 1 1 1 wsj wall street journal 1 1 1 1  version 1 of the indri system  is used for indexing and retrieval. all collections are stemmed using a porter stemmer. a standard list of 1 common terms is removed at retrieval time. 
1.1 training and evaluation 
for each test collection  we divide topics into training topics and test topics  where the training topics are used for parameter estimation and the test topics are used for evaluation. 
 in order to find the best parameter setting we sweep over values for smoothing parameter to construct the language model  ¦Ì ¡Ê {1  1  1  1  1  ...  1}   to construct the relevance model for the number of feedback documents  |r| ¡Ê {1  1  1  1  1  1}   the number of expansion terms   e ¡Ê {1  1  1  1  1}  and the weight of the original query  ¦Ë ¡Ê {1  1  ...  1} . to train the proposed model  we sweep over the number of feedback clusters  |c| ¡Ê {1  1  1  1  1  1}   which corresponds to the number of feedback documents since one cluster can have at most five documents as a member  k = 1  in our k-nearest neighbors clustering. the threshold for clustering is set to 1. expansion terms are represented using the following indri query form:  
	# weight  ¦Ë #combine  q1 ...qm  	 
 
 1 ¦Ë  # weight   p1 t1 ... pe te   
where q1 ... qm are the original query terms  t1 ... te are the e terms with expansion probabilities  p1 ... pe  and ¦Ë is a parameter  combining the original query and the expanded query.   
 all comparison methods are optimized on the  training set using mean average precision  map  defined as  
1
	map = | q | ¡Æap q  	 1  
q¡Êq
where ap q  is average precision for a query in the topic set q. 
  the best parameters on training for each test collection are used for experimental results with the test topics.   
1.1 baselines  
we provide two baselines: the language model and the relevance model.   
  language model  lm : the performance of the baseline retrieval model. the relevance model and the resampling method are also based on this framework.  
  relevance model  rm : the performance of the baseline pseudo-relevance feedback model. the expanded query is combined with the original query. the resampling method is based on the relevance model framework. the difference is the pseudo-relevant documents used.  
1.1 upper-bound: true relevance feedback  
to investigate the performance of the upper-bound of the proposed method  we compare with true relevance feedback. 
  true relevance feedback  truerf : the performance using true relevant documents in the top-retrieved 1 documents. this performance presents the upper-bound when using relevance model. 
1.1 a cluster-based reranking method   
to provide the effectiveness of clusters for the initial retrieval set  we also include a cluster-based reranking method.  
  reranking using clusters  rerank : the performance of reranking by combining query likelihoods for documents and clusters based on k-nn clusters for the top-retrieved n documents. n and k are set to 1 and 1  respectively.   
	p' q | d  = p q | d  max d¡Êclui p q | clui   	 1  
since a document can be a member of several clusters  we choose the maximum query likelihood for the clusters clu which the document d belongs to. 
1 experimental results 
the results for the comparison methods on five test collections are presented in table 1. 
 the resampling method significantly outperforms lm on all test collections  whereas rm does not significantly outperform lm on the wt1g collection. for the gov1 and wt1g heterogeneous web test collections  the resampling method significantly outperforms rm. the relative improvements over rm are 1% and 1% on gov1 and wt1g  respectively. for the robust newswire collection  the resampling method shows slightly lower performance than rm. for the ap and wsj newswire collections  the resampling method shows small  but not significant improvements over rm.  
 in the precision at 1  p 1  evaluation metric  not shown in the table   the resampling method shows 1%  1%  1%  1%  and 1% improvements over lm  whereas rm shows -1%  1%  1%  1% and 1% improvement on gov1  wt1g  robust1  ap and wsj  respectively.    
 the rerank method using clusters shows significant improvements over lm on all test collections. in fact  the rerank method outperforms rm on wt1g collection. the results indicate that document clustering can help find relevant document groups for the initial retrieval set and provide implicit document context to the query.  
 truerf shows significant improvements over all methods on test collections. the results provide upper-bound performance on each collection  when we are able to choose better pseudorelevant documents  approaching to true relevant documents.  
 we have also examined the effectiveness as the number of feedback documents and terms varies. as shown in figure 1  
table 1. performance comparisons using mean average precision for the test topics on test collections. the superscripts ¦Á  ¦Â  ¦Ã and ¦Ä indicate statistically significant improvements over lm  rerank  rm and resampling  respectively. we use the paired t-test with significance at p   1. 
 lm rerankrm resamplingtruerf  gov1 11 ¦Á1 ¦Á¦Â 1 ¦Á¦Â¦Ã 1 ¦Á¦Â¦Ã¦Ä wt1g 11 ¦Á1    1 ¦Á¦Â¦Ã 1 ¦Á¦Â¦Ã¦Ä robust11 ¦Á1 ¦Á¦Â 1 ¦Á¦Â 1 ¦Á¦Â¦Ã¦Ä ap 11 ¦Á1 ¦Á¦Â 1 ¦Á¦Â 1 ¦Á¦Â¦Ã¦Ä wsj 11 ¦Á1 ¦Á¦Â 1 ¦Á¦Â 1 ¦Á¦Â¦Ã¦Ä 
table 1. performance on fixed feedback documents. the number of feedback documents and terms are both set to 1. the superscripts ¦Á and ¦Â indicate statistically significant improvements over lm and rm  respectively. we use the paired t-test using significance at p   1.  
  lm rm chg% resamplingchg%gov1 1 1 ¦Á 1 1 ¦Á¦Â 1wt1g 1 1 1 1 ¦Á 1robust1 1 ¦Á 1 1 ¦Á¦Â 1ap 1 1 ¦Á 1 1 ¦Á 1wsj 1 1  ¦Á 1 1 ¦Á¦Â 1 
resampling achieves better performance over rm for most values. the best parameters selected for feedback on gov1 are 1 documents and 1 terms for rm  1 documents and 1 terms for resampling. for test topics using the best parameters  ¦Ì  e  and ¦Ë  chosen from training  the resampling method outperforms rm regardless of the number of feedback documents. 
1 relevance density  
in this section we aim to develop a deeper understanding of why expansion by the cluster-based resampling method helps.  
 for justification of a cluster-based resampling approach using overlapping clusters  we have analyzed the relevance density by dominant documents and the performance of feedback without redundant documents. 
1.1  relevance density of feedback documents 
we can expect that higher relevance density produces higher performance since more relevant documents are used for feedback.  
 as shown in figure 1  the resampling method shows higher density compared to the relevance model for all test collections.  the density for ap and wsj collections is not shown but has the same pattern as the robust collection.    
 when the number of feedback documents is set to 1  we can expect that the resampling method outperforms the relevance model since the resampling method uses more relevant documents for feedback. 

figure 1. performances on training and test set for rm and resampling according to the number feedback documents and terms on gov1 collection  in mean average precision .  
 

the number of feedback documents
figure 1. the relevance density for rm and resampling according to the number of feedback documents.   to verify our expectation for density  we compared performance with the number of feedback documents and terms set to 1. the performance of feedback on fixed documents is shown in table 1. the resampling method outperforms the relevance model for all collections. the results show that the density of relevant documents supports the improvements from the resampling approach which extracts better feedback documents from the top-ranked 1 documents. 
 from the results of density according to number of feedback documents and effectiveness on all collections  we can conclude that the redundant dominant documents help the density of the relevant documents.  
1.1 feedback without redundant documents 
to support the observation of relevance density and performance  we have examined performance by removing redundant documents in feedback.  that is  a document is not repeated in the feedback even if it occurs in multiple clusters. 
 we assumed that dominant documents for the initial retrieval set are relevant and redundant documents that play a central role in making overlapping clusters. table 1 shows the performance of sampling without redundancy. it outperforms rm  but is worse than resampling. the results show that redundant documents give positive effects for feedback. 
table 1. the effect of redundant documents for feedback.   
  gov1 chg% wt1gchg%  lm 1 - 1 -  rerank 1 1 1 1  rm 1 1 1 1  resampling  1 1 1 1  sampling without redundancy1 1 1 1  we have also examined how redundancy affects the number of relevant documents in the feedback sample.  if we look at the top 1  1  1  1  1  and 1 documents  we find the following.  for the rm approach  the relevance density was 1  1  1  1  and 1  respectively.  for resampling  however  the densities were almost perfect: 1  1  1  1  1  and 1  respectively.   to illustrate the level of redundancy  consider one query where the top 1 clusters contained 1 documents  1 of which were relevant: 1 of those relevant documents appeared in other clusters.  one relevant document appeared in nine of the top 1 clusters and another was in seven. some documents that appear in multiple highly-ranked clusters and their redundancy contribute to query expansion terms. 
 
figure 1. robustness of the relevance model and the resampling method over the language model for gov1  wt1g  and wsj collections. 
 
1 retrieval robustness  
we analyze the robustness of the baseline feedback model and the resampling method over the baseline retrieval model. here  robustness is defined as the number of queries whose performance is improved or hurt as the result of applying these methods.  
 figure 1 presents an analysis of the robustness of the baseline feedback model and the resampling method on gov1  wt1g and wsj. the robustness of robust and ap showed the similar pattern with wsj. for the homogeneous newswire collections such as wsj  ap and robust  the relevance model and resampling method showed a similar pattern for robustness.  
 the resampling method shows strong robustness for each test collection. for the gov1 collection  the resampling method improves 1 queries and hurts 1  whereas the relevance model improves 1 and hurts 1. for the wt1g collection  the resampling method improves 1 and hurts 1  whereas the relevance model improves 1 and hurts 1. although the relevance model improves the performance of 1 more queries than the resampling method  the improvements obtained by the resampling method are significantly larger. for the robust collection  the resampling method improves 1 and hurts 1  whereas the relevance model improves 1 and hurts 1.  
 overall  our resampling method improves the effectiveness for 1%  1%  1%  1% and 1% of the queries for gov1  wt1g  robust  ap and wsj  respectively.  
1. conclusions 
resampling the top-ranked documents using clusters is effective for pseudo-relevance feedback. the improvements obtained were consistent across nearly all collections  and for large web collections  such as gov1 and wt1g  the approach showed substantial gains. the relative improvements on gov1 collection are 1% and 1% over lm and rm  respectively. the improvements on the wt1g collection are 1% and 1% over lm and rm  respectively. we showed that the relevance density was higher than the baseline feedback model for all test collections as a justification of why expansion by the clusterbased resampling method helps. experimental results also show that overlapping clusters are helpful for identifying dominant documents for a query.  
 for future work  we will study how the resampling approach can adopt query variants by considering query characteristics. 
additionally  in our experiments we simply represent a cluster by concatenating documents. using a better representation of a cluster should improve the performance of pseudo-relevance feedback by improving the cluster ranking. 
1. acknowledgments 
this work was supported by the korea research foundation 
grant funded by the korean government  moehrd    krf1-d1  and in part by the center for intelligent information retrieval.  any opinions  findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. 
