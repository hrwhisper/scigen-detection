we present the design of dynabot  a guided deep web discovery system. dynabot's modular architecture supports focused crawling of the deep web with an emphasis on matching  probing  and ranking discovered sources using two key components: service class descriptions and source-biased analysis. we describe the overall architecture of dynabot and discuss how these components support effective exploitation of the massive deep web data available.
categories and subject descriptors:
h.1 information search and retrieval : search process; h.1.m information systems : miscellaneous general terms: algorithms  design
keywords: deep web  crawling  service class  probing
1. problem description
﹛the deep web provides access to huge and growing data repositories on the web and supports tools for searching  manipulating  and analyzing the information contained in those repositories. unlike the surface web  the deep web refers to the collection of web data that is accessible by interacting with a web-based query interface  and not through the traversal of hyperlinks. recent estimates suggest that the size of the deep web greatly exceeds that of the surface web - with nearly 1 terabytes of data on the deep web versus only 1 terabytes on the surface web.
﹛existing search engine indexers often ignore the data offered by deep web sources owing to the technical challenges that arise when attempting to locate  access  and index deep web data. the most significant challenge is the philosophical difference between the surface and deep web with respect to how data is stored: in the surface web  data is stored in document files  while in the deep web  data is stored in databases or produced as the result of a computation. this difference is fundamental and implies that traditional document indexing techniques  which have been applied with extraordinary success on the surface web  are inappropriate for the deep web.

 this work is performed under a subcontract from llnl under the ldrd project. the work of the first three authors is also partially supported by nsf cise  nsf itr  doe scidac  cercs research grant  ibm faculty award  ibm sur grant  and hp equipment grant. the work of the fourth author is performed under the auspices of the u.s. dep't of energy by univ. of california lawrence livermore national laboratory under contract no. w-1-eng-1.
copyright is held by the author/owner.
www 1  may 1  1  chiba  japan. acm 1-1/1.
﹛standardizing to web services using technologies like xml and soap has alleviated some of the heterogeneity of remote invocation  but the problem of discovering relevant sources remains. there have been a number of efforts to alleviate the discovery problem  including the use of deep web portals  like the one offered at www.profusion.com   but these solutions rely on manually listing sites and do not scale well. in the context of web services  registry-based solutions like the uddi web service registries provide standard interfaces for describing  searching  and browsing for registered web services. but  current registries suffer from limited adoption and are limited to searching and browsing by metadata which limits the quality of both discovery and service selection. in addition  registry-based discovery relies on services correctly advertising themselves in a known repository  effectively limiting the number of services that can be discovered. finally  the limited descriptive power in existing registry standards implies that service analysis is still required to ascertain a service's capabilities.
1. design and architecture
﹛with these challenges in mind  we present dynabot  a guided deep web discovery system. dynabot's modular architecture supports focused crawling of the deep web with an emphasis on matching  probing  and ranking discovered sources in an effort to exploit the vast amount of deep web data. figure 1 presents the overall architecture of dynabot. dynabot utilizes an advanced crawler architecture that includes standard crawler components like a url frontier manager  network interaction modules  global storage and associated data managers  and document processors  as well as the pluggable dynabot-specific semantic analyzers  which analyze the candidate deep web sources.
﹛in this poster  we report two dynabot modules from our research experience: the first uses service class descriptions  to determine the capabilities of discovered sources and to match deep web sources that belong to members of a service class. the second is a suite of source-biased analysis techniques  for refined probing and ranking of deep web sources with respect to a domain of interest.
1 service class matching
﹛the first module supports guided matching of candidate deep web sources through the use of service class descriptions. a service class is a set of deep web sources that provide similar functionality or data access. a service class description  scd  is an abstract description of a service class that specifies the minimum functionality that a deep web source must export to be classified as a member of the service class. an scd is modeled as a triple: scd =  t  g p    where t denotes a set of type definitions  g denotes a control
1

figure 1: dynabot system architectureflow graph  and p denotes a set of probing templates. the scd is initially composed by a user or service developer and can be further revised via automated learning algorithms embedded in the dynabot matching process.
﹛types are used to describe the input and output parameters of a service class and any data elements that may be required during the course of interacting with a service. the dynabot service discovery system includes a type system that is modeled after the xml schema type system with constructs for building atomic and complex types.
﹛due to the complexity of current sources  we model the underlying control flow of the source with a control flow graph. for many deep web sources  a query may have multiple response types depending on a number of factors. for example  a query that results in a normal response under regular load conditions may result in a completely different unavailable or wait 1 seconds response depending on the server and data availability. by defining a control flow graph to capture these different scenarios  we guide the choice of semantic analyzer for use on each response.
﹛the third component of the service class description is the set of probing templates p  each of which contains a set of input arguments and an expected response type that can be used to match a candidate service against the service class description and determine if it is an instance of the service class. the template may include hints to supply clues to the service classifier that help select the most appropriate input parameters to match an argument.
1 source-biased analysis
﹛the second dynabot module supports refined probing and ranking of deep web sources with respect to a domain of interest. this second module consists of two sub-modules:
source-biased probing and source-biased relevance ranking.
﹛given a deep web site - the source - the source-biased probing technique leverages the summary information of the source to generate a series of biased probes for analyzing another deep web site - the target. this source-biased probing allows us to determine in very few interactions whether a target site is relevant to the source by probing the target with focused probes. concretely  the source-biased probing algorithm generates a source-biased summary for a target as follows: it uses the estimated summary of the source 考  denoted by esummary 考   as a dictionary of candidate probe terms and sends a series of query requests parameterized by probe terms  selected from esummary  考   to the target service 而; for each probe term  it retrieves the top m matched documents from 而  generates summary terms and updates esummary考 而 . this process repeats until a stopping condition is met.
﹛given a source and a target service  we may evaluate the source-biased relevance of a target deep web site with respect to the source. we define focus考 而  to be a measure of the topical focus of the target 而 with respect to the source of bias 考. the focus metric ranges from 1 to 1  with lower values indicating less focus and higher values indicating more focus. once a set of target sites have been evaluated with the source-biased relevance metric  we can then rank the target deep web sites with respect to the source of bias to identify the most relevant services to the source of bias.
1. closing remarks
﹛dynabot is designed as a foundation for developing a guided deep web discovery system  powered by two novel service discovery modules: the service class description matching module and the source-biased analysis module for probing and ranking. our research on dynabot continues along several directions. first we will continue enhancing the capability and efficiency of these two modules and incorporating additional semantic analyzers for enhanced deep web discovery. we are also interested in iterative learning and efficient extraction of data quality information  managing data provenance  and incorporating dynamic adaptations into the service discovery and ranking process.
