broder et al.'s  shingling algorithm and charikar's  random projection based approach are considered  state-of-theart  algorithms for finding near-duplicate web pages. both algorithms were either developed at or used by popular web search engines. we compare the two algorithms on a very large scale  namely on a set of 1b distinct web pages. the results show that neither of the algorithms works well for finding near-duplicate pairs on the same site  while both achieve high precision for near-duplicate pairs on different sites. since charikar's algorithm finds more near-duplicate pairs on different sites  it achieves a better precision overall  namely 1 versus 1 for broder et al. 's algorithm. we present a combined algorithm which achieves precision 1 with 1% of the recall of the other algorithms.
categories and subject descriptors
h.1  information storage and retrieval : content
analysis and indexing; h.1  information interfaces and presentation : hypertext/hypermedia
general terms
algorithms  measurement  experimentation
keywords
near-duplicate documents  content duplication  web pages
1. introduction
　duplicate and near-duplicate web pages are creating large problems for web search engines: they increase the space needed to store the index  either slow down or increase the cost of serving results  and annoy the users. thus  algorithms for detecting these pages are needed.
　a naive solution is to compare all pairs to documents. since this is prohibitively expensive on large datasets  manber  and heintze  proposed first algorithms for detecting near-duplicate documents with a reduced number
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and/or a fee.
sigir'1  august 1  1  seattle  washington  usa.
copyright 1 acm 1-1/1 ...$1.
of comparisons. both algorithms work on sequences of adjacent characters. brin et al.  started to use word sequences to detect copyright violations. shivakumar and garcia-molina  1  1  continued this research and focused on scaling it up to multi-gigabyte databases . broder et al.  also used word sequences to efficiently find nearduplicate web pages. later  charikar  developed an approach based on random projections of the words in a document. recently hoad and zobel  developed and compared methods for identifying versioned and plagiarised documents.
　both broder et al. 's and charikar's algorithm have elegant theoretical justifications  but neither has been experimentally evaluated and it is not known which algorithm performs better in practice. in this paper we evaluate both algorithms on a very large real-world data set  namely on 1b distinct web pages. we chose these two algorithms as both were developed at or used by successful web search engines and are considered  state-of-the-art  in finding nearduplicate web pages. we call them algorithm b and c.
　we set all parameters in alg. b as suggested in the literature. then we chose the parameters in alg. c so that it uses the same amount of space per document and returns about the same number of correct near-duplicate pairs  i.e.  has about the same recall. we compared the algorithms according to three criteria:  1  precision on a random subset   1  the distribution of the number of term differences per near-duplicate pair  and  1  the distribution of the number of near-duplicates per page.
　the results are:  1  alg. c has precision 1 and alg. b 1. both algorithms perform about the same for pairs on the same site  low precision  and for pairs on different sites  high precision.  however  1% of the near-duplicate pairs found by alg. b belong to the same site  but only 1% of alg. c. thus  alg. c finds more of the pairs for which precision is high and hence has an overall higher precision.  1  the number of term differences per near-duplicate pair are very similar for the two algorithms  but alg. b returns fewer pairs with extremely large term differences.  1  the distribution of the number of near-duplicates per page follows a power-law for both algorithms. however  alg. b has a higher  spread  around the power-law curve. a possible reason for that  noise  is that the bit string representing a page in alg. b is based on a randomly selected subset of terms in the page. thus  there might be  lucky  and  unlucky  choices  leading to pages with an artificially high or low number of near-duplicates. alg. c does not select a subset of terms but is based on all terms in the page.
　finally  we present a combined algorithm that allows for different precision-recall tradeoffs. the precision of one tradeoff is 1 with 1% of the recall of alg. b.
　it is notoriously hard to determine which pages belong to the same site. thus we use the following simplified approach. the site of a page is  1  the domain name of the page if the domain name has at most one dot  i.e.  at most two levels; and  1  it is the domain name minus the string before the first dot  if the domain name has two or more dots  i.e.  three or more levels. for example  the site of www.cs.berkeley.edu/index.html is cs.berkeley.edu.
　the paper is organized as follows: section 1 describes the algorithms in detail. section 1 presents the experiments and the evaluation results. we conclude in section 1.
1. description of the algorithms
　for both algorithms every html page is converted into a token sequence as follows: all html markup in the page is replaced by white space or  in case of formatting instructions  ignored. then every maximal alphanumeric sequence is considered a term and is hashed using rabin's fingerprinting scheme  1  1  to generate tokens  with two exceptions:  1  every url contained in the text of the page is broken at slashes and dots  and is treated like a sequence of individual terms.  1  in order to distinguish pages with different images the url in an img-tag is considered to be a term in the page. more specifically  if the url points to a different host  the whole url is considered to be a term. if it points to the host of the page itself  only the filename of the url is used as term. thus if a page and its images on the same host are mirrored on a different host  the urls of the img-tags generate the same tokens in the original and mirrored version.
　both algorithms generate a bit string from the token sequence of a page and use it to determine the near-duplicates for the page. we compare a variant of broder et al.'s algorithm as presented by fetterly et al. 1 and a slight modification of the algorithm in  as communicated by charikar . we explain next these algorithms.
　let n be the length of the token sequence of a page. for alg. b every subsequence of k tokens is fingerprinted using 1-bit rabin fingerprints  which results in a sequence of n   k + 1 fingerprints  called shingles. let s d  be the set of shingles of page d. alg. b makes the assumption that the percentage of unique shingles on which the two pages d and d agree  i.e.   is a good measure for the similarity of d and d. to approximate this percentage every shingle is fingerprinted with m different fingerprinting functions fi for 1 ＋ i ＋ m that are the same for all pages. this leads to n   k + 1 values for each fi. for each i the smallest of these values is called the i-th minvalue and is stored at the page. thus  alg. b creates an m-dimensional vector of minvalues. note that multiple occurrences of the same shingle will have the same effect on the minvalues as a single occurrence. broder et al. showed that the expected percentage of entries in the minvalues vector that two pages agree on is equal to the percentage of unique shingles on which d and d agree. thus  to estimate the similarity of two pages it suffices to determine the percentage of agreeing entries in the minvalues vectors. to save space and speed up the simi-

1
 the only difference is that we omit the wrapping of the shingling  window  from end to beginning described in . larity computation the m-dimensional vector of minvalues is reduced to a m-dimensional vector of supershingles by fingerprinting non-overlapping sequences of minvalues: let m be divisible by m and let. the concatentation of minvalueis fingerprinted with yet another fingerprinting function and is called supershingle.1 this creates a supershingle vector. the number of identical entries in the supershingle vectors of two pages is their b-similarity. two pages are near-duplicates of alg. b or b-similar iff their b-similarity is at least 1.
　the parameters to be set are  and k. following prior work  1  1  we chose m = 1  l = 1  and. we set k = 1 as this lies between k = 1 used in  and k = 1 used in  1  1 . for each page its supershingle vector is stored  which requires 1-bit values or 1 bytes.
　next we describe alg. c. let b be a constant. each token is projected into b-dimensional space by randomly choosing b entries from { 1}. this projection is the same for all pages. for each page a b-dimensional vector is created by adding the projections of all the tokens in its token sequence. the final vector for the page is created by setting every positive entry in the vector to 1 and every non-positive entry to 1  resulting in a random projection for each page. it has the property that the cosine similarity of two pages is proportional to the number of bits in which the two corresponding projections agree. thus  the c-similarity of two pages is the number of bits their projections agree on. we chose b = 1 so that both algorithms store a bit string of 1 bytes per page. two pages are near-duplicates of alg. c or c-similar iff the number of agreeing bits in their projections lies above a fixed threshold t. we set t = 1  see section 1.
　we briefly compare the two algorithms. in both algorithms the same bit string is assigned to pages with the same token sequence. alg. c ignores the order of the tokens  i.e.  two pages with the same set of tokens have the same bit string. alg. b takes the order into account as the shingles are based on the order of the tokens. alg. b ignores the frequency of shingles  while alg. c takes the frequency of terms into account. for both algorithms there can be false positives  non near-duplicate pairs returned as nearduplicates  as well as false negatives  near-duplicate pairs not returned as near-duplicates.  let t be the sum of the number of tokens in all pages and let d be the number of pages. alg. b takes time . alg. c needs time o tb  to determine the bit string for each page. as described in section 1 the c-similar pairs are computed using a trick similar to supershingles. it takes time o d  so that the total time for alg. c is o tb .
1. experiments
　both algorithms were implemented using the mapreduce framework . mapreduce is a programming model for simplified data processing on machine clusters. programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. the algorithms were executed on a set of 1b unique pages collected during a crawl of google's crawler. a preprocessing step grouped pages with the same token sequence into identity sets and removed for every identity set all but one page.

1
 megashingles were introduced in  to speed up the algorithm even further. since they do not improve precision or recall  we did not implement them.
about 1% of the pages were removed in this step - we do not know the exact number as the preprocessing was done before we received the pages.
　alg. b and c found that between 1% and 1% of the pages after duplicate removal have a near-duplicate. thus the total number of near-duplicates and duplicates is roughly the same as the one reported by broder et al.   1%  and by fetterly et al.   1%  on their collections of web pages. the exact percentage of duplicates and nearduplicates depends on the crawler used to gather the web pages and especially on its handling of session-ids  which frequently lead to exact duplicates. note that the focus of this paper is not on determining the percentage of nearduplicates on the web  but to compare alg. b and c on the same large real-world data set. the web pages resided on 1m hosts with an average of 1 pages per host. the distribution of the number of pages per host follows a powerlaw. we believe that the pages used in our study are fairly representative of the publically available web and thus form a useful large-scale real-world data set for the comparison.
　as it is not possible to determine by hand all near-duplicate pairs in a set of 1b pages we cannot determine the recall of the algorithms. instead we chose the threshold t in alg. c so that both algorithms returned about the same number of correct near-duplicate pairs  i.e.  they have about the same recall  without actually knowing what it is . then we compared the algorithms based on  1  precision   1  the distribution of the number of term differences in the nearduplicate pairs  and  1  the distribution of the number of near-duplicates per page. comparison  1  required human evaluation and will be explained next.
1 human evaluation
　we randomly sampled b-similar and c-similar pairs and had them evaluated by a human1  who labeled each nearduplicate pair either as correct  incorrect  or undecided. we used the following definition for a correct near-duplicate: two web pages are correct near-duplicates if  1  their text differs only by the following: a session id  a timestamp  an execution time  a message id  a visitor count  a server name  and/or all or part of their url  which is included in the document text    1  the difference is invisible to the visitors of the pages   1  the difference is a combination of the items listed in  1  and  1   or  1  the pages are entry pages to the same site. the most common example of url-only differences are  parked domains   i.e. domains that are for sale. in this case the url is a domain name and the html page retrieved by the url is an advertisement page for buying that domain. pages of domains for sale by the same organization differ usually only by the domain name  i.e.  the url. examples of case  1  are entry pages to the same porn site with some different words.
　a near-duplicate pair is incorrect if the main item s  of the page was  were  different. for example  two shopping pages with common boilerplate text but a different product in the page center is an incorrect near-duplicate pair.

1
 a different approach would be to check the correctness of near-duplicate pages  not pairs  i.e.  sample pages for which the algorithm found at least one near-duplicate and then check whether at least one of its near-duplicates is correct. however  this approach seemed to require too many human comparisons since a page with at least one b-similar page has in the average 1 b-similar pages.

figure 1: the degree distribution in the b-similarity graph in log-log scale.
b-similaritynumber of near-duplicatespercentage1 11%1 11%1 11%1 11%1 11%table 1: number of near-duplicate pairs found for each b-similarity value.
　the remaining near-duplicate pairs were rated undecided. the following three reasons covered 1% of the undecided pairs:  1  prefilled forms with different  but erasable values such that erasing the values results in the same form;  1  a different  minor  item  like a different text box on the side or the bottom;  1  pairs which could not be evaluated. to evaluate a pair the pages as stored at the time of the crawl were visually compared and a linux diff operation was performed on the two token sequences. the diff output was used to easily find the differences in the visual comparison. however  for some pages the diff output did not agree with visual inspection. this happened  e.g.  because one of these pages automatically refreshed and the fresh page was different from the crawled page. in this case the pair was labeled as  cannot evaluate . a pair was also labeled as  cannot evaluate  when the evaluator could not discern whether the difference in the two pages was major or minor. this happened mostly for chinese  japanese  or korean pages.
1 the results for algorithm b
　alg. b generated 1 supershingles per page  for a total of 1b supershingles. they were sorted and for each pair of pages with an identical supershingle we determined its bsimilarity. this resulted in 1b b-similar pairs  i.e.  pairs with b-similarity at least 1.
　let us define the following b-similarity graph: every page is a node in the graph. there is an edge between two nodes iff the pair is b-similar. the label of an edge is the bsimilarity of the pair  i.e.  1  1  1  1  or 1. the graph has 1b edges  about half of them have label 1  see table 1.  a node is considered a near-duplicate page iff it is incident to at least one edge. alg. b found 1m near-duplicate pages. the average degree of the b-similarity graph is abount 1. figure 1 shows the degree distribution in log-log scale. it follows a power-law with exponent about -1.
　we randomly sampled 1 b-similar pairs. in 1% of the cases both pages belonged to the same site. we then
nearnumber ofcorrectnotundecideddupspairscorrectall1.1.1.1  1 same site1.1.1.1  1 diff. sites1.1.1.1  1 b-sim 1111  1 b-sim 1111  1 b-sim 1111  1 b-sim 1111  1 b-sim 1111  1 table 1: alg. b: fraction of correct  not correct  and undecided pairs  with the fraction of prefilled  erasable forms out of all pairs in that row in parenthesis.
all pairssame siteurl only1  1% 1  1% time stamp only1  1% 1  1% combination1  1% 1  1% execution time only1  1% 1  1% visitor count only1  1% 1  1% rest1  1% 1  1% table 1: the distribution of differences for correct b-similar pairs.
subsampled these pairs and checked each of the resulting 1 pairs for correctness  see table 1.  the overall precision is 1. however  the errors arise mostly for pairs on the same site: there the precision drops to 1  while for pairs on different sites the precision is 1. the reason is that very often pages on the same site use the same boilerplate text and differ only in the main item in the center of the page. if there is a large amount of boilerplate text  chances are good that the algorithm cannot distinguish the main item from the boilerplate text and classifies the pair as near-duplicate.
　precision improves for pairs with larger b-similarity. this is expected as larger b-similarity means more agreement in supershingles. while the correctness of b-similarity 1 is only
1  this value increases to 1 for b-similarity 1  and to 1 for b-similarity larger than 1. however  less than half of the pairs have b-similarity 1 or more. table 1 analyzes the correct b-similar pairs. it shows that url-only differences account for 1% of the correct pairs. for pairs on different sites 1 out of the 1 b-similar pairs differ only in the url. this explains largely the high precision in this case. time stamps-only differences  execution time-only differences  and combinations of differences are about equally frequent. the remaining cases account for less than 1% of the correct pairs.
　only 1% of all pairs are labeled undecided. table 1 shows that 1% of them are on the same site. almost half the cases are pairs that could not be evaluated. prefilled  erasable forms are the reason for 1% of the cases. differences in minor items account for only 1%.
　next we analyze the distribution of term differences for the 1 b-similar pairs. to determine the term difference of a pair we executed the linux diff command over the two token sequences and used the number of tokens that were returned. the average term difference is 1  the mean is
all pairssame sitecannot evaluate1  1% 1  1% form1  1% 1  1% different minor item1  1% 1  1% other1  1% 1  1% table 1: reasons for undecided b-similar pairs.

figure 1: the distribution of term differences in the sample of alg. b.
1  1% of the pairs have term difference 1  1% have term difference less than 1. for 1 pairs the term difference is larger than 1. none of them are correct near-duplicates. they mostly consist of repeated text in one page  like repeated lists of countries  that is completely missing in the other page and could probably be avoided if the frequency of shingles was taken into account. figure 1 shows the distribution of term difference up to 1. the spike around 1 to 1 consists of 1 pages and is mostly due to two data bases on the web. it contains 1 pairs from the nih nucleotide database and 1 pairs from the historic herefordshire database. these two databases are a main source of errors for alg. b. all of the evaluated pairs between pages of one of these database were rated as incorrect. however  1% of the pairs in the random sample of 1 pairs came from the nih database and 1% came from the herefordshire database1. the pages in the nih database consist of 1 tokens and differ in 1 consecutive tokens  the pages in the herefordshire database consist of 1 tokens and differ in about 1 consecutive tokens. in both cases alg. b has a good chance of picking two out of the six supershingles from the long common token sequences originating from boilerplate text. however  the number of different tokens is large enough so that alg. c returned only three of the pairs in the sample as near-duplicate pairs.
1 the results for algorithm c
　we partitioned the bit string of each page into 1 nonoverlapping 1-byte pieces  creating 1b pieces  and computed the c-similarity of all pages that had at least one piece in common. this approach is guaranteed to find all pairs of pages with difference up to 1  i.e.  c-similarity 1  but might miss some for larger differences.
　alg. c returns all pairs with c-similarity at least t as near-duplicate pairs. as discussed above we chose t so that both algorithms find about the same number of correct near-

1
 a second independent sample of 1 near-duplicate pairs confirmed these percentages.

figure 1: the degree distribution in the c-similarity graph in log-log scale.
duplicate pairs. alg. b found 1m near-duplicate pairs containing about 1m 1 「 1m correct near-duplicate pairs. for t = 1 alg. c found 1m near-duplicate pairs containing about 1m   1 = 1m correct nearduplicate pairs. thus  we set t = 1. in a slight abuse of notation we call a pair c-similar iff it was returned by our implementation. the difference to before is that there might be some pairs with c-similarity 1 that are not c-similar because they were not returned by our implementation.
　we define the c-similarity graph analog to the b-similarity graph. there are 1m nodes with at least one incident edge  i.e.  near-duplicate pages. this is almost 1% more than for alg. b. the average degree in the c-similarity graph is almost 1. figure 1 shows the degree distribution in loglog scale. it follows a power-law with exponent about -1.
　we randomly sampled 1 near-duplicate pairs. out of them 1% belonged to the same site. in a random subsample of 1 near-duplicate pairs alg. c achieves an overall precision of 1 with 1% incorrect pairs and 1% undecided pairs  see table 1.  for pairs on different sites the precision is 1 with only 1% incorrect pairs and 1% undecided pairs. for pairs on the same site the precision is only 1 with 1% incorrect pairs and 1% undecided pairs. the number in parenthesis gives the percentage of pairs out of all pairs that were marked undecided because of prefilled  but erasable forms. it shows that these pages are the main reason for the large number of undecided pairs of alg. c.
　table 1 also lists the precision for different c-similarity ranges. surprisingly precision is highest for c-similarity between 1 and 1. this is due to the way we break urls at slashes and dots. two pages that differ only in the url usually differ in 1 to 1 tokens since these urls are frequently domain names. this often places the pair in the range between 1 and 1. indeed 1% of the pairs that differ only in the url fall into this range. this explains 1 out of the 1 correct near-duplicates for this c-similarity range.
　table 1 analyzes the correct near-duplicate pairs. urlonly differences account for 1%  combinations of differences for 1%  time stamps and execution time for 1% together with about half each. the remaining reasons account for 1%. for near-duplicate pairs on the same site only 1% of the correct near-duplicate pairs are caused by url-only differences  while 1% are due to a combination of reasons and 1% to time stamps and execution time. for pairs on different sites 1 of the 1 c-similar pairs differ only in the url. this explains the high precision in that case.
table 1 shows that 1% of the undecided pairs are on the
nearnumber ofcorrectnotundecideddupspairscorrectall1.1.1.1  1 same site1.1.1.1  1 different site1.1.1.1  1 c-sim − 1111  1 1  
c-sim − 1111  1 1  
c-sim − 1111  1 c-sim   1111  1 table 1: alg. c: fraction of correct  not correct  and undecided pairs  with the fraction of prefilled  erasable forms out of all pairs in that row in parenthesis.
all pairssame siteurl only1  1% 1  1% combination1  1% 1  1% time stamp only1  1% 1  1% execution time only1  1% 1  1% message id only1  1% 1  1% rest1  1% 1  1% table 1: the distribution of differences for correct c-similar pairs.
same site and 1% of the undecided pairs are due to forms. only 1% are could not be evaluated. the total number of such cases  1  is about the same as for alg. b  1 .
　we also analyzed the term difference in the 1 nearduplicate pairs sampled from alg. c. figure 1 shows the number of pairs for a given term difference up to 1. the average term differences is 1  but this is only due to outliers  the mean is 1  1% had a term difference of 1  and 1% had a term difference smaller than 1. there were 1 pairs with term differences larger than 1.
　the site which causes the largest number of incorrect csimilar pairs is http://www.businessline.co.uk/  a uk business directory that lists in the center of each page the phone number for a type of business for a specific area code. two such pages differ in about 1 not necessarily consecutive tokens and agree in about 1 tokens. in a sample of 1 c-similar pairs  1% were pairs that came from this site1  all such pairs that were evaluated in the subsample of 1 were incorrect. since the token differences are non-consecutive  shingling helps: if x different tokens are separated by y common tokens s.t. any consecutive sequence of common tokens has length less than k  then x+y +k  1 different shingles are generated. indeed  none of these pairs in the sample was returned by alg. b.
1 comparison of algorithms b and c
1.1 graph structure
　the average degree in the c-similarity graph is smaller than in the b-similarity graph  1 vs. 1  and there are fewer high-degree nodes in the c-similarity graph: only

1
 in a second independent random sample of 1 nearduplicate pairs we found 1% such pairs.
all pairssame siteform1  1% 1  1% cannot evaluate1  1% 1  1% other1  1% 1  1% different minor item1  1% 1  1% table 1: reasons for undecided c-similar pairs.

figure 1: the distribution of term differences in the sample of alg. c.
1% of the nodes in the c-similarity graph have degree at least 1 vs. 1% in the b-similarity graph. the plots in figures 1 and 1 follow a power-law distribution  but the one for alg. b  spreads  around the power-law curve much more than the one for alg. c. this can be explained as follows: each supershingle depends on 1 terms  1 shingles with 1 terms each.  two pages are b-similar iff two of its supershingles agree. if the page is  lucky   two of its supershingles consist of common sequences of terms  like lists of countries in alphabetical order  and the corresponding node has a high degree. if the page is  unlucky   all supershingles consist of uncommon sequences of terms  leading to low degree. in a large enough set of pages there will always be a certain percentage of pages that are  lucky  and  unlucky   leading to a deviation from the power-law. alg. c does not select subsets of terms  its bit string depends on all terms in the sequence  and thus the power-law is stricter followed.
1.1 manual evaluation
　alg. c outperforms alg. b with a precision of 1 versus 1 for alg. b. a more detailed analysis shows a few interesting differences.
　pairs on different sites: both algorithms achieve high precision  1 resp. 1   1% of the b-similar pairs and 1% of the c-similar pairs are on different sites. thus  alg. c is superior to alg. b  higher precision and recall  for pairs on different sites. the main reason for the higher recall is that alg. c found 1 pairs with url-only differences on different sites  while alg. b returned only 1 such pairs.
　pairs on the same site: neither algorithm achieves high precision for pages on the same site. alg. b returned 1 pairs with 1 correct pairs  precision 1   while alg. c' returns 1 pairs with 1 correct pairs  precision 1 . thus  alg. b has 1% higher recall  while alg. c achieves slightly higher precision for pairs on the same site. however  a combination of the two algorithms as described in the next section can achieve a much higher precision for pairs on the same site without sacrificing much recall.
evaluationalg balg cincorrect1undecided1correct1table 1: the evaluations for the near-duplicate pairs with term difference larger than 1.
b-similarityc-similarity average1.111.111.1table 1: for a given b-similarity the average csimilarity.
　all pairs: alg. c found more near-duplicates with urlonly differences  1 vs. 1   while alg. b found more correct near-duplicates whose differences lie only in the time stamp or execution time  1 vs. 1 . alg. c found many more undecided pairs due to prefilled  erasable forms than alg. b  1 vs. 1 . in many applications these forms would be considered correct near-duplicate pairs. if so the overall precision of alg. c would increase to 1  while the overall precision of alg. b would be 1.
1.1 term differences
　the results for term differences are quite similar  except for the larger number  1 vs. 1  of pairs with term differences larger than 1. to explain this we analyzed each of these 1 pairs  see table 1 . however  there are mistakes in our counting methods due to a large number of image urls that were used for layout improvements and are counted by diff  but are invisible for the user. they affect 1 incorrect pairs and 1 undecided pairs of alg. c  but no pair of alg. b. subtracting them leaves both algorithms with 1 incorrect pairs with large term differences  and alg. b with 1 and alg. c with 1 undecided pairs with large term differences. seventeen pairs of alg. c with large term differences were rated as correct - they are entry pages to the same porn site. altogether we conclude that the performance of the two algorithms with respect to term differences is quite similar  but alg. b returns fewer pairs with very large term differences. however  with 1 incorrect and 1 correct pairs with large term difference the precision of alg. c is barely influenced by its performance on large term differences.
1.1 correlation
　to study the correlation of b-similarity and c-similarity we determined the c-similarity of each pair in a random sample of 1 b-similar pairs. about 1% had c-similarity at least 1. the average c-similarity was almost 1. table 1 gives for different b-similarity values the average csimilarity. as can be seen the larger the b-similarity the larger the average c-similarity. to show the relationship more clearly figure 1 plots for each b-similarity level the distribution of c-similarity values. for b-similarity 1  most of the pairs have c-similarity below 1 with a peak at 1. for higher b-similarity values the peaks are above 1.

figure 1: the c-similarity distribution for various fixed b-similarities.
c-similarityb-similarity average11111111table 1: for a given c-similarity range the average b-similarity.
　we also determined the b-similarity for a random sample of 1 c-similar pairs. again about 1% of the pairs were b-similar  but for 1% of the pairs the b-similarity was 1. table 1 gives the details for various c-similarity ranges.
1 the combined algorithm
　the algorithms wrongly identify pairs as near-duplicates either  1  because a small difference in tokens causes a large semantic difference or  1  because of unlucky random choices. as the bad cases for alg b showed pairs with a large amount of boilerplate text and a not very small number  like 1 or 1  of different tokens that are all consecutive are at risk of being wrongly identified as near-duplicates by alg. b  but are at much less risk by alg. c. thus we studied the following combined algorithm: first compute all b-similar pairs. then filter out those pairs whose c-similarity falls below a certain threshold. to choose a threshold we plotted the precision of the combined algorithm for different threshold values in figure 1. it shows that precision can significantly improve if a fairly high value of c-similarity  like 1  is used.
　to study the impact of different threshold values on recall let us define r to be the number of correct near-duplicate pairs returned by the combined algorithm divided by the number of correct near-duplicate pairs returned by alg. b. we chose alg. b because the combined algorithm tries to filter out the false positives of alg. b. we randomly subsampled 1 pairs out of the 1 pairs that were scored for alg. b  creating the sample s1. the remaining pairs from the 1 pairs form sample s1. we used s1 to choose a cutoff threshold and s1 as testing set to determine the resulting precision and r-value. figure 1 plots for s1 precision versus r for all c-similar thresholds between 1 and 1. as ex-

figure 1: for each c-similarity threshold the corresponding precision of the combined algorithm.

percentage of correct near-duplicate pairs returned
figure 1: for the training set s1 the r-value versus precision for different cutoff thresholds t.
pected precision decreases with increasing r. the long flat range corresponds to different thresholds between 1 and 1 for which precision stays roughly the same while recall increases significantly. for the range between 1 and 1 table 1 gives the resulting precision and r-values. it shows that a c-similarity threshold of 1  1  or 1 would be a good choice  achieving a precision of 1 while keeping r over 1. we picked 1.
　the resulting algorithm returns on the testing set s1 out of the 1 pairs as near-duplicates with a precision of 1 and an r-value of 1. for comparison consider using alg. b with a b-similarity cutoff of 1. that algorithm
percentage ofprecisionc-similaritycorrect pairs returnedthreshold111.1.1111.1.1111.1.1111.1.1111table 1: on the training set s1 for different csimilarity thresholds the corresponding precision and percentage of returned correct pairs.
nearnumbercorrectin-un-rdupsofcorrectdeci-pairsdedall1.1.1.1.1same site1.1.1.1.1different site1.1.1.1.1table 1: the combined algorithm on the testing set s1: fraction of correct  incorrect  and undecided pairs and r-value.
only returns 1 correct near-duplicates with a precision of 1  while the combined algorithm returns 1 correct nearduplicates with a precision of 1. thus  the combined algorithm is superior in both precision and recall to using a stricter cutoff for alg. b. note that the combined algorithm can be implemented with the same amount of space since the bit strings for alg. c can be computed  on the fly  during filtering.
　table 1 also shows that 1% of the returned pairs are on the same site and that the precision improvement is mostly achieved for these pairs. with 1 this number is much better than either of the individual algorithms.
　a further improvement could be achieved by running both alg. c and the combined algorithm and returning the pairs on different sites from alg. c and the pairs for the same site from the combined algorithm. this would generate 1b 1 「 1m pairs one the same site with 1m correct pairs and 1b 1   1 「 1m pairs on different sites with about 1m correct pairs. thus approximately 1m correct pairs would be returned with a precision of 1  i.e.  both recall and precision would be superior to the combined algorithm alone.
1. conclusions
　we performed an evaluation of two near-duplicate algorithms on 1b web pages. neither performed well on pages from the same site  but a combined algorithm did without sacrificing much recall.
　two changes might improve the performance of alg. b and deserve further study:  1  a weighting of shingles by frequency and  1  using a different number k of tokens in a shingle. for example  following  1  1  one could try k = 1. however  recall that 1% of the incorrect pairs are caused by pairs of pages in two databases on the web. in these pairs the difference is formed by one consecutive sequence of tokens. thus  reducing k would actually increase the chances that pairs of pages in these databases are incorrectly identified as near-duplicates.
　note that alg. c also could work with much less space. it would be interesting to study how this affects its performance. additionally it would be interesting to explore whether applying alg. c to sequences of tokens  i.e.  shingles  instead of individual tokens would increase its performance.
　as our results show both algorithms perform poorly on pairs from the same site  mostly due to boilerplate text. using a boilerplate detection algorithm would probably help. another approach would be to use a different  potentially slower algorithm for pairs on the same site and apply  one of  the presented algorithms to pairs on different sites.
1. acknowledgments
　i want to thank mike burrows  lars engebretsen  abhay puri and oren zamir for their help and useful discussions.
